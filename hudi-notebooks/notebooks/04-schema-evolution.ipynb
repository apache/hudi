{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405afd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Licensed to the Apache Software Foundation (ASF) under one\n",
    "#  or more contributor license agreements.  See the NOTICE file\n",
    "#  distributed with this work for additional information\n",
    "#  regarding copyright ownership.  The ASF licenses this file\n",
    "#  to you under the Apache License, Version 2.0 (the\n",
    "#  \"License\"); you may not use this file except in compliance\n",
    "#  with the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e4cce8",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://hudi.apache.org/assets/images/hudi-logo-medium.png\" alt=\"Hudi logo\" width=\"100%\" height=\"320\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf0cdb-d26c-461a-a8ab-5d5c118c623a",
   "metadata": {},
   "source": [
    "# Schema Evolution with Apache Hudi: Concepts and Practical Use\n",
    "Welcome to this hands-on guide to Schema Evolution with Apache Hudi. In a modern data lake, schemas are rarely static. As business requirements change, we need to be able to modify our data's structure, such as adding new columns or changing data types—without breaking our data pipelines. Hudi provides powerful, built-in features to manage these changes gracefully, turning your data lake into a flexible, database-like environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed91294c-38f0-4764-b3bf-8bf86977faa3",
   "metadata": {},
   "source": [
    "In this notebook, we will demonstrate the following key schema evolution concepts:\n",
    "\n",
    "## 1. Schema Evolution on Write\n",
    "\n",
    "Hudi allows several schema changes safely as long as they adhere to backward-compatibility rules.\n",
    "\n",
    "**Supported Changes:**\n",
    "\n",
    "- Adding new nullable columns at root or nested levels\n",
    "- Promoting a field’s data type within compatibility matrix (e.g., int → long)\n",
    "\n",
    "## 2. Schema Evolution on Read (Experimental)\n",
    "Hudi offers experimental, more flexible evolution behavior only during queries. This allows operations like renaming, deleting, or modifying nested columns when reading.\n",
    "\n",
    "To enable it we need to use the below configuration.\n",
    "- hoodie.schema.on.read.enable=true\n",
    "\n",
    "\n",
    "***Supported transformations during read include:***\n",
    "\n",
    "- Add/delete/modify/move columns\n",
    "- Rename columns\n",
    "- Operations on nested columns of the Array type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3c6982-57b4-4d95-bdab-bd913fa0c642",
   "metadata": {},
   "source": [
    "## Setting up the Environment\n",
    "First, we begin by importing our necessary libraries and starting a SparkSession configured to work with Hudi and MinIO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c8782a-689b-43a5-a928-4be73c065d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadc900d-8de8-488a-b7c9-283924331945",
   "metadata": {},
   "source": [
    "Now, let's start the SparkSession. We'll give it the app name 'Schema-Evolution' and configure it to use our Hudi and MinIO settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8daaba-6aef-4e1c-a7f7-268ea972040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = get_spark_session(\"Hudi Schema Evolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff85e1cd-7167-454a-a788-cd5172d16298",
   "metadata": {},
   "source": [
    "## Initial Data and Table Creation\n",
    "We'll start with a simple dataset of ride information. This will be the foundation of our Hudi table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6511f99-4d78-492e-b0e6-f2fec3a77d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, IntegerType\n",
    "\n",
    "initial_data = [\n",
    "    (\"2025-08-10 08:15:30\", \"uuid-101\", \"rider-X\", \"driver-A\", 18.50, \"new_york\", 900),   # 15 mins\n",
    "    (\"2025-08-10 09:22:10\", \"uuid-102\", \"rider-Y\", \"driver-B\", 22.75, \"chicago\", 1200),   # 20 mins\n",
    "    (\"2025-08-10 10:05:45\", \"uuid-103\", \"rider-Z\", \"driver-C\", 14.60, \"boston\", 1100),    # 18 mins\n",
    "    (\"2025-08-10 11:25:25\", \"uuid-104\", \"rider-W\", \"driver-D\", 18.90, \"seattle\", 850),    # 14 mins\n",
    "    (\"2025-08-10 11:55:30\", \"uuid-105\", \"rider-V\", \"driver-E\", 20.40, \"miami\", 1000)      # 16.6 mins\n",
    "]\n",
    "\n",
    "# Schema for our dataset\n",
    "initial_schema = StructType([\n",
    "    StructField(\"ts\", StringType(), False),\n",
    "    StructField(\"uuid\", StringType(), False),\n",
    "    StructField(\"rider\", StringType(), True),\n",
    "    StructField(\"driver\", StringType(), True),\n",
    "    StructField(\"fare\", DoubleType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"trip_duration\", IntegerType(), True),  # Initially int\n",
    "])\n",
    "\n",
    "initial_df = spark.createDataFrame(initial_data, initial_schema)\n",
    "initial_df.printSchema()\n",
    "display(initial_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a2def-6cf4-47a0-8294-0c2bbfe67b73",
   "metadata": {},
   "source": [
    "Now, let's create a new Hudi table using this data. This table is the starting point for all our schema changes.\n",
    "While the concepts of schema evolution apply to both COW and Merge-on-Read (MOR) tables, this specific notebook demonstrates the process using a COW table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31390a96-4782-4c4a-8d55-ca6500962d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"rides_schema_evolution\"\n",
    "base_path = \"s3a://warehouse/hudi-schema-evolution\"\n",
    "\n",
    "hudi_conf = {\n",
    "    \"hoodie.table.name\": table_name,\n",
    "    \"hoodie.datasource.write.recordkey.field\": \"uuid\",\n",
    "    \"hoodie.datasource.write.table.type\": \"COPY_ON_WRITE\",\n",
    "    \"hoodie.datasource.write.precombine.field\": \"ts\",\n",
    "    \"hoodie.datasource.write.partitionpath.field\": \"city\"\n",
    "}\n",
    "\n",
    "initial_df.write.format(\"hudi\") \\\n",
    "    .options(**hudi_conf) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{base_path}/{table_name}\")\n",
    "\n",
    "# Register a temp view to easily query the table\n",
    "spark.read.format(\"hudi\").load(f\"{base_path}/{table_name}\").createOrReplaceTempView(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9717427-d3b3-45f4-a380-7e124ce7e73e",
   "metadata": {},
   "source": [
    "## On-Write Evolution Example: Add a Column & Promote Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177ed273-50ae-44bd-a072-fe3b54612d41",
   "metadata": {},
   "source": [
    "### A. Adding a New Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb498ed2-0628-439b-a118-96c137855b8f",
   "metadata": {},
   "source": [
    "Now, imagine we need to add a new column, ride_status, to our data. Hudi can handle this seamlessly. We'll create a new DataFrame that includes this column for an existing record and upsert it into our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8548ba-7659-461b-b558-a36211ff5748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with a new column for one record\n",
    "new_column_data = [\n",
    "    (\"2025-08-10 08:15:30\", \"uuid-101\", \"rider-X\", \"driver-A\", 18.50, \"new_york\", 900, \"completed\")\n",
    "]\n",
    "\n",
    "new_columns_schema = StructType([\n",
    "    StructField(\"ts\", StringType(), False),\n",
    "    StructField(\"uuid\", StringType(), False),\n",
    "    StructField(\"rider\", StringType(), True),\n",
    "    StructField(\"driver\", StringType(), True),\n",
    "    StructField(\"fare\", DoubleType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"trip_duration\", IntegerType(), True),  # Initially int\n",
    "    StructField(\"ride_status\", StringType(), True),\n",
    "])\n",
    "\n",
    "new_column_df = spark.createDataFrame(new_column_data, new_columns_schema)\n",
    "new_column_df.printSchema()\n",
    "display(new_column_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c528a8-7339-4bcb-a154-82b84267643c",
   "metadata": {},
   "source": [
    "We'll now upsert the new data into our existing Hudi table. Hudi will detect the new schema, merge the data, and update the table's schema automatically. The records that don't have a value for ride_status will have null in the new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbed9c8-7195-48a2-98ed-55b4e6d6ba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_df.write.format(\"hudi\") \\\n",
    "    .options(**hudi_conf) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(f\"{base_path}/{table_name}\")\n",
    "\n",
    "# Query the updated table to see the new column and its schema\n",
    "updated_df = spark.read.format(\"hudi\").load(f\"{base_path}/{table_name}\")\n",
    "updated_df.printSchema()\n",
    "display(updated_df.select(\"uuid\", \"fare\", \"ride_status\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ac960-1991-4e80-a206-2fd8fed7f25d",
   "metadata": {},
   "source": [
    "### B. Changing a Column's Data Type\n",
    "Next, let's see how Hudi handles a data type change. We'll change the trip_duration column from a int to a more precise long. To ensure the write succeeds, we will first load the existing table schema and then merge it with the updated schema of the trip_update_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08246ce3-15cf-4084-8cd1-d7ba423e751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "updated_data = [\n",
    "    (\"2025-08-10 09:30:40\", \"uuid-102\", \"rider-Y\", \"driver-B\", 22.75, \"chicago\", 1350, \"completed\"),  # New value with long type\n",
    "    (\"2025-08-10 15:35:10\", \"uuid-106\", \"rider-Z\", \"driver-C\", 14.60, \"boston\", 1500, \"completed\")    # New record\n",
    "]\n",
    "\n",
    "updated_schema = StructType([\n",
    "    StructField(\"ts\", StringType(), False),\n",
    "    StructField(\"uuid\", StringType(), False),\n",
    "    StructField(\"rider\", StringType(), True),\n",
    "    StructField(\"driver\", StringType(), True),\n",
    "    StructField(\"fare\", DoubleType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"trip_duration\", LongType(), True),  # upgraded to long\n",
    "    StructField(\"ride_status\", StringType(), True),\n",
    "])\n",
    "\n",
    "trips_updated_df = spark.createDataFrame(updated_data, updated_schema)\n",
    "\n",
    "# Update Hudi config to disable schema reconciliation\n",
    "hudi_conf_update = hudi_conf.copy()\n",
    "hudi_conf_update.update({\n",
    "    \"hoodie.datasource.write.reconcile.schema\": \"false\",\n",
    "    \"hoodie.datasource.write.schema.evolution.enable\": \"true\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e844b32-21af-44ce-a3da-f6119ac2f55b",
   "metadata": {},
   "source": [
    "After upserting this new data, Hudi will automatically handle the schema change. The fare column will now be a DecimalType in the table's schema, and the values for the other records will be correctly cast to the new type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1e14f0-81be-4a89-80a9-2e5b1c308f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsert with new schema\n",
    "trips_updated_df.write.format(\"hudi\") \\\n",
    "    .options(**hudi_conf_update) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(f\"{base_path}/{table_name}\")\n",
    "\n",
    "# Query the table and check the schema\n",
    "updated_schema_df = spark.read.format(\"hudi\").load(f\"{base_path}/{table_name}\")\n",
    "updated_schema_df.printSchema()\n",
    "display(updated_schema_df.select(\"uuid\", \"fare\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72e289e-c169-4d05-b8e2-dccfd42b006b",
   "metadata": {},
   "source": [
    "## On-Read Evolution Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38654f7f-4397-46b6-83a5-ad3ca0de87ff",
   "metadata": {},
   "source": [
    "### Renaming a Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672a3f41-f64f-4a1a-85bb-67ac240f1f46",
   "metadata": {},
   "source": [
    "**Step 1:** Load the Hudi Table & Rename the Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79600dfb-186e-4d6e-afc2-0d2241f74f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Hudi table\n",
    "hudi_df = spark.read.format(\"hudi\").load(f\"{base_path}/{table_name}\")\n",
    "\n",
    "# Rename trip_duration → duration_seconds\n",
    "renamed_df = hudi_df.withColumnRenamed(\"trip_duration\", \"duration_in_seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a5bc9-1025-470f-b368-d698321868c4",
   "metadata": {},
   "source": [
    "**Step 2:** Write Back to Hudi (Upsert)\n",
    "\n",
    "When writing back, ensure schema reconciliation is enabled so Hudi registers the new column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf5b541-4965-48a3-aca6-d1a83b4502eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Hudi config to disable schema reconciliation\n",
    "hudi_conf_rename = hudi_conf.copy()\n",
    "hudi_conf_rename.update({\n",
    "    \"hoodie.datasource.write.reconcile.schema\": \"true\",\n",
    "    \"hoodie.schema.on.read.enable\": \"true\",\n",
    "    \"hoodie.datasource.write.schema.on.read.enable\": \"true\"\n",
    "})\n",
    "\n",
    "# Upsert with new schema\n",
    "renamed_df.write.format(\"hudi\") \\\n",
    "    .options(**hudi_conf_rename) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(f\"{base_path}/{table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6726b985-1937-48e4-addf-af4d8c0a5d2c",
   "metadata": {},
   "source": [
    "**Step 3:** Verify Schema Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da6e8a0-f94b-4da3-9189-b74809f7b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = spark.read.format(\"hudi\").load(f\"{base_path}/{table_name}\")\n",
    "updated_df.printSchema()\n",
    "\n",
    "# Should show duration_in_seconds instead of trip_duration\n",
    "display(updated_df.select(\"uuid\", \"fare\", \"duration_in_seconds\", \"ride_status\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588728f-6e19-4c24-bb8e-4ab3125f2623",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
