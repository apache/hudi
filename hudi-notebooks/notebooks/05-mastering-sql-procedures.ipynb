{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048acfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Licensed to the Apache Software Foundation (ASF) under one\n",
    "#  or more contributor license agreements.  See the NOTICE file\n",
    "#  distributed with this work for additional information\n",
    "#  regarding copyright ownership.  The ASF licenses this file\n",
    "#  to you under the Apache License, Version 2.0 (the\n",
    "#  \"License\"); you may not use this file except in compliance\n",
    "#  with the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65eb164",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://hudi.apache.org/assets/images/hudi-logo-medium.png\" alt=\"Hudi logo\" width=\"100%\" height=\"320\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac745a8-1ca1-48a9-bb9b-1b52a74e898f",
   "metadata": {},
   "source": [
    "# A Hands-on Guide to Hudi SQL Procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce87cb3-5ee0-4907-b3cc-a5ed124cdc05",
   "metadata": {},
   "source": [
    "This notebook is a comprehensive guide to using Hudi's powerful SQL procedures directly from Spark SQL. These procedures, invoked using the `CALL` keyword, allow you to perform advanced table maintenance, auditing, and data management tasks using familiar SQL commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2eba17-5dd8-495c-a91d-d39ad52ef836",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834d707d-d4aa-4072-9e60-49dcb651f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = get_spark_session(\"Hudi SQL Procedures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746ba346-312e-4178-b2c3-0fded1e1f517",
   "metadata": {},
   "source": [
    "First, let's set up a sample Hudi table we'll use throughout this guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda8aad7-ac07-43c9-9299-829cfa0ca38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS trips_cow_hudi_sql\")\n",
    "\n",
    "spark.sql(f\"\"\"CREATE TABLE trips_cow_hudi_sql (\n",
    "    ts STRING,\n",
    "    uuid STRING,\n",
    "    rider STRING,\n",
    "    driver STRING,\n",
    "    fare DOUBLE,\n",
    "    city STRING\n",
    ") USING hudi\n",
    "TBLPROPERTIES (\n",
    "    'hoodie.table.type' = 'COPY_ON_WRITE',\n",
    "    'hoodie.datasource.write.recordkey.field' = 'uuid',\n",
    "    'hoodie.datasource.write.precombine.field' = 'ts'\n",
    ")\n",
    "LOCATION 's3a://warehouse/hudi-sql-procedure/trips_cow_hudi_sql'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8258718-28db-4e98-b2ba-fad5cb39226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"DESCRIBE EXTENDED trips_cow_hudi_sql\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ac5f5-2ab9-490f-b967-df6ed1b3674b",
   "metadata": {},
   "source": [
    "Next, we'll insert some sample data into our new table using a standard SQL `INSERT INTO` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b28bd1b-38b1-4170-ae73-b7ac3211a879",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "  INSERT INTO trips_cow_hudi_sql VALUES\n",
    "    ('2025-08-10 08:15:30', 'uuid-001', 'rider-A', 'driver-X', 18.50, 'new_york'),\n",
    "    ('2025-08-10 09:22:10', 'uuid-002', 'rider-B', 'driver-Y', 22.75, 'san_francisco'),\n",
    "    ('2025-08-10 10:05:45', 'uuid-003', 'rider-C', 'driver-Z', 14.60, 'chicago'),\n",
    "    ('2025-08-10 11:40:00', 'uuid-004', 'rider-D', 'driver-W', 31.90, 'new_york'),\n",
    "    ('2025-08-10 12:55:15', 'uuid-005', 'rider-E', 'driver-V', 25.10, 'san_francisco');\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c16546-ee50-4d9e-b21d-933a4530ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"SELECT * from trips_cow_hudi_sql\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a608022-eded-44bb-95c3-039f21fe3522",
   "metadata": {},
   "source": [
    "### Types of SQL Procedures in Apache Hudi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14694995-5894-46af-af8d-94417958f741",
   "metadata": {},
   "source": [
    "Apache Hudi provides a comprehensive set of SQL procedures categorized by their functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85735aa8-52e2-442b-8a1f-a99e50cd5894",
   "metadata": {},
   "source": [
    "![Hudi SQL Procedures](images/Hudi_SQL_Procedures.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb39131-b67d-4152-a4ff-e7c383104edc",
   "metadata": {},
   "source": [
    "### Hudi SQL Procedures: A Detailed Look"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1be30f-0b0f-4c56-900b-69f7e7f56200",
   "metadata": {},
   "source": [
    "Hudi's SQL procedures, invoked using the `CALL` keyword, are a powerful way to interact with table's metadata and services. Let's explore some of the most common ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ae2a70-b0d3-4734-8a53-9116b712f6df",
   "metadata": {},
   "source": [
    "#### `Help Procedure`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e0ed4f-1808-42d6-a613-09c2af5db365",
   "metadata": {},
   "source": [
    "Use `help` procedure to inspect a stored procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdecb4a-0253-455e-8582-da6690cdffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"CALL help(cmd => 'show_commits')\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be419b77-c092-40f9-bc78-23bf7cef2e7d",
   "metadata": {},
   "source": [
    "### 1. Commit Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd74f073-968c-4436-9529-12a8dfc78117",
   "metadata": {},
   "source": [
    "#### `show_commits`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ecccec-01eb-40ac-a9cf-49d7c44dc046",
   "metadata": {},
   "source": [
    "This procedure shows the history of all completed transactions on the table. It's a great way to audit changes and understand the evolution of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b829090c-e786-420c-8269-25afe917dd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"CALL show_commits('trips_cow_hudi_sql')\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4b5e89-2333-4b5c-920a-d8c2462618a0",
   "metadata": {},
   "source": [
    "#### `show_commits_metadata`\n",
    "\n",
    "This procedure provides a detailed breakdown of a specific commit. It shows the commit time, the number of files and records affected, and other key metrics. This is invaluable for understanding the impact of a particular write operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e6ec5d-bc91-41ab-a870-803351ac569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"CALL show_commits_metadata(table => 'trips_cow_hudi_sql', limit => 3)\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d598443-5d2a-41ff-b832-fab3fe201990",
   "metadata": {},
   "source": [
    "#### `show_commit_extra_metadata`\n",
    "This procedure shows extra metadata about a specific commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb5535c-9ed6-4d10-9c59-cfe312af2bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's get a list of all commit times from the table.\n",
    "all_commits = [row[0] for row in spark.sql(f\"CALL show_commits('trips_cow_hudi_sql')\").collect()]\n",
    "\n",
    "# We'll use the latest commit time for this example.\n",
    "latest_commit_time = all_commits[-1]\n",
    "\n",
    "# Now, we'll call the procedure to get the metadata for that commit.\n",
    "display(spark.sql(f\"\"\"CALL show_commit_extra_metadata(table => 'trips_cow_hudi_sql', instant_time => '{latest_commit_time}')\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0a5d0d-f053-4b0e-8c0f-a3091a992431",
   "metadata": {},
   "source": [
    "Let's add some more records into the table to get another commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c76d174-40c7-40b7-8c2b-c0b96e963ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "  INSERT INTO trips_cow_hudi_sql VALUES\n",
    "    ('2025-08-11 08:15:30', 'uuid-006', 'rider-A', 'driver-V', 38.75, 'new_york'),\n",
    "    ('2025-08-11 12:55:15', 'uuid-007', 'rider-E', 'driver-X', 85.10, 'san_francisco');\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cada72-2474-4fc9-b20b-4a405b4b086f",
   "metadata": {},
   "source": [
    "Now, check the commits again on the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2da4d17-639c-4b9f-921a-9318b47b40b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"CALL show_commits('trips_cow_hudi_sql')\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b04d8a4-406b-4218-9c6e-66d621495814",
   "metadata": {},
   "source": [
    "From above output, We can clearly see that we have now 2 commits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ccacc0-8689-4f3b-87e5-586adfd13f95",
   "metadata": {},
   "source": [
    "#### `show_commit_files`\n",
    "\n",
    "This procedure shows the list of files that were part of a specific commit. It's useful for debugging and understanding the physical changes on the file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de052711-995c-4616-b1e3-bf14bf159036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's get a list of all commit times from the table.\n",
    "all_commits = [row[0] for row in spark.sql(\"CALL show_commits('trips_cow_hudi_sql')\").collect()]\n",
    "\n",
    "# We'll use the latest commit time for this example.\n",
    "latest_commit_time = all_commits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c404c27b-ee91-4834-8bed-be964e116e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the latest commit time from our previous step.\n",
    "display(spark.sql(f\"\"\"CALL show_commit_files(table => 'trips_cow_hudi_sql', instant_time => '{latest_commit_time}')\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0dda2b-0bd5-48b1-9021-f201c31ee556",
   "metadata": {},
   "source": [
    "#### `COMMITS_COMPARE`\n",
    "\n",
    "This is a powerful procedure to compare the changes between two specific commits. It shows which files and records were added, updated, or deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1fb4f7-6e13-4974-86c8-bc2a0fd9cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"CALL commits_compare(table => 'trips_cow_hudi_sql', path => 's3a://warehouse/hudi-sql-procedure/trips_cow_hudi_sql/')\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2064aa1-39cd-41fd-8545-279c032a687b",
   "metadata": {},
   "source": [
    "### Savepoints and Rollbacks\n",
    "\n",
    "Savepoints are a way to create a stable, named checkpoint on your Hudi timeline. They protect your data from being cleaned or archived, allowing you to roll back to a known-good state if something goes wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7bb960-39a1-467f-900d-31b432327481",
   "metadata": {},
   "source": [
    "#### `create_savepoints`\n",
    "\n",
    "This procedure creates a named savepoint at the current state of the table. You can add a comment to describe the reason for the savepoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341e3c11-d410-42ae-b752-ea0a403506e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll get the latest commit time to create a savepoint at that instant.\n",
    "latest_commit_time = [row[0] for row in spark.sql(\"CALL show_commits('trips_cow_hudi_sql')\").collect()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d66b36f-6d54-4d35-bf35-3778e0c9bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"CALL create_savepoint(table => 'trips_cow_hudi_sql', commit_time => '{latest_commit_time}')\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6a3a92-62b9-4358-8cd3-8df4e41c996c",
   "metadata": {},
   "source": [
    "#### `show_savepoints`\n",
    "\n",
    "This procedure lists all the savepoints that have been created on the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889a445c-900f-4eab-906f-35611d9321eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"CALL show_savepoints(table => 'trips_cow_hudi_sql')\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04312de0-baa2-4373-80fd-b7a7876e50f4",
   "metadata": {},
   "source": [
    "Now, let us query the table to know the records currently it holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f545e6f-9eaa-404b-9239-bb15afa350f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT * FROM trips_cow_hudi_sql\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bc7378-9ca0-41fc-acd4-def100c39782",
   "metadata": {},
   "source": [
    "#### `rollback_to_savepoint`\n",
    "\n",
    "This procedure allows you to roll back the table to a previously created savepoint. This is useful for recovering from bad data writes or other issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ad56b0-cbe0-4956-bd48-ba2945e210e4",
   "metadata": {},
   "source": [
    "To demonstrate, we'll first make an update and then roll back. Here we are changing the fare amount to `25` for uuid `uuid-002`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575308ec-43fd-4430-a782-db5c76dbb96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"UPDATE trips_cow_hudi_sql SET fare = 25.0 WHERE uuid = 'uuid-002'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b39d465-82e5-4214-8510-2e5bc7a1eeef",
   "metadata": {},
   "source": [
    "Let's verify the change. And we can see that the fame amount has been changed to 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8c61f8-3128-490c-9b89-d6933e1a8b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT * FROM trips_cow_hudi_sql where uuid = 'uuid-002'\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bfe7e0-740a-4733-bde4-83391ad9b192",
   "metadata": {},
   "source": [
    "Now, we'll roll back to the savepoint we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559c6eef-55b8-412d-a372-89a8ab414b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"CALL rollback_to_savepoint(table => 'trips_cow_hudi_sql', instant_time => '{latest_commit_time}')\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085dbd08-acfb-4575-9f06-b91aad0356ea",
   "metadata": {},
   "source": [
    "Let's check the table. The update should be gone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1d1b88-ac2f-45f3-a592-27979fa5a7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT * FROM trips_cow_hudi_sql where uuid = 'uuid-002'\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9938b8-f1d8-4792-9f48-4915842dbb1d",
   "metadata": {},
   "source": [
    "#### `delete_savepoint`\n",
    "\n",
    "After a savepoint is no longer needed, you can delete it to allow Hudi to clean and archive the associated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef75fc8-31de-499e-859f-d12c000d7b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"\n",
    "    CALL hudi.delete_savepoint(\n",
    "        table => 'trips_cow_hudi_sql',\n",
    "        table_path => 's3a://warehouse/hudi-sql-procedure/trips_cow_hudi_sql',\n",
    "        instant_time => '{latest_commit_time}'\n",
    "    )\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d7a9e1-e595-4514-a2cc-3269530feba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"CALL show_savepoints(table => 'trips_cow_hudi_sql')\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e264ec0-f5b2-49d9-a52e-a889b02c0b2e",
   "metadata": {},
   "source": [
    "#### `rollback_to_instant`\n",
    "\n",
    "This procedure is a direct way to roll back a specific commit without needing a named savepoint. It's useful for quickly undoing the last transaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b418ba-a233-4a60-8064-09bf8c1cb447",
   "metadata": {},
   "source": [
    "First, let's make a simple update. Let's update the fare amount to 100 where uuid is `uuid-001`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e5a03a-3a3e-4aac-988a-0cbfb3cdab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"UPDATE trips_cow_hudi_sql SET fare = 100.00 WHERE uuid = 'uuid-001'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91e1b4f-aaa5-4b00-a265-1f5baf5afdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the change\n",
    "display(spark.sql(\"SELECT uuid, fare FROM trips_cow_hudi_sql WHERE uuid = 'uuid-001'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d099eebc-4532-4c34-91b4-cc31be7637aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the commit time from the previous commit.\n",
    "last_commit_time = [row[0] for row in spark.sql(f\"CALL show_commits('trips_cow_hudi_sql')\").collect()][0]\n",
    "print(last_commit_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24da6449-61aa-4afa-bca4-4920ebf99ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"CALL show_commits('trips_cow_hudi_sql')\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3237e7-c57d-43b5-aad2-3e3b8ed1c697",
   "metadata": {},
   "source": [
    "Now, we'll roll back the table to the instant before that commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea2adb2-f5d8-4685-84b2-b354fa50a6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"CALL rollback_to_instant('trips_cow_hudi_sql', '{last_commit_time}')\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1082373a-3e04-4305-ad4f-35dcd099c239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The update to fare amount should now be gone.\n",
    "display(spark.sql(\"SELECT uuid, fare FROM trips_cow_hudi_sql WHERE uuid = 'uuid-001'\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea925e3d-c44b-4c40-b2ec-8bce98fcadaa",
   "metadata": {},
   "source": [
    "#### `show_rollbacks`\n",
    "\n",
    "This procedure shows a list of all rollback actions performed on the table. It provides a history of your recovery operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def6411c-b366-4f26-bf08-3b78dbe2e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"CALL show_rollbacks('trips_cow_hudi_sql')\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ae62f9-9b4d-4bb0-8ff5-e2ed205c6976",
   "metadata": {},
   "source": [
    "#### `show_rollback_detail`\n",
    "\n",
    "For a deeper dive, this procedure gives you a detailed breakdown of a specific rollback, including which files and records were affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b70f6c8-303e-4d14-bcc6-7a0a29bb5899",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_rollback_instant = [row[0] for row in spark.sql(\"CALL show_rollbacks('trips_cow_hudi_sql')\").collect()][0]\n",
    "\n",
    "display(spark.sql(f\"CALL show_rollback_detail(table => 'trips_cow_hudi_sql', instant_time => '{show_rollback_instant}')\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade00380-8e61-42f9-a46a-a80c621f3d2e",
   "metadata": {},
   "source": [
    "### 2. Metadata Table Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64197b19-6b24-4df6-a9b7-33db1bfeec46",
   "metadata": {},
   "source": [
    "The following procedures are used to manage and inspect Hudi's internal Metadata Table. This table acts as a highly efficient index for all the files and partitions in your Hudi table, significantly speeding up operations like file listing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78b67d4-5807-4eff-b858-9a9615271695",
   "metadata": {},
   "source": [
    "Let's create a new table for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed25426-a3e2-4aa4-a16a-ec172add9661",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS trips_metadata\")\n",
    "\n",
    "spark.sql(f\"\"\"CREATE TABLE trips_metadata (\n",
    "    ts STRING,\n",
    "    uuid STRING,\n",
    "    rider STRING,\n",
    "    driver STRING,\n",
    "    fare DOUBLE,\n",
    "    city STRING\n",
    ") USING hudi\n",
    "TBLPROPERTIES (\n",
    "    'hoodie.table.type' = 'COPY_ON_WRITE',\n",
    "    'hoodie.datasource.write.recordkey.field' = 'uuid',\n",
    "    'hoodie.datasource.write.precombine.field' = 'ts',\n",
    "    'hoodie.datasource.write.partitionpath.field' = 'city'\n",
    ")\n",
    "LOCATION 's3a://warehouse/hudi-sql-procedure/trips_metadata'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0b7d51-8a5e-4882-80bc-48f7a5a69bd3",
   "metadata": {},
   "source": [
    "#### `create_metadata_table`\n",
    "This procedure creates the Hudi Metadata Table for an existing Hudi table. It is crucial for enabling performance optimizations and is a one-time operation.\n",
    "\n",
    "First, let's create the Metadata Table for our main Hudi table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7904e2-96a9-48f5-a8ec-b67c969be135",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"CALL create_metadata_table(table => 'trips_metadata')\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbf6b0c-5f50-4c7a-b5bb-e3e03d775c2a",
   "metadata": {},
   "source": [
    "Next, we'll insert some sample data into our new table using a standard SQL INSERT INTO statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae497e4e-4390-477d-9ded-79037a9e8084",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "  INSERT INTO trips_metadata VALUES\n",
    "    ('2025-08-10 08:15:30', 'uuid-001', 'rider-A', 'driver-X', 18.50, 'new_york'),\n",
    "    ('2025-08-10 09:22:10', 'uuid-002', 'rider-B', 'driver-Y', 22.75, 'san_francisco'),\n",
    "    ('2025-08-10 10:05:45', 'uuid-003', 'rider-C', 'driver-Z', 14.60, 'chicago'),\n",
    "    ('2025-08-10 11:40:00', 'uuid-004', 'rider-D', 'driver-W', 31.90, 'new_york'),\n",
    "    ('2025-08-10 12:55:15', 'uuid-005', 'rider-E', 'driver-V', 25.10, 'san_francisco');\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e90f68-0e56-4ed2-b6d1-7227c80cf3ad",
   "metadata": {},
   "source": [
    "#### `init_metadata_table`\n",
    "This procedure is used to initialize or repair the metadata for a Hudi table. It is particularly useful if the metadata has become corrupted or needs to be rebuilt.\n",
    "\n",
    "Now, we initialize the metadata table to populate it with file information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dced452b-374c-48de-8290-963c441e4be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"CALL init_metadata_table(table => 'trips_metadata')\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4128b1c-9197-4c98-a43d-41d0ffd17f19",
   "metadata": {},
   "source": [
    "#### `show_metadata_table_partitions`\n",
    "This procedure shows the partitions that are actively tracked by the Metadata Table. You can inspect this to confirm that Hudi is correctly managing your table's partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3303edf1-7ad0-4c2b-a863-1dd1c15badff",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"CALL show_metadata_table_partitions(table => 'trips_metadata')\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10745838-112b-40d4-a86b-1a6dea9ab026",
   "metadata": {},
   "source": [
    "#### `show_metadata_table_files`\n",
    "This procedure provides a detailed list of all the files and their sizes within a specific partition, as tracked by the Metadata Table.\n",
    "\n",
    "Now this should show the files in the metadata table for the `chicago` partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4719aa-7377-4409-8c34-fee7b9b83150",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"CALL show_metadata_table_files(table => 'trips_metadata', partition => 'city=chicago')\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b7fee2-566c-4fa6-831f-8d82f6b3e67e",
   "metadata": {},
   "source": [
    "#### `delete_metadata_table`\n",
    "This procedure deletes the Metadata Table. You would typically use this if you want to rebuild it from scratch or if you no longer need the metadata table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be8cbe1-7c71-414b-aac6-992efdb8f026",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"CALL delete_metadata_table(table => 'trips_metadata')\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3eaa68-a47f-4551-b0b2-945ea4f86d4e",
   "metadata": {},
   "source": [
    "### 3. Table Information\n",
    "The following procedures allow you to inspect the properties and file structure of your Hudi table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bae7bb2-142b-4f85-9f9b-1ba80e3b7ff4",
   "metadata": {},
   "source": [
    "#### `show_table_properties`\n",
    "This procedure is a simple but powerful way to view all the configurations and metadata associated with your Hudi table. It's especially useful for verifying your settings and understanding how Hudi is configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d47745d-d5fa-4c4d-8d09-fd970bcc30c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the procedure to show all properties of our 'trips_metadata' table.\n",
    "display(spark.sql(\"CALL show_table_properties('trips_metadata')\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b27a5e-7deb-4652-926d-f42b3ce230bb",
   "metadata": {},
   "source": [
    "#### `show_fsview_all`\n",
    "This procedure provides a complete view of all file groups and file slices within your table, including information about both base and log files. This gives you a detailed look at the physical layout of your data on the file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1acbdc-3790-4328-b544-a59e8a2703ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the procedure to show the full file system view for the 'trips_metadata' table.\n",
    "display(spark.sql(\"CALL show_fsview_all('trips_metadata')\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c1bae8-ffb2-4cb3-bbb2-9146392fff91",
   "metadata": {},
   "source": [
    "### 4. Table Services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d441bd3-17ec-43ca-83af-36a0e4186278",
   "metadata": {},
   "source": [
    "In this exercise we will explore `COMPACTION` related SQL Procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93224cd0-8f43-4da8-8e1c-b26b096e9b2d",
   "metadata": {},
   "source": [
    "Compaction is a crucial process for MOR tables. It merges the small `.log` files (which contain your updates and inserts) into larger, more efficient `.parquet` base files. This is important for maintaining optimal query performance and storage efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbd2b55-ea1a-4c25-8a55-017c75b1c5a3",
   "metadata": {},
   "source": [
    "First, let's create a MOR table and insert some data. Notice that we are setting the table type to `MERGE_ON_READ`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf2d0e1-fa16-4e2c-96ba-8bea651c8e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS trips_mor_compaction\")\n",
    "\n",
    "spark.sql(f\"\"\"CREATE TABLE trips_mor_compaction (\n",
    "    ts STRING,\n",
    "    uuid STRING,\n",
    "    rider STRING,\n",
    "    driver STRING,\n",
    "    fare DOUBLE,\n",
    "    city STRING\n",
    ") USING hudi\n",
    "TBLPROPERTIES (\n",
    "    'hoodie.table.type' = 'MERGE_ON_READ',\n",
    "    'hoodie.datasource.write.recordkey.field' = 'uuid',\n",
    "    'hoodie.datasource.write.precombine.field' = 'ts',\n",
    "    'hoodie.compact.inline.max.delta.commits' = '4'\n",
    ")\n",
    "LOCATION 's3a://warehouse/hudi-sql-procedure/trips_mor_compaction'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d30275-7b4a-4ffb-b8b5-2934376f5035",
   "metadata": {},
   "source": [
    "Let's insert some sample data into the newly created table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46af3526-34f3-44ad-a70d-7c2aec7124bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "  INSERT INTO trips_mor_compaction VALUES\n",
    "    ('2025-08-10 08:15:30', 'uuid-001', 'rider-A', 'driver-X', 18.50, 'new_york'),\n",
    "    ('2025-08-10 09:22:10', 'uuid-002', 'rider-B', 'driver-Y', 22.75, 'san_francisco');\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d559d2a3-a06e-4fb1-a0c3-7fa080bc2f38",
   "metadata": {},
   "source": [
    "Let's check the files in the table path. We will see a .parquet file containing the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84b3b39-191e-46b7-96e3-647169445c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls(f\"s3a://warehouse/hudi-sql-procedure/trips_mor_compaction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bac9e1-5c98-4980-a461-9020bb498848",
   "metadata": {},
   "source": [
    "Now, let's perform two updates. This will generate two separate `.log` files in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a69f75-35bf-4590-90bf-afe42de5998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First update\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO trips_mor_compaction VALUES \n",
    "('2025-08-10 08:20:00', 'uuid-001', 'rider-A', 'driver-A', 19.50, 'new_york');\n",
    "\"\"\")\n",
    "\n",
    "# Second update\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO trips_mor_compaction VALUES \n",
    "('2025-08-10 08:25:00', 'uuid-001', 'rider-A', 'driver-B', 20.50, 'new_york');\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e41744-d842-4ecd-83f7-a05b9b733801",
   "metadata": {},
   "source": [
    "Let's inspect the files in the table. You should now see one `.parquet` base file and two `.log` files, each corresponding to an update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caecf390-4964-4a1d-9364-5f41a425e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls(f\"s3a://warehouse/hudi-sql-procedure/trips_mor_compaction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391bf6f6-c5fa-43d2-a549-f611f1d34225",
   "metadata": {},
   "source": [
    "By setting the configuration `hoodie.compact.inline.max.delta.commits` to `4`, we are instructing Hudi to automatically trigger a compaction after every 4 delta commits. This means that once the threshold is reached, Hudi will merge the accumulated .log files into a new base .parquet file, optimizing the data layout for faster reads.\n",
    "\n",
    "Let's check the number of commits we have done till now on our MOR table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e0c63b-7b83-4ebd-8bb8-c5ec4b2e0d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(\"CALL show_commits(table => 'trips_mor_compaction')\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2983570f-6c9b-4d84-a206-fddafd168514",
   "metadata": {},
   "source": [
    "From the above output, it is seen that we have done 3 commits till now and the next commit will trigger the compaction. Let's perform another update. This will generate a separate `.log` file in the table and trigger the `compaction` too. The compaction will generate a new `.parquet` file as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b5e1cd-db99-4d58-a1df-27ce545704c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third update\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO trips_mor_compaction VALUES \n",
    "('2025-08-10 09:22:10', 'uuid-002', 'rider-B', 'driver-Y', 25.50, 'san_francisco');\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c6c8e-3545-4196-a137-ead7864a42cf",
   "metadata": {},
   "source": [
    "Now, let's check the number of commits again. And you can see that there are 4 deltacommits and one commit which happened due to `compaction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b93aab-5bc8-45ea-a542-76334b32cd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(\"CALL show_commits(table => 'trips_mor_compaction')\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ae8154-b660-4234-ae6d-333e8088a820",
   "metadata": {},
   "source": [
    "#### `show_compaction`\n",
    "This procedure shows you the status and plan of any pending or completed compaction jobs. The output will show that a compaction job has been scheduled for our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434680bb-ea1b-42dd-920a-9b2e4f8e02e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Compaction History\n",
    "display(spark.sql(f\"CALL show_compaction(table => 'trips_mor_compaction')\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e129832-1e92-4381-bc80-d257491bb897",
   "metadata": {},
   "source": [
    "After the compaction job completes, let's inspect the files in the table again. You will see that a new `.parquet` file has been created, and the old `.log` files have been merged into it. The previous `.parquet` file may still be present but will be marked for eventual cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9a3664-6cd4-4d2b-b154-0339d10691ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls(f\"s3a://warehouse/hudi-sql-procedure/trips_mor_compaction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee07767b-c15c-479b-b422-b1e46afba89c",
   "metadata": {},
   "source": [
    "Finally, let's query the table to confirm that the changes are present in the new base file. The output shows the latest version of the record for `uuid-001` and `uuid-002`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab078b-318a-4191-853a-b61b3960cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"REFRESH TABLE trips_mor_compaction\")\n",
    "\n",
    "display(spark.sql(f\"SELECT uuid, driver, fare, ts FROM trips_mor_compaction\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98636a7-a4bb-4257-b9ac-3a697ce2f2a7",
   "metadata": {},
   "source": [
    "#### `run_clean`\n",
    "The run_clean procedure is Hudi's garbage collection service. It identifies and deletes old, obsolete versions of data files that are no longer needed for time travel or rollback, based on your configured retention policy (e.g., keeping only the last 10 commits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7107c5d9-b9b1-41ba-8ef4-d8350c8bfb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command schedules and runs the cleaning service immediately. It deletes old file versions,\n",
    "# keeping only the number specified in 'file_versions_retained' policy.\n",
    "spark.sql(f\"\"\"\n",
    "CALL run_clean(\n",
    "  table => 'trips_mor_compaction',\n",
    "  trigger_max_commits => 2,\n",
    "  clean_policy => 'KEEP_LATEST_FILE_VERSIONS',\n",
    "  file_versions_retained => 1\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8546122-f3f9-4e45-9237-03386946d084",
   "metadata": {},
   "source": [
    "Let's check the filesystem, we should see only the newly created base parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beffda75-a4ec-4ee1-83b6-e13b9fe1e9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls(f\"s3a://warehouse/hudi-sql-procedure/trips_mor_compaction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2768df-1da9-41ea-b536-86418f867a09",
   "metadata": {},
   "source": [
    "If you are interested in diving deeper into Hudi SQL Procedures, be sure to check out the [official Hudi documentation](https://hudi.apache.org/docs/procedures). It provides detailed guidance on how these procedures can simplify common data lakehouse operations, making your workflows easier to manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff8ca2-92f3-4796-91aa-0eec6dd3a998",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
