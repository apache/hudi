<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed to the Apache Software Foundation (ASF) under one or more
  contributor license agreements.  See the NOTICE file distributed with
  this work for additional information regarding copyright ownership.
  The ASF licenses this file to You under the Apache License, Version 2.0
  (the "License"); you may not use this file except in compliance with
  the License.  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <parent>
    <artifactId>hudi</artifactId>
    <groupId>org.apache.hudi</groupId>
    <version>0.11.0-SNAPSHOT</version>
    <relativePath>../../pom.xml</relativePath>
  </parent>
  <modelVersion>4.0.0</modelVersion>
  <artifactId>hudi-spark${sparkbundle.version}-bundle_${scala.binary.version}</artifactId>
  <packaging>jar</packaging>

  <properties>
    <checkstyle.skip>true</checkstyle.skip>
    <main.basedir>${project.parent.basedir}</main.basedir>
  </properties>

  <build>
    <plugins>
      <plugin>
        <groupId>org.apache.rat</groupId>
        <artifactId>apache-rat-plugin</artifactId>
      </plugin>
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-shade-plugin</artifactId>
        <version>${maven-shade-plugin.version}</version>
        <executions>
          <execution>
            <phase>package</phase>
            <goals>
              <goal>shade</goal>
            </goals>
            <configuration>
              <createSourcesJar>${shadeSources}</createSourcesJar>
              <dependencyReducedPomLocation>${project.build.directory}/dependency-reduced-pom.xml
              </dependencyReducedPomLocation>
              <transformers>
                <transformer implementation="org.apache.maven.plugins.shade.resource.ApacheLicenseResourceTransformer" />
                <transformer implementation="org.apache.maven.plugins.shade.resource.ApacheNoticeResourceTransformer">
                  <addHeader>true</addHeader>
                </transformer>
                <transformer implementation="org.apache.maven.plugins.shade.resource.IncludeResourceTransformer">
                  <resource>META-INF/LICENSE</resource>
                  <file>target/classes/META-INF/LICENSE</file>
                </transformer>
              </transformers>
              <artifactSet>
                <includes>
                  <include>org.apache.hudi:hudi-common</include>
                  <include>org.apache.hudi:hudi-client-common</include>
                  <include>org.apache.hudi:hudi-spark-client</include>
                  <include>org.apache.hudi:hudi-spark-common_${scala.binary.version}</include>
                  <include>org.apache.hudi:hudi-spark_${scala.binary.version}</include>
                  <include>org.apache.hudi:${hudi.spark.module}_${scala.binary.version}</include>
                  <include>org.apache.hudi:hudi-hive-sync</include>
                  <include>org.apache.hudi:hudi-sync-common</include>
                  <include>org.apache.hudi:hudi-hadoop-mr</include>
                  <include>org.apache.hudi:hudi-timeline-service</include>
                  <include>org.apache.hudi:hudi-aws</include>

                  <include>com.beust:jcommander</include>
                  <include>io.javalin:javalin</include>
                  <!-- Spark only has mortbay jetty -->
                  <include>org.eclipse.jetty:*</include>
                  <include>org.eclipse.jetty.websocket:*</include>
                  <include>org.jetbrains.kotlin:*</include>
                  <include>org.rocksdb:rocksdbjni</include>
                  <include>org.apache.httpcomponents:httpclient</include>
                  <include>org.apache.httpcomponents:httpcore</include>
                  <include>org.apache.httpcomponents:fluent-hc</include>
                  <include>org.antlr:stringtemplate</include>
                  <include>org.apache.parquet:parquet-avro</include>

                  <include>com.github.davidmoten:guava-mini</include>
                  <include>com.github.davidmoten:hilbert-curve</include>
                  <include>com.twitter:bijection-avro_${scala.binary.version}</include>
                  <include>com.twitter:bijection-core_${scala.binary.version}</include>
                  <include>io.dropwizard.metrics:metrics-core</include>
                  <include>io.dropwizard.metrics:metrics-graphite</include>
                  <include>io.dropwizard.metrics:metrics-jmx</include>
                  <include>io.prometheus:simpleclient</include>
                  <include>io.prometheus:simpleclient_httpserver</include>
                  <include>io.prometheus:simpleclient_dropwizard</include>
                  <include>io.prometheus:simpleclient_pushgateway</include>
                  <include>io.prometheus:simpleclient_common</include>
                  <include>com.yammer.metrics:metrics-core</include>
                  <include>com.google.guava:guava</include>

                  <include>com.amazonaws:dynamodb-lock-client</include>
                  <include>com.amazonaws:aws-java-sdk-dynamodb</include>
                  <include>com.amazonaws:aws-java-sdk-core</include>

                  <include>org.apache.spark:spark-avro_${scala.binary.version}</include>
                  <include>org.apache.hive:hive-common</include>
                  <include>org.apache.hive:hive-service</include>
                  <include>org.apache.hive:hive-service-rpc</include>
                  <include>org.apache.hive:hive-metastore</include>
                  <include>org.apache.hive:hive-jdbc</include>

                  <include>org.apache.hbase:hbase-client</include>
                  <include>org.apache.hbase:hbase-common</include>
                  <include>org.apache.hbase:hbase-protocol</include>
                  <include>org.apache.hbase:hbase-server</include>
                  <include>org.apache.htrace:htrace-core</include>
                  <include>org.apache.curator:curator-framework</include>
                  <include>org.apache.curator:curator-client</include>
                  <include>org.apache.curator:curator-recipes</include>
                  <include>commons-codec:commons-codec</include>
                </includes>
              </artifactSet>
              <relocations>
                <relocation>
                  <pattern>com.yammer.metrics.</pattern>
                  <shadedPattern>org.apache.hudi.com.yammer.metrics.</shadedPattern>
                </relocation>
                <relocation>
                  <pattern>com.beust.jcommander.</pattern>
                  <shadedPattern>org.apache.hudi.com.beust.jcommander.</shadedPattern>
                </relocation>
                <relocation>
                  <pattern>org.apache.spark.sql.avro.</pattern>
                  <shadedPattern>${spark.bundle.spark.shade.prefix}org.apache.spark.sql.avro.</shadedPattern>
                </relocation>
                <relocation>
                  <pattern>org.apache.hive.jdbc.</pattern>
                  <shadedPattern>${spark.bundle.hive.shade.prefix}org.apache.hive.jdbc.</shadedPattern>
                </relocation>
                <relocation>
                  <pattern>org.apache.hadoop.hive.metastore.</pattern>
                  <shadedPattern>${spark.bundle.hive.shade.prefix}org.apache.hadoop.hive.metastore.</shadedPattern>
                </relocation>
                <relocation>
                  <pattern>org.apache.hive.common.</pattern>
                  <shadedPattern>${spark.bundle.hive.shade.prefix}org.apache.hive.common.</shadedPattern>
                </relocation>
                <relocation>
                  <pattern>org.apache.hadoop.hive.common.</pattern>
                  <shadedPattern>${spark.bundle.hive.shade.prefix}org.apache.hadoop.hive.common.</shadedPattern>
                </relocation>
                <relocation>
                  <pattern>org.apache.hadoop.hive.conf.</pattern>
                  <shadedPattern>${spark.bundle.hive.shade.prefix}org.apache.hadoop.hive.conf.</shadedPattern>
                </relocation>
                <relocation>
                  <pattern>org.apache.hive.service.</pattern>
                  <shadedPattern>${spark.bundle.hive.shade.prefix}org.apache.hive.service.</shadedPattern>
                </relocation>
                <relocation>
                  <pattern>org.apache.hadoop.hive.service.</pattern>
                  <shadedPattern>${spark.bundle.hive.shade.prefix}org.apache.hadoop.hive.service.</shadedPattern>
                </relocation>
                <relocation>
                  <pattern>com.codahale.metrics.</pattern>
                  <shadedPattern>org.apache.hudi.com.codahale.metrics.</shadedPattern>
                </relocation>
                <relocation>
                  <pattern>org.apache.commons.codec.</pattern>
                  <shadedPattern>org.apache.hudi.org.apache.commons.codec.</shadedPattern>
                </relocation>
                <relocation>
                  <pattern>org.eclipse.jetty.</pattern>
                  <shadedPattern>org.apache.hudi.org.apache.jetty.</shadedPattern>
                </relocation>
                <relocation>
                  <pattern>com.google.common.</pattern>
                  <shadedPattern>${spark.bundle.spark.shade.prefix}com.google.common.</shadedPattern>
                </relocation>
                <!-- TODO: Revisit GH ISSUE #533 & PR#633-->
              </relocations>
              <filters>
                <filter>
                  <artifact>*:*</artifact>
                  <excludes>
                    <exclude>META-INF/*.SF</exclude>
                    <exclude>META-INF/*.DSA</exclude>
                    <exclude>META-INF/*.RSA</exclude>
                    <exclude>META-INF/services/javax.*</exclude>
                  </excludes>
                </filter>
              </filters>
              <finalName>${project.artifactId}-${project.version}</finalName>
            </configuration>
          </execution>
        </executions>
      </plugin>
    </plugins>
    <resources>
      <resource>
        <directory>src/main/resources</directory>
      </resource>
      <resource>
        <directory>src/test/resources</directory>
      </resource>
    </resources>
  </build>

  <dependencies>
    <!-- Hoodie -->
    <dependency>
      <groupId>org.apache.hudi</groupId>
      <artifactId>hudi-common</artifactId>
      <version>${project.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.hudi</groupId>
      <artifactId>hudi-client-common</artifactId>
      <version>${project.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.hudi</groupId>
      <artifactId>hudi-spark-client</artifactId>
      <version>${project.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.hudi</groupId>
      <artifactId>hudi-hadoop-mr</artifactId>
      <version>${project.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.hudi</groupId>
      <artifactId>hudi-hive-sync</artifactId>
      <version>${project.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.hudi</groupId>
      <artifactId>hudi-spark-common_${scala.binary.version}</artifactId>
      <version>${project.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.hudi</groupId>
      <artifactId>hudi-spark_${scala.binary.version}</artifactId>
      <version>${project.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.hudi</groupId>
      <artifactId>${hudi.spark.module}_${scala.binary.version}</artifactId>
      <version>${project.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.hudi</groupId>
      <artifactId>hudi-timeline-service</artifactId>
      <version>${project.version}</version>
    </dependency>

    <!-- Spark (Packages) -->
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-avro_${scala.binary.version}</artifactId>
      <scope>${spark.bundle.avro.scope}</scope>
    </dependency>

    <!-- Parquet -->
    <dependency>
      <groupId>org.apache.parquet</groupId>
      <artifactId>parquet-avro</artifactId>
      <scope>compile</scope>
    </dependency>

    <!-- Hive -->
    <dependency>
      <groupId>${hive.groupid}</groupId>
      <artifactId>hive-service</artifactId>
      <version>${hive.version}</version>
      <scope>${spark.bundle.hive.scope}</scope>
    </dependency>

    <dependency>
      <groupId>${hive.groupid}</groupId>
      <artifactId>hive-service-rpc</artifactId>
      <version>${hive.version}</version>
      <scope>${spark.bundle.hive.scope}</scope>
    </dependency>

    <dependency>
      <groupId>${hive.groupid}</groupId>
      <artifactId>hive-jdbc</artifactId>
      <version>${hive.version}</version>
      <scope>${spark.bundle.hive.scope}</scope>
    </dependency>

    <dependency>
      <groupId>${hive.groupid}</groupId>
      <artifactId>hive-metastore</artifactId>
      <version>${hive.version}</version>
      <scope>${spark.bundle.hive.scope}</scope>
    </dependency>

    <dependency>
      <groupId>${hive.groupid}</groupId>
      <artifactId>hive-common</artifactId>
      <version>${hive.version}</version>
      <scope>${spark.bundle.hive.scope}</scope>
    </dependency>

    <dependency>
      <groupId>org.apache.htrace</groupId>
      <artifactId>htrace-core</artifactId>
      <version>${htrace.version}</version>
      <scope>compile</scope>
    </dependency>

    <!-- Hbase -->
    <dependency>
      <groupId>org.apache.hbase</groupId>
      <artifactId>hbase-common</artifactId>
      <version>${hbase.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.hbase</groupId>
      <artifactId>hbase-server</artifactId>
      <version>${hbase.version}</version>
      <scope>compile</scope>
      <exclusions>
        <exclusion>
          <groupId>org.apache.hbase</groupId>
          <artifactId>hbase-common</artifactId>
        </exclusion>
        <exclusion>
          <groupId>javax.servlet</groupId>
          <artifactId>*</artifactId>
        </exclusion>
        <exclusion>
          <groupId>org.codehaus.jackson</groupId>
          <artifactId>*</artifactId>
        </exclusion>
        <exclusion>
          <groupId>org.mortbay.jetty</groupId>
          <artifactId>*</artifactId>
        </exclusion>
        <exclusion>
          <groupId>tomcat</groupId>
          <artifactId>*</artifactId>
        </exclusion>
      </exclusions>
    </dependency>
    <dependency>
      <groupId>org.apache.hbase</groupId>
      <artifactId>hbase-client</artifactId>
      <version>${hbase.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.hbase</groupId>
      <artifactId>hbase-protocol</artifactId>
      <version>${hbase.version}</version>
    </dependency>

    <!-- zookeeper -->
    <dependency>
      <groupId>org.apache.curator</groupId>
      <artifactId>curator-framework</artifactId>
      <version>${zk-curator.version}</version>
    </dependency>

    <dependency>
      <groupId>org.apache.curator</groupId>
      <artifactId>curator-client</artifactId>
      <version>${zk-curator.version}</version>
    </dependency>

    <dependency>
      <groupId>org.apache.curator</groupId>
      <artifactId>curator-recipes</artifactId>
      <version>${zk-curator.version}</version>
    </dependency>
    <!-- TODO: Reinvestigate PR 633 -->
  </dependencies>

  <profiles>
    <profile>
      <id>spark-bundle-shade-hive</id>
      <properties>
        <spark.bundle.hive.scope>compile</spark.bundle.hive.scope>
        <spark.bundle.hive.shade.prefix>org.apache.hudi.</spark.bundle.hive.shade.prefix>
      </properties>
    </profile>
    <profile>
      <id>spark-shade-unbundle-avro</id>
      <properties>
        <spark.bundle.avro.scope>provided</spark.bundle.avro.scope>
        <spark.bundle.spark.shade.prefix />
      </properties>
    </profile>
  </profiles>
</project>
