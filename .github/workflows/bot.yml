name: Java CI

on:
  push:
    branches:
      - master
  pull_request:
    branches:
      - master

env:
  MVN_ARGS: -ntp -B -V -Pci-log -Pwarn-log -Dorg.slf4j.simpleLogger.log.org.apache.maven.plugins.shade=warn -Dorg.slf4j.simpleLogger.log.org.apache.maven.plugins.dependency=warn
  UTILITIES_MODULE: "hudi-utilities,hudi-examples/hudi-examples-spark"
  UTILITIES_BUNDLE_MODULE: "packaging/hudi-utilities-bundle"
  SPARK_COMMON_MODULE: "hudi-spark-datasource/hudi-spark-common"
  SPARK3_COMMON_MODULE: "hudi-spark-datasource/hudi-spark3-common"
  SPARK3PLUS_COMMON_MODULE: "hudi-spark-datasource/hudi-spark3.2plus-common"
  SPARK_MODULE: "hudi-spark-datasource/hudi-spark"
  SPARK3_MODULE: "hudi-spark-datasource/hudi-spark3.5.x"
  HUDI_COMMON_MODULE: "hudi-common"
  CLIENT_COMMON_MODULE: "hudi-client/hudi-client-common"
  SPARK_CLIENT_MODULE: "hudi-client/hudi-spark-client"
  JAVA_CLIENT_MODULE: "hudi-client/hudi-java-client"
  DATAHUB_SYNC_MODULE: "hudi-sync/hudi-datahub-sync"
  AWS: "aws"

permissions:
  id-token: write
  contents: read

concurrency:
  group: ${{ github.ref }}
  cancel-in-progress: ${{ github.ref != 'master' }}

jobs:
  skip-check:
    runs-on: ubuntu-latest
    outputs:
      skip: ${{ steps.determine-skip.outputs.skip }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 2 # Fetch commits for diffing

      - name: Determine Changed Files
        id: determine-skip
        run: |
          # If there is no base ref, skip the check
          if [ -z "${{ github.base_ref }}" ]; then
            # Check if the commit starts with "[AUDIT"
            if echo "${{ github.event.head_commit.message }}" | grep -Eq '^\[AUDIT'; then
              echo "skip=true" >> $GITHUB_OUTPUT
            else
              echo "skip=false" >> $GITHUB_OUTPUT
            fi
            exit 0
          fi
          git fetch origin ${{ github.base_ref }}
          CHANGED_FILES=$(git diff --name-only origin/${{ github.base_ref }} ${{ github.sha }})
          echo "CHANGED_FILES=$CHANGED_FILES"

          # Check if all changed files are limited to CHANGELOG or README.md
          if echo "$CHANGED_FILES" | grep -vE '^(CHANGELOG|README\.md)$' >/dev/null; then
            # There are files other than CHANGELOG or README.md
            echo "skip=false" >> $GITHUB_OUTPUT
          else
            # All changes are limited to CHANGELOG and README.md
            echo "skip=true" >> $GITHUB_OUTPUT
          fi

  utilities:
    needs: skip-check
    runs-on: ubuntu-latest
    timeout-minutes: 120
    strategy:
      matrix:
        include:
            - scalaProfile: "scala-2.12"
              sparkProfile: "spark3.5"
    steps:
      - uses: actions/checkout@v2
        with:
          fetch-depth: 0 # Needs master to view latest
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-region: us-west-2
          role-to-assume: arn:aws:iam::194159489498:role/GithubActionsPublishHudi-RepositoryPublisherRole-10H7ABJVFNSQ9
          role-session-name: HudiPublishSession
          role-duration-seconds: 900
      - name: Set Env
        run: echo "CODEARTIFACT_AUTH_TOKEN=$(aws codeartifact get-authorization-token --domain onehouse --domain-owner 194159489498 --query authorizationToken --output text)" >> $GITHUB_ENV
      - name: Set up JDK 8
        uses: actions/setup-java@v3
        with:
          java-version: |
            17
            8
          distribution: 'adopt'
          architecture: x64
          server-id: codeartifact
          server-username: AWS
          server-password: CODEARTIFACT_AUTH_TOKEN
      - name: Build
        env:
          SCALA_PROFILE: ${{ matrix.scalaProfile }}
          SPARK_PROFILE: ${{ matrix.sparkProfile }}
        run:
          mvn -am -pl $UTILITIES_MODULE clean install -T 2 -Pintegration-tests -D"$SCALA_PROFILE" -D"$SPARK_PROFILE" -DskipTests=true $MVN_ARGS
      - name: Quickstart Test
        if: needs.skip-check.outputs.skip != 'true' # Skip condition
        env:
          SCALA_PROFILE: ${{ matrix.scalaProfile }}
          SPARK_PROFILE: ${{ matrix.sparkProfile }}
        run: |
          mvn test -D"$SCALA_PROFILE" -D"$SPARK_PROFILE" -DfailIfNoTests=false -pl hudi-examples/hudi-examples-spark $MVN_ARGS
      - name: Hudi Utilties Unit Test
        if: needs.skip-check.outputs.skip != 'true' # Skip condition
        env:
          SCALA_PROFILE: ${{ matrix.scalaProfile }}
          SPARK_PROFILE: ${{ matrix.sparkProfile }}
        run: |
          mvn test -Punit-tests -DskipSparkClientUTs=true -D"$SCALA_PROFILE" -D"$SPARK_PROFILE" -pl $UTILITIES_MODULE $MVN_ARGS
      - name: Hudi Utilties Functional Test
        if: needs.skip-check.outputs.skip != 'true' # Skip condition
        env:
          SCALA_PROFILE: ${{ matrix.scalaProfile }}
          SPARK_PROFILE: ${{ matrix.sparkProfile }}
        run: |
          mvn test -Pfunctional-tests -DskipSparkClientUTs=true -D"$SCALA_PROFILE" -D"$SPARK_PROFILE" -pl $UTILITIES_MODULE $MVN_ARGS
      - name: Sonar Report
        env:
          SCALA_PROFILE: ${{ matrix.scalaProfile }}
          SPARK_PROFILE: ${{ matrix.sparkProfile }}
        run: |  
          export JAVA_HOME="${{env.JAVA_HOME_17_X64}}"
          WORKING_DIR="/home/runner/work/hudi-internal/hudi-internal"
          jacoco_xml_report_files=$(find ${WORKING_DIR} -name 'jacoco.xml' | awk '{printf "%s%s",sep,$0; sep=","} END{print ""}')
          echo "Jacoco xml reports: ${jacoco_xml_report_files}"
          if [ ${{ github.event_name }} == 'pull_request' ]; then
            mvn sonar:sonar -Dargline="-Xmx8192m" -D"$SCALA_PROFILE" -D"$SPARK_PROFILE" $MVN_ARGS -Dsonar.projectName="hudi-internal-utilities" -Dsonar.projectKey=onehouseinc_hudi-internal_utilities \
              -Dsonar.exclusions="hudi-aws/**/*,hudi-cli/**/*,hudi-client/**/*,hudi-common/**/*,hudi-examples/**/*,hudi-flink-datasource/**/*,hudi-gcp/**/*,hudi-hadoop-mr/**/*,hudi-kafka-connect/**/*,hudi-spark-datasource/**/*,hudi-sync/**/*,hudi-tests-common/**/*,hudi-timeline-service/**/*,packaging/**/*" \
              -Dsonar.token=${{ secrets.SONAR_CLOUD_TOKEN }} \
              -Dsonar.coverage.jacoco.xmlReportPaths="${jacoco_xml_report_files}" \
              -Dsonar.pullrequest.key=${{ github.event.number }} \
              -Dsonar.pullrequest.branch=${{ github.HEAD_REF }} \
              -Dsonar.pullrequest.base=${{ github.BASE_REF }} \
              -Dsonar.pullrequest.github.repository=https://github.com/onehouseinc/hudi-internal
          else
            mvn sonar:sonar -Dargline="-Xmx8192m" -D"$SCALA_PROFILE" -D"$SPARK_PROFILE" $MVN_ARGS -Dsonar.projectName="hudi-internal-utilities" -Dsonar.projectKey=onehouseinc_hudi-internal_utilities \
              -Dsonar.exclusions="hudi-aws/**/*,hudi-cli/**/*,hudi-client/**/*,hudi-common/**/*,hudi-examples/**/*,hudi-flink-datasource/**/*,hudi-gcp/**/*,hudi-hadoop-mr/**/*,hudi-kafka-connect/**/*,hudi-spark-datasource/**/*,hudi-sync/**/*,hudi-tests-common/**/*,hudi-timeline-service/**/*,packaging/**/*" \
              -Dsonar.token=${{ secrets.SONAR_CLOUD_TOKEN }} \
              -Dsonar.coverage.jacoco.xmlReportPaths="${jacoco_xml_report_files}" \
              -Dsonar.branch.name=${{ github.HEAD_REF }}
          fi
  utilities_bundle_misc:
    needs: skip-check
    runs-on: ubuntu-latest
    strategy:
      matrix:
        include:
          - scalaProfile: "scala-2.12"
            sparkProfile: "spark3.5"
    steps:
      - uses: actions/checkout@v2
        with:
          fetch-depth: 0 # Needs master to view latest
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-region: us-west-2
          role-to-assume: arn:aws:iam::194159489498:role/GithubActionsPublishHudi-RepositoryPublisherRole-10H7ABJVFNSQ9
          role-session-name: HudiPublishSession
          role-duration-seconds: 900
      - name: Set Env
        run: echo "CODEARTIFACT_AUTH_TOKEN=$(aws codeartifact get-authorization-token --domain onehouse --domain-owner 194159489498 --query authorizationToken --output text)" >> $GITHUB_ENV
      - name: Set up JDK 8
        uses: actions/setup-java@v3
        with:
          java-version: |
            17
            8
          distribution: 'adopt'
          architecture: x64
          server-id: codeartifact
          server-username: AWS
          server-password: CODEARTIFACT_AUTH_TOKEN
      - name: Build
        env:
          SCALA_PROFILE: ${{ matrix.scalaProfile }}
          SPARK_PROFILE: ${{ matrix.sparkProfile }}
        run:
          mvn -am -pl "$DATAHUB_SYNC_MODULE,$UTILITIES_BUNDLE_MODULE" clean install -T 4 -Pintegration-tests -D"$SCALA_PROFILE" -D"$SPARK_PROFILE" -DskipTests=true $MVN_ARGS
      - name: Hudi Utilties Bundle Unit Test
        if: needs.skip-check.outputs.skip != 'true' # Skip condition
        env:
          SCALA_PROFILE: ${{ matrix.scalaProfile }}
          SPARK_PROFILE: ${{ matrix.sparkProfile }}
        run: |
          mvn test -Punit-tests -DskipSparkClientUTs=true -D"$SCALA_PROFILE" -D"$SPARK_PROFILE" -am -pl "$DATAHUB_SYNC_MODULE,$UTILITIES_BUNDLE_MODULE,!$UTILITIES_MODULE,!$SPARK_COMMON_MODULE,!$SPARK3_COMMON_MODULE,!$SPARK3PLUS_COMMON_MODULE,!$SPARK_MODULE,!$SPARK3_MODULE,!$HUDI_COMMON_MODULE,!$CLIENT_COMMON_MODULE,!$SPARK_CLIENT_MODULE,!$JAVA_CLIENT_MODULE" $MVN_ARGS
      - name: Hudi Utilties Bundle Functional Test
        if: needs.skip-check.outputs.skip != 'true' # Skip condition
        env:
          SCALA_PROFILE: ${{ matrix.scalaProfile }}
          SPARK_PROFILE: ${{ matrix.sparkProfile }}
        run: |
          mvn test -Pfunctional-tests -DskipSparkClientUTs=true -D"$SCALA_PROFILE" -D"$SPARK_PROFILE" -am -pl "$DATAHUB_SYNC_MODULE,$UTILITIES_BUNDLE_MODULE,!$UTILITIES_MODULE,!$SPARK_COMMON_MODULE,!$SPARK3_COMMON_MODULE,!$SPARK3PLUS_COMMON_MODULE,!$SPARK_MODULE,!$SPARK3_MODULE,!$HUDI_COMMON_MODULE,!$CLIENT_COMMON_MODULE,!$SPARK_CLIENT_MODULE,!$JAVA_CLIENT_MODULE" $MVN_ARGS
      - name: Sonar Report
        env:
          SCALA_PROFILE: ${{ matrix.scalaProfile }}
          SPARK_PROFILE: ${{ matrix.sparkProfile }}
        run: |
          export JAVA_HOME="${{env.JAVA_HOME_17_X64}}"
          WORKING_DIR="/home/runner/work/hudi-internal/hudi-internal"
          jacoco_xml_report_files=$(find ${WORKING_DIR} -name 'jacoco.xml' | awk '{printf "%s%s",sep,$0; sep=","} END{print ""}')
          echo "Jacoco xml reports: ${jacoco_xml_report_files}"
          if [ ${{ github.event_name }} == 'pull_request' ]; then
            mvn sonar:sonar -DargLine="-Xmx8192m" -D"$SCALA_PROFILE" -D"$SPARK_PROFILE" $MVN_ARGS -Dsonar.projectName="hudi-internal-utilities-bundle-misc" -Dsonar.projectKey=onehouseinc_hudi-internal_utilities-bundle-misc \
              -Dsonar.exclusions="hudi-cli/**/*,hudi-client/**/*,hudi-common/**/*,hudi-flink-datasource/**/*,hudi-kafka-connect/**/*,hudi-spark-datasource/**/*,hudi-tests-common/**/*,hudi-utilities/**/*" \
              -Dsonar.token=${{ secrets.SONAR_CLOUD_TOKEN }} \
              -Dsonar.coverage.jacoco.xmlReportPaths="${jacoco_xml_report_files}" \
              -Dsonar.pullrequest.key=${{ github.event.number }} \
              -Dsonar.pullrequest.branch=${{ github.HEAD_REF }} \
              -Dsonar.pullrequest.base=${{ github.BASE_REF }} \
              -Dsonar.pullrequest.github.repository=https://github.com/onehouseinc/hudi-internal
          else
            mvn sonar:sonar -DargLine="-Xmx8192m" -D"$SCALA_PROFILE" -D"$SPARK_PROFILE" $MVN_ARGS -Dsonar.projectName="hudi-internal-utilities-bundle-misc" -Dsonar.projectKey=onehouseinc_hudi-internal_utilities-bundle-misc \
              -Dsonar.exclusions="hudi-cli/**/*,hudi-client/**/*,hudi-common/**/*,hudi-flink-datasource/**/*,hudi-kafka-connect/**/*,hudi-spark-datasource/**/*,hudi-tests-common/**/*,hudi-utilities/**/*" \
              -Dsonar.token=${{ secrets.SONAR_CLOUD_TOKEN }} \
              -Dsonar.coverage.jacoco.xmlReportPaths="${jacoco_xml_report_files}" \
              -Dsonar.branch.name=${{ github.HEAD_REF }}
          fi
  hudi-client-and-common:
    needs: skip-check
    runs-on: ubuntu-latest
    strategy:
      matrix:
        include:
          - scalaProfile: "scala-2.12"
            sparkProfile: "spark3.5"
    steps:
      - uses: actions/checkout@v2
        with:
            fetch-depth: 0 # Needs master to view latest
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-region: us-west-2
          role-to-assume: arn:aws:iam::194159489498:role/GithubActionsPublishHudi-RepositoryPublisherRole-10H7ABJVFNSQ9
          role-session-name: HudiPublishSession
          role-duration-seconds: 900
      - name: Set Env
        run: echo "CODEARTIFACT_AUTH_TOKEN=$(aws codeartifact get-authorization-token --domain onehouse --domain-owner 194159489498 --query authorizationToken --output text)" >> $GITHUB_ENV
      - name: Set up JDK 8
        uses: actions/setup-java@v3
        with:
          java-version: |
            17
            8
          distribution: 'adopt'
          architecture: x64
          server-id: codeartifact
          server-username: AWS
          server-password: CODEARTIFACT_AUTH_TOKEN
      - name: Build
        env:
          SCALA_PROFILE: ${{ matrix.scalaProfile }}
          SPARK_PROFILE: ${{ matrix.sparkProfile }}
        run:
          mvn clean install -T 2 -DskipTests -D"$SCALA_PROFILE" -D"$SPARK_PROFILE" -am -pl "$CLIENT_COMMON_MODULE,$SPARK_CLIENT_MODULE,$JAVA_CLIENT_MODULE,$HUDI_COMMON_MODULE" $MVN_ARGS
      - name: Hudi Client Unit Tests
        if: needs.skip-check.outputs.skip != 'true' # Skip condition
        env:
          SCALA_PROFILE: ${{ matrix.scalaProfile }}
          SPARK_PROFILE: ${{ matrix.sparkProfile }}
        run: |
          mvn test -Punit-tests -D"$SCALA_PROFILE" -D"$SPARK_PROFILE" -pl "$CLIENT_COMMON_MODULE,$SPARK_CLIENT_MODULE,$JAVA_CLIENT_MODULE,$HUDI_COMMON_MODULE" $MVN_ARGS
      - name: Hudi Client Functional Tests
        if: needs.skip-check.outputs.skip != 'true' # Skip condition
        env:
          SCALA_PROFILE: ${{ matrix.scalaProfile }}
          SPARK_PROFILE: ${{ matrix.sparkProfile }}
        run: |
          mvn test -Pfunctional-tests -Djacoco.skip=true -D"$SCALA_PROFILE" -D"$SPARK_PROFILE" -pl "$CLIENT_COMMON_MODULE,$SPARK_CLIENT_MODULE,$JAVA_CLIENT_MODULE,$HUDI_COMMON_MODULE" $MVN_ARGS
      - name: Sonar Report
        env:
          SCALA_PROFILE: ${{ matrix.scalaProfile }}
          SPARK_PROFILE: ${{ matrix.sparkProfile }}
        run: |
          export JAVA_HOME="${{env.JAVA_HOME_17_X64}}"
          WORKING_DIR="/home/runner/work/hudi-internal/hudi-internal"
          jacoco_xml_report_files=$(find ${WORKING_DIR} -name 'jacoco.xml' | awk '{printf "%s%s",sep,$0; sep=","} END{print ""}')
          echo "Jacoco xml reports: ${jacoco_xml_report_files}"
          if [ ${{ github.event_name }} == 'pull_request' ]; then
            mvn sonar:sonar -DargLine="-Xmx8192m" -D"$SCALA_PROFILE" -D"$SPARK_PROFILE" $MVN_ARGS -Dsonar.projectName="hudi-internal-hudi-client-and-common" -Dsonar.projectKey=onehouseinc_hudi-internal_hudi-client-and-common \
              -Dsonar.exclusions="hudi-client/hudi-flink-client/**/*,hudi-aws/**/*,hudi-cli/**/*,hudi-examples/**/*,hudi-flink-datasource/**/*,hudi-gcp/**/*,hudi-hadoop-mr/**/*,hudi-kafka-connect/**/*,hudi-spark-datasource/**/*,hudi-sync/**/*,hudi-tests-common/**/*,hudi-timeline-service/**/*,hudi-utilities/**/*,packaging/**/*" \
              -Dsonar.token=${{ secrets.SONAR_CLOUD_TOKEN }} \
              -Dsonar.coverage.jacoco.xmlReportPaths="${jacoco_xml_report_files}" \
              -Dsonar.pullrequest.key=${{ github.event.number }} \
              -Dsonar.pullrequest.branch=${{ github.HEAD_REF }} \
              -Dsonar.pullrequest.base=${{ github.BASE_REF }} \
              -Dsonar.pullrequest.github.repository=https://github.com/onehouseinc/hudi-internal
          else
            mvn sonar:sonar -DargLine="-Xmx8192m" -D"$SCALA_PROFILE" -D"$SPARK_PROFILE" $MVN_ARGS -Dsonar.projectName="hudi-internal-hudi-client-and-common" -Dsonar.projectKey=onehouseinc_hudi-internal_hudi-client-and-common \
              -Dsonar.exclusions="hudi-client/hudi-flink-client/**/*,hudi-aws/**/*,hudi-cli/**/*,hudi-examples/**/*,hudi-flink-datasource/**/*,hudi-gcp/**/*,hudi-hadoop-mr/**/*,hudi-kafka-connect/**/*,hudi-spark-datasource/**/*,hudi-sync/**/*,hudi-tests-common/**/*,hudi-timeline-service/**/*,hudi-utilities/**/*,packaging/**/*" \
              -Dsonar.token=${{ secrets.SONAR_CLOUD_TOKEN }} \
              -Dsonar.coverage.jacoco.xmlReportPaths="${jacoco_xml_report_files}" \
              -Dsonar.branch.name=${{ github.HEAD_REF }}
          fi
  spark-tests:
    needs: skip-check
    runs-on: ubuntu-latest
    strategy:
      matrix:
        include:
          - scalaProfile: "scala-2.12"
            sparkProfile: "spark3.5"
    steps:
      - uses: actions/checkout@v2
        with:
          fetch-depth: 0 # Needs master to view latest
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-region: us-west-2
          role-to-assume: arn:aws:iam::194159489498:role/GithubActionsPublishHudi-RepositoryPublisherRole-10H7ABJVFNSQ9
          role-session-name: HudiPublishSession
          role-duration-seconds: 900
      - name: Set Env
        run: echo "CODEARTIFACT_AUTH_TOKEN=$(aws codeartifact get-authorization-token --domain onehouse --domain-owner 194159489498 --query authorizationToken --output text)" >> $GITHUB_ENV
      - name: Set up JDK 8
        uses: actions/setup-java@v3
        with:
          java-version: |
            17
            8
          distribution: 'adopt'
          architecture: x64
          server-id: codeartifact
          server-username: AWS
          server-password: CODEARTIFACT_AUTH_TOKEN
      - name: Build
        env:
          SCALA_PROFILE: ${{ matrix.scalaProfile }}
          SPARK_PROFILE: ${{ matrix.sparkProfile }}
        run: mvn clean install -T 2 -DskipTests -D"$SCALA_PROFILE" -D"$SPARK_PROFILE" -am -pl "$SPARK_COMMON_MODULE,$SPARK3_COMMON_MODULE,$SPARK3PLUS_COMMON_MODULE,$SPARK_MODULE,$SPARK3_MODULE " $MVN_ARGS
      - name: Spark Datasource Unit Tests
        if: needs.skip-check.outputs.skip != 'true' # Skip condition
        env:
          SCALA_PROFILE: ${{ matrix.scalaProfile }}
          SPARK_PROFILE: ${{ matrix.sparkProfile }}
        run: mvn test -Punit-tests -D"$SCALA_PROFILE" -D"$SPARK_PROFILE" -pl "$SPARK_COMMON_MODULE,$SPARK3_COMMON_MODULE,$SPARK3PLUS_COMMON_MODULE,$SPARK_MODULE,$SPARK3_MODULE " $MVN_ARGS
      - name: Spark Datasource Functional Tests
        if: needs.skip-check.outputs.skip != 'true' # Skip condition
        env:
          SCALA_PROFILE: ${{ matrix.scalaProfile }}
          SPARK_PROFILE: ${{ matrix.sparkProfile }}
        run: mvn test -Pfunctional-tests -D"$SCALA_PROFILE" -D"$SPARK_PROFILE" -pl "$SPARK_COMMON_MODULE,$SPARK3_COMMON_MODULE,$SPARK3PLUS_COMMON_MODULE,$SPARK_MODULE,$SPARK3_MODULE " $MVN_ARGS
      - name: Sonar Report
        env:
          SCALA_PROFILE: ${{ matrix.scalaProfile }}
          SPARK_PROFILE: ${{ matrix.sparkProfile }}
        run: |
          export JAVA_HOME="${{env.JAVA_HOME_17_X64}}"
          WORKING_DIR="/home/runner/work/hudi-internal/hudi-internal"
          jacoco_xml_report_files=$(find ${WORKING_DIR} -name 'jacoco.xml' | awk '{printf "%s%s",sep,$0; sep=","} END{print ""}')
          echo "Jacoco xml reports: ${jacoco_xml_report_files}"
          if [ ${{ github.event_name }} == 'pull_request' ]; then
            mvn sonar:sonar -DargLine="-Xmx8192m" -D"$SCALA_PROFILE" -D"$SPARK_PROFILE" $MVN_ARGS -Dsonar.projectName="hudi-internal-spark-tests" -Dsonar.projectKey=onehouseinc_hudi-internal_spark-tests \
              -Dsonar.exclusions="hudi-aws/**/*,hudi-cli/**/*,hudi-client/**/*,hudi-common/**/*,hudi-examples/**/*,hudi-flink-datasource/**/*,hudi-gcp/**/*,hudi-hadoop-mr/**/*,hudi-kafka-connect/**/*,hudi-sync/**/*,hudi-tests-common/**/*,hudi-timeline-service/**/*,hudi-utilities/**/*,hudi-spark-datasource/**/ReflectUtil.java,packaging/**/*" \
              -Dsonar.token=${{ secrets.SONAR_CLOUD_TOKEN }} \
              -Dsonar.coverage.jacoco.xmlReportPaths="${jacoco_xml_report_files}" \
              -Dsonar.pullrequest.key=${{ github.event.number }} \
              -Dsonar.pullrequest.branch=${{ github.HEAD_REF }} \
              -Dsonar.pullrequest.base=${{ github.BASE_REF }} \
              -Dsonar.pullrequest.github.repository=https://github.com/onehouseinc/hudi-internal
          else
            mvn sonar:sonar -DargLine="-Xmx8192m" -D"$SCALA_PROFILE" -D"$SPARK_PROFILE" $MVN_ARGS -Dsonar.projectName="hudi-internal-spark-tests" -Dsonar.projectKey=onehouseinc_hudi-internal_spark-tests \
              -Dsonar.exclusions="hudi-aws/**/*,hudi-cli/**/*,hudi-client/**/*,hudi-common/**/*,hudi-examples/**/*,hudi-flink-datasource/**/*,hudi-gcp/**/*,hudi-hadoop-mr/**/*,hudi-kafka-connect/**/*,hudi-sync/**/*,hudi-tests-common/**/*,hudi-timeline-service/**/*,hudi-utilities/**/*,hudi-spark-datasource/**/ReflectUtil.java,packaging/**/*" \
              -Dsonar.token=${{ secrets.SONAR_CLOUD_TOKEN }} \
              -Dsonar.coverage.jacoco.xmlReportPaths="${jacoco_xml_report_files}" \
              -Dsonar.branch.name=${{ github.HEAD_REF }}
          fi


