hoodie.parquet.max.file.size=20971520
hoodie.parquet.block.size=20971520
hoodie.upsert.shuffle.parallelism=40
hoodie.insert.shuffle.parallelism=40
hoodie.delete.shuffle.parallelism=40
hoodie.bulkinsert.shuffle.parallelism=40
# Key fields, for kafka example
hoodie.datasource.write.recordkey.field=key
hoodie.datasource.write.partitionpath.field=partition
# Schema provider props (change to absolute path based on your installation)
hoodie.deltastreamer.schemaprovider.source.schema.file=file:/Users/ethan/Work/scripts/benchmark_schema.avsc
hoodie.deltastreamer.schemaprovider.target.schema.file=file:/Users/ethan/Work/scripts/benchmark_schema.avsc
# DFS Source
hoodie.deltastreamer.source.dfs.root=file:/Users/ethan/Work/data/hudi/benchmark_sample_upserts_r100
benchmark.input.source.path=file:/Users/ethan/Work/data/hudi/benchmark_sample_upserts_r100
# Compaction
hoodie.compact.inline.max.delta.commits=3
# Clean and archive
hoodie.clean.async=false
hoodie.keep.max.commits=7
hoodie.keep.min.commits=5
hoodie.cleaner.commits.retained=4
# Metadata table
hoodie.metadata.compact.max.delta.commits=5
hoodie.metadata.keep.min.commits=8
hoodie.metadata.keep.max.commits=12
