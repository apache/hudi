
# Key fields, for kafka example
hoodie.datasource.write.recordkey.field=_row_key
hoodie.datasource.write.partitionpath.field=partition_path
# Schema provider props (change to absolute path based on your installation)
hoodie.deltastreamer.schemaprovider.source.schema.file=file:/tmp/input_data/avro.schema
hoodie.deltastreamer.schemaprovider.target.schema.file=file:/tmp/input_data/avro.schema
# DFS Source
hoodie.deltastreamer.source.dfs.root=file:/tmp/test_data/
benchmark.input.source.path=file:/tmp/test_data/
# Clustering
#hoodie.clustering.inline=true
#hoodie.clustering.async.enabled=false
#hoodie.clustering.async.max.commits=6
#hoodie.clustering.inline.max.commits=6
# Clean and archive
hoodie.clean.async=false
hoodie.keep.max.commits=7
hoodie.keep.min.commits=5
hoodie.cleaner.commits.retained=4
# Metadata table
hoodie.metadata.compact.max.delta.commits=5
hoodie.metadata.keep.min.commits=8
hoodie.metadata.keep.max.commits=12

hoodie.write.record.merge.mode=EVENT_TIME_ORDERING
