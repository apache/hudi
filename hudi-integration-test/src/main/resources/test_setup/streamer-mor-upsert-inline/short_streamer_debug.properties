hoodie.parquet.max.file.size=20971520
hoodie.parquet.block.size=20971520

# Key fields, for kafka example
hoodie.datasource.write.recordkey.field=_row_key
hoodie.datasource.write.partitionpath.field=partition_path
# Schema provider props (change to absolute path based on your installation)
hoodie.deltastreamer.schemaprovider.source.schema.file=file:/tmp/input_data/avro.schema
hoodie.deltastreamer.schemaprovider.target.schema.file=file:/tmp/input_data/avro.schema
hoodie.datasource.write.payload.class=org.apache.hudi.common.model.OverwriteWithLatestAvroPayload
# DFS Source
hoodie.deltastreamer.source.dfs.root=file:/tmp/test_short_data/
benchmark.input.source.path=file:/tmp/test_short_data/

# Compaction
hoodie.compact.inline=false
hoodie.compact.inline.max.delta.commits=3
hoodie.parquet.small.file.limit=0
# Clean and archive
hoodie.clean.async=false
hoodie.keep.max.commits=7
hoodie.keep.min.commits=5
hoodie.cleaner.commits.retained=4
# Clustering
hoodie.clustering.inline=false
# Metadata table
hoodie.metadata.enable=false
hoodie.metadata.compact.max.delta.commits=5
hoodie.metadata.keep.min.commits=8
hoodie.metadata.keep.max.commits=12
