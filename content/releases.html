<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Releases - Apache Hudi</title>
<meta name="description" content="Apache Hudi is the Streaming Data Lake Platform.">

<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="">
<meta property="og:title" content="Releases">
<meta property="og:url" content="https://hudi.apache.org/releases">






  <meta property="article:modified_time" content="2020-05-28T11:40:00-04:00">







<!-- end _includes/seo.html -->


<!--<link href="/feed.xml" type="application/atom+xml" rel="alternate" title=" Feed">-->

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



<link rel="icon" type="image/x-icon" href="/assets/images/favicon.ico">
<link rel="stylesheet" href="/assets/css/font-awesome.min.css">
<script src="/assets/js/jquery.min.js"></script>

    
<script src="/assets/js/main.min.js"></script>

  </head>

  <body class="layout--releases">
    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap" id="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/">
              <div style="width: 150px; height: 40px">
              </div>
          </a>
        
        <a class="site-title" href="/">
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/docs/spark_quick-start-guide.html" target="_self" >Documentation</a>
            </li><li class="masthead__menu-item">
              <a href="/community.html" target="_self" >Community</a>
            </li><li class="masthead__menu-item">
              <a href="/blog.html" target="_self" >Blog</a>
            </li><li class="masthead__menu-item">
              <a href="https://cwiki.apache.org/confluence/display/HUDI/FAQ" target="_blank" >FAQ</a>
            </li><li class="masthead__menu-item">
              <a href="/docs/powered_by.html" target="_self" >Powered By</a>
            </li><li class="masthead__menu-item">
              <a href="/releases.html" target="_self" >Releases</a>
            </li><li class="masthead__menu-item">
              <a href="/download.html" target="_self" >Download</a>
            </li></ul>
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>
<!--
<p class="notice--warning" style="margin: 0 !important; text-align: center !important;"><strong>Note:</strong> This site is work in progress, if you notice any issues, please <a target="_blank" href="https://github.com/apache/hudi/issues">Report on Issue</a>.
  Click <a href="/"> here</a> back to old site.</p>
-->

    <div class="initial-content">
      <div id="main" role="main">
  

  <div class="sidebar sticky">

  
    <div itemscope itemtype="https://schema.org/Person">

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Quick Links</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>Hudi is the Streaming Data Lake Platform.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <ul class="author__urls social-icons">
      
        
          <li><a href="/docs/spark_quick-start-guide" target="_self" rel="nofollow noopener noreferrer"><i class="fa fa-book" aria-hidden="true"></i> Documentation</a></li>

          
        
          <li><a href="https://cwiki.apache.org/confluence/display/HUDI" target="_blank" rel="nofollow noopener noreferrer"><i class="fa fa-wikipedia-w" aria-hidden="true"></i> Technical Wiki</a></li>

          
        
          <li><a href="/contributing" target="_self" rel="nofollow noopener noreferrer"><i class="fa fa-thumbs-o-up" aria-hidden="true"></i> Contribution Guide</a></li>

          
        
          <li><a href="https://join.slack.com/t/apache-hudi/shared_invite/enQtODYyNDAxNzc5MTg2LTE5OTBlYmVhYjM0N2ZhOTJjOWM4YzBmMWU2MjZjMGE4NDc5ZDFiOGQ2N2VkYTVkNzU3ZDQ4OTI1NmFmYWQ0NzE" target="_blank" rel="nofollow noopener noreferrer"><i class="fa fa-slack" aria-hidden="true"></i> Join on Slack</a></li>

          
        
          <li><a href="https://github.com/apache/hudi" target="_blank" rel="nofollow noopener noreferrer"><i class="fa fa-github" aria-hidden="true"></i> Fork on GitHub</a></li>

          
        
          <li><a href="https://issues.apache.org/jira/projects/HUDI/summary" target="_blank" rel="nofollow noopener noreferrer"><i class="fa fa-navicon" aria-hidden="true"></i> Report Issues</a></li>

          
        
          <li><a href="/security" target="_self" rel="nofollow noopener noreferrer"><i class="fa fa-navicon" aria-hidden="true"></i> Report Security Issues</a></li>

          
        
      
    </ul>
  </div>
</div>

  

  
  </div>


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Releases
</h1>
        </header>
      

      <section class="page__content releases" itemprop="text">
        
        <aside class="sidebar__right sticky">
          <nav class="toc">
            <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> IN THIS PAGE</h4></header>
            <ul class="toc__menu">
  <li><a href="#release-080-docs">Release 0.8.0 (docs)</a>
    <ul>
      <li><a href="#download-information">Download Information</a></li>
      <li><a href="#migration-guide-for-this-release">Migration Guide for this release</a></li>
      <li><a href="#release-highlights">Release Highlights</a>
        <ul>
          <li><a href="#flink-integration">Flink Integration</a></li>
          <li><a href="#parallel-writers-support">Parallel Writers Support</a></li>
          <li><a href="#writer-side-improvements">Writer side improvements</a></li>
          <li><a href="#query-side-improvements">Query side improvements</a></li>
          <li><a href="#raw-release-notes">Raw Release Notes</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#release-070-docs">Release 0.7.0 (docs)</a>
    <ul>
      <li><a href="#download-information-1">Download Information</a></li>
      <li><a href="#migration-guide-for-this-release-1">Migration Guide for this release</a></li>
      <li><a href="#release-highlights-1">Release Highlights</a>
        <ul>
          <li><a href="#clustering">Clustering</a></li>
          <li><a href="#metadata-table">Metadata Table</a></li>
          <li><a href="#javaflink-writers">Java/Flink Writers</a></li>
          <li><a href="#writer-side-improvements-1">Writer side improvements</a></li>
          <li><a href="#query-side-improvements-1">Query side improvements</a></li>
          <li><a href="#raw-release-notes-1">Raw Release Notes</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#release-060-docs">Release 0.6.0 (docs)</a>
    <ul>
      <li><a href="#download-information-2">Download Information</a></li>
      <li><a href="#migration-guide-for-this-release-2">Migration Guide for this release</a></li>
      <li><a href="#release-highlights-2">Release Highlights</a>
        <ul>
          <li><a href="#writer-side-improvements-2">Writer side improvements:</a></li>
          <li><a href="#query-side-improvements-2">Query side improvements:</a></li>
          <li><a href="#usability">Usability:</a></li>
        </ul>
      </li>
      <li><a href="#raw-release-notes-2">Raw Release Notes</a></li>
    </ul>
  </li>
  <li><a href="#release-053-docs">Release 0.5.3 (docs)</a>
    <ul>
      <li><a href="#download-information-3">Download Information</a></li>
      <li><a href="#migration-guide-for-this-release-3">Migration Guide for this release</a></li>
      <li><a href="#release-highlights-3">Release Highlights</a></li>
      <li><a href="#raw-release-notes-3">Raw Release Notes</a></li>
    </ul>
  </li>
</ul>
          </nav>
        </aside>
        
        <h1 id="release-080-docs"><a href="https://github.com/apache/hudi/releases/tag/release-0.8.0">Release 0.8.0</a> (<a href="/docs/0.8.0-spark_quick-start-guide.html">docs</a>)</h1>

<h2 id="download-information">Download Information</h2>
<ul>
  <li>Source Release : <a href="https://downloads.apache.org/hudi/0.8.0/hudi-0.8.0.src.tgz">Apache Hudi 0.8.0 Source Release</a> (<a href="https://downloads.apache.org/hudi/0.8.0/hudi-0.8.0.src.tgz.asc">asc</a>, <a href="https://downloads.apache.org/hudi/0.8.0/hudi-0.8.0.src.tgz.sha512">sha512</a>)</li>
  <li>Apache Hudi jars corresponding to this release is available <a href="https://repository.apache.org/#nexus-search;quick~hudi">here</a></li>
</ul>

<h2 id="migration-guide-for-this-release">Migration Guide for this release</h2>
<ul>
  <li>If migrating from release older than 0.5.3, please also check the upgrade instructions for each subsequent release below.</li>
  <li>Specifically check upgrade instructions for 0.6.0. This release does not introduce any new table versions.</li>
  <li>The <code class="highlighter-rouge">HoodieRecordPayload</code> interface deprecated existing methods, in favor of new ones that also lets us pass properties at runtime. Users are
encouraged to migrate out of the deprecated methods, since they will be removed in 0.9.0.</li>
</ul>

<h2 id="release-highlights">Release Highlights</h2>

<h3 id="flink-integration">Flink Integration</h3>
<p>Since the initial support for the Hudi Flink Writer in the 0.7.0 release, the Hudi community made great progress on improving the Flink/Hudi integration, 
including redesigning the Flink writer pipeline with better performance and scalability, state-backed indexing with bootstrap support, 
Flink writer for MOR table, batch reader for COW&amp;MOR table, streaming reader for MOR table, and Flink SQL connector for both source and sink. 
In the 0.8.0 release, user is able to use all those features with Flink 1.11+.</p>

<p>Please see <a href="https://cwiki.apache.org/confluence/display/HUDI/RFC+-+24%3A+Hoodie+Flink+Writer+Proposal">RFC-24</a>
for more implementation details for the Flink writer and follow this <a href="/docs/flink-quick-start-guide.html">page</a> 
to get started with Flink!</p>

<h3 id="parallel-writers-support">Parallel Writers Support</h3>
<p>As many users requested, now Hudi supports multiple ingestion writers to the same Hudi Table with optimistic concurrency control.
Hudi supports file level OCC, i.e., for any 2 commits (or writers) happening to the same table, if they do not have writes to overlapping files being changed, 
both writers are allowed to succeed. This feature is currently experimental and requires either Zookeeper or HiveMetastore to acquire locks.</p>

<p>Please see <a href="https://cwiki.apache.org/confluence/display/HUDI/RFC+-+22+%3A+Snapshot+Isolation+using+Optimistic+Concurrency+Control+for+multi-writers">RFC-22</a>
for more implementation details and follow this <a href="/docs/concurrency_control.html">page</a> to get started with concurrency control!</p>

<h3 id="writer-side-improvements">Writer side improvements</h3>
<ul>
  <li>InsertOverwrite Support for Flink writer client.</li>
  <li>Support CopyOnWriteTable in Java writer client.</li>
</ul>

<h3 id="query-side-improvements">Query side improvements</h3>
<ul>
  <li>Support Spark Structured Streaming read from Hudi table.</li>
  <li>Performance improvement of Metadata table.</li>
  <li>Performance improvement of Clustering.</li>
</ul>

<h3 id="raw-release-notes">Raw Release Notes</h3>
<p>The raw release notes are available <a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12322822&amp;version=12349423">here</a></p>

<h1 id="release-070-docs"><a href="https://github.com/apache/hudi/releases/tag/release-0.7.0">Release 0.7.0</a> (<a href="/docs/0.7.0-quick-start-guide.html">docs</a>)</h1>

<h2 id="download-information-1">Download Information</h2>
<ul>
  <li>Source Release : <a href="https://downloads.apache.org/hudi/0.7.0/hudi-0.7.0.src.tgz">Apache Hudi 0.7.0 Source Release</a> (<a href="https://downloads.apache.org/hudi/0.7.0/hudi-0.7.0.src.tgz.asc">asc</a>, <a href="https://downloads.apache.org/hudi/0.7.0/hudi-0.7.0.src.tgz.sha512">sha512</a>)</li>
  <li>Apache Hudi jars corresponding to this release is available <a href="https://repository.apache.org/#nexus-search;quick~hudi">here</a></li>
</ul>

<h2 id="migration-guide-for-this-release-1">Migration Guide for this release</h2>
<ul>
  <li>If migrating from release older than 0.5.3, please also check the upgrade instructions for each subsequent release below.</li>
  <li>Specifically check upgrade instructions for 0.6.0. This release does not introduce any new table versions.</li>
  <li>The <code class="highlighter-rouge">HoodieRecordPayload</code> interface deprecated existing methods, in favor of new ones that also lets us pass properties at runtime. Users are
encouraged to migrate out of the deprecated methods, since they will be removed in 0.9.0.</li>
</ul>

<h2 id="release-highlights-1">Release Highlights</h2>

<h3 id="clustering">Clustering</h3>

<p>0.7.0 brings the ability to cluster your Hudi tables, to optimize for file sizes and also storage layout. Hudi will continue to
enforce file sizes, as it always has been, during the write. Clustering provides more flexibility to increase the file sizes 
down the line or ability to ingest data at much fresher intervals, and later coalesce them into bigger files. <a href="https://gist.github.com/vinothchandar/d7fa1338cddfae68390afcdfe310f94e#gistcomment-3383478">Microbenchmarks</a>
demonstrate a 3-4x reduction in query performance, for a 10-20x reduction in file sizes.</p>

<p>Additionally, clustering data based on fields that are often used in queries, dramatically 
<a href="https://cwiki.apache.org/confluence/display/HUDI/RFC+-+19+Clustering+data+for+freshness+and+query+performance#RFC19Clusteringdataforfreshnessandqueryperformance-PerformanceEvaluation">improves query performance</a> by allowing many files to be
completely skipped. This is very similar to the benefits of clustering delivered by <a href="https://docs.snowflake.com/en/user-guide/tables-clustering-micropartitions.html">cloud data warehouses</a>.
We are proud to announce that such capability is freely available in open source, for the first time, through the 0.7.0 release.</p>

<p>Please see <a href="https://cwiki.apache.org/confluence/display/HUDI/RFC+-+19+Clustering+data+for+freshness+and+query+performance">RFC-19</a> for more implementation details
and checkout configs <a href="/docs/configurations.html#clustering-configs">here</a> for how to use it in your pipelines. At the moment, we support both inline and async clustering modes.</p>

<h3 id="metadata-table">Metadata Table</h3>

<p>Since Hudi was born at Uber, on a HDFS backed data lake, we have since been a tad apathetic to the plight of listing performance on cloud storage (partly in hopes that
cloud providers will fix it over time:)). Nonetheless, 0.7.0 changes this and lays out the foundation for storing more indexes, metadata in an internal metadata table, 
which is implemented using a Hudi MOR table - which means it’s compacted, cleaned and also incrementally updated like any other Hudi table. Also, unlike similar 
implementations in other projects, we have chosen to index the file listing information as HFiles, which offers point-lookup performance to fetch listings for a single partition.</p>

<p>In 0.7.0 release, <code class="highlighter-rouge">hoodie.metadata.enable=true</code> on the writer side, will populate the metadata table with file system listings
so all operations don’t have to explicitly use <code class="highlighter-rouge">fs.listStatus()</code> anymore on data partitions. We have introduced a sync mechanism that
keeps syncing file additions/deletions on the data timeline, to the metadata table, after each write operation.</p>

<p>In our testing, on a large 250K file table, the metadata table delivers <a href="https://github.com/apache/hudi/pull/2441#issuecomment-761742963">2-3x speedup</a> over parallelized 
listing done by the Hudi spark writer. Please check <a href="https://cwiki.apache.org/confluence/display/HUDI/RFC+-+15%3A+HUDI+File+Listing+and+Query+Planning+Improvements">RFC-15 (ongoing)</a>
and the <a href="/docs/configurations.html#metadata-config">configurations</a>, which offer flags to help adopt this feature safely in your production pipelines.</p>

<h3 id="javaflink-writers">Java/Flink Writers</h3>

<p>Hudi was originally designed with a heavy dependency on Spark, given it had simply solve specific problems at Uber. But, as we have evolved as an Apache 
project, we realized the need for abstracting the internal table format, table services and writing layers of code. In 0.7.0, we have additionally added
Java and Flink based writers, as initial steps in this direction.</p>

<p>Specifically, the <code class="highlighter-rouge">HoodieFlinkStreamer</code> allows for Hudi Copy-On-Write table to built by streaming data from a Kafka topic.</p>

<h3 id="writer-side-improvements-1">Writer side improvements</h3>

<ul>
  <li><strong>Spark3 Support</strong>: We have added support for writing/querying data using Spark 3. please be sure to use the scala 2.12 hudi-spark-bundle.</li>
  <li><strong>Parallelized Listing</strong>: We have holistically moved all listings under the <code class="highlighter-rouge">HoodieTableMetadata</code> interface, which does multi-threaded/spark parallelized list operations. 
We expect this to improve cleaner, compaction scheduling performance, even when the metadata table is not used.</li>
  <li><strong>Kafka Commit Callbacks</strong>: We have added <code class="highlighter-rouge">HoodieWriteCommitKafkaCallback</code>, that publishes an event to Apache Kafka, for every commit operation. This can be used to trigger
derived/ETL pipelines similar to data <a href="https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/index.html">sensors</a> in Apache Airflow.</li>
  <li><strong>Insert Overwrite/Insert Overwrite Table</strong>: We have added these two new write operation types, predominantly to help existing batch ETL jobs, which typically overwrite entire 
tables/partitions each run. These operations are much cheaper, than having to issue upserts, given they are bulk replacing the target table.
Check <a href="/docs/spark_quick-start-guide.html#insert-overwrite-table">here</a> for examples.</li>
  <li><strong>Delete Partition</strong>: For users of WriteClient/RDD level apis, we have added an API to delete an entire partition, again without issuing deletes at the record level.</li>
  <li>The current default <code class="highlighter-rouge">OverwriteWithLatestAvroPayload</code> will overwrite the value in storage, even if for e.g the upsert was reissued for an older value of the key.
Added a new <code class="highlighter-rouge">DefaultHoodieRecordPayload</code> and a new payload config <code class="highlighter-rouge">hoodie.payload.ordering.field</code> helps specify a field, that the incoming upsert record can be compared with
the record on storage, to decide whether to overwrite or not. Users are encouraged to adopt this newer, more flexible model.</li>
  <li>Hive sync supports hourly partitions via <code class="highlighter-rouge">SlashEncodedHourPartitionValueExtractor</code></li>
  <li>Support for IBM Cloud storage, Open J9 JVM.</li>
</ul>

<h3 id="query-side-improvements-1">Query side improvements</h3>

<ul>
  <li><strong>Incremental Query on MOR (Spark Datasource)</strong>: Spark datasource now has experimental support for incremental queries on MOR table. This feature will be hardened and certified 
 in the next release, along with a large overhaul of the spark datasource implementation. (sshh!:))</li>
  <li><strong>Metadata Table For File Listings</strong>: Users can also leverage the metadata table on the query side for the following query paths. For Hive, setting the <code class="highlighter-rouge">hoodie.metadata.enable=true</code> session
property and for SparkSQL on Hive registered tables using <code class="highlighter-rouge">--conf spark.hadoop.hoodie.metadata.enable=true</code>, allows the file listings for the partition to be fetched out of the metadata
table, instead of listing the underlying DFS partition.</li>
</ul>

<h3 id="raw-release-notes-1">Raw Release Notes</h3>
<p>The raw release notes are available <a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12322822&amp;version=12348721">here</a></p>

<h1 id="release-060-docs"><a href="https://github.com/apache/hudi/releases/tag/release-0.6.0">Release 0.6.0</a> (<a href="/docs/0.6.0-quick-start-guide.html">docs</a>)</h1>

<h2 id="download-information-2">Download Information</h2>
<ul>
  <li>Source Release : <a href="https://downloads.apache.org/hudi/0.6.0/hudi-0.6.0.src.tgz">Apache Hudi 0.6.0 Source Release</a> (<a href="https://downloads.apache.org/hudi/0.6.0/hudi-0.6.0.src.tgz.asc">asc</a>, <a href="https://downloads.apache.org/hudi/0.6.0/hudi-0.6.0.src.tgz.sha512">sha512</a>)</li>
  <li>Apache Hudi jars corresponding to this release is available <a href="https://repository.apache.org/#nexus-search;quick~hudi">here</a></li>
</ul>

<h2 id="migration-guide-for-this-release-2">Migration Guide for this release</h2>
<ul>
  <li>If migrating from release older than 0.5.3, please also check the upgrade instructions for each subsequent release below.</li>
  <li>With 0.6.0 Hudi is moving from list based rollback to marker based rollbacks. To smoothly aid this transition a 
 new property called <code class="highlighter-rouge">hoodie.table.version</code> is added to <code class="highlighter-rouge">hoodie.properties</code> file. Whenever Hudi is launched with 
 newer table version i.e 1 (or moving from pre 0.6.0 to 0.6.0), an upgrade step will be executed automatically. 
 This automatic upgrade step will happen just once per Hudi table as the <code class="highlighter-rouge">hoodie.table.version</code> will be updated in property file after upgrade is completed.</li>
  <li>Similarly, a command line tool for Downgrading (command - <code class="highlighter-rouge">downgrade</code>) is added if in case some users want to downgrade Hudi from table version 1 to 0 or move from Hudi 0.6.0 to pre 0.6.0</li>
  <li>If you were using a user defined partitioner with bulkInsert() RDD API, the base interface has changed to <code class="highlighter-rouge">BulkInsertPartitioner</code> and will need minor adjustments to your existing implementations.</li>
</ul>

<h2 id="release-highlights-2">Release Highlights</h2>

<h3 id="writer-side-improvements-2">Writer side improvements:</h3>
<ul>
  <li>Bootstrapping existing parquet datasets :  Adds support for bootstrapping existing datasets into Hudi, via both Spark datasource writer and 
 deltastreamer tool, with support for reading from Hive, SparkSQL, AWS Athena (prestoDB support coming soon). See <a href="https://cwiki.apache.org/confluence/display/HUDI/RFC+-+15%3A+HUDI+File+Listing+and+Query+Planning+Improvements">RFC-15</a> for technical details. 
 Note that this is an experimental feature, which will be improved upon further in the 0.6.x versions.</li>
  <li>Native row writing for bulk_insert : Avoids any dataframe-rdd conversion for bulk_insert path, which can improve performance of initial bulk loads.
  Although, this is typically not the bottleneck for upsert/deletes, subsequent releases in 0.6.x versions will expand this to other write operations
  to make reasoning about schema management easier, avoiding the spark-avro conversion totally.</li>
  <li>Bulk insert sort modes : Hudi bulk_insert sorts the input globally to optimize file sizes and avoid out-of-memory issues encountered when writing parallely to multiple DFS partitions. 
 For users who want to prepare the dataframe for writing outside of Hudi, we have made this configurable using <code class="highlighter-rouge">hoodie.bulkinsert.sort.mode</code>.</li>
  <li>Cleaning can now be run concurrently with writing, using <code class="highlighter-rouge">hoodie.clean.async=true</code>which can speed up time taken to finish committing.</li>
  <li>Async compaction for spark streaming writes to hudi table, is now self managed by default, controlling <code class="highlighter-rouge">hoodie.datasource.compaction.async.enable</code>.</li>
  <li>Rollbacks no longer perform full table listings, by leveraging marker files. To enable, set <code class="highlighter-rouge">hoodie.rollback.using.markers=true</code>.</li>
  <li>Added a new index <code class="highlighter-rouge">hoodie.index.type=SIMPLE</code> which can be faster than <code class="highlighter-rouge">BLOOM_INDEX</code> for cases where updates/deletes spread across a large portion of the table.</li>
  <li>Hudi now supports <code class="highlighter-rouge">Azure Data Lake Storage V2</code> , <code class="highlighter-rouge">Alluxio</code> and <code class="highlighter-rouge">Tencent Cloud Object Storage</code> storages.</li>
  <li><a href="https://hudi.apache.org/docs/writing_data.html#multitabledeltastreamer">HoodieMultiDeltaStreamer</a> adds support for ingesting multiple kafka streams in a single DeltaStreamer deployment, effectively reducing operational burden for using delta streamer 
as your data lake ingestion tool (Experimental feature)</li>
  <li>Added a new tool - InitialCheckPointProvider, to set checkpoints when migrating to DeltaStreamer after an initial load of the table is complete.</li>
  <li>Delta Streamer tool now supports ingesting CSV data sources, chaining of multiple transformers to build more advanced ETL jobs.</li>
  <li>Introducing a new <code class="highlighter-rouge">CustomKeyGenerator</code> key generator class, that provides flexible configurations to provide enable different types of key, partition path generation in  single class.
We also added support for more time units and date/time formats in <code class="highlighter-rouge">TimestampBasedKeyGenerator</code>. See <a href="https://hudi.apache.org/docs/writing_data.html#key-generation">docs</a> for more.</li>
</ul>

<h3 id="query-side-improvements-2">Query side improvements:</h3>
<ul>
  <li>Starting 0.6.0, snapshot queries are feasible on MOR tables using spark datasource. (experimental feature)</li>
  <li>In prior versions we only supported <code class="highlighter-rouge">HoodieCombineHiveInputFormat</code> for CopyOnWrite tables to ensure that there is a limit on the number of mappers spawned for
any query. Hudi now supports Merge on Read tables also using <code class="highlighter-rouge">HoodieCombineInputFormat</code>.</li>
  <li>Speedup spark read queries by caching metaclient in HoodieROPathFilter. This helps reduce listing related overheads in S3 when filtering files for read-optimized queries.</li>
</ul>

<h3 id="usability">Usability:</h3>
<ul>
  <li>Spark DAGs are named to aid better debuggability.</li>
  <li>Support pluggable metrics reporting by introducing proper abstraction for user defined metrics. Console, JMX, Prometheus and DataDog metric reporters have been added.</li>
  <li>A new utility called Data snapshot exporter has been added. Latest table snapshot as of a certain point in time can be exported as plain parquet files with this tool.</li>
  <li>Introduce write committed callback hooks for incremental pipelines to be notified and act on new commits in the timeline. For e.g, Apache Airflow jobs can be triggered
as new commits arrive.</li>
  <li>Added support for deleting savepoints via CLI</li>
  <li>Added a new command - <code class="highlighter-rouge">export instants</code>, to export metadata of instants</li>
</ul>

<h2 id="raw-release-notes-2">Raw Release Notes</h2>
<p>The raw release notes are available <a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12322822&amp;version=12346663">here</a></p>

<h1 id="release-053-docs"><a href="https://github.com/apache/hudi/releases/tag/release-0.5.3">Release 0.5.3</a> (<a href="/docs/0.5.3-quick-start-guide.html">docs</a>)</h1>

<h2 id="download-information-3">Download Information</h2>
<ul>
  <li>Source Release : <a href="https://downloads.apache.org/hudi/0.5.3/hudi-0.5.3.src.tgz">Apache Hudi 0.5.3 Source Release</a> (<a href="https://downloads.apache.org/hudi/0.5.3/hudi-0.5.3.src.tgz.asc">asc</a>, <a href="https://downloads.apache.org/hudi/0.5.3/hudi-0.5.3.src.tgz.sha512">sha512</a>)</li>
  <li>Apache Hudi jars corresponding to this release is available <a href="https://repository.apache.org/#nexus-search;quick~hudi">here</a></li>
</ul>

<h2 id="migration-guide-for-this-release-3">Migration Guide for this release</h2>
<ul>
  <li>This is a bug fix only release and no special migration steps needed when upgrading from 0.5.2. If you are upgrading from earlier releases “X”, please make sure you read the migration guide for each subsequent release between “X” and 0.5.3</li>
  <li>0.5.3 is the first hudi release after graduation. As a result, all hudi jars will no longer have “-incubating” in the version name. In all the places where hudi version is referred, please make sure “-incubating” is no longer present.</li>
</ul>

<p>For example hudi-spark-bundle pom dependency would look like:</p>
<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="nt">&lt;dependency&gt;</span>
        <span class="nt">&lt;groupId&gt;</span>org.apache.hudi<span class="nt">&lt;/groupId&gt;</span>
        <span class="nt">&lt;artifactId&gt;</span>hudi-spark-bundle_2.12<span class="nt">&lt;/artifactId&gt;</span>
        <span class="nt">&lt;version&gt;</span>0.5.3<span class="nt">&lt;/version&gt;</span>
    <span class="nt">&lt;/dependency&gt;</span>
</code></pre></div></div>
<h2 id="release-highlights-3">Release Highlights</h2>
<ul>
  <li>Hudi now supports <code class="highlighter-rouge">aliyun OSS</code> storage service.</li>
  <li>Embedded Timeline Server is enabled by default for both delta-streamer and spark datasource writes. This feature was in experimental mode before this release. Embedded Timeline Server caches file listing calls in Spark driver and serves them to Spark writer tasks. This reduces the number of file listings needed to be performed for each write.</li>
  <li>Incremental Cleaning is enabled by default for both delta-streamer and spark datasource writes. This feature was also in experimental mode before this release. In the steady state, incremental cleaning avoids the costly step of scanning all partitions and instead uses Hudi metadata to find files to be cleaned up.</li>
  <li>Delta-streamer config files can now be placed in different filesystem than actual data.</li>
  <li>Hudi Hive Sync now supports tables partitioned by date type column.</li>
  <li>Hudi Hive Sync now supports syncing directly via Hive MetaStore. You simply need to set hoodie.datasource.hive_sync.use_jdbc
=false. Hive Metastore Uri will be read implicitly from environment. For example, when writing through Spark Data Source,</li>
</ul>

<pre><code class="language-Scala"> spark.write.format(“hudi”)
 .option(…)
 .option(“hoodie.datasource.hive_sync.username”, “&lt;user&gt;”)
 .option(“hoodie.datasource.hive_sync.password”, “&lt;password&gt;”)
 .option(“hoodie.datasource.hive_sync.partition_fields”, “&lt;partition_fields&gt;”)
 .option(“hoodie.datasource.hive_sync.database”, “&lt;db_name&gt;”)
 .option(“hoodie.datasource.hive_sync.table”, “&lt;table_name&gt;”)
 .option(“hoodie.datasource.hive_sync.use_jdbc”, “false”)
 .mode(APPEND)
 .save(“/path/to/dataset”)
</code></pre>

<ul>
  <li>Other Writer Performance related fixes:
    <ul>
      <li>DataSource Writer now avoids unnecessary loading of data after write.</li>
      <li>Hudi Writer now leverages spark parallelism when searching for existing files for writing new records.</li>
    </ul>
  </li>
</ul>

<h2 id="raw-release-notes-3">Raw Release Notes</h2>
<p>The raw release notes are available <a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12322822&amp;version=12348256">here</a></p>

<p>For releases older than these versions, please see <a href="/older-releases.html">here</a>.</p>

        
      </section>

      <a href="#masthead__inner-wrap" class="back-to-top">Back to top &uarr;</a>

      

    </div>
  </article>

</div>

    </div>

    <div class="page__footer">
      <footer>
        
<div class="row">
  <div class="col-lg-12 footer">
    <p>
      <table class="table-apache-info">
        <tr>
          <td>
            <a class="footer-link-img" href="https://apache.org">
              <img width="250px" src="/assets/images/asf_logo.svg" alt="The Apache Software Foundation">
            </a>
          </td>
          <td>
            <a style="float: right" href="https://www.apache.org/events/current-event.html">
              <img src="https://www.apache.org/events/current-event-234x60.png" />
            </a>
          </td>
        </tr>
      </table>
    </p>
    <p>
      <a href="https://www.apache.org/licenses/">License</a> | <a href="https://www.apache.org/security/">Security</a> | <a href="https://www.apache.org/foundation/thanks.html">Thanks</a> | <a href="https://www.apache.org/foundation/sponsorship.html">Sponsorship</a>
    </p>
    <p>
      Copyright &copy; <span id="copyright-year">2019</span> <a href="https://apache.org">The Apache Software Foundation</a>, Licensed under the <a href="https://www.apache.org/licenses/LICENSE-2.0"> Apache License, Version 2.0</a>.
      Hudi, Apache and the Apache feather logo are trademarks of The Apache Software Foundation. <a href="/docs/privacy">Privacy Policy</a>
    </p>
  </div>
</div>
      </footer>
    </div>


  </body>
</html>