{
    "version": "https://jsonfeed.org/version/1",
    "title": "Apache Hudi: User-Facing Analytics",
    "home_page_url": "https://hudi.apache.org/blog",
    "description": "Apache Hudi Blog",
    "items": [
        {
            "id": "/2022/06/04/Asynchronous-Indexing-Using-Hudi",
            "content_html": "<div url=\"https://www.onehouse.ai/blog/asynchronous-indexing-using-hudi\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2022/06/04/Asynchronous-Indexing-Using-Hudi",
            "title": "Asynchronous Indexing using Hudi",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2022-06-04T00:00:00.000Z",
            "author": {
                "name": "Sagar Sumit"
            }
        },
        {
            "id": "/2022/05/17/Introducing-Multi-Modal-Index-for-the-Lakehouse-in-Apache-Hudi",
            "content_html": "<div url=\"https://www.onehouse.ai/blog/introducing-multi-modal-index-for-the-lakehouse-in-apache-hudi\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2022/05/17/Introducing-Multi-Modal-Index-for-the-Lakehouse-in-Apache-Hudi",
            "title": "Multi-Modal Index for the Lakehouse in Apache Hudi",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2022-05-17T00:00:00.000Z",
            "author": {
                "name": "Sivabalan Narayanan"
            }
        },
        {
            "id": "/2022/04/04/Key-Learnings-on-Using-Apache-HUDI-in-building-Lakehouse-Architecture-at-Halodoc",
            "content_html": "<div url=\"https://blogs.halodoc.io/key-learnings-on-using-apache-hudi-in-building-lakehouse-architecture-halodoc/\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2022/04/04/Key-Learnings-on-Using-Apache-HUDI-in-building-Lakehouse-Architecture-at-Halodoc",
            "title": "Key Learnings on Using Apache HUDI in building Lakehouse Architecture @ Halodoc",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2022-04-04T00:00:00.000Z",
            "author": {
                "name": "Jitendra Shah"
            }
        },
        {
            "id": "/2022/04/04/New-features-from-Apache-Hudi-0.9.0-on-Amazon-EMR",
            "content_html": "<div url=\"https://aws.amazon.com/blogs/big-data/new-features-from-apache-hudi-0-9-0-on-amazon-emr/\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2022/04/04/New-features-from-Apache-Hudi-0.9.0-on-Amazon-EMR",
            "title": "New features from Apache Hudi 0.9.0 on Amazon EMR",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2022-04-04T00:00:00.000Z",
            "author": {
                "name": "Kunal Gautam"
            }
        },
        {
            "id": "/2022/03/24/Zendesk-Insights-for-CTOs-Part-3-Growing-your-business-with-modern-data-capabilities",
            "content_html": "<div url=\"https://aws.amazon.com/blogs/architecture/insights-for-ctos-part-3-growing-your-business-with-modern-data-capabilities/\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2022/03/24/Zendesk-Insights-for-CTOs-Part-3-Growing-your-business-with-modern-data-capabilities",
            "title": "Zendesk - Insights for CTOs: Part 3 – Growing your business with modern data capabilities",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2022-03-24T00:00:00.000Z",
            "author": {
                "name": "Syed Jaffry"
            }
        },
        {
            "id": "/2022/03/09/Build-a-serverless-pipeline-to-analyze-streaming-data-using-AWS-Glue-Apache-Hudi-and-Amazon-S3",
            "content_html": "<div url=\"https://aws.amazon.com/blogs/big-data/build-a-serverless-pipeline-to-analyze-streaming-data-using-aws-glue-apache-hudi-and-amazon-s3/\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2022/03/09/Build-a-serverless-pipeline-to-analyze-streaming-data-using-AWS-Glue-Apache-Hudi-and-Amazon-S3",
            "title": "Build a serverless pipeline to analyze streaming data using AWS Glue, Apache Hudi, and Amazon S3",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2022-03-09T00:00:00.000Z",
            "author": {
                "name": "Nikhil Khokhar"
            }
        },
        {
            "id": "/2022/03/01/Create-a-low-latency-source-to-data-lake-pipeline-using-Amazon-MSK-Connect-Apache-Flink-and-Apache-Hudi",
            "content_html": "<div url=\"https://aws.amazon.com/blogs/big-data/create-a-low-latency-source-to-data-lake-pipeline-using-amazon-msk-connect-apache-flink-and-apache-hudi/\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2022/03/01/Create-a-low-latency-source-to-data-lake-pipeline-using-Amazon-MSK-Connect-Apache-Flink-and-Apache-Hudi",
            "title": "Create a low-latency source-to-data lake pipeline using Amazon MSK Connect, Apache Flink, and Apache Hudi",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2022-03-01T00:00:00.000Z",
            "author": {
                "name": "Ali Alemi"
            }
        },
        {
            "id": "/2022/02/20/Understanding-its-core-concepts-from-hudi-persistence-files",
            "content_html": "<div url=\"https://programmer.ink/think/understanding-its-core-concepts-from-hudi-persistence-files.html\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2022/02/20/Understanding-its-core-concepts-from-hudi-persistence-files",
            "title": "Understanding its core concepts from hudi persistence files",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2022-02-20T00:00:00.000Z",
            "author": {
                "name": "QbertsBrother"
            }
        },
        {
            "id": "/2022/02/17/Fresher-Data-Lake-on-AWS-S3",
            "content_html": "<div url=\"https://robinhood.engineering/author-balaji-varadarajan-e3f496815ebf\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2022/02/17/Fresher-Data-Lake-on-AWS-S3",
            "title": "Fresher Data Lake on AWS S3",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2022-02-17T00:00:00.000Z",
            "author": {
                "name": "Balaji Varadarajan"
            }
        },
        {
            "id": "/2022/02/12/Open-Source-Data-Lake-Table-Formats-Evaluating-Current-Interest-and-Rate-of-Adoption",
            "content_html": "<div url=\"https://garystafford.medium.com/data-lake-table-formats-interest-and-adoption-rate-40817b87be9e\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2022/02/12/Open-Source-Data-Lake-Table-Formats-Evaluating-Current-Interest-and-Rate-of-Adoption",
            "title": "Open Source Data Lake Table Formats: Evaluating Current Interest and Rate of Adoption",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2022-02-12T00:00:00.000Z",
            "author": {
                "name": "Gary Stafford"
            }
        },
        {
            "id": "/2022/02/09/ACID-transformations-on-Distributed-file-system",
            "content_html": "<div url=\"https://medium.com/walmartglobaltech/acid-transformations-on-distributed-file-system-fdec5301c1b1\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2022/02/09/ACID-transformations-on-Distributed-file-system",
            "title": "ACID transformations on Distributed file system",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2022-02-09T00:00:00.000Z",
            "author": {
                "name": "Rajasekhar"
            }
        },
        {
            "id": "/2022/02/03/Onehouse-brings-a-fully-managed-lakehouse-to-Apache-Hudi",
            "content_html": "<div url=\"https://venturebeat.com/2022/02/03/onehouse-brings-a-fully-managed-lakehouse-to-apache-hudi/\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2022/02/03/Onehouse-brings-a-fully-managed-lakehouse-to-Apache-Hudi",
            "title": "Onehouse brings a fully-managed lakehouse to Apache Hudi",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2022-02-03T00:00:00.000Z",
            "author": {
                "name": "Paul Sawers"
            }
        },
        {
            "id": "/2022/02/02/Onehouse-Commitment-to-Openness",
            "content_html": "<div url=\"https://www.onehouse.ai/blog/onehouse-commitment-to-openness\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2022/02/02/Onehouse-Commitment-to-Openness",
            "title": "Onehouse Commitment to Openness",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2022-02-02T00:00:00.000Z",
            "author": {
                "name": "Vinoth Chandar"
            }
        },
        {
            "id": "/2022/01/25/Cost-Efficiency-Scale-in-Big-Data-File-Format",
            "content_html": "<div url=\"https://eng.uber.com/cost-efficiency-big-data/\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2022/01/25/Cost-Efficiency-Scale-in-Big-Data-File-Format",
            "title": "Cost Efficiency @ Scale in Big Data File Format",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2022-01-25T00:00:00.000Z",
            "author": {
                "name": "Xinli Shang"
            }
        },
        {
            "id": "/2022/01/20/Hudi-powering-data-lake-efforts-at-Walmart-and-Disney-Hotstar",
            "content_html": "<div url=\"https://www.techtarget.com/searchdatamanagement/feature/Hudi-powering-data-lake-efforts-at-Walmart-and-Disney-Hotstar\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2022/01/20/Hudi-powering-data-lake-efforts-at-Walmart-and-Disney-Hotstar",
            "title": "Hudi powering data lake efforts at Walmart and Disney+ Hotstar",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2022-01-20T00:00:00.000Z",
            "author": {
                "name": "Sean Michael Kerner"
            }
        },
        {
            "id": "/2022/01/18/Why-and-How-I-Integrated-Airbyte-and-Apache-Hudi",
            "content_html": "<div url=\"https://selectfrom.dev/why-and-how-i-integrated-airbyte-and-apache-hudi-c18aff3af21a\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2022/01/18/Why-and-How-I-Integrated-Airbyte-and-Apache-Hudi",
            "title": "Why and How I Integrated Airbyte and Apache Hudi",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2022-01-18T00:00:00.000Z",
            "author": {
                "name": "Harsha Teja Kanna"
            }
        },
        {
            "id": "/2022/01/14/change-data-capture-with-debezium-and-apache-hudi",
            "content_html": "<p>As of Hudi v0.10.0, we are excited to announce the availability of <a href=\"https://debezium.io/\">Debezium</a> sources for <a href=\"https://hudi.apache.org/docs/hoodie_deltastreamer\">Deltastreamer</a> that provide the ingestion of change capture data (CDC) from Postgres and Mysql databases to your data lake. For more details, please refer to the original <a href=\"https://github.com/apache/hudi/blob/master/rfc/rfc-39/rfc-39.md\">RFC</a>.</p><h2>Background</h2><img src=\"/assets/images/blog/data-network.png\" alt=\"drawing\" width=\"600\"/><p>When you want to perform analytics on data from transactional databases like Postgres or Mysql you typically need to bring this data into an OLAP system such as a data warehouse or a data lake through a process called <a href=\"https://debezium.io/documentation/faq/#what_is_change_data_capture\">Change Data Capture</a> (CDC). Debezium is a popular tool that makes CDC easy. It provides a way to capture row-level changes in your databases by <a href=\"https://debezium.io/blog/2018/07/19/advantages-of-log-based-change-data-capture/\">reading changelogs</a>. By doing so, Debezium avoids increased CPU load on your database and ensures you capture all changes including deletes.</p><p>Now that <a href=\"https://hudi.apache.org/docs/overview/\">Apache Hudi</a> offers a Debezium source connector, CDC ingestion into a data lake is easier than ever with some <a href=\"https://hudi.apache.org/docs/use_cases\">unique differentiated capabilities</a>. Hudi enables efficient update, merge, and delete transactions on a data lake. Hudi uniquely provides <a href=\"https://hudi.apache.org/docs/table_types#merge-on-read-table\">Merge-On-Read</a> writers which unlock <a href=\"https://aws.amazon.com/blogs/big-data/how-amazon-transportation-service-enabled-near-real-time-event-analytics-at-petabyte-scale-using-aws-glue-with-apache-hudi/\">significantly lower latency</a> ingestion than typical data lake writers with Spark or Flink. Last but not least, Apache Hudi offers <a href=\"https://hudi.apache.org/docs/querying_data#spark-incr-query\">incremental queries</a> so after capturing changes from your database, you can incrementally process these changes downstream throughout all of your subsequent ETL pipelines.</p><h2>Design Overview</h2><img src=\"/assets/images/blog/debezium.png\" alt=\"drawing\" width=\"600\"/><p>The architecture for an end-to-end CDC ingestion flow with Apache Hudi is shown above. The first component is the Debezium deployment, which consists of a Kafka cluster, schema registry (Confluent or Apicurio), and the Debezium connector. The Debezium connector continuously polls the changelogs from the database and writes an AVRO message with the changes for each database row to a dedicated Kafka topic per table.</p><p>The second component is <a href=\"https://hudi.apache.org/docs/hoodie_deltastreamer\">Hudi Deltastreamer</a> that reads and processes the incoming Debezium records from Kafka for each table and writes (updates) the corresponding rows in a Hudi table on your cloud storage.</p><p>To ingest the data from the database table into a Hudi table in near real-time, we implement two classes that can be plugged into the Deltastreamer. Firstly, we implemented a <a href=\"https://github.com/apache/hudi/blob/83f8ed2ae3ba7fb20813cbb8768deae6244b020c/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/debezium/DebeziumSource.java\">Debezium source</a>. With Deltastreamer running in continuous mode, the source continuously reads and processes the Debezium change records in Avro format from the Kafka topic for a given table, and writes the updated record to the destination Hudi table. In addition to the columns from the database table, we also ingest some meta fields that are added by Debezium in the target Hudi table. The meta fields help us correctly merge updates and delete records. The records are read using the latest schema from the <a href=\"https://hudi.apache.org/docs/hoodie_deltastreamer#schema-providers\">Schema Registry</a>.</p><p>Secondly, we implement a custom <a href=\"https://github.com/apache/hudi/blob/83f8ed2ae3ba7fb20813cbb8768deae6244b020c/hudi-common/src/main/java/org/apache/hudi/common/model/debezium/AbstractDebeziumAvroPayload.java\">Debezium Payload</a> that essentially governs how Hudi records are merged when the same row is updated or deleted. When a new Hudi record is received for an existing row, the payload picks the latest record using the higher value of the appropriate column (FILEID and POS fields in MySql and LSN fields in Postgres). In the case that the latter event is a delete record, the payload implementation ensures that the record is hard deleted from the storage. Delete records are identified using the op field, which has a value of <strong>d</strong> for deletes.</p><h2>Apache Hudi Configurations</h2><p>It is important to consider the following configurations of your Hudi deployments when using the Debezium source connector for CDC ingestion.</p><ol><li><strong>Record Keys -</strong> The Hudi <a href=\"https://hudi.apache.org/docs/next/indexing\">record key(s)</a> for a table should be set as the Primary keys of the table in the upstream database. This ensures that updates are applied correctly as record key(s) uniquely identify a row in the Hudi table.</li><li><strong>Source Ordering Fields</strong> -  For de-duplication of changelog records the source ordering field should be set to the actual position of the change event as it happened on the database. For instance, we use the FILEID and POS fields in MySql and LSN fields in Postgres databases respectively to ensure records are processed in the correct order of occurrence in the original database.</li><li><strong>Partition Fields</strong> - Don’t feel restricted to matching the partitioning of your Hudi tables with the same partition fields as the upstream database. You can set partition fields independently for the Hudi table as needed.</li></ol><h3>Bootstrapping Existing tables</h3><p>One important use case might be when CDC ingestion has to be done for existing database tables. There are two ways we can ingest existing database data prior to streaming the changes:</p><ol><li>By default on initialization, Debezium performs an initial consistent snapshot of the database (controlled by config snapshot.mode). After the initial snapshot, it continues streaming updates from the correct position to avoid loss of data.</li><li>While the first approach is simple, for large tables it may take a long time for Debezium to bootstrap the initial snapshot. Alternatively, we could run a Deltastreamer job to bootstrap the table directly from the database using the <a href=\"https://github.com/apache/hudi/blob/master/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/JdbcSource.java\">JDBC source</a>. This provides more flexibility to the users in defining and executing more optimized SQL queries required to bootstrap the database table. Once the bootstrap job finishes successfully, another Deltastreamer job is executed that processes the database changelogs from Debezium. Users will have to use <a href=\"https://hudi.apache.org/docs/hoodie_deltastreamer/#checkpointing\">checkpointing</a> in Deltastreamer to ensure the second job starts processing the changelogs from the correct position to avoid data loss.</li></ol><h3>Example Implementation</h3><p>The following describes steps to implement an end-to-end CDC pipeline using an AWS RDS instance of Postgres, Kubernetes-based Debezium deployment, and Hudi Deltastreamer running on a spark cluster.</p><h3>Database</h3><p>A few configuration changes are required for the RDS instance to enable logical replication.</p><pre><code class=\"language-roomsql\">SET rds.logical_replication to 1 (instead of 0)\n\npsql --host=&lt;aws_rds_instance&gt; --port=5432 --username=postgres --password -d &lt;database_name&gt;;\n\nCREATE PUBLICATION &lt;publication_name&gt; FOR TABLE schema1.table1, schema1.table2;\n\nALTER TABLE schema1.table1 REPLICA IDENTITY FULL;\n</code></pre><h3>Debezium Connector</h3><p><a href=\"https://strimzi.io/blog/2020/01/27/deploying-debezium-with-kafkaconnector-resource/\">Strimzi</a> is the recommended option to deploy and manage Kafka connectors on Kubernetes clusters. Alternatively, you have the option to use the Confluent managed <a href=\"https://docs.confluent.io/debezium-connect-postgres-source/current/overview.html\">Debezium connector</a>.</p><pre><code>kubectl create namespace kafka\nkubectl create -f https://strimzi.io/install/latest?namespace=kafka -n kafka\nkubectl -n kafka apply -f kafka-connector.yaml\n</code></pre><p>An example for kafka-connector.yaml is shown below:</p><pre><code class=\"language-yaml\">apiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaConnect\nmetadata:\nname: debezium-kafka-connect\nannotations:\nstrimzi.io/use-connector-resources: &quot;false&quot;\nspec:\nimage: debezium-kafka-connect:latest\nreplicas: 1\nbootstrapServers: localhost:9092\nconfig:\nconfig.storage.replication.factor: 1\noffset.storage.replication.factor: 1\nstatus.storage.replication.factor: 1\n</code></pre><p>The docker image debezium-kafka-connect can be built using the following Dockerfile that includes the Postgres Debezium Connector.</p><pre><code class=\"language-yaml\">FROM confluentinc/cp-kafka-connect:6.2.0 as cp\nRUN confluent-hub install --no-prompt confluentinc/kafka-connect-avro-converter:6.2.0\nFROM strimzi/kafka:0.18.0-kafka-2.5.0\nUSER root:root\nRUN yum -y update\nRUN yum -y install git\nRUN yum -y install wget\n\nRUN wget https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/1.6.1.Final/debezium-connector-postgres-1.6.1.Final-plugin.tar.gz\nRUN tar xzf debezium-connector-postgres-1.6.1.Final-plugin.tar.gz\n\nRUN mkdir -p /opt/kafka/plugins/debezium &amp;&amp; mkdir -p /opt/kafka/plugins/avro/\nRUN mv debezium-connector-postgres /opt/kafka/plugins/debezium/\nCOPY --from=cp /usr/share/confluent-hub-components/confluentinc-kafka-connect-avro-converter/lib /opt/kafka/plugins/avro/\nUSER 1001\n</code></pre><p>Once the Strimzi operator and the Kafka connect are deployed, we can start the Debezium connector.</p><pre><code>curl -X POST -H &quot;Content-Type:application/json&quot; -d @connect-source.json http://localhost:8083/connectors/\n</code></pre><p>The following is an example of a configuration to setup Debezium connector for generating the changelogs for two tables, table1, and table2.</p><p>Contents of connect-source.json:</p><pre><code class=\"language-json\">{\n  &quot;name&quot;: &quot;postgres-debezium-connector&quot;,\n  &quot;config&quot;: {\n    &quot;connector.class&quot;: &quot;io.debezium.connector.postgresql.PostgresConnector&quot;,\n    &quot;database.hostname&quot;: &quot;localhost&quot;,\n    &quot;database.port&quot;: &quot;5432&quot;,\n    &quot;database.user&quot;: &quot;postgres&quot;,\n    &quot;database.password&quot;: &quot;postgres&quot;,\n    &quot;database.dbname&quot;: &quot;database&quot;,\n    &quot;plugin.name&quot;: &quot;pgoutput&quot;,\n    &quot;database.server.name&quot;: &quot;postgres&quot;,\n    &quot;table.include.list&quot;: &quot;schema1.table1,schema1.table2&quot;,\n    &quot;publication.autocreate.mode&quot;: &quot;filtered&quot;,\n    &quot;tombstones.on.delete&quot;:&quot;false&quot;,\n    &quot;key.converter&quot;: &quot;io.confluent.connect.avro.AvroConverter&quot;,\n    &quot;key.converter.schema.registry.url&quot;: &quot;&lt;schema_registry_host&gt;&quot;,\n    &quot;value.converter&quot;: &quot;io.confluent.connect.avro.AvroConverter&quot;,\n    &quot;value.converter.schema.registry.url&quot;: &quot;&lt;schema_registry_host&gt;&quot;,\n    &quot;slot.name&quot;: &quot;pgslot&quot;\n  }\n}\n</code></pre><h3>Hudi Deltastreamer</h3><p>Next, we run the Hudi Deltastreamer using spark that will ingest the Debezium changelogs from kafka and write them as a Hudi table. One such instance of the command is shown below that works for Postgres database.  A few key configurations are as follows:</p><ol><li>Set the source class to PostgresDebeziumSource.</li><li>Set the payload class to PostgresDebeziumAvroPayload.</li><li>Configure the schema registry URLs for Debezium Source and Kafka Source.</li><li>Set the record key(s) as the primary key(s) of the database table.</li><li>Set the source ordering field (dedup) to _event_lsn</li></ol><pre><code class=\"language-scala\">spark-submit \\\\\n  --jars &quot;/home/hadoop/hudi-utilities-bundle_2.12-0.10.0.jar,/usr/lib/spark/external/lib/spark-avro.jar&quot; \\\\\n  --master yarn --deploy-mode client \\\\\n  --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer /home/hadoop/hudi-packages/hudi-utilities-bundle_2.12-0.10.0-SNAPSHOT.jar \\\\\n  --table-type COPY_ON_WRITE --op UPSERT \\\\\n  --target-base-path s3://bucket_name/path/for/hudi_table1 \\\\\n  --target-table hudi_table1  --continuous \\\\\n  --min-sync-interval-seconds 60 \\\\\n  --source-class org.apache.hudi.utilities.sources.debezium.PostgresDebeziumSource \\\\\n  --source-ordering-field _event_lsn \\\\\n  --payload-class org.apache.hudi.common.model.debezium.PostgresDebeziumAvroPayload \\\\\n  --hoodie-conf schema.registry.url=https://localhost:8081 \\\\\n  --hoodie-conf hoodie.deltastreamer.schemaprovider.registry.url=https://localhost:8081/subjects/postgres.schema1.table1-value/versions/latest \\\\\n  --hoodie-conf hoodie.deltastreamer.source.kafka.value.deserializer.class=io.confluent.kafka.serializers.KafkaAvroDeserializer \\\\\n  --hoodie-conf hoodie.deltastreamer.source.kafka.topic=postgres.schema1.table1 \\\\\n  --hoodie-conf auto.offset.reset=earliest \\\\\n  --hoodie-conf hoodie.datasource.write.recordkey.field=”database_primary_key” \\\\\n  --hoodie-conf hoodie.datasource.write.partitionpath.field=partition_key \\\\\n  --enable-hive-sync \\\\\n  --hoodie-conf hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor \\\\\n  --hoodie-conf hoodie.datasource.write.hive_style_partitioning=true \\\\\n  --hoodie-conf hoodie.datasource.hive_sync.database=default \\\\\n  --hoodie-conf hoodie.datasource.hive_sync.table=hudi_table1 \\\\\n  --hoodie-conf hoodie.datasource.hive_sync.partition_fields=partition_key\n</code></pre><h2>Conclusion</h2><p>This post introduced the Debezium Source for Hudi Deltastreamer to ingest Debezium changelogs into Hudi tables. Database data can now be ingested into data lakes to provide a cost-effective way to store and analyze database data.</p><p>Please follow this <a href=\"https://issues.apache.org/jira/browse/HUDI-1290\">JIRA</a> to learn more about active development on this new feature. I look forward to more contributions and feedback from the community. Come join our <a href=\"https://join.slack.com/t/apache-hudi/shared_invite/enQtODYyNDAxNzc5MTg2LTE5OTBlYmVhYjM0N2ZhOTJjOWM4YzBmMWU2MjZjMGE4NDc5ZDFiOGQ2N2VkYTVkNzU3ZDQ4OTI1NmFmYWQ0NzE\">Hudi Slack</a> channel or attend one of our <a href=\"https://hudi.apache.org/community/syncs\">community events</a> to learn more.</p>",
            "url": "https://hudi.apache.org/blog/2022/01/14/change-data-capture-with-debezium-and-apache-hudi",
            "title": "Change Data Capture with Debezium and Apache Hudi",
            "summary": "As of Hudi v0.10.0, we are excited to announce the availability of Debezium sources for Deltastreamer that provide the ingestion of change capture data (CDC) from Postgres and Mysql databases to your data lake. For more details, please refer to the original RFC.",
            "date_modified": "2022-01-14T00:00:00.000Z",
            "author": {
                "name": "Rajesh Mahindra"
            }
        },
        {
            "id": "/2022/01/06/apache-hudi-2021-a-year-in-review",
            "content_html": "<p>As the year came to end, I took some time to reflect on where we are and what we accomplished in 2021. I am humbled by how strong our community is and how regardless of it being another tough pandemic year, that people from around the globe leaned in together and made this the best year yet for Apache Hudi. In this blog I want to recap some of the 2021 highlights.</p><img src=\"/assets/images/Hudi_community.png\" alt=\"drawing\" width=\"600\"/><p><strong><em>Community</em></strong></p><p>I want to call out how amazing it is to see such a diverse group of people step up and contribute to this project. There were over 30,000 interactions with the <a href=\"https://github.com/apache/hudi/\">project on github</a>, up 2x from last year. Over the last year 300 people have contributed to the project, with over 3,000 PRs over 5 releases. We moved Apache Hudi from release 0.5.X all the way to our feature packed 0.10.0 release. Come and join us on our <a href=\"https://join.slack.com/t/apache-hudi/shared_invite/enQtODYyNDAxNzc5MTg2LTE5OTBlYmVhYjM0N2ZhOTJjOWM4YzBmMWU2MjZjMGE4NDc5ZDFiOGQ2N2VkYTVkNzU3ZDQ4OTI1NmFmYWQ0NzE\">active slack channel</a>! Over 850 community members engaged on our slack, up about 100% from the year before. I want to add a special shout out to our top slack participants who have helped answer so many questions and drive rich discussions on our channel. Sivabalan Narayanan, Nishith Agarwal, Bhavani Sudha Saktheeswaran, Vinay Patil, Rubens Soto, Dave Hagman, Raghav Tandon, Sagar Sumit, Joyan Sil, Jake D, Felix Jose, Nick Vintila, KimL, Andrew Sukhan, Danny Chan, Biswajit Mohapatra, and Pratyaksh Sharma! I know I am missing plenty of other important callouts, every PR that landed this year has helped shape Hudi into what it is today. Thank you!</p><img src=\"/assets/images/powers/logo-wall.png\" alt=\"drawing\" width=\"600\"/><p><strong><em>Impact</em></strong></p><p>In 2021, I personally developed a deeper gratitude and understanding of the magnitude of the impact we are making in the industry. Throughout the year I met more and more people that told me about how Hudi transformed their business and I was impressed by the large variety of use cases and applications that Hudi was able to serve. Some from the community who publicly shared their story include: <a href=\"https://aws.amazon.com/blogs/big-data/how-amazon-transportation-service-enabled-near-real-time-event-analytics-at-petabyte-scale-using-aws-glue-with-apache-hudi/\">Amazon</a>, <a href=\"https://aws.amazon.com/blogs/big-data/how-ge-aviation-built-cloud-native-data-pipelines-at-enterprise-scale-using-the-aws-platform/\">GE</a>, <a href=\"https://s.apache.org/hudi-robinhood-talk\">Robinhood</a>, <a href=\"http://hudi.apache.org/blog/2021/09/01/building-eb-level-data-lake-using-hudi-at-bytedance\">ByteDance</a>, <a href=\"https://blogs.halodoc.io/data-platform-2-0-part-1/\">Halodoc</a>, <a href=\"https://developpaper.com/baixin-banks-real-time-data-lake-evolution-scheme-based-on-apache-hudi/\">Baixin Bank</a>, <a href=\"https://developpaper.com/practice-of-apache-hudi-in-building-real-time-data-lake-at-station-b/\">BiliBili</a>, and so many more that haven’t even shared yet. One particular highlight from 2021 was attending <a href=\"https://youtu.be/lGm8qe4tBrg?t=2115\">AWS Re:Invent</a> and meeting an overwhelmingly large number of users who expressed joy with using Apache Hudi. This raises my sense of responsibility even more to be aware of just how many people depend on Apache Hudi.</p><p><strong><em>New Features</em></strong></p><p>Apache Hudi has come a long way in 2021 from v0.5.X to 0.10.0. Throughout this year we have developed innovative and leading edge features that make it easier and easier to build streaming data lakes. Some of these features include <a href=\"https://hudi.apache.org/docs/table_management\">Spark SQL DML Support</a>, <a href=\"https://hudi.apache.org/docs/clustering\">Clustering</a>, <a href=\"https://hudi.apache.org/blog/2021/12/29/hudi-zorder-and-hilbert-space-filling-curves\">Z-Order/Hilbert curves</a>, <a href=\"https://hudi.apache.org/docs/metadata\">Metadata Table file listing elimination</a>, <a href=\"https://hudi.apache.org/docs/markers\">Timeline Server Markers</a>, <a href=\"https://hudi.apache.org/docs/precommit_validator\">Precommit Validators</a>, <a href=\"https://hudi.apache.org/docs/writing_data#flink-sql-writer\">Flink MOR write/read</a>, <a href=\"https://hudi.apache.org/docs/concurrency_control\">Parallel Write support with OCC</a>, <a href=\"https://hudi.apache.org/docs/clustering\">Clustering</a>, <a href=\"https://hudi.apache.org/docs/querying_data#spark-incr-query\">Incremental Queries for MOR</a>, <a href=\"https://github.com/apache/hudi/tree/master/hudi-kafka-connect\">Kafka Connect Sink</a>, Delta Streamer sources for <a href=\"https://hudi.apache.org/docs/hoodie_deltastreamer/#s3-events\">S3</a> and <a href=\"https://hudi.apache.org/releases/release-0.10.0/#debezium-deltastreamer-sources\">Debezium</a>, <a href=\"https://hudi.apache.org/releases/release-0.10.0/#dbt-support\">DBT Support</a> all of which are were added in 2021. To top it all, we put together <a href=\"https://hudi.apache.org/blog/2021/07/21/streaming-data-lake-platform\">a manifesto</a> to realize our vision for streaming data lakes.</p><p><strong><em>The Road Ahead</em></strong></p><p>2021 may have been our best year so far, but it still feels like we are just getting started when we look at our new year&#x27;s resolutions for 2022. In the year ahead we have bold plans to realize the first cut of our entire vision and take Hudi 1.0, that includes full-featured multi-modal indexing for faster writes/queries, pathbreaking lock free concurrency, new server components for caching/metadata and finally Flink based incremental materialized views!  <em>You can find our</em> <a href=\"https://hudi.apache.org/roadmap\"><em>detailed roadmap here</em></a><em>.</em></p><p>I look forward to continued collaboration with the growing Hudi community! Come join our <a href=\"https://hudi.apache.org/community/syncs\"><em>community events</em></a> <em>and discussions in our</em> <a href=\"https://join.slack.com/t/apache-hudi/shared_invite/enQtODYyNDAxNzc5MTg2LTE5OTBlYmVhYjM0N2ZhOTJjOWM4YzBmMWU2MjZjMGE4NDc5ZDFiOGQ2N2VkYTVkNzU3ZDQ4OTI1NmFmYWQ0NzE\"><em>slack channel</em></a><em>! Happy new year 2022!</em></p>",
            "url": "https://hudi.apache.org/blog/2022/01/06/apache-hudi-2021-a-year-in-review",
            "title": "Apache Hudi - 2021 a Year in Review",
            "summary": "As the year came to end, I took some time to reflect on where we are and what we accomplished in 2021. I am humbled by how strong our community is and how regardless of it being another tough pandemic year, that people from around the globe leaned in together and made this the best year yet for Apache Hudi. In this blog I want to recap some of the 2021 highlights.",
            "date_modified": "2022-01-06T00:00:00.000Z",
            "author": {
                "name": "vinoth"
            }
        },
        {
            "id": "/2021/12/31/The-Art-of-Building-Open-Data-Lakes-with-Apache-Hudi-Kafka-Hive-and-Debezium",
            "content_html": "<div url=\"https://garystafford.medium.com/the-art-of-building-open-data-lakes-with-apache-hudi-kafka-hive-and-debezium-3d2f71c5981f\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2021/12/31/The-Art-of-Building-Open-Data-Lakes-with-Apache-Hudi-Kafka-Hive-and-Debezium",
            "title": "The Art of Building Open Data Lakes with Apache Hudi, Kafka, Hive, and Debezium",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2021-12-31T00:00:00.000Z",
            "author": {
                "name": "Gary Stafford"
            }
        },
        {
            "id": "/2021/12/29/hudi-zorder-and-hilbert-space-filling-curves",
            "content_html": "<p>As of Hudi v0.10.0, we are excited to introduce support for an advanced Data Layout Optimization technique known in the database realm as <a href=\"https://en.wikipedia.org/wiki/Z-order_curve\">Z-order</a> and <a href=\"https://en.wikipedia.org/wiki/Hilbert_curve\">Hilbert</a> space filling curves.</p><h3>Background</h3><p>Amazon EMR team recently published a <a href=\"https://aws.amazon.com/blogs/big-data/new-features-from-apache-hudi-0-7-0-and-0-8-0-available-on-amazon-emr/\">great article</a> show-casing how <a href=\"https://hudi.apache.org/docs/clustering\">clustering</a> your data can improve your <em>query performance</em>.</p><p>To better understand what&#x27;s going on and how it&#x27;s related to space-filling curves, let&#x27;s zoom in to the setup in that article:</p><p>In the article, 2 <a href=\"https://hudi.apache.org/docs/overview\">Apache Hudi</a> tables are compared (both ingested from the well-known <a href=\"https://s3.amazonaws.com/amazon-reviews-pds/tsv/index.txt\">Amazon Reviews</a> dataset):</p><ul><li><code>amazon_reviews</code> table which is not clustered (ie the data has not been re-ordered by any particular key)</li><li><code>amazon_reviews_clustered</code> table which is clustered. When data is clustered by Apache Hudi the data is <a href=\"https://en.wikipedia.org/wiki/Lexicographic_order\"><strong>lexicographically ordered</strong></a> (hereon we will be referring to this kind of ordering as <strong><em>linear ordering</em></strong>) by 2 columns: <code>star_rating</code>, <code>total_votes</code> (see screenshot below)</li></ul><img src=\"/assets/images/hudiconfigz.png\" alt=\"drawing\" width=\"800\"/><p><em>Screenshot of the Hudi configuration (from Amazon EMR team article)</em></p><p>To showcase the improvement in querying performance, the following queries are executed against both of these tables:</p><img src=\"/assets/images/table1.png\" alt=\"drawing\" width=\"800\"/><img src=\"/assets/images/table2.png\" alt=\"drawing\" width=\"800\"/><p><em>Screenshots of the queries run against the previously setup tables (from Amazon EMR team article)</em></p><p>The important consideration to point out here is that the queries were specifying <strong>both of the columns</strong> latter table is ordered by (both <code>star_rating</code> and <code>total_votes</code>).</p><p>And this is unfortunately a crucial limitation of the linear/lexicographic ordering, the value of the ordering diminishes very quickly as you add more columns. It&#x27;s not hard to see why:</p><img src=\"/assets/images/lexicographicorder.png\" alt=\"drawing\" width=\"250\"/><p><em>Courtesy of Wikipedia,</em> <a href=\"https://en.wikipedia.org/wiki/Lexicographic_order\"><em>Lexicographic Order article</em></a></p><p>From this image you can see that with lexicographically ordered 3-tuples of integers, only the first column is able to feature crucial property of <strong>locality</strong> for all of the records having the same value: for ex, all of the records wit values starting with &quot;1&quot;, &quot;2&quot;, &quot;3&quot; (in the first columns) are nicely clumped together. However if you try to find all the values that have &quot;5&quot; as the value in their third column you&#x27;d see that those are now dispersed all over the place, not being localized at all.</p><p>The crucial property that improves query performance is locality: it enables queries to substantially reduce the search space and the number of files that need to be scanned, parsed, etc.</p><p>But... does this mean that our queries are doomed to do a full-scan if we&#x27;re filtering by anything other than the first (or more accurate would be to say prefix) of the list of columns the table is ordered by?</p><p>Not exactly: luckily, locality is also a property that space-filling curves enable while enumerating multi-dimensional spaces (records in our table could be represented as points in N-dimensional space, where N is the number of columns in our table)</p><p>How does it work?</p><p>Let&#x27;s take Z-curve as an example: Z-order curves fitting a 2-dimensional plane would look like the following:</p><img src=\"/assets/images/zordercurve.png\" alt=\"drawing\" width=\"400\"/><p><em>Courtesy of Wikipedia,</em> <a href=\"https://en.wikipedia.org/wiki/Z-order_curve\"><em>Z-order curve article</em></a></p><p>As you can see following its path, instead of simply ordering by one coordinate (&quot;x&quot;) first, following with the other, it&#x27;s actually ordering them as if the bits of those coordinates have been <em>interwoven</em> into a single value:</p><table><thead><tr><th>Coordinate</th><th>X (binary)</th><th>Y (binary)</th><th>Z-values (ordered)</th></tr></thead><tbody><tr><td>(0, 0)</td><td>000</td><td>000</td><td>000000</td></tr><tr><td>(1, 0)</td><td>001</td><td>000</td><td>000001</td></tr><tr><td>(0, 1)</td><td>000</td><td>001</td><td>000010</td></tr><tr><td>(1, 1)</td><td>001</td><td>001</td><td>000011</td></tr></tbody></table><p>This allows for that crucial property of locality (even though a slightly &quot;stretched&quot; one) to be carried over to all columns as compared to just the first one in case of linear ordering.</p><p>In a similar fashion, Hilbert curves also allow you to map points in a N-dimensional space (rows in our table) onto 1-dimensional curve, essentially <em>ordering</em> them, while still preserving the crucial property of locality. Read more details about Hilbert Curves <a href=\"https://drum.lib.umd.edu/handle/1903/804\">here</a>. Our personal experiments so far show that ordering data along a Hilbert curve leads to better clustering and performance outcomes.</p><p>Now, let&#x27;s check it out in action!</p><h3>Setup</h3><p>We will use the <a href=\"https://s3.amazonaws.com/amazon-reviews-pds/readme.html\">Amazon Reviews</a> dataset again, but this time we will use Hudi to Z-Order by <code>product_id</code>, <code>customer_id</code> columns tuple instead of Clustering or <em>linear ordering</em>.</p><p>No special preparations are required for the dataset, you can simply download it from <a href=\"https://s3.amazonaws.com/amazon-reviews-pds/readme.html\">S3</a> in Parquet format and use it directly as an input for Spark ingesting it into Hudi table.</p><p>Launch Spark Shell</p><pre><code>./bin/spark-shell --master &#x27;local[4]&#x27; --driver-memory 8G --executor-memory 8G \\\n  --jars ../../packaging/hudi-spark-bundle/target/hudi-spark3-bundle_2.12-0.10.0.jar \\\n  --packages org.apache.spark:spark-avro_2.12:2.4.4 \\\n  --conf &#x27;spark.serializer=org.apache.spark.serializer.KryoSerializer&#x27;\n</code></pre><p>Paste</p><pre><code class=\"language-scala\">import org.apache.hadoop.fs.{FileStatus, Path}\nimport scala.collection.JavaConversions._\nimport org.apache.spark.sql.SaveMode._\nimport org.apache.hudi.{DataSourceReadOptions, DataSourceWriteOptions}\nimport org.apache.hudi.DataSourceWriteOptions._\nimport org.apache.hudi.common.fs.FSUtils\nimport org.apache.hudi.common.table.HoodieTableMetaClient\nimport org.apache.hudi.common.util.ClusteringUtils\nimport org.apache.hudi.config.HoodieClusteringConfig\nimport org.apache.hudi.config.HoodieWriteConfig._\nimport org.apache.spark.sql.DataFrame\n\nimport java.util.stream.Collectors\n\nval layoutOptStrategy = &quot;z-order&quot;; // OR &quot;hilbert&quot;\n\nval inputPath = s&quot;file:///${System.getProperty(&quot;user.home&quot;)}/datasets/amazon_reviews_parquet&quot;\nval tableName = s&quot;amazon_reviews_${layoutOptStrategy}&quot;\nval outputPath = s&quot;file:///tmp/hudi/$tableName&quot;\n\n\ndef safeTableName(s: String) = s.replace(&#x27;-&#x27;, &#x27;_&#x27;)\n\nval commonOpts =\n  Map(\n    &quot;hoodie.compact.inline&quot; -&gt; &quot;false&quot;,\n    &quot;hoodie.bulk_insert.shuffle.parallelism&quot; -&gt; &quot;10&quot;\n  )\n\n\n////////////////////////////////////////////////////////////////\n// Writing to Hudi\n////////////////////////////////////////////////////////////////\n\nval df = spark.read.parquet(inputPath)\n\ndf.write.format(&quot;hudi&quot;)\n  .option(DataSourceWriteOptions.TABLE_TYPE.key(), COW_TABLE_TYPE_OPT_VAL)\n  .option(&quot;hoodie.table.name&quot;, tableName)\n  .option(PRECOMBINE_FIELD.key(), &quot;review_id&quot;)\n  .option(RECORDKEY_FIELD.key(), &quot;review_id&quot;)\n  .option(DataSourceWriteOptions.PARTITIONPATH_FIELD.key(), &quot;product_category&quot;)\n  .option(&quot;hoodie.clustering.inline&quot;, &quot;true&quot;)\n  .option(&quot;hoodie.clustering.inline.max.commits&quot;, &quot;1&quot;)\n  // NOTE: Small file limit is intentionally kept _ABOVE_ target file-size max threshold for Clustering,\n  // to force re-clustering\n  .option(&quot;hoodie.clustering.plan.strategy.small.file.limit&quot;, String.valueOf(1024 * 1024 * 1024)) // 1Gb\n  .option(&quot;hoodie.clustering.plan.strategy.target.file.max.bytes&quot;, String.valueOf(128 * 1024 * 1024)) // 128Mb\n  // NOTE: We&#x27;re increasing cap on number of file-groups produced as part of the Clustering run to be able to accommodate for the \n  // whole dataset (~33Gb)\n  .option(&quot;hoodie.clustering.plan.strategy.max.num.groups&quot;, String.valueOf(4096))\n  .option(HoodieClusteringConfig.LAYOUT_OPTIMIZE_ENABLE.key, &quot;true&quot;)\n  .option(HoodieClusteringConfig.LAYOUT_OPTIMIZE_STRATEGY.key, layoutOptStrategy)\n  .option(HoodieClusteringConfig.PLAN_STRATEGY_SORT_COLUMNS.key, &quot;product_id,customer_id&quot;)\n  .option(DataSourceWriteOptions.OPERATION.key(), DataSourceWriteOptions.BULK_INSERT_OPERATION_OPT_VAL)\n  .option(BULK_INSERT_SORT_MODE.key(), &quot;NONE&quot;)\n  .options(commonOpts)\n  .mode(ErrorIfExists)\n  \n</code></pre><h3>Testing</h3><p>Please keep in mind, that each individual test is run in a separate spark-shell to avoid caching getting in the way of our measurements.</p><pre><code class=\"language-scala\">////////////////////////////////////////////////////////////////\n// Reading\n///////////////////////////////////////////////////////////////\n\n// Temp Table w/ Data Skipping DISABLED\nval readDf: DataFrame =\n  spark.read.option(DataSourceReadOptions.ENABLE_DATA_SKIPPING.key(), &quot;false&quot;).format(&quot;hudi&quot;).load(outputPath)\n\nval rawSnapshotTableName = safeTableName(s&quot;${tableName}_sql_snapshot&quot;)\n\nreadDf.createOrReplaceTempView(rawSnapshotTableName)\n\n\n// Temp Table w/ Data Skipping ENABLED\nval readDfSkip: DataFrame =\n  spark.read.option(DataSourceReadOptions.ENABLE_DATA_SKIPPING.key(), &quot;true&quot;).format(&quot;hudi&quot;).load(outputPath)\n\nval dataSkippingSnapshotTableName = safeTableName(s&quot;${tableName}_sql_snapshot_skipping&quot;)\n\nreadDfSkip.createOrReplaceTempView(dataSkippingSnapshotTableName)\n\n// Query 1: Total votes by product_category, for 6 months\ndef runQuery1(tableName: String) = {\n  // Query 1: Total votes by product_category, for 6 months\n  spark.sql(s&quot;SELECT sum(total_votes), product_category FROM $tableName WHERE review_date &gt; &#x27;2013-12-15&#x27; AND review_date &lt; &#x27;2014-06-01&#x27; GROUP BY product_category&quot;).show()\n}\n\n// Query 2: Average star rating by product_id, for some product\ndef runQuery2(tableName: String) = {\n  spark.sql(s&quot;SELECT avg(star_rating), product_id FROM $tableName WHERE product_id in (&#x27;B0184XC75U&#x27;) GROUP BY product_id&quot;).show()\n}\n\n// Query 3: Count number of reviews by customer_id for some 5 customers\ndef runQuery3(tableName: String) = {\n  spark.sql(s&quot;SELECT count(*) as num_reviews, customer_id FROM $tableName WHERE customer_id in (&#x27;53096570&#x27;,&#x27;10046284&#x27;,&#x27;53096576&#x27;,&#x27;10000196&#x27;,&#x27;21700145&#x27;) GROUP BY customer_id&quot;).show()\n}\n\n//\n// Query 1: Is a &quot;wide&quot; query and hence it&#x27;s expected to touch a lot of files\n//\nscala&gt; runQuery1(rawSnapshotTableName)\n+----------------+--------------------+\n|sum(total_votes)|    product_category|\n+----------------+--------------------+\n|         1050944|                  PC|\n|          867794|             Kitchen|\n|         1167489|                Home|\n|          927531|            Wireless|\n|            6861|               Video|\n|           39602| Digital_Video_Games|\n|          954924|Digital_Video_Dow...|\n|           81876|             Luggage|\n|          320536|         Video_Games|\n|          817679|              Sports|\n|           11451|  Mobile_Electronics|\n|          228739|  Home_Entertainment|\n|         3769269|Digital_Ebook_Pur...|\n|          252273|                Baby|\n|          735042|             Apparel|\n|           49101|    Major_Appliances|\n|          484732|             Grocery|\n|          285682|               Tools|\n|          459980|         Electronics|\n|          454258|            Outdoors|\n+----------------+--------------------+\nonly showing top 20 rows\n\nscala&gt; runQuery1(dataSkippingSnapshotTableName)\n+----------------+--------------------+\n|sum(total_votes)|    product_category|\n+----------------+--------------------+\n|         1050944|                  PC|\n|          867794|             Kitchen|\n|         1167489|                Home|\n|          927531|            Wireless|\n|            6861|               Video|\n|           39602| Digital_Video_Games|\n|          954924|Digital_Video_Dow...|\n|           81876|             Luggage|\n|          320536|         Video_Games|\n|          817679|              Sports|\n|           11451|  Mobile_Electronics|\n|          228739|  Home_Entertainment|\n|         3769269|Digital_Ebook_Pur...|\n|          252273|                Baby|\n|          735042|             Apparel|\n|           49101|    Major_Appliances|\n|          484732|             Grocery|\n|          285682|               Tools|\n|          459980|         Electronics|\n|          454258|            Outdoors|\n+----------------+--------------------+\nonly showing top 20 rows\n\n//\n// Query 2: Is a &quot;pointwise&quot; query and hence it&#x27;s expected that data-skipping should substantially reduce number \n// of files scanned (as compared to Baseline)\n//\n// NOTE: That Linear Ordering (as compared to Space-curve based on) will have similar effect on performance reducing\n// total # of Parquet files scanned, since we&#x27;re querying on the prefix of the ordering key\n//\nscala&gt; runQuery2(rawSnapshotTableName)\n+----------------+----------+\n|avg(star_rating)|product_id|\n+----------------+----------+\n|             1.0|B0184XC75U|\n+----------------+----------+\n\n\nscala&gt; runQuery2(dataSkippingSnapshotTableName)\n+----------------+----------+\n|avg(star_rating)|product_id|\n+----------------+----------+\n|             1.0|B0184XC75U|\n+----------------+----------+\n\n//\n// Query 3: Similar to Q2, is a &quot;pointwise&quot; query, but querying other part of the ordering-key (product_id, customer_id)\n// and hence it&#x27;s expected that data-skipping should substantially reduce number of files scanned (as compared to Baseline, Linear Ordering).\n//\n// NOTE: That Linear Ordering (as compared to Space-curve based on) will _NOT_ have similar effect on performance reducing\n// total # of Parquet files scanned, since we&#x27;re NOT querying on the prefix of the ordering key\n//\nscala&gt; runQuery3(rawSnapshotTableName)\n+-----------+-----------+\n|num_reviews|customer_id|\n+-----------+-----------+\n|         50|   53096570|\n|          3|   53096576|\n|         25|   10046284|\n|          1|   10000196|\n|         14|   21700145|\n+-----------+-----------+\n\nscala&gt; runQuery3(dataSkippingSnapshotTableName)\n+-----------+-----------+\n|num_reviews|customer_id|\n+-----------+-----------+\n|         50|   53096570|\n|          3|   53096576|\n|         25|   10046284|\n|          1|   10000196|\n|         14|   21700145|\n+-----------+-----------+\n</code></pre><h3>Results</h3><p>We&#x27;ve summarized the measured performance metrics below:</p><table><thead><tr><th><strong>Query</strong></th><th><strong>Baseline (B)</strong> duration (files scanned / size)</th><th><strong>Linear Sorting (S)</strong></th><th><strong>Z-order (Z)</strong> duration (scanned)</th><th><strong>Hilbert (H)</strong> duration (scanned)</th></tr></thead><tbody><tr><td>Q1</td><td>14s (543 / 31.4Gb)</td><td>15s (533 / 28.8Gb)</td><td>15s (543 / 31.4Gb)</td><td>14s (541 / 31.3Gb)</td></tr><tr><td>Q2</td><td>21s (543 / 31.4Gb)</td><td>10s (533 / 28.8Gb)</td><td><strong>8s</strong> <strong>(243 / 14.4Gb)</strong></td><td><strong>7s</strong> <strong>(237 / 13.9Gb)</strong></td></tr><tr><td>Q3</td><td>17s (543 / 31.4Gb)</td><td>15s (533 / 28.8Gb)</td><td><strong>6s</strong> <strong>(224 / 12.4Gb)</strong></td><td><strong>6s</strong> <strong>(219 / 11.9Gb)</strong></td></tr></tbody></table><p>As you can see multi-column linear ordering is not very effective for the queries that do filtering by columns other than the first one (Q2, Q3).</p><p>Which is a very clear contrast with space-filling curves (both Z-order and Hilbert) that allow to speed up query time by up to <strong>3x!</strong></p><p>It&#x27;s worth noting that the performance gains are heavily dependent on your underlying data and queries. In benchmarks on our internal data we were able to achieve queries performance improvements of more than <strong>11x!</strong></p><h3>Epilogue</h3><p>Apache Hudi v0.10 brings new layout optimization capabilities Z-order and Hilbert to open source. Using these industry leading layout optimization techniques can bring substantial performance improvement and cost savings to your queries!</p>",
            "url": "https://hudi.apache.org/blog/2021/12/29/hudi-zorder-and-hilbert-space-filling-curves",
            "title": "Hudi Z-Order and Hilbert Space Filling Curves",
            "summary": "As of Hudi v0.10.0, we are excited to introduce support for an advanced Data Layout Optimization technique known in the database realm as Z-order and Hilbert space filling curves.",
            "date_modified": "2021-12-29T00:00:00.000Z",
            "author": {
                "name": "Alexey Kudinkin and Tao Meng"
            }
        },
        {
            "id": "/2021/12/20/New-features-from-Apache-Hudi-0.7.0-and-0.8.0-available-on-Amazon-EMR",
            "content_html": "<div url=\"https://aws.amazon.com/blogs/big-data/new-features-from-apache-hudi-0-7-0-and-0-8-0-available-on-amazon-emr/\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2021/12/20/New-features-from-Apache-Hudi-0.7.0-and-0.8.0-available-on-Amazon-EMR",
            "title": "New features from Apache Hudi 0.7.0 and 0.8.0 available on Amazon EMR",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2021-12-20T00:00:00.000Z",
            "author": {
                "name": "Udit Mehrotra"
            }
        },
        {
            "id": "/2021/12/16/lakehouse-concurrency-control-are-we-too-optimistic",
            "content_html": "<p>Transactions on data lakes are now considered a key characteristic of a Lakehouse these days. But what has actually been accomplished so far? What are the current approaches? How do they fare in real-world scenarios? These questions are the focus of this blog. </p><p>Having had the good fortune of working on diverse database projects - an RDBMS (<a href=\"https://www.oracle.com/database/\">Oracle</a>), a NoSQL key-value store (<a href=\"https://www.slideshare.net/vinothchandar/voldemort-prototype-to-production-nectar-edits\">Voldemort</a>), a streaming database (<a href=\"https://www.confluent.io/blog/ksqldb-pull-queries-high-availability/\">ksqlDB</a>), a closed-source real-time datastore and of course, Apache Hudi, I can safely say that the nature of workloads deeply influence the concurrency control mechanisms adopted in different databases. This blog will also describe how we rethought concurrency control for the data lake in Apache Hudi.</p><p>First, let&#x27;s set the record straight. RDBMS databases offer the richest set of transactional capabilities and the widest array of concurrency control <a href=\"https://dev.mysql.com/doc/refman/5.7/en/innodb-locking-transaction-model.html\">mechanisms</a>. Different isolation levels, fine grained locking, deadlock detection/avoidance, and more are possible because they have to support row-level mutations and reads across many tables while enforcing <a href=\"https://dev.mysql.com/doc/refman/8.0/en/create-table-foreign-keys.html\">key constraints</a> and maintaining <a href=\"https://dev.mysql.com/doc/refman/8.0/en/create-table-secondary-indexes.html\">indexes</a>. NoSQL stores offer dramatically weaker guarantees like eventual-consistency and simple row level atomicity in exchange for greater scalability for simpler workloads. Drawing a similar parallel, traditional data warehouses offer more or less the full set of capabilities that you would find in an RDBMS, over columnar data, with locking and key constraints <a href=\"https://docs.teradata.com/r/a8IdS6iVHR77Z9RrIkmMGg/wFPZS4jwZgSG21GnOIpEsw\">enforced</a> whereas cloud data warehouses seem to have focused a lot more on separating the data and compute in architecture, while offering fewer isolation levels. As a surprising example, <a href=\"https://docs.snowflake.com/en/sql-reference/constraints-overview.html#supported-constraint-types\">no enforcement</a> of key constraints!</p><h3>Pitfalls in Lake Concurrency Control</h3><p>Historically, data lakes have been viewed as batch jobs reading/writing files on cloud storage and it&#x27;s interesting to see how most new work extends this view and implements glorified file version control using some form of &quot;<a href=\"https://en.wikipedia.org/wiki/Optimistic_concurrency_control\"><strong>Optimistic concurrency control</strong></a>&quot; (OCC). With OCC jobs take a table level lock to check if they have impacted overlapping files and if a conflict exists, they abort their operations completely. Without naming names, the lock is sometimes even just a JVM level lock held on a single Apache Spark driver node. Once again, this may be okay for lightweight coordination of old school batch jobs that mostly append files to tables, but cannot be applied broadly to modern data lake workloads. Such approaches are built with immutable/append-only data models in mind, which are inadequate for incremental data processing or keyed updates/deletes. OCC is very optimistic that real contention never happens. Developer evangelism comparing OCC to the full fledged transactional capabilities of an RDBMS or a traditional data warehouse is rather misinformed. Quoting Wikipedia directly - &quot;<em>if contention for data resources is frequent, the cost of repeatedly restarting transactions hurts performance significantly, in which case other</em> <a href=\"https://en.wikipedia.org/wiki/Concurrency_control\"><em>concurrency control</em></a> <em>methods may be better suited.</em> &quot; When conflicts do occur, they can cause massive resource wastage since you have a batch job that fails after it ran for a few hours, during every attempt!</p><p>Imagine a real-life scenario of two writer processes : an ingest writer job producing new data every 30 minutes and a deletion writer job that is enforcing GDPR, taking 2 hours to issue deletes. It&#x27;s very likely for these to overlap files with random deletes, and the deletion job is almost guaranteed to starve and fail to commit each time. In database speak, mixing long running transactions with optimism leads to disappointment, since the longer the transactions the higher the probability they will overlap.</p><p><img src=\"/assets/images/blog/concurrency/ConcurrencyControlConflicts.png\" alt=\"concurrency\"/></p><p>So, what&#x27;s the alternative? Locking? Wikipedia also says - &quot;<em>However, locking-based (&quot;pessimistic&quot;) methods also can deliver poor performance because locking can drastically limit effective concurrency even when deadlocks are avoided.&quot;.</em> Here is where Hudi takes a different approach, that we believe is more apt for modern lake transactions which are typically long-running and even continuous. Data lake workloads share more characteristics with high throughput stream processing jobs, than they do to standard reads/writes from a database and this is where we borrow from. In stream processing events are serialized into a single ordered log, avoiding any locks/concurrency bottlenecks and you can continuously process millions of events/sec. Hudi implements a file level, log based concurrency control protocol on the Hudi <a href=\"https://hudi.apache.org/docs/timeline\">timeline</a>, which in-turn relies on bare minimum atomic puts to cloud storage. By building on an event log as the central piece for inter process coordination, Hudi is able to offer a few flexible deployment models that offer greater concurrency over pure OCC approaches that just track table snapshots.</p><h3>Model 1 : Single Writer, Inline Table Services</h3><p>The simplest form of concurrency control is just no concurrency at all. A data lake table often has common services operating on it to ensure efficiency. Reclaiming storage space from older versions and logs, coalescing files (clustering in Hudi), merging deltas (compactions in Hudi), and more. Hudi can simply eliminate the need for concurrency control and maximizes throughput by supporting these table services out-of-box and running inline after every write to the table.</p><p>Execution plans are idempotent, persisted to the timeline and auto-recover from failures. For most simple use-cases, this means just writing is sufficient to get a well-managed table that needs no concurrency control.</p><p><img src=\"/assets/images/blog/concurrency/SingleWriterInline.gif\" alt=\"concurrency-single-writer\"/></p><h3>Model 2 : Single Writer, Async Table Services</h3><p>Our delete/ingest example above is n&#x27;t really that simple. While ingest/writer may just be updating the last N partitions on the table, delete may span across the entire table even. Mixing them in the same job, could slow down ingest latency by a lot. But, Hudi provides the option of running the table services in an async fashion, where most of the heavy lifting (e.g actually rewriting the columnar data by compaction service) is done asynchronously, eliminating any repeated wasteful retries whatsoever, while also optimizing the table using clustering techniques. Thus a single writer could consumes both regular updates and GDPR deletes and serialize them into a log. Given Hudi has record level indexing and avro log writes are much cheaper (as opposed to writing parquet, which can be 10x or more expensive), ingest latency can be sustained while enjoying great replayability. In fact, we were able to scale this model at <a href=\"https://eng.uber.com/uber-big-data-platform/\">Uber</a>, across 100s of petabytes, by sequencing all deletes &amp; updates into the same source Apache Kafka topic. There&#x27;s more to concurrency control than locking and Hudi accomplishes all this without needing any external locking.</p><p><img src=\"/assets/images/blog/concurrency/SingleWriterAsync.gif\" alt=\"concurrency-async\"/></p><h3>Model 3 : Multiple Writers</h3><p>But it&#x27;s not always possible to serialize the deletes into the same write stream or sql based deletes are required. With multiple distributed processes, some form of locking is inevitable, but like real databases Hudi&#x27;s concurrency model is intelligent enough to differentiate actual writing to the table, from table services that manage or optimize the table. Hudi offers similar optimistic concurrency control across multiple writers, but table services can still execute completely lock-free and async. This means the delete job can merely encode deletes and the ingest job can log updates, while the compaction service again applies the updates/deletes to base files. Even though the delete job and ingest job can contend and starve each other like like we mentioned above, their run-times are much lower and the wastage is drastically lower, since the compaction does the heavy-lifting of parquet/columnar data writing.</p><p><img src=\"/assets/images/blog/concurrency/MultiWriter.gif\" alt=\"concurrency-multi\"/></p><p>All this said, there are still many ways we can improve upon this foundation.</p><ul><li>For starters, Hudi has already implemented a <a href=\"https://hudi.apache.org/blog/2021/08/18/improving-marker-mechanism/\">marker mechanism</a> that tracks all the files that are part of an active write transaction and a heartbeat mechanism that can track active writers to a table. This can be directly used by other active transactions/writers to detect what other writers are doing and <a href=\"https://issues.apache.org/jira/browse/HUDI-1575\">abort early</a> if conflicts are detected, yielding the cluster resources back to other jobs sooner.</li><li>While optimistic concurrency control is attractive when serializable snapshot isolation is desired, it&#x27;s neither optimal nor the only method for dealing with concurrency between writers. We plan to implement a fully lock-free concurrency control using CRDTs and widely adopted stream processing concepts, over our log <a href=\"https://github.com/apache/hudi/blob/bc8bf043d5512f7afbb9d94882c4e43ee61d6f06/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordPayload.java#L38\">merge API</a>, that has already been <a href=\"https://hudi.apache.org/blog/2021/09/01/building-eb-level-data-lake-using-hudi-at-bytedance/#functionality-support\">proven</a> to sustain enormous continuous write volumes for the data lake.</li><li>Touching upon key constraints, Hudi is the only lake transactional layer that ensures unique <a href=\"https://hudi.apache.org/docs/key_generation\">key</a> constraints today, but limited to the record key of the table. We will be looking to expand this capability in a more general form to non-primary key fields, with the said newer concurrency models.</li></ul><p>Finally, for data lakes to transform successfully into lakehouses, we must learn from the failing of the &quot;hadoop warehouse&quot; vision, which shared similar goals with the new &quot;lakehouse&quot; vision. Designers did not pay closer attention to the missing technology gaps against warehouses and created unrealistic expectations from the actual software. As transactions and database functionality finally goes mainstream on data lakes, we must apply these lessons and remain candid about the current shortcomings. If you are building a lakehouse, I hope this post encourages you to closely consider various operational and efficiency aspects around concurrency control. Join our fast growing community by trying out <a href=\"https://hudi.apache.org/docs/overview\">Apache Hudi</a> or join us in conversations on <a href=\"https://join.slack.com/t/apache-hudi/shared_invite/enQtODYyNDAxNzc5MTg2LTE5OTBlYmVhYjM0N2ZhOTJjOWM4YzBmMWU2MjZjMGE4NDc5ZDFiOGQ2N2VkYTVkNzU3ZDQ4OTI1NmFmYWQ0NzE\">Slack</a>.</p>",
            "url": "https://hudi.apache.org/blog/2021/12/16/lakehouse-concurrency-control-are-we-too-optimistic",
            "title": "Lakehouse Concurrency Control: Are we too optimistic?",
            "summary": "Transactions on data lakes are now considered a key characteristic of a Lakehouse these days. But what has actually been accomplished so far? What are the current approaches? How do they fare in real-world scenarios? These questions are the focus of this blog.",
            "date_modified": "2021-12-16T00:00:00.000Z",
            "author": {
                "name": "vinoth"
            }
        },
        {
            "id": "/2021/11/22/Apache-Hudi-Architecture-Tools-and-Best-Practices",
            "content_html": "<div url=\"https://www.xenonstack.com/insights/what-is-hudi\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2021/11/22/Apache-Hudi-Architecture-Tools-and-Best-Practices",
            "title": "Apache Hudi Architecture Tools and Best Practices",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2021-11-22T00:00:00.000Z",
            "author": {
                "name": "Chandan Gaur"
            }
        },
        {
            "id": "/2021/11/16/How-GE-Aviation-built-cloud-native-data-pipelines-at-enterprise-scale-using-the-AWS-platform",
            "content_html": "<div url=\"https://aws.amazon.com/blogs/big-data/how-ge-aviation-built-cloud-native-data-pipelines-at-enterprise-scale-using-the-aws-platform/\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2021/11/16/How-GE-Aviation-built-cloud-native-data-pipelines-at-enterprise-scale-using-the-AWS-platform",
            "title": "How GE Aviation built cloud-native data pipelines at enterprise scale using the AWS platform",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2021-11-16T00:00:00.000Z",
            "author": {
                "name": "Alcuin Weidus"
            }
        },
        {
            "id": "/2021/10/21/Practice-of-Apache-Hudi-in-building-real-time-data-lake-at-station-B",
            "content_html": "<div url=\"https://developpaper.com/practice-of-apache-hudi-in-building-real-time-data-lake-at-station-b/\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2021/10/21/Practice-of-Apache-Hudi-in-building-real-time-data-lake-at-station-B",
            "title": "Practice of Apache Hudi in building real-time data lake at station B",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2021-10-21T00:00:00.000Z",
            "author": {
                "name": "Yu Zhaojing"
            }
        },
        {
            "id": "/2021/10/14/How-Amazon-Transportation-Service-enabled-near-real-time-event-analytics-at-petabyte-scale-using-AWS-Glue-with-Apache-Hudi",
            "content_html": "<div url=\"https://aws.amazon.com/blogs/big-data/how-amazon-transportation-service-enabled-near-real-time-event-analytics-at-petabyte-scale-using-aws-glue-with-apache-hudi/\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2021/10/14/How-Amazon-Transportation-Service-enabled-near-real-time-event-analytics-at-petabyte-scale-using-AWS-Glue-with-Apache-Hudi",
            "title": "How Amazon Transportation Service enabled near-real-time event analytics at petabyte scale using AWS Glue with Apache Hudi",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2021-10-14T00:00:00.000Z",
            "author": {
                "name": "Madhavan Sriram"
            }
        },
        {
            "id": "/2021/10/05/Data-Platform-2.0-Part-I",
            "content_html": "<div url=\"https://blogs.halodoc.io/data-platform-2-0-part-1/\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2021/10/05/Data-Platform-2.0-Part-I",
            "title": "Data Platform 2.0 - Part I",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2021-10-05T00:00:00.000Z",
            "author": {
                "name": "Jitendra Shah"
            }
        },
        {
            "id": "/2021/09/01/building-eb-level-data-lake-using-hudi-at-bytedance",
            "content_html": "<p>Ziyue Guan from Bytedance shares the experience of building an ExaByte(EB)-level data lake using Apache Hudi at Bytedance.</p><p>This blog is a translated version of <a href=\"https://mp.weixin.qq.com/s/oZz_2HzPCWgzxwZO0nuDUQ\">the same blog originally in Chinese/中文</a>.  Here are the <a href=\"/assets/images/blog/datalake-bytedance-hudi/bytedance-hudi-slides-chinese.pdf\">original slides in Chinese/中文</a> and <a href=\"/assets/images/blog/datalake-bytedance-hudi/bytedance-hudi-slides-english.pdf\">the translated slides in English</a>.</p><p><img src=\"/assets/images/blog/datalake-bytedance-hudi/slide1.png\" alt=\"slide1 title\"/></p><p><img src=\"/assets/images/blog/datalake-bytedance-hudi/slide2.png\" alt=\"slide2 agenda\"/></p><p>Next, I will explain how we use Hudi in Bytedance’s recommendation system in five parts: scenario requirements, design decisions, functionality support, performance tuning, and future work.</p><h2>Scenario Requirements</h2><p><img src=\"/assets/images/blog/datalake-bytedance-hudi/slide3.png\" alt=\"slide3 scenario requirements\"/>\n<img src=\"/assets/images/blog/datalake-bytedance-hudi/slide4.png\" alt=\"slide4 scenario diagram\"/>\n<img src=\"/assets/images/blog/datalake-bytedance-hudi/slide5.png\" alt=\"slide5 scenario details\"/></p><p>In the recommendation system, we use the data lake in the following two scenarios:</p><ol><li><p>We use BigTable as the data storage for the near real-time processing in the entire system. There is an internally developed component TBase, which provides the semantics of BigTable and the abstraction of some requirements in the search advertisement recommendation scenarios, and shields the differences in underlying storage. For a better understanding, it can be directly regarded as an HBase. In this process, in order to serve offline data analysis and mining needs, the data needs to be exported to offline storage. In the past, users either use MR/Spark to directly access the storage, or obtain data by scanning the database, which do not meet the data access characteristics in the OLAP scenario. Therefore, we build BigTable&#x27;s CDC based on the data lake to improve data timeliness, reduce the access pressure of the near real-time system, and provide efficient OLAP access and user-friendly SQL consumption methods.</p></li><li><p>In addition, we also use data lakes in the scenarios of feature engineering and model training. We obtain two types of real-time data streams from internal and external sources. One is the instances returned from the system, which includes the features obtained when the recommendation system is serving. The other is the feedback from event tracking at vantage points and a variety of complex external data sources. This type of data is used as labels and forms a complete machine learning data sample with the previously mentioned features. For this scenario, we need to implement a merging operation based on the primary key to merge the instance and label together.  The time window range may be as long as tens of days, with the volume at the order of hundreds of billions of rows. The system needs to support efficient column selection and predicate pushdown. At the same time, it also needs to support concurrent updates and other related capabilities.</p></li></ol><p>These two scenarios pose the following challenges:</p><ol><li><p><strong>The data is very irregular.</strong> Compared with Binlog, WAL cannot obtain all the information of a row, and the data size changes significantly.</p></li><li><p><strong>The throughput is relatively large.</strong>  The throughput of a single table exceeds <strong>100 GB/s</strong>, and the single table needs <strong>PB-level</strong> storage.</p></li><li><p><strong>The data schema is complex.</strong> The data is highly dimensional and sparse.  The number of table columns ranges from 1000 to 10000+. And there are a lot of complex data types.</p></li></ol><h2>Design Decisions</h2><p><img src=\"/assets/images/blog/datalake-bytedance-hudi/slide6.png\" alt=\"slide6 design decisions\"/>\n<img src=\"/assets/images/blog/datalake-bytedance-hudi/slide7.png\" alt=\"slide7 design details\"/></p><p>When making the decision on the engine, we examine three of the most popular data lake engines, <strong>Hudi</strong>, <strong>Iceberg</strong>, and <strong>DeltaLake</strong>. These three have their own advantages and disadvantages in our scenarios. Finally, <strong>Hudi</strong> is selected as the storage engine based on Hudi&#x27;s openness to the upstream and downstream ecosystems, support for the global index, and customized development interfaces for certain storage logic.</p><ul><li><p>For real-time writing, MOR with better timeliness is selected.</p></li><li><p>We examine the index type.  First of all, because WAL can&#x27;t get the partition of the data each time, it must use a global index. Among several global index implementations, in order to achieve high-performance writing, HBase is the only choice. The other two implementations have major performance gaps from HBase.</p></li><li><p>Regarding the computing engine and API, Hudi&#x27;s support for Flink was not perfect at the time, so we choose Spark which has more mature support. In order to flexibly implement some customized functionality and logic, and because the DataFrame API has more semantic restrictions, we choose the lower-level RDD API.</p></li></ul><h2>Functionality Support</h2><p><img src=\"/assets/images/blog/datalake-bytedance-hudi/slide8.png\" alt=\"slide8 functionality support\"/></p><p>Functionality support includes MVCC and Schema registration systems that store semantics.</p><p><img src=\"/assets/images/blog/datalake-bytedance-hudi/slide9.png\" alt=\"slide9 mvcc\"/></p><p>First of all, in order to support WAL write, we implement the payload for MVCC, and based on Avro, we customized a set of data structure implementation with timestamp. This logic is hidden from users through view access. In addition, we also implement the HBase append semantics, which realizes the appending to the List type instead of overwriting.</p><p><img src=\"/assets/images/blog/datalake-bytedance-hudi/slide10.png\" alt=\"slide10 schema\"/></p><p>Because Hudi obtains the schema from write data, it is not convenient for working with other systems.  We also need some extensions based on the schema, so we build a metadata center to provide metadata-related operations.</p><ul><li><p>First of all, we realized atomic changes and multi-site high availability based on the semantics provided by internal storage. Users can atomically trigger schema changes through the interface and get the results immediately.</p></li><li><p>Achieves versioning of the Schema by adding the version number. After having the version number, we can easily use the schema instead of passing JSON object back and forth. With multiple versions, schema evolution can also be flexibly achieved.</p></li><li><p>We also support additional information encoding at the column level to help the business achieve special extended functionality in some scenarios. We replace column names with IDs to save the cost in the storage process.</p></li><li><p>When the Spark job with Hudi is running, it builds a local cache at the JVM level and syncs the data with the metadata center through the pull method, to achieve rapid access to the schema and singleton instance of the in-process schema.</p></li></ul><h2>Performance Tuning</h2><p><img src=\"/assets/images/blog/datalake-bytedance-hudi/slide11.png\" alt=\"slide11 performance tuning\"/></p><p>In our scenario, the performance challenges are huge. <strong>The maximum data volume of a single table reaches 400PB+, the daily volume increase is PB level, and the total data volume reaches EB level.</strong> Therefore, we have done some work to improve performance based on the performance and data characteristics.</p><p><img src=\"/assets/images/blog/datalake-bytedance-hudi/slide12.png\" alt=\"slide12 serialization\"/></p><p>Serialization includes the following optimizations:</p><ol><li><p>Schema: the cost of data serialization using Avro is very expensive which consumes a lot of compute resources. To address this problem, we first use the singleton schema instance in JVM to avoid CPU-consuming comparison operations during the serialization process.</p></li><li><p>By optimizing the payload logic, the number of times of running serialization is reduced.</p></li><li><p>With the help of a third-party Avro serialization implementation, the serialization process is compiled into bytecode to improve the speed of SerDe and reduce memory usage. The serialization process has been modified to ensure that our complex schema can also be compiled properly.</p></li></ol><p><img src=\"/assets/images/blog/datalake-bytedance-hudi/slide13.png\" alt=\"slide13 compaction\"/></p><p>The optimization of the compaction process is as follows.</p><ul><li><p>In addition to the default Inline/Async compaction options, Hudi also supports flexible deployment of compaction. The characteristics of the compaction job are quite different from the ingestion job. In the same Spark application, it not only is impossible to set targeted settings but also has the problem of insufficient resource flexibility. We first build an independently deployed script so that the compaction job can be triggered and run independently. A low-cost mixed queue is used for resource scheduling for the compaction plan. In addition, we have also developed a compaction strategy based on rules and heuristics. The user&#x27;s requirement is usually to guarantee a day-level or hour-level SLA, and targeted compression of data in certain partitions, so targeted compression capabilities are provided.</p></li><li><p>In order to shorten the time of critical compaction, we usually do compaction in advance to avoid all work being completed in a single compaction job. However, if a FileGroup compacted has a new update, it has to be compacted again. In order to optimize the overall efficiency, we made a heuristic scheduling of when a FileGroup should be compacted based on business logic to reduce additional compaction costs.  The actual benefits of this feature are still being evaluated.</p></li><li><p>Finally, we made some process optimizations for the compaction, such as not using WriteStatus&#x27;s Cache and so on.</p></li></ul><p><img src=\"/assets/images/blog/datalake-bytedance-hudi/slide14.png\" alt=\"slide14 hdfs sla\"/></p><p>As storage designed for throughput, HDFS has serious real-time write glitches when the cluster usage level is relatively high. Through communication and cooperation with the HDFS team, some improvements have been done.</p><ul><li><p>First, we replace the original data HSync operation with HFlush to avoid disk I/O write amplification caused by distributed updates.</p></li><li><p>We make aggressive pipeline switching settings based on the scenario tuning, and the HDFS team has developed a flexible API that can control the pipeline to achieve flexible configurations in this scenario.</p></li><li><p>Finally, the timeliness of real-time writing is ensured through independent I/O isolation of log files.</p></li></ul><p><img src=\"/assets/images/blog/datalake-bytedance-hudi/slide15.png\" alt=\"slide15 process optimization\"/></p><p>There are also some small performance improvements, process modifications, and bug fixes. If you are interested, feel free to discuss that with me.</p><h2>Future Work</h2><p><img src=\"/assets/images/blog/datalake-bytedance-hudi/slide16.png\" alt=\"slide16 future work\"/>\n<img src=\"/assets/images/blog/datalake-bytedance-hudi/slide17.png\" alt=\"slide17 future work details\"/></p><p>In the future, we will continue to iterate in the following aspects.</p><ul><li><p><strong>Productization issues</strong>: The current way of using APIs and tuning parameters are highly demanding for the users, especially for the tuning, operation, and maintenance, which requires a deep understanding of Hudi principles to complete.  This hinders the promotion of that to users.</p></li><li><p><strong>Support issues for ecosystems</strong>: In our scenario, the technology stack is mainly on Flink, and the use of Flink will be explored in the future. In addition, the applications and environments used in upstream and downstream are complex, which requires cross-language and universal interface implementation. The current binding with Spark is cumbersome.</p></li><li><p><strong>Cost and performance issues</strong>: a common topic, since our scenario is relatively broad, the benefits from optimization are highly considerable.</p></li><li><p><strong>Storage semantics</strong>: We use Hudi as storage rather than a table format. Therefore, in the future, we plan to expand scenarios using Hudi, and need richer storage semantics.  We&#x27;ll do more work in this area.</p></li></ul><p><img src=\"/assets/images/blog/datalake-bytedance-hudi/slide19.png\" alt=\"slide19 hiring\"/></p><p>Finally, an advertisement, our recommendation architecture team is responsible for the recommendation architecture design and development for products such as Douyin, Toutiao, and Xigua Video. The challenges are big and the growth is fast. Now we are hiring people and the working locations include: Beijing/Shanghai/Hangzhou/Singapore/Mountain View.  If you are  interested, you are welcomed to add WeChat <code>qinglingcannotfly</code> or send your resume to the email: <code>guanziyue.gzy@bytedance.com</code>.</p>",
            "url": "https://hudi.apache.org/blog/2021/09/01/building-eb-level-data-lake-using-hudi-at-bytedance",
            "title": "Building an ExaByte-level Data Lake Using Apache Hudi at ByteDance",
            "summary": "Ziyue Guan from Bytedance shares the experience of building an ExaByte(EB)-level data lake using Apache Hudi at Bytedance.",
            "date_modified": "2021-09-01T00:00:00.000Z",
            "author": {
                "name": "Ziyue Guan, translated to English by yihua"
            }
        },
        {
            "id": "/2021/08/23/async-clustering",
            "content_html": "<p>In one of the <a href=\"/blog/2021/01/27/hudi-clustering-intro\">previous blog</a> posts, we introduced a new\nkind of table service called clustering to reorganize data for improved query performance without compromising on\ningestion speed. We learnt how to setup inline clustering. In this post, we will discuss what has changed since then and\nsee how asynchronous clustering can be setup using HoodieClusteringJob as well as DeltaStreamer utility.</p><h2>Introduction</h2><p>On a high level, clustering creates a plan based on a configurable strategy, groups eligible files based on specific\ncriteria and then executes the plan. Hudi supports <a href=\"https://hudi.apache.org/docs/concurrency_control#enabling-multi-writing\">multi-writers</a> which provides\nsnapshot isolation between multiple table services, thus allowing writers to continue with ingestion while clustering\nruns in the background. For a more detailed overview of the clustering architecture please check out the previous blog\npost.</p><h2>Clustering Strategies</h2><p>As mentioned before, clustering plan as well as execution depends on configurable strategy. These strategies can be\nbroadly classified into three types: clustering plan strategy, execution strategy and update strategy.</p><h3>Plan Strategy</h3><p>This strategy comes into play while creating clustering plan. It helps to decide what file groups should be clustered.\nLet&#x27;s look at different plan strategies that are available with Hudi. Note that these strategies are easily pluggable\nusing this <a href=\"/docs/next/configurations#hoodieclusteringplanstrategyclass\">config</a>.</p><ol><li><code>SparkSizeBasedClusteringPlanStrategy</code>: It selects file slices based on\nthe <a href=\"/docs/next/configurations/#hoodieclusteringplanstrategysmallfilelimit\">small file limit</a>\nof base files and creates clustering groups upto max file size allowed per group. The max size can be specified using\nthis <a href=\"/docs/next/configurations/#hoodieclusteringplanstrategymaxbytespergroup\">config</a>. This\nstrategy is useful for stitching together medium-sized files into larger ones to reduce lot of files spread across\ncold partitions.</li><li><code>SparkRecentDaysClusteringPlanStrategy</code>: It looks back previous &#x27;N&#x27; days partitions and creates a plan that will\ncluster the &#x27;small&#x27; file slices within those partitions. This is the default strategy. It could be useful when the\nworkload is predictable and data is partitioned by time.</li><li><code>SparkSelectedPartitionsClusteringPlanStrategy</code>: In case you want to cluster only specific partitions within a range,\nno matter how old or new are those partitions, then this strategy could be useful. To use this strategy, one needs\nto set below two configs additionally (both begin and end partitions are inclusive):</li></ol><pre><code>hoodie.clustering.plan.strategy.cluster.begin.partition\nhoodie.clustering.plan.strategy.cluster.end.partition\n</code></pre><p>:::note\nAll the strategies are partition-aware and the latter two are still bound by the size limits of the first strategy.\n:::</p><h3>Execution Strategy</h3><p>After building the clustering groups in the planning phase, Hudi applies execution strategy, for each group, primarily\nbased on sort columns and size. The strategy can be specified using this <a href=\"/docs/next/configurations/#hoodieclusteringexecutionstrategyclass\">config</a>.</p><p><code>SparkSortAndSizeExecutionStrategy</code> is the default strategy. Users can specify the columns to sort the data by, when\nclustering using\nthis <a href=\"/docs/next/configurations/#hoodieclusteringplanstrategysortcolumns\">config</a>. Apart from\nthat, we can also set <a href=\"/docs/next/configurations/#hoodieparquetmaxfilesize\">max file size</a>\nfor the parquet files produced due to clustering. The strategy uses bulk insert to write data into new files, in which\ncase, Hudi implicitly uses a partitioner that does sorting based on specified columns. In this way, the strategy changes\nthe data layout in a way that not only improves query performance but also balance rewrite overhead automatically.</p><p>Now this strategy can be executed either as a single spark job or multiple jobs depending on number of clustering groups\ncreated in the planning phase. By default, Hudi will submit multiple spark jobs and union the results. In case you want\nto force Hudi to use single spark job, set the execution strategy\nclass <a href=\"/docs/next/configurations/#hoodieclusteringexecutionstrategyclass\">config</a>\nto <code>SingleSparkJobExecutionStrategy</code>.</p><h3>Update Strategy</h3><p>Currently, clustering can only be scheduled for tables/partitions not receiving any concurrent updates. By default,\nthe <a href=\"/docs/next/configurations/#hoodieclusteringupdatesstrategy\">config for update strategy</a> is\nset to <strong><em>SparkRejectUpdateStrategy</em></strong>. If some file group has updates during clustering then it will reject updates and\nthrow an exception. However, in some use-cases updates are very sparse and do not touch most file groups. The default\nstrategy to simply reject updates does not seem fair. In such use-cases, users can set the config to <strong><em>SparkAllowUpdateStrategy</em></strong>.</p><p>We discussed the critical strategy configurations. All other configurations related to clustering are\nlisted <a href=\"/docs/next/configurations/#Clustering-Configs\">here</a>. Out of this list, a few\nconfigurations that will be very useful are:</p><table><thead><tr><th>Config key</th><th>Remarks</th><th>Default</th></tr></thead><tbody><tr><td><code>hoodie.clustering.async.enabled</code></td><td>Enable running of clustering service, asynchronously as writes happen on the table.</td><td>False</td></tr><tr><td><code>hoodie.clustering.async.max.commits</code></td><td>Control frequency of async clustering by specifying after how many commits clustering should be triggered.</td><td>4</td></tr><tr><td><code>hoodie.clustering.preserve.commit.metadata</code></td><td>When rewriting data, preserves existing _hoodie_commit_time. This means users can run incremental queries on clustered data without any side-effects.</td><td>False</td></tr></tbody></table><h2>Asynchronous Clustering</h2><p>Previously, we have seen how users\ncan <a href=\"/blog/2021/01/27/hudi-clustering-intro#setting-up-clustering\">setup inline clustering</a>.\nAdditionally, users can\nleverage <a href=\"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+19+Clustering+data+for+freshness+and+query+performance#RFC19Clusteringdataforfreshnessandqueryperformance-SetupforAsyncclusteringJob\">HoodieClusteringJob</a>\nto setup 2-step asynchronous clustering.</p><h3>HoodieClusteringJob</h3><p>With the release of Hudi version 0.9.0, we can schedule as well as execute clustering in the same step. We just need to\nspecify the <code>—mode</code> or <code>-m</code> option. There are three modes:</p><ol><li><code>schedule</code>: Make a clustering plan. This gives an instant which can be passed in execute mode.</li><li><code>execute</code>: Execute a clustering plan at given instant which means --instant-time is required here.</li><li><code>scheduleAndExecute</code>: Make a clustering plan first and execute that plan immediately.</li></ol><p>Note that to run this job while the original writer is still running, please enable multi-writing:</p><pre><code>hoodie.write.concurrency.mode=optimistic_concurrency_control\nhoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.ZookeeperBasedLockProvider\n</code></pre><p>A sample spark-submit command to setup HoodieClusteringJob is as below:</p><pre><code class=\"language-bash\">spark-submit \\\n--class org.apache.hudi.utilities.HoodieClusteringJob \\\n/path/to/hudi-utilities-bundle/target/hudi-utilities-bundle_2.12-0.9.0-SNAPSHOT.jar \\\n--props /path/to/config/clusteringjob.properties \\\n--mode scheduleAndExecute \\\n--base-path /path/to/hudi_table/basePath \\\n--table-name hudi_table_schedule_clustering \\\n--spark-memory 1g\n</code></pre><p>A sample <code>clusteringjob.properties</code> file:</p><pre><code>hoodie.clustering.async.enabled=true\nhoodie.clustering.async.max.commits=4\nhoodie.clustering.plan.strategy.target.file.max.bytes=1073741824\nhoodie.clustering.plan.strategy.small.file.limit=629145600\nhoodie.clustering.execution.strategy.class=org.apache.hudi.client.clustering.run.strategy.SparkSortAndSizeExecutionStrategy\nhoodie.clustering.plan.strategy.sort.columns=column1,column2\n</code></pre><h3>HoodieDeltaStreamer</h3><p>This brings us to our users&#x27; favorite utility in Hudi. Now, we can trigger asynchronous clustering with DeltaStreamer.\nJust set the <code>hoodie.clustering.async.enabled</code> config to true and specify other clustering config in properties file\nwhose location can be pased as <code>—props</code> when starting the deltastreamer (just like in the case of HoodieClusteringJob).</p><p>A sample spark-submit command to setup HoodieDeltaStreamer is as below:</p><pre><code class=\"language-bash\">spark-submit \\\n--class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer \\\n/path/to/hudi-utilities-bundle/target/hudi-utilities-bundle_2.12-0.9.0-SNAPSHOT.jar \\\n--props /path/to/config/clustering_kafka.properties \\\n--schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \\\n--source-class org.apache.hudi.utilities.sources.AvroKafkaSource \\\n--source-ordering-field impresssiontime \\\n--table-type COPY_ON_WRITE \\\n--target-base-path /path/to/hudi_table/basePath \\\n--target-table impressions_cow_cluster \\\n--op INSERT \\\n--hoodie-conf hoodie.clustering.async.enabled=true \\\n--continuous\n</code></pre><h3>Spark Structured Streaming</h3><p>We can also enable asynchronous clustering with Spark structured streaming sink as shown below. </p><pre><code class=\"language-scala\">val commonOpts = Map(\n   &quot;hoodie.insert.shuffle.parallelism&quot; -&gt; &quot;4&quot;,\n   &quot;hoodie.upsert.shuffle.parallelism&quot; -&gt; &quot;4&quot;,\n   DataSourceWriteOptions.RECORDKEY_FIELD.key -&gt; &quot;_row_key&quot;,\n   DataSourceWriteOptions.PARTITIONPATH_FIELD.key -&gt; &quot;partition&quot;,\n   DataSourceWriteOptions.PRECOMBINE_FIELD.key -&gt; &quot;timestamp&quot;,\n   HoodieWriteConfig.TBL_NAME.key -&gt; &quot;hoodie_test&quot;\n)\n\ndef getAsyncClusteringOpts(isAsyncClustering: String, \n                           clusteringNumCommit: String, \n                           executionStrategy: String):Map[String, String] = {\n   commonOpts + (DataSourceWriteOptions.ASYNC_CLUSTERING_ENABLE.key -&gt; isAsyncClustering,\n           HoodieClusteringConfig.ASYNC_CLUSTERING_MAX_COMMITS.key -&gt; clusteringNumCommit,\n           HoodieClusteringConfig.EXECUTION_STRATEGY_CLASS_NAME.key -&gt; executionStrategy\n   )\n}\n\ndef initStreamingWriteFuture(hudiOptions: Map[String, String]): Future[Unit] = {\n   val streamingInput = // define the source of streaming\n   Future {\n      println(&quot;streaming starting&quot;)\n      streamingInput\n              .writeStream\n              .format(&quot;org.apache.hudi&quot;)\n              .options(hudiOptions)\n              .option(&quot;checkpointLocation&quot;, basePath + &quot;/checkpoint&quot;)\n              .mode(Append)\n              .start()\n              .awaitTermination(10000)\n      println(&quot;streaming ends&quot;)\n   }\n}\n\ndef structuredStreamingWithClustering(): Unit = {\n   val df = //generate data frame\n   val hudiOptions = getClusteringOpts(&quot;true&quot;, &quot;1&quot;, &quot;org.apache.hudi.client.clustering.run.strategy.SparkSortAndSizeExecutionStrategy&quot;)\n   val f1 = initStreamingWriteFuture(hudiOptions)\n   Await.result(f1, Duration.Inf)\n}\n</code></pre><h2>Conclusion and Future Work</h2><p>In this post, we discussed different clustering strategies and how to setup asynchronous clustering. The story is not\nover yet and future work entails:</p><ul><li>Support clustering with updates.</li><li>CLI tools to support clustering.</li></ul><p>Please follow this <a href=\"https://issues.apache.org/jira/browse/HUDI-1042\">JIRA</a> to learn more about active development on\nthis issue. We look forward to contributions from the community. Hope you enjoyed this post. Put your Hudi on and keep\nstreaming!</p>",
            "url": "https://hudi.apache.org/blog/2021/08/23/async-clustering",
            "title": "Asynchronous Clustering using Hudi",
            "summary": "In one of the previous blog posts, we introduced a new",
            "date_modified": "2021-08-23T00:00:00.000Z",
            "author": {
                "name": "codope"
            }
        },
        {
            "id": "/2021/08/23/s3-events-source",
            "content_html": "<p>In this post we will talk about a new deltastreamer source which reliably and efficiently processes new data files as they arrive in AWS S3.\nAs of today, to ingest data from S3 into Hudi, users leverage DFS source whose <a href=\"https://github.com/apache/hudi/blob/178767948e906f673d6d4a357c65c11bc574f619/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DFSPathSelector.java\">path selector</a> would identify the source files modified since the last checkpoint based on max modification time.\nThe problem with this approach is that modification time precision is upto seconds in S3. It maybe possible that there were many files (beyond what the configurable source limit allows) modifed in that second and some files might be skipped.\nFor more details, please refer to <a href=\"https://issues.apache.org/jira/browse/HUDI-1723\">HUDI-1723</a>.\nWhile the workaround is to ignore the source limit and keep reading, the problem motivated us to redesign so that users can reliably ingest from S3.</p><h2>Design</h2><p>For use-cases where seconds granularity does not suffice, we have a new source in deltastreamer using log-based approach.\nThe new <a href=\"https://github.com/apache/hudi/blob/178767948e906f673d6d4a357c65c11bc574f619/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/S3EventsSource.java\">S3 events source</a> relies on change notification and incremental processing to ingest from S3.\nThe architecture is as shown in the figure below.</p><p><img src=\"/assets/images/blog/s3_events_source_design.png\" alt=\"Different components in the design\"/></p><p>In this approach, users need to <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html\">enable S3 event notifications</a>.\nThere will be two types of deltastreamers as detailed below. </p><ol><li><a href=\"https://github.com/apache/hudi/blob/178767948e906f673d6d4a357c65c11bc574f619/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/S3EventsSource.java\">S3EventsSource</a>: Create Hudi S3 metadata table.\nThis source leverages AWS <a href=\"https://aws.amazon.com/sns\">SNS</a> and <a href=\"https://aws.amazon.com/sqs/\">SQS</a> services that subscribe to file events from the source bucket.<pre><code>- Events from SQS will be written to this table, which serves as a changelog for the subsequent incremental puller.\n- When the events are committed to the S3 metadata table they will be deleted from SQS.\n</code></pre></li><li><a href=\"https://github.com/apache/hudi/blob/178767948e906f673d6d4a357c65c11bc574f619/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/S3EventsHoodieIncrSource.java\">S3EventsHoodieIncrSource</a> and uses the metadata table written by S3EventsSource.<ul><li>Read the S3 metadata table and get the objects that were added or modified. These objects contain the S3 path for the source files that were added or modified.</li><li>Write to Hudi table with source data corresponding to the source files in the S3 bucket.</li></ul></li></ol><h2>Advantages</h2><ul><li><strong>Decoupling</strong>: Every step in the pipeline is decoupled. The two sources can be started independent of each other. We imagine most users run a single deltastreamer to get all changes for a given bucket and can fan-out multiple tables off that.</li><li><strong>Performance and Scale</strong>: The previous approach used to list all files, sort by modification time and then filter based on checkpoint. While it did prune partition paths, the directory listing could still become a bottleneck. By relying on change notification and native cloud APIs, the new approach avoids directory listing and scales with the number of files being ingested.</li><li><strong>Reliability</strong>: Since there is no longer any dependency on the max modification time and the fact that S3 events are being recorded in the metadata table, users can rest assured that all the events will be processed eventually.</li><li><strong>Fault Tolerance</strong>: There are two levels of fault toerance in this design. Firstly, if some of the messages are not committed to the S3 metadata table, then those messages will remain in the queue so that they can be reprocessed in the next round. Secondly, if the incremental puller fails, then users can query the S3 metadata table for the last commit point and resume the incremental puller from that point onwards (kinda like how Kafka consumers can reset offset).</li><li><strong>Asynchronous backfills</strong>: With the log-based approach, it becomes much easier to trigger backfills. See the &quot;Conclusion and Future Work&quot; section for more details.</li></ul><h2>Configuration and Setup</h2><p>Users only need to specify the SQS queue url and region name to start the S3EventsSource (metadata source).</p><pre><code>hoodie.deltastreamer.s3.source.queue.url=https://sqs.us-west-2.amazonaws.com/queue/url\nhoodie.deltastreamer.s3.source.queue.region=us-west-2\n</code></pre><p>There are a few other configurations for the metadata source which can be tuned to suit specific requirements:</p><ul><li><em><code>hoodie.deltastreamer.s3.source.queue.long.poll.wait</code></em>: Value can range in <!-- -->[0, 20]<!-- --> seconds. If set to 0 then metadata source will consume messages from SQS using short polling. It is recommended to use long polling because it will reduce false empty responses and reduce the cost of using SQS. By default, this value is set to 20 seconds.</li><li><em><code>hoodie.deltastreamer.s3.source.queue.visibility.timeout</code></em>: Value can range in <!-- -->[0, 43200]<!-- --> seconds (i.e. max 12 hours). SQS does not automatically delete the messages once consumed. It is the responsibility of metadata source to delete the message after committing. SQS will move the consumed message to in-flight state during which it becomes invisible for the configured timeout period. By default, this value is set to 30 seconds.</li><li><em><code>hoodie.deltastreamer.s3.source.queue.max.messages.per.batch</code></em>: Maximum number of messages in a batch of one round of metadata source run. By default, this value is set to 5.</li></ul><p>To setup the pipeline, first <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html\">enable S3 event notifications</a>.\nDownload the <a href=\"https://mvnrepository.com/artifact/com.amazonaws/aws-java-sdk-sqs\">aws-java-sdk-sqs</a> jar.\nThen start the S3EventsSource and  S3EventsHoodieIncrSource using the HoodieDeltaStreamer utility as shown in sample commands below.</p><pre><code class=\"language-bash\"># To start S3EventsSource\nspark-submit \\\n--jars &quot;/home/hadoop/hudi-utilities-bundle_2.11-0.9.0.jar,/usr/lib/spark/external/lib/spark-avro.jar,/home/hadoop/aws-java-sdk-sqs-1.12.22.jar&quot; \\\n--master yarn --deploy-mode client \\\n--class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer /home/hadoop/hudi-packages/hudi-utilities-bundle_2.11-0.9.0-SNAPSHOT.jar \\\n--table-type COPY_ON_WRITE --source-ordering-field eventTime \\\n--target-base-path s3://bucket_name/path/for/s3_meta_table \\\n--target-table s3_meta_table  --continuous \\\n--min-sync-interval-seconds 10 \\\n--hoodie-conf hoodie.datasource.write.recordkey.field=&quot;s3.object.key,eventName&quot; \\\n--hoodie-conf hoodie.datasource.write.keygenerator.class=org.apache.hudi.keygen.ComplexKeyGenerator \\\n--hoodie-conf hoodie.datasource.write.partitionpath.field=s3.bucket.name --enable-hive-sync \\\n--hoodie-conf hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor \\\n--hoodie-conf hoodie.datasource.write.hive_style_partitioning=true \\\n--hoodie-conf hoodie.datasource.hive_sync.database=default \\\n--hoodie-conf hoodie.datasource.hive_sync.table=s3_meta_table \\\n--hoodie-conf hoodie.datasource.hive_sync.partition_fields=bucket \\\n--source-class org.apache.hudi.utilities.sources.S3EventsSource \\\n--hoodie-conf hoodie.deltastreamer.source.queue.url=https://sqs.us-west-2.amazonaws.com/queue/url\n--hoodie-conf hoodie.deltastreamer.s3.source.queue.region=us-west-2\n\n# To start S3EventsHoodieIncrSource\nspark-submit \\\n--jars &quot;/home/hadoop/hudi-utilities-bundle_2.11-0.9.0.jar,/usr/lib/spark/external/lib/spark-avro.jar,/home/hadoop/aws-java-sdk-sqs-1.12.22.jar&quot; \\\n--master yarn --deploy-mode client \\\n--class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer /home/hadoop/hudi-packages/hudi-utilities-bundle_2.11-0.9.0-SNAPSHOT.jar \\\n--table-type COPY_ON_WRITE \\\n--source-ordering-field eventTime --target-base-path s3://bucket_name/path/for/s3_hudi_table \\\n--target-table s3_hudi_table  --continuous --min-sync-interval-seconds 10 \\\n--hoodie-conf hoodie.datasource.write.recordkey.field=&quot;pull_request_id&quot; \\\n--hoodie-conf hoodie.datasource.write.keygenerator.class=org.apache.hudi.keygen.SimpleKeyGenerator \\\n--hoodie-conf hoodie.datasource.write.partitionpath.field=s3.bucket.name --enable-hive-sync \\\n--hoodie-conf hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor \\\n--hoodie-conf hoodie.datasource.write.hive_style_partitioning=true \\\n--hoodie-conf hoodie.datasource.hive_sync.database=default \\\n--hoodie-conf hoodie.datasource.hive_sync.table=s3_hudi_v6 \\\n--hoodie-conf hoodie.datasource.hive_sync.partition_fields=bucket \\\n--source-class org.apache.hudi.utilities.sources.S3EventsHoodieIncrSource \\\n--hoodie-conf hoodie.deltastreamer.source.hoodieincr.path=s3://bucket_name/path/for/s3_meta_table \\\n--hoodie-conf hoodie.deltastreamer.source.hoodieincr.read_latest_on_missing_ckpt=true\n</code></pre><h2>Conclusion and Future Work</h2><p>This post introduced a log-based approach to ingest data from S3 into Hudi tables reliably and efficiently. We are actively improving this along the following directions.</p><ul><li>One stream of work is to add support for other cloud-based object storage like Google Cloud Storage, Azure Blob Storage, etc. with this revamped design.</li><li>Another stream of work is to add resource manager that allows users to setup notifications and delete resources when no longer needed.</li><li>Another interesting piece of work is to support <strong>asynchronous backfills</strong>. Notification systems are evntually consistent and typically do not guarantee perfect delivery of all files right away. The log-based approach provides enough flexibility to trigger automatic backfills at a configurable interval e.g. once a day or once a week.</li></ul><p>Please follow this <a href=\"https://issues.apache.org/jira/browse/HUDI-1896\">JIRA</a> to learn more about active development on this issue.\nWe look forward to contributions from the community. Hope you enjoyed this post. </p><p>Put your Hudi on and keep streaming!</p>",
            "url": "https://hudi.apache.org/blog/2021/08/23/s3-events-source",
            "title": "Reliable ingestion from AWS S3 using Hudi",
            "summary": "In this post we will talk about a new deltastreamer source which reliably and efficiently processes new data files as they arrive in AWS S3.",
            "date_modified": "2021-08-23T00:00:00.000Z",
            "author": {
                "name": "codope"
            }
        },
        {
            "id": "/2021/08/18/improving-marker-mechanism",
            "content_html": "<p>Hudi supports fully automatic cleanup of uncommitted data on storage during its write operations. Write operations in an Apache Hudi table use markers to efficiently track the data files written to storage. In this blog, we dive into the design of the existing direct marker file mechanism and explain its performance problems on cloud storage like AWS S3 for\nvery large writes. We demonstrate how we improve write performance with introduction of timeline-server-based markers.</p><h2>Need for Markers during Write Operations</h2><p>A <strong>marker</strong> in Hudi, such as a marker file with a unique filename, is a label to indicate that a corresponding data file exists in storage, which then Hudi\nuses to automatically clean up uncommitted data during failure and rollback scenarios. Each marker entry is composed of three parts, the data file name,\nthe marker extension (<code>.marker</code>), and the I/O operation created the file (<code>CREATE</code> - inserts, <code>MERGE</code> - updates/deletes, or <code>APPEND</code> - either). For example, the marker <code>91245ce3-bb82-4f9f-969e-343364159174-0_140-579-0_20210820173605.parquet.marker.CREATE</code> indicates\nthat the corresponding data file is <code>91245ce3-bb82-4f9f-969e-343364159174-0_140-579-0_20210820173605.parquet</code> and the I/O type is <code>CREATE</code>. Hudi creates a marker before creating the corresponding data file in the file system and deletes all markers pertaining to a commit when it succeeds.</p><p>The markers are useful for efficiently carrying out different operations by the write client.  Markers serve as a way to track data files of interest rather than scanning the whole Hudi table by listing all files in the table.  Two important operations use markers which come in handy to find uncommitted data files of interest efficiently:</p><ul><li><strong>Removing duplicate/partial data files</strong>: in Spark, the Hudi write client delegates the data file writing to multiple executors.  One executor can fail the task, leaving partial data files written, and Spark retries the task in this case until it succeeds. When speculative execution is enabled, there can also be multiple successful attempts at writing out the same data into different files, only one of which is finally handed to the Spark driver process for committing. The markers help efficiently identify the partial data files written, which contain duplicate data compared to the data files written by the successful trial later, and these duplicate data files are cleaned up when the commit is finalized.  If there are no such marker to track the per-commit data files, we have to list all files in the file system, correlate that with the files seen in timeline and then delete the ones that belong to partial write failures.  As you could imagine, this would be very costly in a very large installation of a datalake.</li><li><strong>Rolling back failed commits</strong>: the write operation can fail in the middle, leaving some data files written in storage.  In this case, the marker entries stay in storage as the commit is failed.  In the next write operation, the write client rolls back the failed commit before proceeding with the new write. The rollback is done with the help of markers to identify the data files written as part of the failed commit.</li></ul><p>Next, we dive into the existing marker mechanism, explain its performance problem, and demonstrate the new timeline-server-based marker mechanism to address the problem.</p><h2>Existing Direct Marker Mechanism and its limitations</h2><p>The <strong>existing marker mechanism</strong> simply creates a new marker file corresponding to each data file, with the marker filename as described above.  The marker file does not have any content, i.e., empty.  Each marker file is written to storage in the same directory hierarchy, i.e., commit instant and partition path, under a temporary folder <code>.hoodie/.temp</code> under the base path of the Hudi table.  For example, the figure below shows one example of the marker files created and the corresponding data files when writing data to the Hudi table.  When getting or deleting all the marker file paths, the mechanism first lists all the paths under the temporary folder, <code>.hoodie/.temp/&lt;commit_instant&gt;</code>, and then does the operation.</p><p><img src=\"/assets/images/blog/marker-mechanism/direct-marker-file-mechanism.png\" alt=\"An example of marker and data files in direct marker file mechanism\"/></p><p>While it&#x27;s much efficient over scanning the entire table for uncommitted data files, as the number of data files to write increases, so does the number of marker files to create.  For large writes which need to write significant number of data files, e.g., 10K or more, this can create performance bottlenecks for cloud storage such as AWS S3.  In AWS S3, each file create and delete call triggers an HTTP request and there is <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html\">rate-limiting</a> on how many requests can be processed per second per prefix in a bucket.  When the number of data files to write concurrently and the number of marker files is huge, the marker file operations could take up non-trivial time during the write operation, sometimes on the order of a few minutes or more.  Users may barely notice this on a storage like HDFS, where the file system metadata is efficiently cached in memory.</p><h2>Timeline-server-based marker mechanism improving write performance</h2><p>To address the performance bottleneck due to rate-limiting of AWS S3 explained above, we introduce a <strong>new marker mechanism leveraging the timeline server</strong>, which optimizes the marker-related latency for storage with non-trivial file I/O latency.  The <strong>timeline server</strong> in Hudi serves as a centralized place for providing the file system and timeline views. As shown below, the new timeline-server-based marker mechanism delegates the marker creation and other marker-related operations from individual executors to the timeline server for centralized processing.  The timeline server batches the marker creation requests and writes the markers to a bounded set of files in the file system at regular intervals.  In such a way, the number of actual file operations and latency related to markers can be significantly reduced even with a huge number of data files, thus improving the performance of the writes.</p><p><img src=\"/assets/images/blog/marker-mechanism/timeline-server-based-marker-mechanism.png\" alt=\"Timeline-server-based marker mechanism\"/></p><p>To improve the efficiency of processing marker creation requests, we design the batched handling of marker requests at the timeline server. Each marker creation request is handled asynchronously in the Javalin timeline server and queued before processing. For every batch interval, e.g., 20ms, the timeline server pulls the pending marker creation requests from the queue and writes all markers to the next file in a round robin fashion.  Inside the timeline server, such batch processing is multi-threaded, designed and implemented to guarantee consistency and correctness.  Both the batch interval and the batch concurrency can be configured through the write options.</p><p><img src=\"/assets/images/blog/marker-mechanism/batched-marker-creation.png\" alt=\"Batched processing of marker creation requests\"/></p><p>Note that the worker thread always checks whether the marker has already been created by comparing the marker name from the request with the memory copy of all markers maintained at the timeline server. The underlying files storing the markers are only read upon the first marker request (lazy loading).  The responses of requests are only sent back once the new markers are flushed to the files, so that in the case of the timeline server failure, the timeline server can recover the already created markers. These ensure consistency between storage and the in-memory copy, and improve the performance of processing marker requests.</p><h2>Marker-related write options</h2><p>We introduce the following new marker-related write options in <code>0.9.0</code> release, to configure the marker mechanism.  Note that the timeline-server-based marker mechanism is not yet supported for HDFS in <code>0.9.0</code> release, and we plan to support the timeline-server-based marker mechanism for HDFS in the future.</p><table><thead><tr><th>Property Name</th><th>Default</th><th align=\"center\">Meaning</th></tr></thead><tbody><tr><td><code>hoodie.write.markers.type</code></td><td>direct</td><td align=\"center\">Marker type to use.  Two modes are supported: (1) <code>direct</code>: individual marker file corresponding to each data file is directly created by the executor; (2) <code>timeline_server_based</code>: marker operations are all handled at the timeline service which serves as a proxy.  New marker entries are batch processed and stored in a limited number of underlying files for efficiency.</td></tr><tr><td><code>hoodie.markers.timeline_server_based.batch.num_threads</code></td><td>20</td><td align=\"center\">Number of threads to use for batch processing marker creation requests at the timeline server.</td></tr><tr><td><code>hoodie.markers.timeline_server_based.batch.interval_ms</code></td><td>50</td><td align=\"center\">The batch interval in milliseconds for marker creation batch processing.</td></tr></tbody></table><h2>Performance</h2><p>We evaluate the write performance over both direct and timeline-server-based marker mechanisms by bulk-inserting a large dataset using Amazon EMR with Spark and S3. The input data is around 100GB.  We configure the write operation to generate a large number of data files concurrently by setting the max parquet file size to be 1MB and parallelism to be 240.  Note that it is unlikely to set max parquet file size to 1MB in production and such a setup is only to evaluate the performance regarding the marker mechanisms. As we noted before, while the latency of direct marker mechanism is acceptable for incremental writes with smaller number of data files written, it increases dramatically for large bulk inserts/writes which produce much more data files.</p><p>As shown below, direct marker mechanism works really well, when a part of the table is written, e.g., 1K out of 165K data files.  However, the time of direct marker operations is non-trivial when we need to write significant number of data files. Compared to the direct marker mechanism, the timeline-server-based marker mechanism generates much fewer files storing markers because of the batch processing, leading to much less time on marker-related I/O operations, thus achieving 31% lower write completion time compared to the direct marker file mechanism.</p><table><thead><tr><th>Marker Type</th><th>Total Files</th><th align=\"center\">Num data files written</th><th align=\"center\">Files created for markers</th><th align=\"center\">Marker deletion time</th><th align=\"center\">Bulk Insert Time (including marker deletion)</th></tr></thead><tbody><tr><td>Direct</td><td>165k</td><td align=\"center\">1k</td><td align=\"center\">1k</td><td align=\"center\">5.4secs</td><td align=\"center\">-</td></tr><tr><td>Direct</td><td>165k</td><td align=\"center\">165k</td><td align=\"center\">165k</td><td align=\"center\">15min</td><td align=\"center\">55min</td></tr><tr><td>Timeline-server-based</td><td>165k</td><td align=\"center\">165k</td><td align=\"center\">20</td><td align=\"center\">~3s</td><td align=\"center\">38min</td></tr></tbody></table><h2>Conclusion</h2><p>We identify that for large writes which need to write significant number of data files, the existing direct marker file mechanism can incur performance bottlenecks due to the rate-limiting of file create and delete calls on cloud storage like AWS S3.  To address this issue, we introduce a new marker mechanism leveraging the timeline server, which delegates the marker creation and other marker-related operations from individual executors to the timeline server and uses batch processing to improve performance.  Performance evaluations on Amazon EMR with Spark and S3 show that the marker-related I/O latency and overall write time are reduced.</p>",
            "url": "https://hudi.apache.org/blog/2021/08/18/improving-marker-mechanism",
            "title": "Improving Marker Mechanism in Apache Hudi",
            "summary": "Hudi supports fully automatic cleanup of uncommitted data on storage during its write operations. Write operations in an Apache Hudi table use markers to efficiently track the data files written to storage. In this blog, we dive into the design of the existing direct marker file mechanism and explain its performance problems on cloud storage like AWS S3 for",
            "date_modified": "2021-08-18T00:00:00.000Z",
            "author": {
                "name": "yihua"
            }
        },
        {
            "id": "/2021/08/18/virtual-keys",
            "content_html": "<p>Apache Hudi helps you build and manage data lakes with different table types, config knobs to cater to everyone&#x27;s need.\nHudi adds per record metadata fields like <code>_hoodie_record_key</code>, <code>_hoodie_partition path</code>, <code>_hoodie_commit_time</code> which serves multiple purposes.\nThey assist in avoiding re-computing the record key, partition path during merges, compaction and other table operations\nand also assists in supporting <a href=\"/blog/2021/07/21/streaming-data-lake-platform#readers\">record-level</a> incremental queries (in comparison to other table formats, that merely track files).\nIn addition, it ensures data quality by ensuring unique key constraints are enforced even if the key field changes for a given table, during its lifetime.\nBut one of the repeated asks from the community is to leverage existing fields and not to add additional meta fields, for simple use-cases where such benefits are not desired or key changes are very rare.  </p><h2>Virtual Key support</h2><p>Hudi now supports virtual keys, where Hudi meta fields can be computed on demand from the data fields. Currently, the meta fields are\ncomputed once and stored as per record metadata and re-used across various operations. If one does not need incremental query support,\nthey can start leveraging Hudi&#x27;s Virtual key support and still go about using Hudi to build and manage their data lake to reduce the storage\noverhead due to per record metadata. </p><h3>Configurations</h3><p>Virtual keys can be enabled for a given table using the below config. When set to <code>hoodie.populate.meta.fields=false</code>,\nHudi will use virtual keys for the corresponding table. Default value for this config is <code>true</code>, which means, all  meta fields will be added by default.</p><p>Once virtual keys are enabled, it can&#x27;t be disabled for a given hudi table, because already stored records may not have\nthe meta fields populated. But if you have an existing table from an older version of hudi, virtual keys can be enabled.\nAnother constraint w.r.t virtual key support is that, Key generator properties for a given table cannot be changed through\nthe course of the lifecycle of a given hudi table. In this model, the user also shares responsibility of ensuring uniqueness\nof key within a table. For instance, if you configure record key to point to <code>field_5</code> for few batches of write and later switch to <code>field_10</code>,\nHudi cannot guarantee uniqueness of key, since older writes could have had duplicates for <code>field_10</code>. </p><p>With virtual keys, keys will have to be re-computed everytime when in need (merges, compaction, MOR snapshot read). Hence we\nsupport virtual keys for all built-in key generators on Copy-On-Write tables. Supporting all key generators on Merge-On-Read table\nwould entail reading all fields out of base and delta logs, sacrificing core columnar query performance, which will be prohibitively expensive\nfor users. Thus, we support only simple key generators (the default key generator, where both record key and partition path refer\nto an existing field ) for now.</p><h4>Supported Key Generators with CopyOnWrite(COW) table:</h4><p>SimpleKeyGenerator, ComplexKeyGenerator, CustomKeyGenerator, TimestampBasedKeyGenerator and NonPartitionedKeyGenerator. </p><h4>Supported Key Generators with MergeOnRead(MOR) table:</h4><p>SimpleKeyGenerator</p><h4>Supported Index types:</h4><p>Only &quot;SIMPLE&quot; and &quot;GLOBAL_SIMPLE&quot; index types are supported in the first cut. We plan to add support for other index\n(BLOOM, etc) in future releases. </p><h3>Supported Operations</h3><p>All existing features are supported for a hudi table with virtual keys, except the incremental\nqueries. Which means, cleaning, archiving, metadata table, clustering, etc can be enabled for a hudi table with\nvirtual keys enabled. So, you are able to merely use Hudi as a transactional table format with all the awesome\ntable service runtimes and platform services, if you wish to do so, without incurring any overheads associated with\nsupport for incremental data processing.</p><h3>Sample Output</h3><p>As called out earlier, one has to set <code>hoodie.populate.meta.fields=false</code> to enable virtual keys. Let&#x27;s see the\ndifference between records of a hudi table with and without virtual keys.</p><p>Here are some sample records for a regular hudi table (virtual keys disabled)</p><pre><code>+--------------------+--------------------------------------+--------------------------------------+---------+---------+-------------------+\n|_hoodie_commit_time |           _hoodie_record_key         |        _hoodie_partition_path        |  rider  | driver  |        fare       |\n+--------------------+--------------------------------------+--------------------------------------+---------+---------+-------------------+\n|   20210825154123   | eb7819f1-6f04-429d-8371-df77620b9527 | americas/united_states/san_francisco |rider-284|driver-284|98.3428192817987  |\n|   20210825154123   | 37ea44f1-fda7-4ec4-84de-f43f5b5a4d84 | americas/united_states/san_francisco |rider-213|driver-213|19.179139106643607|\n|   20210825154123   | aa601d6b-7cc5-4b82-9687-675d0081616e | americas/united_states/san_francisco |rider-213|driver-213|93.56018115236618 |\n|   20210825154123   | 494bc080-881c-48be-8f8a-8f1739781816 | americas/united_states/san_francisco |rider-284|driver-284|90.9053809533154  |\n|   20210825154123   | 09573277-e1c1-4cdd-9b45-57176f184d4d | americas/united_states/san_francisco |rider-284|driver-284|49.527694252432056|\n|   20210825154123   | c9b055ed-cd28-4397-9704-93da8b2e601f | americas/brazil/sao_paulo            |rider-213|driver-213|43.4923811219014  |\n|   20210825154123   | e707355a-b8c0-432d-a80f-723b93dc13a8 | americas/brazil/sao_paulo            |rider-284|driver-284|63.72504913279929 |\n|   20210825154123   | d3c39c9e-d128-497a-bf3e-368882f45c28 | americas/brazil/sao_paulo            |rider-284|driver-284|91.99515909032544 |\n|   20210825154123   | 159441b0-545b-460a-b671-7cc2d509f47b | asia/india/chennai                   |rider-284|driver-284|9.384124531808036 |\n|   20210825154123   | 16031faf-ad8d-4968-90ff-16cead211d3c | asia/india/chennai                   |rider-284|driver-284|90.25710109008239 |\n+--------------------+--------------------------------------+--------------------------------------+---------+----------+------------------+\n</code></pre><p>And here are some sample records for a hudi table with virtual keys enabled.</p><pre><code>+--------------------+------------------------+-------------------------+---------+---------+-------------------+\n|_hoodie_commit_time |    _hoodie_record_key  |  _hoodie_partition_path |  rider  | driver  |        fare       |\n+--------------------+------------------------+-------------------------+---------+---------+-------------------+\n|        null        |            null        |          null           |rider-284|driver-284|98.3428192817987  |\n|        null        |            null        |          null           |rider-213|driver-213|19.179139106643607|\n|        null        |            null        |          null           |rider-213|driver-213|93.56018115236618 |\n|        null        |            null        |          null           |rider-284|driver-284|90.9053809533154  |\n|        null        |            null        |          null           |rider-284|driver-284|49.527694252432056|\n|        null        |            null        |          null           |rider-213|driver-213|43.4923811219014  |\n|        null        |            null        |          null           |rider-284|driver-284|63.72504913279929 |\n|        null        |            null        |          null           |rider-284|driver-284|91.99515909032544 |\n|        null        |            null        |          null           |rider-284|driver-284|9.384124531808036 |\n|        null        |            null        |          null           |rider-284|driver-284|90.25710109008239 |\n+--------------------+------------------------+-------------------------+---------+----------+------------------+\n</code></pre><p>:::note\nAs you could see, all meta fields are null in storage, but all users fields remain intact similar to a regular table.\n:::</p><h3>Incremental Queries</h3><p>Since hudi does not maintain any metadata (like commit time at a record level) for a table with virtual keys enabled,<br/>\n<!-- -->incremental queries are not supported. An exception will be thrown as below when an incremental query is triggered for such\na table.</p><pre><code>scala&gt; val tripsIncrementalDF = spark.read.format(&quot;hudi&quot;).\n     |   option(QUERY_TYPE_OPT_KEY, QUERY_TYPE_INCREMENTAL_OPT_VAL).\n     |   option(BEGIN_INSTANTTIME_OPT_KEY, &quot;20210827180901&quot;).load(basePath)\norg.apache.hudi.exception.HoodieException: Incremental queries are not supported when meta fields are disabled\n  at org.apache.hudi.IncrementalRelation.&lt;init&gt;(IncrementalRelation.scala:69)\n  at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:120)\n  at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:67)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:344)\n  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)\n  at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)\n  at scala.Option.getOrElse(Option.scala:189)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:232)\n  ... 61 elided\n</code></pre><h3>Conclusion</h3><p>Hope this blog was useful for you to learn yet another feature in Apache Hudi. If you are interested in\nHudi and looking to contribute, do check out <a href=\"https://hudi.apache.org/contribute/get-involved\">here</a>.</p>",
            "url": "https://hudi.apache.org/blog/2021/08/18/virtual-keys",
            "title": "Adding support for Virtual Keys in Hudi",
            "summary": "Apache Hudi helps you build and manage data lakes with different table types, config knobs to cater to everyone's need.",
            "date_modified": "2021-08-18T00:00:00.000Z",
            "author": {
                "name": "shivnarayan"
            }
        },
        {
            "id": "/2021/08/16/kafka-custom-deserializer",
            "content_html": "<p>The schema used for data exchange between services can change rapidly with new business requirements.\nApache Hudi is often used in combination with kafka as a event stream where all events are transmitted according to a record schema.\nIn our case a Confluent schema registry is used to maintain the schema and as schema evolves, newer versions are updated in the schema registry.</p><h2>What do we want to achieve?</h2><p>We have multiple instances of DeltaStreamer running, consuming many topics with different schemas ingesting to multiple Hudi tables. Deltastreamer is a utility in Hudi to assist in ingesting data from multiple sources like DFS, kafka, etc into Hudi. If interested, you can read more about DeltaStreamer tool <a href=\"https://hudi.apache.org/docs/writing_data#deltastreamer\">here</a>\nIdeally every topic should be able to evolve the schema to match new business requirements. Producers start producing data with a new schema version and the DeltaStreamer picks up the new schema and ingests the data with the new schema. For this to work, we run our DeltaStreamer instances with the latest schema version available from the Schema Registry to ensure that we always use the freshest schema with all attributes.\nA prerequisites is that all the mentioned Schema evolutions must be <code>BACKWARD_TRANSITIVE</code> compatible (see <a href=\"https://docs.confluent.io/platform/current/schema-registry/avro.html\">Schema Evolution and Compatibility of Avro Schema changes</a>. This ensures that every record in the kafka topic can always be read using the latest schema.</p><h2>What is the problem?</h2><p>The normal operation looks like this. Multiple (or a single) producers write records to the kafka topic.\nIn regular flow of events, all records are in the same schema v1 and is in sync with schema registry.\n<img src=\"/assets/images/blog/kafka-custom-deserializer/normal_operation.png\" alt=\"Normal operation\"/><br/>\nThings get complicated when a producer switches to a new Writer-Schema v2 (in this case <code>Producer A</code>). <code>Producer B</code> remains on Schema v1. E.g. an attribute <code>myattribute</code> was added to the schema, resulting in schema version v2.\nDeltastreamer is capable of handling such schema evolution, if all incoming records were evolved and serialized with evolved schema. But the complication is that, some records are serialized with schema version v1 and some are serialized with schema version v2.</p><p><img src=\"/assets/images/blog/kafka-custom-deserializer/schema_evolution.png\" alt=\"Schema evolution\"/><br/>\nThe default deserializer used by Hudi <code>io.confluent.kafka.serializers.KafkaAvroDeserializer</code> uses the schema that the record was serialized with for deserialization. This causes Hudi to get records with multiple different schema from the kafka client. E.g. Event #13 has the new attribute <code>myattribute</code>, Event #14 does not have the new attribute <code>myattribute</code>. This makes things complicated and error-prone for Hudi.</p><p><img src=\"/assets/images/blog/kafka-custom-deserializer/confluent_deserializer.png\" alt=\"Confluent Deserializer\"/><br/></p><h2>Solution</h2><p>Hudi added a new custom Deserializer <code>KafkaAvroSchemaDeserializer</code> to solve this problem of different producers producing records in different schema versions, but to use the latest schema from schema registry to deserialize all the records.<br/>\nAs first step the Deserializer gets the latest schema from the Hudi SchemaProvider. The SchemaProvider can get the schema for example from a Confluent Schema-Registry or a file.\nThe Deserializer then reads the records from the topic using the schema the record was written with. As next step it will convert all the records to the latest schema from the SchemaProvider, in our case the latest schema. As a result, the kafka client will return all records with a unified schema i.e. the latest schema as per schema registry. Hudi does not need to handle different schemas inside a single batch.</p><p><img src=\"/assets/images/blog/kafka-custom-deserializer/KafkaAvroSchemaDeserializer.png\" alt=\"KafkaAvroSchemaDeserializer\"/><br/></p><h2>Configurations</h2><p>As of upcoming release 0.9.0, normal Confluent Deserializer is used by default. One has to explicitly set KafkaAvroSchemaDeserializer as below,\nin order to ensure smooth schema evolution with different producers producing records in different versions.</p><p><code>hoodie.deltastreamer.source.kafka.value.deserializer.class=org.apache.hudi.utilities.deser.KafkaAvroSchemaDeserializer</code></p><h2>Conclusion</h2><p>Hope this blog helps in ingesting data from kafka into Hudi using Deltastreamer tool catering to different schema evolution\nneeds. Hudi has a very active development community and we look forward for more contributions.\nPlease check out <a href=\"https://hudi.apache.org/contribute/get-involved\">this</a> link to start contributing.</p>",
            "url": "https://hudi.apache.org/blog/2021/08/16/kafka-custom-deserializer",
            "title": "Schema evolution with DeltaStreamer using KafkaSource",
            "summary": "The schema used for data exchange between services can change rapidly with new business requirements.",
            "date_modified": "2021-08-16T00:00:00.000Z",
            "author": {
                "name": "sbernauer"
            }
        },
        {
            "id": "/2021/08/11/Cost-Efficient-Open-Source-Big-Data-Platform-at-Uber",
            "content_html": "<div url=\"https://eng.uber.com/cost-efficient-big-data-platform/\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2021/08/11/Cost-Efficient-Open-Source-Big-Data-Platform-at-Uber",
            "title": "Cost-Efficient Open Source Big Data Platform at Uber",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2021-08-11T00:00:00.000Z",
            "author": {
                "name": "Zheng Shao"
            }
        },
        {
            "id": "/2021/08/03/MLOps-Wars-Versioned-Feature-Data-with-a-Lakehouse",
            "content_html": "<div url=\"https://www.logicalclocks.com/blog/mlops-wars-versioned-feature-data-with-a-lakehouse\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2021/08/03/MLOps-Wars-Versioned-Feature-Data-with-a-Lakehouse",
            "title": "MLOps Wars: Versioned Feature Data with a Lakehouse",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2021-08-03T00:00:00.000Z",
            "author": {
                "name": "David Bzhalava"
            }
        },
        {
            "id": "/2021/07/26/Baixin-banksreal-time-data-lake-evolution-scheme-based-on-Apache-Hudi",
            "content_html": "<div url=\"https://developpaper.com/baixin-banks-real-time-data-lake-evolution-scheme-based-on-apache-hudi/\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2021/07/26/Baixin-banksreal-time-data-lake-evolution-scheme-based-on-Apache-Hudi",
            "title": "Baixin bank’s real-time data lake evolution scheme based on Apache Hudi",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2021-07-26T00:00:00.000Z"
        },
        {
            "id": "/2021/07/21/streaming-data-lake-platform",
            "content_html": "<p>As early as 2016, we set out a <a href=\"https://www.oreilly.com/content/ubers-case-for-incremental-processing-on-hadoop/\">bold, new vision</a> reimagining batch data processing through a new “<strong>incremental</strong>” data processing stack - alongside the existing batch and streaming stacks.\nWhile a stream processing pipeline does row-oriented processing, delivering a few seconds of processing latency, an incremental pipeline would apply the same principles to <em>columnar</em> data in the data lake,\ndelivering orders of magnitude improvements in processing efficiency within few minutes, on extremely scalable batch storage/compute infrastructure. This new stack would be able to effortlessly support regular batch processing for bulk reprocessing/backfilling as well.\nHudi was built as the manifestation of this vision, rooted in real, hard problems faced at <a href=\"https://eng.uber.com/uber-big-data-platform/\">Uber</a> and later took a life of its own in the open source community. Together, we have been able to\nusher in fully incremental data ingestion and moderately complex ETLs on data lakes already.</p><p><img src=\"/assets/images/blog/datalake-platform/hudi-data-lake-platform_-_Page_2_4.png\" alt=\"the different components that make up the stream and batch processing stack today, showing how an incremental stack blends the best of both the worlds.\"/></p><p>Today, this grand vision of being able to express almost any batch pipeline incrementally is more attainable than it ever was. Stream processing is <a href=\"https://flink.apache.org/blog/\">maturing rapidly</a> and gaining <a href=\"https://www.confluent.io/blog/every-company-is-becoming-software/\">tremendous momentum</a>,\nwith <a href=\"https://flink.apache.org/2021/03/11/batch-execution-mode.html\">generalization</a> of stream processing APIs to work over a batch execution model. Hudi completes the missing pieces of the puzzle by providing streaming optimized lake storage,\nmuch like how Kafka/Pulsar enable efficient storage for event streaming. <a href=\"https://hudi.apache.org/powered_by\">Many organizations</a> have already reaped real benefits of adopting a streaming model for their data lakes, in terms of fresh data, simplified architecture and great cost reductions.</p><p>But first, we needed to tackle the basics - transactions and mutability - on the data lake. In many ways, Apache Hudi pioneered the transactional data lake movement as we know it today. Specifically, during a time when more special-purpose systems were being born, Hudi introduced a server-less, transaction layer, which worked over the general-purpose Hadoop FileSystem abstraction on Cloud Stores/HDFS. This model helped Hudi to scale writers/readers to 1000s of cores on day one, compared to warehouses which offer a richer set of transactional guarantees but are often bottlenecked by the 10s of servers that need to handle them. We also experience a lot of joy to see similar systems (Delta Lake for e.g) later adopt the same server-less transaction layer model that we originally shared way back in <a href=\"https://eng.uber.com/hoodie/\">early &#x27;17</a>. We consciously introduced two table types Copy On Write (with simpler operability) and Merge On Read (for greater flexibility) and now these terms are used in <a href=\"https://github.com/apache/iceberg/pull/1862\">projects</a> outside Hudi, to refer to similar ideas being borrowed from Hudi. Through open sourcing and <a href=\"https://blogs.apache.org/foundation/entry/the-apache-software-foundation-announces64\">graduating</a> from the Apache Incubator, we have made some great progress elevating these ideas <a href=\"http://hudi.apache.org/docs/powered_by.html\">across the industry</a>, as well as bringing them to life with a cohesive software stack. Given the exciting developments in the past year or so that have propelled data lakes further mainstream, we thought some perspective can help users see Hudi with the right lens, appreciate what it stands for, and be a part of where it’s headed. At this time, we also wanted to shine some light on all the great work done by <a href=\"https://github.com/apache/hudi/graphs/contributors\">180+ contributors</a> on the project, working with more than 2000 unique users over slack/github/jira, contributing all the different capabilities Hudi has gained over the past years, from its humble beginnings.</p><p>This is going to be a rather long post, but we will do our best to make it worth your time. Let’s roll.</p><h2>Data Lake Platform</h2><p>We have noticed that, Hudi is sometimes positioned as a “<a href=\"https://cloud.google.com/blog/products/data-analytics/getting-started-with-new-table-formats-on-dataproc\">table format</a>” or “transactional layer”. While this is not incorrect, this does not do full justice to all that Hudi has to offer. </p><h3>Is Hudi a “format”?</h3><p>Hudi was not designed as a general purpose table format, tracking files/folders for batch processing. Rather, the functionality provided by a table format is merely one layer in the Hudi software stack. Hudi was designed to play well with the Hive format (if you will), given how popular and widespread it is. Over time, to solve scaling challenges or bring in additional functionality, we have invested in our own native table format with an eye for incremental processing vision. for e.g, we need to support shorter transactions that commit every few seconds. We believe these requirements would fully subsume challenges solved by general purpose table formats over time. But, we are also open to plugging in or syncing to other open table formats, so their users can also benefit from the rest of the Hudi stack. Unlike the file formats, a table format is merely a representation of table metadata and it’s actually quite possible to translate from Hudi to other formats/vice versa if users are willing to accept the trade-offs.</p><h3>Is Hudi a transactional layer?</h3><p>Of course, Hudi had to provide transactions for implementing deletes/updates, but Hudi’s transactional layer is designed around an <a href=\"https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying\">event log</a> that is also well-integrated with an entire set of built-in table/data services. For e.g compaction is aware of clustering actions already scheduled and optimizes by skipping over the files being clustered - while the user is blissfully unaware of all this. Hudi also provides out-of-box tools for ingesting, ETLing data, and much more. We have always been thinking of Hudi as solving a database problem around stream processing - areas that are actually <a href=\"https://www.infoq.com/presentations/streaming-databases/\">very related to each other</a>. In fact, Stream processing is enabled by logs (capture/emit event streams, rewind/reprocess) and databases (state stores, updatable sinks). With Hudi, the idea was that if we build a database supporting efficient updates and extracting data streams while remaining optimized for large batch queries, incremental pipelines can be built using Hudi tables as state store &amp; update-able sinks. </p><p>Thus, the best way to describe Apache Hudi is as a <strong>Streaming Data Lake Platform</strong> built around a <em>database kernel</em>. The words carry significant meaning.</p><p><img src=\"/assets/images/blog/datalake-platform/Screen_Shot_2021-07-20_at_5.35.47_PM.png\" alt=\"/assets/images/blog/datalake-platform/Screen_Shot_2021-07-20_at_5.35.47_PM.png\"/></p><p><strong>Streaming</strong>: At its core, by optimizing for fast upserts &amp; change streams, Hudi provides the primitives to data lake workloads that are comparable to what <a href=\"https://kafka.apache.org/\">Apache Kafka</a> does for event-streaming (namely, incremental produce/consume of events and a state-store for interactive querying).</p><p><strong>Data Lake</strong>: Nonetheless, Hudi provides an optimized, self-managing data plane for large scale data processing on the lake (adhoc queries, ML pipelines, batch pipelines), powering arguably the <a href=\"https://eng.uber.com/apache-hudi-graduation/\">largest transactional lake</a> in the world. While Hudi can be used to build a <a href=\"https://databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html\">lakehouse</a>, given its transactional capabilities, Hudi goes beyond and unlocks an end-to-end streaming architecture. In contrast, the word “streaming” appears just 3 times in the lakehouse <a href=\"http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf\">paper</a>, and one of them is talking about Hudi.</p><p><strong>Platform</strong>: Oftentimes in open source, there is great tech, but there is just too many of them - all differing ever so slightly in their opinionated ways, ultimately making the integration task onerous on the end user. Lake users deserve the same great usability that cloud warehouses provide, with the additional freedom and transparency of a true open source community. Hudi’s data and table services, tightly integrated with the Hudi “kernel”, gives us the ability to deliver cross layer optimizations with reliability and ease of use.</p><h2>Hudi Stack</h2><p>The following stack captures layers of software components that make up Hudi, with each layer depending on and drawing strength from the layer below. Typically, data lake users write data out once using an open file format like Apache <a href=\"http://parquet.apache.org/\">Parquet</a>/<a href=\"https://orc.apache.org/\">ORC</a> stored on top of extremely scalable cloud storage or distributed file systems. Hudi provides a self-managing data plane to ingest, transform and manage this data, in a way that unlocks incremental data processing on them.</p><p><img src=\"/assets/images/blog/datalake-platform/hudi-data-lake-platform_-_Copy_of_Page_1_3.png\" alt=\"Figure showing the Hudi stack\"/></p><p>Furthermore, Hudi either already provides or plans to add components that make this data universally accessible to all the different query engines out there. The features annotated with <code>*</code> represent work in progress and dotted boxes represent planned future work, to complete our vision for the project.\nWhile we have strawman designs outlined for the newer components in the blog, we welcome with open arms fresh perspectives from the community.\nRest of the blog will delve into each layer in our stack - explaining what it does, how it&#x27;s designed for incremental processing and how it will evolve in the future.</p><h2>Lake Storage</h2><p>Hudi interacts with lake storage using the <a href=\"https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/FileSystem.html\">Hadoop FileSystem API</a>, which makes it compatible with all of its implementations ranging from HDFS to Cloud Stores to even in-memory filesystems like <a href=\"https://www.alluxio.io/blog/building-high-performance-data-lake-using-apache-hudi-and-alluxio-at-t3go/\">Alluxio</a>/Ignite. Hudi internally implements its own <a href=\"https://github.com/apache/hudi/blob/9d2a65a6a6ff9add81411147f1cddd03f7c08e6c/hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java\">wrapper filesystem</a> on top to provide additional storage optimizations (e.g: file sizing), performance optimizations (e.g: buffering), and metrics. Uniquely, Hudi takes full advantage of append support, for storage schemes that support it, like HDFS. This helps Hudi deliver streaming writes without causing an explosion in file counts/table metadata. Unfortunately, most cloud/object storages do not offer append capability today (except maybe <a href=\"https://azure.microsoft.com/en-us/updates/append-blob-support-in-azure-data-lake-storage-is-now-generally-available/\">Azure</a>). In the future, we plan to leverage the lower-level APIs of major cloud object stores, to provide similar controls over file counts at streaming ingest latencies.</p><h2>File Format</h2><p>Hudi is designed around the notion of base file and delta log files that store updates/deltas to a given base file (called a file slice). Their formats are pluggable, with Parquet (columnar access) and HFile (indexed access) being the supported base file formats today. The delta logs encode data in <a href=\"http://avro.apache.org/\">Avro</a> (row oriented) format for speedier logging (just like Kafka topics for e.g). Going forward, we plan to <a href=\"https://github.com/apache/hudi/pull/3228\">inline any base file format</a> into log blocks in the coming releases, providing columnar access to delta logs depending on block sizes. Future plans also include Orc base/log file formats, unstructured data formats (free form json, images), and even tiered storage layers in event-streaming systems/OLAP engines/warehouses, work with their native file formats.</p><p>Zooming one level up, Hudi&#x27;s unique file layout scheme encodes all changes to a given base file, as a sequence of blocks (data blocks, delete blocks, rollback blocks) that are merged in order to derive newer base files. In essence, this makes up a self contained redo log that the lets us implement interesting features on top. For e.g, most of today&#x27;s data privacy enforcement happens by masking data read off the lake storage on-the-fly, invoking hashing/encryption algorithms over and over on the same set of records and incurring significant compute overhead/cost. Users would be able to keep multiple pre-masked/encrypted copies of the same key in the logs and hand out the correct one based on a policy, avoiding all the overhead.</p><p><img src=\"/assets/images/blog/datalake-platform/hudi-design-diagrams_-_Page_2_1.png\" alt=\"Hudi base and delta logs\"/></p><h2>Table Format</h2><p>The term “table format” is new and still means many things to many people. Drawing an analogy to file formats, a table format simply consists of : the file layout of the table, table’s schema and metadata tracking changes to the table. Hudi is not a table format, it implements one internally. Hudi uses Avro schemas to store, manage and evolve a table’s schema. Currently, Hudi enforces schema-on-write, which although stricter than schema-on-read, is adopted <a href=\"https://docs.confluent.io/platform/current/schema-registry/avro.html\">widely</a> in the stream processing world to ensure pipelines don&#x27;t break from non backwards compatible changes.</p><p>Hudi consciously lays out files within a table/partition into groups and maintains a mapping between an incoming record’s key to an existing file group. All updates are recorded into delta log files specific to a given file group and this design ensures low merge overhead compared to approaches like Hive ACID, which have to merge all delta records against all base files to satisfy queries. For e.g, with uuid keys (used very widely) all base files are very likely to overlap with all delta logs, rendering any range based pruning useless. Much like state stores, Hudi’s design anticipates fast key based upserts/deletes and only requires merging delta logs within each file group. This design choice also lets Hudi provide more capabilities for writing/querying as we will explain below. </p><p><img src=\"/assets/images/blog/datalake-platform/hudi-design-diagrams-table-format.png\" alt=\"Shows the Hudi table format components\"/></p><p>The <em>timeline</em> is the source-of-truth event log for all Hudi’s table metadata, stored under the <code>.hoodie</code> folder, that provides an ordered log of all actions performed on the table. Events are retained on the timeline up to a configured interval of time/activity. Each file group is also designed as it’s own self-contained log, which means that even if an action that affected a file group is archived from the timeline, the right state of the records in each file group can be reconstructed by simply locally applying the delta logs to the base file. This design bounds the metadata size, proportional to how often the table is being written to/operated on, independent of how large the entire table is. This is a critical design element needs for supporting frequent writes/commits to tables.</p><p>Lastly, new events on the timeline are then consumed and reflected onto an internal metadata table, implemented as another merge-on-read table offering low write amplification. Hudi is able to absorb quick/rapid changes to table’s metadata, unlike table formats designed for slow-moving data. Additionally, the metadata table uses the <a href=\"https://hbase.apache.org/2.0/devapidocs/org/apache/hadoop/hbase/io/hfile/HFile.html\">HFile</a> base file format, which provides indexed lookups of keys avoiding the need for reading the entire metadata table to satisfy metadata reads. It currently stores all the physical file paths that are part of the table, to avoid expensive cloud file listings.</p><p>A key challenge faced by all the table formats out there today, is the need for expiring snapshots/controlling retention for time travel queries such that it does not interfere with query planning/performance. In the future, we plan to build an indexed timeline in Hudi, which can span the entire history of the table, supporting a time travel look back window of several months/years.</p><h2>Indexes</h2><p>Indexes help databases plan better queries, that reduce the overall amount of I/O and deliver faster response times. Table metadata about file listings and column statistics are often enough for lake query engines to generate optimized, engine specific query plans quickly. This is however not sufficient for Hudi to realize fast upserts. Hudi already supports different key based indexing schemes to quickly map incoming record keys into the file group they reside in. For this purpose, Hudi exposes a pluggable indexing layer to the writer implementations, with built-in support for range pruning (when keys are ordered and largely arrive in order) using interval trees and bloom filters (e.g: for uuid based keys where ordering is of very little help). Hudi also implements a HBase backed external index which is much more performant although more expensive to operate. Hudi also consciously exploits the partitioning scheme of the table to implement global and non-global indexing schemes. Users can choose to enforce key constraints only within a partition, in return for <em><code>O(num_affected_partitions)</code></em> upsert performance as opposed to <em><code>O(total_partitions)</code></em> in the global indexing scenarios. We refer you to this <a href=\"http://hudi.apache.org/blog/2020/11/11/hudi-indexing-mechanisms\">blog</a>, that goes over indexing in detail. Ultimately, Hudi&#x27;s writer path ensures the index is always kept in sync with the timeline and data, which is cumbersome and error prone to implement on top of a table format by hand.</p><p><img src=\"/assets/images/blog/datalake-platform/hudi-design-diagrams_-_Page_5.png\" alt=\"/assets/images/blog/datalake-platform/hudi-design-diagrams_-_Page_5.png\"/></p><p>In the future, we intend to add additional forms of indexing as new partitions on the metadata table. Let’s discuss the role  each one has to play briefly. Query engines typically rely on partitioning to cut down the number of files read for a given query. In database terms, a Hive partition is nothing but a coarse range index, that maps a set of columns to a list of files. Table formats born in the cloud like Iceberg/Delta Lake, have built-in tracking of column ranges per file in a single flat file (json/avro), that helps avoid planning costs for large/poorly sized tables. This need has been largely reduced for Hudi tables thus far, given Hudi automatically enforces file sizes which help bound time taken to read out stats from parquet footers for e.g. However, with the advent of features like clustering, there is a need for writing smaller files first and then reclustering in a query optimized way. We plan to add indexed column ranges, that can scale to lots of small files and support faster mutations . See <a href=\"https://cwiki.apache.org/confluence/display/HUDI/RFC-27+Data+skipping+index+to+improve+query+performance\">RFC-27</a> to track the design process and get involved.</p><p>While Hudi already supports external indexes for random write workloads, we would like to support <a href=\"https://github.com/apache/hudi/pull/2487\">point-lookup-ish queries</a> right on top of lake storage, which helps avoid the overhead of an additional database for many classes of data applications. We also anticipate that uuid/key based joins will be sped up a lot, by leveraging record level indexing schemes, we build out for fast upsert performance. We also plan to move our tracking of bloom filters out of the file footers and into its <a href=\"https://issues.apache.org/jira/browse/HUDI-1295\">own partition</a> on the metadata table. Ultimately, we look to exposing all of this to the queries as well in the coming releases.</p><h2>Concurrency Control</h2><p>Concurrency control defines how different writers/readers coordinate access to the table. Hudi ensures atomic writes, by way of publishing commits atomically to the timeline, stamped with an instant time that denotes the time at which the action is deemed to have occurred. Unlike general purpose file version control, Hudi draws clear distinction between writer processes (that issue user’s upserts/deletes), table services (that write data/metadata to optimize/perform bookkeeping) and readers (that execute queries and read data). Hudi provides snapshot isolation between all three types of processes, meaning they all operate on a consistent snapshot of the table. Hudi provides <a href=\"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+22+%3A+Snapshot+Isolation+using+Optimistic+Concurrency+Control+for+multi-writers\">optimistic concurrency control</a> (OCC) between writers, while providing lock-free, non-blocking MVCC  based concurrency control between writers and table-services and between different table services.</p><p>Projects that solely rely on OCC deal with competing operations, by either implementing a lock or relying on atomic renames. Such approaches are optimistic that real contention never happens and resort to failing one of the writer operations if conflicts occur, which can cause significant resource wastage or operational overhead. Imagine a scenario of two writer processes : an ingest writer job producing new data every 30 minutes and a deletion writer job that is enforcing GDPR taking 2 hours to issue deletes. If there were to overlap on the same files (very likely to happen in real situations with random deletes), the deletion job is almost guaranteed to starve and fail to commit each time, wasting tons of cluster resources. Hudi takes a very different approach that we believe is more apt for lake transactions, which are typically long-running. For e.g async compaction that can keep deleting records in the background without blocking the ingest job. This is implemented via a file level, log based concurrency control protocol which orders actions based on their start instant times on the timeline.</p><p><img src=\"/assets/images/blog/datalake-platform/Hudi_design_diagram_-_Page_2_1.png\" alt=\"Figure showing competing transactions leading to starvation with just OCC\"/></p><p>We are hard at work, improving our OCC based implementation around early detection of conflicts for concurrent writers and terminate early without burning up CPU resources. We are also working on <a href=\"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+22+%3A+Snapshot+Isolation+using+Optimistic+Concurrency+Control+for+multi-writers#RFC22:SnapshotIsolationusingOptimisticConcurrencyControlformultiwriters-FutureWork(LockFree-ishConcurrencyControl)\">adding fully log based</a>, non-blocking concurrency control between writers, where writers proceed to write deltas and conflicts are resolved later in some deterministic timeline order - again much like how stream processing programs are written. This is possible only due to Hudi’s unique design that sequences actions into an ordered event log and the transaction handling code is aware of the relationship/interdependence of actions to each other.</p><h2>Writers</h2><p>Hudi tables can be used as sinks for Spark/Flink pipelines and the Hudi writing path provides several enhanced capabilities over file writing done by vanilla parquet/avro sinks. Hudi classifies write operations carefully into incremental (<code>insert</code>, <code>upsert</code>, <code>delete</code>) and batch/bulk operations (<code>insert_overwrite</code>, <code>insert_overwrite_table</code>, <code>delete_partition</code>, <code>bulk_insert</code>) and provides relevant functionality for each operation in a performant and cohesive way. Both upsert and delete operations automatically handle merging of records with the same key in the input stream (say, a CDC stream obtained from upstream table) and then lookup the index, finally invoke a bin packing algorithm to pack data into files, while <a href=\"http://hudi.apache.org/blog/2021/03/01/hudi-file-sizing\">respecting a pre-configured target file size</a>. An insert operation on the other hand, is intelligent enough to avoid the precombining and index lookup, while retaining the benefits of the rest of the pipeline. Similarly, bulk_insert operation provides several sort modes for controlling initial file sizes and file counts, when importing data from an external table to Hudi. The other batch write operations provide MVCC based implementations of typical overwrite semantics used in batch data pipelines, while retaining all the transactional and incremental processing capabilities, making it seamless to switch between incremental pipelines for regular runs and batch pipelines for backfilling/dropping older partitions. The write pipeline also contains lower layers optimizations around handling large merges by spilling to <a href=\"https://rocksdb.org/\">rocksDB</a> or an external spillable map, multi-threaded/concurrent I/O to improve write performance.</p><p>Keys are first class citizens inside Hudi and the pre-combining/index lookups done before upsert/deletes ensure a key is unique across partitions or within partitions, as desired. In contrast with other approaches where this is left to data engineer to co-ordinate using <code>MERGE INTO</code> statements, this approach ensures quality data especially for critical use-cases. Hudi also ships with several <a href=\"http://hudi.apache.org/blog/2021/02/13/hudi-key-generators/\">built-in key generators</a> that can parse all common date/timestamps, handle malformed data with an extensible framework for defining custom key generators. Keys are also materialized with the records using the <code>_hoodie_record_key</code> meta column, which makes it possible to change the key fields and perform repairs on older data with incorrect keys for e.g. Finally, Hudi provides a <code>HoodieRecordPayload</code> interface is very similar to processor APIs in Flink or Kafka Streams, and allows for expressing arbitrary merge conditions, between the base and delta log records. This allows users to express partial merges (e.g log only updated columns to the delta log for efficiency) and avoid reading all the base records before every merge. Routinely, we find users leverage such custom merge logic during replaying/backfilling older data onto a table, while ensuring newer updates are not overwritten causing the table&#x27;s snapshot to go back in time. This is achieved by simply using the  <code>HoodieDefaultPayload</code> where latest value for a given key is picked based a configured precombine field value in the data.</p><p>Hudi writers add metadata to each record, that codify the commit time and a sequence number for each record within that commit (comparable to a Kafka offset), which make it possible to derive record level change streams. Hudi also provides users the ability to specify event time fields in incoming data streams and track them in the timeline.Mapping these to stream processing concepts, Hudi contains both <a href=\"https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/\">arrival and event time</a> for records for each commit, that can help us build good <a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/datastream/event-time/generating_watermarks/\">watermarks</a> that inform complex incremental processing pipelines. In the near future, we are looking to add new metadata columns, that encode the source operation (insert, update, delete) for each record, before we embark on this grand goal of full end-end incremental ETL pipelines. All said, we realized many users may simply want to use Hudi as an efficient write layer that supports transactions, fast updates/deletes. We are looking into adding support for <a href=\"https://github.com/apache/hudi/pull/3306\">virtual keys</a> and making the <a href=\"https://github.com/apache/hudi/pull/3247\">meta columns optional</a>, to lower storage overhead, while still making rest of Hudi&#x27;s capabilities (metadata table, table services, ..) available.</p><h2>Readers</h2><p>Hudi provides snapshot isolation between writers and readers and allows for any table snapshot to be queries consistently from all major lake query engines (Spark, Hive, Flink, Presto, Trino, Impala) and even cloud warehouses like Redshift. In fact, we would love to bring Hudi tables as external tables with BigQuery/Snowflake as well, once they also embrace the lake table formats more natively. Our design philosophy around query performance has been to make Hudi as lightweight as possible whenever only base columnar files are read (CoW  snapshot, MOR read-optimized queries),  employing the engine specific vectorized readers in Presto, Trino, Spark for e.g to be employed. This model is far more scalable than maintaining our own readers and users to benefit from engine specific optimizations. For e.g <a href=\"https://prestodb.io/blog/2021/02/04/raptorx\">Presto</a>, <a href=\"https://trino.io/docs/current/connector/hive-caching.html\">Trino</a> all have their own data/metadata caches. Whenever, Hudi has to merge base and log files for a query, Hudi takes control and employs several mechanisms (spillable maps, lazy reading) to improve merge performance, while also providing a read-optimized query on the data that trades off data freshness for query performance. In the near future, we are investing deeply into improving MoR snapshot query performance in many ways such as inlining parquet data, special handling of overwrite payloads/merges. </p><p><img src=\"/assets/images/blog/datalake-platform/hudi-design-diagram_-incr-read.png\" alt=\"Log merging done for incremental queries\"/></p><p>True to its design goals, Hudi provides some very powerful incremental querying capabilities that tied together the meta fields added during writing and the file group based storage layout. While table formats that merely track files, are only able to provide information about files that changed during each snapshot or commits, Hudi generates the exact set of records that changed given a point in the timeline, due to tracking of record level event and arrival times. Further more, this design allows large commits to be consumed in smaller chunks by an incremental query, fully decoupling the writing and incremental querying of data. Time travel is merely implemented as an incremental query that starts and stops at an older portion of the timeline. Since Hudi ensures that a key is atomically mapped to a single file group at any point in time, it makes it possible to support full CDC capabilities on Hudi tables, such as providing all possible values for a given record since time <code>t</code>, CDC streams with both before and after images. All of these functionalities can be built local to each file group, given each file group is a self-contained log. Much of our future work in this area will be around bringing such a powerful set of <a href=\"https://debezium.io/\">debezium</a> like capabilities to life in the coming months. </p><h2>Table Services</h2><p>What defines and sustains a project’s value over years are its fundamental design principles and the subtle trade offs. Databases often consist of several internal components, working in tandem to deliver efficiency, performance and great operability to its users. True to intent to act as state store for incremental data pipelines, we designed Hudi with built-in table services and self-managing runtime that can orchestrate/trigger these services to optimize everything internally. In fact, if we compare rocksDB (a very popular stream processing state-store) and Hudi’s components, the similarities become obvious.</p><p><img src=\"/assets/images/blog/datalake-platform/hudi-design-diagrams_-_Page_4.png\" alt=\"/assets/images/blog/datalake-platform/hudi-design-diagrams_-_Page_4.png\"/></p><p>There are several built-in table services, all with the goal of ensuring performant table storage layout and metadata management, which are automatically invoked either synchronously after each write operation, or asynchronously as a separate background job. Furthermore, Spark (and Flink) streaming writers can run in continuous mode, and invoke table services asynchronously sharing the underlying executors intelligently with writers. Archival service ensures that the timeline holds sufficient history for inter service co-ordination (e.g compactions wait for other compactions to complete on the same file group), incremental queries. Once events expire from the timeline, the archival service cleans up any side-effects from lake storage (e.g. rolling back of failing concurrent transactions). Hudi&#x27;s transaction management implementation allows all of these services to be idempotent and thus resilient to failure via just simple retries.  <a href=\"http://hudi.apache.org/blog/2021/06/10/employing-right-configurations-for-hudi-cleaner\">Cleaner</a> service works off the timeline incrementally (eating our own incremental design dog food), removing file slices that are past the configured retention period for incremental queries, while also allowing sufficient time for long running batch jobs (e.g Hive ETLs) to finish running. Compaction service comes with built-in strategies (date partitioning based, I/O bounded), that merges a base file with a set of delta log files to produce new base file, all while allowing writes to happen concurrently to the file group. This is only possible due to Hudi&#x27;s grouping of files into groups and support for flexible log merging, and unlocks non-blocking execution of deletes while concurrent updates are being issues to the same set of records. <a href=\"http://hudi.apache.org/blog/2021/01/27/hudi-clustering-intro/\">Clustering</a> service functions similar to what users find in BigQuery or Snowflake, where users can group records that are often queried together by sort keys or control file sizes by coalescing smaller base files into larger ones. Clustering is fully aware of other actions on the timeline such as cleaning, compaction, and it helps Hudi implement intelligent optimizations like avoiding compaction on file groups that are already being clustered, to save on I/O. Hudi also performs rollback of partial writes and cleans up any uncommitted data from lake storage, by use of marker files that track any files created as a part of write operations. Finally, the bootstrap service performs one time zero copy migration of plain parquet tables to Hudi, while allowing both pipelines to operate in parallel, for data validation purposes. Cleaner service is once again aware of these bootstrapped base files and can optionally clean them up, to ensure use-cases like GDPR compliance are met.</p><p>We are always looking for ways to improve and enhance our table services in meaningful ways. In the coming releases, we are working towards a much more <a href=\"https://github.com/apache/hudi/pull/3233\">scalable model</a> of cleaning up partial writes, by consolidating marker file creation using our timeline metaserver, which avoids expensive full table scans to seek out and remove uncommitted files. We also have <a href=\"https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=181307144\">various proposals</a> to add more clustering schemes, unlock clustering with concurrent updates using fully log based concurrency control. </p><h2>Data Services</h2><p>As noted at the start, we wanted to make Hudi immediately usable for common end-end use-cases and thus invested deeply into a set of data services, that provide functionality that is data/workload specific, sitting on top of the table services, writers/readers directly. Foremost in that list, is the Hudi DeltaStreamer utility, which has been an extremely popular choice for painlessly building a data lake out of  Kafka streams and files landing in different formats on top of lake storage. Over time, we have also built out sources that cover all major systems like a JDBC source for RDBMS/other warehouses, Hive source and even incrementally pulling data from other Hudi tables. The utility supports automatic checkpoint management tracking source checkpoints as a part of target Hudi table metadata, with support for backfills/one-off runs. DeltaStreamer also integrates with major schema registries such as Confluent&#x27;s and also provides checkpoint translation from other popular mechanisms like Kafka connect. It also supports de-duplication of data, multi-level configuration management system, built in transformers that take arbitrary SQL or coerce <a href=\"http://hudi.apache.org/blog/2020/10/19/hudi-meets-aws-emr-and-aws-dms/\">CDC log changes</a> into writable forms, that combined with other aforementioned features can be used for deploying production grade incremental pipelines. Finally, just like the Spark/Flink streaming writers, DeltaStreamer is able to run in a continuous mode, with automatic management of table services. Hudi also provides several other tools for snapshotting and incrementally exporting Hudi tables, also importing/<a href=\"http://hudi.apache.org/blog/2020/03/22/exporting-hudi-datasets/\">exporting</a>/bootstrapping new tables into Hudi. Hudi also provides commit notifications into Http endpoints or Kafka topics, about table commit activity, which can be used for analytics or building data sensors in workflow managers like Airflow to trigger pipelines.</p><p><img src=\"/assets/images/blog/datalake-platform/hudi-design-diagrams_-_Page_8.png\" alt=\"/assets/images/blog/datalake-platform/hudi-design-diagrams_-_Page_8.png\"/></p><p>Going forward, we would love contributions to enhance our <a href=\"http://hudi.apache.org/blog/2020/08/22/ingest-multiple-tables-using-hudi/\">multi delta streamer utility</a>, which can ingest entire Kafka clusters in a single large Spark application, to be on par and hardened. To further our progress towards end-end complex incremental pipelines, we plan to work towards enhancing the delta streamer utility and its SQL transformers to be triggered by multiple source streams (as opposed to just the one today) and unlock materialized views at scale. We would like to bring an array of useful transformers that perform masking or data monitoring, and extend support for egress of data off Hudi tables into other external sinks as well. Finally, we would love to merge the FlinkStreamer and the DeltaStreamer utilities into one cohesive utility, that can be used across engines. We are constantly improving existing sources (e.g support for parallelized listings of DFS sources) and adding new ones (e.g S3 event based DFS source)</p><h2>Timeline Metaserver</h2><p>Storing and serving table metadata right on the lake storage is scalable, but can be much less performant compared to RPCs against a scalable meta server. Most cloud warehouses internally are built on a metadata layer that leverages an external database (e.g <a href=\"https://www.snowflake.com/blog/how-foundationdb-powers-snowflake-metadata-forward/\">Snowflake uses foundationDB</a>). Hudi also provides a metadata server, called the “Timeline server”, which offers an alternative backing store for Hudi’s table metadata. Currently, the timeline server runs embedded in the Hudi writer processes, serving file listings out of a local rocksDB store/<a href=\"https://javalin.io/\">Javalin</a> REST API during the write process, without needing to repeatedly list the cloud storage. Given we have hardened this as the default option since our 0.6.0 release, we are considering standalone timeline server installations, with support for horizontal scaling, database/table mappings, security and all the features necessary to turn it into a highly performant next generation lake metastore.</p><p><img src=\"/assets/images/blog/datalake-platform/hudi-design-diagrams_-_Page_6.png\" alt=\"/assets/images/blog/datalake-platform/hudi-design-diagrams_-_Page_6.png\"/></p><h2>Lake Cache</h2><p>There is a fundamental tradeoff today in data lakes between faster writing and great query performance. Faster writing typically involves writing smaller files (and later clustering them) or logging deltas (and later merging on read). While this provides good performance already, the pursuit of great query performance often warrants opening fewer number of files/objects on lake storage and may be pre-materializing the merges between base and delta logs. After all, most databases employ a <a href=\"https://dev.mysql.com/doc/refman/8.0/en/innodb-buffer-pool.html\">buffer pool</a> or <a href=\"https://github.com/facebook/rocksdb/wiki/Block-Cache\">block cache</a>, to amortize the cost of accessing storage. Hudi already contains several design elements that are conducive for building a caching tier (write-through or even just populated by an incremental query), that will be multi-tenant and can cache pre-merged images of the latest file slices, consistent with the timeline. Hudi timeline can be used to simply communicate caching policies, just like how we perform inter table service co-ordination. Historically, caching has been done closer to the query engines or via intermediate in-memory file systems. By placing a caching tier closer and more tightly integrated with a transactional lake storage like Hudi, all query engines would be able to share and amortize the cost of the cache, while supporting updates/deletes as well. We look forward to building a buffer pool for the lake that works across all major engines, with the contributions from the rest of the community. </p><p><img src=\"/assets/images/blog/datalake-platform/hudi-design-diagrams_-_Page_7.png\" alt=\"/assets/images/blog/datalake-platform/hudi-design-diagrams_-_Page_7.png\"/></p><h2>Onwards</h2><p>We hope that this blog painted a complete picture of Apache Hudi, staying true to its founding principles. Interested users and readers can expect blogs delving into each layer of the stack and an overhaul of our docs along these lines in the coming weeks/months. We view the current efforts around table formats as merely removing decade-old bottlenecks in data lake storage/query planes, problems which have been already solved very well in cloud warehouses like Big Query/Snowflake. We would like to underscore that our vision here is much greater, much more technically challenging. We as an industry are just wrapping our heads around many of these deep, open-ended problems, that need to be solved to marry stream processing and data lakes, with scale and simplicity. We hope to continue to put community first and build/solve these hard problems together. If these challenges excite you and you would like to build for that exciting future, please come join our <a href=\"http://hudi.apache.org/contribute/get-involved\">community</a>.</p>",
            "url": "https://hudi.apache.org/blog/2021/07/21/streaming-data-lake-platform",
            "title": "Apache Hudi - The Data Lake Platform",
            "summary": "As early as 2016, we set out a bold, new vision reimagining batch data processing through a new “incremental” data processing stack - alongside the existing batch and streaming stacks.",
            "date_modified": "2021-07-21T00:00:00.000Z",
            "author": {
                "name": "vinoth"
            }
        },
        {
            "id": "/2021/07/16/Amazon-Athena-expands-Apache-Hudi-support",
            "content_html": "<div url=\"https://aws.amazon.com/about-aws/whats-new/2021/07/amazon-athena-expands-apache-hudi-support/\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2021/07/16/Amazon-Athena-expands-Apache-Hudi-support",
            "title": "Amazon Athena expands Apache Hudi support",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2021-07-16T00:00:00.000Z"
        },
        {
            "id": "/2021/07/16/Query-apache-hudi-dataset-in-an-amazon-S3-data-lake-with-amazon-athena-Read-optimized-queries",
            "content_html": "<div url=\"https://aws.amazon.com/blogs/big-data/part-1-query-an-apache-hudi-dataset-in-an-amazon-s3-data-lake-with-amazon-athena-part-1-read-optimized-queries/\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2021/07/16/Query-apache-hudi-dataset-in-an-amazon-S3-data-lake-with-amazon-athena-Read-optimized-queries",
            "title": "Part1: Query apache hudi dataset in an amazon S3 data lake with amazon athena : Read optimized queries",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2021-07-16T00:00:00.000Z",
            "author": {
                "name": "Dhiraj Thakur"
            }
        },
        {
            "id": "/2021/06/10/employing-right-configurations-for-hudi-cleaner",
            "content_html": "<p>Apache Hudi provides snapshot isolation between writers and readers. This is made possible by Hudi’s MVCC concurrency model. In this blog, we will explain how to employ the right configurations to manage multiple file versions. Furthermore, we will discuss mechanisms available to users on how to maintain just the required number of old file versions so that long running readers do not fail. </p><h3>Reclaiming space and keeping your data lake storage costs in check</h3><p>Hudi provides different table management services to be able to manage your tables on the data lake. One of these services is called the <strong>Cleaner</strong>. As you write more data to your table, for every batch of updates received, Hudi can either generate a new version of the data file with updates applied to records (COPY_ON_WRITE) or write these delta updates to a log file, avoiding rewriting newer version of an existing file (MERGE_ON_READ). In such situations, depending on the frequency of your updates, the number of file versions of log files can grow indefinitely. If your use-cases do not require keeping an infinite history of these versions, it is imperative to have a process that reclaims older versions of the data. This is Hudi’s cleaner service.</p><h3>Problem Statement</h3><p>In a data lake architecture, it is a very common scenario to have readers and writers concurrently accessing the same table. As the Hudi cleaner service periodically reclaims older file versions, scenarios arise where a long running query might be accessing a file version that is deemed to be reclaimed by the cleaner. Here, we need to employ the correct configs to ensure readers (aka queries) don’t fail.</p><h3>Deeper dive into Hudi Cleaner</h3><p>To deal with the mentioned scenario, lets understand the  different cleaning policies that Hudi offers and the corresponding properties that need to be configured. Options are available to schedule cleaning asynchronously or synchronously. Before going into more details, we would like to explain a few underlying concepts:</p><ul><li><strong>Hudi base file</strong>: Columnar file which consists of final data after compaction. A base file’s name follows the following naming convention: <code>&lt;fileId&gt;_&lt;writeToken&gt;_&lt;instantTime&gt;.parquet</code>. In subsequent writes of this file, file id remains the same and commit time gets updated to show the latest version. This also implies any particular version of a record, given its partition path, can be uniquely located using the file id and instant time. </li><li><strong>File slice</strong>: A file slice consists of the base file and any log files consisting of the delta, in case of MERGE_ON_READ table type.</li><li><strong>Hudi File Group</strong>: Any file group in Hudi is uniquely identified by the partition path and the  file id that the files in this group have as part of their name. A file group consists of all the file slices in a particular partition path. Also any partition path can have multiple file groups.</li></ul><h3>Cleaning Policies</h3><p>Hudi cleaner currently supports below cleaning policies:</p><ul><li><strong>KEEP_LATEST_COMMITS</strong>: This is the default policy. This is a temporal cleaning policy that ensures the effect of having lookback into all the changes that happened in the last X commits. Suppose a writer is ingesting data  into a Hudi dataset every 30 minutes and the longest running query can take 5 hours to finish, then the user should retain atleast the last 10 commits. With such a configuration, we ensure that the oldest version of a file is kept on disk for at least 5 hours, thereby preventing the longest running query from failing at any point in time. Incremental cleaning is also possible using this policy.</li><li><strong>KEEP_LATEST_FILE_VERSIONS</strong>: This policy has the effect of keeping N number of file versions irrespective of time. This policy is useful when it is known how many MAX versions of the file does one want to keep at any given time. To achieve the same behaviour as before of preventing long running queries from failing, one should do their calculations based on data patterns. Alternatively, this policy is also useful if a user just wants to maintain 1 latest version of the file.</li></ul><h3>Examples</h3><p>Suppose a user is ingesting data into a hudi dataset of type COPY_ON_WRITE every 30 minutes as shown below:</p><p><img src=\"/assets/images/blog/hoodie-cleaner/Initial_timeline.png\" alt=\"Initial timeline\"/>\n<em>Figure1: Incoming records getting ingested into a hudi dataset every 30 minutes</em></p><p>The figure shows a particular partition on DFS where commits and corresponding file versions are color coded. 4 different file groups are created in this partition as depicted by fileGroup1, fileGroup2, fileGroup3 and fileGroup4. File group corresponding to fileGroup2 has records ingested from all the 5 commits, while the group corresponding to fileGroup4 has records from the latest 2 commits only.</p><p>Suppose the user uses the below configs for cleaning:</p><pre><code class=\"language-java\">hoodie.cleaner.policy=KEEP_LATEST_COMMITS\nhoodie.cleaner.commits.retained=2\n</code></pre><p>Cleaner selects the versions of files to be cleaned by taking care of the following:</p><ul><li>Latest version of a file should not be cleaned.</li><li>The commit times of the last 2 (configured) + 1 commits are determined. In Figure1, <code>commit 10:30</code> and <code>commit 10:00</code> correspond to the latest 2 commits in the timeline. One extra commit is included because the time window for retaining commits is essentially equal to the longest query run time. So if the longest query takes 1 hour to finish, and ingestion happens every 30 minutes, you need to retain last 2 commits since 2*30 = 60 (1 hour). At this point of time, the longest query can still be using files written in 3rd commit in reverse order. Essentially this means if a query started executing after <code>commit 9:30</code>, it will still be running when clean action is triggered right after <code>commit 10:30</code> as in Figure2. </li><li>Now for any file group, only those file slices are scheduled for cleaning which are not savepointed (another Hudi table service) and whose commit time is less than the 3rd commit (<code>commit 9:30</code> in figure below) in reverse order.</li></ul><p><img src=\"/assets/images/blog/hoodie-cleaner/Retain_latest_commits.png\" alt=\"Retain latest commits\"/>\n<em>Figure2: Files corresponding to latest 3 commits are retained</em></p><p>Now, suppose the user uses the below configs for cleaning:</p><pre><code class=\"language-java\">hoodie.cleaner.policy=KEEP_LATEST_FILE_VERSIONS\nhoodie.cleaner.fileversions.retained=1\n</code></pre><p>Cleaner does the following:</p><ul><li>For any file group, latest version (including any for pending compaction) of file slices are kept and the rest are scheduled for cleaning. Clearly as shown in Figure3, if clean action is triggered right after <code>commit 10:30</code>, the cleaner will simply leave the latest version in every file group and delete the rest.</li></ul><p><img src=\"/assets/images/blog/hoodie-cleaner/Retain_latest_versions.png\" alt=\"Retain latest versions\"/>\n<em>Figure3: Latest file version in every file group is retained</em></p><h3>Configurations</h3><p>You can find the details about all the possible configurations along with the default values <a href=\"https://hudi.apache.org/docs/configurations#compaction-configs\">here</a>.</p><h3>Run command</h3><p>Hudi&#x27;s cleaner table service can be run as a separate process or along with your data ingestion. As mentioned earlier, it basically cleans up any stale/old files lying around. In case you want to run it along with ingesting data, configs are available which enable you to run it <a href=\"https://hudi.apache.org/docs/configurations#withAsyncClean\">synchronously or asynchronously</a>. You can use the below command for running the cleaner independently:</p><pre><code class=\"language-java\">[hoodie]$ spark-submit --class org.apache.hudi.utilities.HoodieCleaner \\\n  --props s3:///temp/hudi-ingestion-config/kafka-source.properties \\\n  --target-base-path s3:///temp/hudi \\\n  --spark-master yarn-cluster\n</code></pre><p>In case you wish to run the cleaner service asynchronously with writing, please configure the below:</p><pre><code class=\"language-java\">hoodie.clean.automatic=true\nhoodie.clean.async=true\n</code></pre><p>Further you can use <a href=\"https://hudi.apache.org/docs/deployment#cli\">Hudi CLI</a> for managing your Hudi dataset. CLI provides the below commands for cleaner service:</p><ul><li><code>cleans show</code></li><li><code>clean showpartitions</code></li><li><code>cleans run</code></li></ul><p>You can find more details and the relevant code for these commands in <a href=\"https://github.com/apache/hudi/blob/master/hudi-cli/src/main/java/org/apache/hudi/cli/commands/CleansCommand.java\"><code>org.apache.hudi.cli.commands.CleansCommand</code> class</a>. </p><h3>Future Scope</h3><p>Work is currently going on for introducing a new cleaning policy based on time elapsed. This will help in achieving a consistent retention throughout regardless of how frequently ingestion happens. You may track the progress <a href=\"https://issues.apache.org/jira/browse/HUDI-349\">here</a>.</p><p>We hope this blog gives you an idea about how to configure the Hudi cleaner and the supported cleaning policies. Please visit the <a href=\"https://hudi.apache.org/blog\">blog section</a> for a deeper understanding of various Hudi concepts. Cheers!</p>",
            "url": "https://hudi.apache.org/blog/2021/06/10/employing-right-configurations-for-hudi-cleaner",
            "title": "Employing correct configurations for Hudi's cleaner table service",
            "summary": "Apache Hudi provides snapshot isolation between writers and readers. This is made possible by Hudi’s MVCC concurrency model. In this blog, we will explain how to employ the right configurations to manage multiple file versions. Furthermore, we will discuss mechanisms available to users on how to maintain just the required number of old file versions so that long running readers do not fail.",
            "date_modified": "2021-06-10T00:00:00.000Z",
            "author": {
                "name": "pratyakshsharma"
            }
        },
        {
            "id": "/2021/06/04/Apache-Hudi-How-Uber-gets-data-a-ride-to-its-destination",
            "content_html": "<div url=\"https://www.rtinsights.com/apache-hudi-how-uber-gets-data-a-ride-to-its-destination/\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2021/06/04/Apache-Hudi-How-Uber-gets-data-a-ride-to-its-destination",
            "title": "Apache Hudi: How Uber gets data a ride to its destination",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2021-06-04T00:00:00.000Z",
            "author": {
                "name": "Joe McKendrick"
            }
        },
        {
            "id": "/2021/05/12/Experts-primer-on-Apache-Hudi",
            "content_html": "<div url=\"https://www.dbta.com/Editorial/News-Flashes/Experts-Present-a-Primer-on-Apache-Hudi-at-Data-Summit-Connect-2021-146834.aspx\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2021/05/12/Experts-primer-on-Apache-Hudi",
            "title": "Experts primer on Apache Hudi",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2021-05-12T00:00:00.000Z",
            "author": {
                "name": "Stephanie Simone"
            }
        },
        {
            "id": "/2021/04/12/Build-Slowly-Changing-Dimensions-Type-2-SCD2-with-Apache-Spark-and-Apache-Hudi-on-Amazon-EMR",
            "content_html": "<div url=\"https://aws.amazon.com/blogs/big-data/build-slowly-changing-dimensions-type-2-scd2-with-apache-spark-and-apache-hudi-on-amazon-emr/\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2021/04/12/Build-Slowly-Changing-Dimensions-Type-2-SCD2-with-Apache-Spark-and-Apache-Hudi-on-Amazon-EMR",
            "title": "Build Slowly Changing Dimensions Type 2 (SCD2) with Apache Spark and Apache Hudi on Amazon EMR",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2021-04-12T00:00:00.000Z",
            "author": {
                "name": "David Greenshtein"
            }
        },
        {
            "id": "/2021/03/11/New-features-from-Apache-hudi-in-Amazon-EMR",
            "content_html": "<div url=\"https://aws.amazon.com/blogs/big-data/new-features-from-apache-hudi-available-in-amazon-emr/\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2021/03/11/New-features-from-Apache-hudi-in-Amazon-EMR",
            "title": "New features from Apache hudi in Amazon EMR",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2021-03-11T00:00:00.000Z",
            "author": {
                "name": "Udit Mehrotra"
            }
        },
        {
            "id": "/2021/03/04/Build-a-data-lake-using-amazon-kinesis-data-stream-for-amazon-dynamodb-and-apache-hudi",
            "content_html": "<div url=\"https://aws.amazon.com/blogs/big-data/build-a-data-lake-using-amazon-kinesis-data-streams-for-amazon-dynamodb-and-apache-hudi/\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2021/03/04/Build-a-data-lake-using-amazon-kinesis-data-stream-for-amazon-dynamodb-and-apache-hudi",
            "title": "Build a data lake using amazon kinesis data stream for amazon dynamodb and apache hudi",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2021-03-04T00:00:00.000Z",
            "author": {
                "name": "Dhiraj Thakur"
            }
        },
        {
            "id": "/2021/03/01/hudi-file-sizing",
            "content_html": "<p>Apache Hudi is a data lake platform technology that provides several functionalities needed to build and manage data lakes.\nOne such key feature that hudi provides is self-managing file sizing so that users don’t need to worry about\nmanual table maintenance. Having a lot of small files will make it harder to achieve good query performance, due to query engines\nhaving to open/read/close files way too many times, to plan and execute queries. But for streaming data lake use-cases,\ninherently ingests are going to end up having smaller volume of writes, which might result in lot of small files if no special handling is done.</p><h2>During Write vs After Write</h2><p>Common approaches to writing very small files and then later stitching them together solve for system scalability issues posed\nby small files but might violate query SLA&#x27;s by exposing small files to them. In fact, you can easily do so on a Hudi table,\nby running a clustering operation, as detailed in a <a href=\"/blog/2021/01/27/hudi-clustering-intro\">previous blog</a>. </p><p>In this blog, we discuss file sizing optimizations in Hudi, during the initial write time, so we don&#x27;t have to effectively\nre-write all data again, just for file sizing. If you want to have both (a) self managed file sizing and\n(b) Avoid exposing small files to queries, automatic file sizing feature saves the day.</p><p>Hudi has the ability to maintain a configured target file size, when performing inserts/upsert operations.\n(Note: bulk_insert operation does not provide this functionality and is designed as a simpler replacement for\nnormal <code>spark.write.parquet</code>).</p><h3>Configs</h3><p>For illustration purposes, we are going to consider only COPY_ON_WRITE table.</p><p>Configs of interest before we dive into the algorithm:</p><ul><li><a href=\"/docs/configurations#limitFileSize\">Max file size</a>: Max size for a given data file. Hudi will try to maintain file sizes to this configured value <br/></li><li><a href=\"/docs/configurations#compactionSmallFileSize\">Soft file limit</a>: Max file size below which a given data file is considered to a small file <br/></li><li><a href=\"/docs/configurations#insertSplitSize\">Insert split size</a>: Number of inserts grouped for a single partition. This value should match\nthe number of records in a single file (you can determine based on max file size and per record size)</li></ul><p>For instance, if your first config value is 120MB and 2nd config value is set to 100MB, any file whose size is &lt; 100MB\nwould be considered a small file.</p><p>If you wish to turn off this feature, set the config value for soft file limit to 0.</p><h3>Example</h3><p>Let’s say this is the layout of data files for a given partition.</p><p><img src=\"/assets/images/blog/hudi-file-sizing/initial_layout.png\" alt=\"Initial layout\"/>\n<em>Figure: Initial data file sizes for a given partition of interest</em></p><p>Let’s assume the configured values for max file size and small file size limit are 120MB and 100MB. File_1’s current\nsize is 40MB, File_2’s size is 80MB, File_3’s size is 90MB, File_4’s size is 130MB and File_5’s size is 105MB. Let’s see\nwhat happens when a new write happens. </p><p><strong>Step 1:</strong> Assigning updates to files. In this step, We look up the index to find the tagged location and records are\nassigned to respective files. Note that we assume updates are only going to increase the file size and that would simply result\nin a much bigger file. When updates lower the file size (by say, nulling out lot of fields), then a subsequent write will deem\nit a small file.</p><p><strong>Step 2:</strong>  Determine small files for each partition path. The soft file limit config value will be leveraged here\nto determine eligible small files. In our example, given the config value is set to 100MB, the small files are File_1(40MB)\nand File_2(80MB) and file_3’s (90MB).</p><p><strong>Step 3:</strong> Once small files are determined, incoming inserts are assigned to them so that they reach their max capacity of\n120MB. File_1 will be ingested with 80MB worth of inserts, file_2 will be ingested with 40MB worth of inserts and\nFile_3 will be ingested with 30MB worth of inserts.</p><p><img src=\"/assets/images/blog/hudi-file-sizing/bin_packing_existing_data_files.png\" alt=\"Bin packing small files\"/>\n<em>Figure: Incoming records are bin packed to existing small files</em></p><p><strong>Step 4:</strong> Once all small files are bin packed to its max capacity and if there are pending inserts unassigned, new file\ngroups/data files are created and inserts are assigned to them. Number of records per new data file is determined from insert split\nsize config. Assuming the insert split size is configured to 120k records, if there are 300k remaining records, 3 new\nfiles will be created in which 2 of them (File_6 and File_7) will be filled with 120k records and the last one (File_8)\nwill be filled with 60k records (assuming each record is 1000 bytes). In future ingestions, 3rd new file will be\nconsidered as a small file to be packed with more data.</p><p><img src=\"/assets/images/blog/hudi-file-sizing/adding_new_files.png\" alt=\"Assigning to new files\"/>\n<em>Figure: Remaining records are assigned to new files</em></p><p>Hudi leverages mechanisms such as custom partitioning for optimized record distribution to different files, executing\nthe algorithm above. After this round of ingestion is complete, all files except File_8 are nicely sized to the optimum size.\nThis process is followed during every ingestion to ensure there are no small files in your Hudi tables. </p><p>Hopefully the blog gave you an overview into how hudi manages small files and assists in boosting your query performance.</p>",
            "url": "https://hudi.apache.org/blog/2021/03/01/hudi-file-sizing",
            "title": "Streaming Responsibly - How Apache Hudi maintains optimum sized files",
            "summary": "Apache Hudi is a data lake platform technology that provides several functionalities needed to build and manage data lakes.",
            "date_modified": "2021-03-01T00:00:00.000Z",
            "author": {
                "name": "shivnarayan"
            }
        },
        {
            "id": "/2021/03/01/Data-Lakehouse-Building-the-Next-Generation-of-Data-Lakes-using-Apache-Hudi",
            "content_html": "<div url=\"https://medium.com/slalom-build/data-lakehouse-building-the-next-generation-of-data-lakes-using-apache-hudi-41550f62f5f\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2021/03/01/Data-Lakehouse-Building-the-Next-Generation-of-Data-Lakes-using-Apache-Hudi",
            "title": "Data Lakehouse: Building the Next Generation of Data Lakes using Apache Hudi",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2021-03-01T00:00:00.000Z",
            "author": {
                "name": "Ryan D'Souza"
            }
        },
        {
            "id": "/2021/02/24/Time-travel-operations-in-Hopsworks-Feature-Store",
            "content_html": "<div url=\"https://examples.hopsworks.ai/master/featurestore/hsfs/time_travel/time_travel_scala/\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2021/02/24/Time-travel-operations-in-Hopsworks-Feature-Store",
            "title": "Time travel operations in Hopsworks Feature Store",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2021-02-24T00:00:00.000Z"
        },
        {
            "id": "/2021/02/13/hudi-key-generators",
            "content_html": "<p>Every record in Hudi is uniquely identified by a primary key, which is a pair of record key and partition path where\nthe record belongs to. Using primary keys, Hudi can impose a) partition level uniqueness integrity constraint\nb) enable fast updates and deletes on records. One should choose the partitioning scheme wisely as it could be a\ndetermining factor for your ingestion and query latency.</p><p>In general, Hudi supports both partitioned and global indexes. For a dataset with partitioned index(which is most\ncommonly used), each record is uniquely identified by a pair of record key and partition path. But for a dataset with\nglobal index, each record is uniquely identified by just the record key. There won&#x27;t be any duplicate record keys across\npartitions.</p><h2>Key Generators</h2><p>Hudi provides several key generators out of the box that users can use based on their need, while having a pluggable\nimplementation for users to implement and use their own KeyGenerator. This blog goes over all different types of key\ngenerators that are readily available to use.</p><p><a href=\"https://github.com/apache/hudi/blob/master/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/KeyGenerator.java\">Here</a>\nis the interface for KeyGenerator in Hudi for your reference.</p><p>Before diving into different types of key generators, let’s go over some of the common configs required to be set for\nkey generators.</p><table><thead><tr><th>Config</th><th align=\"center\">Meaning/purpose</th></tr></thead><tbody><tr><td><code>hoodie.datasource.write.recordkey.field</code></td><td align=\"center\">Refers to record key field. This is a mandatory field.</td></tr><tr><td><code>hoodie.datasource.write.partitionpath.field</code></td><td align=\"center\">Refers to partition path field. This is a mandatory field.</td></tr><tr><td><code>hoodie.datasource.write.keygenerator.class</code></td><td align=\"center\">Refers to Key generator class(including full path). Could refer to any of the available ones or user defined one. This is a mandatory field.</td></tr><tr><td><code>hoodie.datasource.write.partitionpath.urlencode</code></td><td align=\"center\">When set to true, partition path will be url encoded. Default value is false.</td></tr><tr><td><code>hoodie.datasource.write.hive_style_partitioning</code></td><td align=\"center\">When set to true, uses hive style partitioning. Partition field name will be prefixed to the value. Format: “&lt;partition_path_field_name&gt;=&lt;partition_path_value&gt;”. Default value is false.</td></tr></tbody></table><p>There are few more configs involved if you are looking for TimestampBasedKeyGenerator. Will cover those in the respective section.</p><p>Lets go over different key generators available to be used with Hudi.</p><h3><a href=\"https://github.com/apache/hudi/blob/master/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/SimpleKeyGenerator.java\">SimpleKeyGenerator</a></h3><p>Record key refers to one field(column in dataframe) by name and partition path refers to one field (single column in dataframe)\nby name. This is one of the most commonly used one. Values are interpreted as is from dataframe and converted to string.</p><h3><a href=\"https://github.com/apache/hudi/blob/master/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/ComplexKeyGenerator.java\">ComplexKeyGenerator</a></h3><p>Both record key and partition paths comprise one or more than one field by name(combination of multiple fields). Fields\nare expected to be comma separated in the config value. For example <code>&quot;Hoodie.datasource.write.recordkey.field&quot; : “col1,col4”</code></p><h3><a href=\"https://github.com/apache/hudi/blob/master/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/GlobalDeleteKeyGenerator.java\">GlobalDeleteKeyGenerator</a></h3><p>Global index deletes do not require partition value. So this key generator avoids using partition value for generating HoodieKey.</p><h3><a href=\"https://github.com/apache/hudi/blob/master/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/TimestampBasedKeyGenerator.java\">TimestampBasedKeyGenerator</a></h3><p>This key generator relies on timestamps for the partition field. The field values are interpreted as timestamps\nand not just converted to string while generating partition path value for records.  Record key is same as before where it is chosen by\nfield name.  Users are expected to set few more configs to use this KeyGenerator.</p><p>Configs to be set:</p><table><thead><tr><th>Config</th><th>Meaning/purpose</th></tr></thead><tbody><tr><td><code>hoodie.deltastreamer.keygen.timebased.timestamp.type</code></td><td>One of the timestamp types supported(UNIX_TIMESTAMP, DATE_STRING, MIXED, EPOCHMILLISECONDS, SCALAR)</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.output.dateformat</code></td><td>Output date format</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.timezone</code></td><td>Timezone of the data format</td></tr><tr><td><code>oodie.deltastreamer.keygen.timebased.input.dateformat</code></td><td>Input date format</td></tr></tbody></table><p>Let&#x27;s go over some example values for TimestampBasedKeyGenerator.</p><h4>Timestamp is GMT</h4><table><thead><tr><th>Config field</th><th>Value</th></tr></thead><tbody><tr><td><code>hoodie.deltastreamer.keygen.timebased.timestamp.type</code></td><td>&quot;EPOCHMILLISECONDS&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.output.dateformat</code></td><td>&quot;yyyy-MM-dd hh&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.timezone</code></td><td>&quot;GMT+8:00&quot;</td></tr></tbody></table><p>Input Field value: “1578283932000L” <br/>\nPartition path generated from key generator: “2020-01-06 12”</p><p>If input field value is null for some rows. <br/>\nPartition path generated from key generator: “1970-01-01 08”</p><h4>Timestamp is DATE_STRING</h4><table><thead><tr><th>Config field</th><th>Value</th></tr></thead><tbody><tr><td><code>hoodie.deltastreamer.keygen.timebased.timestamp.type</code></td><td>&quot;DATE_STRING&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.output.dateformat</code></td><td>&quot;yyyy-MM-dd hh&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.timezone</code></td><td>&quot;GMT+8:00&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.input.dateformat</code></td><td>&quot;yyyy-MM-dd hh:mm:ss&quot;</td></tr></tbody></table><p>Input field value: “2020-01-06 12:12:12” <br/>\nPartition path generated from key generator: “2020-01-06 12”</p><p>If input field value is null for some rows. <br/>\nPartition path generated from key generator: “1970-01-01 12:00:00”</p><br/><h4>Scalar examples</h4><table><thead><tr><th>Config field</th><th>Value</th></tr></thead><tbody><tr><td><code>hoodie.deltastreamer.keygen.timebased.timestamp.type</code></td><td>&quot;SCALAR&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.output.dateformat</code></td><td>&quot;yyyy-MM-dd hh&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.timezone</code></td><td>&quot;GMT&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.timestamp.scalar.time.unit</code></td><td>&quot;days&quot;</td></tr></tbody></table><p>Input field value: “20000L” <br/>\nPartition path generated from key generator: “2024-10-04 12”</p><p>If input field value is null. <br/>\nPartition path generated from key generator: “1970-01-02 12”</p><h4>ISO8601WithMsZ with Single Input format</h4><table><thead><tr><th>Config field</th><th>Value</th></tr></thead><tbody><tr><td><code>hoodie.deltastreamer.keygen.timebased.timestamp.type</code></td><td>&quot;DATE_STRING&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.input.dateformat</code></td><td>&quot;yyyy-MM-dd&#x27;T&#x27;HH:mm:ss.SSSZ&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.input.dateformat.list.delimiter.regex</code></td><td>&quot;&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.input.timezone</code></td><td>&quot;&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.output.dateformat</code></td><td>&quot;yyyyMMddHH&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.output.timezone</code></td><td>&quot;GMT&quot;</td></tr></tbody></table><p>Input field value: &quot;2020-04-01T13:01:33.428Z&quot; <br/>\nPartition path generated from key generator: &quot;2020040113&quot;</p><h4>ISO8601WithMsZ with Multiple Input formats</h4><table><thead><tr><th>Config field</th><th>Value</th></tr></thead><tbody><tr><td><code>hoodie.deltastreamer.keygen.timebased.timestamp.type</code></td><td>&quot;DATE_STRING&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.input.dateformat</code></td><td>&quot;yyyy-MM-dd&#x27;T&#x27;HH:mm:ssZ,yyyy-MM-dd&#x27;T&#x27;HH:mm:ss.SSSZ&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.input.dateformat.list.delimiter.regex</code></td><td>&quot;&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.input.timezone</code></td><td>&quot;&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.output.dateformat</code></td><td>&quot;yyyyMMddHH&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.output.timezone</code></td><td>&quot;UTC&quot;</td></tr></tbody></table><p>Input field value: &quot;2020-04-01T13:01:33.428Z&quot; <br/>\nPartition path generated from key generator: &quot;2020040113&quot;</p><h4>ISO8601NoMs with offset using multiple input formats</h4><table><thead><tr><th>Config field</th><th>Value</th></tr></thead><tbody><tr><td><code>hoodie.deltastreamer.keygen.timebased.timestamp.type</code></td><td>&quot;DATE_STRING&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.input.dateformat</code></td><td>&quot;yyyy-MM-dd&#x27;T&#x27;HH:mm:ssZ,yyyy-MM-dd&#x27;T&#x27;HH:mm:ss.SSSZ&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.input.dateformat.list.delimiter.regex</code></td><td>&quot;&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.input.timezone</code></td><td>&quot;&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.output.dateformat</code></td><td>&quot;yyyyMMddHH&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.output.timezone</code></td><td>&quot;UTC&quot;</td></tr></tbody></table><p>Input field value: &quot;2020-04-01T13:01:33-<strong>05:00</strong>&quot; <br/>\nPartition path generated from key generator: &quot;2020040118&quot;</p><h4>Input as short date string and expect date in date format</h4><table><thead><tr><th>Config field</th><th>Value</th></tr></thead><tbody><tr><td><code>hoodie.deltastreamer.keygen.timebased.timestamp.type</code></td><td>&quot;DATE_STRING&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.input.dateformat</code></td><td>&quot;yyyy-MM-dd&#x27;T&#x27;HH:mm:ssZ,yyyy-MM-dd&#x27;T&#x27;HH:mm:ss.SSSZ,yyyyMMdd&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.input.dateformat.list.delimiter.regex</code></td><td>&quot;&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.input.timezone</code></td><td>&quot;UTC&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.output.dateformat</code></td><td>&quot;MM/dd/yyyy&quot;</td></tr><tr><td><code>hoodie.deltastreamer.keygen.timebased.output.timezone</code></td><td>&quot;UTC&quot;</td></tr></tbody></table><p>Input field value: &quot;220200401&quot; <br/>\nPartition path generated from key generator: &quot;04/01/2020&quot;</p><h3><a href=\"https://github.com/apache/hudi/blob/master/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/CustomKeyGenerator.java\">CustomKeyGenerator</a></h3><p>This is a generic implementation of KeyGenerator where users are able to leverage the benefits of SimpleKeyGenerator,\nComplexKeyGenerator and TimestampBasedKeyGenerator all at the same time. One can configure record key and partition\npaths as a single field or a combination of fields. This keyGenerator is particularly useful if you want to define\ncomplex partition paths involving regular fields and timestamp based fields. It expects value for prop <code>&quot;hoodie.datasource.write.partitionpath.field&quot;</code>\nin a specific format. The format should be &quot;field1:PartitionKeyType1,field2:PartitionKeyType2...&quot;</p><p>The complete partition path is created as\n<code>&lt;value for field1 basis PartitionKeyType1&gt;/&lt;value for field2 basis PartitionKeyType2&gt; </code>\nand so on. Each partition key type could either be SIMPLE or TIMESTAMP.</p><p>Example config value: <code>“field_3:simple,field_5:timestamp”</code></p><p>RecordKey config value is either single field incase of SimpleKeyGenerator or a comma separate field names if referring to ComplexKeyGenerator.\nEg: “col1” or “col3,col4”.</p><h3><a href=\"https://github.com/apache/hudi/blob/master/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/NonpartitionedKeyGenerator.java\">NonPartitionedKeyGenerator</a></h3><p>If your hudi dataset is not partitioned, you could use this “NonPartitionedKeyGenerator” which will return an empty\npartition for all records. In other words, all records go to the same partition (which is empty “”) </p><p>Hope this blog gave you a good understanding of different types of Key Generators available in Apache Hudi. Thanks for your continued support for Hudi&#x27;s community.</p>",
            "url": "https://hudi.apache.org/blog/2021/02/13/hudi-key-generators",
            "title": "Apache Hudi Key Generators",
            "summary": "Every record in Hudi is uniquely identified by a primary key, which is a pair of record key and partition path where",
            "date_modified": "2021-02-13T00:00:00.000Z",
            "author": {
                "name": "shivnarayan"
            }
        },
        {
            "id": "/2021/01/27/hudi-clustering-intro",
            "content_html": "<h2>Background</h2><p>Apache Hudi brings stream processing to big data, providing fresh data while being an order of magnitude efficient over traditional batch processing. In a data lake/warehouse, one of the key trade-offs is between ingestion speed and query performance. Data ingestion typically prefers small files to improve parallelism and make data available to queries as soon as possible. However, query performance degrades poorly with a lot of small files. Also, during ingestion, data is typically co-located based on arrival time. However, the query engines perform better when the data frequently queried is co-located together. In most architectures each of these systems tend to add optimizations independently to improve performance which hits limitations due to un-optimized data layouts. This blog introduces a new kind of table service called clustering <a href=\"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+19+Clustering+data+for+freshness+and+query+performance\">[RFC-19]</a> to reorganize data for improved query performance without compromising on ingestion speed.</p><h2>Clustering Architecture</h2><p>At a high level, Hudi provides different operations such as insert/upsert/bulk_insert through it’s write client API to be able to write data to a Hudi table. To be able to choose a trade-off between file size and ingestion speed, Hudi provides a knob <code>hoodie.parquet.small.file.limit</code> to be able to configure the smallest allowable file size. Users are able to configure the small file <a href=\"https://hudi.apache.org/docs/configurations#compactionSmallFileSize\">soft limit</a> to <code>0</code> to force new data to go into a new set of filegroups or set it to a higher value to ensure new data gets “padded” to existing files until it meets that limit that adds to ingestion latencies.</p><p>To be able to support an architecture that allows for fast ingestion without compromising query performance, we have introduced a ‘clustering’ service to rewrite the data to optimize Hudi data lake file layout.</p><p>Clustering table service can run asynchronously or synchronously adding a new action type called “REPLACE”, that will mark the clustering action in the Hudi metadata timeline.</p><h4>Overall, there are 2 parts to clustering</h4><ol><li>Scheduling clustering: Create a clustering plan using a pluggable clustering strategy.</li><li>Execute clustering: Process the plan using an execution strategy to create new files and replace old files.</li></ol><h4>Scheduling clustering</h4><p>Following steps are followed to schedule clustering.</p><ol><li>Identify files that are eligible for clustering: Depending on the clustering strategy chosen, the scheduling logic will identify the files eligible for clustering.</li><li>Group files that are eligible for clustering based on specific criteria. Each group is expected to have data size in multiples of ‘targetFileSize’. Grouping is done as part of ‘strategy’ defined in the plan. Additionally, there is an option to put a cap on group size to improve parallelism and avoid shuffling large amounts of data.</li><li>Finally, the clustering plan is saved to the timeline in an avro <a href=\"https://github.com/apache/hudi/blob/master/hudi-common/src/main/avro/HoodieClusteringPlan.avsc\">metadata format</a>.</li></ol><h4>Running clustering</h4><ol><li>Read the clustering plan and get the ‘clusteringGroups’ that mark the file groups that need to be clustered.</li><li>For each group, we instantiate appropriate strategy class with strategyParams (example: sortColumns) and apply that strategy to rewrite the data.</li><li>Create a “REPLACE” commit and update the metadata in <a href=\"https://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceCommitMetadata.java\">HoodieReplaceCommitMetadata</a>.</li></ol><p>Clustering Service builds on Hudi’s MVCC based design to allow for writers to continue to insert new data while clustering action runs in the background to reformat data layout, ensuring snapshot isolation between concurrent readers and writers.</p><p>NOTE: Clustering can only be scheduled for tables / partitions not receiving any concurrent updates. In the future, concurrent updates use-case will be supported as well.</p><p><img src=\"/assets/images/blog/clustering/example_perf_improvement.png\" alt=\"Clustering example\"/>\n<em>Figure: Illustrating query performance improvements by clustering</em></p><h4>Setting up clustering</h4><p>Inline clustering can be setup easily using spark dataframe options. See sample below</p><pre><code class=\"language-scala\">import org.apache.hudi.QuickstartUtils._\nimport scala.collection.JavaConversions._\nimport org.apache.spark.sql.SaveMode._\nimport org.apache.hudi.DataSourceReadOptions._\nimport org.apache.hudi.DataSourceWriteOptions._\nimport org.apache.hudi.config.HoodieWriteConfig._\n\n\nval df =  //generate data frame\ndf.write.format(&quot;org.apache.hudi&quot;).\n        options(getQuickstartWriteConfigs).\n        option(PRECOMBINE_FIELD_OPT_KEY, &quot;ts&quot;).\n        option(RECORDKEY_FIELD_OPT_KEY, &quot;uuid&quot;).\n        option(PARTITIONPATH_FIELD_OPT_KEY, &quot;partitionpath&quot;).\n        option(TABLE_NAME, &quot;tableName&quot;).\n        option(&quot;hoodie.parquet.small.file.limit&quot;, &quot;0&quot;).\n        option(&quot;hoodie.clustering.inline&quot;, &quot;true&quot;).\n        option(&quot;hoodie.clustering.inline.max.commits&quot;, &quot;4&quot;).\n        option(&quot;hoodie.clustering.plan.strategy.target.file.max.bytes&quot;, &quot;1073741824&quot;).\n        option(&quot;hoodie.clustering.plan.strategy.small.file.limit&quot;, &quot;629145600&quot;).\n        option(&quot;hoodie.clustering.plan.strategy.sort.columns&quot;, &quot;column1,column2&quot;). //optional, if sorting is needed as part of rewriting data\n        mode(Append).\n        save(&quot;dfs://location&quot;);\n</code></pre><p>For more advanced usecases, async clustering pipeline can also be setup. See an example <a href=\"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+19+Clustering+data+for+freshness+and+query+performance#RFC19Clusteringdataforfreshnessandqueryperformance-SetupforAsyncclusteringJob\">here</a>.</p><h2>Table Query Performance</h2><p>We created a dataset from one partition of a known production style table with ~20M records and on-disk size of ~200GB. The dataset has rows for multiple “sessions”. Users always query this data using a predicate on session. Data for a single session is spread across multiple data files because ingestion groups data based on arrival time. The below experiment shows that by clustering on session, we are able to improve the data locality and reduce query execution time by more than 50%.</p><p>Query: </p><pre><code class=\"language-scala\">spark.sql(&quot;select  *  from table where session_id=123&quot;)\n</code></pre><h3>Before Clustering</h3><p>Query took 2.2 minutes to complete. Note that the number of output rows in the “scan parquet” part of the query plan includes all 20M rows in the table.</p><p><img src=\"/assets/images/blog/clustering/Query_Plan_Before_Clustering.png\" alt=\"Query Plan Before Clustering\"/>\n<em>Figure: Spark SQL query details before clustering</em></p><h3>After Clustering</h3><p>The query plan is similar to above. But, because of improved data locality and predicate push down, spark is able to prune a lot of rows. After clustering, the same query only outputs 110K rows (out of 20M rows) while scanning parquet files. This cuts query time to less than a minute from 2.2 minutes.</p><p><img src=\"/assets/images/blog/clustering/Query_Plan_After_Clustering.png\" alt=\"Query Plan Before Clustering\"/>\n<em>Figure: Spark SQL query details after clustering</em></p><p>The table below summarizes query performance improvements from experiments run using Spark3</p><table><thead><tr><th>Table State</th><th>Query runtime</th><th>Num Records Processed</th><th>Num files on disk</th><th>Size of each file</th></tr></thead><tbody><tr><td><strong>Unclustered</strong></td><td>130,673 ms</td><td>~20M</td><td>13642</td><td>~150 MB</td></tr><tr><td><strong>Clustered</strong></td><td>55,963 ms</td><td>~110K</td><td>294</td><td>~600 MB</td></tr></tbody></table><p>Query runtime is reduced by 60% after clustering. Similar results were observed on other sample datasets. See example query plans and more details at the <a href=\"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+19+Clustering+data+for+freshness+and+query+performance#RFC19Clusteringdataforfreshnessandqueryperformance-PerformanceEvaluation\">RFC-19 performance evaluation</a>.</p><p>We expect dramatic speedup for large tables, where the query runtime is almost entirely dominated by actual I/O and not query planning, unlike the example above.</p><h2>Summary</h2><p>Using clustering, we can improve query performance by</p><ol><li>Leveraging concepts such as <a href=\"https://en.wikipedia.org/wiki/Z-order_curve\">space filling curves</a> to adapt data lake layout and reduce the amount of data read during queries.</li><li>Stitch small files into larger ones and reduce the total number of files that need to be scanned by the query engine.</li></ol><p>Clustering also enables stream processing over big data. Ingestion can write small files to satisfy latency requirements of stream processing. Clustering can be used in the background to stitch these small files into larger files and reduce file count.</p><p>Besides this, the clustering framework also provides the flexibility to asynchronously rewrite data based on specific requirements. We foresee many other use-cases adopting clustering framework with custom pluggable strategies to satisfy on-demand data lake management activities. Some such notable use-cases that are actively being solved using clustering:</p><ol><li>Rewrite data and encrypt data at rest.</li><li>Prune unused columns from tables and reduce storage footprint.</li></ol>",
            "url": "https://hudi.apache.org/blog/2021/01/27/hudi-clustering-intro",
            "title": "Optimize Data lake layout using Clustering in Apache Hudi",
            "summary": "Background",
            "date_modified": "2021-01-27T00:00:00.000Z",
            "author": {
                "name": "satish.kotha"
            }
        },
        {
            "id": "/2020/12/01/high-perf-data-lake-with-hudi-and-alluxio-t3go",
            "content_html": "<h2>Building High-Performance Data Lake Using Apache Hudi and Alluxio at T3Go</h2><p><a href=\"https://www.t3go.cn/\">T3Go</a>  is China’s first platform for smart travel based on the Internet of Vehicles. In this article, Trevor Zhang and Vino Yang from T3Go describe the evolution of their data lake architecture, built on cloud-native or open-source technologies including Alibaba OSS, Apache Hudi, and Alluxio. Today, their data lake stores petabytes of data, supporting hundreds of pipelines and tens of thousands of tasks daily. It is essential for business units at T3Go including Data Warehouse, Internet of Vehicles, Order Dispatching, Machine Learning, and self-service query analysis.</p><p>In this blog, you will see how we slashed data ingestion time by half using Hudi and Alluxio. Furthermore, data analysts using Presto, Hudi, and Alluxio saw the queries speed up by 10 times. We built our data lake based on data orchestration for multiple stages of our data pipeline, including ingestion and analytics.</p><h2>I. T3Go data lake Overview</h2><p>Prior to the data lake, different business units within T3Go managed their own data processing solutions, utilizing different storage systems, ETL tools, and data processing frameworks. Data for each became siloed from every other unit, significantly increasing cost and complexity. Due to the rapid business expansion of T3Go, this inefficiency became our engineering bottleneck.</p><p>We moved to a unified data lake solution based on Alibaba OSS, an object store similar to AWS S3, to provide a centralized location to store structured and unstructured data, following the design principles of  <em>Multi-cluster Shared-data Architecture</em>; all the applications access OSS storage as the source of truth, as opposed to different data silos. This architecture allows us to store the data as-is, without having to first structure the data, and run different types of analytics to guide better decisions, building dashboards and visualizations from big data processing, real-time analytics, and machine learning.</p><h2>II. Efficient Near Real-time Analytics Using Hudi</h2><p>Our business in smart travel drives the need to process and analyze data in a near real-time manner. With a traditional data warehouse, we faced the following challenges:  </p><ol><li>High overhead when updating due to long-tail latency</li><li>High cost of order analysis due to the long window of a business session</li><li>Reduced query accuracy due to late or ad-hoc updates</li><li>Unreliability in data ingestion pipeline</li><li>Data lost in the distributed data pipeline that cannot be reconciled</li><li>High latency to access data storage</li></ol><p>As a result, we adopted Apache Hudi on top of OSS to address these issues. The following diagram outlines the architecture:</p><p><img src=\"/assets/images/blog/2020-12-01-t3go-architecture.png\" alt=\"architecture\"/></p><h3>Enable Near real time data ingestion and analysis</h3><p>With Hudi, our data lake supports multiple data sources including Kafka, MySQL binlog, GIS, and other business logs in near real time. As a result, more than 60% of the company’s data is stored in the data lake and this proportion continues to increase.</p><p>We are also able to speed up the data ingestion time down to a few minutes by introducing Apache Hudi into the data pipeline. Combined with big data interactive query and analysis framework such as Presto and SparkSQL, real-time data analysis and insights are achieved.</p><h3>Enable Incremental processing pipeline</h3><p>With the help of Hudi, it is possible to provide incremental changes to the downstream derived table when the upstream table updates frequently. Even with a large number of interdependent tables, we can quickly run partial data updates. This also effectively avoids updating the full partitions of cold tables in the traditional Hive data warehouse.</p><h3>Accessing Data using Hudi as a unified format</h3><p>Traditional data warehouses often deploy Hadoop to store data and provide batch analysis. Kafka is used separately to distribute Hadoop data to other data processing frameworks, resulting in duplicated data. Hudi helps effectively solve this problem; we always use Spark pipelines to insert new updates into the Hudi tables, then incrementally read the update of Hudi tables. In other words, Hudi tables are used as the unified storage format to access data.</p><h2>III. Efficient Data Caching Using Alluxio</h2><p>In the early version of our data lake without Alluxio, data received from Kafka in real time is processed by Spark and then written to OSS data lake using Hudi DeltaStreamer tasks. With this architecture, Spark often suffered high network latency when writing to OSS directly. Since all data is in OSS storage, OLAP queries on Hudi data may also be slow due to lack of data locality.</p><p>To address the latency issue, we deployed Alluxio as a data orchestration layer, co-located with computing engines such as Spark and Presto, and used Alluxio to accelerate read and write on the data lake as shown in the following diagram:</p><p><img src=\"/assets/images/blog/2020-12-01-t3go-architecture-alluxio.png\" alt=\"architecture-alluxio\"/></p><p>Data in formats such as Hudi, Parquet, ORC, and JSON are stored mostly on OSS, consisting of 95% of the data. Computing engines such as Flink, Spark, Kylin, and Presto are deployed in isolated clusters respectively. When each engine accesses OSS, Alluxio acts as a virtual distributed storage system to accelerate data, being co-located with each of the computing clusters.</p><p>Specifically, here are a few applications leveraging Alluxio in the T3Go data lake.</p><h3>Data lake ingestion</h3><p>We mount the corresponding OSS path to the Alluxio file system and set Hudi’s  <em>“<strong>target-base-path</strong>”</em>  parameter value to use the alluxio:// scheme in place of oss:// scheme. Spark pipelines with Hudi continuously ingest data to Alluxio. After data is written to Alluxio, it is asynchronously persisted from the Alluxio cache to the remote OSS every minute. These modifications allow Spark to write to a local Alluxio node instead of writing to remote OSS, significantly reducing the time for the data to be available in data lake after ingestion.</p><h3>Data analysis on the lake</h3><p>We use Presto as an ad-hoc query engine to analyze the Hudi tables in the lake, co-locating Alluxio workers on each Presto worker node. When Presto and Alluxio services are co-located and running, Alluxio caches the input data locally in the Presto worker which greatly benefits Presto for subsequent retrievals. On a cache hit, Presto can read from the local Alluxio worker storage at memory speed without any additional data transfer over the network.</p><h3>Concurrent accesses across multiple storage systems</h3><p>In order to ensure the accuracy of training samples, our machine learning team often synchronizes desensitized data in production to an offline machine learning environment. During synchronization, the data flows across multiple file systems, from production OSS to an offline HDFS followed by another offline Machine Learning HDFS.</p><p>This data migration process is not only inefficient but also error-prune for modelers because multiple different storages with varying configurations are involved. Alluxio helps in this specific scenario by mounting the destination storage systems under the same filesystem to be accessed by their corresponding logical paths in Alluxio namespace. By decoupling the physical storage, this allows applications with different APIs to access and transfer data seamlessly. This data access layout also improves performance.</p><h3>Microbenchmark</h3><p>Overall, we observed the following improvements with Alluxio:</p><ol><li>It supports a hierarchical and transparent caching mechanism</li><li>It supports cache promote omode mode when reading</li><li>It supports asynchronous writing mode</li><li>It supports LRU recycling strategy</li><li>It has pin and TTL features</li></ol><p>After comparison and verification, we choose to use Spark SQL as the query engine. Our performance testing queries the Hudi table, comparing Alluxio + OSS together against OSS directly as well as HDFS.</p><p><img src=\"/assets/images/blog/2020-12-01-t3go-microbenchmark.png\" alt=\"microbench\"/></p><p>In the stress test shown above, after the data volume is greater than a certain magnitude (2400W), the query speed using Alluxio+OSS surpasses the HDFS query speed of the hybrid deployment. After the data volume is greater than 1E, the query speed starts to double. After reaching 6E data, it is up to 12 times higher than querying native OSS and 8 times higher than querying native HDFS. The improvement depends on the machine configuration.</p><p>Based on our performance benchmarking, we found that the performance can be improved by over 10 times with the help of Alluxio. Furthermore, the larger the data scale, the more prominent the performance improvement.</p><h2>IV. Next Step</h2><p>As T3Go’s data lake ecosystem expands, we will continue facing the critical scenario of compute and storage segregation. With T3Go’s growing data processing needs, our team plans to deploy Alluxio on a larger scale to accelerate our data lake storage.</p><p>In addition to the deployment of Alluxio on the data lake computing engine, which currently is mainly SparkSQL, we plan to add a layer of Alluxio to the OLAP cluster using Apache Kylin and an ad_hoc cluster using Presto. The goal is to have Alluxio cover all computing scenarios, with Alluxio interconnected between each scene to improve the read and write efficiency of the data lake and the surrounding lake ecology.</p><h2>V. Conclusion</h2><p>As mentioned earlier, Hudi and Alluxio covers all scenarios of Hudi’s near real-time ingestion, near real-time analysis, incremental processing, and data distribution on DFS, among many others, and plays the role of a powerful accelerator on data ingestion and data analysis on the lake. With Hudi and Alluxio together,  <strong>our R&amp;D engineers shortened the time for data ingestion into the lake by up to a factor of 2. Data analysts using Presto, Hudi, and Alluxio in conjunction to query data on the lake saw their queries speed up by 10 times faster.</strong> Furthermore, the larger the data scale, the more prominent the performance improvement. Alluxio is an important part of T3Go’s plan to become a leading enterprise data lake in China. We look forward to seeing further integration with Alluxio in T3Go’s data lake ecosystem.</p>",
            "url": "https://hudi.apache.org/blog/2020/12/01/high-perf-data-lake-with-hudi-and-alluxio-t3go",
            "title": "Building High-Performance Data Lake Using Apache Hudi and Alluxio at T3Go",
            "summary": "Building High-Performance Data Lake Using Apache Hudi and Alluxio at T3Go",
            "date_modified": "2020-12-01T00:00:00.000Z",
            "author": {
                "name": "t3go"
            }
        },
        {
            "id": "/2020/11/29/Can-Big-Data-Solutions-Be-Affordable",
            "content_html": "<div url=\"https://www.analyticsinsight.net/can-big-data-solutions-be-affordable/\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2020/11/29/Can-Big-Data-Solutions-Be-Affordable",
            "title": "Can Big Data Solutions Be Affordable?",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2020-11-29T00:00:00.000Z"
        },
        {
            "id": "/2020/11/11/hudi-indexing-mechanisms",
            "content_html": "<p>Apache Hudi employs an index to locate the file group, that an update/delete belongs to. For Copy-On-Write tables, this enables\nfast upsert/delete operations, by avoiding the need to join against the entire dataset to determine which files to rewrite.\nFor Merge-On-Read tables, this design allows Hudi to bound the amount of records any given base file needs to be merged against.\nSpecifically, a given base file needs to merged only against updates for records that are part of that base file. In contrast,\ndesigns without an indexing component (e.g: <a href=\"https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions\">Apache Hive ACID</a>),\ncould end up having to merge all the base files against all incoming updates/delete records.</p><p>At a high level, an index maps a record key + an optional partition path to a file group ID on storage (explained\nmore in detail <a href=\"/docs/concepts\">here</a>) and during write operations, we lookup this mapping to route an incoming update/delete\nto a log file attached to the base file (MOR) or to the latest base file that now needs to be merged against (COW). The index also enables\nHudi to enforce unique constraints based on the record keys.</p><p><img src=\"/assets/images/blog/hudi-indexes/with-and-without-index.png\" alt=\"Fact table\"/>\n<em>Figure: Comparison of merge cost for updates (yellow blocks) against base files (white blocks)</em></p><p>Given that Hudi already supports few different indexing techniques and is also continuously improving/adding more to its toolkit, the rest of the blog\nattempts to explain different categories of workloads, from our experience and suggests what index types to use for each. We will also interlace\ncommentary on existing limitations, upcoming work and optimizations/tradeoffs along the way. </p><h2>Index Types in Hudi</h2><p>Currently, Hudi supports the following indexing options. </p><ul><li><strong>Bloom Index (default):</strong> Employs bloom filters built out of the record keys, optionally also pruning candidate files using record key ranges.</li><li><strong>Simple Index:</strong> Performs a lean join of the incoming update/delete records against keys extracted from the table on storage.</li><li><strong>HBase Index:</strong> Manages the index mapping in an external Apache HBase table.</li></ul><p>Writers can pick one of these options using <code>hoodie.index.type</code> config option. Additionally, a custom index implementation can also be employed\nusing <code>hoodie.index.class</code> and supplying a subclass of <code>SparkHoodieIndex</code> (for Apache Spark writers) </p><p>Another key aspect worth understanding is the difference between global and non-global indexes. Both bloom and simple index have\nglobal options - <code>hoodie.index.type=GLOBAL_BLOOM</code> and <code>hoodie.index.type=GLOBAL_SIMPLE</code> - respectively. HBase index is by nature a global index.</p><ul><li><p><strong>Global index:</strong>  Global indexes enforce uniqueness of keys across all partitions of a table i.e guarantees that exactly\none record exists in the table for a given record key. Global indexes offer stronger guarantees, but the update/delete cost grows\nwith size of the table <code>O(size of table)</code>, which might still be acceptable for smaller tables.</p></li><li><p><strong>Non Global index:</strong> On the other hand, the default index implementations enforce this constraint only within a specific partition.\nAs one might imagine, non global indexes depends on the writer to provide the same consistent partition path for a given record key during update/delete,\nbut can deliver much better performance since the index lookup operation becomes <code>O(number of records updated/deleted)</code> and\nscales well with write volume.</p></li></ul><p>Since data comes in at different volumes, velocity and has different access patterns, different indices could be used for different workloads.\nNext, let’s walk through some typical workloads and see how to leverage the right Hudi index for such use-cases.</p><h2>Workload: Late arriving updates to fact tables</h2><p>Many companies store large volumes of transactional data in NoSQL data stores. For eg, trip tables in case of ride-sharing, buying and selling of shares,\norders in an e-commerce site. These tables are usually ever growing with random updates on most recent data with long tail updates going to older data, either\ndue to transactions settling at a later date/data corrections. In other words, most updates go into the latest partitions with few updates going to older ones.</p><p><img src=\"/assets/images/blog/hudi-indexes/Fact20tables.gif\" alt=\"Fact table\"/>\n<em>Figure: Typical update pattern for Fact tables</em></p><p>For such workloads, the <code>BLOOM</code> index performs well, since index look-up will prune a lot of data files based on a well-sized bloom filter.\nAdditionally, if the keys can be constructed such that they have a certain ordering, the number of files to be compared is further reduced by range pruning.\nHudi constructs an interval tree with all the file key ranges and efficiently filters out the files that don&#x27;t match any key ranges in the updates/deleted records.</p><p>In order to efficiently compare incoming record keys against bloom filters i.e with minimal number of bloom filter reads and uniform distribution of work across\nthe executors, Hudi leverages caching of input records and employs a custom partitioner that can iron out data skews using statistics. At times, if the bloom filter\nfalse positive ratio is high, it could increase the amount of data shuffled to perform the lookup. Hudi supports dynamic bloom filters\n(enabled using <code>hoodie.bloom.index.filter.type=DYNAMIC_V0</code>), which adjusts its size based on the number of records stored in a given file to deliver the\nconfigured false positive ratio. </p><p>In the near future, we plan to introduce a much speedier version of the BLOOM index that tracks bloom filters/ranges in an internal Hudi metadata table, indexed for fast\npoint lookups. This would avoid any current limitations around reading bloom filters/ranges from the base files themselves, to perform the lookup. (see\n<a href=\"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+15%3A+HUDI+File+Listing+and+Query+Planning+Improvements?src=contextnavpagetreemode\">RFC-15</a> for the general design)</p><h2>Workload: De-Duplication in event tables</h2><p>Event Streaming is everywhere. Events coming from Apache Kafka or similar message bus are typically 10-100x the size of fact tables and often treat &quot;time&quot; (event&#x27;s arrival time/processing\ntime) as a first class citizen. For eg, IoT event stream, click stream data, ad impressions etc. Inserts and updates only span the last few partitions as these are mostly append only data.\nGiven duplicate events can be introduced anywhere in the end-end pipeline, de-duplication before storing on the data lake is a common requirement. </p><p><img src=\"/assets/images/blog/hudi-indexes/Event20tables.gif\" alt=\"Event table\"/>\n<em>Figure showing the spread of updates for Event table.</em></p><p>In general, this is a very challenging problem to solve at lower cost. Although, we could even employ a key value store to perform this de-duplication ala HBASE index, the index storage\ncosts would grow linear with number of events and thus can be prohibitively expensive. In fact, <code>BLOOM</code> index with range pruning is the optimal solution here. One can leverage the fact\nthat time is often a first class citizen and construct a key such as <code>event_ts + event_id</code> such that the inserted records have monotonically increasing keys. This yields great returns\nby pruning large amounts of files even within the latest table partitions. </p><h2>Workload: Random updates/deletes to a dimension table</h2><p>These types of tables usually contain high dimensional data and hold reference data e.g user profile, merchant information. These are high fidelity tables where the updates are often small but also spread\nacross a lot of partitions and data files ranging across the dataset from old to new. Often times, these tables are also un-partitioned, since there is also not a good way to partition these tables.</p><p><img src=\"/assets/images/blog/hudi-indexes/Dimension20tables.gif\" alt=\"Dimensions table\"/>\n<em>Figure showing the spread of updates for Dimensions table.</em></p><p>As discussed before, the <code>BLOOM</code> index may not yield benefits if a good number of files cannot be pruned out by comparing ranges/filters. In such a random write workload, updates end up touching\nmost files within in the table and thus bloom filters will typically indicate a true positive for all files based on some incoming update. Consequently, we would end up comparing ranges/filter, only\nto finally check the incoming updates against all files. The <code>SIMPLE</code> Index will be a better fit as it does not do any upfront pruning based, but directly joins with interested fields from every data file.\n<code>HBASE</code> index can be employed, if the operational overhead is acceptable and would provide much better lookup times for these tables. </p><p>When using a global index, users should also consider setting <code>hoodie.bloom.index.update.partition.path=true</code> or <code>hoodie.simple.index.update.partition.path=true</code> to deal with cases where the\npartition path value could change due to an update e.g users table partitioned by home city; user relocates to a different city. These tables are also excellent candidates for the Merge-On-Read table type.</p><p>Going forward, we plan to build <a href=\"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+08+%3A+Record+level+indexing+mechanisms+for+Hudi+datasets?src=contextnavpagetreemode\">record level indexing</a>\nright within Hudi, which will improve the index look-up time and will also avoid additional overhead of maintaining an external system like hbase. </p><h2>Summary</h2><p>Without the indexing capabilities in Hudi, it would not been possible to make upserts/deletes happen at <a href=\"https://eng.uber.com/apache-hudi-graduation/\">very large scales</a>.\nHopefully this post gave you good enough context on the indexing mechanisms today and how different tradeoffs play out. </p><p>Some interesting work underway in this area:</p><ul><li>Apache Flink based writing with a RocksDB state store backed indexing mechanism, unlocking true streaming upserts on data lakes. </li><li>A brand new MetadataIndex, which reimagines the bloom index today on top of the metadata table in Hudi.</li><li>Record level index implementation, as a secondary index using another Hudi table.</li></ul><p>Going forward, this will remain an area of active investment for the project. we are always looking for contributors who can drive these roadmap items forward.\nPlease <a href=\"/contribute/get-involved\">engage</a> with our community if you want to get involved.</p>",
            "url": "https://hudi.apache.org/blog/2020/11/11/hudi-indexing-mechanisms",
            "title": "Employing the right indexes for fast updates, deletes in Apache Hudi",
            "summary": "Apache Hudi employs an index to locate the file group, that an update/delete belongs to. For Copy-On-Write tables, this enables",
            "date_modified": "2020-11-11T00:00:00.000Z",
            "author": {
                "name": "vinoth"
            }
        },
        {
            "id": "/2020/10/21/Architecting-Data-Lakes-for-the-Modern-Enterprise-at-Data-Summit-Connect-Fall-2020",
            "content_html": "<div url=\"https://www.dbta.com/Editorial/News-Flashes/Architecting-Data-Lakes-for-the-Modern-Enterprise-at-Data-Summit-Connect-Fall-2020-143512.aspx\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2020/10/21/Architecting-Data-Lakes-for-the-Modern-Enterprise-at-Data-Summit-Connect-Fall-2020",
            "title": "Architecting Data Lakes for the Modern Enterprise at Data Summit Connect Fall 2020",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2020-10-21T00:00:00.000Z",
            "author": {
                "name": "Stephanie Simone"
            }
        },
        {
            "id": "/2020/10/21/Data-Lake-Change-Capture-using-Apache-Hudi-and-Amazon-AMS-EMR",
            "content_html": "<div url=\"https://towardsdatascience.com/data-lake-change-data-capture-cdc-using-apache-hudi-on-amazon-emr-part-2-process-65e4662d7b4b\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2020/10/21/Data-Lake-Change-Capture-using-Apache-Hudi-and-Amazon-AMS-EMR",
            "title": "Data Lake Change Capture using Apache Hudi & Amazon AMS/EMR",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2020-10-21T00:00:00.000Z",
            "author": {
                "name": "Manoj Kukreja"
            }
        },
        {
            "id": "/2020/10/19/hudi-meets-aws-emr-and-aws-dms",
            "content_html": "<p>This <a href=\"https://aws.amazon.com/blogs/big-data/apply-record-level-changes-from-relational-databases-to-amazon-s3-data-lake-using-apache-hudi-on-amazon-emr-and-aws-database-migration-service/\">blog</a> published by AWS shows how to build a CDC pipeline that captures data from an Amazon Relational Database Service (Amazon RDS) for MySQL database using AWS Database Migration Service (AWS DMS) and applies those changes to a dataset in Amazon S3 using Apache Hudi on Amazon EMR.</p>",
            "url": "https://hudi.apache.org/blog/2020/10/19/hudi-meets-aws-emr-and-aws-dms",
            "title": "Apply record level changes from relational databases to Amazon S3 data lake using Apache Hudi on Amazon EMR and AWS Database Migration Service",
            "summary": "This blog published by AWS shows how to build a CDC pipeline that captures data from an Amazon Relational Database Service (Amazon RDS) for MySQL database using AWS Database Migration Service (AWS DMS) and applies those changes to a dataset in Amazon S3 using Apache Hudi on Amazon EMR.",
            "date_modified": "2020-10-19T00:00:00.000Z",
            "author": {
                "name": "aws"
            }
        },
        {
            "id": "/2020/10/19/Origins-of-Data-Lake-at-Grofers",
            "content_html": "<div url=\"https://lambda.grofers.com/origins-of-data-lake-at-grofers-6c011f94b86c\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2020/10/19/Origins-of-Data-Lake-at-Grofers",
            "title": "Origins of Data Lake at Grofers",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2020-10-19T00:00:00.000Z",
            "author": {
                "name": "Akshay Agarwal"
            }
        },
        {
            "id": "/2020/10/15/apache-hudi-meets-apache-flink",
            "content_html": "<p>Apache Hudi (Hudi for short) is a data lake framework created at Uber. Hudi joined the Apache incubator for incubation in January 2019, and was promoted to the top Apache project in May 2020. It is one of the most popular data lake frameworks.</p><h2>1. Why decouple</h2><p>Hudi has been using Spark as its data processing engine since its birth. If users want to use Hudi as their data lake framework, they must introduce Spark into their platform technology stack.\nA few years ago, using Spark as a big data processing engine can be said to be very common or even natural. Since Spark can either perform batch processing or use micro-batch to simulate streaming, one engine solves both streaming and batch problems.\nHowever, in recent years, with the development of big data technology, Flink, which is also a big data processing engine, has gradually entered people&#x27;s vision and has occupied a certain market in the field of computing engines.\nIn the big data technology community, forums and other territories, the voice of whether Hudi supports Flink has gradually appeared and has become more frequent. Therefore, it is a valuable thing to make Hudi support the Flink engine, and the first step of integrating the Flink engine is that Hudi and Spark are decoupled.</p><p>In addition, looking at the mature, active, and viable frameworks in the big data, all frameworks are elegant in design and can be integrated with other frameworks and leverage each other&#x27;s expertise.\nTherefore, decoupling Hudi from Spark and turning it into an engine-independent data lake framework will undoubtedly create more possibilities for the integration of Hudi and other components, allowing Hudi to better integrate into the big data ecosystem.</p><h2>2. Challenges</h2><p>Hudi&#x27;s internal use of Spark API is as common as our usual development and use of List. Since the data source reads the data, and finally writes the data to the table, Spark RDD is used as the main data structure everywhere, and even ordinary tools are implemented using the Spark API.\nIt can be said that Hudi is a universal data lake framework implemented by Spark. Hudi also leverages deep Spark functionality like custom partitioning, in-memory caching to implement indexing and file sizing using workload heuristics.\nFor some of these, Flink offers better out-of-box support (e.g using Flink’s state store for indexing) and can in fact, make Hudi approach real-time latencies more and more. </p><p>In addition, the primary engine integrated after this decoupling is Flink. Flink and Spark differ greatly in core abstraction. Spark believes that data is bounded, and its core abstraction is a limited set of data.\nFlink believes that the essence of data is a stream, and its core abstract DataStream contains various operations on data. Hudi has a streaming first design (record level updates, record level streams), that arguably fit the Flink model more naturally.\nAt the same time, there are multiple RDDs operating at the same time in Hudi, and the processing result of one RDD is combined with another RDD.\nThis difference in abstraction and the reuse of intermediate results during implementation make it difficult for Hudi to use a unified API to operate both RDD and DataStream in terms of decoupling abstraction.</p><h2>3. Decoupling Spark</h2><p>In theory, Hudi uses Spark as its computing engine to use Spark&#x27;s distributed computing power and RDD&#x27;s rich operator capabilities. Apart from distributed computing power, Hudi uses RDD more as a data structure, and RDD is essentially a bounded data set.\nTherefore, it is theoretically feasible to replace RDD with List (of course, it may sacrifice performance/scale). In order to ensure the performance and stability of the Hudi Spark version as much as possible. We can keep setting the bounded data set as the basic operation unit.\nHudi&#x27;s main operation API remains unchanged, and RDD is extracted as a generic type. The Spark engine implementation still uses RDD, and other engines use List or other bounded  data set according to the actual situation.</p><h3>Decoupling principle</h3><p>1) Unified generics. The input records <code>JavaRDD&lt;HoodieRecord&gt;</code>, key of input records <code>JavaRDD&lt;HoodieKey&gt;</code>, and result of write operations <code>JavaRDD&lt;WriteStatus&gt;</code> used by the Spark API use generic <code>I,K,O</code> instead;</p><p>2) De-sparkization. All APIs of the abstraction layer must have nothing to do with Spark. Involving specific operations that are difficult to implement in the abstract layer, rewrite them as abstract methods and introduce Spark subclasses.</p><p>For example: Hudi uses the <code>JavaSparkContext#map()</code> method in many places. To de-spark, you need to hide the <code>JavaSparkContext</code>. For this problem, we introduced the <code>HoodieEngineContext#map()</code> method, which will block the specific implementation details of <code>map</code>, so as to achieve de-sparkization in abstraction.</p><p>3) Minimize changes to the abstraction layer to ensure the original function and performance of Hudi;</p><p>4) Replace the <code>JavaSparkContext</code> with the <code>HoodieEngineContext</code> abstract class to provide the running environment context.</p><p>In addition, some of the core algorithms in Hudi, like <a href=\"https://github.com/apache/hudi/pull/1756\">rollback</a>, has been redone without the need for computing a workload profile ahead of time, which used to rely on Spark caching. </p><h2>4. Flink integration design</h2><p>Hudi&#x27;s write operation is batch processing in nature, and the continuous mode of <code>DeltaStreamer</code> is realized by looping batch processing. In order to use a unified API, when Hudi integrates Flink, we choose to collect a batch of data before processing, and finally submit it in a unified manner (here we use List to collect data in Flink).\nIn Hudi terminology, we will stream data for a given commit, but only publish the commits every so often, making it practical to scale storage on cloud storage and also tunable.</p><p>The easiest way to think of batch operation is to use a time window. However, when using a window, when there is no data flowing in a window, there will be no output data, and it is difficult for the Flink sink to judge whether all the data from a given batch has been processed.\nTherefore, we use Flink&#x27;s checkpoint mechanism to collect batches. The data between every two barriers is a batch. When there is no data in a subtask, the mock result data is made up.\nIn this way, on the sink side, when each subtask has result data issued, it can be considered that a batch of data has been processed and the commit can be executed.</p><p>The DAG is as follows:</p><p><img src=\"/assets/images/blog/hudi-meets-flink/image1.png\" alt=\"dualism\"/></p><ul><li><strong>Source:</strong> receives Kafka data and converts it into <code>List&lt;HoodieRecord&gt;</code>;</li><li><strong>InstantGeneratorOperator:</strong> generates a globally unique instant. When the previous instant is not completed or the current batch has no data, no new instant is created;</li><li><strong>KeyBy partitionPath:</strong> partitions according to <code>partitionPath</code> to avoid multiple subtasks from writing the same partition;</li><li><strong>WriteProcessOperator:</strong> performs a write operation. When there is no data in the current partition, it sends empty result data to the downstream to make up the number;</li><li><strong>CommitSink:</strong> receives the calculation results of the upstream task. When receiving the parallelism results, it is considered that all the upstream subtasks are completed and the commit is executed.</li></ul><p>Note:\n<code>InstantGeneratorOperator</code> and <code>WriteProcessOperator</code> are both custom Flink operators. <code>InstantGeneratorOperator</code> will block checking the state of the previous instant to ensure that there is only one instant in the global (or requested) state.\n<code>WriteProcessOperator</code> is the actual execution Where a write operation is performed, the write operation is triggered at checkpoint.</p><h3>4.1 Index design based on Flink State</h3><p>Stateful computing is one of the highlights of the Flink engine. Compared with using external storage, using Flink&#x27;s built-in <code>State</code> can significantly improve the performance of Flink applications.\nTherefore, it would be a good choice to implement a Hudi index based on Flink&#x27;s State.</p><p>The core of the Hudi index is to maintain the mapping of the Hudi key <code>HoodieKey</code> and the location of the Hudi data <code>HoodieRecordLocation</code>.\nTherefore, based on the current design, we can simply maintain a <code>MapState&lt;HoodieKey, HoodieRecordLocation&gt;</code> in Flink UDF to map the <code>HoodieKey</code> and <code>HoodieRecordLocation</code>, and leave the fault tolerance and persistence of State to the Flink framework.</p><p><img src=\"/assets/images/blog/hudi-meets-flink/image2.png\" alt=\"dualism\"/></p><h2>5. Implementation examples</h2><h3>1) HoodieTable</h3><pre><code>/**\n  * Abstract implementation of a HoodieTable.\n  *\n  * @param &lt;T&gt; Sub type of HoodieRecordPayload\n  * @param &lt;I&gt; Type of inputs\n  * @param &lt;K&gt; Type of keys\n  * @param &lt;O&gt; Type of outputs\n  */\npublic abstract class HoodieTable&lt;T extends HoodieRecordPayload, I, K, O&gt; implements Serializable {\n\n   protected final HoodieWriteConfig config;\n   protected final HoodieTableMetaClient metaClient;\n   protected final HoodieIndex&lt;T, I, K, O&gt; index;\n\n   public abstract HoodieWriteMetadata&lt;O&gt; upsert(HoodieEngineContext context, String instantTime,\n       I records);\n\n   public abstract HoodieWriteMetadata&lt;O&gt; insert(HoodieEngineContext context, String instantTime,\n       I records);\n\n   public abstract HoodieWriteMetadata&lt;O&gt; bulkInsert(HoodieEngineContext context, String instantTime,\n       I records, Option&lt;BulkInsertPartitioner&lt;I&gt;&gt; bulkInsertPartitioner);\n\n   ...\n}\n</code></pre><p><code>HoodieTable</code> is one of the core abstractions of Hudi, which defines operations such as <code>insert</code>, <code>upsert</code>, and <code>bulkInsert</code> supported by the table.\nTake <code>upsert</code> as an example, the input data is changed from the original <code>JavaRDD&lt;HoodieRecord&gt; inputRdds</code> to <code>I records</code>, and the runtime <code>JavaSparkContext jsc</code> is changed to <code>HoodieEngineContext context</code>.</p><p>From the class annotations, we can see that <code>T, I, K, O</code> represents the load data type, input data type, primary key type and output data type of Hudi operation respectively.\nThese generics will run through the entire abstraction layer.</p><h3>2) HoodieEngineContext</h3><pre><code>/**\n * Base class contains the context information needed by the engine at runtime. It will be extended by different\n * engine implementation if needed.\n */\npublic abstract class HoodieEngineContext {\n\n  public abstract &lt;I, O&gt; List&lt;O&gt; map(List&lt;I&gt; data, SerializableFunction&lt;I, O&gt; func, int parallelism);\n\n  public abstract &lt;I, O&gt; List&lt;O&gt; flatMap(List&lt;I&gt; data, SerializableFunction&lt;I, Stream&lt;O&gt;&gt; func, int parallelism);\n\n  public abstract &lt;I&gt; void foreach(List&lt;I&gt; data, SerializableConsumer&lt;I&gt; consumer, int parallelism);\n\n  ......\n}\n</code></pre><p><code>HoodieEngineContext</code> plays the role of <code>JavaSparkContext</code>, it not only provides all the information that <code>JavaSparkContext</code> can provide,\nbut also encapsulates many methods such as <code>map</code>, <code>flatMap</code>, <code>foreach</code>, and hides The specific implementation of <code>JavaSparkContext#map()</code>,<code>JavaSparkContext#flatMap()</code>, <code>JavaSparkContext#foreach()</code> and other methods.</p><p>Take the <code>map</code> method as an example. In the Spark implementation class <code>HoodieSparkEngineContext</code>, the <code>map</code> method is as follows:</p><pre><code>  @Override\n  public &lt;I, O&gt; List&lt;O&gt; map(List&lt;I&gt; data, SerializableFunction&lt;I, O&gt; func, int parallelism) {\n    return javaSparkContext.parallelize(data, parallelism).map(func::apply).collect();\n  }\n</code></pre><p>In the engine that operates List, the implementation can be as follows (different methods need to pay attention to thread-safety issues, use <code>parallel()</code> with caution):</p><pre><code>  @Override\n  public &lt;I, O&gt; List&lt;O&gt; map(List&lt;I&gt; data, SerializableFunction&lt;I, O&gt; func, int parallelism) {\n    return data.stream().parallel().map(func::apply).collect(Collectors.toList());\n  }\n</code></pre><p>Note:\nThe exception thrown in the map function can be solved by wrapping <code>SerializableFunction&lt;I, O&gt; func</code>.</p><p>Here is a brief introduction to <code>SerializableFunction</code>:</p><pre><code>@FunctionalInterface\npublic interface SerializableFunction&lt;I, O&gt; extends Serializable {\n  O apply(I v1) throws Exception;\n}\n</code></pre><p>This method is actually a variant of <code>java.util.function.Function</code>. The difference from <code>java.util.function.Function</code> is that <code>SerializableFunction</code> can be serialized and can throw exceptions.\nThis function is introduced because the input parameters that the <code>JavaSparkContext#map()</code> function can receive must be serializable.\nAt the same time, there are many exceptions that need to be thrown in the logic of Hudi, and the code for <code>try-catch</code> in the Lambda expression will be omitted It is bloated and not very elegant.</p><h2>6. Current progress and follow-up plan</h2><h3>6.1 Working time axis</h3><p><img src=\"/assets/images/blog/hudi-meets-flink/image3.png\" alt=\"dualism\"/></p><p><a href=\"https://www.t3go.cn/\">T3go</a>\n<a href=\"https://cn.aliyun.com/\">Aliyun</a>\n<a href=\"https://www.sf-express.com/cn/sc/\">SF-express</a></p><h3>6.2 Follow-up plan</h3><h4>1) Promote the integration of Hudi and Flink</h4><p>Push the integration of Flink and Hudi to the community as soon as possible. In the initial stage, this feature may only support Kafka data sources.</p><h4>2) Performance optimization</h4><p>In order to ensure the stability and performance of the Hudi-Spark version, the decoupling did not take too much into consideration the possible performance problems of the Flink version.</p><h4>3) flink-connector-hudi like third-party package development</h4><p>Make the binding of Hudi-Flink into a third-party package. Users can this third-party package to read/write from/to Hudi with Flink.</p>",
            "url": "https://hudi.apache.org/blog/2020/10/15/apache-hudi-meets-apache-flink",
            "title": "Apache Hudi meets Apache Flink",
            "summary": "Apache Hudi (Hudi for short) is a data lake framework created at Uber. Hudi joined the Apache incubator for incubation in January 2019, and was promoted to the top Apache project in May 2020. It is one of the most popular data lake frameworks.",
            "date_modified": "2020-10-15T00:00:00.000Z",
            "author": {
                "name": "wangxianghu"
            }
        },
        {
            "id": "/2020/10/06/cdc-solution-using-hudi-by-nclouds",
            "content_html": "<p>This <a href=\"https://aws.amazon.com/blogs/apn/how-nclouds-helps-accelerate-data-delivery-with-apache-hudi-on-amazon-emr/\">blog</a> published by nClouds in partnership with AWS shows how to build a CDC pipeline using Apache Hudi on Amazon EMR and other managed services like Amazon RDS and AWS DMS, including Amazon QuickSight for data visualization.</p>",
            "url": "https://hudi.apache.org/blog/2020/10/06/cdc-solution-using-hudi-by-nclouds",
            "title": "How nClouds Helps Accelerate Data Delivery with Apache Hudi on Amazon EMR",
            "summary": "This blog published by nClouds in partnership with AWS shows how to build a CDC pipeline using Apache Hudi on Amazon EMR and other managed services like Amazon RDS and AWS DMS, including Amazon QuickSight for data visualization.",
            "date_modified": "2020-10-06T00:00:00.000Z",
            "author": {
                "name": "nclouds"
            }
        },
        {
            "id": "/2020/08/22/ingest-multiple-tables-using-hudi",
            "content_html": "<p>When building a change data capture pipeline for already existing or newly created relational databases, one of the most common problems that one faces is simplifying the onboarding process for multiple tables. Ingesting multiple tables to Hudi dataset at a single go is now possible using <code>HoodieMultiTableDeltaStreamer</code> class which is a wrapper on top of the more popular <code>HoodieDeltaStreamer</code> class. Currently <code>HoodieMultiTableDeltaStreamer</code> supports <strong>COPY_ON_WRITE</strong> storage type only and the ingestion is done in a <strong>sequential</strong> way.</p><p>This blog will guide you through configuring and running <code>HoodieMultiTableDeltaStreamer</code>.</p><h3>Configuration</h3><ul><li><code>HoodieMultiTableDeltaStreamer</code> expects users to maintain table wise overridden properties in separate files in a dedicated config folder. Common properties can be configured via common properties file also.</li><li>By default, hudi datasets are created under the path <code>&lt;base-path-prefix&gt;/&lt;database_name&gt;/&lt;name_of_table_to_be_ingested&gt;</code>. You need to provide the names of tables to be ingested via the property <code>hoodie.deltastreamer.ingestion.tablesToBeIngested</code> in the format <code>&lt;database&gt;.&lt;table&gt;</code>, for example </li></ul><pre><code class=\"language-java\">hoodie.deltastreamer.ingestion.tablesToBeIngested=db1.table1,db2.table2\n</code></pre><ul><li>If you do not provide database name, then it is assumed the table belongs to default database and the hudi dataset for the concerned table is created under the path <code>&lt;base-path-prefix&gt;/default/&lt;name_of_table_to_be_ingested&gt;</code>. Also there is a provision to override the default path for hudi datasets. You can create hudi dataset for a particular table by setting the property <code>hoodie.deltastreamer.ingestion.targetBasePath</code> in table level config file</li><li>There are a lot of properties that one might like to override per table, for example</li></ul><pre><code class=\"language-java\">hoodie.datasource.write.recordkey.field=_row_key\nhoodie.datasource.write.partitionpath.field=created_at\nhoodie.deltastreamer.source.kafka.topic=topic2\nhoodie.deltastreamer.keygen.timebased.timestamp.type=UNIX_TIMESTAMP\nhoodie.deltastreamer.keygen.timebased.input.dateformat=yyyy-MM-dd HH:mm:ss.S\nhoodie.datasource.hive_sync.table=short_trip_uber_hive_dummy_table\nhoodie.deltastreamer.ingestion.targetBasePath=s3:///temp/hudi/table1\n</code></pre><ul><li>Properties like above need to be set for every table to be ingested. As already suggested at the beginning, users are expected to maintain separate config files for every table by setting the below property</li></ul><pre><code class=\"language-java\">hoodie.deltastreamer.ingestion.&lt;db&gt;.&lt;table&gt;.configFile=s3:///tmp/config/config1.properties\n</code></pre><p>If you do not want to set the above property for every table, you can simply create config files for every table to be ingested under the config folder with the name - <code>&lt;database&gt;_&lt;table&gt;_config.properties</code>. For example if you want to ingest table1 and table2 from dummy database, where config folder is set to <code>s3:///tmp/config</code>, then you need to create 2 config files on the given paths - <code>s3:///tmp/config/dummy_table1_config.properties</code> and <code>s3:///tmp/config/dummy_table2_config.properties</code>.</p><ul><li>Finally you can specify all the common properties in a common properties file. Common properties file does not necessarily have to lie under config folder but it is advised to keep it along with other config files. This file will contain the below properties</li></ul><pre><code class=\"language-java\">hoodie.deltastreamer.ingestion.tablesToBeIngested=db1.table1,db2.table2\nhoodie.deltastreamer.ingestion.db1.table1.configFile=s3:///tmp/config_table1.properties\nhoodie.deltastreamer.ingestion.db2.table2.configFile=s3:///tmp/config_table2.properties\n</code></pre><h3>Run Command</h3><p><code>HoodieMultiTableDeltaStreamer</code> can be run similar to how one runs <code>HoodieDeltaStreamer</code>. Please refer to the example given below for the command. </p><h3>Example</h3><p>Suppose you want to ingest table1 and table2 from db1 and want to ingest the 2 tables under the path <code>s3:///temp/hudi</code>. You can ingest them using the below command</p><pre><code class=\"language-java\">[hoodie]$ spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieMultiTableDeltaStreamer `ls packaging/hudi-utilities-bundle/target/hudi-utilities-bundle-*.jar` \\\n  --props s3:///temp/hudi-ingestion-config/kafka-source.properties \\\n  --config-folder s3:///temp/hudi-ingestion-config \\\n  --schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \\\n  --source-class org.apache.hudi.utilities.sources.AvroKafkaSource \\\n  --source-ordering-field impresssiontime \\\n  --base-path-prefix s3:///temp/hudi \\ \n  --target-table dummy_table \\\n  --op UPSERT\n</code></pre><p>s3:///temp/config/kafka-source.properties</p><pre><code class=\"language-java\">hoodie.deltastreamer.ingestion.tablesToBeIngested=db1.table1,db1.table2\nhoodie.deltastreamer.ingestion.db1.table1.configFile=s3:///temp/hudi-ingestion-config/config_table1.properties\nhoodie.deltastreamer.ingestion.db21.table2.configFile=s3:///temp/hudi-ingestion-config/config_table2.properties\n\n#Kafka props\nbootstrap.servers=localhost:9092\nauto.offset.reset=earliest\nschema.registry.url=http://localhost:8081\n\nhoodie.datasource.write.keygenerator.class=org.apache.hudi.keygen.CustomKeyGenerator\n</code></pre><p>s3:///temp/hudi-ingestion-config/config_table1.properties</p><pre><code class=\"language-java\">hoodie.datasource.write.recordkey.field=_row_key1\nhoodie.datasource.write.partitionpath.field=created_at\nhoodie.deltastreamer.source.kafka.topic=topic1\n</code></pre><p>s3:///temp/hudi-ingestion-config/config_table2.properties</p><pre><code class=\"language-java\">hoodie.datasource.write.recordkey.field=_row_key2\nhoodie.datasource.write.partitionpath.field=created_at\nhoodie.deltastreamer.source.kafka.topic=topic2\n</code></pre><p>Contributions are welcome for extending multiple tables ingestion support to <strong>MERGE_ON_READ</strong> storage type and enabling <code>HoodieMultiTableDeltaStreamer</code> ingest multiple tables parallely. </p><p>Happy ingesting!</p>",
            "url": "https://hudi.apache.org/blog/2020/08/22/ingest-multiple-tables-using-hudi",
            "title": "Ingest multiple tables using Hudi",
            "summary": "When building a change data capture pipeline for already existing or newly created relational databases, one of the most common problems that one faces is simplifying the onboarding process for multiple tables. Ingesting multiple tables to Hudi dataset at a single go is now possible using HoodieMultiTableDeltaStreamer class which is a wrapper on top of the more popular HoodieDeltaStreamer class. Currently HoodieMultiTableDeltaStreamer supports COPY_ON_WRITE storage type only and the ingestion is done in a sequential way.",
            "date_modified": "2020-08-22T00:00:00.000Z",
            "author": {
                "name": "pratyakshsharma"
            }
        },
        {
            "id": "/2020/08/21/async-compaction-deployment-model",
            "content_html": "<p>We will look at different deployment models for executing compactions asynchronously.</p><h2>Compaction</h2><p>For Merge-On-Read table, data is stored using a combination of columnar (e.g parquet) + row based (e.g avro) file formats.\nUpdates are logged to delta files &amp; later compacted to produce new versions of columnar files synchronously or\nasynchronously. One of th main motivations behind Merge-On-Read is to reduce data latency when ingesting records.\nHence, it makes sense to run compaction asynchronously without blocking ingestion.</p><h2>Async Compaction</h2><p>Async Compaction is performed in 2 steps:</p><ol><li><strong><em>Compaction Scheduling</em></strong>: This is done by the ingestion job. In this step, Hudi scans the partitions and selects <strong>file\nslices</strong> to be compacted. A compaction plan is finally written to Hudi timeline.</li><li><strong><em>Compaction Execution</em></strong>: A separate process reads the compaction plan and performs compaction of file slices.</li></ol><h2>Deployment Models</h2><p>There are few ways by which we can execute compactions asynchronously. </p><h3>Spark Structured Streaming</h3><p>With 0.6.0, we now have support for running async compactions in Spark\nStructured Streaming jobs. Compactions are scheduled and executed asynchronously inside the\nstreaming job.  Async Compactions are enabled by default for structured streaming jobs\non Merge-On-Read table.</p><p>Here is an example snippet in java</p><pre><code class=\"language-properties\">import org.apache.hudi.DataSourceWriteOptions;\nimport org.apache.hudi.HoodieDataSourceHelpers;\nimport org.apache.hudi.config.HoodieCompactionConfig;\nimport org.apache.hudi.config.HoodieWriteConfig;\n\nimport org.apache.spark.sql.streaming.OutputMode;\nimport org.apache.spark.sql.streaming.ProcessingTime;\n\n\n DataStreamWriter&lt;Row&gt; writer = streamingInput.writeStream().format(&quot;org.apache.hudi&quot;)\n        .option(DataSourceWriteOptions.OPERATION_OPT_KEY(), operationType)\n        .option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY(), tableType)\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), &quot;_row_key&quot;)\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), &quot;partition&quot;)\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), &quot;timestamp&quot;)\n        .option(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP, &quot;10&quot;)\n        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE_OPT_KEY(), &quot;true&quot;)\n        .option(HoodieWriteConfig.TABLE_NAME, tableName).option(&quot;checkpointLocation&quot;, checkpointLocation)\n        .outputMode(OutputMode.Append());\n writer.trigger(new ProcessingTime(30000)).start(tablePath);\n</code></pre><h3>DeltaStreamer Continuous Mode</h3><p>Hudi DeltaStreamer provides continuous ingestion mode where a single long running spark application<br/>\n<!-- -->ingests data to Hudi table continuously from upstream sources. In this mode, Hudi supports managing asynchronous\ncompactions. Here is an example snippet for running in continuous mode with async compactions</p><pre><code class=\"language-properties\">spark-submit --packages org.apache.hudi:hudi-utilities-bundle_2.11:0.6.0 \\\n--class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer \\\n--table-type MERGE_ON_READ \\\n--target-base-path &lt;hudi_base_path&gt; \\\n--target-table &lt;hudi_table&gt; \\\n--source-class org.apache.hudi.utilities.sources.JsonDFSSource \\\n--source-ordering-field ts \\\n--schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider \\\n--props /path/to/source.properties \\\n--continous\n</code></pre><h3>Hudi CLI</h3><p>Hudi CLI is yet another way to execute specific compactions asynchronously. Here is an example </p><pre><code class=\"language-properties\">hudi:trips-&gt;compaction run --tableName &lt;table_name&gt; --parallelism &lt;parallelism&gt; --compactionInstant &lt;InstantTime&gt;\n...\n</code></pre><h3>Hudi Compactor Script</h3><p>Hudi provides a standalone tool to also execute specific compactions asynchronously. Here is an example</p><pre><code class=\"language-properties\">spark-submit --packages org.apache.hudi:hudi-utilities-bundle_2.11:0.6.0 \\\n--class org.apache.hudi.utilities.HoodieCompactor \\\n--base-path &lt;base_path&gt; \\\n--table-name &lt;table_name&gt; \\\n--instant-time &lt;compaction_instant&gt; \\\n--schema-file &lt;schema_file&gt;\n</code></pre>",
            "url": "https://hudi.apache.org/blog/2020/08/21/async-compaction-deployment-model",
            "title": "Async Compaction Deployment Models",
            "summary": "We will look at different deployment models for executing compactions asynchronously.",
            "date_modified": "2020-08-21T00:00:00.000Z",
            "author": {
                "name": "vbalaji"
            }
        },
        {
            "id": "/2020/08/20/efficient-migration-of-large-parquet-tables",
            "content_html": "<p>We will look at how to migrate a large parquet table to Hudi without having to rewrite the entire dataset. </p><h2>Motivation:</h2><p>Apache Hudi maintains per record metadata to perform core operations such as upserts and incremental pull. To take advantage of Hudi’s upsert and incremental processing support, users would need to rewrite their whole dataset to make it an Apache Hudi table.  Hudi 0.6.0 comes with an <strong><em>experimental feature</em></strong> to support efficient migration of large Parquet tables to Hudi without the need to rewrite the entire dataset.</p><h2>High Level Idea:</h2><h3>Per Record Metadata:</h3><p>Apache Hudi maintains record level metadata for perform efficient upserts and incremental pull.</p><p><img src=\"/assets/images/blog/2020-08-20-per-record.png\" alt=\"Per Record Metadata\"/></p><p>Apache HUDI physical file contains 3 parts</p><ol><li>For each record, 5 HUDI metadata fields with column indices 0 to 4</li><li>For each record, the original data columns that comprises the record (Original Data)</li><li>Additional Hudi Metadata at file footer for index lookup</li></ol><p>The parts (1) and (3) constitute what we term as  “Hudi skeleton”. Hudi skeleton contains additional metadata that it maintains in each physical parquet file for supporting Hudi primitives. The conceptual idea is to decouple Hudi skeleton data from original data (2). Hudi skeleton can be stored in a Hudi file while the original data is stored in an external non-Hudi file. A migration of large parquet would result in creating only Hudi skeleton files without having to rewrite original data.</p><p><img src=\"/assets/images/blog/2020-08-20-skeleton.png\" alt=\"skeleton\"/></p><h2>Design Deep Dive:</h2><p> For a deep dive on the internals, please take a look at the <a href=\"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+12+%3A+Efficient+Migration+of+Large+Parquet+Tables+to+Apache+Hudi\">RFC document</a> </p><h2>Migration:</h2><p>Hudi supports 2 modes when migrating parquet tables.  We will use the term bootstrap and migration interchangeably in this document.  </p><ul><li>METADATA_ONLY : In this mode, record level metadata alone is generated for each source record and stored in new bootstrap location.</li><li>FULL_RECORD : In this mode, record level metadata is generated for each source record and both original record and metadata for each record copied</li></ul><p>You can pick and choose these modes at partition level. One of the common strategy would be to use FULL_RECORD mode for a small set of &quot;hot&quot; partitions which are accessed more frequently and METADATA_ONLY for a larger set of &quot;warm&quot; partitions. </p><h3>Query Engine Support:</h3><p>For a METADATA_ONLY bootstrapped table, Spark - data source, Spark-Hive and native Hive query engines are supported. Presto support is in the works.</p><h3>Ways To Migrate :</h3><p>There are 2 ways to migrate a large parquet table to Hudi. </p><ul><li>Spark Datasource Write</li><li>Hudi DeltaStreamer</li></ul><p>We will look at how to migrate using both these approaches.</p><h3>Configurations:</h3><p>These are bootstrap specific configurations that needs to be set in addition to regular hudi write configurations.</p><table><thead><tr><th>Configuration Name</th><th>Default</th><th>Mandatory ?</th><th>Description</th></tr></thead><tbody><tr><td>hoodie.bootstrap.base.path</td><td></td><td>Yes</td><td>Base Path of  source parquet table.</td></tr><tr><td>hoodie.bootstrap.parallelism</td><td>1500</td><td>Yes</td><td>Spark Parallelism used when running bootstrap</td></tr><tr><td>hoodie.bootstrap.keygen.class</td><td></td><td>Yes</td><td>Bootstrap Index internally used by Hudi to map Hudi skeleton and source parquet files.</td></tr><tr><td>hoodie.bootstrap.mode.selector</td><td>org.apache.hudi.client.bootstrap.selector.MetadataOnlyBootstrapModeSelector</td><td>Yes</td><td>Bootstap Mode Selector class. By default, Hudi employs METADATA_ONLY boostrap for all partitions.</td></tr><tr><td>hoodie.bootstrap.partitionpath.translator.class</td><td>org.apache.hudi.client.bootstrap.translator. IdentityBootstrapPartitionPathTranslator</td><td>No</td><td>For METADATA_ONLY bootstrap, this class allows customization of partition paths used in Hudi target dataset. By default, no customization is done and the partition paths reflects what is available in source parquet table.</td></tr><tr><td>hoodie.bootstrap.full.input.provider</td><td>org.apache.hudi.bootstrap.SparkParquetBootstrapDataProvider</td><td>No</td><td>For FULL_RECORD bootstrap, this class provides the input RDD of Hudi records to write.</td></tr><tr><td>hoodie.bootstrap.mode.selector.regex.mode</td><td>METADATA_ONLY</td><td>No</td><td>Bootstrap Mode used when the partition matches the regex pattern in hoodie.bootstrap.mode.selector.regex . Used only when hoodie.bootstrap.mode.selector set to BootstrapRegexModeSelector.</td></tr><tr><td>hoodie.bootstrap.mode.selector.regex</td><td>.<!-- -->*</td><td>No</td><td>Partition Regex used when  hoodie.bootstrap.mode.selector set to BootstrapRegexModeSelector.</td></tr></tbody></table><h3>Spark Data Source:</h3><p>Here, we use a Spark Datasource Write to perform bootstrap.\nHere is an example code snippet to perform METADATA_ONLY bootstrap.</p><pre><code class=\"language-properties\">import org.apache.hudi.{DataSourceWriteOptions, HoodieDataSourceHelpers}\nimport org.apache.hudi.config.{HoodieBootstrapConfig, HoodieWriteConfig}\nimport org.apache.hudi.keygen.SimpleKeyGenerator\nimport org.apache.spark.sql.SaveMode\n \nval bootstrapDF = spark.emptyDataFrame\nbootstrapDF.write\n      .format(&quot;hudi&quot;)\n      .option(HoodieWriteConfig.TABLE_NAME, &quot;hoodie_test&quot;)\n      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.BOOTSTRAP_OPERATION_OPT_VAL)\n      .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY, &quot;_row_key&quot;)\n      .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY, &quot;datestr&quot;)\n      .option(HoodieBootstrapConfig.BOOTSTRAP_BASE_PATH_PROP, srcPath)\n      .option(HoodieBootstrapConfig.BOOTSTRAP_KEYGEN_CLASS, classOf[SimpleKeyGenerator].getName)\n      .mode(SaveMode.Overwrite)\n      .save(basePath)\n</code></pre><p>Here is an example code snippet to perform METADATA_ONLY bootstrap for August 20 2020 - August 29 2020 partitions and FULL_RECORD bootstrap for other partitions.</p><pre><code class=\"language-properties\">import org.apache.hudi.bootstrap.SparkParquetBootstrapDataProvider\nimport org.apache.hudi.client.bootstrap.selector.BootstrapRegexModeSelector\nimport org.apache.hudi.{DataSourceWriteOptions, HoodieDataSourceHelpers}\nimport org.apache.hudi.config.{HoodieBootstrapConfig, HoodieWriteConfig}\nimport org.apache.hudi.keygen.SimpleKeyGenerator\nimport org.apache.spark.sql.SaveMode\n \nval bootstrapDF = spark.emptyDataFrame\nbootstrapDF.write\n      .format(&quot;hudi&quot;)\n      .option(HoodieWriteConfig.TABLE_NAME, &quot;hoodie_test&quot;)\n      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.BOOTSTRAP_OPERATION_OPT_VAL)\n      .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY, &quot;_row_key&quot;)\n      .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY, &quot;datestr&quot;)\n      .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY, &quot;timestamp&quot;)\n      .option(HoodieBootstrapConfig.BOOTSTRAP_BASE_PATH_PROP, srcPath)\n      .option(HoodieBootstrapConfig.BOOTSTRAP_KEYGEN_CLASS, classOf[SimpleKeyGenerator].getName)\n      .option(HoodieBootstrapConfig.BOOTSTRAP_MODE_SELECTOR, classOf[BootstrapRegexModeSelector].getName)\n      .option(HoodieBootstrapConfig.BOOTSTRAP_MODE_SELECTOR_REGEX, &quot;2020/08/2[0-9]&quot;)\n      .option(HoodieBootstrapConfig.BOOTSTRAP_MODE_SELECTOR_REGEX_MODE, &quot;METADATA_ONLY&quot;)\n      .option(HoodieBootstrapConfig.FULL_BOOTSTRAP_INPUT_PROVIDER, classOf[SparkParquetBootstrapDataProvider].getName)\n      .mode(SaveMode.Overwrite)\n      .save(basePath)\n</code></pre><h3>Hoodie DeltaStreamer:</h3><p>Hoodie Deltastreamer allows bootstrap to be performed using --run-bootstrap command line option.</p><p>If you are planning to use delta-streamer after the initial boostrap to incrementally ingest data to the new hudi dataset, you need to pass either --checkpoint or --initial-checkpoint-provider to set the initial checkpoint for the deltastreamer.</p><p>Here is an example for running METADATA_ONLY bootstrap using Delta Streamer.</p><pre><code class=\"language-properties\">spark-submit --package org.apache.hudi:hudi-spark-bundle_2.11:0.6.0\n--conf &#x27;spark.serializer=org.apache.spark.serializer.KryoSerializer&#x27; \\\n--class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer  \\\n--run-bootstrap \\\n--target-base-path &lt;Hudi_Base_Path&gt; \\\n--target-table &lt;Hudi_Table_Name&gt; \\\n--props &lt;props_file&gt; \\\n--checkpoint &lt;initial_checkpoint_if_you_are_going_to_use_deltastreamer_to_incrementally_ingest&gt; \\\n--hoodie-conf hoodie.bootstrap.base.path=&lt;Parquet_Source_base_Path&gt; \\\n--hoodie-conf hoodie.datasource.write.recordkey.field=_row_key \\\n--hoodie-conf hoodie.datasource.write.partitionpath.field=datestr \\\n--hoodie-conf hoodie.bootstrap.keygen.class=org.apache.hudi.keygen.SimpleKeyGenerator\n</code></pre><pre><code class=\"language-properties\">spark-submit --package org.apache.hudi:hudi-spark-bundle_2.11:0.6.0\n--conf &#x27;spark.serializer=org.apache.spark.serializer.KryoSerializer&#x27; \\\n--class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer  \\\n--run-bootstrap \\\n--target-base-path &lt;Hudi_Base_Path&gt; \\\n--target-table &lt;Hudi_Table_Name&gt; \\\n--props &lt;props_file&gt; \\\n--checkpoint &lt;initial_checkpoint_if_you_are_going_to_use_deltastreamer_to_incrementally_ingest&gt; \\\n--hoodie-conf hoodie.bootstrap.base.path=&lt;Parquet_Source_base_Path&gt; \\\n--hoodie-conf hoodie.datasource.write.recordkey.field=_row_key \\\n--hoodie-conf hoodie.datasource.write.partitionpath.field=datestr \\\n--hoodie-conf hoodie.bootstrap.keygen.class=org.apache.hudi.keygen.SimpleKeyGenerator \\\n--hoodie-conf hoodie.bootstrap.full.input.provider=org.apache.hudi.bootstrap.SparkParquetBootstrapDataProvider \\\n--hoodie-conf hoodie.bootstrap.mode.selector=org.apache.hudi.client.bootstrap.selector.BootstrapRegexModeSelector \\\n--hoodie-conf hoodie.bootstrap.mode.selector.regex=&quot;2020/08/2[0-9]&quot; \\\n--hoodie-conf hoodie.bootstrap.mode.selector.regex.mode=METADATA_ONLY\n</code></pre><h3>Known Caveats</h3><ol><li>Need proper defaults for the bootstrap config : hoodie.bootstrap.full.input.provider. Here is the <a href=\"https://issues.apache.org/jira/browse/HUDI-1213\">ticket</a></li><li>DeltaStreamer manages checkpoints inside hoodie commit files and expects checkpoints in previously committed metadata. Users are expected to pass checkpoint or initial checkpoint provider when performing bootstrap through deltastreamer. Such support is not present when doing bootstrap using Spark Datasource. Here is the <a href=\"https://issues.apache.org/jira/browse/HUDI-1214\">ticket</a>.</li></ol>",
            "url": "https://hudi.apache.org/blog/2020/08/20/efficient-migration-of-large-parquet-tables",
            "title": "Efficient Migration of Large Parquet Tables to Apache Hudi",
            "summary": "We will look at how to migrate a large parquet table to Hudi without having to rewrite the entire dataset.",
            "date_modified": "2020-08-20T00:00:00.000Z",
            "author": {
                "name": "vbalaji"
            }
        },
        {
            "id": "/2020/08/18/hudi-incremental-processing-on-data-lakes",
            "content_html": "<h3>NOTE: This article is a translation of the infoq.cn article, found <a href=\"https://www.infoq.cn/article/CAgIDpfJBVcJHKJLSbhe\">here</a>, with minor edits</h3><p>Apache Hudi is a data lake framework which provides the ability to ingest, manage and query large analytical data sets on a distributed file system/cloud stores.\nHudi joined the Apache incubator for incubation in January 2019, and was promoted to the top Apache project in May 2020. This article mainly discusses the importance\nof Hudi to the data lake from the perspective of &quot;incremental processing&quot;. More information about Apache Hudi&#x27;s framework functions, features, usage scenarios, and\nlatest developments can be found at <a href=\"https://qconplus.infoq.cn/2020/shanghai/presentation/2646\">QCon Global Software Development Conference (Shanghai Station) 2020</a>.</p><p>Throughout the development of big data technology, Hadoop has steadily seized the opportunities of this era and has become the de-facto standard for enterprises to build big data infrastructure.\nAmong them, the distributed file system HDFS that supports the Hadoop ecosystem almost naturally has become the standard interface for big data storage systems. In recent years, with the rise of\ncloud-native architectures, we have seen a wave of newer models embracing low-cost cloud storage emerging, a number of data lake frameworks compatible with HDFS interfaces\nembracing cloud vendor storage have emerged in the industry as well. </p><p>However, we are still processing data pretty much in the same way we did 10 years ago. This article will try to talk about its importance to the data lake from the perspective of &quot;incremental processing&quot;.</p><h2>Traditional data lakes lack the primitives for incremental processing</h2><p>In the era of mobile Internet and Internet of Things, delayed arrival of data is very common.\nHere we are involved in the definition of two time semantics: <a href=\"https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/\">event time and processing time</a>. </p><p>As the name suggests:</p><ul><li><strong>Event time:</strong> the time when the event actually occurred;</li><li><strong>Processing time:</strong> the time when an event is observed (processed) in the system;</li></ul><p>Ideally, the event time and the processing time are the same, but in reality, they may have more or less deviation, which we often call &quot;Time Skew&quot;.\nWhether for low-latency stream computing or common batch processing, the processing of event time and processing time and late data is a common and difficult problem.\nIn general, in order to ensure correctness, when we strictly follow the &quot;event time&quot; semantics, late data will trigger the\n<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/stream/operators/windows#late-elements-considerations\">recalculation of the time window</a>\n(usually Hive partitions for batch processing), although the results of these &quot;windows&quot; may have been calculated or even interacted with the end user.\nFor recalculation, the extensible key-value storage structure is usually used in streaming processing, which is processed incrementally at the record/event level and optimized\nbased on point queries and updates. However, in data lakes, recalculating usually means rewriting the entire (immutable) Hive partition (or simply a folder in DFS), and\nre-triggering the recalculation of cascading tasks that have consumed that Hive partition.</p><p>With data lakes supporting massive amounts of data, many long-tail businesses still have a strong demand for updating cold data. However, for a long time,\nthe data in a single partition in the data lake was designed to be non-updatable. If it needs to be updated, the entire partition needs to be rewritten.\nThis will seriously damage the efficiency of the entire ecosystem. From the perspective of latency and resource utilization, these operations on Hadoop will incur expensive overhead.\nBesides, this overhead is usually also cascaded to the entire Hadoop data processing pipeline, which ultimately leads to an increase in latency by several hours.</p><p>In response to the two problems mentioned above, if the data lake supports fine-grained incremental processing, we can incorporate changes into existing Hive partitions\nmore effectively, and provide a way for downstream table consumers to obtain only the changed data. For effectively supporting incremental processing, we can decompose it into the\nfollowing two primitive operations:</p><ul><li><p><strong>Update insert (upsert):</strong> Conceptually, rewriting the entire partition can be regarded as a very inefficient upsert operation, which will eventually write much more data than the\noriginal data itself. Therefore, support for (bulk) upsert is considered a very important feature. <a href=\"https://research.google/pubs/pub42851/\">Google&#x27;s Mesa</a> (Google&#x27;s data warehouse system) also\ntalks about several techniques that can be applied to rapid data ingestion scenarios.</p></li><li><p><strong>Incremental consumption:</strong> Although upsert can solve the problem of quickly releasing new data to a partition, downstream data consumers do not know\nwhich data has been changed from which time in the past. Usually, consumers can only know the changed data by scanning the entire partition/data table and\nrecalculating all the data, which requires considerable time and resources. Therefore, we also need a mechanism to more efficiently obtain data records that\nhave changed since the last time the partition was consumed.</p></li></ul><p>With the above two primitive operations, you can upsert a data set, and then incrementally consume from it, and create another (also incremental) data set to solve the two problems\nwe mentioned above and support many common cases, so as to support end-to-end incremental processing and reduce end-to-end latency. These two primitives combine with each other,\nunlocking the ability of stream/incremental processing based on DFS abstraction.</p><p>The storage scale of the data lake far exceeds that of the data warehouse. Although the two have different focuses on the definition of functions,\nthere is still a considerable intersection (of course, there are still disputes and deviations from definition and implementation.\nThis is not the topic this article tries to discuss). In any case, the data lake will support larger analytical data sets with cheaper storage,\nso incremental processing is also very important for it. Next let&#x27;s discuss the significance of incremental processing for the data lake.</p><h2>The significance of incremental processing for the data lake</h2><h3>Streaming Semantics</h3><p>It has long been stated that there is a &quot;<a href=\"https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying\">dualism</a>&quot;\nbetween the change log (that is, the &quot;flow&quot; in the conventional sense we understand) and the table.</p><p><img src=\"/assets/images/blog/incr-processing/image4.jpg\" alt=\"dualism\"/></p><p>The core of this discussion is: if there is a change log, you can use these changes to generate a data table and get the current status. If you update a table,\nyou can record these changes and publish all &quot;change logs&quot; to the table&#x27;s status information. This interchangeable nature is called &quot;stream table duality&quot; for short.</p><p>A more general understanding of &quot;stream table duality&quot;: when the business system is modifying the data in the MySQL table, MySQL will reflect these changes as Binlog,\nif we publish these continuous Binlog (stream) to Kafka, and then let the downstream processing system subscribe to the Kafka, and use the state store to gradually\naccumulate the intermediate results. Then the current state of this intermediate result can reflects the current snapshot of the table.</p><p>If the two primitives mentioned above that support incremental processing can be introduced to the data lake, the above pipeline, which can reflect the\n&quot;stream table duality&quot;, is also applicable on the data lake. Based on the first primitive, the data lake can also ingest the Binlog log streams in Kafka,\nand then store these Binlog log streams into &quot;tables&quot; on the data lake. Based on the second primitive, these tables recognize the changed records as &quot;Binlog&quot;\nstreams to support the incremental consumption of subsequent cascading tasks.</p><p>Of course, as the data in the data lake needs to be landed on the final file/object storage, considering the trade-off between throughput and write performance,\nBinlog on the data lake reacts to a small batch of change logs over a period of time on the stream. For example, the Apache Hudi community is further trying to\nprovide an incremental view similar to Binlog for different Commits (a Commit refers to a batch of data write commit),\nas shown in the following figure:</p><p><img src=\"/assets/images/blog/incr-processing/image1.png\" alt=\"idu\"/></p><p>Remarks in the &quot;Flag&quot; column:</p><p>I: Insert;\nD: Delete;\nU: After image of Update;\nX: Before image of Update;</p><p>Based on the above discussion, we can think that incremental processing and stream are naturally compatible, and we can naturally connect them on the data lake.</p><h3>Warehousing needs Incremental Processing</h3><p>In the data warehouse, whether it is dimensional modeling or relational modeling theory, it is usually constructed based on the <a href=\"https://en.wikipedia.org/wiki/Data_warehouse#Design_methods\">layered design ideas</a>.\nIn terms of technical implementation, multiple stages (steps) of a long pipeline are formed by connecting multiple levels of ETL tasks through a workflow scheduling engine,\nas shown in the following figure:</p><p><img src=\"/assets/images/blog/incr-processing/image2.png\" alt=\"image2\"/></p><p>As the main application of the data warehouse, in the OLAP field, for the conventional business scenarios(for no or few changes), there are already some frameworks in the industry\nthat focus on the scenarios where they are good at providing efficient analysis capabilities. However, in the Hadoop data warehouse/data lake ecosystem,\nthere is still no good solution for the analysis scenario of frequent changes of business data.</p><p>For example, let’s consider the scenario of updating the order status of a travel business. This scenario has a typical long-tail effect:\nyou cannot know whether an order will be billed tomorrow, one month later, or one year later. In this scenario, the order table is the main data table,\nbut usually we will derive other derived tables based on this table to support the modeling of various business scenarios.\nThe initial update takes place in the order table at the ODS level, but the derived tables need to be updated in cascade.</p><p>For this scenario, in the past, once there is a change, people usually need to find the partition where the data to be updated is located in the Hive order\ntable of the ODS layer, and update the entire partition, besides, the partition of the relevant data of the derived table needs to be updated in cascade.</p><p>Yes, someone will definitely think of that Kudu&#x27;s support for Upsert can solve the problem of the old version of Hive missing the first incremental primitive.\nBut the Kudu storage engine has its own limitations:</p><ol><li>Performance: additional requirements for the hardware itself;</li><li>Ecologically: In terms of adapting to mainstream big data computing frameworks and machine learning frameworks, it is far less advantageous than Hive;</li><li>Cost: requires special maintenance costs and expenses;</li><li>Did not solve the second primitive of incremental processing mentioned above: the problem of incremental consumption.</li></ol><p>In summary, incremental processing has the following advantages on the data lake:</p><p><strong>Performance improvement:</strong> Ingesting data usually needs to handle updates, deletes, and enforce unique key constraints. Since incremental primitives support record-level updates,\nit can bring orders of magnitude performance improvements to these operations. </p><p><strong>Faster ETL/derived Pipelines:</strong> An ubiquitous next step, once the data has been ingested from external sources is to build derived data pipelines using\nApache Spark/Apache Hive or any other data processing framework to ETL the ingested data for a variety of use-cases like data warehouse,\nmachine learning, or even just analytics. Typically, such processes again rely on batch processing jobs expressed in code or SQL. Such data pipelines can be speed up dramatically,\nby querying one or more input tables using an incremental query instead of a regular snapshot query, resulting in only processing the incremental changes from upstream tables and\nthen upsert or delete the target derived table.Similar to raw data ingestion, in order to reduce the data delay of the modelled table, the ETL job only needs to gradually extract the\nchanged data from the original table and update the previously derived output table instead of rebuilding the entire output table every few hours .</p><p><strong>Unified storage:</strong> Based on the above two advantages, faster and lighter processing on the existing data lake means that only for the purpose of accessing near real-time data,\nno special storage or data mart is needed.</p><p>Next, we use two simple examples to illustrate how <a href=\"https://www.oreilly.com/content/ubers-case-for-incremental-processing-on-hadoop/\">incremental processing</a> can speed up the processing\nof pipelines in analytical scenarios. First of all, data projection is the most common and easy to understand case:</p><p><img src=\"/assets/images/blog/incr-processing/image7.png\" alt=\"image7\"/></p><p>This simple example shows that: by upserting new changes into table_1 and establishing a simple projected table (projected_table) through incremental consumption, we can\noperate simpler with lower latency more efficiently projection.</p><p>Next, for a more complex scenario, we can use incremental processing to support the stream and batch connections supported by the stream computing framework,\nand stream-stream connections (just need to add some additional logic to align window) :</p><p><img src=\"/assets/images/blog/incr-processing/image6.png\" alt=\"image6\"/></p><p>The example in the figure above connects a fact table to multiple dimension tables to create a connected table. This case is one of the rare scenarios where we can save hardware\ncosts while significantly reducing latency.</p><h3>Quasi-real-time scenarios, resource/efficiency trade-offs</h3><p>Incremental processing of new data in mini batches can use resources more efficiently. Let&#x27;s refer to a specific example. We have a Kafka event stream that is pouring in\nat a rate of 10,000 per second. We want to count the number of messages in some dimensions over the past 15 minutes. Many stream processing pipelines use an external/internal\nresult state store (such as RocksDB, Cassandra, ElasticSearch) to save the aggregated count results, and run the containers in resource managers such as YARN/Mesos continuously,\nwhich is very reasonable in less than a five-minute delay window scene. In fact, the YARN container itself has some startup overhead. In addition, in order to improve the\nperformance of writing to result storage system, we usually cache the results before performing batch updates. This kind of protocol requires the container to run continuously.</p><p>However, in quasi-real-time processing scenarios, these options may not be optimal. To achieve the same effect, you can use short-life containers and optimize overall\nresource utilization. For example, a streaming processor may need to perform six million updates to the result storage system in 15 minutes. However, in the incremental\nbatch mode, we only need to perform an in-memory merge on the accumulated data and update the result storage system only once, then only use the resource container for\nfive minutes. Compared with the pure stream processing mode, the incremental batch processing mode has several times the CPU efficiency improvement, and there are several\norders of magnitude efficiency improvement in updating to the result storage. Basically, this processing method obtains resources on demand, instead of swallowing CPU and\nmemory while waiting for data to be calculated in real time.</p><h3>Incremental processing facilitates unified data lake architecture</h3><p>Whether in the data warehouse or in the data lake, data processing is an unavoidable problem. Data processing involves the selection of computing engines and\nthe design of architectures. There are currently two mainstream architectures in the industry: Lambda and Kappa architectures. Each architecture has its own\ncharacteristics and existing problems. Derivative versions of these architectures are also <a href=\"https://www.infoq.cn/article/Uo4pFswlMzBVhq*Y2tB9\">emerging endlessly</a>.</p><p>In reality, many enterprises still maintain the implementation of the <a href=\"https://en.wikipedia.org/wiki/Lambda_architecture\">Lambda architecture</a>.\nThe typical Lambda architecture has two modules for the data processing part: the speed layer and the batch layer.</p><p><img src=\"/assets/images/blog/incr-processing/image5.png\" alt=\"image5\"/></p><p>They are usually two independent implementations (from code to infrastructure). For example, Flink (formerly Storm) is a popular option on the speed layer,\nwhile MapReduce/Spark can serve as a batch layer. In fact, people often rely on the speed layer to provide updated results (which may not be accurate), and\nonce the data is considered complete, the results of the speed layer are corrected at a later time through the batch layer. With incremental processing,\nwe have the opportunity to implement the Lambda architecture for batch processing and quasi-real-time processing at the code level and infrastructure level in\na unified manner. It typically looks like below:</p><p><img src=\"/assets/images/blog/incr-processing/image3.png\" alt=\"image3\"/></p><p>As we said, you can use SQL or a batch processing framework like Spark to consistently implement your processing logic. The result table is built incrementally,\nand SQL is executed on &quot;new data&quot; like streaming to produce a quick view of the results. The same SQL can be executed periodically on the full amount of data to\ncorrect any inaccurate results (remember, join operations are always tricky!) and produce a more &quot;complete&quot; view of the results. In both cases, we will use the\nsame infrastructure to perform calculations, which can reduce overall operating costs and complexity.</p><p>Setting aside the Lambda architecture, even in the Kappa architecture, the first primitive of incremental processing (upsert) also plays an important role.\nUber <a href=\"https://www.slideshare.net/FlinkForward/flink-forward-san-francisco-2019-moving-from-lambda-and-kappa-architectures-to-kappa-at-uber-roshan-naik\">proposed</a> the Kappa + architecture\nbased on this. The Kappa architecture advocates a single stream computing layer sufficient to become a general solution\nfor data processing. Although the batch layer is removed in this model, there are still two problems in the service layer:</p><p>Now days many stream processing engines support row-level data processing, which requires that our service layer should also support row-level updates;\nThe trade-offs between data ingestion delay, scanning performance and computing resources and operational complexity are unavoidable.</p><p><img src=\"/assets/images/blog/incr-processing/image8.png\" alt=\"image8\"/></p><p>However, if our business scenarios have low latency requirements, for example, we can accept a delay of about 10 minutes. And if we can quickly ingest and prepare data on DFS,\neffectively connect and propagate updates to the upper-level modeling data set, Speed Serving in the service layer is unnecessary. Then the service layer can be unified,\ngreatly reducing the overall complexity and resource consumption of the system.</p><p>Above, we introduced the significance of incremental processing for the data lake. Next, we introduce the implementation and support of incremental processing.\nAmong the three open source data lake frameworks (Apache Hudi/Iceberg, Delta Lake), only Apache Hudi provides good support for incremental processing.\nThis is completely rooted in a framework developed by Uber at the time when it encountered the pain points of data analysis on the Hadoop data lake.\nSo, next, let&#x27;s introduce how Hudi supports incremental processing.</p><h2>Hudi&#x27;s support for incremental processing</h2><p>Apache Hudi (Hadoop Upserts Deletes and Incrementals) is a top-level project of the Apache Foundation. It allows you to process very large-scale data on\ntop of Hadoop-compatible storage, and it also provides two primitives that enable stream processing on the data lake in addition to classic batch processing.</p><p>From the naming of the letter &quot;I&quot; denotes &quot;Incremental Processing&quot;, we can see that it will support incremental processing as a first class citizen.\nThe two primitives we mentioned at the beginning of this article that support incremental processing are reflected in the following two aspects in Apache Hudi:</p><p>Update/Delete operation:Hudi provides support for updating/deleting records, using fine-grained file/record level indexes while providing transactional guarantees\nfor the write operation. Queries process the last such committed snapshot, to produce results..</p><p>Change stream: Hudi also provides first-class support for obtaining an incremental stream of all the records that were updated/inserted/deleted in a given table, from a given point-in-time.</p><p>The specific implementation of the change flow is &quot;incremental view&quot;. Hudi is the only one of the three open source data lake frameworks that supports\nthe incremental query feature, with support for record level change streams. The following sample code snippet shows us how to query the incremental view:</p><pre><code class=\"language-java\">// spark-shell\n// reload data\nspark.\n  read.\n  format(&quot;hudi&quot;).\n  load(basePath + &quot;/*/*/*/*&quot;).\n  createOrReplaceTempView(&quot;hudi_trips_snapshot&quot;)\n\nval commits = spark.sql(&quot;select distinct(_hoodie_commit_time) as commitTime from  hudi_trips_snapshot order by commitTime&quot;).map(k =&gt; k.getString(0)).take(50)\nval beginTime = commits(commits.length - 2) // commit time we are interested in\n\n// incrementally query data\nval tripsIncrementalDF = spark.read.format(&quot;hudi&quot;).\n  option(QUERY_TYPE_OPT_KEY, QUERY_TYPE_INCREMENTAL_OPT_VAL).\n  option(BEGIN_INSTANTTIME_OPT_KEY, beginTime).\n  load(basePath)\ntripsIncrementalDF.createOrReplaceTempView(&quot;hudi_trips_incremental&quot;)\n\nspark.sql(&quot;select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from  hudi_trips_incremental where fare &gt; 20.0&quot;).show()\n\n</code></pre><p>The code snippet above creates a Hudi trip increment table (hudi_trips_incremental), and then queries all the change records in the increment table after the &quot;beginTime&quot; submission time\nand the &quot;cost&quot;  is greater than 20.0. Based on this query, you can create incremental data pipelines on batch data.</p><h2>Summary</h2><p>In this article, we first elaborated many problems caused by the lack of incremental processing primitives in the traditional Hadoop data warehouse due to the trade-off between data integrity\nand latency, and some long-tail applications that rely heavily on updates. Next, we argued that to support incremental processing, we must have at least two primitives: upsert and\nincremental consumption, and explained why these two primitives can solve the problems explained above.</p><p>Then, we introduced why incremental processing is also important to the data lake. There are many common parts in data processing between the data lake and the data warehouse.\nIn the data warehouse, some &quot;pain points&quot; caused by the lack of incremental processing also exist in the data lake. We elaborated its significance to the data lake from four\naspects: incremental processing of semantics of natural fit flow, the need for analytical scenarios, quasi-real-time scene resource/efficiency trade-offs, and unified lake architecture.</p><p>Finally, we introduced the open source data lake storage framework Apache Hudi&#x27;s support for incremental processing and simple cases.</p>",
            "url": "https://hudi.apache.org/blog/2020/08/18/hudi-incremental-processing-on-data-lakes",
            "title": "Incremental Processing on the Data Lake",
            "summary": "NOTE: This article is a translation of the infoq.cn article, found here, with minor edits",
            "date_modified": "2020-08-18T00:00:00.000Z",
            "author": {
                "name": "vinoyang"
            }
        },
        {
            "id": "/2020/08/04/PrestoDB-and-Apache-Hudi",
            "content_html": "<div url=\"https://prestodb.io/blog/2020/08/04/prestodb-and-hudi\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2020/08/04/PrestoDB-and-Apache-Hudi",
            "title": "PrestoDB and Apache Hudi",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2020-08-04T00:00:00.000Z",
            "author": {
                "name": "Bhavani Sudha Saktheeswaran"
            }
        },
        {
            "id": "/2020/06/16/Apache-Hudi-grows-cloud-data-lake-maturity",
            "content_html": "<div url=\"https://searchdatamanagement.techtarget.com/news/252484740/Apache-Hudi-grows-cloud-data-lake-maturity\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2020/06/16/Apache-Hudi-grows-cloud-data-lake-maturity",
            "title": "Apache Hudi grows cloud data lake maturity",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2020-06-16T00:00:00.000Z",
            "author": {
                "name": "Sean Michael Kerner"
            }
        },
        {
            "id": "/2020/06/09/Building-a-Large-scale-Transactional-Data-Lake-at-Uber-Using-Apache-Hudi",
            "content_html": "<div url=\"https://eng.uber.com/apache-hudi-graduation/\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2020/06/09/Building-a-Large-scale-Transactional-Data-Lake-at-Uber-Using-Apache-Hudi",
            "title": "Building a Large-scale Transactional Data Lake at Uber Using Apache Hudi",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2020-06-09T00:00:00.000Z",
            "author": {
                "name": "Nishith Agarwal"
            }
        },
        {
            "id": "/2020/06/04/The-Apache-Software-Foundation-Announces-Apache-Hudi-as-a-Top-Level-Project",
            "content_html": "<div url=\"https://blogs.apache.org/foundation/entry/the-apache-software-foundation-announces64\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2020/06/04/The-Apache-Software-Foundation-Announces-Apache-Hudi-as-a-Top-Level-Project",
            "title": "The Apache Software Foundation Announces Apache® Hudi™ as a Top-Level Project",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2020-06-04T00:00:00.000Z"
        },
        {
            "id": "/2020/05/28/monitoring-hudi-metrics-with-datadog",
            "content_html": "<h2>Availability</h2><p><strong>0.6.0 (unreleased)</strong></p><h2>Introduction</h2><p><a href=\"https://www.datadoghq.com/\">Datadog</a> is a popular monitoring service. In the upcoming <code>0.6.0</code> release of Apache Hudi, we will introduce the feature of reporting Hudi metrics via Datadog HTTP API, in addition to the current reporter types: Graphite and JMX.</p><h2>Configurations</h2><p>Similar to other supported reporters, turning on Datadog reporter requires these 2 properties.</p><pre><code class=\"language-properties\">hoodie.metrics.on=true\nhoodie.metrics.reporter.type=DATADOG\n</code></pre><p>The following property sets the Datadog API site. It determines whether the requests will be sent to <code>api.datadoghq.eu</code> (EU) or <code>api.datadoghq.com</code> (US). Set this according to your Datadog account settings.</p><pre><code class=\"language-properties\">hoodie.metrics.datadog.api.site=EU # or US\n</code></pre><p>The property <code>hoodie.metrics.datadog.api.key</code> allows you to set the api key directly from the configuration. </p><pre><code class=\"language-properties\">hoodie.metrics.datadog.api.key=&lt;your api key&gt;\nhoodie.metrics.datadog.api.key.supplier=&lt;your api key supplier&gt;\n</code></pre><p>Due to security consideration in some cases, you may prefer to return the api key at runtime. To go with this approach, implement <code>java.util.function.Supplier&lt;String&gt;</code> and set the implementation&#x27;s FQCN to <code>hoodie.metrics.datadog.api.key.supplier</code>, and make sure <code>hoodie.metrics.datadog.api.key</code> is <em>not</em> set since it will take higher precedence.</p><p>The following property helps segregate metrics by setting different prefixes for different jobs. </p><pre><code class=\"language-properties\">hoodie.metrics.datadog.metric.prefix=&lt;your metrics prefix&gt;\n</code></pre><p>Note that it will use <code>.</code> to delimit the prefix and the metric name. For example, if the prefix is set to <code>foo</code>, then <code>foo.</code> will be prepended to the metric name.</p><p>There are other optional properties, which are explained in the configuration reference page.</p><h2>Demo</h2><p>In this demo, we ran a <code>HoodieDeltaStreamer</code> job with metrics turn on and other configurations set properly. </p><p><img src=\"/assets/images/blog/2020-05-28-datadog-metrics-demo.png\" alt=\"datadog metrics demo\"/></p><p>As shown above, we were able to collect Hudi&#x27;s action-related metrics like</p><ul><li><code>&lt;prefix&gt;.&lt;table name&gt;.commit.totalScanTime</code></li><li><code>&lt;prefix&gt;.&lt;table name&gt;.clean.duration</code></li><li><code>&lt;prefix&gt;.&lt;table name&gt;.index.lookup.duration</code></li></ul><p>as well as <code>HoodieDeltaStreamer</code>-specific metrics</p><ul><li><code>&lt;prefix&gt;.&lt;table name&gt;.deltastreamer.duration</code></li><li><code>&lt;prefix&gt;.&lt;table name&gt;.deltastreamer.hiveSyncDuration</code></li></ul>",
            "url": "https://hudi.apache.org/blog/2020/05/28/monitoring-hudi-metrics-with-datadog",
            "title": "Monitor Hudi metrics with Datadog",
            "summary": "Availability",
            "date_modified": "2020-05-28T00:00:00.000Z",
            "author": {
                "name": "rxu"
            }
        },
        {
            "id": "/2020/04/27/apache-hudi-apache-zepplin",
            "content_html": "<h2>1. Introduction</h2><p>Apache Zeppelin is a web-based notebook that provides interactive data analysis. It is convenient for you to make beautiful documents that can be data-driven, interactive, and collaborative, and supports multiple languages, including Scala (using Apache Spark), Python (Apache Spark), SparkSQL, Hive, Markdown, Shell, and so on. Hive and SparkSQL currently support querying Hudi’s read-optimized view and real-time view. So in theory, Zeppelin’s notebook should also have such query capabilities.</p><h2>2. Achieve the effect</h2><h3>2.1 Hive</h3><h3>2.1.1 Read optimized view</h3><p><img src=\"/assets/images/blog/read_optimized_view.png\" alt=\"Read Optimized View\"/></p><h3>2.1.2 Real-time view</h3><p><img src=\"/assets/images/blog/real_time_view.png\" alt=\"Real-time View\"/></p><h3>2.2 Spark SQL</h3><h3>2.2.1 Read optimized view</h3><p><img src=\"/assets/images/blog/spark_read_optimized_view.png\" alt=\"Read Optimized View\"/></p><h3>2.2.2 Real-time view</h3><p><img src=\"/assets/images/blog/spark_real_time_view.png\" alt=\"Real-time View\"/></p><h2>3. Common problems</h2><h3>3.1 Hudi package adaptation</h3><p>Zeppelin will load the packages under lib by default when starting. For external dependencies such as Hudi, it is suitable to be placed directly under zeppelin / lib to avoid Hive or Spark SQL not finding the corresponding Hudi dependency on the cluster.</p><h3>3.2 Parquet jar package adaptation</h3><p>The parquet version of the Hudi package is 1.10, and the current parquet version of the CDH cluster is 1.9, so when executing the Hudi table query, many jar package conflict errors will be reported.</p><p><strong>Solution</strong>: upgrade the parquet package to 1.10 in the spark / jars directory of the node where zepeelin is located.\n<strong>Side effects</strong>: The tasks of saprk jobs other than zeppelin assigned to the cluster nodes of parquet 1.10 may fail.\n<strong>Suggestions</strong>: Clients other than zeppelin will also have jar conflicts. Therefore, it is recommended to fully upgrade the spark jar, parquet jar and related dependent jars of the cluster to better adapt to Hudi’s capabilities.</p><h3>3.3 Spark Interpreter adaptation</h3><p>The same SQL using Spark SQL query on Zeppelin will have more records than the hive query.</p><p><strong>Cause of the problem</strong>: When reading and writing Parquet tables to the Hive metastore, Spark SQL will use the Parquet SerDe (SerDe: Serialize / Deserilize for short) for Spark serialization and deserialization, not the Hive’s SerDe, because Spark SQL’s own SerDe has better performance.</p><p>This causes Spark SQL to only query Hudi’s pipeline records, not the final merge result.</p><p><strong>Solution</strong>: set <code>spark.sql.hive.convertMetastoreParquet=false</code></p><ol><li><strong>Method 1</strong>: Edit properties directly on the page**\n<img src=\"/assets/images/blog/spark_edit_properties.png\"/></li><li><strong>Method 2</strong>: Edit <code>zeppelin / conf / interpreter.json</code> and add**</li></ol><pre><code class=\"language-json\">&quot;spark.sql.hive.convertMetastoreParquet&quot;: {\n  &quot;name&quot;: &quot;spark.sql.hive.convertMetastoreParquet&quot;,\n  &quot;value&quot;: false,\n  &quot;type&quot;: &quot;checkbox&quot;\n}\n</code></pre><h2>4. Hudi incremental view</h2><p>For Hudi incremental view, currently only supports pulling by writing Spark code. Considering that Zeppelin has the ability to execute code and shell commands directly on the notebook, later consider packaging these notebooks to query Hudi incremental views in a way that supports SQL.</p>",
            "url": "https://hudi.apache.org/blog/2020/04/27/apache-hudi-apache-zepplin",
            "title": "Apache Hudi Support on Apache Zeppelin",
            "summary": "1. Introduction",
            "date_modified": "2020-04-27T00:00:00.000Z",
            "author": {
                "name": "leesf"
            }
        },
        {
            "id": "/2020/03/22/exporting-hudi-datasets",
            "content_html": "<h3>Copy to Hudi dataset</h3><p>Similar to the existing  <code>HoodieSnapshotCopier</code>, the Exporter scans the source dataset and then makes a copy of it to the target output path.</p><pre><code class=\"language-bash\">spark-submit \\\n  --jars &quot;packaging/hudi-spark-bundle/target/hudi-spark-bundle_2.11-0.6.0-SNAPSHOT.jar&quot; \\\n  --deploy-mode &quot;client&quot; \\\n  --class &quot;org.apache.hudi.utilities.HoodieSnapshotExporter&quot; \\\n      packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.6.0-SNAPSHOT.jar \\\n  --source-base-path &quot;/tmp/&quot; \\\n  --target-output-path &quot;/tmp/exported/hudi/&quot; \\\n  --output-format &quot;hudi&quot;\n</code></pre><h3>Export to json or parquet dataset</h3><p>The Exporter can also convert the source dataset into other formats. Currently only &quot;json&quot; and &quot;parquet&quot; are supported.</p><pre><code class=\"language-bash\">spark-submit \\\n  --jars &quot;packaging/hudi-spark-bundle/target/hudi-spark-bundle_2.11-0.6.0-SNAPSHOT.jar&quot; \\\n  --deploy-mode &quot;client&quot; \\\n  --class &quot;org.apache.hudi.utilities.HoodieSnapshotExporter&quot; \\\n      packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.6.0-SNAPSHOT.jar \\\n  --source-base-path &quot;/tmp/&quot; \\\n  --target-output-path &quot;/tmp/exported/json/&quot; \\\n  --output-format &quot;json&quot;  # or &quot;parquet&quot;\n</code></pre><h3>Re-partitioning</h3><p>When export to a different format, the Exporter takes parameters to do some custom re-partitioning. By default, if neither of the 2 parameters below is given, the output dataset will have no partition.</p><h4><code>--output-partition-field</code></h4><p>This parameter uses an existing non-metadata field as the output partitions. All  <code>_hoodie_*</code>  metadata field will be stripped during export.</p><pre><code class=\"language-bash\">spark-submit \\\n  --jars &quot;packaging/hudi-spark-bundle/target/hudi-spark-bundle_2.11-0.6.0-SNAPSHOT.jar&quot; \\\n  --deploy-mode &quot;client&quot; \\\n  --class &quot;org.apache.hudi.utilities.HoodieSnapshotExporter&quot; \\\n      packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.6.0-SNAPSHOT.jar \\  \n  --source-base-path &quot;/tmp/&quot; \\\n  --target-output-path &quot;/tmp/exported/json/&quot; \\\n  --output-format &quot;json&quot; \\\n  --output-partition-field &quot;symbol&quot;  # assume the source dataset contains a field `symbol`\n</code></pre><p>The output directory will look like this</p><pre><code class=\"language-bash\">`_SUCCESS symbol=AMRS symbol=AYX symbol=CDMO symbol=CRC symbol=DRNA ...`\n</code></pre><h4><code>--output-partitioner</code></h4><p>This parameter takes in a fully-qualified name of a class that implements  <code>HoodieSnapshotExporter.Partitioner</code>. This parameter takes higher precedence than  <code>--output-partition-field</code>, which will be ignored if this is provided.</p><p>An example implementation is shown below:</p><p><strong>MyPartitioner.java</strong></p><pre><code class=\"language-java\">package com.foo.bar;\npublic class MyPartitioner implements HoodieSnapshotExporter.Partitioner {\n\n  private static final String PARTITION_NAME = &quot;date&quot;;\n \n  @Override\n  public DataFrameWriter&lt;Row&gt; partition(Dataset&lt;Row&gt; source) {\n    // use the current hoodie partition path as the output partition\n    return source\n        .withColumnRenamed(HoodieRecord.PARTITION_PATH_METADATA_FIELD, PARTITION_NAME)\n        .repartition(new Column(PARTITION_NAME))\n        .write()\n        .partitionBy(PARTITION_NAME);\n  }\n}\n</code></pre><p>After putting this class in <code>my-custom.jar</code>, which is then placed on the job classpath, the submit command will look like this:</p><pre><code class=\"language-bash\">spark-submit \\\n  --jars &quot;packaging/hudi-spark-bundle/target/hudi-spark-bundle_2.11-0.6.0-SNAPSHOT.jar,my-custom.jar&quot; \\\n  --deploy-mode &quot;client&quot; \\\n  --class &quot;org.apache.hudi.utilities.HoodieSnapshotExporter&quot; \\\n      packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.6.0-SNAPSHOT.jar \\\n  --source-base-path &quot;/tmp/&quot; \\\n  --target-output-path &quot;/tmp/exported/json/&quot; \\\n  --output-format &quot;json&quot; \\\n  --output-partitioner &quot;com.foo.bar.MyPartitioner&quot;\n</code></pre>",
            "url": "https://hudi.apache.org/blog/2020/03/22/exporting-hudi-datasets",
            "title": "Export Hudi datasets as a copy or as different formats",
            "summary": "Copy to Hudi dataset",
            "date_modified": "2020-03-22T00:00:00.000Z",
            "author": {
                "name": "rxu"
            }
        },
        {
            "id": "/2020/01/20/change-capture-using-aws",
            "content_html": "<p>One of the core use-cases for Apache Hudi is enabling seamless, efficient database ingestion to your data lake. Even though a lot has been talked about and even users already adopting this model, content on how to go about this is sparse.</p><p>In this blog, we will build an end-end solution for capturing changes from a MySQL instance running on AWS RDS to a Hudi table on S3, using capabilities in the Hudi  <strong>0.5.1 release</strong></p><p>We can break up the problem into two pieces.</p><ol><li><strong>Extracting change logs from MySQL</strong>  : Surprisingly, this is still a pretty tricky problem to solve and often Hudi users get stuck here. Thankfully, at-least for AWS users, there is a  <a href=\"https://aws.amazon.com/dms/\">Database Migration service</a>  (DMS for short), that does this change capture and uploads them as parquet files on S3</li><li><strong>Applying these change logs to your data lake table</strong>  : Once there are change logs in some form, the next step is to apply them incrementally to your table. This mundane task can be fully automated using the Hudi  <a href=\"http://hudi.apache.org/docs/writing_data#deltastreamer\">DeltaStreamer</a>  tool.</li></ol><p>The actual end-end architecture looks something like this.\n<img src=\"/assets/images/blog/change-capture-architecture.png\" alt=\"enter image description here\"/></p><p>Let&#x27;s now illustrate how one can accomplish this using a simple <em>orders</em> table, stored in MySQL (these instructions should broadly apply to other database engines like Postgres, or Aurora as well, though SQL/Syntax may change)</p><pre><code class=\"language-java\">CREATE DATABASE hudi_dms;\nUSE hudi_dms;\n     \nCREATE TABLE orders(\n   order_id INTEGER,\n   order_qty INTEGER,\n   customer_name VARCHAR(100),\n   updated_at TIMESTAMP DEFAULT NOW() ON UPDATE NOW(),\n   created_at TIMESTAMP DEFAULT NOW(),\n   CONSTRAINT orders_pk PRIMARY KEY(order_id)\n);\n \nINSERT INTO orders(order_id, order_qty, customer_name) VALUES(1, 10, &#x27;victor&#x27;);\nINSERT INTO orders(order_id, order_qty, customer_name) VALUES(2, 20, &#x27;peter&#x27;);\n</code></pre><p>In the table, <em>order_id</em> is the primary key which will be enforced on the Hudi table as well. Since a batch of change records can contain changes to the same primary key, we also include <em>updated_at</em> and <em>created_at</em> fields, which are kept upto date as writes happen to the table.</p><h3>Extracting Change logs from MySQL</h3><p>Before we can configure DMS, we first need to <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/enable-binary-logging-aurora/\">prepare the MySQL instance</a>  for change capture, by ensuring backups are enabled and binlog is turned on.\n<img src=\"/assets/images/blog/change-logs-mysql.png\"/></p><p>Now, proceed to create endpoints in DMS that capture MySQL data and  <a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3\">store in S3, as parquet files</a>.</p><ul><li>Source <em>hudi-source-db</em> endpoint, points to the DB server and provides basic authentication details</li><li>Target <em>parquet-s3</em> endpoint, points to the bucket and folder on s3 to store the change logs records as parquet files\n<img src=\"/assets/images/blog/s3-endpoint-configuration-1.png\"/><img src=\"/assets/images/blog/s3-endpoint-configuration-2.png\"/><img src=\"/assets/images/blog/s3-endpoint-list.png\"/></li></ul><p>Then proceed to create a migration task, as below. Give it a name, connect the source to the target and be sure to pick the right <em>Migration type</em> as shown below, to ensure ongoing changes are continuously replicated to S3. Also make sure to specify, the rules using which DMS decides which MySQL schema/tables to replicate. In this example, we simply whitelist <em>orders</em> table under the <em>hudi_dms</em> schema, as specified in the table SQL above.</p><p><img src=\"/assets/images/blog/s3-migration-task-1.png\"/>\n<img src=\"/assets/images/blog/s3-migration-task-2.png\"/></p><p>Starting the DMS task and should result in an initial load, like below.</p><p><img src=\"/assets/images/blog/dms-task.png\"/></p><p>Simply reading the raw initial load file, shoud give the same values as the upstream table</p><pre><code class=\"language-scala\">scala&gt; spark.read.parquet(&quot;s3://hudi-dms-demo/orders/hudi_dms/orders/*&quot;).sort(&quot;updated_at&quot;).show\n \n+--------+---------+-------------+-------------------+-------------------+\n|order_id|order_qty|customer_name|         updated_at|         created_at|\n+--------+---------+-------------+-------------------+-------------------+\n|       2|       10|        peter|2020-01-20 20:12:22|2020-01-20 20:12:22|\n|       1|       10|       victor|2020-01-20 20:12:31|2020-01-20 20:12:31|\n+--------+---------+-------------+-------------------+-------------------+\n\n</code></pre><h2>Applying Change Logs using Hudi DeltaStreamer</h2><p>Now, we are ready to start consuming the change logs. Hudi DeltaStreamer runs as Spark job on your favorite workflow scheduler (it also supports a continuous mode using <em>--continuous</em> flag, where it runs as a long running Spark job), that tails a given path on S3 (or any DFS implementation) for new files and can issue an <em>upsert</em> to a target hudi dataset. The tool automatically checkpoints itself and thus to repeatedly ingest, all one needs to do is to keep executing the DeltaStreamer periodically.</p><p>With an initial load already on S3, we then run the following command (deltastreamer command, here on) to ingest the full load first and create a Hudi dataset on S3.</p><pre><code class=\"language-bash\">spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer  \\\n  --packages org.apache.spark:spark-avro_2.11:2.4.4 \\\n  --master yarn --deploy-mode client \\\n  hudi-utilities-bundle_2.11-0.5.1-SNAPSHOT.jar \\\n  --table-type COPY_ON_WRITE \\\n  --source-ordering-field updated_at \\\n  --source-class org.apache.hudi.utilities.sources.ParquetDFSSource \\\n  --target-base-path s3://hudi-dms-demo/hudi_orders --target-table hudi_orders \\\n  --transformer-class org.apache.hudi.utilities.transform.AWSDmsTransformer \\\n  --payload-class org.apache.hudi.payload.AWSDmsAvroPayload \\\n  --hoodie-conf hoodie.datasource.write.recordkey.field=order_id,hoodie.datasource.write.partitionpath.field=customer_name,hoodie.deltastreamer.source.dfs.root=s3://hudi-dms-demo/orders/hudi_dms/orders\n</code></pre><p>A few things are going on here</p><ul><li>First, we specify the <em>--table-type</em> as COPY_ON_WRITE. Hudi also supports another _MERGE_ON_READ ty_pe you can use if you choose from.</li><li>To handle cases where the input parquet files contain multiple updates/deletes or insert/updates to the same record, we use <em>updated_at</em> as the ordering field. This ensures that the change record which has the latest timestamp will be reflected in Hudi.</li><li>We specify a target base path and a table table, all needed for creating and writing to the Hudi table</li><li>We use a special payload class - <em>AWSDMSAvroPayload</em> , to handle the different change operations correctly. The parquet files generated have an <em>Op</em> field, that indicates whether a given change record is an insert (I), delete (D) or update (U) and the payload implementation uses this field to decide how to handle a given change record.</li><li>You may also notice a special transformer class <em>AWSDmsTransformer</em> , being specified. The reason here is tactical, but important. The initial load file does not contain an <em>Op</em> field, so this adds one to Hudi table schema additionally.</li><li>Finally, we specify the record key for the Hudi table as same as the upstream table. Then we specify partitioning by <em>customer_name</em>  and also the root of the DMS output.</li></ul><p>Once the command is run, the Hudi table should be created and have same records as the upstream table (with all the _hoodie fields as well).</p><pre><code class=\"language-scala\">scala&gt; spark.read.format(&quot;org.apache.hudi&quot;).load(&quot;s3://hudi-dms-demo/hudi_orders/*/*.parquet&quot;).show\n+-------------------+--------------------+------------------+----------------------+--------------------+--------+---------+-------------+-------------------+-------------------+---+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|order_id|order_qty|customer_name|         updated_at|         created_at| Op|\n+-------------------+--------------------+------------------+----------------------+--------------------+--------+---------+-------------+-------------------+-------------------+---+\n|     20200120205028|  20200120205028_0_1|                 2|                 peter|af9a2525-a486-40e...|       2|       10|        peter|2020-01-20 20:12:22|2020-01-20 20:12:22|   |\n|     20200120205028|  20200120205028_1_1|                 1|                victor|8e431ece-d51c-4c7...|       1|       10|       victor|2020-01-20 20:12:31|2020-01-20 20:12:31|   |\n+-------------------+--------------------+------------------+----------------------+--------------------+--------+---------+-------------+-------------------+-------------------+---+\n</code></pre><p>Now, let&#x27;s do an insert and an update</p><pre><code class=\"language-java\">INSERT INTO orders(order_id, order_qty, customer_name) VALUES(3, 30, &#x27;sandy&#x27;);\nUPDATE orders set order_qty = 20 where order_id = 2;\n</code></pre><p>This will add a new parquet file to the DMS output folder and when the deltastreamer command is run again, it will go ahead and apply these to the Hudi table.</p><p>So, querying the Hudi table now would yield 3 rows and the <em>hoodie_commit_time</em> accurately reflects when these writes happened. You can notice that order_qty for order_id=2, is updated from 10 to 20!</p><pre><code class=\"language-bash\">+-------------------+--------------------+------------------+----------------------+--------------------+---+--------+---------+-------------+-------------------+-------------------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| Op|order_id|order_qty|customer_name|         updated_at|         created_at|\n+-------------------+--------------------+------------------+----------------------+--------------------+---+--------+---------+-------------+-------------------+-------------------+\n|     20200120211526|  20200120211526_0_1|                 2|                 peter|af9a2525-a486-40e...|  U|       2|       20|        peter|2020-01-20 21:11:47|2020-01-20 20:12:22|\n|     20200120211526|  20200120211526_1_1|                 3|                 sandy|566eb34a-e2c5-44b...|  I|       3|       30|        sandy|2020-01-20 21:11:24|2020-01-20 21:11:24|\n|     20200120205028|  20200120205028_1_1|                 1|                victor|8e431ece-d51c-4c7...|   |       1|       10|       victor|2020-01-20 20:12:31|2020-01-20 20:12:31|\n+-------------------+--------------------+------------------+----------------------+--------------------+---+--------+---------+-------------+-------------------+-------------------+\n</code></pre><p>A nice debugging aid would be read all of the DMS output now and sort it by update_at, which should give us a sequence of changes that happened on the upstream table. As we can see, the Hudi table above is a compacted snapshot of this raw change log.</p><pre><code class=\"language-bash\">+----+--------+---------+-------------+-------------------+-------------------+\n|  Op|order_id|order_qty|customer_name|         updated_at|         created_at|\n+----+--------+---------+-------------+-------------------+-------------------+\n|null|       2|       10|        peter|2020-01-20 20:12:22|2020-01-20 20:12:22|\n|null|       1|       10|       victor|2020-01-20 20:12:31|2020-01-20 20:12:31|\n|   I|       3|       30|        sandy|2020-01-20 21:11:24|2020-01-20 21:11:24|\n|   U|       2|       20|        peter|2020-01-20 21:11:47|2020-01-20 20:12:22|\n+----+--------+---------+-------------+-------------------+-------------------+\n</code></pre><p>Initial load with no <em>Op</em> field value , followed by an insert and an update.</p><p>Now, lets do deletes an inserts</p><pre><code class=\"language-java\">DELETE FROM orders WHERE order_id = 2;\nINSERT INTO orders(order_id, order_qty, customer_name) VALUES(4, 40, &#x27;barry&#x27;);\nINSERT INTO orders(order_id, order_qty, customer_name) VALUES(5, 50, &#x27;nathan&#x27;);\n</code></pre><p>This should result in more files on S3, written by DMS , which the DeltaStreamer command will continue to process incrementally (i.e only the newly written files are read each time)</p><p><img src=\"/assets/images/blog/dms-demo-files.png\"/></p><p>Running the deltastreamer command again, would result in the follow state for the Hudi table. You can notice the two new records and that the <em>order_id=2</em> is now gone</p><pre><code class=\"language-bash\">+-------------------+--------------------+------------------+----------------------+--------------------+---+--------+---------+-------------+-------------------+-------------------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| Op|order_id|order_qty|customer_name|         updated_at|         created_at|\n+-------------------+--------------------+------------------+----------------------+--------------------+---+--------+---------+-------------+-------------------+-------------------+\n|     20200120212522|  20200120212522_1_1|                 5|                nathan|3da94b20-c70b-457...|  I|       5|       50|       nathan|2020-01-20 21:23:00|2020-01-20 21:23:00|\n|     20200120212522|  20200120212522_2_1|                 4|                 barry|8cc46715-8f0f-48a...|  I|       4|       40|        barry|2020-01-20 21:22:49|2020-01-20 21:22:49|\n|     20200120211526|  20200120211526_1_1|                 3|                 sandy|566eb34a-e2c5-44b...|  I|       3|       30|        sandy|2020-01-20 21:11:24|2020-01-20 21:11:24|\n|     20200120205028|  20200120205028_1_1|                 1|                victor|8e431ece-d51c-4c7...|   |       1|       10|       victor|2020-01-20 20:12:31|2020-01-20 20:12:31|\n+-------------------+--------------------+------------------+----------------------+--------------------+---+--------+---------+-------------+-------------------+-------------------+\n</code></pre><p>Our little informal change log query yields the following.</p><pre><code class=\"language-bash\">+----+--------+---------+-------------+-------------------+-------------------+\n|  Op|order_id|order_qty|customer_name|         updated_at|         created_at|\n+----+--------+---------+-------------+-------------------+-------------------+\n|null|       2|       10|        peter|2020-01-20 20:12:22|2020-01-20 20:12:22|\n|null|       1|       10|       victor|2020-01-20 20:12:31|2020-01-20 20:12:31|\n|   I|       3|       30|        sandy|2020-01-20 21:11:24|2020-01-20 21:11:24|\n|   U|       2|       20|        peter|2020-01-20 21:11:47|2020-01-20 20:12:22|\n|   D|       2|       20|        peter|2020-01-20 21:11:47|2020-01-20 20:12:22|\n|   I|       4|       40|        barry|2020-01-20 21:22:49|2020-01-20 21:22:49|\n|   I|       5|       50|       nathan|2020-01-20 21:23:00|2020-01-20 21:23:00|\n+----+--------+---------+-------------+-------------------+-------------------+\n</code></pre><p>Note that the delete and update have the same <em>updated_at,</em> value. thus it can very well order differently here.. In short this way of looking at the changelog has its caveats. For a true changelog of the Hudi table itself, you can issue an <a href=\"http://hudi.apache.org/docs/querying_data\">incremental query</a>.</p><p>And Life goes on ..... Hope this was useful to all the data engineers out there!</p>",
            "url": "https://hudi.apache.org/blog/2020/01/20/change-capture-using-aws",
            "title": "Change Capture Using AWS Database Migration Service and Hudi",
            "summary": "One of the core use-cases for Apache Hudi is enabling seamless, efficient database ingestion to your data lake. Even though a lot has been talked about and even users already adopting this model, content on how to go about this is sparse.",
            "date_modified": "2020-01-20T00:00:00.000Z",
            "author": {
                "name": "vinoth"
            }
        },
        {
            "id": "/2020/01/15/delete-support-in-hudi",
            "content_html": "<p>Deletes are supported at a record level in Hudi with 0.5.1 release. This blog is a &quot;how to&quot; blog on how to delete records in hudi. Deletes can be done with 3 flavors: Hudi RDD APIs, with Spark data source and with DeltaStreamer.</p><h3>Delete using RDD Level APIs</h3><p>If you have embedded  <em>HoodieWriteClient</em> , then deletion is as simple as passing in a  <em>JavaRDD<!-- -->&lt;<!-- -->HoodieKey<!-- -->&gt;</em> to the delete api.</p><pre><code class=\"language-java\">// Fetch list of HoodieKeys from elsewhere that needs to be deleted\n// convert to JavaRDD if required. JavaRDD&lt;HoodieKey&gt; toBeDeletedKeys\nList&lt;WriteStatus&gt; statuses = writeClient.delete(toBeDeletedKeys, commitTime);\n</code></pre><h3>Deletion with Datasource</h3><p>Now we will walk through an example of how to perform deletes on a sample dataset using the Datasource API. Quick Start has the same example as below. Feel free to check it out.</p><p><strong>Step 1</strong> : Launch spark shell</p><pre><code class=\"language-bash\">bin/spark-shell --packages org.apache.hudi:hudi-spark-bundle:0.5.1-incubating \\\n  --conf &#x27;spark.serializer=org.apache.spark.serializer.KryoSerializer&#x27;\n</code></pre><p><strong>Step 2</strong> : Import as required and set up table name, etc for sample dataset</p><pre><code class=\"language-scala\">import org.apache.hudi.QuickstartUtils._\nimport scala.collection.JavaConversions._\nimport org.apache.spark.sql.SaveMode._\nimport org.apache.hudi.DataSourceReadOptions._\nimport org.apache.hudi.DataSourceWriteOptions._\nimport org.apache.hudi.config.HoodieWriteConfig._\n     \nval tableName = &quot;hudi_cow_table&quot;\nval basePath = &quot;file:///tmp/hudi_cow_table&quot;\nval dataGen = new DataGenerator\n</code></pre><p><strong>Step 3</strong> : Insert data. Generate some new trips, load them into a DataFrame and write the DataFrame into the Hudi dataset as below.</p><pre><code class=\"language-scala\">val inserts = convertToStringList(dataGen.generateInserts(10))\nval df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\ndf.write.format(&quot;org.apache.hudi&quot;).\n  options(getQuickstartWriteConfigs).\n  option(PRECOMBINE_FIELD_OPT_KEY, &quot;ts&quot;).\n  option(RECORDKEY_FIELD_OPT_KEY, &quot;uuid&quot;).\n  option(PARTITIONPATH_FIELD_OPT_KEY, &quot;partitionpath&quot;).\n  option(TABLE_NAME, tableName).\n  mode(Overwrite).\n  save(basePath);\n</code></pre><p><strong>Note:</strong> For non-partitioned table, set</p><pre><code>option(KEYGENERATOR_CLASS_PROP, &quot;org.apache.hudi.keygen.NonpartitionedKeyGenerator&quot;)\n</code></pre><p> Checkout <a href=\"https://hudi.apache.org/blog/2021/02/13/hudi-key-generators\">https://hudi.apache.org/blog/2021/02/13/hudi-key-generators</a> for more options</p><p><strong>Step 4</strong> : Query data. Load the data files into a DataFrame.</p><pre><code class=\"language-scala\">val roViewDF = spark.read.\n  format(&quot;org.apache.hudi&quot;).\n  load(basePath + &quot;/*/*/*/*&quot;)\nroViewDF.createOrReplaceTempView(&quot;hudi_ro_table&quot;)\nspark.sql(&quot;select count(*) from hudi_ro_table&quot;).show() // should return 10 (number of records inserted above)\nval riderValue = spark.sql(&quot;select distinct rider from hudi_ro_table&quot;).show()\n// copy the value displayed to be used in next step\n</code></pre><p><strong>Step 5</strong> : Fetch records that needs to be deleted, with the above rider value. This example is just to illustrate how to delete. In real world, use a select query using spark sql to fetch records that needs to be deleted and from the result we could invoke deletes as given below. Example rider value used is &quot;rider-213&quot;.</p><pre><code class=\"language-scala\">val df = spark.sql(&quot;select uuid, partitionPath from hudi_ro_table where rider = &#x27;rider-213&#x27;&quot;)\n</code></pre><p>// Replace the above query with any other query that will fetch records to be deleted.</p><p><strong>Step 6</strong> : Issue deletes</p><pre><code class=\"language-scala\">val deletes = dataGen.generateDeletes(df.collectAsList())\nval df = spark.read.json(spark.sparkContext.parallelize(deletes, 2));\ndf.write.format(&quot;org.apache.hudi&quot;).\n  options(getQuickstartWriteConfigs).\n  option(OPERATION_OPT_KEY,&quot;delete&quot;).\n  option(PRECOMBINE_FIELD_OPT_KEY, &quot;ts&quot;).\n  option(RECORDKEY_FIELD_OPT_KEY, &quot;uuid&quot;).\n  option(PARTITIONPATH_FIELD_OPT_KEY, &quot;partitionpath&quot;).\n  option(TABLE_NAME, tableName).\n  mode(Append).\n  save(basePath);\n</code></pre><p><strong>Note:</strong> For non-partitioned table, set</p><pre><code>option(KEYGENERATOR_CLASS_PROP, &quot;org.apache.hudi.keygen.NonpartitionedKeyGenerator&quot;)\n</code></pre><p> Checkout <a href=\"https://hudi.apache.org/blog/2021/02/13/hudi-key-generators\">https://hudi.apache.org/blog/2021/02/13/hudi-key-generators</a> for more options</p><p><strong>Step 7</strong> : Reload the table and verify that the records are deleted</p><pre><code class=\"language-scala\">val roViewDFAfterDelete = spark.\n  read.\n  format(&quot;org.apache.hudi&quot;).\n  load(basePath + &quot;/*/*/*/*&quot;)\nroViewDFAfterDelete.createOrReplaceTempView(&quot;hudi_ro_table&quot;)\nspark.sql(&quot;select uuid, partitionPath from hudi_ro_table where rider = &#x27;rider-213&#x27;&quot;).show() // should not return any rows\n</code></pre><h2>Deletion with HoodieDeltaStreamer</h2><p>Deletion with <code>HoodieDeltaStreamer</code> takes the same path as upsert and so it relies on a specific field called  &quot;<em>_hoodie_is_deleted</em>&quot; of type boolean in each record.</p><ul><li>If a record has the field value set to  <em>false</em> or it&#x27;s not present, then it is considered a regular upsert</li><li>if not (if the value is set to  <em>true</em> ), then its considered to be deleted record.</li></ul><p>This essentially means that the schema has to be changed for the source, to add this field and all incoming records are expected to have this field set. We will be working to relax this in future releases.</p><p>Lets say the original schema is:</p><pre><code class=\"language-json\">{\n  &quot;type&quot;:&quot;record&quot;,\n  &quot;name&quot;:&quot;example_tbl&quot;,\n  &quot;fields&quot;:[{\n     &quot;name&quot;: &quot;uuid&quot;,\n     &quot;type&quot;: &quot;String&quot;\n  }, {\n     &quot;name&quot;: &quot;ts&quot;,\n     &quot;type&quot;: &quot;string&quot;\n  },  {\n     &quot;name&quot;: &quot;partitionPath&quot;,\n     &quot;type&quot;: &quot;string&quot;\n  }, {\n     &quot;name&quot;: &quot;rank&quot;,\n     &quot;type&quot;: &quot;long&quot;\n  }\n]}\n</code></pre><p>To leverage deletion capabilities of <code>DeltaStreamer</code>, you have to change the schema as below.</p><pre><code class=\"language-json\">{\n  &quot;type&quot;:&quot;record&quot;,\n  &quot;name&quot;:&quot;example_tbl&quot;,\n  &quot;fields&quot;:[{\n     &quot;name&quot;: &quot;uuid&quot;,\n     &quot;type&quot;: &quot;String&quot;\n  }, {\n     &quot;name&quot;: &quot;ts&quot;,\n     &quot;type&quot;: &quot;string&quot;\n  },  {\n     &quot;name&quot;: &quot;partitionPath&quot;,\n     &quot;type&quot;: &quot;string&quot;\n  }, {\n     &quot;name&quot;: &quot;rank&quot;,\n     &quot;type&quot;: &quot;long&quot;\n  }, {\n    &quot;name&quot; : &quot;_hoodie_is_deleted&quot;,\n    &quot;type&quot; : &quot;boolean&quot;,\n    &quot;default&quot; : false\n  }\n]}\n</code></pre><p>Example incoming record for upsert</p><pre><code class=\"language-json\">{\n  &quot;ts&quot;: 0.0,\n  &quot;uuid&quot;:&quot;69cdb048-c93e-4532-adf9-f61ce6afe605&quot;,\n  &quot;rank&quot;: 1034,\n  &quot;partitionpath&quot;:&quot;americas/brazil/sao_paulo&quot;,\n  &quot;_hoodie_is_deleted&quot;:false\n}\n</code></pre><p>Example incoming record that needs to be deleted</p><pre><code class=\"language-json\">{\n  &quot;ts&quot;: 0.0,\n  &quot;uuid&quot;: &quot;19tdb048-c93e-4532-adf9-f61ce6afe10&quot;,\n  &quot;rank&quot;: 1045,\n  &quot;partitionpath&quot;:&quot;americas/brazil/sao_paulo&quot;,\n  &quot;_hoodie_is_deleted&quot;:true\n}\n</code></pre><p>These are one time changes. Once these are in, then the DeltaStreamer pipeline will handle both upserts and deletions within every batch. Each batch could contain a mix of upserts and deletes and no additional step or changes are required after this. Note that this is to perform hard deletion instead of soft deletion.</p>",
            "url": "https://hudi.apache.org/blog/2020/01/15/delete-support-in-hudi",
            "title": "Delete support in Hudi",
            "summary": "Deletes are supported at a record level in Hudi with 0.5.1 release. This blog is a \"how to\" blog on how to delete records in hudi. Deletes can be done with 3 flavors: Hudi RDD APIs, with Spark data source and with DeltaStreamer.",
            "date_modified": "2020-01-15T00:00:00.000Z",
            "author": {
                "name": "shivnarayan"
            }
        },
        {
            "id": "/2019/11/15/New-Insert-Update-Delete-Data-on-S3-with-Amazon-EMR-and-Apache-Hudi",
            "content_html": "<div url=\"https://aws.amazon.com/blogs/aws/new-insert-update-delete-data-on-s3-with-amazon-emr-and-apache-hudi/\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2019/11/15/New-Insert-Update-Delete-Data-on-S3-with-Amazon-EMR-and-Apache-Hudi",
            "title": "New – Insert, Update, Delete Data on S3 with Amazon EMR and Apache Hudi",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2019-11-15T00:00:00.000Z",
            "author": {
                "name": "Danilo Poccia"
            }
        },
        {
            "id": "/2019/10/22/Hudi-On-Hops",
            "content_html": "<div url=\"https://www.diva-portal.org/smash/get/diva2:1413103/FULLTEXT01.pdf\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2019/10/22/Hudi-On-Hops",
            "title": "Hudi On Hops",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2019-10-22T00:00:00.000Z",
            "author": {
                "name": "NETSANET GEBRETSADKAN KIDANE"
            }
        },
        {
            "id": "/2019/09/09/ingesting-database-changes",
            "content_html": "<p>Very simple in just 2 steps.</p><p><strong>Step 1</strong>: Extract new changes to users table in MySQL, as avro data files on DFS</p><pre><code class=\"language-bash\">// Command to extract incrementals using sqoop\nbin/sqoop import \\\n  -Dmapreduce.job.user.classpath.first=true \\\n  --connect jdbc:mysql://localhost/users \\\n  --username root \\\n  --password ******* \\\n  --table users \\\n  --as-avrodatafile \\\n  --target-dir \\ \n  s3:///tmp/sqoop/import-1/users\n</code></pre><p><strong>Step 2</strong>: Use your fav datasource to read extracted data and directly “upsert” the users table on DFS/Hive</p><pre><code class=\"language-scala\">// Spark Datasource\nimport org.apache.hudi.DataSourceWriteOptions._\n// Use Spark datasource to read avro\nval inputDataset = spark.read.avro(&quot;s3://tmp/sqoop/import-1/users/*&quot;);\n     \n// save it as a Hudi dataset\ninputDataset.write.format(&quot;org.apache.hudi”)\n  .option(HoodieWriteConfig.TABLE_NAME, &quot;hoodie.users&quot;)\n  .option(RECORDKEY_FIELD_OPT_KEY(), &quot;userID&quot;)\n  .option(PARTITIONPATH_FIELD_OPT_KEY(),&quot;country&quot;)\n  .option(PRECOMBINE_FIELD_OPT_KEY(), &quot;last_mod&quot;)\n  .option(OPERATION_OPT_KEY(), UPSERT_OPERATION_OPT_VAL())\n  .mode(SaveMode.Append)\n  .save(&quot;/path/on/dfs&quot;);\n</code></pre><p>Alternatively, you can also use the Hudi <a href=\"https://hudi.apache.org/writing_data#deltastreamer\">DeltaStreamer</a> tool with the DFSSource.</p>",
            "url": "https://hudi.apache.org/blog/2019/09/09/ingesting-database-changes",
            "title": "Ingesting Database changes via Sqoop/Hudi",
            "summary": "Very simple in just 2 steps.",
            "date_modified": "2019-09-09T00:00:00.000Z",
            "author": {
                "name": "vinoth"
            }
        },
        {
            "id": "/2019/05/14/registering-dataset-to-hive",
            "content_html": "<p>Hudi hive sync tool typically handles registration of the dataset into Hive metastore. In case, there are issues with quickstart around this, following page shows commands that can be used to do this manually via beeline.  </p><p>Add in the <em>packaging/hoodie-hive-bundle/target/hoodie-hive-bundle-0.4.6-SNAPSHOT.jar,</em> so that Hive can read the Hudi dataset and answer the query.</p><pre><code class=\"language-java\">hive&gt; set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;\nhive&gt; set hive.stats.autogather=false;\nhive&gt; add jar file:///path/to/hoodie-hive-bundle-0.5.2-SNAPSHOT.jar;\nAdded [file:///path/to/hoodie-hive-bundle-0.5.2-SNAPSHOT.jar] to class path\nAdded resources: [file:///path/to/hoodie-hive-bundle-0.5.2-SNAPSHOT.jar]\n</code></pre><p>Then, you need to create a <em>ReadOptimized</em> Hive table as below and register the sample partitions</p><pre><code class=\"language-java\">DROP TABLE hoodie_test;\nCREATE EXTERNAL TABLE hoodie_test(`_row_key` string,\n    `_hoodie_commit_time` string,\n    `_hoodie_commit_seqno` string,\n    rider string,\n    driver string,\n    begin_lat double,\n    begin_lon double,\n    end_lat double,\n    end_lon double,\n    fare double)\n    PARTITIONED BY (`datestr` string)\n    ROW FORMAT SERDE\n    &#x27;org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe&#x27;\n    STORED AS INPUTFORMAT\n    &#x27;com.uber.hoodie.hadoop.HoodieInputFormat&#x27;\n    OUTPUTFORMAT\n    &#x27;org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat&#x27;\n    LOCATION\n    &#x27;hdfs:///tmp/hoodie/sample-table&#x27;;\n     \nALTER TABLE `hoodie_test` ADD IF NOT EXISTS PARTITION (datestr=&#x27;2016-03-15&#x27;) LOCATION &#x27;hdfs:///tmp/hoodie/sample-table/2016/03/15&#x27;;\nALTER TABLE `hoodie_test` ADD IF NOT EXISTS PARTITION (datestr=&#x27;2015-03-16&#x27;) LOCATION &#x27;hdfs:///tmp/hoodie/sample-table/2015/03/16&#x27;;\nALTER TABLE `hoodie_test` ADD IF NOT EXISTS PARTITION (datestr=&#x27;2015-03-17&#x27;) LOCATION &#x27;hdfs:///tmp/hoodie/sample-table/2015/03/17&#x27;;\n     \nset mapreduce.framework.name=yarn;\n</code></pre><p>And you can add a <em>Realtime</em> Hive table, as below</p><pre><code class=\"language-java\">DROP TABLE hoodie_rt;\nCREATE EXTERNAL TABLE hoodie_rt(\n    `_hoodie_commit_time` string,\n    `_hoodie_commit_seqno` string,\n    `_hoodie_record_key` string,\n    `_hoodie_partition_path` string,\n    `_hoodie_file_name` string,\n    timestamp double,\n    `_row_key` string,\n    rider string,\n    driver string,\n    begin_lat double,\n    begin_lon double,\n    end_lat double,\n    end_lon double,\n    fare double)\n    PARTITIONED BY (`datestr` string)\n    ROW FORMAT SERDE\n    &#x27;com.uber.hoodie.hadoop.realtime.HoodieParquetSerde&#x27;\n    STORED AS INPUTFORMAT\n    &#x27;com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat&#x27;\n    OUTPUTFORMAT\n    &#x27;org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat&#x27;\n    LOCATION\n    &#x27;file:///tmp/hoodie/sample-table&#x27;;\n     \nALTER TABLE `hoodie_rt` ADD IF NOT EXISTS PARTITION (datestr=&#x27;2016-03-15&#x27;) LOCATION &#x27;file:///tmp/hoodie/sample-table/2016/03/15&#x27;;\nALTER TABLE `hoodie_rt` ADD IF NOT EXISTS PARTITION (datestr=&#x27;2015-03-16&#x27;) LOCATION &#x27;file:///tmp/hoodie/sample-table/2015/03/16&#x27;;\nALTER TABLE `hoodie_rt` ADD IF NOT EXISTS PARTITION (datestr=&#x27;2015-03-17&#x27;) LOCATION &#x27;file:///tmp/hoodie/sample-table/2015/03/17&#x27;;\n</code></pre>",
            "url": "https://hudi.apache.org/blog/2019/05/14/registering-dataset-to-hive",
            "title": "Registering sample dataset to Hive via beeline",
            "summary": "Hudi hive sync tool typically handles registration of the dataset into Hive metastore. In case, there are issues with quickstart around this, following page shows commands that can be used to do this manually via beeline.",
            "date_modified": "2019-05-14T00:00:00.000Z",
            "author": {
                "name": "vinoth"
            }
        },
        {
            "id": "/2019/03/07/batch-vs-incremental",
            "content_html": "<p><img src=\"/assets/images/blog/batch_vs_incremental.png\"/></p>",
            "url": "https://hudi.apache.org/blog/2019/03/07/batch-vs-incremental",
            "title": "Big Batch vs Incremental Processing",
            "date_modified": "2019-03-07T00:00:00.000Z",
            "author": {
                "name": "vinoth"
            }
        },
        {
            "id": "/2019/01/18/asf-incubation",
            "content_html": "<p>In the coming weeks, we will be moving in our new home on the Apache Incubator.</p>",
            "url": "https://hudi.apache.org/blog/2019/01/18/asf-incubation",
            "title": "Hudi entered Apache Incubator",
            "summary": "In the coming weeks, we will be moving in our new home on the Apache Incubator.",
            "date_modified": "2019-01-18T00:00:00.000Z",
            "author": {
                "name": "admin"
            }
        },
        {
            "id": "/2017/03/12/Hoodie-Uber-Engineerings-Incremental-Processing-Framework-on-Hadoop",
            "content_html": "<div url=\"https://eng.uber.com/hoodie/\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2017/03/12/Hoodie-Uber-Engineerings-Incremental-Processing-Framework-on-Hadoop",
            "title": "Hoodie: Uber Engineering's Incremental Processing Framework on Hadoop",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2017-03-12T00:00:00.000Z",
            "author": {
                "name": "Prasanna Rajaperumal"
            }
        },
        {
            "id": "/2016/12/30/strata-talk-2017",
            "content_html": "<p>We will be presenting Hudi &amp; general concepts around how incremental processing works at Uber.\nCatch our talk <strong>&quot;Incremental Processing on Hadoop At Uber&quot;</strong></p>",
            "url": "https://hudi.apache.org/blog/2016/12/30/strata-talk-2017",
            "title": "Connect with us at Strata San Jose March 2017",
            "summary": "We will be presenting Hudi & general concepts around how incremental processing works at Uber.",
            "date_modified": "2016-12-30T00:00:00.000Z",
            "author": {
                "name": "admin"
            }
        },
        {
            "id": "/2016/08/04/The-Case-for-incremental-processing-on-Hadoop",
            "content_html": "<div url=\"https://www.oreilly.com/ideas/ubers-case-for-incremental-processing-on-hadoop\">Redirecting... please wait!! </div>",
            "url": "https://hudi.apache.org/blog/2016/08/04/The-Case-for-incremental-processing-on-Hadoop",
            "title": "The Case for incremental processing on Hadoop",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2016-08-04T00:00:00.000Z",
            "author": {
                "name": "Vinoth Chandar"
            }
        }
    ]
}