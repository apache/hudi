{
    "version": "https://jsonfeed.org/version/1",
    "title": "Apache Hudi: User-Facing Analytics",
    "home_page_url": "https://hudi.apache.org/blog",
    "description": "Apache Hudi Blog",
    "items": [
        {
            "id": "https://hudi.apache.org/blog/2025/12/03/Mastering-Schema-Evolution-with-Apache-Hudi",
            "content_html": "<span>Redirecting... please wait!! <!-- -->or click <a href=\"https://medium.com/@shaiksameer0045/the-chameleon-architecture-mastering-schema-evolution-with-apache-hudi-446da1a2f0c6\">here</a></span>",
            "url": "https://hudi.apache.org/blog/2025/12/03/Mastering-Schema-Evolution-with-Apache-Hudi",
            "title": "Mastering Schema Evolution with Apache Hudi",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2025-12-03T00:00:00.000Z",
            "author": {
                "name": "Shaik Sameer"
            },
            "tags": [
                "blog",
                "Apache Hudi",
                "Schema Evolution",
                "Data Lakehouse",
                "Lakehouse",
                "Use-case"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2025/12/01/apache-hudi-JD-meetup-asia-2025-recap",
            "content_html": "<hr>\n<p><em>This blog was translated from the <a href=\"https://mp.weixin.qq.com/s/LNMZGl-kXJTblOCO6s0BxQ\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">original blog in Chinese</a>.</em></p>\n<hr>\n<p>Recently, the Apache Hudi Meetup Asia, hosted by JD.com, was successfully held at JD.com Group headquarters. Four technical experts from Onehouse, JD.com, Kuaishou, and Huawei gathered together, not only bringing a preview of Apache Hudi release 1.1, but also sharing their unique approaches to building data lakehouses. From AI scenario support to real-time data processing and cost optimization, each topic directly addressed the pain points that data engineers care about most.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"hudi-community-leader-joined-remotely\">Hudi Community Leader Joined Remotely<a href=\"https://hudi.apache.org/blog/2025/12/01/apache-hudi-JD-meetup-asia-2025-recap#hudi-community-leader-joined-remotely\" class=\"hash-link\" aria-label=\"Direct link to Hudi Community Leader Joined Remotely\" title=\"Direct link to Hudi Community Leader Joined Remotely\" translate=\"no\">​</a></h2>\n<p>First, Vinoth Chandar, CEO &amp; Founder of Onehouse and Apache Hudi PMC Chair, delivered the opening remarks via video. He stated that after eight years of development, Hudi has become an important cornerstone in the data lake domain, and its vision has transformed into widely recognized achievements in the industry. The 1.0 version released last year marked the project's entry into a mature stage, bringing many database-like capabilities to the lakehouse.</p>\n<p>Currently, the community is steadily advancing the 1.x series of versions, focusing on improving Flink performance, launching a new Trino connector, and enhancing interoperability through a pluggable table format layer. Facing the rapid development in the data lake field, Vinoth emphasized that excellent technology and robust design are the keys to long-term success. Hudi has now achieved many capabilities that commercial engines have not been able to deliver, thanks to its intelligent and creative community. Looking ahead, the community will be committed to building Hudi into a storage engine that supports all scenarios from BI to AI, exploring trending areas including unstructured data management and vector search.</p>\n<p>Vinoth specially thanked JD.com for its significant contributions to Apache Hudi. Among the top 100 contributors, 6 were from JD.com. Finally, he also invited more developers to join this vibrant community to jointly promote innovation and development in data infrastructure.</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"image 1\" src=\"https://hudi.apache.org/assets/images/jdpost-image1-98f8473e19ed6ff3c1f91a0a47c779f8.png\" width=\"1075\" height=\"576\" class=\"img_ev3q\"></p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"jd-retail-data-lake-technical-challenges-and-outlook\">JD Retail: Data Lake Technical Challenges and Outlook<a href=\"https://hudi.apache.org/blog/2025/12/01/apache-hudi-JD-meetup-asia-2025-recap#jd-retail-data-lake-technical-challenges-and-outlook\" class=\"hash-link\" aria-label=\"Direct link to JD Retail: Data Lake Technical Challenges and Outlook\" title=\"Direct link to JD Retail: Data Lake Technical Challenges and Outlook\" translate=\"no\">​</a></h2>\n<p>As the co-host of the event, Zhang Ke, Head of AI Infra &amp; Big Data Computing at JD Retail, welcomed guests and attendees who participated in this Meetup. He also pointed out two core challenges facing the data domain:</p>\n<p>At the BI level, the long-standing problem of \"unified stream and batch processing\" has not yet been perfectly solved, forcing data R&amp;D personnel to duplicate work across multiple systems. This requires fundamentally reconstructing the data architecture and finding a new paradigm for unified stream and batch processing.</p>\n<p>At the AI level, with the arrival of the multimodal era, traditional solutions that only handle structured data can no longer meet the needs. Whether it is data supply efficiency for model training, real-time feature computation for recommendation systems, or knowledge base construction required for large models, there is an urgent need for an underlying support system that can unify storage of multimodal data while balancing cost and performance.</p>\n<p>The industry is looking forward to building a storage foundation through open-source technologies like Apache Hudi that can uniformly carry batch processing, stream computing, data analysis, and AI workloads.</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"image 2\" src=\"https://hudi.apache.org/assets/images/jdpost-image2-205cca47f2d4b91f38bc923e5c937be5.jpg\" width=\"4032\" height=\"3024\" class=\"img_ev3q\"></p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"apache-hudi-11-preview-and-ai-native-lakehouse-evolution\">Apache Hudi 1.1 Preview and AI-Native Lakehouse Evolution<a href=\"https://hudi.apache.org/blog/2025/12/01/apache-hudi-JD-meetup-asia-2025-recap#apache-hudi-11-preview-and-ai-native-lakehouse-evolution\" class=\"hash-link\" aria-label=\"Direct link to Apache Hudi 1.1 Preview and AI-Native Lakehouse Evolution\" title=\"Direct link to Apache Hudi 1.1 Preview and AI-Native Lakehouse Evolution\" translate=\"no\">​</a></h2>\n<p>In the session \"Apache Hudi 1.1 Preview and AI-Native Lakehouse Evolution,\" Ethan Guo (Yihua Guo), Data Architecture Engineer at Onehouse and Apache Hudi PMC member, shared Hudi's technical evolution path and future outlook. As the top contributor to the Hudi codebase, he systematically elaborated on the project positioning, version planning, and AI-native architecture.</p>\n<p>Ethan pointed out that Apache Hudi's positioning goes far beyond being an open table format—it is an embedded, headless, distributed database system built on top of cloud storage. Hudi is moving from \"a transactional database on the lakehouse\" toward \"an AI-native Lakehouse platform.\"</p>\n<p>In the then-upcoming 1.1 release (now released), Hudi has achieved several important breakthroughs. Among them, the pluggable table format architecture effectively solves the pain point of format fragmentation in the current data lake ecosystem, enabling users to \"write once, read in multiple formats.\" At the same time, Hudi has deeply optimized Flink integration, solving the throughput bottleneck in streaming writes through an asynchronous generation mechanism, and building a brand-new native writer that achieves end-to-end processing from Avro format to Flink RowData, significantly reducing serialization overhead and GC pressure. Real-world tests showed that Hudi 1.1's throughput performance in streaming lake ingestion scenarios was 3.5 times that of version 1.0.</p>\n<p>Facing new challenges brought by the AI era, Hudi is actively building a native AI data foundation. By supporting unstructured data storage, optimizing column group structures for multimodal data, providing built-in vector indexing capabilities, and building a unified storage layer that supports transactions and version control, Hudi is committed to providing highly real-time, traceable, and easily extensible data support for AI workflows. This series of evolutions will propel Apache Hudi from an excellent data lake framework to a core data infrastructure supporting the AI era.</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"image 3\" src=\"https://hudi.apache.org/assets/images/jdpost-image3-c843eba5797513a93582fb8e5682b52c.jpg\" width=\"4032\" height=\"3024\" class=\"img_ev3q\"></p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"latest-architecture-evolution-of-apache-hudi-at-jdcom\">Latest Architecture Evolution of Apache Hudi at JD.com<a href=\"https://hudi.apache.org/blog/2025/12/01/apache-hudi-JD-meetup-asia-2025-recap#latest-architecture-evolution-of-apache-hudi-at-jdcom\" class=\"hash-link\" aria-label=\"Direct link to Latest Architecture Evolution of Apache Hudi at JD.com\" title=\"Direct link to Latest Architecture Evolution of Apache Hudi at JD.com\" translate=\"no\">​</a></h2>\n<p>In the session \"Latest Architecture Evolution of Apache Hudi at JD.com,\" Han Fei, Head of JD Real-time Data Platform, systematically introduced the latest architectural evolution and implementation results of Hudi in JD's production environment.</p>\n<p>Addressing the performance bottleneck of native MOR tables in high-throughput scenarios, JD's Data Lake team reconstructed the data organization protocol of Hudi MOR tables based on LSM-Tree architecture. By replacing the original \"Avro + Append\" update mode with \"Parquet + Create\" mode, lock-free concurrent write capability was achieved. Combined with a series of optimization methods such as Engine-Native data format, Remote Partitioner strategy, and streaming incremental Compaction scheduling mechanism, read and write performance were significantly improved. Benchmark test results showed that the MOR-LSM solution's read and write performance was 2-10 times that of the native MOR-Avro solution, demonstrating significant technical advantages.</p>\n<p>Facing the growing near-real-time requirements of BI scenarios, streaming dimension widening had gradually become a common challenge for multi-subject domain data processing. Traditional Flink streaming Join had problems such as state bloat and high maintenance complexity. JD's Data Lake team, drawing on Hudi's partial-update multi-stream splicing approach, built an indexing mechanism that supported primary-foreign key mapping. This mechanism efficiently completed streaming dimension association and real-time updates through the coordinated operation of forward and reverse indexes. At the same time, pluggable HBase was introduced as index storage, ensuring high-performance access capability in point query scenarios.</p>\n<p>In exploring AI scenarios, the team designed and implemented the Hudi NativeIO SDK. This SDK builds four core modules: data invocation layer, cross-language Transformation layer, Hudi view management layer, and high-performance query layer, creating an end-to-end process for sample training engines to complete training directly based on data lake tables.</p>\n<p>JD had deeply integrated these capabilities with business scenarios, applying them to the near-real-time transformation of the traffic data warehouse ADM layer. After a series of optimizations, the write throughput of the traffic browsing link increased from 45 million per minute to 80 million, Compaction execution efficiency doubled, and real-time consistency maintenance of SKU dimension information was achieved, completing a comprehensive transformation from T+1 offline repair mode to real-time processing mode.</p>\n<p>While promoting self-developed technology, JD also actively gave back to the open-source community, with a total of 109 contributed and merged PRs. In the future, the team will continue to deepen Hudi's application in the real-time data lake domain, providing stronger data support capabilities for business innovation.</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"image 4\" src=\"https://hudi.apache.org/assets/images/jdpost-image4-0bc644b6a57a3145ebccc064d497349e.jpg\" width=\"4032\" height=\"3024\" class=\"img_ev3q\"></p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"how-kuaishous-real-time-lake-ingestion-empowered-bi--ai-scenario-architecture-upgrade\">How Kuaishou's Real-time Lake Ingestion Empowered BI &amp; AI Scenario Architecture Upgrade<a href=\"https://hudi.apache.org/blog/2025/12/01/apache-hudi-JD-meetup-asia-2025-recap#how-kuaishous-real-time-lake-ingestion-empowered-bi--ai-scenario-architecture-upgrade\" class=\"hash-link\" aria-label=\"Direct link to How Kuaishou's Real-time Lake Ingestion Empowered BI &amp; AI Scenario Architecture Upgrade\" title=\"Direct link to How Kuaishou's Real-time Lake Ingestion Empowered BI &amp; AI Scenario Architecture Upgrade\" translate=\"no\">​</a></h2>\n<p>In the session \"How Kuaishou's Real-time Lake Ingestion Empowers BI &amp; AI Scenario Architecture Upgrade,\" Wang Zeyu, Data Architecture R&amp;D Engineer at Kuaishou, introduced Kuaishou's complete evolution path and practical experience in building a real-time data lake based on Apache Hudi.</p>\n<p>For traditional BI data warehouse scenarios, Kuaishou achieved an architecture upgrade from Mysql2Hive to Mysql2Hudi2.0. By introducing Hudi hourly partition tables, supporting multiple query modes such as full, incremental, and snapshot, and innovatively designing Full Compact and Minor Compact mechanisms to optimize data layout, Kuaishou improved the overall architecture. The introduction of bucket heterogeneity allowed full partitions and incremental partitions to support different bucket numbers, significantly reducing lake ingestion resource consumption. Compared with the original architecture, the new solution naturally supported long lifecycles and richer query behaviors. While reducing storage costs, it achieved a leap in data readiness time from day-level to minute-level.</p>\n<p>At the AI storage architecture level, Kuaishou built a unified stream-batch data lake architecture, solving the core pain point of inconsistent offline and real-time training data. Through unified storage media, support for unified stream-batch consumption, logical wide table column splicing, and other capabilities, unified management and efficient reuse of training data were achieved. The metadata management mechanism based on Event-time timeline not only ensured data orderliness but also guaranteed real-time write performance through lock-free design.</p>\n<p>In the future, Kuaishou will continue to improve the data lake's service capabilities in training, retrieval, analysis, and other multi-scenarios, promoting the evolution of the data lake toward a more intelligent and unified direction. Kuaishou's practice fully proves that the real-time data lake architecture based on Hudi can effectively support the modernization and upgrade needs of large-scale BI and AI scenarios.</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"image 5\" src=\"https://hudi.apache.org/assets/images/jdpost-image5-07e8b4eb07ad352e0410e07c0aab1f61.jpg\" width=\"4032\" height=\"3024\" class=\"img_ev3q\"></p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"deep-optimization-and-ai-exploration-of-apache-hudi-on-huawei-cloud\">Deep Optimization and AI Exploration of Apache Hudi on Huawei Cloud<a href=\"https://hudi.apache.org/blog/2025/12/01/apache-hudi-JD-meetup-asia-2025-recap#deep-optimization-and-ai-exploration-of-apache-hudi-on-huawei-cloud\" class=\"hash-link\" aria-label=\"Direct link to Deep Optimization and AI Exploration of Apache Hudi on Huawei Cloud\" title=\"Direct link to Deep Optimization and AI Exploration of Apache Hudi on Huawei Cloud\" translate=\"no\">​</a></h2>\n<p>In the session \"Deep Optimization and AI Exploration of Apache Hudi on Huawei Cloud,\" Yang Xuan, Big Data Lakehouse Kernel R&amp;D Engineer at Huawei, shared Huawei Cloud's technical practices and innovative breakthroughs in building a new generation Lakehouse architecture based on Apache Hudi. Facing challenges in real-time performance, intelligence, and management efficiency for enterprise-level data platforms, Huawei conducted in-depth exploration in three dimensions: platform architecture, kernel optimization, and ecosystem integration.</p>\n<p>At the platform architecture level, Huawei developed the LDMS unified lakehouse management service platform, achieving fully managed operation and maintenance of table services. Through core capabilities such as intelligent data layout optimization and CBO statistics collection, this platform significantly reduced the operational complexity of the lakehouse platform, allowing users to focus more on business logic rather than underlying maintenance.</p>\n<p>In terms of kernel optimization, Huawei made multiple deep modifications to Apache Hudi. Through de-Avro serialization optimization implemented via RFC-84/87, Flink write performance improved up to 10 times while significantly reducing GC pressure; the innovative LogIndex mechanism effectively solved the streaming read performance bottleneck in object storage scenarios; dynamic Schema change support made CDC lake ingestion processes more flexible; and the introduction of the column clustering mechanism provided a feasible solution for real-time processing of thousand-column sparse wide tables.</p>\n<p>Hudi Native built a high-performance IO acceleration layer by rewriting Parquet read/write logic using Rust and adopting Arrow memory format to replace Avro. By providing a unified high-performance Java read/write interface through JNI, it achieved seamless integration with compute engines such as Spark and Flink, laying a solid foundation for future performance breakthroughs.</p>\n<p>In ecosystem integration and AI exploration, Huawei built a management architecture supporting multimodal data. By using lake table formats to manage metadata of unstructured data, with actual files stored in object storage, it ensured ACID properties while avoiding data redundancy. At the same time, it integrated LanceDB to provide efficient vector retrieval capabilities, providing comprehensive data infrastructure support for AI application scenarios such as document retrieval and intelligent Q&amp;A.</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"image 6\" src=\"https://hudi.apache.org/assets/images/jdpost-image6-87ec3eaac37a810679ae6090c8953f9f.jpg\" width=\"4032\" height=\"3024\" class=\"img_ev3q\"></p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"conclusion\">Conclusion<a href=\"https://hudi.apache.org/blog/2025/12/01/apache-hudi-JD-meetup-asia-2025-recap#conclusion\" class=\"hash-link\" aria-label=\"Direct link to Conclusion\" title=\"Direct link to Conclusion\" translate=\"no\">​</a></h2>\n<p>This meetup made us believe that the vast ocean of data lakehouses could not be separated from the \"collective effort\" of the open-source community and enterprises. Those technologies tempered on the business battlefield ultimately gave back as nutrients nourishing the entire ecosystem. This may be the purest romance of technology: making complex things simple and making the impossible possible. The road ahead is full of imagination, and together, we are shaping a more elegant and powerful future for data processing.</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"image 7\" src=\"https://hudi.apache.org/assets/images/jdpost-image7-361b48a09e72bed4c19f8be9bf616ed5.jpg\" width=\"4032\" height=\"3024\" class=\"img_ev3q\"></p>",
            "url": "https://hudi.apache.org/blog/2025/12/01/apache-hudi-JD-meetup-asia-2025-recap",
            "title": "Next Generation Lakehouse: New Engine for the Intelligent Future | Apache Hudi Meetup Asia Recap",
            "summary": "---",
            "date_modified": "2025-12-01T00:00:00.000Z",
            "author": {
                "name": "Team at JD.com"
            },
            "tags": [
                "hudi",
                "meetup",
                "lakehouse",
                "community"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2025/11/28/Apache-Hudi-Dynamic-Bloom-Filter",
            "content_html": "<span>Redirecting... please wait!! <!-- -->or click <a href=\"https://codepointer.substack.com/p/apache-hudi-dynamic-bloom-filter\">here</a></span>",
            "url": "https://hudi.apache.org/blog/2025/11/28/Apache-Hudi-Dynamic-Bloom-Filter",
            "title": "Apache Hudi Dynamic Bloom Filter\"",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2025-11-28T00:00:00.000Z",
            "author": {
                "name": "Yongkyun Lee"
            },
            "tags": [
                "blog",
                "Apache Hudi",
                "data lakehouse",
                "Lakehouse",
                "use-case",
                "Bloom Filter",
                "Indexing"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2025/11/25/apache-hudi-release-1-1-announcement",
            "content_html": "<p>The Hudi community is excited to announce the <a href=\"https://hudi.apache.org/releases/release-1.1.0\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">release of Hudi 1.1</a>, a major milestone that sets the stage for the next generation of data lakehouse capabilities. This release represents months of focused engineering on foundational improvements, engine-specific optimizations, and key architectural enhancements, laying the foundation for ambitious features coming in future releases.</p>\n<p>Hudi continues to evolve rapidly, with contributions from a vibrant community of developers and users. The 1.1 release brings over 700 commits addressing performance bottlenecks, expanding engine support, and introducing new capabilities that make Hudi tables more reliable, faster, and easier to operate. Let’s dive into the highlights.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"pluggable-table-formatthe-foundation-for-multi-format-support\">Pluggable Table Format—The Foundation for Multi-Format Support<a href=\"https://hudi.apache.org/blog/2025/11/25/apache-hudi-release-1-1-announcement#pluggable-table-formatthe-foundation-for-multi-format-support\" class=\"hash-link\" aria-label=\"Direct link to Pluggable Table Format—The Foundation for Multi-Format Support\" title=\"Direct link to Pluggable Table Format—The Foundation for Multi-Format Support\" translate=\"no\">​</a></h2>\n<p>Hudi 1.1 introduces a <a href=\"https://hudi.apache.org/docs/hudi_stack#pluggable-table-format\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">pluggable table format</a> framework that opens up the powerful storage engine capabilities beyond Hudi’s native storage format to other table formats like Apache Iceberg and Delta Lake. This framework represents a fundamental shift in how Hudi approaches table format support, enabling native integration of multiple formats and giving you a unified system with total read-write compatibility across formats.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"vision-and-design\">Vision and Design<a href=\"https://hudi.apache.org/blog/2025/11/25/apache-hudi-release-1-1-announcement#vision-and-design\" class=\"hash-link\" aria-label=\"Direct link to Vision and Design\" title=\"Direct link to Vision and Design\" translate=\"no\">​</a></h3>\n<p>The table format landscape in the modern lakehouse ecosystem is diverse and evolving. Like a game of rock-paper-scissors, different formats—Hudi, Iceberg, Delta Lake—each have unique strengths for specific use cases. Rather than forcing a one-size-fits-all approach, Hudi 1.1 introduces a pluggable table format framework that embraces the open lakehouse ecosystem and prevents vendor lock-in.</p>\n<p>The framework is built on a clean abstraction layer that decouples Hudi’s core capabilities—transaction management, indexing, concurrency control, and table services—from the specific storage format used for data files. At the heart of this design is the <code>HoodieTableFormat</code> interface, which different format implementations can extend.</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"pluggable table format\" src=\"https://hudi.apache.org/assets/images/1-pluggable-TF-36f7e26bf8dc4a479bcaff713a24debd.png\" width=\"894\" height=\"665\" class=\"img_ev3q\"></p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"key-architectural-components\">Key Architectural Components<a href=\"https://hudi.apache.org/blog/2025/11/25/apache-hudi-release-1-1-announcement#key-architectural-components\" class=\"hash-link\" aria-label=\"Direct link to Key Architectural Components\" title=\"Direct link to Key Architectural Components\" translate=\"no\">​</a></h3>\n<ul>\n<li class=\"\">Storage engine: Hudi’s storage engine capabilities, such as timeline management, concurrency control mechanisms, indexes, and table services, can work across multiple table formats</li>\n<li class=\"\">Pluggable adapters: Format-specific implementations handle the generation of conforming metadata upon writes</li>\n</ul>\n<p>Hudi’s artifact provides support for the native Hudi format, while <a href=\"https://xtable.apache.org/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">Apache XTable (incubating)</a> supplies pluggable format adapters. For example, <a href=\"https://github.com/apache/incubator-xtable/pull/723\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">this XTable PR</a> implements the Iceberg adapter to allow you to add dependencies to your running pipelines as needed. This architecture enables organizations to choose the right format for each use case while maintaining a unified operational experience and leveraging Hudi’s sophisticated storage engine across all of them.</p>\n<p>In the 1.1 release, the framework comes with native Hudi format support (configured via <code>hoodie.table.format=native</code> by default). Existing users don't need to change anything—tables continue to work exactly as before. The real excitement lies ahead: the framework paves the way for supporting additional formats like Iceberg and Delta Lake. Imagine writing high-frequency updates to a Hudi table efficiently with Hudi's record-level indexing capability while maintaining Iceberg metadata through the Iceberg adapter, which supports a wide range of catalogs for reads. The pluggable table format framework in 1.1 makes such usage patterns possible—a game-changer for organizations that need flexibility and openness in their data architecture.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"indexing-improvementsfaster-and-smarter-lookups\">Indexing Improvements—Faster and Smarter Lookups<a href=\"https://hudi.apache.org/blog/2025/11/25/apache-hudi-release-1-1-announcement#indexing-improvementsfaster-and-smarter-lookups\" class=\"hash-link\" aria-label=\"Direct link to Indexing Improvements—Faster and Smarter Lookups\" title=\"Direct link to Indexing Improvements—Faster and Smarter Lookups\" translate=\"no\">​</a></h2>\n<p>Hudi’s indexing subsystem is one of its most powerful features, enabling fast record lookups during writes and efficient data skipping during reads.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"partitioned-record-index\">Partitioned Record Index<a href=\"https://hudi.apache.org/blog/2025/11/25/apache-hudi-release-1-1-announcement#partitioned-record-index\" class=\"hash-link\" aria-label=\"Direct link to Partitioned Record Index\" title=\"Direct link to Partitioned Record Index\" translate=\"no\">​</a></h3>\n<p>Since version 0.14.0, Hudi has supported a global record index in the indexing subsystem—a breakthrough that enables blazing-fast lookups on large datasets. While this is ideal for globally unique identifiers like order IDs or SSNs, many scenarios only require uniqueness within a partition—for example, user events partitioned by date. Hudi 1.1 introduces the <a href=\"https://hudi.apache.org/docs/indexes#record-index\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">partitioned record index</a>, a non-global variant of the record index that works with the combination of partition path and record key, leveraging partition information to prune irrelevant partitions during lookups and dramatically reducing the search space, and thus achieving efficient lookups even on very large datasets.</p>\n<div class=\"language-sql codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-sql codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token comment\" style=\"color:rgb(98, 114, 164)\">-- Spark SQL: Create table with partitioned record index</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">CREATE</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">TABLE</span><span class=\"token plain\"> user_activity </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  user_id STRING</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  activity_type STRING</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">timestamp</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">BIGINT</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  event_date </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">DATE</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">USING</span><span class=\"token plain\"> hudi</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">TBLPROPERTIES </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'primaryKey'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'user_id'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'preCombineField'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'timestamp'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token comment\" style=\"color:rgb(98, 114, 164)\">-- Enable partitioned record index</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'hoodie.metadata.record.level.index.enable'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'true'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'hoodie.index.type'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'RECORD_LEVEL_INDEX'</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">PARTITIONED </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">BY</span><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\">event_date</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><br></span></code></pre></div></div>\n<p>The partitioned record index enables index lookups that scale proportionally with partition size—file group accesses correlate directly to the data partition size, optimizing performance across heterogeneous data distributions. The design also supports future clustering operations that can dynamically expand file groups within partitions as they grow.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"partition-level-bucket-index\">Partition-level Bucket Index<a href=\"https://hudi.apache.org/blog/2025/11/25/apache-hudi-release-1-1-announcement#partition-level-bucket-index\" class=\"hash-link\" aria-label=\"Direct link to Partition-level Bucket Index\" title=\"Direct link to Partition-level Bucket Index\" translate=\"no\">​</a></h3>\n<p>The bucket index is a popular choice for high-throughput write workloads because it eliminates expensive record lookups by deterministically mapping keys to file groups. However, the existing bucket index has a key limitation: once you set the number of buckets, changing it requires rewriting the entire table.</p>\n<p>The 1.1 release introduces partition-level bucket index, which enables different bucket counts for different partitions using regex-based rules. This design allows tables to adapt as data volumes change over time—for example, older, smaller partitions can use fewer buckets while newer, larger partitions can have more.</p>\n<div class=\"language-sql codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-sql codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token comment\" style=\"color:rgb(98, 114, 164)\">-- Spark SQL: Create table with partition-level bucket index</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">CREATE</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">TABLE</span><span class=\"token plain\"> sales_transactions </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  transaction_id </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">BIGINT</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  user_id </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">BIGINT</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  amount </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">DOUBLE</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  transaction_date </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">DATE</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">USING</span><span class=\"token plain\"> hudi</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">TBLPROPERTIES </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'primaryKey'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'transaction_id'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token comment\" style=\"color:rgb(98, 114, 164)\">-- Partition-level bucket index</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'hoodie.index.type'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'BUCKET'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'hoodie.bucket.index.hash.field'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'transaction_id'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'hoodie.bucket.index.partition.rule.type'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'regex'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'hoodie.bucket.index.partition.expressions'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'2023-.*,16;2024-.*,32;2025-.*,64'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'hoodie.bucket.index.num.buckets'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'8'</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">PARTITIONED </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">BY</span><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\">transaction_date</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><br></span></code></pre></div></div>\n<p>The partition-level bucket index is ideal for time-series data where partition sizes vary significantly over time. The adaptive bucket sizing helps you maintain optimal write performance as your data volume changes. See the <a href=\"https://hudi.apache.org/docs/indexes#additional-writer-side-indexes\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">docs</a> and <a href=\"https://github.com/apache/hudi/blob/master/rfc/rfc-89/rfc-89.md\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">RFC 89</a> for more information.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"indexing-performance-optimizations\">Indexing Performance Optimizations<a href=\"https://hudi.apache.org/blog/2025/11/25/apache-hudi-release-1-1-announcement#indexing-performance-optimizations\" class=\"hash-link\" aria-label=\"Direct link to Indexing Performance Optimizations\" title=\"Direct link to Indexing Performance Optimizations\" translate=\"no\">​</a></h3>\n<p>Beyond new indexes, Hudi 1.1 delivers substantial performance improvements for metadata table operations:</p>\n<ul>\n<li class=\"\">HFile block cache and prefetching: The new block cache stores recently accessed data blocks in memory, avoiding repeated reads from storage. For smaller HFiles, Hudi prefetches the entire file upfront rather than making multiple read requests. Benchmarks show approximately 4x speedup for repeated lookups, enabled by default.</li>\n</ul>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"metadata table key lookup\" src=\"https://hudi.apache.org/assets/images/2-metadata-table-lookup-451d218e2a4fc0aac40b6d3c37522572.png\" width=\"960\" height=\"540\" class=\"img_ev3q\"></p>\n<ul>\n<li class=\"\">HFile Bloom filter: Adding Bloom filters to HFiles enables Hudi to quickly determine whether a key might exist in a file before fetching data blocks, avoiding unnecessary I/O and dramatically speeding up point lookups. You can enable it with <code>hoodie.metadata.bloom.filter.enable=true</code>.</li>\n</ul>\n<p>These optimizations compound to make the metadata table significantly faster, directly improving both write and read performance across your Hudi tables. Additionally, Hudi 1.1 adds its own native HFile writer implementation, eliminating the dependency on HBase libraries. This refactoring significantly reduces the Hudi bundle size and provides the foundation for future HFile performance optimizations.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"faster-clustering-with-parquet-file-binary-copy\">Faster Clustering with Parquet File Binary Copy<a href=\"https://hudi.apache.org/blog/2025/11/25/apache-hudi-release-1-1-announcement#faster-clustering-with-parquet-file-binary-copy\" class=\"hash-link\" aria-label=\"Direct link to Faster Clustering with Parquet File Binary Copy\" title=\"Direct link to Faster Clustering with Parquet File Binary Copy\" translate=\"no\">​</a></h2>\n<p>Clustering reorganizes data to improve query performance, but traditional approaches are expensive—decompressing, decoding, transforming, re-encoding, and re-compressing data even when no transformation is needed.</p>\n<p>Hudi 1.1 implements Parquet file binary copy for clustering operations. Instead of processing records, this optimization directly copies Parquet RowGroups from source to destination files when schema-compatible, eliminating redundant transformations entirely.</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"parquet binary copy\" src=\"https://hudi.apache.org/assets/images/3-binary-copy-d58175c8f9cbd5817d1ace637a617a18.png\" width=\"739\" height=\"407\" class=\"img_ev3q\"></p>\n<p>On 100GB test data, using Parquet file binary copy achieved 15x faster execution (18 minutes → 1.2 minutes) and 95% reduction in compute (28.7 task-hours → 1.3 task-hours) compared to the normal rewriting of Parquet files. Real-world validation with 1.7TB datasets (300 columns) showed approximately 5x performance improvement (35 min → 7.7 min) with CPU usage dropping from 90% to 60%.</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"parquet binary copy chart\" src=\"https://hudi.apache.org/assets/images/4-binary-copy-chart-f7366c95b72eb12b3ce39cc1b83bfcff.png\" width=\"960\" height=\"540\" class=\"img_ev3q\"></p>\n<p>The optimization is currently supported for Copy-on-Write tables and enabled automatically when safe, with Hudi intelligently falling back to traditional clustering when schema reconciliation is required. You may refer to <a href=\"https://github.com/apache/hudi/pull/13365\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">this PR</a> for more detail.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"storage-based-lock-providereliminating-external-dependencies-for-concurrent-writers\">Storage-Based Lock Provider—Eliminating External Dependencies for Concurrent Writers<a href=\"https://hudi.apache.org/blog/2025/11/25/apache-hudi-release-1-1-announcement#storage-based-lock-providereliminating-external-dependencies-for-concurrent-writers\" class=\"hash-link\" aria-label=\"Direct link to Storage-Based Lock Provider—Eliminating External Dependencies for Concurrent Writers\" title=\"Direct link to Storage-Based Lock Provider—Eliminating External Dependencies for Concurrent Writers\" translate=\"no\">​</a></h2>\n<p>Multi-writer concurrency is critical for production data lakehouses, where multiple jobs need to write to the same table simultaneously. Historically, enabling multi-writer support in Hudi required setting up external lock providers like AWS DynamoDB, Apache Zookeeper, or Hive Metastore. While these work well, they add operational complexity—you need to provision, maintain, and monitor additional infrastructure.</p>\n<p>Hudi 1.1 introduces a storage-based lock provider that eliminates this dependency entirely by managing concurrency directly using the <code>.hoodie/</code> directory in your table's storage layer.</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"storage based lock provider\" src=\"https://hudi.apache.org/assets/images/5-storage-based-lp-8528a28ca24084f9c01e550a1c312a36.png\" width=\"708\" height=\"373\" class=\"img_ev3q\"></p>\n<p>The implementation uses conditional writes on a single lock file under <code>.hoodie/.locks/</code> to ensure only one writer holds the lock at a time, with heartbeat-based renewal and automatic expiration for fault tolerance. To use the storage-based lock provider, you need to add the corresponding Hudi cloud bundle (<code>hudi-aws-bundle</code> for S3 and <code>hudi-gcp-bundle</code> for GCS) and set the following configuration:</p>\n<div class=\"language-properties codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-properties codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">hoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.StorageBasedLockProvider</span><br></span></code></pre></div></div>\n<p>This approach eliminates the need for DynamoDB, ZooKeeper, or Hive Metastore dependencies, reducing operational costs and infrastructure complexity. The cloud-native design works directly with S3 or GCS storage features, with support for additional storage systems planned, making Hudi easier to operate at scale in cloud-native environments. Check out the <a href=\"https://hudi.apache.org/docs/concurrency_control#storage-based-lock-provider\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">docs</a> and <a href=\"https://github.com/apache/hudi/blob/master/rfc/rfc-91/rfc-91.md\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">RFC 91</a> for more detail.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"use-merge-modes-and-custom-mergerssay-goodbye-to-payload-classes\">Use Merge Modes and Custom Mergers—Say Goodbye to Payload Classes<a href=\"https://hudi.apache.org/blog/2025/11/25/apache-hudi-release-1-1-announcement#use-merge-modes-and-custom-mergerssay-goodbye-to-payload-classes\" class=\"hash-link\" aria-label=\"Direct link to Use Merge Modes and Custom Mergers—Say Goodbye to Payload Classes\" title=\"Direct link to Use Merge Modes and Custom Mergers—Say Goodbye to Payload Classes\" translate=\"no\">​</a></h2>\n<p>A core design principle of Hudi is enabling the storage layer to understand how to merge updates to the same record key, even when changes arrive out of order—a common scenario with mobile apps, IoT devices, and distributed systems. Prior to Hudi 1.1, record merging logic was primarily implemented through payload classes, which were fragmented and lacked standardized semantics.</p>\n<p>Hudi 1.1 deprecates payload classes and encourages users to adopt the new APIs introduced since 1.0 for record merging: merge modes and the <code>HoodieRecordMerger</code> interface.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"merge-modesdeclarative-record-merging\">Merge Modes—Declarative Record Merging<a href=\"https://hudi.apache.org/blog/2025/11/25/apache-hudi-release-1-1-announcement#merge-modesdeclarative-record-merging\" class=\"hash-link\" aria-label=\"Direct link to Merge Modes—Declarative Record Merging\" title=\"Direct link to Merge Modes—Declarative Record Merging\" translate=\"no\">​</a></h3>\n<p>For common use cases, the <code>COMMIT_TIME_ORDERING</code> and <code>EVENT_TIME_ORDERING</code> merge modes provide a declarative way to specify merge behavior:</p>\n<table><thead><tr><th style=\"text-align:left\">Merge mode</th><th style=\"text-align:left\">What does it do?</th></tr></thead><tbody><tr><td style=\"text-align:left\"><code>COMMIT_TIME_ORDERING</code></td><td style=\"text-align:left\">Picks the record with the highest completion time/instant as the final merge result (standard relational semantics or arrival time processing)</td></tr><tr><td style=\"text-align:left\"><code>EVENT_TIME_ORDERING</code></td><td style=\"text-align:left\">Picks the record with the highest value on a user-specified ordering field as the final merge result. Enables event time processing semantics for handling late-arriving data without corrupting record state.</td></tr></tbody></table>\n<p>The default behavior is adaptive: if no ordering field (<code>hoodie.table.ordering.fields</code>) is configured, Hudi defaults to <code>COMMIT_TIME_ORDERING</code>; if one or more ordering fields are set, it uses <code>EVENT_TIME_ORDERING</code>. This makes Hudi work out-of-the-box for simple use cases while still supporting event-time ordering when needed.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"custom-mergersthe-flexible-approach\">Custom Mergers—The Flexible Approach<a href=\"https://hudi.apache.org/blog/2025/11/25/apache-hudi-release-1-1-announcement#custom-mergersthe-flexible-approach\" class=\"hash-link\" aria-label=\"Direct link to Custom Mergers—The Flexible Approach\" title=\"Direct link to Custom Mergers—The Flexible Approach\" translate=\"no\">​</a></h3>\n<p>For complex merging logic—such as field-level reconciliation, aggregating counters, or preserving audit fields—the <code>HoodieRecordMerger</code> interface provides a modern, engine-native alternative to payload classes. You need to set the merge mode to <code>CUSTOM</code> and provide your own implementation of <code>HoodieRecordMerger</code>. By using the new API, you can achieve consistent merging across all code paths: precombine, updating writes, compaction, and snapshot reads—you are strongly encouraged to migrate to the new APIs. See <a href=\"https://hudi.apache.org/docs/record_merger\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">the docs</a> for more details. For migration guidance, see the <a href=\"https://hudi.apache.org/releases/release-1.1.0/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">release notes</a> and <a href=\"https://github.com/apache/hudi/pull/13499\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">RFC-97</a>.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"apache-spark-integration-improvements\">Apache Spark Integration Improvements<a href=\"https://hudi.apache.org/blog/2025/11/25/apache-hudi-release-1-1-announcement#apache-spark-integration-improvements\" class=\"hash-link\" aria-label=\"Direct link to Apache Spark Integration Improvements\" title=\"Direct link to Apache Spark Integration Improvements\" translate=\"no\">​</a></h2>\n<p>Spark remains one of the most popular engines for working with Hudi tables, and the 1.1 release brings several important enhancements.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"spark-40-support\">Spark 4.0 Support<a href=\"https://hudi.apache.org/blog/2025/11/25/apache-hudi-release-1-1-announcement#spark-40-support\" class=\"hash-link\" aria-label=\"Direct link to Spark 4.0 Support\" title=\"Direct link to Spark 4.0 Support\" translate=\"no\">​</a></h3>\n<p>Spark 4.0 brought significant performance gains for ML/AI workloads, smarter query optimization with automatic join strategy switching, dynamic partition skew mitigation, and enhanced streaming capabilities. Hudi 1.1 adds Spark 4.0 support to unlock these improvements for working with Hudi tables. To get started, use the new <code>hudi-spark4.0-bundle_2.13:1.1.0</code> artifact in your dependency list.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"metadata-table-streaming-writes\">Metadata Table Streaming Writes<a href=\"https://hudi.apache.org/blog/2025/11/25/apache-hudi-release-1-1-announcement#metadata-table-streaming-writes\" class=\"hash-link\" aria-label=\"Direct link to Metadata Table Streaming Writes\" title=\"Direct link to Metadata Table Streaming Writes\" translate=\"no\">​</a></h3>\n<p>Hudi 1.1 introduces streaming writes to the metadata table, unifying data and metadata writes into a single RDD execution chain. The key design generates metadata records directly during data writes in parallel across executors, eliminating redundant file lookups that previously created bottlenecks and enhancing reliability when performing stage retries in Spark.</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"spark upsert time chart\" src=\"https://hudi.apache.org/assets/images/6-spark-upsert-write-time-chart-734ca6975e38a28de049809099a99caa.png\" width=\"960\" height=\"540\" class=\"img_ev3q\"></p>\n<p>A benchmark with update-intensive workloads showed that this 1.1 feature delivered about 18% faster write times for tables with record index, compared to Hudi 1.0. The feature is enabled by default for Spark writers.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"new-and-enhanced-sql-procedures\">New and Enhanced SQL Procedures<a href=\"https://hudi.apache.org/blog/2025/11/25/apache-hudi-release-1-1-announcement#new-and-enhanced-sql-procedures\" class=\"hash-link\" aria-label=\"Direct link to New and Enhanced SQL Procedures\" title=\"Direct link to New and Enhanced SQL Procedures\" translate=\"no\">​</a></h3>\n<p>Hudi 1.1 expands the <a href=\"https://hudi.apache.org/docs/procedures\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">SQL procedure</a> library with useful additions and enhanced capabilities for table management and observability, bringing operational capabilities directly into Spark SQL.</p>\n<p>The new procedures, <code>show_cleans</code>, <code>show_clean_plans</code>, and <code>show_cleans_metadata</code>, provide visibility into cleaning operations:</p>\n<div class=\"language-sql codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-sql codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">CALL</span><span class=\"token plain\"> show_cleans</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">table</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token operator\">&gt;</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'hudi_table'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">limit</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token operator\">&gt;</span><span class=\"token plain\"> </span><span class=\"token number\">10</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">CALL</span><span class=\"token plain\"> show_clean_plans</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">table</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token operator\">&gt;</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'hudi_table'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">limit</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token operator\">&gt;</span><span class=\"token plain\"> </span><span class=\"token number\">10</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">CALL</span><span class=\"token plain\"> show_cleans_metadata</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">table</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token operator\">&gt;</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'hudi_table'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">limit</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token operator\">&gt;</span><span class=\"token plain\"> </span><span class=\"token number\">10</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><br></span></code></pre></div></div>\n<p>The enhanced <code>run_clustering</code> procedure supports partition filtering with regex patterns:</p>\n<div class=\"language-sql codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-sql codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token comment\" style=\"color:rgb(98, 114, 164)\">-- Cluster all 2025 partitions matching a pattern</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">CALL</span><span class=\"token plain\"> run_clustering</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">table</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token operator\">&gt;</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'hudi_table'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  partition_regex_pattern </span><span class=\"token operator\">=</span><span class=\"token operator\">&gt;</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'2025-.*'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><br></span></code></pre></div></div>\n<p>All <code>show</code> procedures, where applicable, were enhanced with <code>path</code> and <code>filter</code> parameters. <code>path</code> helps when <code>table_name</code> is not able to identify a table properly. <code>filter</code> can support advanced predicate expressions. For example:</p>\n<div class=\"language-sql codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-sql codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token comment\" style=\"color:rgb(98, 114, 164)\">-- Find large files in recent partitions</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">CALL</span><span class=\"token plain\"> show_file_status</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  path </span><span class=\"token operator\">=</span><span class=\"token operator\">&gt;</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'/data/warehouse/transactions'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  filter </span><span class=\"token operator\">=</span><span class=\"token operator\">&gt;</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">\"partition LIKE '2025-11%' AND file_size &gt; 524288000\"</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><br></span></code></pre></div></div>\n<p>The new and enhanced SQL procedures bring table management directly into Spark SQL, streamlining operations for SQL-focused workflows.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"apache-flink-integration-improvements\">Apache Flink Integration Improvements<a href=\"https://hudi.apache.org/blog/2025/11/25/apache-hudi-release-1-1-announcement#apache-flink-integration-improvements\" class=\"hash-link\" aria-label=\"Direct link to Apache Flink Integration Improvements\" title=\"Direct link to Apache Flink Integration Improvements\" translate=\"no\">​</a></h2>\n<p>Flink is a popular choice for real-time data pipelines, and Hudi 1.1 brings substantial improvements to the Flink integration.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"flink-20-support\">Flink 2.0 Support<a href=\"https://hudi.apache.org/blog/2025/11/25/apache-hudi-release-1-1-announcement#flink-20-support\" class=\"hash-link\" aria-label=\"Direct link to Flink 2.0 Support\" title=\"Direct link to Flink 2.0 Support\" translate=\"no\">​</a></h3>\n<p>Hudi 1.1 brings support for Flink 2.0, the first major Flink release in nine years. Flink 2.0 introduced disaggregated state storage (ForSt) that decouples state from compute for unlimited scalability, asynchronous state execution for improved resource utilization, adaptive broadcast join for efficient query processing, and materialized tables for simplified stream-batch unification. Use the new <code>hudi-flink2.0-bundle:1.1.0</code> artifact to get started.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"engine-native-record-support\">Engine-Native Record Support<a href=\"https://hudi.apache.org/blog/2025/11/25/apache-hudi-release-1-1-announcement#engine-native-record-support\" class=\"hash-link\" aria-label=\"Direct link to Engine-Native Record Support\" title=\"Direct link to Engine-Native Record Support\" translate=\"no\">​</a></h3>\n<p>Hudi 1.1 eliminates expensive Avro conversions by processing Flink's native <code>RowData</code> format directly, enabling zero-copy operations throughout the pipeline. This automatic change (no configuration required) delivers 2-3x improvement in write and read performance on average compared to Hudi 1.0.</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"flink throughput chart\" src=\"https://hudi.apache.org/assets/images/7-flink-write-throughput-chart-040f5478de7a2bf3fa8ddb03ffa499c0.png\" width=\"960\" height=\"540\" class=\"img_ev3q\"></p>\n<p>The above shows a benchmark that inserted 500 million records with a schema of 1 STRING and 10 BIGINT fields: Hudi 1.1 achieved 235.3k records per second and Hudi 1.0 67k records per second—over 3 times higher throughput.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"buffer-sort\">Buffer Sort<a href=\"https://hudi.apache.org/blog/2025/11/25/apache-hudi-release-1-1-announcement#buffer-sort\" class=\"hash-link\" aria-label=\"Direct link to Buffer Sort\" title=\"Direct link to Buffer Sort\" translate=\"no\">​</a></h3>\n<p>For append-only tables, Hudi 1.1 introduces in-memory buffer sorting that pre-sorts records before flushing to Parquet. This delivers 15-30% better compression and faster queries through better min/max filtering. You can enable this feature with <code>write.buffer.sort.enabled=true</code> and specify sort keys via <code>write.buffer.sort.keys</code> (e.g., \"timestamp,event_type\"). You may also adjust the buffer size for sorting via <code>write.buffer.size</code> (default 1000 records).</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"new-integration-apache-polaris-incubating\">New Integration: Apache Polaris (Incubating)<a href=\"https://hudi.apache.org/blog/2025/11/25/apache-hudi-release-1-1-announcement#new-integration-apache-polaris-incubating\" class=\"hash-link\" aria-label=\"Direct link to New Integration: Apache Polaris (Incubating)\" title=\"Direct link to New Integration: Apache Polaris (Incubating)\" translate=\"no\">​</a></h2>\n<p><a href=\"https://polaris.apache.org/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">Polaris (incubating)</a> is an open-source catalog for lakehouse platforms that provides multi-engine interoperability and unified governance across diverse table formats and query engines. Its key feature is enabling data teams to use multiple engines—Spark, Trino, Dremio, Flink, Presto—on a single copy of data with consistent metadata, governed openly by a diverse committee including Snowflake, AWS, Google Cloud, Azure, and others to prevent vendor lock-in.</p>\n<p>Hudi 1.1 introduces <a href=\"https://hudi.apache.org/docs/catalog_polaris\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">native integration with Polaris</a> (pending a Polaris release that includes <a href=\"https://github.com/apache/polaris/pull/1862\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">this PR</a>), allowing users to register Hudi tables in the Polaris catalog and query them from any Polaris-compatible engine, simplifying multi-engine workflows and providing centralized role-based access control that works uniformly across S3, Azure Blob Storage, and Google Cloud Storage.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"whats-nextjoin-us-in-building-the-future\">What’s Next—Join Us in Building the Future<a href=\"https://hudi.apache.org/blog/2025/11/25/apache-hudi-release-1-1-announcement#whats-nextjoin-us-in-building-the-future\" class=\"hash-link\" aria-label=\"Direct link to What’s Next—Join Us in Building the Future\" title=\"Direct link to What’s Next—Join Us in Building the Future\" translate=\"no\">​</a></h2>\n<p>The future of Hudi is incredibly exciting, and we're building it together with a vibrant, global community of contributors. Building on the strong foundation of 1.1, we're actively developing transformative AI/ML-focused capabilities for Hudi 1.2 and beyond—unstructured data types and column groups for efficient storage of embeddings and documents, Lance, Vortex, blob-optimized Parquet support, and vector search capabilities for lakehouse tables. This is just the beginning—we're reimagining what's possible in the lakehouse, from multi-format interoperability to next-generation AI/ML workloads, and we need your ideas, code, and creativity to make it happen.</p>\n<p>Join us in building the future. Check out the <a href=\"https://hudi.apache.org/releases/release-1.1.0\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">1.1 release notes</a> to get started, join our <a href=\"https://hudi.apache.org/slack/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">Slack space</a>, follow us on <a href=\"https://www.linkedin.com/company/apache-hudi\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">LinkedIn</a> and <a href=\"http://x.com/apachehudi\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">X (twitter)</a>, and subscribe (send an empty email) to the <a href=\"mailto:dev@hudi.apache.org\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">mailing list</a>—let's build the next generation of Hudi together.</p>",
            "url": "https://hudi.apache.org/blog/2025/11/25/apache-hudi-release-1-1-announcement",
            "title": "Apache Hudi 1.1 is Here—Building the Foundation for the Next Generation of Lakehouse",
            "summary": "The Hudi community is excited to announce the release of Hudi 1.1, a major milestone that sets the stage for the next generation of data lakehouse capabilities. This release represents months of focused engineering on foundational improvements, engine-specific optimizations, and key architectural enhancements, laying the foundation for ambitious features coming in future releases.",
            "date_modified": "2025-11-25T00:00:00.000Z",
            "author": {
                "name": "Shiyan Xu"
            },
            "tags": [
                "hudi",
                "release",
                "feature",
                "performance"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2025/11/12/deep-dive-into-hudis-indexing-subsystem-part-2-of-2",
            "content_html": "<p>In <a href=\"https://hudi.apache.org/blog/2025/10/29/deep-dive-into-hudis-indexing-subsystem-part-1-of-2/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">part 1</a>, we explored how Hudi's metadata table functions as a self-managed, multimodal indexing subsystem. We covered its internal architecture—a partitioned Hudi Merge-on-Read (MOR) table using HFile format for efficient key lookups—and how the files, column stats, and partition stats indexes work together to implement powerful data skipping. These indexes dramatically reduce I/O by pruning partitions and files that don't contain the data your query needs.</p>\n<p>Now in part 2, we'll dive into more specialized indexes that handle different query patterns. We'll look at the record and secondary indexes, which provide exact file locations for equality-matching predicates rather than just skipping irrelevant files. We'll explore expression indexes that optimize queries with inline transformations like <code>from_unixtime()</code> or <code>substring()</code>. Finally, we'll cover async indexing, which lets you build resource-intensive indexes in the background without blocking your active read and write operations.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"equality-matching-with-record-and-secondary-indexes\">Equality Matching with Record and Secondary Indexes<a href=\"https://hudi.apache.org/blog/2025/11/12/deep-dive-into-hudis-indexing-subsystem-part-2-of-2#equality-matching-with-record-and-secondary-indexes\" class=\"hash-link\" aria-label=\"Direct link to Equality Matching with Record and Secondary Indexes\" title=\"Direct link to Equality Matching with Record and Secondary Indexes\" translate=\"no\">​</a></h2>\n<p>Queries may contain equality-matching predicates like <code>A = X</code> or <code>B IN (X, Y, Z)</code>. While data skipping indexes such as column stats and partition stats help here too, record-level indexing goes further by pinpointing the exact data files containing those values.</p>\n<p>Hudi’s multimodal indexing subsystem implements the <a href=\"https://hudi.apache.org/blog/2023/11/01/record-level-index/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\"><em>record index</em></a> and <a href=\"https://hudi.apache.org/blog/2025/04/02/secondary-index/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\"><em>secondary index</em></a> to meet this need:</p>\n<ul>\n<li class=\"\"><strong>Record index</strong>: Stores mappings between record keys and the file locations that contain them.</li>\n<li class=\"\"><strong>Secondary index</strong>: Stores mappings between non-record-key column values and their corresponding record keys to support mapping to file locations.</li>\n</ul>\n<p>Note that the record index is located at the <code>record_index/</code> partition of the metadata table. You can create multiple secondary indexes, each for a chosen column, stored under a dedicated partition (prefixed with <code>secondary_index_</code>) in the metadata table.</p>\n<p>The record index is a high-performance, general-purpose index that works on both the writer and reader sides. As described in <a href=\"https://hudi.apache.org/blog/2023/11/01/record-level-index/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">this blog</a>, its direct record-location lookup allows Hudi writers to efficiently route updates and deletes to their corresponding file groups in a Hudi table. The secondary index leverages the record index to look up non-record-key columns efficiently. The remainder of this section focuses on the reader side to show how these two indexes optimize equality-matching predicates.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"the-lookup-process\">The lookup process<a href=\"https://hudi.apache.org/blog/2025/11/12/deep-dive-into-hudis-indexing-subsystem-part-2-of-2#the-lookup-process\" class=\"hash-link\" aria-label=\"Direct link to The lookup process\" title=\"Direct link to The lookup process\" translate=\"no\">​</a></h3>\n<p>Similar to the data skipping process, the query engine parses equality-matching predicates and pushes them down to the Hudi integration component. This component then performs the index lookup and returns the file locations to scan.</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"Record and secondary index lookup process\" src=\"https://hudi.apache.org/assets/images/fig1-86b4d7f2e4b5c535b2ec0036d223065c.png\" width=\"974\" height=\"769\" class=\"img_ev3q\"></p>\n<p>First, let's consider the record index. When a query with an equality filter like <code>id = '001'</code> runs against a Hudi table where <code>id</code> is the record key, the engine uses the record index to find the exact file locations for that key. The index returns these locations to the query engine, which then plans the read execution.</p>\n<p>This direct lookup dramatically optimizes the query by ensuring only the relevant file locations are scanned. For example, on a 400 GB synthetic Hudi table with 20,000 file groups, a query filtering on a single record key saw its execution time drop from 977 seconds to just 12 seconds—a 98% reduction—when using the record index.</p>\n<p>Now, let's consider the case when the equality filter is <code>name = 'foo'</code> where <code>name</code> is not a record key field. A secondary index built for the column <code>name</code> will be used for the lookup process. Entries in the secondary index contain mappings of all <code>name</code> values and their corresponding record keys. Because multiple distinct records can have the same <code>name</code> value, the lookup may return multiple record keys. The next step is to look up these returned record keys in the record index to find the enclosing file locations for scanning. As you can tell, the record index must be enabled for using the secondary index.</p>\n<p><a href=\"https://hudi.apache.org/blog/2025/04/02/secondary-index/#benchmarking\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">A recent TPCDS benchmarking</a> shows that, by using the secondary index, query performance improved by about 45% on average, and the amount of data scanned was reduced by 90%.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"sql-examples\">SQL examples<a href=\"https://hudi.apache.org/blog/2025/11/12/deep-dive-into-hudis-indexing-subsystem-part-2-of-2#sql-examples\" class=\"hash-link\" aria-label=\"Direct link to SQL examples\" title=\"Direct link to SQL examples\" translate=\"no\">​</a></h3>\n<p>You can specify <code>hoodie.metadata.record.index.enable</code> during table creation to enable the record index for the table:</p>\n<div class=\"language-sql codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-sql codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">CREATE</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">TABLE</span><span class=\"token plain\"> trips </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    ts </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">BIGINT</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    id STRING</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    rider STRING</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    driver STRING</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    fare </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">DOUBLE</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    city STRING</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    state STRING</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">USING</span><span class=\"token plain\"> hudi</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"> OPTIONS</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    primaryKey </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'id'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    hoodie</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token plain\">metadata</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token plain\">record</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">index</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">enable</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'true'</span><span class=\"token plain\"> </span><span class=\"token comment\" style=\"color:rgb(98, 114, 164)\">-- enable record index</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">PARTITIONED </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">BY</span><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\">city</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> state</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><br></span></code></pre></div></div>\n<p>To create a secondary index on a specific column, you can use <code>CREATE INDEX</code> like this:</p>\n<div class=\"language-sql codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-sql codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">CREATE</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">INDEX</span><span class=\"token plain\"> driver_idx </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">ON</span><span class=\"token plain\"> trips </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\">driver</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><span class=\"token plain\"> </span><span class=\"token comment\" style=\"color:rgb(98, 114, 164)\">-- enable secondary index on column `driver`</span><br></span></code></pre></div></div>\n<p>When you write data to the example table, index data gets written to the record index and secondary index partitions in the metadata table, which then accelerates query execution during reads. Check out the <a href=\"https://hudi.apache.org/docs/sql_ddl\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">SQL DDL page</a> for more examples.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"expression-index\">Expression Index<a href=\"https://hudi.apache.org/blog/2025/11/12/deep-dive-into-hudis-indexing-subsystem-part-2-of-2#expression-index\" class=\"hash-link\" aria-label=\"Direct link to Expression Index\" title=\"Direct link to Expression Index\" translate=\"no\">​</a></h2>\n<p>Query predicates often contain expressions that perform inline transformations on columns, such as <code>from_unixtime()</code> or <code>substring()</code>. These expressions prevent a direct match with standard column indexes like column stats or partition stats. To optimize such queries, Hudi provides the <em>expression index</em> that operates on transformed column values. A full list of supported expressions is available in the <a href=\"https://hudi.apache.org/docs/sql_ddl/#create-expression-index\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">documentation</a>.</p>\n<p>Hudi currently supports two types of expression indexes:</p>\n<ul>\n<li class=\"\"><strong>Column stats type</strong>: Stores file-level statistics (min, max, null count, value count) for the transformed values after applying the expression.</li>\n<li class=\"\"><strong>Bloom filter type</strong>: Stores a file-level bloom filter built from the transformed values after applying the expression.</li>\n</ul>\n<p>Each expression index—defined by its type, the expression used, and the target column—occupies a dedicated partition within the metadata table, identified by an <code>expr_index_</code> prefix in its partition path.</p>\n<p>The column stats expression index functions similarly to a standard column stats index and is effective for data skipping. As the diagram below illustrates, a predicate containing a <code>from_unixtime()</code> expression is processed for lookup, and the corresponding expression index prunes the file list for the query engine.</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"Expression index lookup process\" src=\"https://hudi.apache.org/assets/images/fig2-7177a573864028788685add3456491ec.png\" width=\"860\" height=\"620\" class=\"img_ev3q\"></p>\n<p>The bloom filter expression index is designed for equality-matching predicates. Unlike the record and secondary indexes, which provide exact file locations, this index uses a bloom filter—a space-efficient data structure for quick presence checks—to prune files. The query planner can skip a file if the bloom filter indicates a target value is definitively not present.</p>\n<p>The bloom filter expression index is most effective for high-cardinality columns, where the probability of a \"not present\" result is higher, allowing more files to be skipped. For low-cardinality columns, the proposed <a href=\"https://github.com/apache/hudi/blob/master/rfc/rfc-92/rfc-92.md\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">bitmap index</a> would be more efficient and represents a valuable future extension to Hudi's indexing subsystem.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"sql-examples-1\">SQL examples<a href=\"https://hudi.apache.org/blog/2025/11/12/deep-dive-into-hudis-indexing-subsystem-part-2-of-2#sql-examples-1\" class=\"hash-link\" aria-label=\"Direct link to SQL examples\" title=\"Direct link to SQL examples\" translate=\"no\">​</a></h3>\n<p>Similar to creating a secondary index, you can create an expression index (column stats type) like this:</p>\n<div class=\"language-sql codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-sql codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">CREATE</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">INDEX</span><span class=\"token plain\"> ts_date </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">ON</span><span class=\"token plain\"> trips</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">USING</span><span class=\"token plain\"> column_stats</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\">ts</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"> </span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  OPTIONS</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\">expr</span><span class=\"token operator\">=</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'from_unixtime'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> format</span><span class=\"token operator\">=</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'yyyy-MM-dd'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><br></span></code></pre></div></div>\n<p>This example creates a column stats expression index on the column <code>ts</code> with the expression <code>from_unixtime</code> that transforms an epoch timestamp into a date string, allowing effective data skipping based on dates.</p>\n<p>You can create a bloom filter expression index similarly:</p>\n<div class=\"language-sql codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-sql codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">CREATE</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">INDEX</span><span class=\"token plain\"> bloom_idx_rider </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">ON</span><span class=\"token plain\"> trips</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">USING</span><span class=\"token plain\"> bloom_filters</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\">rider</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  OPTIONS</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\">expr</span><span class=\"token operator\">=</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'lower'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><br></span></code></pre></div></div>\n<p>This example builds a bloom filter expression index using the lowercase values of column <code>rider</code>, optimizing for predicates that match lowercase rider names. Check out the <a href=\"https://hudi.apache.org/docs/sql_ddl\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">SQL DDL page</a> for more examples.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"building-indexes-efficiently-with-the-async-indexer\">Building Indexes Efficiently with the Async Indexer<a href=\"https://hudi.apache.org/blog/2025/11/12/deep-dive-into-hudis-indexing-subsystem-part-2-of-2#building-indexes-efficiently-with-the-async-indexer\" class=\"hash-link\" aria-label=\"Direct link to Building Indexes Efficiently with the Async Indexer\" title=\"Direct link to Building Indexes Efficiently with the Async Indexer\" translate=\"no\">​</a></h2>\n<p>Hudi provides flexible mechanisms for managing indexes. You can use SQL DDL commands—such as <code>CREATE INDEX</code>, <code>DROP INDEX</code>, and <code>SHOW INDEXES</code>—or programmatically set writer configurations via the Spark DataSource and Flink DataStream APIs. For example, setting <code>hoodie.metadata.index.partition.stats.enable=false</code> during a write operation drops the partition stats index. This action deletes the corresponding partition from the metadata table and skips indexing computations for subsequent writes until the configuration is re-enabled.</p>\n<p>Creating a new index can be a resource-intensive operation, particularly for large tables and for indexes with high space complexity. For instance, the space complexity of the column stats index is O(columns × files), while the record index requires O(records) space. When adding such an index to a large table via DDL or a writer configuration, the time-consuming index initialization process must not block ongoing read and write operations.</p>\n<p>To address this challenge, Hudi's index management is designed with two key goals: index creation should not block concurrent reads and writes, and once built, an index must serve consistent data up to the latest table commit. Hudi meets these requirements with its <a href=\"https://hudi.apache.org/docs/metadata_indexing/#setup-async-indexing\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">async indexing</a> (illustrated below), which builds indexes in the background without interrupting active writers and readers.</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"Async indexing process\" src=\"https://hudi.apache.org/assets/images/fig3-ce0dd7f4352540791995b4b6b9b5387e.png\" width=\"1112\" height=\"814\" class=\"img_ev3q\"></p>\n<p>The async indexing process consists of two phases: scheduling and execution. First, the scheduler creates an indexing plan that covers data up to the latest data table commit. Next, the executor reads the required file groups from the data table and writes the corresponding index data to the metadata table. While this process runs, concurrent writers can continue ingesting data. The async indexing executor writes index data to base files in the target index partitions in the metadata table, while the ongoing writer append new index data to log files in those partitions. Hudi uses a conflict resolution mechanism to determine if an indexing operation needs to be retried due to concurrent write conflicts.</p>\n<p>To manage this concurrency, a lock provider must be configured for both the indexer and the data writers. Upon successful completion, the operation is marked by a completed indexing commit in the Hudi table’s timeline. For future improvements, the metadata table will employ non-blocking concurrency control to gracefully absorb conflicting updates from both indexing and write operations, thus avoiding wasteful retries. You can find configuration examples in the <a href=\"https://hudi.apache.org/docs/metadata_indexing/#setup-async-indexing\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">documentation</a>.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"summary\">Summary<a href=\"https://hudi.apache.org/blog/2025/11/12/deep-dive-into-hudis-indexing-subsystem-part-2-of-2#summary\" class=\"hash-link\" aria-label=\"Direct link to Summary\" title=\"Direct link to Summary\" translate=\"no\">​</a></h2>\n<p>Throughout this two-part series, we've explored how Hudi's indexing subsystem brings database-grade performance to the data lakehouse. In <a href=\"https://hudi.apache.org/blog/2025/10/29/deep-dive-into-hudis-indexing-subsystem-part-1-of-2/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">part 1</a>, we examined the metadata table's architecture and how files, column stats, and partition stats indexes work together to skip irrelevant data. In part 2, we covered specialized indexes—record, secondary, and expression indexes—that provide exact file locations for equality matching and handle transformed predicates. We also looked at async indexing, which lets you add resource-intensive indexes without blocking ongoing operations.</p>\n<p>Here's a quick guide for choosing the right indexes for your workload:</p>\n<ul>\n<li class=\"\"><strong>Files</strong>: Always enabled in the metadata table—provides partition and file lists in the table to facilitate common indexing processes</li>\n<li class=\"\"><strong>Column stats and partition stats</strong>: Enable by default and configure <code>hoodie.metadata.index.column.stats.column.list</code> to include only the columns you frequently filter on. These indexes are essential for range predicates and data skipping</li>\n<li class=\"\"><strong>Record index</strong>: Enable when you have frequent point lookups on record keys or when you need secondary indexes. The record index also optimizes Hudi's write path by efficiently routing updates and deletes</li>\n<li class=\"\"><strong>Secondary index</strong>: Create secondary indexes for non-record-key columns that appear in equality predicates. Each secondary index adds maintenance overhead, so focus on high-value columns</li>\n<li class=\"\"><strong>Expression index</strong>: Use expression indexes when queries contain predicates with inline transformations. Choose column stats type for range queries on transformed values, or bloom filter type for equality matching on high-cardinality columns</li>\n<li class=\"\"><strong>Async indexing</strong>: Use async indexing when adding indexes to large tables. The async indexer builds indexes in the background, keeping your writers and readers unblocked</li>\n</ul>\n<p>All indexes are maintained transactionally alongside data writes, ensuring consistency without sacrificing performance. The metadata table uses HFile format for fast point lookups and periodic compaction to keep reads efficient. This design makes Hudi's indexing subsystem both powerful and practical—ready to handle lakehouse-scale data while remaining simple to configure and operate.</p>\n<p>As Hudi continues to evolve, the indexing subsystem is designed for extensibility. Upcoming features like the bitmap index for low-cardinality columns and vector search index for AI workloads will further expand its capabilities. By understanding these indexing patterns and following the configuration guidelines in this series, you can build lakehouse tables that deliver the query performance your analytics and data pipelines demand.</p>",
            "url": "https://hudi.apache.org/blog/2025/11/12/deep-dive-into-hudis-indexing-subsystem-part-2-of-2",
            "title": "Deep Dive Into Hudi's Indexing Subsystem (Part 2 of 2)",
            "summary": "In part 1, we explored how Hudi's metadata table functions as a self-managed, multimodal indexing subsystem. We covered its internal architecture—a partitioned Hudi Merge-on-Read (MOR) table using HFile format for efficient key lookups—and how the files, column stats, and partition stats indexes work together to implement powerful data skipping. These indexes dramatically reduce I/O by pruning partitions and files that don't contain the data your query needs.",
            "date_modified": "2025-11-12T00:00:00.000Z",
            "author": {
                "name": "Shiyan Xu"
            },
            "tags": [
                "hudi",
                "indexing",
                "data lakehouse",
                "data skipping"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2025/11/07/how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse",
            "content_html": "<p><img decoding=\"async\" loading=\"lazy\" alt=\"Talk title slide\" src=\"https://hudi.apache.org/assets/images/image1-352adf99bd976cab9ea093daab842339.png\" width=\"1257\" height=\"437\" class=\"img_ev3q\"></p>\n<p><em>This post summarizes a FreeWheel talk from the Apache Hudi Community Sync. Watch the recording on <a href=\"https://www.youtube.com/watch?v=hQNSf82o3Rk\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">YouTube</a>.</em></p>\n<p><a href=\"https://www.freewheel.com/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">FreeWheel</a>, a division of Comcast, provides advanced video advertising solutions across TV and digital platforms. As the business scaled, FreeWheel faced growing challenges maintaining consistency, freshness, and operational efficiency in its data systems. To address these challenges, the team began transitioning from a legacy Lambda architecture to a modern, <a href=\"https://hudi.apache.org/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">Apache Hudi</a>-powered lakehouse approach.</p>\n<p>Their original stack, shown below, used multiple systems like <strong>Presto</strong>, <strong>ClickHouse</strong>, and <strong>Druid</strong> to serve analytical and real-time use cases. However, the architecture had some limitations:</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"Original multi-engine architecture\" src=\"https://hudi.apache.org/assets/images/image2-b8e66c22453fcee2810fc23d8cb480d0.png\" width=\"1999\" height=\"848\" class=\"img_ev3q\"></p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"data-freshness-issues\">Data freshness issues<a href=\"https://hudi.apache.org/blog/2025/11/07/how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse#data-freshness-issues\" class=\"hash-link\" aria-label=\"Direct link to Data freshness issues\" title=\"Direct link to Data freshness issues\" translate=\"no\">​</a></h2>\n<ul>\n<li class=\"\">Presto tables had a 3–4 hour delay, which was too slow for operational use cases.</li>\n<li class=\"\">Only ClickHouse and Druid offered near‑real‑time access (~5 minutes) but added complexity.</li>\n</ul>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"complex-ingestion\">Complex ingestion<a href=\"https://hudi.apache.org/blog/2025/11/07/how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse#complex-ingestion\" class=\"hash-link\" aria-label=\"Direct link to Complex ingestion\" title=\"Direct link to Complex ingestion\" translate=\"no\">​</a></h2>\n<ul>\n<li class=\"\">Data came from logs, CDC streams, files, and databases.</li>\n<li class=\"\">Each system had its own ingestion pipeline and refresh logic.</li>\n</ul>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"query-performance-bottlenecks\">Query performance bottlenecks<a href=\"https://hudi.apache.org/blog/2025/11/07/how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse#query-performance-bottlenecks\" class=\"hash-link\" aria-label=\"Direct link to Query performance bottlenecks\" title=\"Direct link to Query performance bottlenecks\" translate=\"no\">​</a></h2>\n<ul>\n<li class=\"\">With ~15 PB of data and 20M+ queries/day, scaling across three engines was costly and hard to operate.</li>\n</ul>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"use-case-1-lambda-architecture-and-its-drawbacks\">Use Case 1: Lambda Architecture and Its Drawbacks<a href=\"https://hudi.apache.org/blog/2025/11/07/how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse#use-case-1-lambda-architecture-and-its-drawbacks\" class=\"hash-link\" aria-label=\"Direct link to Use Case 1: Lambda Architecture and Its Drawbacks\" title=\"Direct link to Use Case 1: Lambda Architecture and Its Drawbacks\" translate=\"no\">​</a></h2>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"Lambda architecture overview\" src=\"https://hudi.apache.org/assets/images/image3-ecdbeac000b14fa4bf639c7e0daf8654.png\" width=\"1999\" height=\"857\" class=\"img_ev3q\"></p>\n<p>FreeWheel initially followed a traditional Lambda architecture, which separated the processing of batch and real‑time data. This approach created several problems: it required duplicate pipelines for batch and real‑time processing (leading to inefficient engineering workflows), and it struggled to scale ClickHouse for large aggregates.</p>\n<p>By consolidating on Hudi as the table format for both streaming and historical data, FreeWheel unified the storage layer and eliminated duplicate pipelines. Hudi’s <a href=\"https://hudi.apache.org/docs/write_operations/#upsert\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">upserts</a> by key and <a href=\"https://hudi.apache.org/docs/table_types/#incremental-queries\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">incremental processing</a> make it possible to serve near–real‑time analytics. The result is simpler operations, consistent logic, and a platform that scales with data volume and query complexity.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"use-case-2-real-time-inventory-management\">Use Case 2: Real-Time Inventory Management<a href=\"https://hudi.apache.org/blog/2025/11/07/how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse#use-case-2-real-time-inventory-management\" class=\"hash-link\" aria-label=\"Direct link to Use Case 2: Real-Time Inventory Management\" title=\"Direct link to Use Case 2: Real-Time Inventory Management\" translate=\"no\">​</a></h2>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"Real-time inventory with Hudi\" src=\"https://hudi.apache.org/assets/images/image4-d88e573a6dba93c4a1edd9e87a09fa5f.png\" width=\"1999\" height=\"1621\" class=\"img_ev3q\"></p>\n<p>Historically, daily ad inventory updates were a significant challenge. This method led to low forecasting accuracy and frequent delivery-performance mismatches.</p>\n<p>By modernizing the platform with Hudi, FreeWheel updates inventory within minutes. Order changes are applied as upserts to Hudi tables and become queryable shortly thereafter, dramatically improving forecast accuracy and reducing delivery‑vs‑forecast mismatches.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"use-case-3-scalable-audience-data-processing\">Use Case 3: Scalable Audience Data Processing<a href=\"https://hudi.apache.org/blog/2025/11/07/how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse#use-case-3-scalable-audience-data-processing\" class=\"hash-link\" aria-label=\"Direct link to Use Case 3: Scalable Audience Data Processing\" title=\"Direct link to Use Case 3: Scalable Audience Data Processing\" translate=\"no\">​</a></h2>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"Audience data architecture with Hudi snapshot\" src=\"https://hudi.apache.org/assets/images/image5-61c7ba9d9a0b31e28ce22ae67eb12d08.png\" width=\"941\" height=\"746\" class=\"img_ev3q\"></p>\n<p>FreeWheel uses Aerospike to ingest audience segments for its online services, which involves handling high‑frequency, real‑time data. However, this setup brought a few key challenges—chiefly, the need for analytical insights on top of real‑time data and the need to efficiently manage bulk loads alongside frequent updates.</p>\n<p>To address these challenges, FreeWheel introduced Hudi into the data pipeline. Hudi maintains a snapshot table for all audience data, enabling more flexible and efficient data management. It supports <a href=\"https://hudi.apache.org/docs/write_operations/#bulk_insert\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">bulk inserts</a>, <a href=\"https://hudi.apache.org/docs/write_operations/#upsert\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">upserts</a>, and change data capture (CDC), enabling smoother handling of updates and large‑scale data loads. Using CDC, large batches of audience updates are applied incrementally to the snapshot. With Hudi in place, the back‑end analytics system became much stronger, while the responsiveness of the online systems was preserved. This setup also improved the stability of the online targeting system, as heavy analytical workloads were moved off the key‑value store, reducing pressure on Aerospike and enhancing overall performance.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"hudi-in-practice-1-billionscale-updates-for-audiencesegment-ingestion\">Hudi in practice 1: Billion‑scale updates for audience‑segment ingestion<a href=\"https://hudi.apache.org/blog/2025/11/07/how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse#hudi-in-practice-1-billionscale-updates-for-audiencesegment-ingestion\" class=\"hash-link\" aria-label=\"Direct link to Hudi in practice 1: Billion‑scale updates for audience‑segment ingestion\" title=\"Direct link to Hudi in practice 1: Billion‑scale updates for audience‑segment ingestion\" translate=\"no\">​</a></h2>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"use-case-overview\">Use case overview<a href=\"https://hudi.apache.org/blog/2025/11/07/how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse#use-case-overview\" class=\"hash-link\" aria-label=\"Direct link to Use case overview\" title=\"Direct link to Use case overview\" translate=\"no\">​</a></h3>\n<p>This implementation showcases how a large‑scale platform ingests and updates audience‑segmentation data at the billion‑record scale using Hudi tables.\nThe architecture efficiently handles high‑frequency updates across more than 63,000 partitions and a table over 600 TB, with performance optimizations at both the data and infrastructure levels.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"key-architecture-and-design-principles\">Key architecture and design principles<a href=\"https://hudi.apache.org/blog/2025/11/07/how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse#key-architecture-and-design-principles\" class=\"hash-link\" aria-label=\"Direct link to Key architecture and design principles\" title=\"Direct link to Key architecture and design principles\" translate=\"no\">​</a></h3>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"Audience ingestion architecture and scheduler\" src=\"https://hudi.apache.org/assets/images/image6-c6d05709a20d20f096539322b86f933d.png\" width=\"1333\" height=\"1360\" class=\"img_ev3q\"></p>\n<h4 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"partitioning-and-orchestration\"><strong>Partitioning and orchestration</strong><a href=\"https://hudi.apache.org/blog/2025/11/07/how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse#partitioning-and-orchestration\" class=\"hash-link\" aria-label=\"Direct link to partitioning-and-orchestration\" title=\"Direct link to partitioning-and-orchestration\" translate=\"no\">​</a></h4>\n<p>FreeWheel uses the audience‑segment ID as the <a href=\"https://hudi.apache.org/docs/key_generation\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">partition key</a>. Each partition can be processed independently, allowing many Spark jobs to run in parallel. Each job upserts data to the Hudi lakehouse table.</p>\n<p>A central scheduler allocates work based on input size, priority, and write concurrency limits. This enables dynamic scaling across more than 63,000 partitions, where per-partition input sizes range from 1 million to 100 billion records.</p>\n<h4 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"decoupled-ingestion-pipeline\"><strong>Decoupled ingestion pipeline</strong><a href=\"https://hudi.apache.org/blog/2025/11/07/how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse#decoupled-ingestion-pipeline\" class=\"hash-link\" aria-label=\"Direct link to decoupled-ingestion-pipeline\" title=\"Direct link to decoupled-ingestion-pipeline\" translate=\"no\">​</a></h4>\n<ul>\n<li class=\"\">Scheduler: allocates resources based on input size and supports job priority, <a href=\"https://hudi.apache.org/docs/concurrency_control/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">multi-writer concurrency control</a>, and concurrency planning.</li>\n<li class=\"\">Ingestion job: Spark jobs process data and write it to the Hudi segment table in the lakehouse.</li>\n</ul>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"challenges-of-input-data-at-scale\">Challenges of input data at scale<a href=\"https://hudi.apache.org/blog/2025/11/07/how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse#challenges-of-input-data-at-scale\" class=\"hash-link\" aria-label=\"Direct link to Challenges of input data at scale\" title=\"Direct link to Challenges of input data at scale\" translate=\"no\">​</a></h3>\n<ul>\n<li class=\"\">Table size: over 600 TB.</li>\n<li class=\"\">Partition count: 63,000 audience‑segment partitions.</li>\n<li class=\"\">Data skew: massive variation in partition sizes, ranging from 1 million to 100 billion records.</li>\n</ul>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"metrics-and-performance-insights\">Metrics and performance insights<a href=\"https://hudi.apache.org/blog/2025/11/07/how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse#metrics-and-performance-insights\" class=\"hash-link\" aria-label=\"Direct link to Metrics and performance insights\" title=\"Direct link to Metrics and performance insights\" translate=\"no\">​</a></h3>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"Ingestion metrics and throughput\" src=\"https://hudi.apache.org/assets/images/image7-a67a3aa2b20416cf15c3229553c626ce.png\" width=\"1999\" height=\"979\" class=\"img_ev3q\"></p>\n<ul>\n<li class=\"\">Cost optimization<!-- -->\n<ul>\n<li class=\"\">Unit cost on AWS: ~$0.10 per million records updated.</li>\n</ul>\n</li>\n<li class=\"\">Throughput: the pipeline supports up to 12 million upserts per second.</li>\n</ul>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"operational-optimizations\"><strong>Operational optimizations</strong><a href=\"https://hudi.apache.org/blog/2025/11/07/how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse#operational-optimizations\" class=\"hash-link\" aria-label=\"Direct link to operational-optimizations\" title=\"Direct link to operational-optimizations\" translate=\"no\">​</a></h3>\n<ul>\n<li class=\"\">Handle S3 throttling by increasing partition parallelism. Hash partition prefixes and coordinate with AWS to raise per‑bucket request caps and remove I/O bottlenecks.</li>\n<li class=\"\">Balance SLA and cost with adaptive resource provisioning through the scheduler; choose resources based on input size to keep jobs stable while controlling spend.</li>\n<li class=\"\">Deduplicate before commit: group by record key, order by event timestamp, and write only the latest value to reduce churn and speed up writes.</li>\n</ul>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"hudi-in-practice-2-realtime-aggregated-ingestion-using-spark-streaming--clustering\">Hudi in practice 2: Real‑time aggregated ingestion using Spark Streaming + clustering<a href=\"https://hudi.apache.org/blog/2025/11/07/how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse#hudi-in-practice-2-realtime-aggregated-ingestion-using-spark-streaming--clustering\" class=\"hash-link\" aria-label=\"Direct link to Hudi in practice 2: Real‑time aggregated ingestion using Spark Streaming + clustering\" title=\"Direct link to Hudi in practice 2: Real‑time aggregated ingestion using Spark Streaming + clustering\" translate=\"no\">​</a></h2>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"pipeline-overview\">Pipeline overview<a href=\"https://hudi.apache.org/blog/2025/11/07/how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse#pipeline-overview\" class=\"hash-link\" aria-label=\"Direct link to Pipeline overview\" title=\"Direct link to Pipeline overview\" translate=\"no\">​</a></h3>\n<p>This implementation showcases an efficient pipeline where Spark Streaming ingests aggregated data into a Hudi lakehouse using the <a href=\"https://hudi.apache.org/docs/write_operations/#bulk_insert\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">bulk_insert</a> operation, followed by <a href=\"https://hudi.apache.org/blog/2021/08/23/async-clustering/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">asynchronous clustering</a>.</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"Streaming ingestion and clustering flow\" src=\"https://hudi.apache.org/assets/images/image8-20b16694604281c92df9a8a9079c22b5.png\" width=\"1003\" height=\"445\" class=\"img_ev3q\"></p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"data-ingestion-flow\">Data ingestion flow<a href=\"https://hudi.apache.org/blog/2025/11/07/how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse#data-ingestion-flow\" class=\"hash-link\" aria-label=\"Direct link to Data ingestion flow\" title=\"Direct link to Data ingestion flow\" translate=\"no\">​</a></h3>\n<ul>\n<li class=\"\">Kafka: raw events are streamed into Kafka.</li>\n<li class=\"\">Spark SQL on Streaming: consumes Kafka messages and performs near‑real‑time aggregations.</li>\n<li class=\"\">bulk_insert into Hudi lakehouse: aggregated data is appended using <a href=\"https://hudi.apache.org/docs/write_operations/#bulk_insert\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">bulk_insert</a>.</li>\n<li class=\"\">Clustering plan generation: clustering plans are created asynchronously.</li>\n<li class=\"\">HoodieClusteringJob: a cron job runs hourly to execute <a href=\"https://hudi.apache.org/docs/clustering/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">clustering</a> and consolidate small files.</li>\n</ul>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"results-at-a-glance\">Results at a glance<a href=\"https://hudi.apache.org/blog/2025/11/07/how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse#results-at-a-glance\" class=\"hash-link\" aria-label=\"Direct link to Results at a glance\" title=\"Direct link to Results at a glance\" translate=\"no\">​</a></h3>\n<ul>\n<li class=\"\">Massive file reduction: clustering reduced total file count by nearly 90%, minimizing small‑file pressure and improving metadata performance.</li>\n<li class=\"\">Write throughput boost: increased by about 114% due to optimized file layout.</li>\n<li class=\"\">Faster queries: Presto query performance improved significantly after clustering.</li>\n</ul>\n<p>However, Spark Streaming is a macro‑batch system, typically executing every one or two minutes. As a result, it does not trigger clustering jobs immediately but instead generates clustering plans for later execution. In production, clustering jobs are scheduled to run hourly and apply only to stable partitions, ensuring compaction and file optimization without impacting real‑time ingestion.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"conclusion\">Conclusion<a href=\"https://hudi.apache.org/blog/2025/11/07/how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse#conclusion\" class=\"hash-link\" aria-label=\"Direct link to Conclusion\" title=\"Direct link to Conclusion\" translate=\"no\">​</a></h2>\n<p>FreeWheel’s journey with Hudi transformed its data architecture—offering unified access, real‑time freshness, and scalable operations. The team credits Hudi’s community and feature set as key to its success.</p>\n<blockquote>\n<p>“We’re lucky to choose Hudi as our Lakehouse. Thanks to the powerful Hudi community!” – Bing Jiang</p>\n</blockquote>",
            "url": "https://hudi.apache.org/blog/2025/11/07/how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse",
            "title": "How FreeWheel Uses Apache Hudi to Power Its Data Lakehouse",
            "summary": "Talk title slide",
            "date_modified": "2025-11-07T00:00:00.000Z",
            "author": {
                "name": "The Hudi Community"
            },
            "tags": [
                "hudi",
                "lakehouse",
                "case-study",
                "freewheel"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2025/10/29/deep-dive-into-hudis-indexing-subsystem-part-1-of-2",
            "content_html": "<p>For decades, databases have relied on indexes—specialized data structures—to dramatically improve read and write performance by quickly locating specific records. Apache Hudi extends this fundamental principle to the data lakehouse with a unique and powerful approach. Every Hudi table contains a self-managed metadata table that functions as an indexing subsystem, enabling efficient data skipping and fast record lookups across a wide range of read and write scenarios.</p>\n<p>This two-part series dives into Hudi’s indexing subsystem. Part 1 explains the internal layout and data-skipping capabilities. <a href=\"https://hudi.apache.org/blog/2025/11/12/deep-dive-into-hudis-indexing-subsystem-part-2-of-2/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">Part 2</a> covers advanced features—record, secondary, and expression indexes—and asynchronous index maintenance. By the end, you’ll know how to leverage Hudi’s multimodal index to build more efficient lakehouse tables.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"the-metadata-table\">The Metadata Table<a href=\"https://hudi.apache.org/blog/2025/10/29/deep-dive-into-hudis-indexing-subsystem-part-1-of-2#the-metadata-table\" class=\"hash-link\" aria-label=\"Direct link to The Metadata Table\" title=\"Direct link to The Metadata Table\" translate=\"no\">​</a></h2>\n<p>Within a Hudi table (the data table), the metadata table itself is a Hudi Merge-on-Read (MOR) table. Unlike a typical data table, it features a specialized layout. The table is physically partitioned by index type, with each partition containing the relevant index entries. For its physical storage, the metadata table uses HFile as the base file format. This choice is deliberate: HFile is exceptionally efficient at handling key lookups—the predominant query pattern for indexing. Let’s explore the partitioned layout and HFile’s internal structure.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"multimodal-indexing\">Multimodal indexing<a href=\"https://hudi.apache.org/blog/2025/10/29/deep-dive-into-hudis-indexing-subsystem-part-1-of-2#multimodal-indexing\" class=\"hash-link\" aria-label=\"Direct link to Multimodal indexing\" title=\"Direct link to Multimodal indexing\" translate=\"no\">​</a></h3>\n<p>The metadata table is often referred to as a multimodal index because it houses a diverse range of index types, providing versatile capabilities to accelerate various query patterns. The following diagram illustrates the layout of the metadata table and its relationship with the main data table.</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"Metadata table and data table layout\" src=\"https://hudi.apache.org/assets/images/fig1-7461f8a910c9f7c87745a4a5e15c3498.png\" width=\"956\" height=\"650\" class=\"img_ev3q\"></p>\n<p>The metadata table is located in the <code>.hoodie/metadata/</code> directory under the data table’s base path. It contains partitions for different indexes, such as the files index (under the <code>files/</code> partition) for tracking the data table’s partitions and files, and the column stats index (under the <code>column_stats/</code> partition) for tracking file-level statistics (e.g., min/max values) for specific columns. Each index partition stores mapping entries tailored to its specific purpose.</p>\n<p>This partitioned design provides great flexibility, allowing you to enable only the indexes that suit your workload. It also ensures extensibility, making it straightforward to support new index types in the future. For example, the <a href=\"https://github.com/apache/hudi/blob/master/rfc/rfc-92/rfc-92.md\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">bitmap index</a> and the vector search index are on the <a href=\"https://hudi.apache.org/roadmap\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">roadmap</a> and will be maintained in their own dedicated partitions.</p>\n<p>When committing to a data table, the metadata table is updated within the same transactional write. This crucial step ensures that index entries are always synchronized with data table records, upholding data integrity across the table. Therefore, choosing Merge-on-Read (MOR) as the table type for the metadata table is an obvious choice. MOR offers the advantage of absorbing high-frequency write operations, preventing the metadata table’s update process from becoming a bottleneck for overall table writes. To ensure efficient reading, Hudi automatically performs compaction on the metadata table based on its compaction configuration. By default, an inline compaction will be executed every 10 writes to the metadata table, merging accumulated log files with base files to produce a new set of read-optimized base files in HFile format.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"hfile-format\">HFile format<a href=\"https://hudi.apache.org/blog/2025/10/29/deep-dive-into-hudis-indexing-subsystem-part-1-of-2#hfile-format\" class=\"hash-link\" aria-label=\"Direct link to HFile format\" title=\"Direct link to HFile format\" translate=\"no\">​</a></h3>\n<p>The HFile format stores key-value pairs in a sorted, immutable, and block-indexed way, modeled after Google’s SSTable introduced by the <a href=\"https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">Bigtable paper</a>. Here is the description of SSTable quoted from the paper:</p>\n<blockquote>\n<p>An SSTable provides a persistent, ordered immutable map from keys to values, where both keys and values are arbitrary byte strings. Operations are provided to look up the value associated with a specified key, and to iterate over all key/value pairs in a specified key range. Internally, each SSTable contains a sequence of blocks (typically each block is 64KB in size, but this is configurable). A block index (stored at the end of the SSTable) is used to locate blocks; the index is loaded into memory when the SSTable is opened. A lookup can be performed with a single disk seek: we first find the appropriate block by performing a binary search in the in-memory index, and then reading the appropriate block from disk.</p>\n</blockquote>\n<p>As you can tell, by implementing the SSTable, HFile is especially efficient at performing random access, which is the primary query pattern for indexing—given a specific piece of information, like a record key or a partition value, return matching results, such as the file ID that contains the record key, or the list of files that belong to the partition.</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"HFile structure\" src=\"https://hudi.apache.org/assets/images/fig2-686d385820b946c158ab5110d3bcbaf8.png\" width=\"647\" height=\"594\" class=\"img_ev3q\"></p>\n<p>Because the keys in an HFile are stored in lexicographic order, a batched lookup with a common key prefix is also highly efficient, requiring only a sequential read of nearby keys.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"default-behaviors\">Default behaviors<a href=\"https://hudi.apache.org/blog/2025/10/29/deep-dive-into-hudis-indexing-subsystem-part-1-of-2#default-behaviors\" class=\"hash-link\" aria-label=\"Direct link to Default behaviors\" title=\"Direct link to Default behaviors\" translate=\"no\">​</a></h3>\n<p>When a Hudi table is created, the metadata table will be enabled with three partitions by default: <em>files</em>, <em>column stats</em>, and <em>partition stats</em>:</p>\n<ul>\n<li class=\"\"><strong>Files</strong>: stores the list of all partitions and the lists of all base files and log files of each partition, located at the <code>files/</code> partition of the metadata table.</li>\n<li class=\"\"><strong>Column stats</strong>: stores file-level statistics like min, max, value count, and null count for specified columns, located at the <code>column_stats/</code> partition of the metadata table.</li>\n<li class=\"\"><strong>Partition stats</strong>: stores partition-level statistics like min, max, value count, and null count for specified columns, located at the <code>partition_stats/</code> partition of the metadata table.</li>\n</ul>\n<p>By default, when no column is specified for column_stats and partition_stats, Hudi will index the first 32 columns (controlled by <code>hoodie.metadata.index.column.stats.max.columns.to.index</code>) available in the table schema.</p>\n<p>Whenever a new write is performed on the data table, the metadata table will be updated accordingly. For any available index, new index entries will be upserted to its corresponding partition. For example, if the new write creates a new partition in the data table with some new base files, the files partition will be updated and contain the latest partition and file lists. Similarly, the column stats and partition stats partitions will receive new entries indicating the updated statistics for the new files and partitions.</p>\n<p>Note that by design, you cannot disable the files partition, as it is a fundamental index that serves both read and write processes. You can still, although not recommended, disable the entire metadata table by setting <code>hoodie.metadata.enable=false</code> during a write.</p>\n<p>We will discuss more details about how the default indexes work to improve read and write performance. We will also introduce more indexes supported by the metadata table with usage examples in the following sections.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"data-skipping-with-files-column-stats-and-partition-stats\">Data Skipping with Files, Column Stats, and Partition Stats<a href=\"https://hudi.apache.org/blog/2025/10/29/deep-dive-into-hudis-indexing-subsystem-part-1-of-2#data-skipping-with-files-column-stats-and-partition-stats\" class=\"hash-link\" aria-label=\"Direct link to Data Skipping with Files, Column Stats, and Partition Stats\" title=\"Direct link to Data Skipping with Files, Column Stats, and Partition Stats\" translate=\"no\">​</a></h2>\n<p>Data skipping is a core optimization technique that avoids unnecessary data scanning. Its most basic form is physical partitioning, where data is organized into directories based on columns like <code>order_date</code> in a customer order table. When a query filters on a partitioned column, the engine uses <em>partition pruning</em> to read only the relevant directories. More advanced techniques store lightweight statistics—such as min/max values—for data within each file. The query engine consults this metadata first; if the stats indicate a file cannot contain the required data, the engine skips reading it entirely. This reduction in I/O is a key strategy for accelerating queries and lowering compute costs.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"the-data-skipping-process\">The data skipping process<a href=\"https://hudi.apache.org/blog/2025/10/29/deep-dive-into-hudis-indexing-subsystem-part-1-of-2#the-data-skipping-process\" class=\"hash-link\" aria-label=\"Direct link to The data skipping process\" title=\"Direct link to The data skipping process\" translate=\"no\">​</a></h3>\n<p>Hudi’s indexing subsystem implements a multi-level skipping strategy using a combination of indexes. Query engines like Spark or Trino can leverage Hudi’s files, partition stats, and column stats indexes to improve performance dramatically. The process, illustrated in the figure below, unfolds in several stages.</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"Data skipping process flow\" src=\"https://hudi.apache.org/assets/images/fig3-468ec18846bf7194631d838fd9824bcf.png\" width=\"981\" height=\"706\" class=\"img_ev3q\"></p>\n<p>First, the query engine parses the input SQL and extracts relevant filter predicates, such as <code>price &gt;= 300</code>. These predicates are pushed down to Hudi’s integration component, which manages the index lookup process.</p>\n<p>The component then consults the files index to get an initial list of partitions. It prunes this list using the partition stats index, which holds partition-level statistics like min/max values. For example, any partition with a maximum price below 300 is skipped entirely.</p>\n<p>After this initial pruning, the component consults the files index again to retrieve the list of data files within the remaining partitions. This file list is pruned further using the column stats index, which provides the same min/max statistics at the file level.</p>\n<p>This multi-step process ensures that the query engine reads only the minimum set of files required to satisfy the query, significantly reducing the total amount of data processed.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"sql-examples\">SQL examples<a href=\"https://hudi.apache.org/blog/2025/10/29/deep-dive-into-hudis-indexing-subsystem-part-1-of-2#sql-examples\" class=\"hash-link\" aria-label=\"Direct link to SQL examples\" title=\"Direct link to SQL examples\" translate=\"no\">​</a></h3>\n<p>The following examples demonstrate data skipping in action. We will create a Hudi table and execute Spark SQL queries against it, starting with both partition and column stats disabled to establish a baseline.</p>\n<div class=\"language-sql codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-sql codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">CREATE</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">TABLE</span><span class=\"token plain\"> orders </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    order_id STRING</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    price </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">DECIMAL</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token number\">12</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    order_status STRING</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    update_ts </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">BIGINT</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    shipping_date </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">DATE</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    shipping_country STRING</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">USING</span><span class=\"token plain\"> HUDI</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">PARTITIONED </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">BY</span><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\">shipping_country</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">OPTIONS </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    primaryKey </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'order_id'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    preCombineField </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'update_ts'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    hoodie</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token plain\">metadata</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">index</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">column</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token plain\">stats</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">enable</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'false'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    hoodie</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token plain\">metadata</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">index</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">partition</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token plain\">stats</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">enable</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'false'</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><br></span></code></pre></div></div>\n<p>And insert some sample data:</p>\n<div class=\"language-sql codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-sql codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">INSERT</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">INTO</span><span class=\"token plain\"> orders </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">VALUES</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'ORD001'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token number\">389.99</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'PENDING'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\">    </span><span class=\"token number\">17495166353</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">DATE</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'2023-01-01'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'A'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'ORD002'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token number\">199.99</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'CONFIRMED'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\">  </span><span class=\"token number\">17495167353</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">DATE</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'2023-01-01'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'A'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'ORD003'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token number\">59.50</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'SHIPPED'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\">    </span><span class=\"token number\">17495168353</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">DATE</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'2023-01-11'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'B'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'ORD004'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token number\">99.00</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'PENDING'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\">    </span><span class=\"token number\">17495169353</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">DATE</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'2023-02-09'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'B'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'ORD005'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token number\">19.99</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'PENDING'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\">    </span><span class=\"token number\">17495170353</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">DATE</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'2023-06-12'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'C'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'ORD006'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token number\">5.99</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\">   </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'SHIPPED'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\">    </span><span class=\"token number\">17495171353</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">DATE</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'2023-07-31'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'C'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><br></span></code></pre></div></div>\n<h4 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"only-the-files-index\">Only the files index<a href=\"https://hudi.apache.org/blog/2025/10/29/deep-dive-into-hudis-indexing-subsystem-part-1-of-2#only-the-files-index\" class=\"hash-link\" aria-label=\"Direct link to Only the files index\" title=\"Direct link to Only the files index\" translate=\"no\">​</a></h4>\n<p>With both column stats and partition stats disabled, only the files index is built during the insert operation. We’ll use the SQL below for our test:</p>\n<div class=\"language-sql codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-sql codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">SELECT</span><span class=\"token plain\"> order_id</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> price</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> shipping_country</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">FROM</span><span class=\"token plain\"> orders</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">WHERE</span><span class=\"token plain\"> price </span><span class=\"token operator\">&gt;</span><span class=\"token plain\"> </span><span class=\"token number\">300</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><br></span></code></pre></div></div>\n<p>This query looks for orders with price greater than 300, which only exist in partition 'A' (shipping_country = 'A'). After running the SQL, here's what we see in the Spark UI:</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"Spark UI: files index only\" src=\"https://hudi.apache.org/assets/images/fig4-d28ef18dd51d094cfb34e8bae0e65420.png\" width=\"747\" height=\"504\" class=\"img_ev3q\"></p>\n<p>Spark read all 3 partitions and 3 files to find potential matches, but only 1 record from partition A actually satisfied the query condition.</p>\n<h4 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"enabling-column-stats\">Enabling column stats<a href=\"https://hudi.apache.org/blog/2025/10/29/deep-dive-into-hudis-indexing-subsystem-part-1-of-2#enabling-column-stats\" class=\"hash-link\" aria-label=\"Direct link to Enabling column stats\" title=\"Direct link to Enabling column stats\" translate=\"no\">​</a></h4>\n<p>Now let's enable column stats while keeping partition stats disabled. Note that we can't do it the other way around—partition stats requires column stats to be enabled first.</p>\n<div class=\"language-sql codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-sql codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">CREATE</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">TABLE</span><span class=\"token plain\"> orders </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    order_id STRING</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    price </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">DECIMAL</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token number\">12</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    order_status STRING</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    update_ts </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">BIGINT</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    shipping_date </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">DATE</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    shipping_country STRING</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">USING</span><span class=\"token plain\"> HUDI</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">PARTITIONED </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">BY</span><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\">shipping_country</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">OPTIONS </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    primaryKey </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'order_id'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    preCombineField </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'update_ts'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    hoodie</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token plain\">metadata</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">index</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">column</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token plain\">stats</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">enable</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'true'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    hoodie</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token plain\">metadata</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">index</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">partition</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token plain\">stats</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">enable</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'false'</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><br></span></code></pre></div></div>\n<p>Running the same SQL gives us this in the Spark UI:</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"Spark UI: column stats enabled\" src=\"https://hudi.apache.org/assets/images/fig5-8c6a7e86f7bb1789e91ac4c539fd1b78.png\" width=\"692\" height=\"461\" class=\"img_ev3q\"></p>\n<p>Now it shows all 3 partitions but only 1 file was scanned. Without partition stats, the query engine couldn't prune partitions, but column stats successfully filtered out the non-matching files. The compute cost of examining those 2 irrelevant partitions and their files could have been avoided with partition stats enabled.</p>\n<h4 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"enabling-column-stats-and-partition-stats\">Enabling column stats and partition stats<a href=\"https://hudi.apache.org/blog/2025/10/29/deep-dive-into-hudis-indexing-subsystem-part-1-of-2#enabling-column-stats-and-partition-stats\" class=\"hash-link\" aria-label=\"Direct link to Enabling column stats and partition stats\" title=\"Direct link to Enabling column stats and partition stats\" translate=\"no\">​</a></h4>\n<p>Now let's enable partition stats as well. Since both indexes are enabled by default in Hudi 1.x, we can simply omit those additional configs from the CREATE statement:</p>\n<div class=\"language-sql codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-sql codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">CREATE</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">TABLE</span><span class=\"token plain\"> orders </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    order_id STRING</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    price </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">DECIMAL</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token number\">12</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    order_status STRING</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    update_ts </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">BIGINT</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    shipping_date </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">DATE</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    shipping_country STRING</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">USING</span><span class=\"token plain\"> HUDI</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">PARTITIONED </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">BY</span><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\">shipping_country</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">OPTIONS </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    primaryKey </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'order_id'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    preCombineField </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'update_ts'</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><br></span></code></pre></div></div>\n<p>Running the same SQL gives us this in the Spark UI:</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"Spark UI: column + partition stats enabled\" src=\"https://hudi.apache.org/assets/images/fig6-29897046b22cae13ecde0bad971f9544.png\" width=\"685\" height=\"457\" class=\"img_ev3q\"></p>\n<p>Now we see the full pruning effect happened—only 1 relevant partition and 1 relevant file were scanned, thanks to both indexes working together. <a href=\"https://hudi.apache.org/blog/2025/10/22/Partition_Stats_Enhancing_Column_Stats_in_Hudi_1.0/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">This blog</a> shows a 93% reduction in query time running on a 1 TB dataset.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"configure-relevant-columns-to-be-indexed\">Configure relevant columns to be indexed<a href=\"https://hudi.apache.org/blog/2025/10/29/deep-dive-into-hudis-indexing-subsystem-part-1-of-2#configure-relevant-columns-to-be-indexed\" class=\"hash-link\" aria-label=\"Direct link to Configure relevant columns to be indexed\" title=\"Direct link to Configure relevant columns to be indexed\" translate=\"no\">​</a></h3>\n<p>By default, Hudi indexes the first 32 columns for both partition stats and column stats. This limit prevents excessive metadata overhead—each indexed column requires computing min, max, null-count, and value-count statistics for every partition and data file. In most cases, you only need to index a small subset of columns that are frequently used in query predicates. You can specify which columns to be indexed to reduce the maintenance costs:</p>\n<div class=\"language-sql codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-sql codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">CREATE</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">TABLE</span><span class=\"token plain\"> orders </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    order_id STRING</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    price </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">DECIMAL</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token number\">12</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    order_status STRING</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    update_ts </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">BIGINT</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    shipping_date </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">DATE</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    shipping_country STRING</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">USING</span><span class=\"token plain\"> HUDI</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">PARTITIONED </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">BY</span><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\">shipping_country</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">OPTIONS </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    primaryKey </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'order_id'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    preCombineField </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'update_ts'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'hoodie.metadata.index.column.stats.column.list'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'price,shipping_date'</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><br></span></code></pre></div></div>\n<p>The config <code>hoodie.metadata.index.column.stats.column.list</code> applies to both partition stats and column stats. By indexing just the <code>price</code> and <code>shipping_date</code> columns, queries filtering on price comparisons or shipping date ranges will already see significant performance improvements.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"key-takeaways-and-whats-next\">Key Takeaways and What's Next<a href=\"https://hudi.apache.org/blog/2025/10/29/deep-dive-into-hudis-indexing-subsystem-part-1-of-2#key-takeaways-and-whats-next\" class=\"hash-link\" aria-label=\"Direct link to Key Takeaways and What's Next\" title=\"Direct link to Key Takeaways and What's Next\" translate=\"no\">​</a></h2>\n<p>Hudi’s metadata table is itself a Hudi Merge‑on‑Read (MOR) table that acts as a multimodal indexing subsystem. It is physically partitioned by index type (for example, <code>files/</code>, <code>column_stats/</code>, <code>partition_stats/</code>) and stores base files in the HFile (SSTable‑like) format. This layout provides fast point lookups and efficient batched scans by key prefix—exactly the access patterns indexing needs at lakehouse scale.</p>\n<p>Index maintenance happens transactionally alongside data writes, keeping index entries consistent with the data table. Periodic compaction merges log files into read‑optimized HFile base files to keep point lookups fast and predictable. On the read path, Hudi composes multiple indexes to minimize I/O: the files index enumerates candidates, partition stats prune irrelevant partitions, and column stats prune non‑matching files. In effect, the engine scans only the minimum set of files required to satisfy a query.</p>\n<p>In practice, the defaults are a strong starting point. Keep the metadata table enabled and explicitly list only the columns you frequently filter on via <code>hoodie.metadata.index.column.stats.column.list</code> to control metadata overhead. In <a href=\"https://hudi.apache.org/blog/2025/11/12/deep-dive-into-hudis-indexing-subsystem-part-2-of-2/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">part 2</a>, we’ll go deeper into accelerating equality‑matching and expression‑based predicates using the record, secondary, and expression indexes, and discuss how asynchronous index maintenance keeps writers unblocked while indexes build in the background.</p>",
            "url": "https://hudi.apache.org/blog/2025/10/29/deep-dive-into-hudis-indexing-subsystem-part-1-of-2",
            "title": "Deep Dive Into Hudi’s Indexing Subsystem (Part 1 of 2)",
            "summary": "For decades, databases have relied on indexes—specialized data structures—to dramatically improve read and write performance by quickly locating specific records. Apache Hudi extends this fundamental principle to the data lakehouse with a unique and powerful approach. Every Hudi table contains a self-managed metadata table that functions as an indexing subsystem, enabling efficient data skipping and fast record lookups across a wide range of read and write scenarios.",
            "date_modified": "2025-10-29T00:00:00.000Z",
            "author": {
                "name": "Shiyan Xu"
            },
            "tags": [
                "hudi",
                "indexing",
                "data lakehouse",
                "data skipping"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2025/10/22/Partition_Stats_Enhancing_Column_Stats_in_Hudi_1.0",
            "content_html": "<p>For those tracking Apache Hudi's performance enhancements, the introduction of the column stats index was a significant development, as <a href=\"https://www.onehouse.ai/blog/hudis-column-stats-index-and-data-skipping-feature-help-speed-up-queries-by-an-orders-of-magnitude\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">detailed in this blog</a>. It represented a major advancement for query optimization by implementing a straightforward yet highly effective concept: storing lightweight, file-level statistics (such as min/max values and null counts) for specific columns. This provided Hudi's query engine a substantial performance improvement.</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"cover\" src=\"https://hudi.apache.org/assets/images/fig1-103edc705ab1254fb8b23ba25db76fd6.jpg\" width=\"1944\" height=\"1654\" class=\"img_ev3q\"></p>\n<p>Instead of blindly scanning every single file for a query, the engine could first peek at the index entries—which is far more efficient than reading all the Parquet footers—to determine which files <em>couldn't</em> possibly contain the relevant data. This data-skipping capability meant engines could bypass large amounts of irrelevant data, slashing query latency. But that skipping process is conducted at the file level—what if we could apply a similar skipping logic at the partition level? Since a single physical partition can contain thousands of data files, applying this logic at the partition level can further amplify the performance gains by only considering files in the relevant partitions. This is precisely the capability that Hudi 1.0’s partition stats index introduces.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"multimodal-indexing\">Multimodal Indexing<a href=\"https://hudi.apache.org/blog/2025/10/22/Partition_Stats_Enhancing_Column_Stats_in_Hudi_1.0#multimodal-indexing\" class=\"hash-link\" aria-label=\"Direct link to Multimodal Indexing\" title=\"Direct link to Multimodal Indexing\" translate=\"no\">​</a></h2>\n<p>Hudi’s <a href=\"https://hudi.apache.org/docs/indexes#multi-modal-indexing\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">multimodal indexing subsystem</a> enhances both read and write performance in data lakehouses by supporting versatile index types optimized for different workloads. This subsystem is built on a scalable, internal metadata table that ensures ACID-compliant updates and efficient lookups, which in turn reduces full data scans. It houses various indexes—such as the files, column stats, and partition stats—which work together to improve efficiency in reads, writes, and upserts, providing scalable, low-latency query performance for large datasets in the lakehouse.</p>\n<p>The partition stats index is built on top of the column stats index by aggregating its file-level statistics up to the partition level. As we've covered, the column stats index tracks statistics (min, max, null counts) for <em>individual files</em>, enabling fine-grained file pruning. The partition stats index, in contrast, summarizes these same statistics across <em>all files</em> within a single partition.</p>\n<p>This partition-level aggregation allows Hudi to efficiently prune entire physical partitions before even examining file-level indexes, leading to faster query planning and execution by skipping large chunks of irrelevant data early in the process. In other words, the partition stats index provides a coarse-grained, high-level pruning layer on top of the fine-grained, file-level pruning enabled by the column stats index.</p>\n<p>Because partition-level pruning happens first, it narrows down the scope of files that the column stats index needs to inspect, improving overall query performance and reducing overhead on large datasets. The diagram below illustrates the file pruning process:</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"file pruning process\" src=\"https://hudi.apache.org/assets/images/fig2-468ec18846bf7194631d838fd9824bcf.png\" width=\"981\" height=\"706\" class=\"img_ev3q\"></p>\n<p>During query planning, the Hudi integration for the query engine takes the predicates parsed from user queries and queries the indexes within the metadata table.</p>\n<ul>\n<li class=\"\">The files index is queried first to return an initial list of all partitions in the table.</li>\n<li class=\"\">The partition stats index then filters this partition list by checking if each partition’s min/max values for the indexed columns fall within the predicate's range. For example, with a predicate of <code>A = 100</code>, the index skips any partition whose <code>min(A)</code> is greater than 100 or whose <code>max(A)</code> is less than 100.</li>\n<li class=\"\">The files index is queried again to retrieve a list of all files <em>within</em> these pruned partitions.</li>\n<li class=\"\">This file list is then passed to the column stats index, which performs the final, fine-grained pruning by applying the query predicates to the file-level statistics.</li>\n<li class=\"\">Finally, this pruned list of files is returned to the query engine to complete query planning.</li>\n</ul>\n<p>This dual-layer pruning strategy is especially impactful in production systems managing large amounts of data. By complementing the fine-grained column stats index with this coarse-grained partition skipping, Hudi’s metadata table significantly reduces I/O, computation, and cost. For end-users, this translates directly into a better experience, turning queries that once took minutes into operations that complete in seconds.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"example-us-shipping-addresses\">Example: US Shipping Addresses<a href=\"https://hudi.apache.org/blog/2025/10/22/Partition_Stats_Enhancing_Column_Stats_in_Hudi_1.0#example-us-shipping-addresses\" class=\"hash-link\" aria-label=\"Direct link to Example: US Shipping Addresses\" title=\"Direct link to Example: US Shipping Addresses\" translate=\"no\">​</a></h2>\n<p>To understand the impact, let's use the example table below, which stores US shipping addresses for online orders and is partitioned by <code>state</code>. This table could contain billions of records, and we want to run a query filtering on the <code>zip_code</code> column.</p>\n<p>By default, the files, column stats, and partition stats indexes are all enabled in Hudi 1.0. You can create the Hudi table using Spark SQL, for example, without needing additional configs to enable column stats and partition stats:</p>\n<div class=\"language-sql codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-sql codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">CREATE</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">TABLE</span><span class=\"token plain\"> shipping_address </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    order_id STRING</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    state STRING</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    zip_code STRING</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">USING</span><span class=\"token plain\"> HUDI</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">TBLPROPERTIES </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    primaryKey </span><span class=\"token operator\">=</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'order_id'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    hoodie</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token plain\">metadata</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">index</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">column</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token plain\">stats</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">column</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token plain\">list </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'zip_code'</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">PARTITIONED </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">BY</span><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\">state</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><br></span></code></pre></div></div>\n<p>Note that, in practice, you would most likely want to use <code>hoodie.metadata.index.column.stats.column.list</code> to indicate which column(s) to index according to your business use case, otherwise, the first 32 columns in the table schema will be indexed by default, which probably won’t be optimal. The specified columns apply to both the column stats and partition stats indexes.</p>\n<p>Without the column and partition stats indexes, a query for a specific ZIP code (e.g., <code>zip_code = '90001'</code>) would force the query engine to perform a full table scan. This is highly inefficient, leading to high query latency and excessive resource consumption.</p>\n<p>With the indexes enabled, the process is drastically different.</p>\n<ol>\n<li class=\"\">During write operations, the Hudi writer tracks statistics for the <code>zip_code</code> column. The column stats index stores min/max values for each data file, and the partition stats index aggregates and stores the min/max <code>zip_code</code> for each <code>state</code>.</li>\n<li class=\"\">At query time, suppose the partition stats index shows that the \"California\" partition contains ZIP codes from \"90000\" to \"96199\", while the \"New York\" partition contains ZIP codes from \"10000\" to \"14999\". When the query for <code>zip_code = '90001'</code> is executed, the query planner first consults the partition stats index. It sees that \"90001\" falls within the \"California\" partition's range but outside the \"New York\" partition's range.</li>\n<li class=\"\">The engine can therefore skip the entire \"New York\" partition (and any other partition like \"Texas\" or \"Florida\" whose ZIP code range doesn't include \"90001\"). The query proceeds by only reading data from the \"California\" partition—the only one that could possibly contain the data.</li>\n</ol>\n<p>This ability to prune entire partitions before reading any files is what provides such a significant performance gain.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"results-the-data-skipping-effect\">Results: the Data Skipping Effect<a href=\"https://hudi.apache.org/blog/2025/10/22/Partition_Stats_Enhancing_Column_Stats_in_Hudi_1.0#results-the-data-skipping-effect\" class=\"hash-link\" aria-label=\"Direct link to Results: the Data Skipping Effect\" title=\"Direct link to Results: the Data Skipping Effect\" translate=\"no\">​</a></h2>\n<p>We conducted a focused benchmarking exercise using a synthetic dataset generated by the open-source tool <a href=\"https://github.com/onehouseinc/lake-loader\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">lake_loader</a>. Specifically, we created a 1 TB table for the US shipping addresses example and built both the column stats and partition stats indexes on this dataset.</p>\n<p>The benchmarking objective was to evaluate the performance impact from the two indexes for data skipping. To do this, we executed the following query in two scenarios:</p>\n<div class=\"language-sql codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-sql codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">select</span><span class=\"token plain\"> </span><span class=\"token function\" style=\"color:rgb(80, 250, 123)\">count</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">from</span><span class=\"token plain\"> shipping_address </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">where</span><span class=\"token plain\"> zip_code </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'10001'</span><br></span></code></pre></div></div>\n<p>One with the column and partition stats indexes enabled (default), and one with both indexes disabled for reads, which forced a full table scan.</p>\n<p>The Spark job was configured with:</p>\n<ul>\n<li class=\"\">Executor cores = 4</li>\n<li class=\"\">Executor memory = 10g</li>\n<li class=\"\">Number of executors = 60</li>\n</ul>\n<p>The Spark DAGs for the two scenarios show the file pruning effect:</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"Spark DAGs comparison\" src=\"https://hudi.apache.org/assets/images/fig3-2a993e3d03e054e6e2697772a56e673f.png\" width=\"3456\" height=\"1992\" class=\"img_ev3q\"></p>\n<p>With both column stats and partition stats indexes enabled (the left-side DAG), the number of files read was 19,304. In contrast, the disabled setup (the right-side DAG) resulted in reading 393,360 files—about 20 times more.</p>\n<p>The runtime comparison chart below shows the query time difference (shorter is better):</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"perf run time chart\" src=\"https://hudi.apache.org/assets/images/fig4-8df00f82c190ca1663131f7dbb6ddf8d.jpg\" width=\"2428\" height=\"1720\" class=\"img_ev3q\"></p>\n<p>Enabling data skipping with both the column stats and partition stats indexes for the Hudi table delivers approximately a 93% reduction in query runtime compared to the full scan (no data skipping).</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"conclusion\">Conclusion<a href=\"https://hudi.apache.org/blog/2025/10/22/Partition_Stats_Enhancing_Column_Stats_in_Hudi_1.0#conclusion\" class=\"hash-link\" aria-label=\"Direct link to Conclusion\" title=\"Direct link to Conclusion\" translate=\"no\">​</a></h2>\n<p>The new partition stats index is a powerful addition to Hudi's multimodal indexing subsystem, directly addressing the challenge of query performance on large-scale partitioned tables. By working in concert with the existing column stats index, it provides a crucial layer of coarse-grained pruning, allowing the query engine to eliminate entire partitions from consideration <em>before</em> inspecting individual files. As our benchmark showed, this two-level pruning strategy—first by partition, then by file—is not just a minor tweak. It results in a dramatic reduction in I/O, slashing query runtimes by over 93% and enabling near-interactive query speeds. This feature solidifies Hudi's data-skipping capabilities, making it even more efficient to run demanding analytical queries directly on the data lakehouse, saving both time and computation costs.</p>",
            "url": "https://hudi.apache.org/blog/2025/10/22/Partition_Stats_Enhancing_Column_Stats_in_Hudi_1.0",
            "title": "Partition Stats: Enhancing Column Stats in Hudi 1.0",
            "summary": "For those tracking Apache Hudi's performance enhancements, the introduction of the column stats index was a significant development, as detailed in this blog. It represented a major advancement for query optimization by implementing a straightforward yet highly effective concept: storing lightweight, file-level statistics (such as min/max values and null counts) for specific columns. This provided Hudi's query engine a substantial performance improvement.",
            "date_modified": "2025-10-22T00:00:00.000Z",
            "author": {
                "name": "Aditya Goenka and Shiyan Xu"
            },
            "tags": [
                "hudi",
                "indexing",
                "data lakehouse",
                "data skipping"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless",
            "content_html": "<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"introduction\">Introduction<a href=\"https://hudi.apache.org/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless#introduction\" class=\"hash-link\" aria-label=\"Direct link to Introduction\" title=\"Direct link to Introduction\" translate=\"no\">​</a></h2>\n<p>In <a href=\"https://www.youtube.com/watch?v=dAM2zOvnPmw\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">this community sharing session</a>, Manish Gaurav from Upstox shared insights into the complexities of managing data ingestion at scale. Drawing from the company’s experience as a leading online trading platform in India, the discussion highlighted challenges around file-level upserts, ensuring atomic operations, and handling small files effectively. Upstox shared how they built a modern data platform using Apache Hudi and dbt to address these issues. In this blog post, we’ll break down their solution and why it matters.</p>\n<p>Upstox is a leading online trading platform that enables millions of users to invest in equities, commodities, derivatives, and currencies. With over 12 million customers generating 300,000 data requests daily, the company's data team is responsible for delivering the real-time insights that power key products, including:</p>\n<ul>\n<li class=\"\">Search functionality</li>\n<li class=\"\">A customer service chatbot (powered by OpenAI)</li>\n<li class=\"\">Personalized portfolio recommendations</li>\n</ul>\n<p><img decoding=\"async\" loading=\"lazy\" src=\"https://hudi.apache.org/assets/images/fig1-3baa485e75ef728786f15b45d2d97d6b.png\" width=\"1999\" height=\"1312\" class=\"img_ev3q\"></p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"data-sources\">Data Sources<a href=\"https://hudi.apache.org/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless#data-sources\" class=\"hash-link\" aria-label=\"Direct link to Data Sources\" title=\"Direct link to Data Sources\" translate=\"no\">​</a></h3>\n<p>Upstox ingests 250–300 GB of structured and semi-structured data per day from a variety of sources:</p>\n<ul>\n<li class=\"\">Order and transaction data from exchanges</li>\n<li class=\"\">Microservice telemetry from Cloudflare</li>\n<li class=\"\">Customer support data from platforms like Freshdesk and SquadStack</li>\n<li class=\"\">Behavioral analytics from Mixpanel</li>\n<li class=\"\">Data from operational databases (MongoDB, MySQL, and MS SQL) via AWS DMS</li>\n</ul>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"the-challenges-with-initial-data-platform\">The Challenges with Initial Data Platform<a href=\"https://hudi.apache.org/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless#the-challenges-with-initial-data-platform\" class=\"hash-link\" aria-label=\"Direct link to The Challenges with Initial Data Platform\" title=\"Direct link to The Challenges with Initial Data Platform\" translate=\"no\">​</a></h2>\n<p>As Upstox grew, so did the complexity of its data operations. Here are some of the early bottlenecks the company faced:</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"data-ingestion-issues\">Data Ingestion Issues<a href=\"https://hudi.apache.org/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless#data-ingestion-issues\" class=\"hash-link\" aria-label=\"Direct link to Data Ingestion Issues\" title=\"Direct link to Data Ingestion Issues\" translate=\"no\">​</a></h3>\n<p>Prior to 2023, Upstox relied on no-code ingestion platforms like Hevo. While easy to adopt, these platforms introduced several limitations, including high licensing costs and a lack of fine-grained control over ingestion logic. File-level upserts required complex joins between incoming CDC (change data capture) datasets and target tables. Additionally, a lack of atomicity often led to inconsistent data writes, and small-file issues were rampant. To combat these problems, the team had to implement time-consuming re-partitioning and coalescing, along with complex salting strategies to distribute data evenly.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"downstream-consumption-struggles\">Downstream Consumption Struggles<a href=\"https://hudi.apache.org/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless#downstream-consumption-struggles\" class=\"hash-link\" aria-label=\"Direct link to Downstream Consumption Struggles\" title=\"Direct link to Downstream Consumption Struggles\" translate=\"no\">​</a></h3>\n<p>Analytics queries were primarily served through Amazon Athena, which presented several key limitations. For instance, it frequently timed out when querying large datasets and often exceeded the maximum number of partitions it could handle. Additionally, Athena's lack of support for stored procedures made it challenging to manage and reuse complex query logic. Attempts to improve performance with bucketing often created more small files, and the lack of native support for incremental queries further complicated their analytics workflow.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"the-modern-lakehouse-architecture\">The Modern Lakehouse Architecture<a href=\"https://hudi.apache.org/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless#the-modern-lakehouse-architecture\" class=\"hash-link\" aria-label=\"Direct link to The Modern Lakehouse Architecture\" title=\"Direct link to The Modern Lakehouse Architecture\" translate=\"no\">​</a></h2>\n<p><img decoding=\"async\" loading=\"lazy\" src=\"https://hudi.apache.org/assets/images/fig2-a2a9161b7ad75628a36b03514ae4a9c4.png\" width=\"1934\" height=\"1016\" class=\"img_ev3q\"></p>\n<p>To tackle these problems, Upstox implemented a medallion architecture, organizing data into bronze, silver, and gold layers:</p>\n<ul>\n<li class=\"\"><strong>Bronze (Raw Data):</strong> Data is ingested and stored in its raw format as Parquet files.</li>\n<li class=\"\"><strong>Silver (Cleaned and Filtered):</strong> Data is cleaned, filtered, and stored in Apache Hudi tables, which are updated incrementally.</li>\n<li class=\"\"><strong>Gold (Business-Ready):</strong> Data is aggregated for specific business use cases, modeled with dbt, and stored in Hudi.</li>\n</ul>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"the-solution-a-modern-stack-with-hudi-dbt-and-emr-serverless\">The Solution: A Modern Stack with Hudi, dbt, and EMR Serverless<a href=\"https://hudi.apache.org/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless#the-solution-a-modern-stack-with-hudi-dbt-and-emr-serverless\" class=\"hash-link\" aria-label=\"Direct link to The Solution: A Modern Stack with Hudi, dbt, and EMR Serverless\" title=\"Direct link to The Solution: A Modern Stack with Hudi, dbt, and EMR Serverless\" translate=\"no\">​</a></h3>\n<p>Upstox re-architected its platform using Apache Hudi as the core data lake technology, dbt for transformations, and EMR Serverless for scalable compute. Airflow was used to orchestrate the entire workflow. Here's how this new stack addressed their challenges:</p>\n<p><strong>Simplified Data Updates:</strong> Hudi provides built-in support for record-level upserts with atomic guarantees and snapshot isolation. This helped Upstox overcome the challenge of ensuring consistent updates to their fact and dimension tables.</p>\n<p><strong>Improved Upsert Performance:</strong> To optimize upsert performance, the team leveraged Bloom index, especially for transaction-heavy fact tables. Indexing strategies were chosen based on data characteristics to balance latency and efficiency.</p>\n<p><strong>Resolved Small-File Issues:</strong> Small files, which are common in streaming workloads, were mitigated using clustering jobs supported by Hudi. This process was scheduled to run weekly and ensured efficient file sizes and reduced storage overhead without manual intervention.</p>\n<p><strong>Enabled Incremental Processing:</strong> Incremental joins allowed Upstox to process only new data daily. This enabled timely updates to the aggregated tables in the gold layer that power user-facing dashboards—a task that was not feasible with traditional Athena queries.</p>\n<p><strong>Managed Metadata Growth:</strong> The accumulation of commit and metadata files in the Hudi table’s `.hoodie/` directory increased S3 listing costs and slowed down operations. Hudi's archival feature helped manage this by archiving older commits after a certain threshold, keeping metadata lean and efficient.</p>\n<p><strong>Streamlined Data Modeling:</strong> The team used dbt on EMR Serverless to create materialized views over the Hudi datasets. This enabled the creation of efficient transformation layers (silver and gold) using familiar SQL workflows and managed compute.</p>\n<p><strong>Flexible Data Materialization:</strong> dbt supported a variety of model types, including tables, views, and ephemeral models (Common Table Expressions, or CTEs). This gave teams the flexibility to optimize for performance, reuse, or simplicity, depending on the use case.</p>\n<p><strong>Out-of-the-Box Lineage and Documentation:</strong> dbt helps visualize how data flows from one table to another, making it easier to debug and understand dependencies. The glossary feature allows teams to document column meanings and transformations clearly.</p>\n<p><strong>Enforced Data Quality:</strong> With dbt, specific data quality rules can be added to individual tables or pipelines. This adds an extra layer of validation beyond the basic checks performed during data ingestion.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"cicd-and-orchestration\">CI/CD and Orchestration<a href=\"https://hudi.apache.org/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless#cicd-and-orchestration\" class=\"hash-link\" aria-label=\"Direct link to CI/CD and Orchestration\" title=\"Direct link to CI/CD and Orchestration\" translate=\"no\">​</a></h3>\n<p><img decoding=\"async\" loading=\"lazy\" src=\"https://hudi.apache.org/assets/images/fig3-3a9b031ca307c28c17434af09a0ee7bc.png\" width=\"1932\" height=\"882\" class=\"img_ev3q\"></p>\n<p>Upstox uses Apache Airflow for orchestration, with dbt pipelines deployed via a Git-based CI/CD process. Merging a pull request in GitLab triggers the CI/CD pipeline, which automatically builds a new dbt image and publishes the updated data catalog. Airflow then runs the corresponding dbt jobs daily or on-demand, automating the entire transformation workflow.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"the-impact\">The Impact<a href=\"https://hudi.apache.org/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless#the-impact\" class=\"hash-link\" aria-label=\"Direct link to The Impact\" title=\"Direct link to The Impact\" translate=\"no\">​</a></h3>\n<p>The adoption of this modern data stack had a significant impact on Upstox's data platform. The company achieved extremely high data availability and consistency for critical datasets, reducing SLA breaches for complex joins by 70%. Furthermore, pipeline costs dropped by 40%, and query performance improved drastically thanks to Hudi's clustering and optimized joins.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"conclusion\">Conclusion<a href=\"https://hudi.apache.org/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless#conclusion\" class=\"hash-link\" aria-label=\"Direct link to Conclusion\" title=\"Direct link to Conclusion\" translate=\"no\">​</a></h2>\n<p>By leveraging Apache Hudi, dbt, and EMR Serverless, Upstox built a robust and cost-efficient data platform to serve its 12M+ customers, overcoming the significant challenges of data ingestion and analytics at scale. This transformation resolved critical issues like inconsistent data writes, small-file problems, and query timeouts, leading to tangible improvements in both performance and efficiency. With a 70% reduction in SLA breaches and a 40% drop in pipeline costs, the new architecture has empowered their BI and ML teams to move faster. Ultimately, this success story demonstrates how a modern data stack can not only solve immediate technical bottlenecks but also lay the groundwork for a scalable, self-service future that enables continued innovation.</p>",
            "url": "https://hudi.apache.org/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless",
            "title": "Modernizing Upstox's Data Platform with Apache Hudi, dbt, and EMR Serverless",
            "summary": "Introduction",
            "date_modified": "2025-10-16T00:00:00.000Z",
            "author": {
                "name": "The Hudi Community"
            },
            "tags": [
                "hudi",
                "upstox",
                "dbt",
                "data lakehouse"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph",
            "content_html": "<p><a href=\"https://www.crowdstrike.com/en-us/global-threat-report/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">CrowdStrike’s 2025 Global Threat Report</a> puts average eCrime breakout time at 48 minutes, with the fastest at 51 seconds. This means that by the time security teams are even alerted about the potential breach, attackers have already long infiltrated the system. And that’s assuming they even get alerted. Cloud environments generate massive amounts of access logs, configuration changes, alerts, and telemetry. Reviewing these events in isolation rarely surfaces patterns like lateral movement or privilege escalation.</p>\n<p>Security tools such as SIEM, CSPM, and cloud workload protection need relationship-based analysis. It is not only a login attempt or a policy change, but also who acted, which systems were touched, what privileges were active, and what happened next. Event-centric methods struggle to answer those questions at scale. Graph analysis fits better because it captures paths and context across entities.</p>\n<p>To keep up, the data pipeline must support:</p>\n<ul>\n<li class=\"\">Continuous upserts with low lag so detections run on the latest state</li>\n<li class=\"\">Incremental consumption so analytics read only “what changed since T”</li>\n<li class=\"\">A rewindable timeline so responders can review state during investigations</li>\n</ul>\n<p>With Apache Hudi and PuppyGraph, this becomes straightforward. Hudi tables support fast upserts and incremental processing. PuppyGraph queries relationships in place using openCypher or Gremlin. In this blog, we explore how to get started with real-time security graph analytics at scale using the data already stored in your Hudi lakehouse tables.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"why-apache-hudi-for-cybersecurity-data\">Why Apache Hudi for Cybersecurity Data?<a href=\"https://hudi.apache.org/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph#why-apache-hudi-for-cybersecurity-data\" class=\"hash-link\" aria-label=\"Direct link to Why Apache Hudi for Cybersecurity Data?\" title=\"Direct link to Why Apache Hudi for Cybersecurity Data?\" translate=\"no\">​</a></h2>\n<p><a href=\"https://hudi.apache.org/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">Apache Hudi</a> is an open data lakehouse platform that brings ACID transaction guarantees to data lakes. It enables efficient, record-level updates and deletes on massive datasets, which makes it a strong foundation for storing and analyzing cybersecurity data such as logs, telemetry, and threat intelligence. Its combination of performance, flexibility, and broad ecosystem integration is well-suited for threat detection, forensic investigation, and compliance work.</p>\n<p>Hudi speeds up large-scale security analytics through features that keep tables both current and query-efficient. Hudi writers excel at handling continuous, mutable workloads without requiring costly full rewrites. Hudi’s <a href=\"https://hudi.apache.org/docs/metadata\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">multi-modal indexing subsystem</a>, backed by its internal metadata table, offers efficient lookups and data skipping, dramatically accelerating queries that scan massive log sets to isolate suspicious activity. Hudi keeps tables updatable and queryable as they change, with time-travel and incremental reads for point-in-time forensic analysis.</p>\n<p>Even as data volumes grow, operations remain manageable. Hudi tracks every commit on a timeline, enabling powerful time-travel queries for historical investigations. Asynchronous table services like <a href=\"https://hudi.apache.org/docs/compaction\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">compaction</a>, <a href=\"https://hudi.apache.org/docs/clustering\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">clustering</a>, <a href=\"https://hudi.apache.org/docs/cleaning\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">cleaning</a>, and <a href=\"https://hudi.apache.org/docs/metadata_indexing\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">indexing</a> run in the background to maintain peak performance and storage health while minimizing disruption to ingestion pipelines. Furthermore, its consistent commit and delete semantics support the creation of reliable audit trails, simplify data retention policies, and help meet privacy requirements.</p>\n<figure><p><img decoding=\"async\" loading=\"lazy\" src=\"https://hudi.apache.org/assets/images/hudi-stack-1-x-7ac3c524e79ef6771783245fef6ce062.png\" width=\"1333\" height=\"978\" class=\"img_ev3q\">\n</p><figcaption>The <a href=\"https://hudi.apache.org/docs/hudi_stack\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">Apache Hudi Stack</a></figcaption><p></p></figure>\n<p>Hudi also integrates seamlessly with the tools security teams already use. You can stream data from Apache Kafka or Debezium CDC into Hudi, register tables in Hive Metastore or AWS Glue Catalog, and query them from popular query engines like Apache Spark, Apache Flink, Presto, Trino, or Amazon Athena. PuppyGraph connects to the same Hudi tables and runs openCypher or Gremlin queries directly on them via the user access layer, so you get real-time graph analytics on the lake with no ETL and no data duplication.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"why-puppygraph-for-cybersecurity-data\">Why PuppyGraph for Cybersecurity Data?<a href=\"https://hudi.apache.org/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph#why-puppygraph-for-cybersecurity-data\" class=\"hash-link\" aria-label=\"Direct link to Why PuppyGraph for Cybersecurity Data?\" title=\"Direct link to Why PuppyGraph for Cybersecurity Data?\" translate=\"no\">​</a></h2>\n<p><a href=\"https://puppygraph.com/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">PuppyGraph</a> is the first real-time, zero-ETL graph query engine. It lets data teams query existing relational stores as a single graph and get up and running in under 10 minutes, avoiding the cost, latency, and maintenance of a separate graph database. To understand why this is so important, let’s take a look at the status quo.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"traditional-analytics-on-the-lake\">Traditional Analytics on the Lake<a href=\"https://hudi.apache.org/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph#traditional-analytics-on-the-lake\" class=\"hash-link\" aria-label=\"Direct link to Traditional Analytics on the Lake\" title=\"Direct link to Traditional Analytics on the Lake\" translate=\"no\">​</a></h3>\n<p>Security teams already store logs, configs, and alerts in a lakehouse. SQL engines are great for counts, filters, rollups, and point lookups. They struggle when questions depend on relationships. Lateral movement, privilege escalation, and blast radius span many tables and time windows. Each new join adds complexity, pushes latency up, and breaks easily when schemas evolve or events arrive late. You can stitch context with views and pipelines, but it is fragile and slow to adapt.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"dedicated-graph-databases\">Dedicated Graph Databases<a href=\"https://hudi.apache.org/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph#dedicated-graph-databases\" class=\"hash-link\" aria-label=\"Direct link to Dedicated Graph Databases\" title=\"Direct link to Dedicated Graph Databases\" translate=\"no\">​</a></h3>\n<p>Graphs make paths and neighborhoods first class. Graph queries let you answer “what connects to what” in a way that makes sense, without the need for confusing data joins. The tradeoff is operations and freshness. Most graph databases want their own storage. That means ETL, a second copy, and lag between source and graph. Continuous upserts are heavy because every change can touch nodes, edges, and multiple indexes. Running a separate cluster adds backups, upgrades, sizing, and vendor-specific tuning. During an incident, that overhead shows up as stale data and slower investigations.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"how-puppygraph-helps\">How PuppyGraph Helps<a href=\"https://hudi.apache.org/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph#how-puppygraph-helps\" class=\"hash-link\" aria-label=\"Direct link to How PuppyGraph Helps\" title=\"Direct link to How PuppyGraph Helps\" translate=\"no\">​</a></h3>\n<p>PuppyGraph is not a traditional graph database but a graph query engine designed to run directly on top of your existing data infrastructure without costly and complex ETL (Extract, Transform, Load) processes. This \"zero-ETL\" approach is its core differentiator, allowing you to query relational data in data warehouses, data lakes, and databases as a unified graph model in minutes.</p>\n<p>Instead of migrating data into a specialized store, PuppyGraph connects to sources including <a href=\"https://www.puppygraph.com/blog/postgresql-graph-database\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">PostgreSQL</a>, <a href=\"https://docs.puppygraph.com/connecting/connecting-to-iceberg/?h=ice\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">Apache Iceberg</a>, <a href=\"https://docs.puppygraph.com/connecting/connecting-to-apache-hudi/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">Apache Hudi</a>, <a href=\"https://docs.puppygraph.com/connecting/connecting-to-bigquery/?h=bigq\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">BigQuery</a>, and others, then builds a virtual graph layer over them. Graph models are defined through simple JSON schema files, making it easy to update, version, or switch graph views without touching the underlying data. From there, you can quickly begin exploring your data with graph queries written in Gremlin or openCypher.</p>\n<figure><p><img decoding=\"async\" loading=\"lazy\" src=\"https://hudi.apache.org/assets/images/fig-2-PuppyGraph_Supported_Data_Sources-530816b1bc98047b2113f8f1fb791b8c.png\" width=\"1313\" height=\"745\" class=\"img_ev3q\">\n</p><figcaption>PuppyGraph Supported Data Sources</figcaption><p></p></figure>\n<figure><p><img decoding=\"async\" loading=\"lazy\" src=\"https://hudi.apache.org/assets/images/fig-3-Architecture-with-Graph-Database-vs-with-PuppyGraph-c6601e01f1e7ae2fcd8ffcc58aec90ac.png\" width=\"1497\" height=\"843\" class=\"img_ev3q\">\n</p><figcaption>Architecture with Graph Database vs. with PuppyGraph</figcaption><p></p></figure>\n<p>This approach aligns with the broader shift in modern data stacks to separate compute from storage. You keep data where it belongs and scale query power independently, which supports petabyte-level workloads without duplicating data or managing fragile pipelines.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"real-world-use-case\">Real-World Use Case<a href=\"https://hudi.apache.org/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph#real-world-use-case\" class=\"hash-link\" aria-label=\"Direct link to Real-World Use Case\" title=\"Direct link to Real-World Use Case\" translate=\"no\">​</a></h2>\n<p>We have shown why cloud security benefits from a relationship-first view of identities, resources, and events. In this demo, we’ll show how easy it is to begin querying your cloud security data as a graph. Apache Hudi keeps those tables current with streaming upserts and an investigation-friendly timeline. PuppyGraph lets you query your existing lake tables as a graph. Together they give you real-time security graph analytics on the data you already store.</p>\n<p>Getting started is straightforward. You will deploy the stack, load security data into Hudi, connect PuppyGraph to your catalog, define a graph view, and run a few queries. All in a matter of minutes.</p>\n<figure><p><img decoding=\"async\" loading=\"lazy\" src=\"https://hudi.apache.org/assets/images/fig-4-Sample-Architecture-of-PuppyGraph-Hudi-adcd64f4ffaf1a8c9b6e13f7d9d07e4a.png\" width=\"858\" height=\"1100\" class=\"img_ev3q\">\n</p><figcaption>Sample Architecture of PuppyGraph + Hudi</figcaption><p></p></figure>\n<p>The components of this demo project include:</p>\n<ul>\n<li class=\"\">Storage: MinIO/S3 – Object store for Hudi data</li>\n<li class=\"\">Data Lakehouse: Apache Hudi – Brings database functionality to your data lakes</li>\n<li class=\"\">Catalog: Hive Metastore – Backed by Postgres</li>\n<li class=\"\">Compute engines:<!-- -->\n<ul>\n<li class=\"\">Spark – Initial table writes</li>\n<li class=\"\">PuppyGraph – Graph query engine for complex, multi-hop graph queries</li>\n</ul>\n</li>\n</ul>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"prerequisites\">Prerequisites<a href=\"https://hudi.apache.org/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph#prerequisites\" class=\"hash-link\" aria-label=\"Direct link to Prerequisites\" title=\"Direct link to Prerequisites\" translate=\"no\">​</a></h3>\n<p>This tutorial assumes that you have the following:</p>\n<ol>\n<li class=\"\"><strong>Docker</strong> and <strong>Docker</strong> <strong>Compose</strong> (for setting up the Docker container)</li>\n<li class=\"\"><strong>Python 3</strong> (for managing dependencies)</li>\n<li class=\"\"><a href=\"https://github.com/puppygraph/puppygraph-getting-started/tree/main/integration-demos/hudi-demo\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">PuppyGraph-Hudi Demo Repository</a></li>\n</ol>\n<h4 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"data-preparation\">Data Preparation<a href=\"https://hudi.apache.org/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph#data-preparation\" class=\"hash-link\" aria-label=\"Direct link to Data Preparation\" title=\"Direct link to Data Preparation\" translate=\"no\">​</a></h4>\n<p>Before we can load our data into our Hudi tables, we need to make sure they’re in the correct file format. Hudi currently supports Parquet and ORC for base files, and we’ll be going with Parquet for this demo:</p>\n<div class=\"language-shell codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-shell codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">python3 -m venv demo</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">source demo/bin/activate</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">pip install -r requirements.txt</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">python3 CsvToParquet.py ./csv_data ./parquet_data</span><br></span></code></pre></div></div>\n<p>Since we’ll be connecting to the Hudi Catalog via the Hive Metastore (HMS), we also have to install the following dependencies:</p>\n<div class=\"language-shell codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-shell codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">mkdir -p lib</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">curl -L -o lib/postgresql-42.5.1.jar \\</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">https://repo1.maven.org/maven2/org/postgresql/postgresql/42.5.1/postgresql-42.5.1.jar</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\" style=\"display:inline-block\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">curl -L -o lib/hadoop-aws-3.3.4.jar \\</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\" style=\"display:inline-block\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">curl -L -o lib/aws-java-sdk-bundle-1.12.262.jar \\</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar</span><br></span></code></pre></div></div>\n<h4 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"loading-data\">Loading Data<a href=\"https://hudi.apache.org/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph#loading-data\" class=\"hash-link\" aria-label=\"Direct link to Loading Data\" title=\"Direct link to Loading Data\" translate=\"no\">​</a></h4>\n<p>With all our dependencies installed and data prepared, we can launch the required services:</p>\n<div class=\"language-shell codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-shell codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">docker compose up -d</span><br></span></code></pre></div></div>\n<p>Once everything is up and running, we can finally populate the tables with our data:</p>\n<div class=\"language-shell codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-shell codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">docker compose exec spark /opt/spark/bin/spark-sql -f /init.sql</span><br></span></code></pre></div></div>\n<h4 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"modeling-the-graph\">Modeling the Graph<a href=\"https://hudi.apache.org/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph#modeling-the-graph\" class=\"hash-link\" aria-label=\"Direct link to Modeling the Graph\" title=\"Direct link to Modeling the Graph\" translate=\"no\">​</a></h4>\n<p>Now that our data is loaded in, we can log into the PuppyGraph Web UI at <a href=\"http://localhost:8081/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">http://localhost:8081</a> with the default credentials (username: puppygraph, password: puppygraph123)</p>\n<figure><p><img decoding=\"async\" loading=\"lazy\" src=\"https://hudi.apache.org/assets/images/fig-5-PuppyGraph-Login-Page-c4b9d16af5789a65c1f796e0411dfd40.png\" width=\"3024\" height=\"1710\" class=\"img_ev3q\">\n</p><figcaption>PuppyGraph Login Page</figcaption><p></p></figure>\n<p>To model your data as a graph, you can simply select the file `schema.json` in the Upload Graph Schema JSON section and click on Upload.</p>\n<figure><p><img decoding=\"async\" loading=\"lazy\" src=\"https://hudi.apache.org/assets/images/fig-6-Schema-Page-in-PuppyGraph-UI-ff0eb0aac444ad00b098145a9c84fe69.png\" width=\"1600\" height=\"907\" class=\"img_ev3q\">\n</p><figcaption>Schema Page in PuppyGraph UI</figcaption><p></p></figure>\n<p>Once you see your graph schema loaded in, you’re ready to start querying your data as a graph.</p>\n<figure><p><img decoding=\"async\" loading=\"lazy\" src=\"https://hudi.apache.org/assets/images/fig-7-Loaded-Schema-in-PuppyGraph-UI-b32e7eb53b88d6290b5f463308445bfa.png\" width=\"1600\" height=\"907\" class=\"img_ev3q\">\n</p><figcaption>Loaded Schema in PuppyGraph UI</figcaption><p></p></figure>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"sample-queries\">Sample Queries<a href=\"https://hudi.apache.org/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph#sample-queries\" class=\"hash-link\" aria-label=\"Direct link to Sample Queries\" title=\"Direct link to Sample Queries\" translate=\"no\">​</a></h3>\n<p>By modeling the network infrastructure as a graph, users can identify potential security risks, such as:</p>\n<ul>\n<li class=\"\">Public IP addresses exposed to the internet</li>\n<li class=\"\">Network interfaces not protected by any security group</li>\n<li class=\"\">Roles granted excessive access permissions</li>\n<li class=\"\">Security groups with overly permissive ingress rules</li>\n</ul>\n<p>Listed below are some sample queries you can try running to explore the data:</p>\n<ol>\n<li class=\"\">Tracing Admin Access Paths from Users to Internet Gateways</li>\n</ol>\n<div class=\"language-javascript codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-javascript codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">g</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token constant\" style=\"color:rgb(189, 147, 249)\">V</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">hasLabel</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'User'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword module\" style=\"color:rgb(189, 147, 249);font-style:italic\">as</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'user'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">outE</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'ACCESS'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">has</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'access_level'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'admin'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword module\" style=\"color:rgb(189, 147, 249);font-style:italic\">as</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'edge'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">inV</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">path</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><br></span></code></pre></div></div>\n<p><img decoding=\"async\" loading=\"lazy\" src=\"https://hudi.apache.org/assets/images/fig-8-PuppyGraph-Query-1-9d56715c417b1f35667d91d153df9547.png\" width=\"1600\" height=\"907\" class=\"img_ev3q\"></p>\n<ol start=\"2\">\n<li class=\"\">Find all public IP addresses exposed to the internet, along with their associated virtual machine instances, security groups, subnets, VPCs, internet gateways, and users, displaying all these entities in the traversal path.</li>\n</ol>\n<div class=\"language-javascript codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-javascript codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"> g</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token constant\" style=\"color:rgb(189, 147, 249)\">V</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">hasLabel</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'PublicIP'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword module\" style=\"color:rgb(189, 147, 249);font-style:italic\">as</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'ip'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">in</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'HAS_PUBLIC_IP'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword module\" style=\"color:rgb(189, 147, 249);font-style:italic\">as</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'ni'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">in</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'PROTECTS'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">hasLabel</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'SecurityGroup'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword module\" style=\"color:rgb(189, 147, 249);font-style:italic\">as</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'sg'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">out</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'HAS_RULE'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">hasLabel</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'IngressRule'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword module\" style=\"color:rgb(189, 147, 249);font-style:italic\">as</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'rule'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">where</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">   __</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">out</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'ALLOWS_TRAFFIC_FROM'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">hasLabel</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'InternetGateway'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">select</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'ni'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">   </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">out</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'ATTACHED_TO'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">hasLabel</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'VMInstance'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword module\" style=\"color:rgb(189, 147, 249);font-style:italic\">as</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'vm'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">select</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'ni'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">   </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">in</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'HOSTS_INTERFACE'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">hasLabel</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'Subnet'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword module\" style=\"color:rgb(189, 147, 249);font-style:italic\">as</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'subnet'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">   </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">in</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'CONTAINS'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">hasLabel</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'VPC'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword module\" style=\"color:rgb(189, 147, 249);font-style:italic\">as</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'vpc'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">   </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">in</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'GATEWAY_TO'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">hasLabel</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'InternetGateway'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword module\" style=\"color:rgb(189, 147, 249);font-style:italic\">as</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'igw'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">   </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">in</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'ACCESS'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">hasLabel</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'User'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword module\" style=\"color:rgb(189, 147, 249);font-style:italic\">as</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'user'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">path</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">limit</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token number\">1000</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><br></span></code></pre></div></div>\n<p><img decoding=\"async\" loading=\"lazy\" src=\"https://hudi.apache.org/assets/images/fig-9-PuppyGraph-Query-2-59da47f7532f00f65a89d7ac108865a8.png\" width=\"1600\" height=\"907\" class=\"img_ev3q\"></p>\n<ol start=\"3\">\n<li class=\"\">Find roles that have been granted excessive access permissions, along with their associated virtual machine instances.</li>\n</ol>\n<div class=\"language-javascript codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-javascript codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">g</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token constant\" style=\"color:rgb(189, 147, 249)\">V</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">hasLabel</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'Role'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword module\" style=\"color:rgb(189, 147, 249);font-style:italic\">as</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'role'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">where</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  __</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">out</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'ALLOWS_ACCESS_TO'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">count</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">is</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token function\" style=\"color:rgb(80, 250, 123)\">gt</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\">4L</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">out</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'ALLOWS_ACCESS_TO'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">hasLabel</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'Resource'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword module\" style=\"color:rgb(189, 147, 249);font-style:italic\">as</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'resource'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">select</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'role'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">in</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'ASSIGNED_ROLE'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">hasLabel</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'VMInstance'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword module\" style=\"color:rgb(189, 147, 249);font-style:italic\">as</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'vm'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">path</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><br></span></code></pre></div></div>\n<p><img decoding=\"async\" loading=\"lazy\" src=\"https://hudi.apache.org/assets/images/fig-10-PuppyGraph-Query-3-c324707053b02056cc824229fd1d1ac0.png\" width=\"1600\" height=\"907\" class=\"img_ev3q\"></p>\n<ol start=\"4\">\n<li class=\"\">Find security groups that have ingress rules permitting traffic from any IP address (0.0.0.0/0) to sensitive ports (22 or 3389), and retrieve the associated ingress rules, network interfaces, and virtual machine instances in the traversal path.</li>\n</ol>\n<div class=\"language-javascript codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-javascript codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">g</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token constant\" style=\"color:rgb(189, 147, 249)\">V</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">hasLabel</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'SecurityGroup'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword module\" style=\"color:rgb(189, 147, 249);font-style:italic\">as</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'sg'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">out</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'HAS_RULE'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">   </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">has</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'source'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'0.0.0.0/0'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">   </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">has</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'port_range'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token constant\" style=\"color:rgb(189, 147, 249)\">P</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">within</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'22'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'3389'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">   </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">hasLabel</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'IngressRule'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword module\" style=\"color:rgb(189, 147, 249);font-style:italic\">as</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'rule'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">in</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'HAS_RULE'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword module\" style=\"color:rgb(189, 147, 249);font-style:italic\">as</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'sg'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">out</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'PROTECTS'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">hasLabel</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'NetworkInterface'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword module\" style=\"color:rgb(189, 147, 249);font-style:italic\">as</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'ni'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">out</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'ATTACHED_TO'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">hasLabel</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'VMInstance'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token keyword module\" style=\"color:rgb(189, 147, 249);font-style:italic\">as</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'vm'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">.</span><span class=\"token method function property-access\" style=\"color:rgb(80, 250, 123)\">path</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><br></span></code></pre></div></div>\n<p><img decoding=\"async\" loading=\"lazy\" src=\"https://hudi.apache.org/assets/images/fig-11-PuppyGraph-Query-4-66848df36086049fd3ac42b78a5de47c.png\" width=\"1600\" height=\"907\" class=\"img_ev3q\"></p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"conclusion\">Conclusion<a href=\"https://hudi.apache.org/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph#conclusion\" class=\"hash-link\" aria-label=\"Direct link to Conclusion\" title=\"Direct link to Conclusion\" translate=\"no\">​</a></h2>\n<p>Real-time security work comes down to two needs: fresh tables and connected questions. Apache Hudi keeps lakehouse data current with streaming upserts, incremental reads, and a rewindable timeline. PuppyGraph reads those same tables in place and runs multi-hop graph queries with openCypher or Gremlin. One data copy. No ETL.</p>\n<p>The result is faster investigations and clearer decisions. You can trace attack paths, size blast radius, and correlate alerts to recent changes while keeping governance and access controls in a single lake. When you need to look back, time travel gives you point-in-time views without rebuilding pipelines.</p>",
            "url": "https://hudi.apache.org/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph",
            "title": "Real-Time Cloud Security Graphs with Apache Hudi and PuppyGraph",
            "summary": "CrowdStrike’s 2025 Global Threat Report puts average eCrime breakout time at 48 minutes, with the fastest at 51 seconds. This means that by the time security teams are even alerted about the potential breach, attackers have already long infiltrated the system. And that’s assuming they even get alerted. Cloud environments generate massive amounts of access logs, configuration changes, alerts, and telemetry. Reviewing these events in isolation rarely surfaces patterns like lateral movement or privilege escalation.",
            "date_modified": "2025-10-02T00:00:00.000Z",
            "author": {
                "name": "Jaz Samantha Ku, in collaboration with Shiyan Xu"
            },
            "tags": [
                "Apache Hudi",
                "PuppyGraph",
                "security"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2025/09/17/hudi-auto-gen-keys",
            "content_html": "<p>In database systems, the primary key is a foundational design principle for managing data at the record level. Its function is to provide each record with a unique and stable logical identifier, which decouples the record's identity from its physical location on storage. While using direct physical address pointers (e.g., position inside a file being used as a key) can be convenient, the physical address can change when records are moved around within the table for things like clustering or z-ordering (<a href=\"https://x.com/apachehudi/status/1641572485325017089\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">called out here</a>).</p>\n<p>By using a primary key that is stable across record movement, a system can efficiently perform operations like updates and deletes, enabling critical features like relational integrity.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"first-class-support-of-record-keys\">First-Class Support of Record Keys<a href=\"https://hudi.apache.org/blog/2025/09/17/hudi-auto-gen-keys#first-class-support-of-record-keys\" class=\"hash-link\" aria-label=\"Direct link to First-Class Support of Record Keys\" title=\"Direct link to First-Class Support of Record Keys\" translate=\"no\">​</a></h2>\n<p>Apache Hudi was the first lakehouse storage project to introduce the notion of record keys. For mutable workloads, this addressed a significant architectural challenge. In a typical data lake table, updating records usually required rewriting entire partitions—a process that is slow and expensive. By supporting the record key as the stable identifier for every record, Hudi offered unique and advanced capabilities among lakehouse frameworks:</p>\n<ul>\n<li class=\"\">Hudi supports <a href=\"https://hudi.apache.org/blog/2023/11/01/record-level-index/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">record-level indexing</a> for directly locating records in <a href=\"https://hudi.apache.org/docs/storage_layouts\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">file groups</a> for highly efficient upserts and queries, and <a href=\"https://hudi.apache.org/blog/2025/04/02/secondary-index/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">secondary indexes</a> that enable performant lookups for predicates on non-record key fields.</li>\n<li class=\"\">Hudi implements <a href=\"https://hudi.apache.org/blog/2025/03/03/record-mergers-in-hudi/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">merge modes</a>, standardizing record-merging semantics to handle requirements such as unordered events, duplicate records, and custom merge logic.</li>\n<li class=\"\">By materializing record keys along with other <a href=\"https://www.onehouse.ai/blog/hudi-metafields-demystified\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">record-level meta-fields</a>, Hudi unlocks features such as efficient <a href=\"https://hudi.apache.org/blog/2024/07/30/data-lake-cdc/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">change data capture (CDC)</a> that serves record-level change streams, near-infinite history for time-travel queries, and the <a href=\"https://hudi.apache.org/docs/clustering\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">clustering table service</a> that can significantly optimize file sizes.</li>\n</ul>\n<figure><p><img decoding=\"async\" loading=\"lazy\" src=\"https://hudi.apache.org/assets/images/2025-09-17-hudi-auto-gen-keys.fig1-fb5004b3f1cd1832795f39f6c7255411.jpg\" width=\"647\" height=\"351\" class=\"img_ev3q\">\n</p><figcaption>Replicating operational databases to a Hudi lakehouse using CDC</figcaption><p></p></figure>\n<p>Append-only writes are very common in the data lakehouse, such as ingesting application logs streamed continuously from numerous servers or capturing clickstream events from user interactions on a website. Even for this kind of scenario, having record keys is beneficial in scenarios like concurrently running data-fixing backfill writers (e.g., a GDPR deletion process) with ongoing writers to the same table. Without record keys, engineers typically had to coordinate the backfill to run on different partitions than the active writes to avoid conflicts. With record keys and the support provided by Hudi’s <a href=\"https://hudi.apache.org/docs/concurrency_control\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">concurrency control</a> and merge modes, this restriction can be lifted, with Hudi handling the concurrent writes properly.</p>\n<p>Given the advantages of supporting record keys, Hudi required users to set one or multiple record key fields when creating a table prior to <a href=\"https://hudi.apache.org/releases/release-0.14.0\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">release 0.14</a>. However, this requirement created friction for users in cases where there were no natural record keys in the incoming stream for simply setting another config variable. Even for users who understood the benefits of record keys, they had to put careful thought into their record key generation to ensure uniqueness and idempotency. The initial friction of generating keys was a barrier to adoption for teams who simply wanted to land their append-only workloads in a lakehouse with as few lines of code and configuration as possible.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"automatic-key-generation\">Automatic Key Generation<a href=\"https://hudi.apache.org/blog/2025/09/17/hudi-auto-gen-keys#automatic-key-generation\" class=\"hash-link\" aria-label=\"Direct link to Automatic Key Generation\" title=\"Direct link to Automatic Key Generation\" translate=\"no\">​</a></h2>\n<p>With the release of version 0.14 (this is actually old news), Hudi has introduced automatic record key generation, a feature designed to simplify the user experience with append-only writes. This enhancement eliminates the mandatory requirement to specify record key fields for every write operation.</p>\n<figure><p><img decoding=\"async\" loading=\"lazy\" src=\"https://hudi.apache.org/assets/images/2025-09-17-hudi-auto-gen-keys.fig2-760500605f2a1ecfa253caffaa013c4a.jpg\" width=\"639\" height=\"316\" class=\"img_ev3q\">\n</p><figcaption>Hudi's auto key generation for append-only writes</figcaption><p></p></figure>\n<p>Now, to perform append-only writes, you can simply omit the <code>primaryKey</code> property in <code>CREATE TABLE</code> statements (see the example below) or skip setting the <code>hoodie.datasource.write.recordkey.field</code> or <code>hoodie.table.recordkey.fields</code> configurations.</p>\n<div class=\"language-sql codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-sql codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">CREATE</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">TABLE</span><span class=\"token plain\"> hudi_table </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    ts </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">BIGINT</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    uuid STRING</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    rider STRING</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    driver STRING</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    fare </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">DOUBLE</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    city STRING</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">USING</span><span class=\"token plain\"> HUDI</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">PARTITIONED </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">BY</span><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\">city</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><br></span></code></pre></div></div>\n<p>In this example, you’re creating a Copy-on-Write table partitioned by <code>city</code>. Because the <code>primaryKey</code> property is not present, Hudi automatically detects the omission and engages the auto key generation feature.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"design-considerations\">Design Considerations<a href=\"https://hudi.apache.org/blog/2025/09/17/hudi-auto-gen-keys#design-considerations\" class=\"hash-link\" aria-label=\"Direct link to Design Considerations\" title=\"Direct link to Design Considerations\" translate=\"no\">​</a></h3>\n<p>Designing a key generation mechanism that operates efficiently at petabyte scale requires careful thought. We established five core requirements for the auto-generated keys:</p>\n<ol>\n<li class=\"\"><strong>Global Uniqueness:</strong> Keys must be unique across the entire table to maintain the integrity of a primary key.</li>\n<li class=\"\"><strong>Low Storage Footprint:</strong> The keys should be highly compressible to add minimal storage overhead.</li>\n<li class=\"\"><strong>Computational Efficiency:</strong> The encoding and decoding process must be lightweight so as not to slow down the write process.</li>\n<li class=\"\"><strong>Idempotency:</strong> The generation process must be resilient to task retries, producing the same key for the same record every time.</li>\n<li class=\"\"><strong>Engine Agnostic:</strong> The logic must be reusable and implemented consistently across different execution engines like Spark and Flink.</li>\n</ol>\n<p>These principles guided the technical design. To align with primary key semantics, global uniqueness was non-negotiable. To minimize storage footprint, the generated keys needed to be compact and highly compressible, especially for tables with billions of records. The computational cost was also critical; any expensive operation would be amplified by the number of records, creating a significant performance overhead. Furthermore, in distributed systems where task failures and retries are common, the key generation process had to be idempotent—ensuring the same input record always produces the exact same key. Finally, the solution needed to be engine-agnostic to provide consistent behavior, whether data is written via Spark, Flink, or another supported engine.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"determining-the-format\">Determining the Format<a href=\"https://hudi.apache.org/blog/2025/09/17/hudi-auto-gen-keys#determining-the-format\" class=\"hash-link\" aria-label=\"Direct link to Determining the Format\" title=\"Direct link to Determining the Format\" translate=\"no\">​</a></h3>\n<p>Based on the requirements mentioned previously, we eliminated several common ID generation techniques. For instance, we cannot use simple auto-incrementing IDs for each batch of writes, as it will not satisfy global uniqueness in the table across different writes. We also rule out using the <code>monotonically_increasing_id</code> function in Spark, as it does not guarantee global uniqueness either. Furthermore, using such functions violates the rule of being engine-agnostic. We do not use random ID generation such as UUID (v4, v6, and v7) and ULID, which do not satisfy the idempotency requirement. The final format that we chose is a deterministic, composite key with the following structure:</p>\n<div class=\"language-text codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-text codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">&lt;write action start time&gt;-&lt;workload partition ID&gt;-&lt;record sequence ID&gt;</span><br></span></code></pre></div></div>\n<p>Each component serves a specific purpose:</p>\n<ul>\n<li class=\"\"><strong>Write Action Start Time:</strong> The timestamp from the Hudi timeline that marks the beginning of a write transaction.</li>\n<li class=\"\"><strong>Workload Partition ID:</strong> An internal identifier that execution engines use to track the specific data split being processed by a given distributed write task.</li>\n<li class=\"\"><strong>Record Sequence ID:</strong> A counter that uniquely identifies each record within that data split.</li>\n</ul>\n<p>Together, these three components—all readily accessible during the write process—form a record identifier that satisfies the requirements of global uniqueness, idempotency, and being engine-agnostic.</p>\n<p>Next, we evaluate the generated keys against the requirements of low storage footprint and computational efficiency. The following tables highlight some experiment numbers based on the <a href=\"https://github.com/apache/hudi/blob/master/rfc/rfc-76/rfc-76.md\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">RFC document</a> of the auto key generation feature.</p>\n<p>For storage efficiency, we compare the original strings with UUID v6/7, Base64, and ASCII encoding schemes:</p>\n<table><thead><tr><th style=\"text-align:left\">Format</th><th style=\"text-align:left\">Uncompressed size (bytes)</th><th style=\"text-align:left\">Compressed size (bytes)</th><th style=\"text-align:left\">Compression ratio</th></tr></thead><tbody><tr><td style=\"text-align:left\">Original string</td><td style=\"text-align:left\">4,000,185</td><td style=\"text-align:left\">244,373</td><td style=\"text-align:left\">11.1</td></tr><tr><td style=\"text-align:left\">UUID v6/7</td><td style=\"text-align:left\">4,000,184</td><td style=\"text-align:left\">1,451,897</td><td style=\"text-align:left\">2.74</td></tr><tr><td style=\"text-align:left\">Base64</td><td style=\"text-align:left\">2,400,184</td><td style=\"text-align:left\">202,095</td><td style=\"text-align:left\">11.9</td></tr><tr><td style=\"text-align:left\">ASCII</td><td style=\"text-align:left\">1,900,185</td><td style=\"text-align:left\">176,606</td><td style=\"text-align:left\">10.8</td></tr></tbody></table>\n<p>We also compare their compute efficiency using the original string format as the baseline:</p>\n<table><thead><tr><th style=\"text-align:left\">Format</th><th style=\"text-align:left\">Average runtime (ms)</th><th style=\"text-align:left\">Ratio to baseline</th></tr></thead><tbody><tr><td style=\"text-align:left\">Original string</td><td style=\"text-align:left\">0.00001</td><td style=\"text-align:left\">1</td></tr><tr><td style=\"text-align:left\">UUID v6/7</td><td style=\"text-align:left\">0.0001</td><td style=\"text-align:left\">10</td></tr><tr><td style=\"text-align:left\">Base64</td><td style=\"text-align:left\">0.004</td><td style=\"text-align:left\">400</td></tr><tr><td style=\"text-align:left\">ASCII</td><td style=\"text-align:left\">0.004</td><td style=\"text-align:left\">400</td></tr></tbody></table>\n<p>Based on the micro-benchmarking results, UUID v6/7 resulted in a much larger and undesired compressed size compared to others. Base64 and ASCII encoding had a lower storage footprint compared to the original string, with around 17% and 28% reduction respectively. However, both Base64 and ASCII require 400x more CPU power for encoding than the original string format. Given that write performance is often more critical than marginal storage savings in high-throughput data systems, we opted for the original string format for auto-generating record keys.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"summary\">Summary<a href=\"https://hudi.apache.org/blog/2025/09/17/hudi-auto-gen-keys#summary\" class=\"hash-link\" aria-label=\"Direct link to Summary\" title=\"Direct link to Summary\" translate=\"no\">​</a></h2>\n<p>Hudi’s first-class support for record keys provides a database-like experience for lakehouses, enabling powerful features such as record-level indexing, merge modes, and CDC. The introduction of automatic record key generation thoughtfully extends the record key support, removing a barrier for teams performing append-only writes. By following the design principles of uniqueness, idempotency, and efficiency, the feature allows more users to easily adopt Hudi and benefit from its rich set of lakehouse capabilities without the initial overhead of manual key generation. This enhancement reinforces Hudi’s position as a versatile and user-friendly platform for building modern data lakehouses.</p>",
            "url": "https://hudi.apache.org/blog/2025/09/17/hudi-auto-gen-keys",
            "title": "Automatic Record Key Generation in Apache Hudi",
            "summary": "In database systems, the primary key is a foundational design principle for managing data at the record level. Its function is to provide each record with a unique and stable logical identifier, which decouples the record's identity from its physical location on storage. While using direct physical address pointers (e.g., position inside a file being used as a key) can be convenient, the physical address can change when records are moved around within the table for things like clustering or z-ordering (called out here).",
            "date_modified": "2025-09-17T00:00:00.000Z",
            "author": {
                "name": "Shiyan Xu"
            },
            "tags": [
                "hudi",
                "record key generation",
                "database",
                "data lakehouse"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2025/08/29/building-a-rag-based-ai-recommender-2",
            "content_html": "<span>Redirecting... please wait!! <!-- -->or click <a href=\"https://blog.datumagic.ai/p/building-a-rag-based-ai-recommender-147\">here</a></span>",
            "url": "https://hudi.apache.org/blog/2025/08/29/building-a-rag-based-ai-recommender-2",
            "title": "Building a RAG-based AI Recommender (2/2)",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2025-08-29T00:00:00.000Z",
            "author": {
                "name": "Shiyan Xu"
            },
            "tags": [
                "blog",
                "Apache Hudi",
                "AI",
                "RAG",
                "Artificial Intelligence",
                "data lakehouse",
                "Lakehouse",
                "use-case",
                "datumagic"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2025/07/21/mor-comparison",
            "content_html": "<div class=\"theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success\"><div class=\"admonitionHeading_Gvgb\"><span class=\"admonitionIcon_Rf37\"><svg viewBox=\"0 0 12 16\"><path fill-rule=\"evenodd\" d=\"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z\"></path></svg></span>tip</div><div class=\"admonitionContent_BuS1\"><p>TL;DR</p><ul>\n<li class=\"\">Merge-on-Read tables help manage updates on immutable files without constant rewrites.</li>\n<li class=\"\">Apache Hudi’s MoR tables, with delta logs, file groups, asynchronous compaction, and event-time merging, are well-suited for update-heavy, low-latency streaming and CDC workloads.</li>\n<li class=\"\">Iceberg and Delta Lake also support MoR, but with design differences around delete files and deletion vectors.</li>\n</ul></div></div>\n<p>As <a href=\"https://www.onehouse.ai/blog/open-table-formats-and-the-open-data-lakehouse-in-perspective\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">open table formats</a> like Apache Hudi, Apache Iceberg, and Delta Lake become foundational to modern data lakes, understanding how data is written and read becomes critical for designing high-performance pipelines. One such key dimension is the table's write mechanism, specifically, what happens when <em>updates or deletes</em> are made to these lakehouse tables.</p>\n<p>This is where <a href=\"https://hudi.apache.org/docs/table_types#copy-on-write-table\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">Copy-on-Write (CoW)</a> and <a href=\"https://hudi.apache.org/docs/table_types#merge-on-read-table\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">Merge-on-Read (MoR)</a> table types come into play. These terms were popularized by <a href=\"https://hudi.apache.org/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">Apache Hudi</a>, in the <a href=\"https://www.uber.com/blog/hoodie/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">original blog</a> from Uber Engineering, when the project was open-sourced in 2017. These strategies exist to overcome a fundamental limitation: data file formats like Parquet and ORC are immutable in nature. Therefore, any update or delete operation that is executed on these files (managed by a lakehouse table format) requires a specific way to deal with it - either by merging changes right away during writes, rewriting entire files (CoW) or maintaining a differential log or delete index that can  be merged at read time (MoR).</p>\n<p>Viewed through the lens of the <a href=\"https://substack.com/home/post/p-159031300?utm_campaign=post&amp;utm_medium=web\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">RUM Conjecture</a> - which states that optimizing for two of Read, Update, and Memory inevitably requires trading off the third. CoW and MoR emerge as two natural design responses to the trade-offs in lakehouse table formats:</p>\n<ul>\n<li class=\"\">\n<p>Copy-on-Write tables optimize for read performance. They rewrite Parquet files entirely when a change is made, ensuring clean, columnar files with no extra merge logic at query time. This suits batch-style, read-optimized analytics workloads where write frequency is low.</p>\n</li>\n<li class=\"\">\n<p>Merge-on-Read, in contrast, introduces flexibility for write-intensive and latency-sensitive workloads by avoiding expensive writes. Instead of rewriting files for every change, MoR tables store updates in delta logs (Hudi), delete files (Iceberg V2), or deletion vectors (Delta Lake). Reads then stitch together the base data files with these changes to present an up-to-date view. This tradeoff favors streaming or near real-time workloads where low write latency is critical.</p>\n</li>\n</ul>\n<p>Here is a generic comparison table between CoW and MoR tables.</p>\n<table><thead><tr><th>Trade-Off</th><th>CoW</th><th>MoR</th></tr></thead><tbody><tr><td>Write latency</td><td>Higher</td><td>Lower</td></tr><tr><td>Query latency</td><td>Lower</td><td>Higher</td></tr><tr><td>Update cost</td><td>High</td><td>Low</td></tr><tr><td>File Size Guidance</td><td>Base files should be smaller to keep rewrites manageable</td><td>Base files can be larger, as updates don’t rewrite them directly</td></tr><tr><td>Read Amplification</td><td>Minimal - all changes are already materialized into base files</td><td>Higher - readers must combine base files with change logs or metadata (e.g., delete files or vectors)</td></tr><tr><td>Write Amplification</td><td>Higher - changes often rewrite full files, even for small updates</td><td>Lower - only incremental data (e.g., updates/deletes) is written as separate files or metadata</td></tr></tbody></table>\n<p>In this blog, we will understand how various lakehouse table formats implement <strong>MoR</strong> strategy and how the design influences performance and other related factors.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"how-merge-on-read-works-across-table-formats\">How Merge-on-Read Works Across Table Formats<a href=\"https://hudi.apache.org/blog/2025/07/21/mor-comparison#how-merge-on-read-works-across-table-formats\" class=\"hash-link\" aria-label=\"Direct link to How Merge-on-Read Works Across Table Formats\" title=\"Direct link to How Merge-on-Read Works Across Table Formats\" translate=\"no\">​</a></h2>\n<p>Although Merge-on-Read is a shared concept across open table formats, each system implements it using different techniques, influenced by their internal design philosophy and read-write optimization goals. Here’s a breakdown of how Apache Hudi, Apache Iceberg, and Delta Lake enable Merge-on-Read behavior.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"apache-hudi\">Apache Hudi<a href=\"https://hudi.apache.org/blog/2025/07/21/mor-comparison#apache-hudi\" class=\"hash-link\" aria-label=\"Direct link to Apache Hudi\" title=\"Direct link to Apache Hudi\" translate=\"no\">​</a></h3>\n<p>Hudi implements Merge-on-Read as one of its two core table types (along with Copy-on-Write), offering a trade-off between read and write costs by maintaining base files alongside delta log files. Instead of rewriting columnar files for every update or delete, MoR tables maintain a combination of base files and log files that encode delta updates/deletes to the base file, enabling fast ingestion and deferred file merging via asynchronous <a href=\"https://hudi.apache.org/docs/compaction\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">compaction</a>. This design is particularly suited for streaming ingestion and update-heavy workloads, where minimizing write amplification and achieving high throughput are critical, without any downtime whatsoever for the writers.</p>\n<h4 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"storage-layout\">Storage Layout<a href=\"https://hudi.apache.org/blog/2025/07/21/mor-comparison#storage-layout\" class=\"hash-link\" aria-label=\"Direct link to Storage Layout\" title=\"Direct link to Storage Layout\" translate=\"no\">​</a></h4>\n<p>At the physical level, a Hudi MoR table stores data in <a href=\"https://hudi.apache.org/tech-specs/#file-layout-hierarchy\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\"><strong>File Groups</strong></a>, each uniquely identified by a <code>fileId</code>. A file group consists of:</p>\n<ul>\n<li class=\"\">Base File (<code>.parquet, .orc</code>): Stores the base snapshot of records in columnar format.</li>\n<li class=\"\">Delta Log Files (<code>.log</code>): Append-only files that capture incremental updates, inserts, and deletes since the last compaction, in either row-oriented data formats like Apache Avro, Hudi’s native SSTable format or columnar-formats like Apache Parquet</li>\n</ul>\n<p>This hybrid design enables fast writes and defers expensive columnar file writing to asynchronous compaction.</p>\n<h4 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"write-path\">Write Path<a href=\"https://hudi.apache.org/blog/2025/07/21/mor-comparison#write-path\" class=\"hash-link\" aria-label=\"Direct link to Write Path\" title=\"Direct link to Write Path\" translate=\"no\">​</a></h4>\n<p>In a Merge-on-Read table, insert and update operations are handled differently to strike a balance between write efficiency and read performance.</p>\n<ul>\n<li class=\"\">\n<p>Insert operations behave similarly to those in Copy-on-Write tables. New records are written to freshly created <em>base files</em>, aligned to a configured block size. In some cases, these inserts may be merged into the smallest existing base file in the partition to control file counts and sizes.</p>\n</li>\n<li class=\"\">\n<p>Update operations, however, are written to <em>log files</em> associated with the corresponding file group. These updates in the log files are written using Hudi’s <a href=\"https://github.com/apache/hudi/blob/45312d437a51ccd1d8c75ba0bd8af21a47dbb9e0/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkMergeOnReadTable.java#L205\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\"><code>HoodieAppendHandle</code></a> class. At runtime, a new instance of <code>HoodieAppendHandle</code> is created with the target <em>partition</em> and <em>file ID</em>. The update records are passed to its <code>write()</code> method, which processes and appends them to the active <em>log file</em> associated with that file group. This mechanism avoids rewriting large Parquet base files and instead accumulates changes in a rolling log structure associated with each base file.</p>\n</li>\n</ul>\n<div class=\"language-java codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-java codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">HoodieAppendHandle appendHandle = new HoodieAppendHandle(config, instantTime, this,</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    partitionPath, fileId, recordMap.values().iterator(), taskContextSupplier, header);</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">appendHandle.write(recordMap);</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">List&lt;WriteStatus&gt; writeStatuses = appendHandle.close();</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">return Collections.singletonList(writeStatuses).iterator();</span><br></span></code></pre></div></div>\n<ul>\n<li class=\"\">Delete operations are also appended to log files as either delete keys or deleted vector positions, to refer to the base file records that were deleted. These delete entries are not applied to the base files immediately. Instead, they are taken into account during snapshot reads, which merge the base and log files to produce the latest view, and during compaction, which merges the accumulated log files (including deletes) into new base files.</li>\n</ul>\n<p>This design ensures that write operations remain lightweight and fast, regardless of the size of the base files. Writers are not blocked by background compaction or cleanup operations, making the system well-suited for streaming and CDC workloads.</p>\n<h4 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"read-path\">Read Path<a href=\"https://hudi.apache.org/blog/2025/07/21/mor-comparison#read-path\" class=\"hash-link\" aria-label=\"Direct link to Read Path\" title=\"Direct link to Read Path\" translate=\"no\">​</a></h4>\n<p>Hudi MoR tables offer flexible read semantics by supporting both <a href=\"https://hudi.apache.org/docs/sql_queries/#snapshot-query\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">snapshot queries</a> and <a href=\"https://hudi.apache.org/docs/table_types#query-types\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">read-optimized queries</a>, depending on the user's performance and freshness requirements.</p>\n<ul>\n<li class=\"\">\n<p>Snapshot queries provide the most current view of the dataset by dynamically merging base files with their corresponding log files at read time. The system selects between different reader types based on the nature of the query and the presence of log files:</p>\n<ul>\n<li class=\"\">A <strong>full-schema</strong> reader reads the complete row data to ensure correct application of updates and deletes.</li>\n<li class=\"\">A <strong>required-schema</strong> reader projects only the needed columns to reduce I/O, while still applying log file merges.</li>\n<li class=\"\">A <strong>skip-merging</strong> reader is used when log files are absent for a file group, allowing the query engine to read directly from base files without incurring merge costs.</li>\n</ul>\n</li>\n<li class=\"\">\n<p>Read-optimized queries, in contrast, skip reading the delta log files altogether. These queries only scan the base Parquet files, providing faster response times at the cost of not reflecting the latest un-compacted changes. This mode is suitable for applications where slightly stale data is acceptable or where performance is critical.</p>\n</li>\n</ul>\n<p>Together, these two read strategies allow Hudi MoR tables to serve both real-time and interactive queries from the same dataset, adjusting behavior depending on the workload and latency constraints.</p>\n<h4 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"compaction\">Compaction<a href=\"https://hudi.apache.org/blog/2025/07/21/mor-comparison#compaction\" class=\"hash-link\" aria-label=\"Direct link to Compaction\" title=\"Direct link to Compaction\" translate=\"no\">​</a></h4>\n<p>As log files accumulate new updates and deletes, Hudi triggers a compaction operation to merge these log files back into columnar base files. This process is <a href=\"https://hudi.apache.org/docs/compaction#async--offline-compaction-models\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">configurable and asynchronous</a>, and plays a key role in balancing write and read performance.</p>\n<p>Compaction in Hudi is triggered based on thresholds that can be configured by the user, such as the <em>number of commits (NUM_COMMITS)</em>. During compaction, all log files associated with a file group are read and merged with the existing base file to produce a new compacted base file.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"apache-iceberg\">Apache Iceberg<a href=\"https://hudi.apache.org/blog/2025/07/21/mor-comparison#apache-iceberg\" class=\"hash-link\" aria-label=\"Direct link to Apache Iceberg\" title=\"Direct link to Apache Iceberg\" translate=\"no\">​</a></h3>\n<p>Apache Iceberg supports Merge-on-Read (MoR) semantics by maintaining immutable base data files and tracking updates and deletions through separate <a href=\"https://iceberg.apache.org/spec/#delete-formats\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\"><em>delete files</em></a>. This design avoids rewriting data files for every update or delete operation. Instead, these changes are applied at query time by merging delete files with the base files to produce an up-to-date view.</p>\n<h4 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"storage-layout-1\">Storage Layout<a href=\"https://hudi.apache.org/blog/2025/07/21/mor-comparison#storage-layout-1\" class=\"hash-link\" aria-label=\"Direct link to Storage Layout\" title=\"Direct link to Storage Layout\" translate=\"no\">​</a></h4>\n<p>An Iceberg table consists of:</p>\n<ul>\n<li class=\"\">Base Data Files: Immutable Parquet, ORC, or Avro files that contain the primary data.</li>\n<li class=\"\">Delete Files: Auxiliary files that record row-level deletions.</li>\n</ul>\n<h4 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"write-path-1\">Write Path<a href=\"https://hudi.apache.org/blog/2025/07/21/mor-comparison#write-path-1\" class=\"hash-link\" aria-label=\"Direct link to Write Path\" title=\"Direct link to Write Path\" translate=\"no\">​</a></h4>\n<p>In Iceberg’s MoR tables, write operations implement row-level updates by encoding them as a delete of the old record and an insert of the new one. Rather than modifying existing Parquet base files directly, Iceberg maintains a clear separation between new data and logical deletes by introducing delete files alongside new data files.</p>\n<ul>\n<li class=\"\">Inserts behave in the same way as CoW tables. The new data is appended to the table as part of a new snapshot.</li>\n<li class=\"\">For delete operations, Iceberg writes a delete file containing rows to be logically removed across multiple base files. Delete files are of two types:<!-- -->\n<ul>\n<li class=\"\">Position Deletes: Reference row positions in a specific data file.</li>\n<li class=\"\">Equality Deletes: Encode a predicate that matches rows based on one or more column values.</li>\n</ul>\n</li>\n</ul>\n<p>Equality deletes are typically not favored in performance sensitive data platforms, since it forces predicate evaluation against every single base file during snapshot reads.</p>\n<p>The <code>DeleteFile</code> interface captures these semantics:</p>\n<div class=\"language-java codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-java codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">public interface DeleteFile extends ContentFile&lt;StructLike&gt; {</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  enum DeleteType {</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    EQUALITY, POSITION</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  }</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">}</span><br></span></code></pre></div></div>\n<p><strong>Note:</strong> Iceberg v3 introduces Deletion Vectors as a more efficient alternative to positional deletes. Deletion vectors attach a <em>bitmap</em> to a data file to indicate deleted rows, allowing query engines to skip over deleted rows at read time. Deletion Vectors are already supported by Delta Lake and Hudi and this is now borrowed into the Iceberg spec as well.</p>\n<ul>\n<li class=\"\">For update operations, Iceberg uses a two-step process. An update is implemented as a delete + insert pattern. First, a delete file is created to logically remove the old record, using either a position or equality delete. Then, a new data file is written that contains the full image of the updated record. Both the delete file and the new data file are added in a single atomic commit, creating a new snapshot of the table. This behavior is implemented via the <code>RowDelta</code> interface:</li>\n</ul>\n<div class=\"language-java codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-java codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">RowDelta rowDelta = table.newRowDelta()</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    .addDeletes(deleteFile)</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">    .addRows(dataFile);</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">rowDelta.commit();</span><br></span></code></pre></div></div>\n<p>All write operations, whether adding new data files or new delete files, produce a new snapshot in Iceberg’s timeline. This guarantees consistent isolation across readers and writers while avoiding any rewriting of immutable data files.</p>\n<h4 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"read-path-1\">Read Path<a href=\"https://hudi.apache.org/blog/2025/07/21/mor-comparison#read-path-1\" class=\"hash-link\" aria-label=\"Direct link to Read Path\" title=\"Direct link to Read Path\" translate=\"no\">​</a></h4>\n<p>During query execution, Iceberg performs a Merge-on-Read query by combining the immutable base data files with any relevant delete files to present a consistent and up-to-date view. Before reading, the scan planning logic identifies which delete files apply to each data file, ensuring that deletes are correctly associated with their targets.</p>\n<p>This planning step guarantees that any row marked for deletion through either position deletes or equality deletes is filtered out of the final results, while the original base files remain unchanged. The merging of base data with delete files is applied dynamically by the query engine, allowing Iceberg to preserve the immutable file structure and still deliver row-level updates.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"delta-lake\">Delta Lake<a href=\"https://hudi.apache.org/blog/2025/07/21/mor-comparison#delta-lake\" class=\"hash-link\" aria-label=\"Direct link to Delta Lake\" title=\"Direct link to Delta Lake\" translate=\"no\">​</a></h3>\n<p>Delta Lake supports Merge-on-Read semantics using <a href=\"https://docs.delta.io/latest/delta-deletion-vectors.html\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\"><em>Deletion Vectors (DVs)</em></a>, a feature that allows rows to be logically removed from a dataset without rewriting the base Parquet files. This enables efficient row-level delete   while preserving immutability of data files. For updates, Delta Lake encodes changes as a combination of DELETE and INSERT operations, i.e. the old row is marked as deleted, and a new row with updated values is appended.</p>\n<h4 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"storage-layout-2\">Storage Layout<a href=\"https://hudi.apache.org/blog/2025/07/21/mor-comparison#storage-layout-2\" class=\"hash-link\" aria-label=\"Direct link to Storage Layout\" title=\"Direct link to Storage Layout\" translate=\"no\">​</a></h4>\n<p>In Delta Lake, the storage layout consists of:</p>\n<ul>\n<li class=\"\">Base Data Files: Immutable Parquet files that hold the core data</li>\n<li class=\"\">Deletion Vectors: Structures that track rows that should be considered deleted during reads, instead of physically removing them from Parquet</li>\n</ul>\n<p>A deletion vector is described by a descriptor which captures its storage type (inline, on-disk, or UUID-based), its physical location or inline data, an offset if stored on disk, its size in bytes, and the cardinality (number of rows it marks as deleted).</p>\n<p>Small deletion vectors can be embedded directly into the Delta transaction log (inline), while larger ones are stored as separate files, with the UUIDs referencing them by a unique identifier.</p>\n<h4 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"write-path-2\">Write Path<a href=\"https://hudi.apache.org/blog/2025/07/21/mor-comparison#write-path-2\" class=\"hash-link\" aria-label=\"Direct link to Write Path\" title=\"Direct link to Write Path\" translate=\"no\">​</a></h4>\n<p>When a DELETE, UPDATE, or MERGE operation is performed on a Delta table, Delta Lake does not rewrite the affected base Parquet files. Instead, it generates a deletion vector that identifies which rows are logically removed. These deletion vectors are built as compressed bitmap structures (using Roaring Bitmaps), which efficiently encode the positions of the deleted rows.</p>\n<p>Smaller deletion vectors are kept inline in the transaction log for quick lookup, while larger ones are persisted as separate deletion vector files. All write operations that affect rows in this way update the metadata to track the associated deletion vectors, maintaining a consistent and atomic snapshot view for downstream reads.</p>\n<h4 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"read-path-2\">Read Path<a href=\"https://hudi.apache.org/blog/2025/07/21/mor-comparison#read-path-2\" class=\"hash-link\" aria-label=\"Direct link to Read Path\" title=\"Direct link to Read Path\" translate=\"no\">​</a></h4>\n<p>During query execution, Delta Lake consults any deletion vectors attached to the current snapshot. The query execution loads these deletion vectors and applies them dynamically, filtering out rows marked as deleted before returning results to the user. This happens without rewriting or modifying the base Parquet files, preserving their immutability while still providing correct row-level semantics.</p>\n<p>This Merge-on-Read approach allows Delta Lake to combine efficient write operations with the ability to serve up-to-date views, ensuring that queries see a consistent, deletion-aware representation of the dataset.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"comparative-design-analysis\">Comparative Design Analysis<a href=\"https://hudi.apache.org/blog/2025/07/21/mor-comparison#comparative-design-analysis\" class=\"hash-link\" aria-label=\"Direct link to Comparative Design Analysis\" title=\"Direct link to Comparative Design Analysis\" translate=\"no\">​</a></h2>\n<p>Merge-on-Read semantics are implemented differently across open table formats, with each approach reflecting distinct trade-offs that influence workload performance, complexity, and operational flexibility. MoR is generally well-suited for high-throughput, low-latency streaming ingestion scenarios in a lakehouse, where frequent updates and late-arriving data are expected. In contrast, Copy-on-Write (CoW) tables often work best for simpler, batch-oriented workloads where updates are infrequent and read-optimized behavior is a priority.</p>\n<p>In this section, we focus on Apache Hudi and Apache Iceberg table formats and explore how their MoR designs influence real-world workloads.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"streaming-data-support--event-time-ordering\">Streaming Data Support &amp; Event-Time Ordering<a href=\"https://hudi.apache.org/blog/2025/07/21/mor-comparison#streaming-data-support--event-time-ordering\" class=\"hash-link\" aria-label=\"Direct link to Streaming Data Support &amp; Event-Time Ordering\" title=\"Direct link to Streaming Data Support &amp; Event-Time Ordering\" translate=\"no\">​</a></h3>\n<p>Hudi’s Merge-on-Read design supports event-time ordering and late-arriving data for streaming workloads by providing <a href=\"https://hudi.apache.org/docs/record_merger#record-payloads\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\"><code>RecordPayload</code></a> and <a href=\"https://hudi.apache.org/docs/record_merger\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\"><code>RecordMerger</code></a> APIs. These allow updates to be merged based on database sequence numbers or event timestamps, so that if data arrives out of order or has late arriving data, the final state is still correct from a temporal perspective.</p>\n<img src=\"https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig1.png\" alt=\"index\" width=\"1000\" align=\"middle\">\n<p>Iceberg uses a last-writer-wins approach, where the most recent commit determines record values regardless of event time. This design may be tricky to deal with late-arriving data  in streaming workloads or CDC ingestion. For e.g. if the source stream is ever repositioned to an earlier time, it will cause the table to move backwards in time where older record values from the replayed stream overwrite newer record images in the table.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"scalable-incremental-write-costs\">Scalable Incremental Write Costs<a href=\"https://hudi.apache.org/blog/2025/07/21/mor-comparison#scalable-incremental-write-costs\" class=\"hash-link\" aria-label=\"Direct link to Scalable Incremental Write Costs\" title=\"Direct link to Scalable Incremental Write Costs\" translate=\"no\">​</a></h3>\n<p>One of the main goals of MoR is to reduce write costs and latencies by avoiding full file rewrites. Hudi achieves this by appending changes to <em>delta logs</em> and using <a href=\"https://hudi.apache.org/docs/indexes\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\"><strong>indexing</strong></a> to quickly identify which file group an incoming update belongs to. Hudi supports different index types to accelerate this lookup process, so it does not need to scan the entire table on every update. This ensures that even if you are updating a relatively small amount of data - for example, 1GB of changes into a 1TB table every five to ten minutes, the system can efficiently target only the affected files.</p>\n<img src=\"https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig2.png\" alt=\"index\" width=\"1000\" align=\"middle\">\n<p>Iceberg handles row-level updates and deletes by recording them as <a href=\"https://iceberg.apache.org/spec/#delete-formats\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\"><em>delete files</em></a>. To identify which records to update or delete, Iceberg relies on scanning table metadata, and in some cases file-level data, to locate affected rows. This design uses a simple metadata approach but if partitioning is not highly selective, this lookup step can become a bottleneck for write performance on large tables with frequent small updates.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"asynchronous-compaction-during-merge\">Asynchronous Compaction during ‘Merge’<a href=\"https://hudi.apache.org/blog/2025/07/21/mor-comparison#asynchronous-compaction-during-merge\" class=\"hash-link\" aria-label=\"Direct link to Asynchronous Compaction during ‘Merge’\" title=\"Direct link to Asynchronous Compaction during ‘Merge’\" translate=\"no\">​</a></h3>\n<p>Hudi employs <a href=\"https://hudi.apache.org/blog/2025/01/28/concurrency-control#occ-multi-writers\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">optimistic concurrency control</a> (OCC) between writers and maintains blocking-free <a href=\"https://hudi.apache.org/blog/2025/01/28/concurrency-control#mvcc-writer-table-service-and-table-service-table-service\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">multi-version concurrency control</a> (MVCC) between writers and its asynchronous compaction process. This means writers can continue appending updates to the same records while earlier versions are being compacted in the background. Compaction operates <em>asynchronously</em>, creating new base files from accumulated log files, without interfering with active writers. This ensures great data freshness as well as better compression ratio and thus excellent query performance for columnar files longer term.</p>\n<img src=\"https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig3.png\" alt=\"index\" width=\"1000\" align=\"middle\">\n<p>Iceberg maintains consistent snapshots across all operations, but it does not separate a dedicated compaction action from other write operations. As a result, if both a writer and a maintenance process try to modify overlapping data, standard snapshot conflict resolution ensures only one succeeds and might require retries in some concurrent write scenarios, but there is no asynchronous way to run compaction services. This could lead to livelocking between the writer and table maintenance, where one of them continuously causes the other to fail.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"non-blocking-concurrency-control-nbcc-for-real-time-applications\">Non-Blocking Concurrency Control (NBCC) for Real-time applications<a href=\"https://hudi.apache.org/blog/2025/07/21/mor-comparison#non-blocking-concurrency-control-nbcc-for-real-time-applications\" class=\"hash-link\" aria-label=\"Direct link to Non-Blocking Concurrency Control (NBCC) for Real-time applications\" title=\"Direct link to Non-Blocking Concurrency Control (NBCC) for Real-time applications\" translate=\"no\">​</a></h3>\n<p>Hudi 1.0 further extends its concurrency model to allow multiple writers to safely update the same record at the same time with <a href=\"https://hudi.apache.org/blog/2025/01/28/concurrency-control#non-blocking-concurrency-control-multi-writers\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">non-blocking conflict resolution</a>. It supports serializability guarantees based on write completion timestamps (arrival-time processing), while also allowing record merging according to event-time order if required. This flexible concurrency strategy enables concurrent writes to proceed, without the need to wait, making it ideal for real-time applications that demand faster ingestion.</p>\n<img src=\"https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig4.png\" alt=\"index\" width=\"700\" align=\"middle\">\n<p>Iceberg applies OCC through its snapshot approach, where writers commit updates against the latest known snapshot, and if conflicts are detected, retries are required. There is no explicit distinction between arrival-time and event-time semantics for concurrent record updates.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"minimizing-read-costs\">Minimizing Read Costs<a href=\"https://hudi.apache.org/blog/2025/07/21/mor-comparison#minimizing-read-costs\" class=\"hash-link\" aria-label=\"Direct link to Minimizing Read Costs\" title=\"Direct link to Minimizing Read Costs\" translate=\"no\">​</a></h3>\n<p>Hudi organizes records into <em>file groups,</em> ensuring that updates are consistently routed back to the same group where the original records were stored. This approach means that when a query is executed, it only needs to scan the base file and any delta log files within that specific file group, reducing the data that must be read and merged at query time. By tying updates and inserts to a consistent file group, Hudi preserves locality and limits merge complexity.</p>\n<img src=\"https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig5.png\" alt=\"index\" width=\"1000\" align=\"middle\">\n<p>Iceberg applies updates and deletes using <em>delete files</em>, and these can reference any row in any base file. As a result, readers must examine all relevant delete files along with all associated base data files during scan planning and execution, which can increase I/O and metadata processing requirements for large tables.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"performant-read-side-merge\">Performant Read-Side Merge<a href=\"https://hudi.apache.org/blog/2025/07/21/mor-comparison#performant-read-side-merge\" class=\"hash-link\" aria-label=\"Direct link to Performant Read-Side Merge\" title=\"Direct link to Performant Read-Side Merge\" translate=\"no\">​</a></h3>\n<p>Hudi’s MoR implementation uses <em>key-based</em> merging to reconcile delta log records with base files, which allows query engines to push down filters and still correctly merge updates based on record keys. This selective merging reduces unnecessary I/O and improves performance for queries that only need a subset of columns or rows.</p>\n<img src=\"https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig6.png\" alt=\"index\" width=\"800\" align=\"middle\">\n<p>Iceberg historically required readers (particularly Spark readers) to load entire base files when applying positional deletes. This was because pushing down filters could change the order or number of rows returned by the Parquet reader, making positional delete applications incorrect. As a result, filter pushdowns could not be safely applied, forcing a full file scan to maintain correctness. There has been ongoing work in the Iceberg community to address this limitation by improving how positional information is tracked through filtered reads.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"efficient-compaction-planning\">Efficient Compaction Planning<a href=\"https://hudi.apache.org/blog/2025/07/21/mor-comparison#efficient-compaction-planning\" class=\"hash-link\" aria-label=\"Direct link to Efficient Compaction Planning\" title=\"Direct link to Efficient Compaction Planning\" translate=\"no\">​</a></h3>\n<p>Hudi’s compaction strategy operates at the level of individual file groups, which means it can plan and execute small, predictable units of compaction work. This fine-grained approach allows compaction to proceed <em>incrementally</em> and avoids large, unpredictable workloads.</p>\n<img src=\"https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig7.png\" alt=\"index\" width=\"800\" align=\"middle\">\n<p>In Iceberg, compaction must consider all base files and their related delete files together, because delete files reference rows in the base data files. This creates a dependency graph where all related files must be handled in a coordinated way. As delete files accumulate over time, these compaction operations can become increasingly large and complex to plan, making it harder to schedule resources efficiently. If compaction falls behind, the amount of data that must be compacted in future operations continues to grow, potentially making the problem worse.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"temporal-and-spatial-locality-for-event-time-filters\">Temporal and Spatial Locality for Event-Time Filters<a href=\"https://hudi.apache.org/blog/2025/07/21/mor-comparison#temporal-and-spatial-locality-for-event-time-filters\" class=\"hash-link\" aria-label=\"Direct link to Temporal and Spatial Locality for Event-Time Filters\" title=\"Direct link to Temporal and Spatial Locality for Event-Time Filters\" translate=\"no\">​</a></h3>\n<p>Hudi maintains temporal and spatial locality by ensuring that updates and deletes are routed back to the same file group where the original record was first stored. This preserves the time-based clustering or ordering of records, which is especially beneficial for queries filtering by event time or operating within specific time windows. By keeping related records together, Hudi enables efficient pruning of file groups along with partition pruning, during time-based queries.</p>\n<img src=\"https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig8.png\" alt=\"index\" width=\"1000\" align=\"middle\">\n<p>Iceberg handles updates by deleting the existing record and inserting a new one, which may place the updated record in a different data file. Over time, this can scatter records that belong to the same logical or temporal group across multiple files, reducing the effectiveness of partition pruning and requiring periodic clustering or optimization to restore temporal locality.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"partial-updates-for-performant-merge\">Partial Updates for Performant Merge<a href=\"https://hudi.apache.org/blog/2025/07/21/mor-comparison#partial-updates-for-performant-merge\" class=\"hash-link\" aria-label=\"Direct link to Partial Updates for Performant Merge\" title=\"Direct link to Partial Updates for Performant Merge\" translate=\"no\">​</a></h3>\n<p>Hudi supports partial updates by encoding only the columns that have changed into its delta log files. This means the cost of merging updates is proportional to the number of columns actually modified, rather than the total width of the record. For columnar datasets with wide schemas, this can significantly reduce write amplification and improve merge performance.</p>\n<img src=\"https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig9.png\" alt=\"index\" width=\"800\" align=\"middle\">\n<p>In Iceberg, updates are implemented as a delete plus a full-row insert, which requires rewriting the entire record even if only a single column has changed. As a result, update costs in Iceberg scale with the total number of columns in the record, increasing I/O and storage requirements for wide tables with frequent column-level updates.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"conclusion\">Conclusion<a href=\"https://hudi.apache.org/blog/2025/07/21/mor-comparison#conclusion\" class=\"hash-link\" aria-label=\"Direct link to Conclusion\" title=\"Direct link to Conclusion\" translate=\"no\">​</a></h2>\n<p>Merge-on-Read (MoR) table type provides an alternative approach to managing updates and deletes on immutable columnar files in a lakehouse. While multiple open table formats support MoR semantics, their design choices significantly affect suitability for real-time and change-data driven workloads.</p>\n<p>Apache Hudi’s MoR implementation specifically addresses the needs of high-ingestion, update-heavy pipelines. By appending changes to delta logs, preserving file-group-based data locality, supporting event-time ordering, and enabling asynchronous, non-blocking compaction, Hudi minimizes write amplification and supports low-latency data availability. These design primitives directly align with streaming and CDC patterns, where data arrives frequently and potentially out of order. Iceberg and Delta Lake also implement MoR semantics in their own ways to address transactional consistency and immutable storage goals.</p>\n<hr>",
            "url": "https://hudi.apache.org/blog/2025/07/21/mor-comparison",
            "title": "A Deep Dive on Merge-on-Read (MoR) in Lakehouse Table Formats",
            "summary": "TL;DR",
            "date_modified": "2025-07-21T00:00:00.000Z",
            "author": {
                "name": "Dipankar Mazumdar"
            },
            "tags": [
                "Apache Hudi",
                "Merge-on-Read (MoR)",
                "Streaming"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi",
            "content_html": "<div class=\"theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success\"><div class=\"admonitionHeading_Gvgb\"><span class=\"admonitionIcon_Rf37\"><svg viewBox=\"0 0 12 16\"><path fill-rule=\"evenodd\" d=\"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z\"></path></svg></span>TL;DR</div><div class=\"admonitionContent_BuS1\"><p>Peloton re-architected its data platform using Apache Hudi to overcome snapshot delays, rigid service coupling, and high operational costs. By adopting CDC-based ingestion from PostgreSQL and DynamoDB, moving from CoW to MoR tables, and leveraging asynchronous services with fine-grained schema control, Peloton achieved 10-minute ingestion cycles, reduced compute/storage overhead, and enabled time travel and GDPR compliance.</p></div></div>\n<p>Peloton is a global interactive fitness platform that delivers connected, instructor-led fitness experiences to millions of members worldwide. Known for its immersive classes and cutting-edge equipment, Peloton combines software, hardware, and data to create personalized workout journeys. With a growing member base and increasing product diversity, data has become central to how Peloton delivers value. The <em>Data Platform</em> team at Peloton is responsible for building and maintaining the core infrastructure that powers analytics, reporting, and real-time data applications. Their work ensures that data flows seamlessly from transactional systems to the data lake, enabling teams across the organization to make timely, data-driven decisions.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"the-challenge-data-growth-latency-and-operational-bottlenecks\">The Challenge: Data Growth, Latency, and Operational Bottlenecks<a href=\"https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi#the-challenge-data-growth-latency-and-operational-bottlenecks\" class=\"hash-link\" aria-label=\"Direct link to The Challenge: Data Growth, Latency, and Operational Bottlenecks\" title=\"Direct link to The Challenge: Data Growth, Latency, and Operational Bottlenecks\" translate=\"no\">​</a></h2>\n<p>As Peloton evolved into a global interactive fitness platform, its data infrastructure was challenged by the growing need for timely insights, agile service migrations, and cost-effective analytics. Daily operations, recommendation systems, and compliance requirements demanded an architecture that could support near real-time access, high-frequency updates, and scalable service boundaries.</p>\n<p>However, the team faced persistent bottlenecks with the existing setup:</p>\n<ul>\n<li class=\"\">Reporting pipelines were gated by the completion of full snapshot jobs.</li>\n<li class=\"\">Recommender systems could only function on daily refreshed datasets.</li>\n<li class=\"\">The analytics platform was tightly coupled with operational systems.</li>\n<li class=\"\">Microservice migrations were constrained to all-at-once shifts.</li>\n<li class=\"\">Database read replicas incurred high infrastructure costs.</li>\n</ul>\n<p>These limitations made it difficult to meet SLA expectations, scale workloads efficiently, and adapt the platform to new user and product needs.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"the-legacy-architecture\">The Legacy Architecture<a href=\"https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi#the-legacy-architecture\" class=\"hash-link\" aria-label=\"Direct link to The Legacy Architecture\" title=\"Direct link to The Legacy Architecture\" translate=\"no\">​</a></h2>\n<img src=\"https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig1.png\" alt=\"challenge\" width=\"1000\" align=\"middle\">\n<p>Peloton's earlier architecture relied on daily snapshots from a monolithic <strong>PostgreSQL</strong> database. The analytics systems would consume these snapshots, often waiting hours for completion. This not only delayed reporting but also introduced downstream rigidity.</p>\n<p>Because the same data platform supported both online and analytical workloads, any schema or service migration required significant planning and coordination. Database read replicas, used to scale reads, increased cost overhead. Moreover, recommendation systems that depended on data freshness were constrained by the snapshot interval, limiting personalization capabilities. This architecture struggled to support a fast-moving product roadmap, near real-time analytics, and the data agility needed to experiment and iterate.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"reimagining-the-data-platform-with-apache-hudi\">Reimagining the Data Platform with Apache Hudi<a href=\"https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi#reimagining-the-data-platform-with-apache-hudi\" class=\"hash-link\" aria-label=\"Direct link to Reimagining the Data Platform with Apache Hudi\" title=\"Direct link to Reimagining the Data Platform with Apache Hudi\" translate=\"no\">​</a></h2>\n<img src=\"https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig2.png\" alt=\"challenge\" width=\"1000\" align=\"middle\">\n<p>To address these challenges, the data platform team introduced Apache Hudi as the foundation of its modern data lake. The architecture was rebuilt to support Change Data Capture (CDC) ingestion from both PostgreSQL and DynamoDB using Debezium, with Kafka acting as the transport layer. A custom-built Hudi writer was developed to ingest CDC records into S3 using Apache Spark on EMR (version 6.12.0 with Hudi 0.13.1).</p>\n<p>Peloton initially chose Copy-on-Write (CoW) table formats to support querying via Redshift Spectrum and simplify adoption. However, performance and cost bottlenecks prompted a transition to Merge-on-Read (MoR) tables with asynchronous table services for cleaning and compaction.</p>\n<p>Key architectural enhancements included:</p>\n<ul>\n<li class=\"\"><strong>Support for GDPR compliance</strong> through structured delete propagation.</li>\n<li class=\"\"><strong>Time travel queries</strong> for recommender model training and data recovery.</li>\n<li class=\"\"><strong>Phased migration support</strong> for microservices via decoupled ingestion.</li>\n</ul>\n<p>Peloton's broader data platform tech stack supports this architecture with a range of tools for orchestration, analytics, and governance. This includes EMR for compute, Redshift for querying, DBT for data transformations, Looker for BI and visualization, Airflow for orchestration, and DataHub for metadata management. These components complement Apache Hudi in forming a modular and production-ready lakehouse stack.</p>\n<img src=\"https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig3.png\" alt=\"challenge\" width=\"1000\" align=\"middle\">\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"learnings-from-running-hudi-at-scale\">Learnings from Running Hudi at Scale<a href=\"https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi#learnings-from-running-hudi-at-scale\" class=\"hash-link\" aria-label=\"Direct link to Learnings from Running Hudi at Scale\" title=\"Direct link to Learnings from Running Hudi at Scale\" translate=\"no\">​</a></h2>\n<p>With Hudi now integrated into Peloton's data lake, the team began to observe and address new operational and architectural challenges that emerged at scale. This section outlines the major lessons learned while maintaining high-ingestion throughput, ensuring data reliability, and keeping infrastructure costs under control.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"cow-vs-mor-performance-trade-offs\">CoW vs MoR: Performance Trade-offs<a href=\"https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi#cow-vs-mor-performance-trade-offs\" class=\"hash-link\" aria-label=\"Direct link to CoW vs MoR: Performance Trade-offs\" title=\"Direct link to CoW vs MoR: Performance Trade-offs\" translate=\"no\">​</a></h3>\n<p>Initially, Copy-on-Write (CoW) tables were chosen to simplify deployment and ensure compatibility with Redshift Spectrum. However, as ingestion frequency increased and update volumes spanned hundreds of partitions, performance became a bottleneck. Some high-frequency tables with updates across 256 partitions took nearly an hour to process per run. Additionally, retaining 30 days of commits for training recommender models significantly inflated storage requirements, reaching into the hundreds of gigabytes.</p>\n<img src=\"https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig4.png\" alt=\"challenge\" width=\"1000\" align=\"middle\">\n<p>To resolve this, the team migrated to Hudi’s Merge-on-Read (MoR) tables and reduced commit retention to 7 days. With ingestion jobs now running every 10 minutes, latency dropped significantly, and storage and compute usage became more efficient.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"async-vs-inline-table-services\">Async vs Inline Table Services<a href=\"https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi#async-vs-inline-table-services\" class=\"hash-link\" aria-label=\"Direct link to Async vs Inline Table Services\" title=\"Direct link to Async vs Inline Table Services\" translate=\"no\">​</a></h3>\n<p>To improve write throughput and meet low-latency ingestion goals, the Peloton team initially configured Apache Hudi with asynchronous cleaner and compactor services. This approach worked well across most tables, allowing ingestion pipelines to run every 10 minutes with minimal blocking but introduced some operational edge cases. Some of the challenges encountered included:</p>\n<ul>\n<li class=\"\">Concurrent execution of writer and cleaner jobs, leading to conflicts. These were mitigated by introducing DynamoDB-based locks to serialize access.</li>\n<li class=\"\">Reader-cleaner race conditions, where time travel queries intermittently failed with <code>\"File Not Found\"</code> errors - traced back to cleaners deleting files mid-read.</li>\n<li class=\"\">Compaction disruptions caused by EMR node terminations, which led to orphaned files when jobs failed mid-way.</li>\n</ul>\n<p>These edge cases were largely due to the operational complexity of managing concurrent workloads at Peloton’s scale. After weighing reliability against latency, the team opted to switch to inline table services for compaction and cleaning, augmented with custom logic to control when these actions would run. This change improved system stability while maintaining acceptable latency trade-offs.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"glue-schema-version-limits\">Glue Schema Version Limits<a href=\"https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi#glue-schema-version-limits\" class=\"hash-link\" aria-label=\"Direct link to Glue Schema Version Limits\" title=\"Direct link to Glue Schema Version Limits\" translate=\"no\">​</a></h3>\n<p>As schema evolution continued, the team used Hudi's <code>META_SYNC_ENABLED</code> to sync schema updates with AWS Glue. Over time, high-frequency schema updates pushed the number of <code>TABLE_VERSION</code> resources in Glue beyond the <em>1 million</em> limit. This caused jobs to fail in ways that were initially difficult to trace.</p>\n<p>One such failure manifested as the following error:</p>\n<div class=\"language-text codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_QJqH\"><pre tabindex=\"0\" class=\"prism-code language-text codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">ERROR Client: Application diagnostics message: User class threw exception:</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">java.lang.NoSuchMethodError: 'org.apache.hudi.exception.HoodieException </span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">org.apache.hudi.sync.common.util.SyncUtilHelpers.getExceptionFromList(java.util.Collection)'</span><br></span></code></pre></div></div>\n<p>After significant debugging, the issue was traced to AWS Glue limits. The team implemented a multi-step fix:</p>\n<ul>\n<li class=\"\">Worked with AWS to temporarily raise resource limits.</li>\n<li class=\"\">Developed a Python service to identify and delete outdated table versions, removing over 1 million entries.</li>\n<li class=\"\">Added an Airflow job to schedule weekly cleanup tasks.</li>\n<li class=\"\">Improved schema sync logic to trigger only when the schema changed.</li>\n</ul>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"debezium--toast-handling\">Debezium &amp; TOAST Handling<a href=\"https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi#debezium--toast-handling\" class=\"hash-link\" aria-label=\"Direct link to Debezium &amp; TOAST Handling\" title=\"Direct link to Debezium &amp; TOAST Handling\" translate=\"no\">​</a></h3>\n<p>PostgreSQL CDC ingestion posed unique challenges due to the database’s handling of large fields using TOAST (The Oversized-Attribute Storage Technique). When fields over 8KB were unchanged, Debezium emitted a placeholder value <code>__debezium_unavailable_value</code>, making it impossible to determine whether the value had changed.</p>\n<img src=\"https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig5.png\" alt=\"challenge\" width=\"1000\" align=\"middle\">\n<p>To address this, Peloton:</p>\n<ul>\n<li class=\"\">Populated initial data using PostgreSQL snapshots.</li>\n<li class=\"\">Implemented self-joins between incoming CDC records and existing Hudi records to fill in missing values.</li>\n<li class=\"\">Separated inserts, updates, and deletes within Spark batch processing.</li>\n<li class=\"\">Used the <code>ts</code> field as the precombine key to ensure only the latest record state was retained.</li>\n</ul>\n<p>A reconciliation pipeline was also developed to heal data inconsistencies caused by multiple operations on the same key within a batch (e.g., create-delete-create).</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"data-validation-and-quality-enforcement\">Data Validation and Quality Enforcement<a href=\"https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi#data-validation-and-quality-enforcement\" class=\"hash-link\" aria-label=\"Direct link to Data Validation and Quality Enforcement\" title=\"Direct link to Data Validation and Quality Enforcement\" translate=\"no\">​</a></h3>\n<p>Data quality was critical to ensure trust in the newly established data lake. The team developed several internal libraries and checks:</p>\n<ul>\n<li class=\"\">A Crypto Shredding Library to encrypt <code>user_id</code> and other PII fields before storage.</li>\n<li class=\"\">A Data Validation Framework that compared records in the lake against snapshot data.</li>\n<li class=\"\">A Data Quality Library that enforced column-level thresholds. These checks integrated with DataHub and were tied to Airflow sensors to halt downstream jobs on failures.</li>\n</ul>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"dynamodb-ingestion-and-schema-challenges\">DynamoDB Ingestion and Schema Challenges<a href=\"https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi#dynamodb-ingestion-and-schema-challenges\" class=\"hash-link\" aria-label=\"Direct link to DynamoDB Ingestion and Schema Challenges\" title=\"Direct link to DynamoDB Ingestion and Schema Challenges\" translate=\"no\">​</a></h3>\n<p>Some Peloton services relied on DynamoDB for operational workloads (NoSQL). To ingest these datasets into the lake, the team used DynamoDB Streams and a Kafka Connector, allowing reuse of the existing Kafka-based Hudi ingestion path.</p>\n<img src=\"https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig6.png\" alt=\"challenge\" width=\"1000\" align=\"middle\">\n<p>However, the NoSQL nature of DynamoDB introduced schema management challenges. Two strategies were evaluated:</p>\n<ol>\n<li class=\"\">Stakeholder-defined schemas, using SUPER-type fields.</li>\n<li class=\"\">Dynamic schema inference, where incoming JSON records were parsed, and the evolving schema was inferred and reconciled.</li>\n</ol>\n<p>The team opted for dynamic inference despite increased processing time, as it enabled better support for exploratory workloads. Daily snapshots and reconciliation steps helped clean up inconsistent schema states.</p>\n<h3 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"reducing-operational-costs\">Reducing Operational Costs<a href=\"https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi#reducing-operational-costs\" class=\"hash-link\" aria-label=\"Direct link to Reducing Operational Costs\" title=\"Direct link to Reducing Operational Costs\" translate=\"no\">​</a></h3>\n<img src=\"https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig7.png\" alt=\"challenge\" width=\"1000\" align=\"middle\">\n<p>As the system matured, cost optimization became a priority. The team used <a href=\"https://github.com/ganglia/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">Ganglia</a> to analyze job profiles and identify areas for improvement:</p>\n<ul>\n<li class=\"\">EMR resources were gradually right-sized based on CPU and memory usage.</li>\n<li class=\"\">Conditional Hive syncing was introduced to avoid unnecessary sync operations during each run.</li>\n<li class=\"\">A Spark-side inefficiency was discovered where archived timelines were unnecessarily loaded, causing jobs to take 4x longer. Fixing this reduced overall latency and compute resource usage.</li>\n</ul>\n<p>These operational refinements significantly reduced idle times and improved the cost-efficiency of the platform.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"gains-from-hudi-adoption\">Gains from Hudi Adoption<a href=\"https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi#gains-from-hudi-adoption\" class=\"hash-link\" aria-label=\"Direct link to Gains from Hudi Adoption\" title=\"Direct link to Gains from Hudi Adoption\" translate=\"no\">​</a></h2>\n<p>Peloton's transition to Apache Hudi led to measurable performance, operational, and cost-related improvements across its modern data platform.</p>\n<p>Peloton's transition to Apache Hudi yielded several measurable improvements:</p>\n<ul>\n<li class=\"\">Ingestion frequency increased from once daily to every 10 minutes.</li>\n<li class=\"\">Reduced snapshot job durations from an hour to under 15 minutes.</li>\n<li class=\"\">Cost savings by eliminating read replicas and optimizing EMR cluster usage.</li>\n<li class=\"\">Time travel support enabled retrospective analysis and model re-training.</li>\n<li class=\"\">Improved compliance posture through structured deletes and encrypted PII.</li>\n</ul>\n<p>The modernization laid the groundwork for future evolution, including real-time streaming ingestion using Apache Flink and continued improvements in data freshness, latency, and governance.</p>\n<p>This blog is based on Peloton’s presentation at the Apache Hudi Community Sync. If you are interested in watching the recorded version of the video, you can find it <a href=\"https://youtu.be/-Pyid5K9dyU?feature=shared\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">here</a>.</p>\n<hr>",
            "url": "https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi",
            "title": "Modernizing Data Infrastructure at Peloton Using Apache Hudi",
            "summary": "Peloton re-architected its data platform using Apache Hudi to overcome snapshot delays, rigid service coupling, and high operational costs. By adopting CDC-based ingestion from PostgreSQL and DynamoDB, moving from CoW to MoR tables, and leveraging asynchronous services with fine-grained schema control, Peloton achieved 10-minute ingestion cycles, reduced compute/storage overhead, and enabled time travel and GDPR compliance.",
            "date_modified": "2025-07-15T00:00:00.000Z",
            "author": {
                "name": "Amaresh Bingumalla, Thinh Kenny Vu, Gabriel Wang, Arun Vasudevan in collaboration with Dipankar Mazumdar"
            },
            "tags": [
                "Apache Hudi",
                "Peloton",
                "Community"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2025/07/15/PayU-built-a-secure-enterprise-AI-assistant",
            "content_html": "<span>Redirecting... please wait!! <!-- -->or click <a href=\"https://aws.amazon.com/blogs/machine-learning/how-payu-built-a-secure-enterprise-ai-assistant-using-amazon-bedrock/\">here</a></span>",
            "url": "https://hudi.apache.org/blog/2025/07/15/PayU-built-a-secure-enterprise-AI-assistant",
            "title": "How PayU built a secure enterprise AI assistant using Amazon Bedrock",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2025-07-15T00:00:00.000Z",
            "author": {
                "name": "Deepesh Dhapola, Mudit Chopra, Rahmat Khan, Rahul Ghosh, Saikat Dey, and Sandeep Kumar Veerlapati"
            },
            "tags": [
                "blog",
                "Apache Hudi",
                "AWS"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2025/07/10/building-a-rag-based-ai-recommender",
            "content_html": "<span>Redirecting... please wait!! <!-- -->or click <a href=\"https://blog.datumagic.ai/p/building-a-rag-based-ai-recommender\">here</a></span>",
            "url": "https://hudi.apache.org/blog/2025/07/10/building-a-rag-based-ai-recommender",
            "title": "Building a RAG-based AI Recommender (Part 1/2)",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2025-07-10T00:00:00.000Z",
            "author": {
                "name": "Shiyan Xu"
            },
            "tags": [
                "blog",
                "Apache Hudi",
                "AI",
                "RAG",
                "Artificial Intelligence",
                "data lakehouse",
                "Lakehouse",
                "use-case",
                "datumagic"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2025/07/07/how-stifel-built-a-modern-data-platform-using-aws-glue-and-an-event-driven-domain-architecture",
            "content_html": "<span>Redirecting... please wait!! <!-- -->or click <a href=\"https://aws.amazon.com/blogs/big-data/how-stifel-built-a-modern-data-platform-using-aws-glue-and-an-event-driven-domain-architecture/\">here</a></span>",
            "url": "https://hudi.apache.org/blog/2025/07/07/how-stifel-built-a-modern-data-platform-using-aws-glue-and-an-event-driven-domain-architecture",
            "title": "How Stifel built a modern data platform using AWS Glue and an event-driven domain architecture",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2025-07-07T00:00:00.000Z",
            "author": {
                "name": "Amit Maindola and Srinivas Kandi, Hossein Johari, Ahmad Rawashdeh, Lei Meng"
            },
            "tags": [
                "blog",
                "Apache Hudi",
                "aws",
                "AWS Glue",
                "AWS Blogs",
                "Amazon EMR",
                "AWS Lake Formation",
                "Data Governance",
                "Lakehouse",
                "use-case",
                "det"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2025/07/03/why-uber-built-hudi-the-strategic-decision-behind-a-custom-table-format",
            "content_html": "<span>Redirecting... please wait!! <!-- -->or click <a href=\"https://thamizhelango.medium.com/why-uber-built-hudi-the-strategic-decision-behind-a-custom-table-format-f57db68b0cb9\">here</a></span>",
            "url": "https://hudi.apache.org/blog/2025/07/03/why-uber-built-hudi-the-strategic-decision-behind-a-custom-table-format",
            "title": "Why Uber Built Hudi: The Strategic Decision Behind a Custom Table Format",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2025-07-03T00:00:00.000Z",
            "author": {
                "name": "ThamizhElango Natarajan"
            },
            "tags": [
                "blog",
                "Apache Hudi",
                "Apache Iceberg",
                "Lakehouse",
                "use-case",
                "Uber",
                "det"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2025/07/02/Lakehouse-Architecture-apache-hudi-and-apache-iceberg",
            "content_html": "<span>Redirecting... please wait!! <!-- -->or click <a href=\"https://www.linkedin.com/pulse/lakehouse-architecture-apache-hudi-iceberg-becloudready-4b1ac/\">here</a></span>",
            "url": "https://hudi.apache.org/blog/2025/07/02/Lakehouse-Architecture-apache-hudi-and-apache-iceberg",
            "title": "Lakehouse Architecture - Apache Hudi and Apache Iceberg",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2025-07-02T00:00:00.000Z",
            "author": {
                "name": "beCloudReady"
            },
            "tags": [
                "blog",
                "Apache Hudi",
                "Apache Iceberg",
                "Lakehouse",
                "use-case",
                "det"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2025/06/30/uber-hudi",
            "content_html": "<div class=\"theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success\"><div class=\"admonitionHeading_Gvgb\"><span class=\"admonitionIcon_Rf37\"><svg viewBox=\"0 0 12 16\"><path fill-rule=\"evenodd\" d=\"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z\"></path></svg></span>TL;DR</div><div class=\"admonitionContent_BuS1\"><p>Uber’s trip and order collection pipelines grew highly complex, with long runtimes, massive DAGs, and rigid SQL logic that hampered scalability and maintainability. By adopting Apache Hudi, Uber re-architected these pipelines to enable incremental processing, custom merge behavior, and rule-based functional transformations. This reduced runtime from 20 hours to 4 hours, improved test coverage to 95%, cut costs by 60%, and delivered a composable, explainable, and scalable data workflow architecture.</p></div></div>\n<p>At Uber, the Core Services Data Engineering team supports a wide range of use cases across products like Uber Mobility and Uber Eats. One critical use case is computing the collection - the net payable amount - from a trip or an order. While this sounds straightforward at first, it quickly becomes a complex data problem when you factor in real-world scenarios like refunds, tips, driver disputes, location updates, and settlement adjustments across multiple verticals.</p>\n<p>To solve this problem at scale, Uber re-architected their pipelines using <a href=\"https://hudi.apache.org/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">Apache Hudi</a> to enable low-latency, incremental, and rule-based processing. This post outlines the challenges they faced, the architectural shifts they made, and the measurable outcomes they achieved in production.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"the-challenge-scale-latency-and-complexity\">The Challenge: Scale, Latency, and Complexity<a href=\"https://hudi.apache.org/blog/2025/06/30/uber-hudi#the-challenge-scale-latency-and-complexity\" class=\"hash-link\" aria-label=\"Direct link to The Challenge: Scale, Latency, and Complexity\" title=\"Direct link to The Challenge: Scale, Latency, and Complexity\" translate=\"no\">​</a></h2>\n<img src=\"https://hudi.apache.org/assets/images/blog/figure2_uber.png\" alt=\"challenge\" width=\"800\" align=\"middle\">\n<p>Our original data pipelines were processing nearly 90 million records a day, but the nature of updates made them inefficient. For instance, a trip taken three years ago could still be updated due to a late settlement. Our statistical analysis showed most updates occur within 180 days, so we designed the system to read and write a 180-day window every day - leading to severe read and write amplification.</p>\n<p>The pipeline itself was a massive DAG with over 50–60 tasks, taking close to 20 hours to complete. These long runtimes made recovery difficult and introduced operational risks. Making a change meant tracing the logic across this sprawling DAG, which affected developer productivity and increased the chances of regressions.</p>\n<p>Despite the large window, we still missed updates that fell outside the 180-day mark, leading to data quality issues. The long development cycles and heavy debugging effort further hindered our ability to iterate and maintain the system.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"rigid-sql-and-tight-coupling\">Rigid SQL and Tight Coupling<a href=\"https://hudi.apache.org/blog/2025/06/30/uber-hudi#rigid-sql-and-tight-coupling\" class=\"hash-link\" aria-label=\"Direct link to Rigid SQL and Tight Coupling\" title=\"Direct link to Rigid SQL and Tight Coupling\" translate=\"no\">​</a></h2>\n<p>Digging deeper, we identified multiple underlying causes. The pipeline relied heavily on SQL for all transformations. But expressing the evolving business rules for different Uber products in SQL was limiting. The logic had grown too complex to be managed effectively, and granular transformations led to a proliferation of intermediate stages. This made unit testing and debugging difficult, and the absence of structured logging made observability poor.</p>\n<img src=\"https://hudi.apache.org/assets/images/blog/figure3_uber.png\" alt=\"redshift\" width=\"800\" align=\"middle\">\n<p>Additionally, data and logic were tightly coupled. The system often required joining tables at very fine granularities, introducing redundancy and making logic harder to reason about. Complex joins, table scans, and late-arriving data amplified processing costs. It was also difficult to trace how a specific row was transformed through the DAG, making explainability a real challenge.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"how-we-solved-it\">How We Solved It?<a href=\"https://hudi.apache.org/blog/2025/06/30/uber-hudi#how-we-solved-it\" class=\"hash-link\" aria-label=\"Direct link to How We Solved It?\" title=\"Direct link to How We Solved It?\" translate=\"no\">​</a></h2>\n<ol>\n<li class=\"\"><strong>Solving Read Amplification</strong></li>\n</ol>\n<p>The first step in addressing inefficiencies was eliminating the brute-force strategy of scanning and processing a 180-day window of data on every pipeline run. With the help of Apache Hudi’s <a href=\"https://hudi.apache.org/docs/table_types#incremental-queries\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\"><em>incremental</em> <em>read</em></a> capabilities, we restructured the ingestion layer to read only the records that had mutated since the last checkpoint.</p>\n<img src=\"https://hudi.apache.org/assets/images/blog/fig4_uber.png\" alt=\"redshift\" width=\"800\" align=\"middle\">\n<p>We introduced an intermediate Hudi table that consolidated all related records for a trip or order into a single row, using complex data types such as structs, lists, and maps. This model allowed us to capture the complete state of a trip - including all updates, tips, disputes, and refunds in one place, without scattering information across multiple joins.</p>\n<p>By using this intermediate table as the foundation, all downstream logic could operate on change-driven inputs. The result was a pipeline that avoided unnecessary scans, improved correctness by processing all real changes (not just those in a time window), and reduced overall I/O dramatically.</p>\n<ol start=\"2\">\n<li class=\"\"><strong>Eliminating Self Joins with Custom Payloads</strong></li>\n</ol>\n<p>Self joins - especially for reconciling updates to the same trip were one of the costliest operations in our original pipeline.</p>\n<img src=\"https://hudi.apache.org/assets/images/blog/fig5_uber.png\" alt=\"redshift\" width=\"800\" align=\"middle\">\n<p>To solve this, we implemented a custom Hudi payload class that allows us to control how updates are applied during the merge phase. This class overrides methods such as <code>combineAndGetUpdateValue</code> and <code>getInsertValue</code>, and executes the merge logic as part of the write path, eliminating the need for a full table scan or shuffle.</p>\n<p>This approach helped us efficiently handle updates to complex, nested records in the intermediate Hudi table, and dramatically reduced the cost associated with self joins.</p>\n<ol start=\"3\">\n<li class=\"\"><strong>Simplifying Processing with a Rule-Based Framework</strong></li>\n</ol>\n<p>To move away from the rigidity of SQL, we designed a rule engine framework based on functional programming principles.</p>\n<p>Instead of expressing business logic as large, monolithic SQL queries, we cast each input row (from the intermediate table) into a strongly typed object (e.g., a Trip object). These objects were then passed through a series of declarative rules - each consisting of a condition and an action.</p>\n<img src=\"https://hudi.apache.org/assets/images/blog/fig6_uber.png\" alt=\"redshift\" width=\"800\" align=\"middle\">\n<p>This framework was implemented as a custom <a href=\"https://hudi.apache.org/docs/hoodie_streaming_ingestion#transformers\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\"><em>transformer</em></a> plugged into <a href=\"https://hudi.apache.org/docs/hoodie_streaming_ingestion\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">HudiStreamer</a>. The transformer intercepts the ingested data, applies the rule engine logic, and emits the transformed object to the final Hudi output table. We also built in capabilities for:</p>\n<ul>\n<li class=\"\">Logging and observability (for metrics and debugging)</li>\n<li class=\"\">Unreachable state detection (flagging invalid rows)</li>\n<li class=\"\">Unit testing support for each rule independently</li>\n</ul>\n<p>This architecture replaced the huge DAG with modular, testable, and composable rule definitions, dramatically improving developer productivity and data pipeline clarity.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"final-architecture\">Final Architecture<a href=\"https://hudi.apache.org/blog/2025/06/30/uber-hudi#final-architecture\" class=\"hash-link\" aria-label=\"Direct link to Final Architecture\" title=\"Direct link to Final Architecture\" translate=\"no\">​</a></h2>\n<img src=\"https://hudi.apache.org/assets/images/blog/fig7_uber.png\" alt=\"redshift\" width=\"800\" align=\"middle\">\n<p>The redesigned system follows a clean, composable structure:</p>\n<ul>\n<li class=\"\">Incremental ingestion from the data lake is done using HudiStreamer, which writes to an intermediate Hudi table.</li>\n<li class=\"\">The intermediate table consolidates all records for a trip using complex types, serving as the central input for downstream processing.</li>\n<li class=\"\">A custom Transformer intercepts the records, casts them into typed domain objects, and passes them through a rule engine.</li>\n<li class=\"\">The rule engine applies business logic declaratively and emits fully processed objects.</li>\n<li class=\"\">The output is written to a final Hudi table that supports efficient, incremental consumption.</li>\n</ul>\n<p>This design eliminates redundant scans, reduces shuffle overhead, enables full test coverage, and offers detailed observability across all transformation stages.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"the-wins-with-hudi\">The Wins with Hudi<a href=\"https://hudi.apache.org/blog/2025/06/30/uber-hudi#the-wins-with-hudi\" class=\"hash-link\" aria-label=\"Direct link to The Wins with Hudi\" title=\"Direct link to The Wins with Hudi\" translate=\"no\">​</a></h2>\n<p>The improvements were substantial and measurable:</p>\n<ul>\n<li class=\"\">Runtime reduced from ~20 hours to ~4 hours (~75% improvement)</li>\n<li class=\"\">Test coverage increased to 95% for transformation logic</li>\n<li class=\"\">Single run cost reduced by 60%</li>\n<li class=\"\">Improved data completeness, processing all updates—not just those in a statistical window</li>\n<li class=\"\">Reusable and modular logic, reducing DAG complexity</li>\n<li class=\"\">Higher developer productivity, with isolated unit testing and simplified debugging</li>\n<li class=\"\">Improved self-join performance through custom payloads</li>\n<li class=\"\">A generic rule engine design, portable across Spark and Flink</li>\n</ul>\n<p>Apache Hudi has been central to Nexus’ success, providing the core data lake storage layer for scalable ingestion, updates, and metadata management. It enables fast, incremental updates at massive scale while maintaining transactional guarantees on top of Amazon S3.</p>\n<h2 class=\"anchor anchorTargetStickyNavbar_Vzrq\" id=\"conclusion\">Conclusion<a href=\"https://hudi.apache.org/blog/2025/06/30/uber-hudi#conclusion\" class=\"hash-link\" aria-label=\"Direct link to Conclusion\" title=\"Direct link to Conclusion\" translate=\"no\">​</a></h2>\n<p>By redesigning the system around Apache Hudi and adopting functional, rule-based processing, Uber was able to transform a brittle, long-running pipeline into a maintainable and efficient architecture. The changes allowed them to scale their data workflows to meet the needs of complex, multi-product use cases without compromising on performance, observability, or data quality.</p>\n<p>This work highlights the power of pairing the right storage format with a principled architectural approach. Apache Hudi was instrumental in helping achieve these outcomes and continues to play a key role in Uber’s evolving data platform.</p>\n<p>This blog is based on Uber’s presentation at the Apache Hudi Community Sync. If you are interested in watching the recorded version of the video, you can find it <a href=\"https://www.youtube.com/watch?v=VpdimpH_nsI\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"\">here</a>.</p>\n<hr>",
            "url": "https://hudi.apache.org/blog/2025/06/30/uber-hudi",
            "title": "Scaling Complex Data Workflows at Uber Using Apache Hudi",
            "summary": "Uber’s trip and order collection pipelines grew highly complex, with long runtimes, massive DAGs, and rigid SQL logic that hampered scalability and maintainability. By adopting Apache Hudi, Uber re-architected these pipelines to enable incremental processing, custom merge behavior, and rule-based functional transformations. This reduced runtime from 20 hours to 4 hours, improved test coverage to 95%, cut costs by 60%, and delivered a composable, explainable, and scalable data workflow architecture.",
            "date_modified": "2025-06-30T00:00:00.000Z",
            "author": {
                "name": "Ankit Shrivastava in collaboration with Dipankar"
            },
            "tags": [
                "Apache Hudi",
                "Uber",
                "Community"
            ]
        }
    ]
}