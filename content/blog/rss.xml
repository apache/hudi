<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>Apache Hudi: User-Facing Analytics</title>
        <link>https://hudi.apache.org/blog</link>
        <description>Apache Hudi! Blog</description>
        <lastBuildDate>Mon, 23 Aug 2021 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <item>
            <title><![CDATA[Asynchronous Clustering using Hudi]]></title>
            <link>https://hudi.apache.org/blog/2021/08/23/async-clustering</link>
            <guid>Asynchronous Clustering using Hudi</guid>
            <pubDate>Mon, 23 Aug 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[In one of the previous blog posts, we introduced a new]]></description>
        </item>
        <item>
            <title><![CDATA[Reliable ingestion from AWS S3 using Hudi]]></title>
            <link>https://hudi.apache.org/blog/2021/08/23/s3-events-source</link>
            <guid>Reliable ingestion from AWS S3 using Hudi</guid>
            <pubDate>Mon, 23 Aug 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[In this post we will talk about a new deltastreamer source which reliably and efficiently processes new data files as they arrive in AWS S3.]]></description>
        </item>
        <item>
            <title><![CDATA[Improving Marker Mechanism in Apache Hudi]]></title>
            <link>https://hudi.apache.org/blog/2021/08/18/improving-marker-mechanism</link>
            <guid>Improving Marker Mechanism in Apache Hudi</guid>
            <pubDate>Wed, 18 Aug 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Hudi supports fully automatic cleanup of uncommitted data on storage during its write operations. Write operations in an Apache Hudi table use markers to efficiently track the data files written to storage. In this blog, we dive into the design of the existing direct marker file mechanism and explain its performance problems on cloud storage like AWS S3 for]]></description>
        </item>
        <item>
            <title><![CDATA[Adding support for Virtual Keys in Hudi]]></title>
            <link>https://hudi.apache.org/blog/2021/08/18/virtual-keys</link>
            <guid>Adding support for Virtual Keys in Hudi</guid>
            <pubDate>Wed, 18 Aug 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Apache Hudi helps you build and manage data lakes with different table types, config knobs to cater to everyone's need.]]></description>
        </item>
        <item>
            <title><![CDATA[Schema evolution with DeltaStreamer using KafkaSource]]></title>
            <link>https://hudi.apache.org/blog/2021/08/16/kafka-custom-deserializer</link>
            <guid>Schema evolution with DeltaStreamer using KafkaSource</guid>
            <pubDate>Mon, 16 Aug 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[The schema used for data exchange between services can change change rapidly with new business requirements.]]></description>
        </item>
        <item>
            <title><![CDATA[Apache Hudi - The Data Lake Platform]]></title>
            <link>https://hudi.apache.org/blog/2021/07/21/streaming-data-lake-platform</link>
            <guid>Apache Hudi - The Data Lake Platform</guid>
            <pubDate>Wed, 21 Jul 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[As early as 2016, we set out a bold, new vision reimagining batch data processing through a new “incremental” data processing stack - alongside the existing batch and streaming stacks.]]></description>
        </item>
        <item>
            <title><![CDATA[Employing correct configurations for Hudi's cleaner table service]]></title>
            <link>https://hudi.apache.org/blog/2021/06/10/employing-right-configurations-for-hudi-cleaner</link>
            <guid>Employing correct configurations for Hudi's cleaner table service</guid>
            <pubDate>Thu, 10 Jun 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Apache Hudi provides snapshot isolation between writers and readers. This is made possible by Hudi’s MVCC concurrency model. In this blog, we will explain how to employ the right configurations to manage multiple file versions. Furthermore, we will discuss mechanisms available to users on how to maintain just the required number of old file versions so that long running readers do not fail.]]></description>
        </item>
        <item>
            <title><![CDATA[Streaming Responsibly - How Apache Hudi maintains optimum sized files]]></title>
            <link>https://hudi.apache.org/blog/2021/03/01/hudi-file-sizing</link>
            <guid>Streaming Responsibly - How Apache Hudi maintains optimum sized files</guid>
            <pubDate>Mon, 01 Mar 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Apache Hudi is a data lake platform technology that provides several functionalities needed to build and manage data lakes.]]></description>
        </item>
        <item>
            <title><![CDATA[Apache Hudi Key Generators]]></title>
            <link>https://hudi.apache.org/blog/2021/02/13/hudi-key-generators</link>
            <guid>Apache Hudi Key Generators</guid>
            <pubDate>Sat, 13 Feb 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Every record in Hudi is uniquely identified by a primary key, which is a pair of record key and partition path where]]></description>
        </item>
        <item>
            <title><![CDATA[Optimize Data lake layout using Clustering in Apache Hudi]]></title>
            <link>https://hudi.apache.org/blog/2021/01/27/hudi-clustering-intro</link>
            <guid>Optimize Data lake layout using Clustering in Apache Hudi</guid>
            <pubDate>Wed, 27 Jan 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Apache Hudi brings stream processing to big data, providing fresh data while being an order of magnitude efficient over traditional batch processing. In a data lake/warehouse, one of the key trade-offs is between ingestion speed and query performance. Data ingestion typically prefers small files to improve parallelism and make data available to queries as soon as possible. However, query performance degrades poorly with a lot of small files. Also, during ingestion, data is typically co-located based on arrival time. However, the query engines perform better when the data frequently queried is co-located together. In most architectures each of these systems tend to add optimizations independently to improve performance which hits limitations due to un-optimized data layouts. This blog introduces a new kind of table service called clustering [RFC-19] to reorganize data for improved query performance without compromising on ingestion speed.]]></description>
        </item>
        <item>
            <title><![CDATA[Building High-Performance Data Lake Using Apache Hudi and Alluxio at T3Go]]></title>
            <link>https://hudi.apache.org/blog/2020/12/01/high-perf-data-lake-with-hudi-and-alluxio-t3go</link>
            <guid>Building High-Performance Data Lake Using Apache Hudi and Alluxio at T3Go</guid>
            <pubDate>Tue, 01 Dec 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[T3Go  is China’s first platform for smart travel based on the Internet of Vehicles. In this article, Trevor Zhang and Vino Yang from T3Go describe the evolution of their data lake architecture, built on cloud-native or open-source technologies including Alibaba OSS, Apache Hudi, and Alluxio. Today, their data lake stores petabytes of data, supporting hundreds of pipelines and tens of thousands of tasks daily. It is essential for business units at T3Go including Data Warehouse, Internet of Vehicles, Order Dispatching, Machine Learning, and self-service query analysis.]]></description>
        </item>
        <item>
            <title><![CDATA[Employing the right indexes for fast updates, deletes in Apache Hudi]]></title>
            <link>https://hudi.apache.org/blog/2020/11/11/hudi-indexing-mechanisms</link>
            <guid>Employing the right indexes for fast updates, deletes in Apache Hudi</guid>
            <pubDate>Wed, 11 Nov 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[Apache Hudi employs an index to locate the file group, that an update/delete belongs to. For Copy-On-Write tables, this enables]]></description>
        </item>
        <item>
            <title><![CDATA[Apply record level changes from relational databases to Amazon S3 data lake using Apache Hudi on Amazon EMR and AWS Database Migration Service]]></title>
            <link>https://hudi.apache.org/blog/2020/10/19/hudi-meets-aws-emr-and-aws-dms</link>
            <guid>Apply record level changes from relational databases to Amazon S3 data lake using Apache Hudi on Amazon EMR and AWS Database Migration Service</guid>
            <pubDate>Mon, 19 Oct 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[This blog published by AWS shows how to build a CDC pipeline that captures data from an Amazon Relational Database Service (Amazon RDS) for MySQL database using AWS Database Migration Service (AWS DMS) and applies those changes to a dataset in Amazon S3 using Apache Hudi on Amazon EMR.]]></description>
        </item>
        <item>
            <title><![CDATA[Apache Hudi meets Apache Flink]]></title>
            <link>https://hudi.apache.org/blog/2020/10/15/apache-hudi-meets-apache-flink</link>
            <guid>Apache Hudi meets Apache Flink</guid>
            <pubDate>Thu, 15 Oct 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[Apache Hudi (Hudi for short) is a data lake framework created at Uber. Hudi joined the Apache incubator for incubation in January 2019, and was promoted to the top Apache project in May 2020. It is one of the most popular data lake frameworks.]]></description>
        </item>
        <item>
            <title><![CDATA[How nClouds Helps Accelerate Data Delivery with Apache Hudi on Amazon EMR]]></title>
            <link>https://hudi.apache.org/blog/2020/10/06/cdc-solution-using-hudi-by-nclouds</link>
            <guid>How nClouds Helps Accelerate Data Delivery with Apache Hudi on Amazon EMR</guid>
            <pubDate>Tue, 06 Oct 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[This blog published by nClouds in partnership with AWS shows how to build a CDC pipeline using Apache Hudi on Amazon EMR and other managed services like Amazon RDS and AWS DMS, including Amazon QuickSight for data visualization.]]></description>
        </item>
        <item>
            <title><![CDATA[Ingest multiple tables using Hudi]]></title>
            <link>https://hudi.apache.org/blog/2020/08/22/ingest-multiple-tables-using-hudi</link>
            <guid>Ingest multiple tables using Hudi</guid>
            <pubDate>Sat, 22 Aug 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[When building a change data capture pipeline for already existing or newly created relational databases, one of the most common problems that one faces is simplifying the onboarding process for multiple tables. Ingesting multiple tables to Hudi dataset at a single go is now possible using HoodieMultiTableDeltaStreamer class which is a wrapper on top of the more popular HoodieDeltaStreamer class. Currently HoodieMultiTableDeltaStreamer supports COPY_ON_WRITE storage type only and the ingestion is done in a sequential way.]]></description>
        </item>
        <item>
            <title><![CDATA[Async Compaction Deployment Models]]></title>
            <link>https://hudi.apache.org/blog/2020/08/21/async-compaction-deployment-model</link>
            <guid>Async Compaction Deployment Models</guid>
            <pubDate>Fri, 21 Aug 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[We will look at different deployment models for executing compactions asynchronously.]]></description>
        </item>
        <item>
            <title><![CDATA[Efficient Migration of Large Parquet Tables to Apache Hudi]]></title>
            <link>https://hudi.apache.org/blog/2020/08/20/efficient-migration-of-large-parquet-tables</link>
            <guid>Efficient Migration of Large Parquet Tables to Apache Hudi</guid>
            <pubDate>Thu, 20 Aug 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[We will look at how to migrate a large parquet table to Hudi without having to rewrite the entire dataset.]]></description>
        </item>
        <item>
            <title><![CDATA[Incremental Processing on the Data Lake]]></title>
            <link>https://hudi.apache.org/blog/2020/08/18/hudi-incremental-processing-on-data-lakes</link>
            <guid>Incremental Processing on the Data Lake</guid>
            <pubDate>Tue, 18 Aug 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[NOTE: This article is a translation of the infoq.cn article, found here, with minor edits]]></description>
        </item>
        <item>
            <title><![CDATA[Monitor Hudi metrics with Datadog]]></title>
            <link>https://hudi.apache.org/blog/2020/05/28/monitoring-hudi-metrics-with-datadog</link>
            <guid>Monitor Hudi metrics with Datadog</guid>
            <pubDate>Thu, 28 May 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[Availability]]></description>
        </item>
        <item>
            <title><![CDATA[Apache Hudi Support on Apache Zeppelin]]></title>
            <link>https://hudi.apache.org/blog/2020/04/27/apache-hudi-apache-zepplin</link>
            <guid>Apache Hudi Support on Apache Zeppelin</guid>
            <pubDate>Mon, 27 Apr 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[1. Introduction]]></description>
        </item>
        <item>
            <title><![CDATA[Export Hudi datasets as a copy or as different formats]]></title>
            <link>https://hudi.apache.org/blog/2020/03/22/exporting-hudi-datasets</link>
            <guid>Export Hudi datasets as a copy or as different formats</guid>
            <pubDate>Sun, 22 Mar 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[Copy to Hudi dataset]]></description>
        </item>
        <item>
            <title><![CDATA[Change Capture Using AWS Database Migration Service and Hudi]]></title>
            <link>https://hudi.apache.org/blog/2020/01/20/change-capture-using-aws</link>
            <guid>Change Capture Using AWS Database Migration Service and Hudi</guid>
            <pubDate>Mon, 20 Jan 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[One of the core use-cases for Apache Hudi is enabling seamless, efficient database ingestion to your data lake. Even though a lot has been talked about and even users already adopting this model, content on how to go about this is sparse.]]></description>
        </item>
        <item>
            <title><![CDATA[Delete support in Hudi]]></title>
            <link>https://hudi.apache.org/blog/2020/01/15/delete-support-in-hudi</link>
            <guid>Delete support in Hudi</guid>
            <pubDate>Wed, 15 Jan 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[Deletes are supported at a record level in Hudi with 0.5.1 release. This blog is a "how to" blog on how to delete records in hudi. Deletes can be done with 3 flavors: Hudi RDD APIs, with Spark data source and with DeltaStreamer.]]></description>
        </item>
        <item>
            <title><![CDATA[Ingesting Database changes via Sqoop/Hudi]]></title>
            <link>https://hudi.apache.org/blog/2019/09/09/ingesting-database-changes</link>
            <guid>Ingesting Database changes via Sqoop/Hudi</guid>
            <pubDate>Mon, 09 Sep 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[Very simple in just 2 steps.]]></description>
        </item>
        <item>
            <title><![CDATA[Registering sample dataset to Hive via beeline]]></title>
            <link>https://hudi.apache.org/blog/2019/05/14/registering-dataset-to-hive</link>
            <guid>Registering sample dataset to Hive via beeline</guid>
            <pubDate>Tue, 14 May 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[Hudi hive sync tool typically handles registration of the dataset into Hive metastore. In case, there are issues with quickstart around this, following page shows commands that can be used to do this manually via beeline.]]></description>
        </item>
        <item>
            <title><![CDATA[Big Batch vs Incremental Processing]]></title>
            <link>https://hudi.apache.org/blog/2019/03/07/batch-vs-incremental</link>
            <guid>Big Batch vs Incremental Processing</guid>
            <pubDate>Thu, 07 Mar 2019 00:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Hudi entered Apache Incubator]]></title>
            <link>https://hudi.apache.org/blog/2019/01/18/asf-incubation</link>
            <guid>Hudi entered Apache Incubator</guid>
            <pubDate>Fri, 18 Jan 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[In the coming weeks, we will be moving in our new home on the Apache Incubator.]]></description>
        </item>
        <item>
            <title><![CDATA[Connect with us at Strata San Jose March 2017]]></title>
            <link>https://hudi.apache.org/blog/2016/12/30/strata-talk-2017</link>
            <guid>Connect with us at Strata San Jose March 2017</guid>
            <pubDate>Fri, 30 Dec 2016 00:00:00 GMT</pubDate>
            <description><![CDATA[We will be presenting Hudi & general concepts around how incremental processing works at Uber.]]></description>
        </item>
    </channel>
</rss>