<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Apache Hudi: User-Facing Analytics</title>
        <link>https://hudi.apache.org/blog</link>
        <description>Apache Hudi Blog</description>
        <lastBuildDate>Wed, 17 Sep 2025 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Automatic Record Key Generation in Apache Hudi]]></title>
            <link>https://hudi.apache.org/blog/2025/09/17/hudi-auto-gen-keys</link>
            <guid>https://hudi.apache.org/blog/2025/09/17/hudi-auto-gen-keys</guid>
            <pubDate>Wed, 17 Sep 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[In database systems, the primary key is a foundational design principle for managing data at the record level. Its function is to provide each record with a unique and stable logical identifier, which decouples the record's identity from its physical location on storage. While using direct physical address pointers (e.g., position inside a file being used as a key) can be convenient, the physical address can change when records are moved around within the table for things like clustering or z-ordering (called out here).]]></description>
            <content:encoded><![CDATA[<p>In database systems, the primary key is a foundational design principle for managing data at the record level. Its function is to provide each record with a unique and stable logical identifier, which decouples the record's identity from its physical location on storage. While using direct physical address pointers (e.g., position inside a file being used as a key) can be convenient, the physical address can change when records are moved around within the table for things like clustering or z-ordering (<a href="https://x.com/apachehudi/status/1641572485325017089" target="_blank" rel="noopener noreferrer">called out here</a>).</p>
<p>By using a primary key that is stable across record movement, a system can efficiently perform operations like updates and deletes, enabling critical features like relational integrity.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="first-class-support-of-record-keys">First-Class Support of Record Keys<a href="https://hudi.apache.org/blog/2025/09/17/hudi-auto-gen-keys#first-class-support-of-record-keys" class="hash-link" aria-label="Direct link to First-Class Support of Record Keys" title="Direct link to First-Class Support of Record Keys">​</a></h2>
<p>Apache Hudi was the first lakehouse storage project to introduce the notion of record keys. For mutable workloads, this addressed a significant architectural challenge. In a typical data lake table, updating records usually required rewriting entire partitions—a process that is slow and expensive. By supporting the record key as the stable identifier for every record, Hudi offered unique and advanced capabilities among lakehouse frameworks:</p>
<ul>
<li>Hudi supports <a href="https://hudi.apache.org/blog/2023/11/01/record-level-index/" target="_blank" rel="noopener noreferrer">record-level indexing</a> for directly locating records in <a href="https://hudi.apache.org/docs/storage_layouts" target="_blank" rel="noopener noreferrer">file groups</a> for highly efficient upserts and queries, and <a href="https://hudi.apache.org/blog/2025/04/02/secondary-index/" target="_blank" rel="noopener noreferrer">secondary indexes</a> that enable performant lookups for predicates on non-record key fields.</li>
<li>Hudi implements <a href="https://hudi.apache.org/blog/2025/03/03/record-mergers-in-hudi/" target="_blank" rel="noopener noreferrer">merge modes</a>, standardizing record-merging semantics to handle requirements such as unordered events, duplicate records, and custom merge logic.</li>
<li>By materializing record keys along with other <a href="https://www.onehouse.ai/blog/hudi-metafields-demystified" target="_blank" rel="noopener noreferrer">record-level meta-fields</a>, Hudi unlocks features such as efficient <a href="https://hudi.apache.org/blog/2024/07/30/data-lake-cdc/" target="_blank" rel="noopener noreferrer">change data capture (CDC)</a> that serves record-level change streams, near-infinite history for time-travel queries, and the <a href="https://hudi.apache.org/docs/clustering" target="_blank" rel="noopener noreferrer">clustering table service</a> that can significantly optimize file sizes.</li>
</ul>
<p><img decoding="async" loading="lazy" alt="Replicating operational databases to a Hudi lakehouse using CDC" src="https://hudi.apache.org/assets/images/2025-09-17-hudi-auto-gen-keys.fig1-fb5004b3f1cd1832795f39f6c7255411.jpg" width="647" height="351" class="img_ev3q"></p>
<p>Append-only writes are very common in the data lakehouse, such as ingesting application logs streamed continuously from numerous servers or capturing clickstream events from user interactions on a website. Even for this kind of scenario, having record keys is beneficial in scenarios like concurrently running data-fixing backfill writers (e.g., a GDPR deletion process) with ongoing writers to the same table. Without record keys, engineers typically had to coordinate the backfill to run on different partitions than the active writes to avoid conflicts. With record keys and the support provided by Hudi’s <a href="https://hudi.apache.org/docs/concurrency_control" target="_blank" rel="noopener noreferrer">concurrency control</a> and merge modes, this restriction can be lifted, with Hudi handling the concurrent writes properly.</p>
<p>Given the advantages of supporting record keys, Hudi required users to set one or multiple record key fields when creating a table prior to <a href="https://hudi.apache.org/releases/release-0.14.0" target="_blank" rel="noopener noreferrer">release 0.14</a>. However, this requirement created friction for users in cases where there were no natural record keys in the incoming stream for simply setting another config variable. Even for users who understood the benefits of record keys, they had to put careful thought into their record key generation to ensure uniqueness and idempotency. The initial friction of generating keys was a barrier to adoption for teams who simply wanted to land their append-only workloads in a lakehouse with as few lines of code and configuration as possible.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="automatic-key-generation">Automatic Key Generation<a href="https://hudi.apache.org/blog/2025/09/17/hudi-auto-gen-keys#automatic-key-generation" class="hash-link" aria-label="Direct link to Automatic Key Generation" title="Direct link to Automatic Key Generation">​</a></h2>
<p>With the release of version 0.14 (this is actually old news), Hudi has introduced automatic record key generation, a feature designed to simplify the user experience with append-only writes. This enhancement eliminates the mandatory requirement to specify record key fields for every write operation.</p>
<p><img decoding="async" loading="lazy" alt="Hudi&amp;#39;s auto key generation for append-only writes" src="https://hudi.apache.org/assets/images/2025-09-17-hudi-auto-gen-keys.fig2-760500605f2a1ecfa253caffaa013c4a.jpg" width="639" height="316" class="img_ev3q"></p>
<p>Now, to perform append-only writes, you can simply omit the <code>primaryKey</code> property in <code>CREATE TABLE</code> statements (see the example below) or skip setting the <code>hoodie.datasource.write.recordkey.field</code> or <code>hoodie.table.recordkey.fields</code> configurations.</p>
<div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">TABLE</span><span class="token plain"> hudi_table </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ts </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">BIGINT</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    uuid STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    rider STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    driver STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    fare </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">DOUBLE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    city STRING</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">USING</span><span class="token plain"> HUDI</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">PARTITIONED </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">BY</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">city</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><br></span></code></pre></div></div>
<p>In this example, you’re creating a Copy-on-Write table partitioned by <code>city</code>. Because the <code>primaryKey</code> property is not present, Hudi automatically detects the omission and engages the auto key generation feature.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="design-considerations">Design Considerations<a href="https://hudi.apache.org/blog/2025/09/17/hudi-auto-gen-keys#design-considerations" class="hash-link" aria-label="Direct link to Design Considerations" title="Direct link to Design Considerations">​</a></h3>
<p>Designing a key generation mechanism that operates efficiently at petabyte scale requires careful thought. We established five core requirements for the auto-generated keys:</p>
<ol>
<li><strong>Global Uniqueness:</strong> Keys must be unique across the entire table to maintain the integrity of a primary key.</li>
<li><strong>Low Storage Footprint:</strong> The keys should be highly compressible to add minimal storage overhead.</li>
<li><strong>Computational Efficiency:</strong> The encoding and decoding process must be lightweight so as not to slow down the write process.</li>
<li><strong>Idempotency:</strong> The generation process must be resilient to task retries, producing the same key for the same record every time.</li>
<li><strong>Engine Agnostic:</strong> The logic must be reusable and implemented consistently across different execution engines like Spark and Flink.</li>
</ol>
<p>These principles guided the technical design. To align with primary key semantics, global uniqueness was non-negotiable. To minimize storage footprint, the generated keys needed to be compact and highly compressible, especially for tables with billions of records. The computational cost was also critical; any expensive operation would be amplified by the number of records, creating a significant performance overhead. Furthermore, in distributed systems where task failures and retries are common, the key generation process had to be idempotent—ensuring the same input record always produces the exact same key. Finally, the solution needed to be engine-agnostic to provide consistent behavior, whether data is written via Spark, Flink, or another supported engine.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="determining-the-format">Determining the Format<a href="https://hudi.apache.org/blog/2025/09/17/hudi-auto-gen-keys#determining-the-format" class="hash-link" aria-label="Direct link to Determining the Format" title="Direct link to Determining the Format">​</a></h3>
<p>Based on the requirements mentioned previously, we eliminated several common ID generation techniques. For instance, we cannot use simple auto-incrementing IDs for each batch of writes, as it will not satisfy global uniqueness in the table across different writes. We also rule out using the <code>monotonically_increasing_id</code> function in Spark, as it does not guarantee global uniqueness either. Furthermore, using such functions violates the rule of being engine-agnostic. We do not use random ID generation such as UUID (v4, v6, and v7) and ULID, which do not satisfy the idempotency requirement. The final format that we chose is a deterministic, composite key with the following structure:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">&lt;write action start time&gt;-&lt;workload partition ID&gt;-&lt;record sequence ID&gt;</span><br></span></code></pre></div></div>
<p>Each component serves a specific purpose:</p>
<ul>
<li><strong>Write Action Start Time:</strong> The timestamp from the Hudi timeline that marks the beginning of a write transaction.</li>
<li><strong>Workload Partition ID:</strong> An internal identifier that execution engines use to track the specific data split being processed by a given distributed write task.</li>
<li><strong>Record Sequence ID:</strong> A counter that uniquely identifies each record within that data split.</li>
</ul>
<p>Together, these three components—all readily accessible during the write process—form a record identifier that satisfies the requirements of global uniqueness, idempotency, and being engine-agnostic.</p>
<p>Next, we evaluate the generated keys against the requirements of low storage footprint and computational efficiency. The following tables highlight some experiment numbers based on the <a href="https://github.com/apache/hudi/blob/master/rfc/rfc-76/rfc-76.md" target="_blank" rel="noopener noreferrer">RFC document</a> of the auto key generation feature.</p>
<p>For storage efficiency, we compare the original strings with UUID v6/7, Base64, and ASCII encoding schemes:</p>
<table><thead><tr><th style="text-align:left">Format</th><th style="text-align:left">Uncompressed size (bytes)</th><th style="text-align:left">Compressed size (bytes)</th><th style="text-align:left">Compression ratio</th></tr></thead><tbody><tr><td style="text-align:left">Original string</td><td style="text-align:left">4,000,185</td><td style="text-align:left">244,373</td><td style="text-align:left">11.1</td></tr><tr><td style="text-align:left">UUID v6/7</td><td style="text-align:left">4,000,184</td><td style="text-align:left">1,451,897</td><td style="text-align:left">2.74</td></tr><tr><td style="text-align:left">Base64</td><td style="text-align:left">2,400,184</td><td style="text-align:left">202,095</td><td style="text-align:left">11.9</td></tr><tr><td style="text-align:left">ASCII</td><td style="text-align:left">1,900,185</td><td style="text-align:left">176,606</td><td style="text-align:left">10.8</td></tr></tbody></table>
<p>We also compare their compute efficiency using the original string format as the baseline:</p>
<table><thead><tr><th style="text-align:left">Format</th><th style="text-align:left">Average runtime (ms)</th><th style="text-align:left">Ratio to baseline</th></tr></thead><tbody><tr><td style="text-align:left">Original string</td><td style="text-align:left">0.00001</td><td style="text-align:left">1</td></tr><tr><td style="text-align:left">UUID v6/7</td><td style="text-align:left">0.0001</td><td style="text-align:left">10</td></tr><tr><td style="text-align:left">Base64</td><td style="text-align:left">0.004</td><td style="text-align:left">400</td></tr><tr><td style="text-align:left">ASCII</td><td style="text-align:left">0.004</td><td style="text-align:left">400</td></tr></tbody></table>
<p>Based on the micro-benchmarking results, UUID v6/7 resulted in a much larger and undesired compressed size compared to others. Base64 and ASCII encoding had a lower storage footprint compared to the original string, with around 17% and 28% reduction respectively. However, both Base64 and ASCII require 400x more CPU power for encoding than the original string format. Given that write performance is often more critical than marginal storage savings in high-throughput data systems, we opted for the original string format for auto-generating record keys.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="summary">Summary<a href="https://hudi.apache.org/blog/2025/09/17/hudi-auto-gen-keys#summary" class="hash-link" aria-label="Direct link to Summary" title="Direct link to Summary">​</a></h2>
<p>Hudi’s first-class support for record keys provides a database-like experience for lakehouses, enabling powerful features such as record-level indexing, merge modes, and CDC. The introduction of automatic record key generation thoughtfully extends the record key support, removing a barrier for teams performing append-only writes. By following the design principles of uniqueness, idempotency, and efficiency, the feature allows more users to easily adopt Hudi and benefit from its rich set of lakehouse capabilities without the initial overhead of manual key generation. This enhancement reinforces Hudi’s position as a versatile and user-friendly platform for building modern data lakehouses.</p>]]></content:encoded>
            <category>hudi</category>
            <category>record key generation</category>
            <category>database</category>
            <category>data lakehouse</category>
        </item>
        <item>
            <title><![CDATA[Building a RAG-based AI Recommender (2/2)]]></title>
            <link>https://hudi.apache.org/blog/2025/08/29/building-a-rag-based-ai-recommender-2</link>
            <guid>https://hudi.apache.org/blog/2025/08/29/building-a-rag-based-ai-recommender-2</guid>
            <pubDate>Fri, 29 Aug 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://blog.datumagic.ai/p/building-a-rag-based-ai-recommender-147">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>Apache Hudi</category>
            <category>AI</category>
            <category>RAG</category>
            <category>Artificial Intelligence</category>
            <category>data lakehouse</category>
            <category>Lakehouse</category>
            <category>use-case</category>
            <category>datumagic</category>
        </item>
        <item>
            <title><![CDATA[A Deep Dive on Merge-on-Read (MoR) in Lakehouse Table Formats]]></title>
            <link>https://hudi.apache.org/blog/2025/07/21/mor-comparison</link>
            <guid>https://hudi.apache.org/blog/2025/07/21/mor-comparison</guid>
            <pubDate>Mon, 21 Jul 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[TL;DR]]></description>
            <content:encoded><![CDATA[<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>TL;DR</p><ul>
<li>Merge-on-Read tables help manage updates on immutable files without constant rewrites.</li>
<li>Apache Hudi’s MoR tables, with delta logs, file groups, asynchronous compaction, and event-time merging, are well-suited for update-heavy, low-latency streaming and CDC workloads.</li>
<li>Iceberg and Delta Lake also support MoR, but with design differences around delete files and deletion vectors.</li>
</ul></div></div>
<p>As <a href="https://www.onehouse.ai/blog/open-table-formats-and-the-open-data-lakehouse-in-perspective" target="_blank" rel="noopener noreferrer">open table formats</a> like Apache Hudi, Apache Iceberg, and Delta Lake become foundational to modern data lakes, understanding how data is written and read becomes critical for designing high-performance pipelines. One such key dimension is the table's write mechanism, specifically, what happens when <em>updates or deletes</em> are made to these lakehouse tables.</p>
<p>This is where <a href="https://hudi.apache.org/docs/table_types#copy-on-write-table" target="_blank" rel="noopener noreferrer">Copy-on-Write (CoW)</a> and <a href="https://hudi.apache.org/docs/table_types#merge-on-read-table" target="_blank" rel="noopener noreferrer">Merge-on-Read (MoR)</a> table types come into play. These terms were popularized by <a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer">Apache Hudi</a>, in the <a href="https://www.uber.com/blog/hoodie/" target="_blank" rel="noopener noreferrer">original blog</a> from Uber Engineering, when the project was open-sourced in 2017. These strategies exist to overcome a fundamental limitation: data file formats like Parquet and ORC are immutable in nature. Therefore, any update or delete operation that is executed on these files (managed by a lakehouse table format) requires a specific way to deal with it - either by merging changes right away during writes, rewriting entire files (CoW) or maintaining a differential log or delete index that can  be merged at read time (MoR).</p>
<p>Viewed through the lens of the <a href="https://substack.com/home/post/p-159031300?utm_campaign=post&amp;utm_medium=web" target="_blank" rel="noopener noreferrer">RUM Conjecture</a> - which states that optimizing for two of Read, Update, and Memory inevitably requires trading off the third. CoW and MoR emerge as two natural design responses to the trade-offs in lakehouse table formats:</p>
<ul>
<li>
<p>Copy-on-Write tables optimize for read performance. They rewrite Parquet files entirely when a change is made, ensuring clean, columnar files with no extra merge logic at query time. This suits batch-style, read-optimized analytics workloads where write frequency is low.</p>
</li>
<li>
<p>Merge-on-Read, in contrast, introduces flexibility for write-intensive and latency-sensitive workloads by avoiding expensive writes. Instead of rewriting files for every change, MoR tables store updates in delta logs (Hudi), delete files (Iceberg V2), or deletion vectors (Delta Lake). Reads then stitch together the base data files with these changes to present an up-to-date view. This tradeoff favors streaming or near real-time workloads where low write latency is critical.</p>
</li>
</ul>
<p>Here is a generic comparison table between CoW and MoR tables.</p>
<table><thead><tr><th>Trade-Off</th><th>CoW</th><th>MoR</th></tr></thead><tbody><tr><td>Write latency</td><td>Higher</td><td>Lower</td></tr><tr><td>Query latency</td><td>Lower</td><td>Higher</td></tr><tr><td>Update cost</td><td>High</td><td>Low</td></tr><tr><td>File Size Guidance</td><td>Base files should be smaller to keep rewrites manageable</td><td>Base files can be larger, as updates don’t rewrite them directly</td></tr><tr><td>Read Amplification</td><td>Minimal - all changes are already materialized into base files</td><td>Higher - readers must combine base files with change logs or metadata (e.g., delete files or vectors)</td></tr><tr><td>Write Amplification</td><td>Higher - changes often rewrite full files, even for small updates</td><td>Lower - only incremental data (e.g., updates/deletes) is written as separate files or metadata</td></tr></tbody></table>
<p>In this blog, we will understand how various lakehouse table formats implement <strong>MoR</strong> strategy and how the design influences performance and other related factors.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-merge-on-read-works-across-table-formats">How Merge-on-Read Works Across Table Formats<a href="https://hudi.apache.org/blog/2025/07/21/mor-comparison#how-merge-on-read-works-across-table-formats" class="hash-link" aria-label="Direct link to How Merge-on-Read Works Across Table Formats" title="Direct link to How Merge-on-Read Works Across Table Formats">​</a></h2>
<p>Although Merge-on-Read is a shared concept across open table formats, each system implements it using different techniques, influenced by their internal design philosophy and read-write optimization goals. Here’s a breakdown of how Apache Hudi, Apache Iceberg, and Delta Lake enable Merge-on-Read behavior.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="apache-hudi">Apache Hudi<a href="https://hudi.apache.org/blog/2025/07/21/mor-comparison#apache-hudi" class="hash-link" aria-label="Direct link to Apache Hudi" title="Direct link to Apache Hudi">​</a></h3>
<p>Hudi implements Merge-on-Read as one of its two core table types (along with Copy-on-Write), offering a trade-off between read and write costs by maintaining base files alongside delta log files. Instead of rewriting columnar files for every update or delete, MoR tables maintain a combination of base files and log files that encode delta updates/deletes to the base file, enabling fast ingestion and deferred file merging via asynchronous <a href="https://hudi.apache.org/docs/compaction" target="_blank" rel="noopener noreferrer">compaction</a>. This design is particularly suited for streaming ingestion and update-heavy workloads, where minimizing write amplification and achieving high throughput are critical, without any downtime whatsoever for the writers.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="storage-layout">Storage Layout<a href="https://hudi.apache.org/blog/2025/07/21/mor-comparison#storage-layout" class="hash-link" aria-label="Direct link to Storage Layout" title="Direct link to Storage Layout">​</a></h4>
<p>At the physical level, a Hudi MoR table stores data in <a href="https://hudi.apache.org/tech-specs/#file-layout-hierarchy" target="_blank" rel="noopener noreferrer"><strong>File Groups</strong></a>, each uniquely identified by a <code>fileId</code>. A file group consists of:</p>
<ul>
<li>Base File (<code>.parquet, .orc</code>): Stores the base snapshot of records in columnar format.</li>
<li>Delta Log Files (<code>.log</code>): Append-only files that capture incremental updates, inserts, and deletes since the last compaction, in either row-oriented data formats like Apache Avro, Hudi’s native SSTable format or columnar-formats like Apache Parquet</li>
</ul>
<p>This hybrid design enables fast writes and defers expensive columnar file writing to asynchronous compaction.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="write-path">Write Path<a href="https://hudi.apache.org/blog/2025/07/21/mor-comparison#write-path" class="hash-link" aria-label="Direct link to Write Path" title="Direct link to Write Path">​</a></h4>
<p>In a Merge-on-Read table, insert and update operations are handled differently to strike a balance between write efficiency and read performance.</p>
<ul>
<li>
<p>Insert operations behave similarly to those in Copy-on-Write tables. New records are written to freshly created <em>base files</em>, aligned to a configured block size. In some cases, these inserts may be merged into the smallest existing base file in the partition to control file counts and sizes.</p>
</li>
<li>
<p>Update operations, however, are written to <em>log files</em> associated with the corresponding file group. These updates in the log files are written using Hudi’s <a href="https://github.com/apache/hudi/blob/45312d437a51ccd1d8c75ba0bd8af21a47dbb9e0/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkMergeOnReadTable.java#L205" target="_blank" rel="noopener noreferrer"><code>HoodieAppendHandle</code></a> class. At runtime, a new instance of <code>HoodieAppendHandle</code> is created with the target <em>partition</em> and <em>file ID</em>. The update records are passed to its <code>write()</code> method, which processes and appends them to the active <em>log file</em> associated with that file group. This mechanism avoids rewriting large Parquet base files and instead accumulates changes in a rolling log structure associated with each base file.</p>
</li>
</ul>
<div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">HoodieAppendHandle appendHandle = new HoodieAppendHandle(config, instantTime, this,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    partitionPath, fileId, recordMap.values().iterator(), taskContextSupplier, header);</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">appendHandle.write(recordMap);</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">List&lt;WriteStatus&gt; writeStatuses = appendHandle.close();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">return Collections.singletonList(writeStatuses).iterator();</span><br></span></code></pre></div></div>
<ul>
<li>Delete operations are also appended to log files as either delete keys or deleted vector positions, to refer to the base file records that were deleted. These delete entries are not applied to the base files immediately. Instead, they are taken into account during snapshot reads, which merge the base and log files to produce the latest view, and during compaction, which merges the accumulated log files (including deletes) into new base files.</li>
</ul>
<p>This design ensures that write operations remain lightweight and fast, regardless of the size of the base files. Writers are not blocked by background compaction or cleanup operations, making the system well-suited for streaming and CDC workloads.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="read-path">Read Path<a href="https://hudi.apache.org/blog/2025/07/21/mor-comparison#read-path" class="hash-link" aria-label="Direct link to Read Path" title="Direct link to Read Path">​</a></h4>
<p>Hudi MoR tables offer flexible read semantics by supporting both <a href="https://hudi.apache.org/docs/sql_queries/#snapshot-query" target="_blank" rel="noopener noreferrer">snapshot queries</a> and <a href="https://hudi.apache.org/docs/table_types#query-types" target="_blank" rel="noopener noreferrer">read-optimized queries</a>, depending on the user's performance and freshness requirements.</p>
<ul>
<li>
<p>Snapshot queries provide the most current view of the dataset by dynamically merging base files with their corresponding log files at read time. The system selects between different reader types based on the nature of the query and the presence of log files:</p>
<ul>
<li>A <strong>full-schema</strong> reader reads the complete row data to ensure correct application of updates and deletes.</li>
<li>A <strong>required-schema</strong> reader projects only the needed columns to reduce I/O, while still applying log file merges.</li>
<li>A <strong>skip-merging</strong> reader is used when log files are absent for a file group, allowing the query engine to read directly from base files without incurring merge costs.</li>
</ul>
</li>
<li>
<p>Read-optimized queries, in contrast, skip reading the delta log files altogether. These queries only scan the base Parquet files, providing faster response times at the cost of not reflecting the latest un-compacted changes. This mode is suitable for applications where slightly stale data is acceptable or where performance is critical.</p>
</li>
</ul>
<p>Together, these two read strategies allow Hudi MoR tables to serve both real-time and interactive queries from the same dataset, adjusting behavior depending on the workload and latency constraints.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="compaction">Compaction<a href="https://hudi.apache.org/blog/2025/07/21/mor-comparison#compaction" class="hash-link" aria-label="Direct link to Compaction" title="Direct link to Compaction">​</a></h4>
<p>As log files accumulate new updates and deletes, Hudi triggers a compaction operation to merge these log files back into columnar base files. This process is <a href="https://hudi.apache.org/docs/compaction#async--offline-compaction-models" target="_blank" rel="noopener noreferrer">configurable and asynchronous</a>, and plays a key role in balancing write and read performance.</p>
<p>Compaction in Hudi is triggered based on thresholds that can be configured by the user, such as the <em>number of commits (NUM_COMMITS)</em>. During compaction, all log files associated with a file group are read and merged with the existing base file to produce a new compacted base file.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="apache-iceberg">Apache Iceberg<a href="https://hudi.apache.org/blog/2025/07/21/mor-comparison#apache-iceberg" class="hash-link" aria-label="Direct link to Apache Iceberg" title="Direct link to Apache Iceberg">​</a></h3>
<p>Apache Iceberg supports Merge-on-Read (MoR) semantics by maintaining immutable base data files and tracking updates and deletions through separate <a href="https://iceberg.apache.org/spec/#delete-formats" target="_blank" rel="noopener noreferrer"><em>delete files</em></a>. This design avoids rewriting data files for every update or delete operation. Instead, these changes are applied at query time by merging delete files with the base files to produce an up-to-date view.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="storage-layout-1">Storage Layout<a href="https://hudi.apache.org/blog/2025/07/21/mor-comparison#storage-layout-1" class="hash-link" aria-label="Direct link to Storage Layout" title="Direct link to Storage Layout">​</a></h4>
<p>An Iceberg table consists of:</p>
<ul>
<li>Base Data Files: Immutable Parquet, ORC, or Avro files that contain the primary data.</li>
<li>Delete Files: Auxiliary files that record row-level deletions.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="write-path-1">Write Path<a href="https://hudi.apache.org/blog/2025/07/21/mor-comparison#write-path-1" class="hash-link" aria-label="Direct link to Write Path" title="Direct link to Write Path">​</a></h4>
<p>In Iceberg’s MoR tables, write operations implement row-level updates by encoding them as a delete of the old record and an insert of the new one. Rather than modifying existing Parquet base files directly, Iceberg maintains a clear separation between new data and logical deletes by introducing delete files alongside new data files.</p>
<ul>
<li>Inserts behave in the same way as CoW tables. The new data is appended to the table as part of a new snapshot.</li>
<li>For delete operations, Iceberg writes a delete file containing rows to be logically removed across multiple base files. Delete files are of two types:<!-- -->
<ul>
<li>Position Deletes: Reference row positions in a specific data file.</li>
<li>Equality Deletes: Encode a predicate that matches rows based on one or more column values.</li>
</ul>
</li>
</ul>
<p>Equality deletes are typically not favored in performance sensitive data platforms, since it forces predicate evaluation against every single base file during snapshot reads.</p>
<p>The <code>DeleteFile</code> interface captures these semantics:</p>
<div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">public interface DeleteFile extends ContentFile&lt;StructLike&gt; {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  enum DeleteType {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    EQUALITY, POSITION</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre></div></div>
<p><strong>Note:</strong> Iceberg v3 introduces Deletion Vectors as a more efficient alternative to positional deletes. Deletion vectors attach a <em>bitmap</em> to a data file to indicate deleted rows, allowing query engines to skip over deleted rows at read time. Deletion Vectors are already supported by Delta Lake and Hudi and this is now borrowed into the Iceberg spec as well.</p>
<ul>
<li>For update operations, Iceberg uses a two-step process. An update is implemented as a delete + insert pattern. First, a delete file is created to logically remove the old record, using either a position or equality delete. Then, a new data file is written that contains the full image of the updated record. Both the delete file and the new data file are added in a single atomic commit, creating a new snapshot of the table. This behavior is implemented via the <code>RowDelta</code> interface:</li>
</ul>
<div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">RowDelta rowDelta = table.newRowDelta()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    .addDeletes(deleteFile)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    .addRows(dataFile);</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">rowDelta.commit();</span><br></span></code></pre></div></div>
<p>All write operations, whether adding new data files or new delete files, produce a new snapshot in Iceberg’s timeline. This guarantees consistent isolation across readers and writers while avoiding any rewriting of immutable data files.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="read-path-1">Read Path<a href="https://hudi.apache.org/blog/2025/07/21/mor-comparison#read-path-1" class="hash-link" aria-label="Direct link to Read Path" title="Direct link to Read Path">​</a></h4>
<p>During query execution, Iceberg performs a Merge-on-Read query by combining the immutable base data files with any relevant delete files to present a consistent and up-to-date view. Before reading, the scan planning logic identifies which delete files apply to each data file, ensuring that deletes are correctly associated with their targets.</p>
<p>This planning step guarantees that any row marked for deletion through either position deletes or equality deletes is filtered out of the final results, while the original base files remain unchanged. The merging of base data with delete files is applied dynamically by the query engine, allowing Iceberg to preserve the immutable file structure and still deliver row-level updates.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="delta-lake">Delta Lake<a href="https://hudi.apache.org/blog/2025/07/21/mor-comparison#delta-lake" class="hash-link" aria-label="Direct link to Delta Lake" title="Direct link to Delta Lake">​</a></h3>
<p>Delta Lake supports Merge-on-Read semantics using <a href="https://docs.delta.io/latest/delta-deletion-vectors.html" target="_blank" rel="noopener noreferrer"><em>Deletion Vectors (DVs)</em></a>, a feature that allows rows to be logically removed from a dataset without rewriting the base Parquet files. This enables efficient row-level delete   while preserving immutability of data files. For updates, Delta Lake encodes changes as a combination of DELETE and INSERT operations, i.e. the old row is marked as deleted, and a new row with updated values is appended.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="storage-layout-2">Storage Layout<a href="https://hudi.apache.org/blog/2025/07/21/mor-comparison#storage-layout-2" class="hash-link" aria-label="Direct link to Storage Layout" title="Direct link to Storage Layout">​</a></h4>
<p>In Delta Lake, the storage layout consists of:</p>
<ul>
<li>Base Data Files: Immutable Parquet files that hold the core data</li>
<li>Deletion Vectors: Structures that track rows that should be considered deleted during reads, instead of physically removing them from Parquet</li>
</ul>
<p>A deletion vector is described by a descriptor which captures its storage type (inline, on-disk, or UUID-based), its physical location or inline data, an offset if stored on disk, its size in bytes, and the cardinality (number of rows it marks as deleted).</p>
<p>Small deletion vectors can be embedded directly into the Delta transaction log (inline), while larger ones are stored as separate files, with the UUIDs referencing them by a unique identifier.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="write-path-2">Write Path<a href="https://hudi.apache.org/blog/2025/07/21/mor-comparison#write-path-2" class="hash-link" aria-label="Direct link to Write Path" title="Direct link to Write Path">​</a></h4>
<p>When a DELETE, UPDATE, or MERGE operation is performed on a Delta table, Delta Lake does not rewrite the affected base Parquet files. Instead, it generates a deletion vector that identifies which rows are logically removed. These deletion vectors are built as compressed bitmap structures (using Roaring Bitmaps), which efficiently encode the positions of the deleted rows.</p>
<p>Smaller deletion vectors are kept inline in the transaction log for quick lookup, while larger ones are persisted as separate deletion vector files. All write operations that affect rows in this way update the metadata to track the associated deletion vectors, maintaining a consistent and atomic snapshot view for downstream reads.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="read-path-2">Read Path<a href="https://hudi.apache.org/blog/2025/07/21/mor-comparison#read-path-2" class="hash-link" aria-label="Direct link to Read Path" title="Direct link to Read Path">​</a></h4>
<p>During query execution, Delta Lake consults any deletion vectors attached to the current snapshot. The query execution loads these deletion vectors and applies them dynamically, filtering out rows marked as deleted before returning results to the user. This happens without rewriting or modifying the base Parquet files, preserving their immutability while still providing correct row-level semantics.</p>
<p>This Merge-on-Read approach allows Delta Lake to combine efficient write operations with the ability to serve up-to-date views, ensuring that queries see a consistent, deletion-aware representation of the dataset.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="comparative-design-analysis">Comparative Design Analysis<a href="https://hudi.apache.org/blog/2025/07/21/mor-comparison#comparative-design-analysis" class="hash-link" aria-label="Direct link to Comparative Design Analysis" title="Direct link to Comparative Design Analysis">​</a></h2>
<p>Merge-on-Read semantics are implemented differently across open table formats, with each approach reflecting distinct trade-offs that influence workload performance, complexity, and operational flexibility. MoR is generally well-suited for high-throughput, low-latency streaming ingestion scenarios in a lakehouse, where frequent updates and late-arriving data are expected. In contrast, Copy-on-Write (CoW) tables often work best for simpler, batch-oriented workloads where updates are infrequent and read-optimized behavior is a priority.</p>
<p>In this section, we focus on Apache Hudi and Apache Iceberg table formats and explore how their MoR designs influence real-world workloads.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="streaming-data-support--event-time-ordering">Streaming Data Support &amp; Event-Time Ordering<a href="https://hudi.apache.org/blog/2025/07/21/mor-comparison#streaming-data-support--event-time-ordering" class="hash-link" aria-label="Direct link to Streaming Data Support &amp; Event-Time Ordering" title="Direct link to Streaming Data Support &amp; Event-Time Ordering">​</a></h3>
<p>Hudi’s Merge-on-Read design supports event-time ordering and late-arriving data for streaming workloads by providing <a href="https://hudi.apache.org/docs/record_merger#record-payloads" target="_blank" rel="noopener noreferrer"><code>RecordPayload</code></a> and <a href="https://hudi.apache.org/docs/record_merger" target="_blank" rel="noopener noreferrer"><code>RecordMerger</code></a> APIs. These allow updates to be merged based on database sequence numbers or event timestamps, so that if data arrives out of order or has late arriving data, the final state is still correct from a temporal perspective.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig1.png" alt="index" width="1000" align="middle">
<p>Iceberg uses a last-writer-wins approach, where the most recent commit determines record values regardless of event time. This design may be tricky to deal with late-arriving data  in streaming workloads or CDC ingestion. For e.g. if the source stream is ever repositioned to an earlier time, it will cause the table to move backwards in time where older record values from the replayed stream overwrite newer record images in the table.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="scalable-incremental-write-costs">Scalable Incremental Write Costs<a href="https://hudi.apache.org/blog/2025/07/21/mor-comparison#scalable-incremental-write-costs" class="hash-link" aria-label="Direct link to Scalable Incremental Write Costs" title="Direct link to Scalable Incremental Write Costs">​</a></h3>
<p>One of the main goals of MoR is to reduce write costs and latencies by avoiding full file rewrites. Hudi achieves this by appending changes to <em>delta logs</em> and using <a href="https://hudi.apache.org/docs/indexes" target="_blank" rel="noopener noreferrer"><strong>indexing</strong></a> to quickly identify which file group an incoming update belongs to. Hudi supports different index types to accelerate this lookup process, so it does not need to scan the entire table on every update. This ensures that even if you are updating a relatively small amount of data - for example, 1GB of changes into a 1TB table every five to ten minutes, the system can efficiently target only the affected files.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig2.png" alt="index" width="1000" align="middle">
<p>Iceberg handles row-level updates and deletes by recording them as <a href="https://iceberg.apache.org/spec/#delete-formats" target="_blank" rel="noopener noreferrer"><em>delete files</em></a>. To identify which records to update or delete, Iceberg relies on scanning table metadata, and in some cases file-level data, to locate affected rows. This design uses a simple metadata approach but if partitioning is not highly selective, this lookup step can become a bottleneck for write performance on large tables with frequent small updates.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="asynchronous-compaction-during-merge">Asynchronous Compaction during ‘Merge’<a href="https://hudi.apache.org/blog/2025/07/21/mor-comparison#asynchronous-compaction-during-merge" class="hash-link" aria-label="Direct link to Asynchronous Compaction during ‘Merge’" title="Direct link to Asynchronous Compaction during ‘Merge’">​</a></h3>
<p>Hudi employs <a href="https://hudi.apache.org/blog/2025/01/28/concurrency-control#occ-multi-writers" target="_blank" rel="noopener noreferrer">optimistic concurrency control</a> (OCC) between writers and maintains blocking-free <a href="https://hudi.apache.org/blog/2025/01/28/concurrency-control#mvcc-writer-table-service-and-table-service-table-service" target="_blank" rel="noopener noreferrer">multi-version concurrency control</a> (MVCC) between writers and its asynchronous compaction process. This means writers can continue appending updates to the same records while earlier versions are being compacted in the background. Compaction operates <em>asynchronously</em>, creating new base files from accumulated log files, without interfering with active writers. This ensures great data freshness as well as better compression ratio and thus excellent query performance for columnar files longer term.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig3.png" alt="index" width="1000" align="middle">
<p>Iceberg maintains consistent snapshots across all operations, but it does not separate a dedicated compaction action from other write operations. As a result, if both a writer and a maintenance process try to modify overlapping data, standard snapshot conflict resolution ensures only one succeeds and might require retries in some concurrent write scenarios, but there is no asynchronous way to run compaction services. This could lead to livelocking between the writer and table maintenance, where one of them continuously causes the other to fail.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="non-blocking-concurrency-control-nbcc-for-real-time-applications">Non-Blocking Concurrency Control (NBCC) for Real-time applications<a href="https://hudi.apache.org/blog/2025/07/21/mor-comparison#non-blocking-concurrency-control-nbcc-for-real-time-applications" class="hash-link" aria-label="Direct link to Non-Blocking Concurrency Control (NBCC) for Real-time applications" title="Direct link to Non-Blocking Concurrency Control (NBCC) for Real-time applications">​</a></h3>
<p>Hudi 1.0 further extends its concurrency model to allow multiple writers to safely update the same record at the same time with <a href="https://hudi.apache.org/blog/2025/01/28/concurrency-control#non-blocking-concurrency-control-multi-writers" target="_blank" rel="noopener noreferrer">non-blocking conflict resolution</a>. It supports serializability guarantees based on write completion timestamps (arrival-time processing), while also allowing record merging according to event-time order if required. This flexible concurrency strategy enables concurrent writes to proceed, without the need to wait, making it ideal for real-time applications that demand faster ingestion.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig4.png" alt="index" width="700" align="middle">
<p>Iceberg applies OCC through its snapshot approach, where writers commit updates against the latest known snapshot, and if conflicts are detected, retries are required. There is no explicit distinction between arrival-time and event-time semantics for concurrent record updates.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="minimizing-read-costs">Minimizing Read Costs<a href="https://hudi.apache.org/blog/2025/07/21/mor-comparison#minimizing-read-costs" class="hash-link" aria-label="Direct link to Minimizing Read Costs" title="Direct link to Minimizing Read Costs">​</a></h3>
<p>Hudi organizes records into <em>file groups,</em> ensuring that updates are consistently routed back to the same group where the original records were stored. This approach means that when a query is executed, it only needs to scan the base file and any delta log files within that specific file group, reducing the data that must be read and merged at query time. By tying updates and inserts to a consistent file group, Hudi preserves locality and limits merge complexity.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig5.png" alt="index" width="1000" align="middle">
<p>Iceberg applies updates and deletes using <em>delete files</em>, and these can reference any row in any base file. As a result, readers must examine all relevant delete files along with all associated base data files during scan planning and execution, which can increase I/O and metadata processing requirements for large tables.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="performant-read-side-merge">Performant Read-Side Merge<a href="https://hudi.apache.org/blog/2025/07/21/mor-comparison#performant-read-side-merge" class="hash-link" aria-label="Direct link to Performant Read-Side Merge" title="Direct link to Performant Read-Side Merge">​</a></h3>
<p>Hudi’s MoR implementation uses <em>key-based</em> merging to reconcile delta log records with base files, which allows query engines to push down filters and still correctly merge updates based on record keys. This selective merging reduces unnecessary I/O and improves performance for queries that only need a subset of columns or rows.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig6.png" alt="index" width="800" align="middle">
<p>Iceberg historically required readers (particularly Spark readers) to load entire base files when applying positional deletes. This was because pushing down filters could change the order or number of rows returned by the Parquet reader, making positional delete applications incorrect. As a result, filter pushdowns could not be safely applied, forcing a full file scan to maintain correctness. There has been ongoing work in the Iceberg community to address this limitation by improving how positional information is tracked through filtered reads.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="efficient-compaction-planning">Efficient Compaction Planning<a href="https://hudi.apache.org/blog/2025/07/21/mor-comparison#efficient-compaction-planning" class="hash-link" aria-label="Direct link to Efficient Compaction Planning" title="Direct link to Efficient Compaction Planning">​</a></h3>
<p>Hudi’s compaction strategy operates at the level of individual file groups, which means it can plan and execute small, predictable units of compaction work. This fine-grained approach allows compaction to proceed <em>incrementally</em> and avoids large, unpredictable workloads.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig7.png" alt="index" width="800" align="middle">
<p>In Iceberg, compaction must consider all base files and their related delete files together, because delete files reference rows in the base data files. This creates a dependency graph where all related files must be handled in a coordinated way. As delete files accumulate over time, these compaction operations can become increasingly large and complex to plan, making it harder to schedule resources efficiently. If compaction falls behind, the amount of data that must be compacted in future operations continues to grow, potentially making the problem worse.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="temporal-and-spatial-locality-for-event-time-filters">Temporal and Spatial Locality for Event-Time Filters<a href="https://hudi.apache.org/blog/2025/07/21/mor-comparison#temporal-and-spatial-locality-for-event-time-filters" class="hash-link" aria-label="Direct link to Temporal and Spatial Locality for Event-Time Filters" title="Direct link to Temporal and Spatial Locality for Event-Time Filters">​</a></h3>
<p>Hudi maintains temporal and spatial locality by ensuring that updates and deletes are routed back to the same file group where the original record was first stored. This preserves the time-based clustering or ordering of records, which is especially beneficial for queries filtering by event time or operating within specific time windows. By keeping related records together, Hudi enables efficient pruning of file groups along with partition pruning, during time-based queries.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig8.png" alt="index" width="1000" align="middle">
<p>Iceberg handles updates by deleting the existing record and inserting a new one, which may place the updated record in a different data file. Over time, this can scatter records that belong to the same logical or temporal group across multiple files, reducing the effectiveness of partition pruning and requiring periodic clustering or optimization to restore temporal locality.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="partial-updates-for-performant-merge">Partial Updates for Performant Merge<a href="https://hudi.apache.org/blog/2025/07/21/mor-comparison#partial-updates-for-performant-merge" class="hash-link" aria-label="Direct link to Partial Updates for Performant Merge" title="Direct link to Partial Updates for Performant Merge">​</a></h3>
<p>Hudi supports partial updates by encoding only the columns that have changed into its delta log files. This means the cost of merging updates is proportional to the number of columns actually modified, rather than the total width of the record. For columnar datasets with wide schemas, this can significantly reduce write amplification and improve merge performance.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig9.png" alt="index" width="800" align="middle">
<p>In Iceberg, updates are implemented as a delete plus a full-row insert, which requires rewriting the entire record even if only a single column has changed. As a result, update costs in Iceberg scale with the total number of columns in the record, increasing I/O and storage requirements for wide tables with frequent column-level updates.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://hudi.apache.org/blog/2025/07/21/mor-comparison#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>Merge-on-Read (MoR) table type provides an alternative approach to managing updates and deletes on immutable columnar files in a lakehouse. While multiple open table formats support MoR semantics, their design choices significantly affect suitability for real-time and change-data driven workloads.</p>
<p>Apache Hudi’s MoR implementation specifically addresses the needs of high-ingestion, update-heavy pipelines. By appending changes to delta logs, preserving file-group-based data locality, supporting event-time ordering, and enabling asynchronous, non-blocking compaction, Hudi minimizes write amplification and supports low-latency data availability. These design primitives directly align with streaming and CDC patterns, where data arrives frequently and potentially out of order. Iceberg and Delta Lake also implement MoR semantics in their own ways to address transactional consistency and immutable storage goals.</p>
<hr>]]></content:encoded>
            <category>Apache Hudi</category>
            <category>Merge-on-Read (MoR)</category>
            <category>Streaming</category>
        </item>
        <item>
            <title><![CDATA[Modernizing Data Infrastructure at Peloton Using Apache Hudi]]></title>
            <link>https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi</link>
            <guid>https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi</guid>
            <pubDate>Tue, 15 Jul 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Peloton re-architected its data platform using Apache Hudi to overcome snapshot delays, rigid service coupling, and high operational costs. By adopting CDC-based ingestion from PostgreSQL and DynamoDB, moving from CoW to MoR tables, and leveraging asynchronous services with fine-grained schema control, Peloton achieved 10-minute ingestion cycles, reduced compute/storage overhead, and enabled time travel and GDPR compliance.]]></description>
            <content:encoded><![CDATA[<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>TL;DR</div><div class="admonitionContent_BuS1"><p>Peloton re-architected its data platform using Apache Hudi to overcome snapshot delays, rigid service coupling, and high operational costs. By adopting CDC-based ingestion from PostgreSQL and DynamoDB, moving from CoW to MoR tables, and leveraging asynchronous services with fine-grained schema control, Peloton achieved 10-minute ingestion cycles, reduced compute/storage overhead, and enabled time travel and GDPR compliance.</p></div></div>
<p>Peloton is a global interactive fitness platform that delivers connected, instructor-led fitness experiences to millions of members worldwide. Known for its immersive classes and cutting-edge equipment, Peloton combines software, hardware, and data to create personalized workout journeys. With a growing member base and increasing product diversity, data has become central to how Peloton delivers value. The <em>Data Platform</em> team at Peloton is responsible for building and maintaining the core infrastructure that powers analytics, reporting, and real-time data applications. Their work ensures that data flows seamlessly from transactional systems to the data lake, enabling teams across the organization to make timely, data-driven decisions.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-challenge-data-growth-latency-and-operational-bottlenecks">The Challenge: Data Growth, Latency, and Operational Bottlenecks<a href="https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi#the-challenge-data-growth-latency-and-operational-bottlenecks" class="hash-link" aria-label="Direct link to The Challenge: Data Growth, Latency, and Operational Bottlenecks" title="Direct link to The Challenge: Data Growth, Latency, and Operational Bottlenecks">​</a></h2>
<p>As Peloton evolved into a global interactive fitness platform, its data infrastructure was challenged by the growing need for timely insights, agile service migrations, and cost-effective analytics. Daily operations, recommendation systems, and compliance requirements demanded an architecture that could support near real-time access, high-frequency updates, and scalable service boundaries.</p>
<p>However, the team faced persistent bottlenecks with the existing setup:</p>
<ul>
<li>Reporting pipelines were gated by the completion of full snapshot jobs.</li>
<li>Recommender systems could only function on daily refreshed datasets.</li>
<li>The analytics platform was tightly coupled with operational systems.</li>
<li>Microservice migrations were constrained to all-at-once shifts.</li>
<li>Database read replicas incurred high infrastructure costs.</li>
</ul>
<p>These limitations made it difficult to meet SLA expectations, scale workloads efficiently, and adapt the platform to new user and product needs.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-legacy-architecture">The Legacy Architecture<a href="https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi#the-legacy-architecture" class="hash-link" aria-label="Direct link to The Legacy Architecture" title="Direct link to The Legacy Architecture">​</a></h2>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig1.png" alt="challenge" width="1000" align="middle">
<p>Peloton's earlier architecture relied on daily snapshots from a monolithic <strong>PostgreSQL</strong> database. The analytics systems would consume these snapshots, often waiting hours for completion. This not only delayed reporting but also introduced downstream rigidity.</p>
<p>Because the same data platform supported both online and analytical workloads, any schema or service migration required significant planning and coordination. Database read replicas, used to scale reads, increased cost overhead. Moreover, recommendation systems that depended on data freshness were constrained by the snapshot interval, limiting personalization capabilities. This architecture struggled to support a fast-moving product roadmap, near real-time analytics, and the data agility needed to experiment and iterate.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="reimagining-the-data-platform-with-apache-hudi">Reimagining the Data Platform with Apache Hudi<a href="https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi#reimagining-the-data-platform-with-apache-hudi" class="hash-link" aria-label="Direct link to Reimagining the Data Platform with Apache Hudi" title="Direct link to Reimagining the Data Platform with Apache Hudi">​</a></h2>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig2.png" alt="challenge" width="1000" align="middle">
<p>To address these challenges, the data platform team introduced Apache Hudi as the foundation of its modern data lake. The architecture was rebuilt to support Change Data Capture (CDC) ingestion from both PostgreSQL and DynamoDB using Debezium, with Kafka acting as the transport layer. A custom-built Hudi writer was developed to ingest CDC records into S3 using Apache Spark on EMR (version 6.12.0 with Hudi 0.13.1).</p>
<p>Peloton initially chose Copy-on-Write (CoW) table formats to support querying via Redshift Spectrum and simplify adoption. However, performance and cost bottlenecks prompted a transition to Merge-on-Read (MoR) tables with asynchronous table services for cleaning and compaction.</p>
<p>Key architectural enhancements included:</p>
<ul>
<li><strong>Support for GDPR compliance</strong> through structured delete propagation.</li>
<li><strong>Time travel queries</strong> for recommender model training and data recovery.</li>
<li><strong>Phased migration support</strong> for microservices via decoupled ingestion.</li>
</ul>
<p>Peloton's broader data platform tech stack supports this architecture with a range of tools for orchestration, analytics, and governance. This includes EMR for compute, Redshift for querying, DBT for data transformations, Looker for BI and visualization, Airflow for orchestration, and DataHub for metadata management. These components complement Apache Hudi in forming a modular and production-ready lakehouse stack.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig3.png" alt="challenge" width="1000" align="middle">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="learnings-from-running-hudi-at-scale">Learnings from Running Hudi at Scale<a href="https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi#learnings-from-running-hudi-at-scale" class="hash-link" aria-label="Direct link to Learnings from Running Hudi at Scale" title="Direct link to Learnings from Running Hudi at Scale">​</a></h2>
<p>With Hudi now integrated into Peloton's data lake, the team began to observe and address new operational and architectural challenges that emerged at scale. This section outlines the major lessons learned while maintaining high-ingestion throughput, ensuring data reliability, and keeping infrastructure costs under control.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cow-vs-mor-performance-trade-offs">CoW vs MoR: Performance Trade-offs<a href="https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi#cow-vs-mor-performance-trade-offs" class="hash-link" aria-label="Direct link to CoW vs MoR: Performance Trade-offs" title="Direct link to CoW vs MoR: Performance Trade-offs">​</a></h3>
<p>Initially, Copy-on-Write (CoW) tables were chosen to simplify deployment and ensure compatibility with Redshift Spectrum. However, as ingestion frequency increased and update volumes spanned hundreds of partitions, performance became a bottleneck. Some high-frequency tables with updates across 256 partitions took nearly an hour to process per run. Additionally, retaining 30 days of commits for training recommender models significantly inflated storage requirements, reaching into the hundreds of gigabytes.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig4.png" alt="challenge" width="1000" align="middle">
<p>To resolve this, the team migrated to Hudi’s Merge-on-Read (MoR) tables and reduced commit retention to 7 days. With ingestion jobs now running every 10 minutes, latency dropped significantly, and storage and compute usage became more efficient.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="async-vs-inline-table-services">Async vs Inline Table Services<a href="https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi#async-vs-inline-table-services" class="hash-link" aria-label="Direct link to Async vs Inline Table Services" title="Direct link to Async vs Inline Table Services">​</a></h3>
<p>To improve write throughput and meet low-latency ingestion goals, the Peloton team initially configured Apache Hudi with asynchronous cleaner and compactor services. This approach worked well across most tables, allowing ingestion pipelines to run every 10 minutes with minimal blocking but introduced some operational edge cases. Some of the challenges encountered included:</p>
<ul>
<li>Concurrent execution of writer and cleaner jobs, leading to conflicts. These were mitigated by introducing DynamoDB-based locks to serialize access.</li>
<li>Reader-cleaner race conditions, where time travel queries intermittently failed with <code>"File Not Found"</code> errors - traced back to cleaners deleting files mid-read.</li>
<li>Compaction disruptions caused by EMR node terminations, which led to orphaned files when jobs failed mid-way.</li>
</ul>
<p>These edge cases were largely due to the operational complexity of managing concurrent workloads at Peloton’s scale. After weighing reliability against latency, the team opted to switch to inline table services for compaction and cleaning, augmented with custom logic to control when these actions would run. This change improved system stability while maintaining acceptable latency trade-offs.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="glue-schema-version-limits">Glue Schema Version Limits<a href="https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi#glue-schema-version-limits" class="hash-link" aria-label="Direct link to Glue Schema Version Limits" title="Direct link to Glue Schema Version Limits">​</a></h3>
<p>As schema evolution continued, the team used Hudi's <code>META_SYNC_ENABLED</code> to sync schema updates with AWS Glue. Over time, high-frequency schema updates pushed the number of <code>TABLE_VERSION</code> resources in Glue beyond the <em>1 million</em> limit. This caused jobs to fail in ways that were initially difficult to trace.</p>
<p>One such failure manifested as the following error:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">ERROR Client: Application diagnostics message: User class threw exception:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">java.lang.NoSuchMethodError: 'org.apache.hudi.exception.HoodieException </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">org.apache.hudi.sync.common.util.SyncUtilHelpers.getExceptionFromList(java.util.Collection)'</span><br></span></code></pre></div></div>
<p>After significant debugging, the issue was traced to AWS Glue limits. The team implemented a multi-step fix:</p>
<ul>
<li>Worked with AWS to temporarily raise resource limits.</li>
<li>Developed a Python service to identify and delete outdated table versions, removing over 1 million entries.</li>
<li>Added an Airflow job to schedule weekly cleanup tasks.</li>
<li>Improved schema sync logic to trigger only when the schema changed.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="debezium--toast-handling">Debezium &amp; TOAST Handling<a href="https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi#debezium--toast-handling" class="hash-link" aria-label="Direct link to Debezium &amp; TOAST Handling" title="Direct link to Debezium &amp; TOAST Handling">​</a></h3>
<p>PostgreSQL CDC ingestion posed unique challenges due to the database’s handling of large fields using TOAST (The Oversized-Attribute Storage Technique). When fields over 8KB were unchanged, Debezium emitted a placeholder value <code>__debezium_unavailable_value</code>, making it impossible to determine whether the value had changed.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig5.png" alt="challenge" width="1000" align="middle">
<p>To address this, Peloton:</p>
<ul>
<li>Populated initial data using PostgreSQL snapshots.</li>
<li>Implemented self-joins between incoming CDC records and existing Hudi records to fill in missing values.</li>
<li>Separated inserts, updates, and deletes within Spark batch processing.</li>
<li>Used the <code>ts</code> field as the precombine key to ensure only the latest record state was retained.</li>
</ul>
<p>A reconciliation pipeline was also developed to heal data inconsistencies caused by multiple operations on the same key within a batch (e.g., create-delete-create).</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="data-validation-and-quality-enforcement">Data Validation and Quality Enforcement<a href="https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi#data-validation-and-quality-enforcement" class="hash-link" aria-label="Direct link to Data Validation and Quality Enforcement" title="Direct link to Data Validation and Quality Enforcement">​</a></h3>
<p>Data quality was critical to ensure trust in the newly established data lake. The team developed several internal libraries and checks:</p>
<ul>
<li>A Crypto Shredding Library to encrypt <code>user_id</code> and other PII fields before storage.</li>
<li>A Data Validation Framework that compared records in the lake against snapshot data.</li>
<li>A Data Quality Library that enforced column-level thresholds. These checks integrated with DataHub and were tied to Airflow sensors to halt downstream jobs on failures.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="dynamodb-ingestion-and-schema-challenges">DynamoDB Ingestion and Schema Challenges<a href="https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi#dynamodb-ingestion-and-schema-challenges" class="hash-link" aria-label="Direct link to DynamoDB Ingestion and Schema Challenges" title="Direct link to DynamoDB Ingestion and Schema Challenges">​</a></h3>
<p>Some Peloton services relied on DynamoDB for operational workloads (NoSQL). To ingest these datasets into the lake, the team used DynamoDB Streams and a Kafka Connector, allowing reuse of the existing Kafka-based Hudi ingestion path.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig6.png" alt="challenge" width="1000" align="middle">
<p>However, the NoSQL nature of DynamoDB introduced schema management challenges. Two strategies were evaluated:</p>
<ol>
<li>Stakeholder-defined schemas, using SUPER-type fields.</li>
<li>Dynamic schema inference, where incoming JSON records were parsed, and the evolving schema was inferred and reconciled.</li>
</ol>
<p>The team opted for dynamic inference despite increased processing time, as it enabled better support for exploratory workloads. Daily snapshots and reconciliation steps helped clean up inconsistent schema states.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="reducing-operational-costs">Reducing Operational Costs<a href="https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi#reducing-operational-costs" class="hash-link" aria-label="Direct link to Reducing Operational Costs" title="Direct link to Reducing Operational Costs">​</a></h3>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig7.png" alt="challenge" width="1000" align="middle">
<p>As the system matured, cost optimization became a priority. The team used <a href="https://github.com/ganglia/" target="_blank" rel="noopener noreferrer">Ganglia</a> to analyze job profiles and identify areas for improvement:</p>
<ul>
<li>EMR resources were gradually right-sized based on CPU and memory usage.</li>
<li>Conditional Hive syncing was introduced to avoid unnecessary sync operations during each run.</li>
<li>A Spark-side inefficiency was discovered where archived timelines were unnecessarily loaded, causing jobs to take 4x longer. Fixing this reduced overall latency and compute resource usage.</li>
</ul>
<p>These operational refinements significantly reduced idle times and improved the cost-efficiency of the platform.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="gains-from-hudi-adoption">Gains from Hudi Adoption<a href="https://hudi.apache.org/blog/2025/07/15/modernizing-datainfra-peloton-hudi#gains-from-hudi-adoption" class="hash-link" aria-label="Direct link to Gains from Hudi Adoption" title="Direct link to Gains from Hudi Adoption">​</a></h2>
<p>Peloton's transition to Apache Hudi led to measurable performance, operational, and cost-related improvements across its modern data platform.</p>
<p>Peloton's transition to Apache Hudi yielded several measurable improvements:</p>
<ul>
<li>Ingestion frequency increased from once daily to every 10 minutes.</li>
<li>Reduced snapshot job durations from an hour to under 15 minutes.</li>
<li>Cost savings by eliminating read replicas and optimizing EMR cluster usage.</li>
<li>Time travel support enabled retrospective analysis and model re-training.</li>
<li>Improved compliance posture through structured deletes and encrypted PII.</li>
</ul>
<p>The modernization laid the groundwork for future evolution, including real-time streaming ingestion using Apache Flink and continued improvements in data freshness, latency, and governance.</p>
<p>This blog is based on Peloton’s presentation at the Apache Hudi Community Sync. If you are interested in watching the recorded version of the video, you can find it <a href="https://youtu.be/-Pyid5K9dyU?feature=shared" target="_blank" rel="noopener noreferrer">here</a>.</p>
<hr>]]></content:encoded>
            <category>Apache Hudi</category>
            <category>Peloton</category>
            <category>Community</category>
        </item>
        <item>
            <title><![CDATA[How PayU built a secure enterprise AI assistant using Amazon Bedrock]]></title>
            <link>https://hudi.apache.org/blog/2025/07/15/PayU-built-a-secure-enterprise-AI-assistant</link>
            <guid>https://hudi.apache.org/blog/2025/07/15/PayU-built-a-secure-enterprise-AI-assistant</guid>
            <pubDate>Tue, 15 Jul 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://aws.amazon.com/blogs/machine-learning/how-payu-built-a-secure-enterprise-ai-assistant-using-amazon-bedrock/">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>Apache Hudi</category>
            <category>AWS</category>
        </item>
        <item>
            <title><![CDATA[Building a RAG-based AI Recommender (Part 1/2)]]></title>
            <link>https://hudi.apache.org/blog/2025/07/10/building-a-rag-based-ai-recommender</link>
            <guid>https://hudi.apache.org/blog/2025/07/10/building-a-rag-based-ai-recommender</guid>
            <pubDate>Thu, 10 Jul 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://blog.datumagic.ai/p/building-a-rag-based-ai-recommender">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>Apache Hudi</category>
            <category>AI</category>
            <category>RAG</category>
            <category>Artificial Intelligence</category>
            <category>data lakehouse</category>
            <category>Lakehouse</category>
            <category>use-case</category>
            <category>datumagic</category>
        </item>
        <item>
            <title><![CDATA[How Stifel built a modern data platform using AWS Glue and an event-driven domain architecture]]></title>
            <link>https://hudi.apache.org/blog/2025/07/07/how-stifel-built-a-modern-data-platform-using-aws-glue-and-an-event-driven-domain-architecture</link>
            <guid>https://hudi.apache.org/blog/2025/07/07/how-stifel-built-a-modern-data-platform-using-aws-glue-and-an-event-driven-domain-architecture</guid>
            <pubDate>Mon, 07 Jul 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://aws.amazon.com/blogs/big-data/how-stifel-built-a-modern-data-platform-using-aws-glue-and-an-event-driven-domain-architecture/">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>Apache Hudi</category>
            <category>aws</category>
            <category>AWS Glue</category>
            <category>AWS Blogs</category>
            <category>Amazon EMR</category>
            <category>AWS Lake Formation</category>
            <category>Data Governance</category>
            <category>Lakehouse</category>
            <category>use-case</category>
            <category>det</category>
        </item>
        <item>
            <title><![CDATA[Why Uber Built Hudi: The Strategic Decision Behind a Custom Table Format]]></title>
            <link>https://hudi.apache.org/blog/2025/07/03/why-uber-built-hudi-the-strategic-decision-behind-a-custom-table-format</link>
            <guid>https://hudi.apache.org/blog/2025/07/03/why-uber-built-hudi-the-strategic-decision-behind-a-custom-table-format</guid>
            <pubDate>Thu, 03 Jul 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://thamizhelango.medium.com/why-uber-built-hudi-the-strategic-decision-behind-a-custom-table-format-f57db68b0cb9">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>Apache Hudi</category>
            <category>Apache Iceberg</category>
            <category>Lakehouse</category>
            <category>use-case</category>
            <category>Uber</category>
            <category>det</category>
        </item>
        <item>
            <title><![CDATA[Lakehouse Architecture - Apache Hudi and Apache Iceberg]]></title>
            <link>https://hudi.apache.org/blog/2025/07/02/Lakehouse-Architecture-apache-hudi-and-apache-iceberg</link>
            <guid>https://hudi.apache.org/blog/2025/07/02/Lakehouse-Architecture-apache-hudi-and-apache-iceberg</guid>
            <pubDate>Wed, 02 Jul 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://www.linkedin.com/pulse/lakehouse-architecture-apache-hudi-iceberg-becloudready-4b1ac/">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>Apache Hudi</category>
            <category>Apache Iceberg</category>
            <category>Lakehouse</category>
            <category>use-case</category>
            <category>det</category>
        </item>
        <item>
            <title><![CDATA[Scaling Complex Data Workflows at Uber Using Apache Hudi]]></title>
            <link>https://hudi.apache.org/blog/2025/06/30/uber-hudi</link>
            <guid>https://hudi.apache.org/blog/2025/06/30/uber-hudi</guid>
            <pubDate>Mon, 30 Jun 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Uber’s trip and order collection pipelines grew highly complex, with long runtimes, massive DAGs, and rigid SQL logic that hampered scalability and maintainability. By adopting Apache Hudi, Uber re-architected these pipelines to enable incremental processing, custom merge behavior, and rule-based functional transformations. This reduced runtime from 20 hours to 4 hours, improved test coverage to 95%, cut costs by 60%, and delivered a composable, explainable, and scalable data workflow architecture.]]></description>
            <content:encoded><![CDATA[<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>TL;DR</div><div class="admonitionContent_BuS1"><p>Uber’s trip and order collection pipelines grew highly complex, with long runtimes, massive DAGs, and rigid SQL logic that hampered scalability and maintainability. By adopting Apache Hudi, Uber re-architected these pipelines to enable incremental processing, custom merge behavior, and rule-based functional transformations. This reduced runtime from 20 hours to 4 hours, improved test coverage to 95%, cut costs by 60%, and delivered a composable, explainable, and scalable data workflow architecture.</p></div></div>
<p>At Uber, the Core Services Data Engineering team supports a wide range of use cases across products like Uber Mobility and Uber Eats. One critical use case is computing the collection - the net payable amount - from a trip or an order. While this sounds straightforward at first, it quickly becomes a complex data problem when you factor in real-world scenarios like refunds, tips, driver disputes, location updates, and settlement adjustments across multiple verticals.</p>
<p>To solve this problem at scale, Uber re-architected their pipelines using <a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer">Apache Hudi</a> to enable low-latency, incremental, and rule-based processing. This post outlines the challenges they faced, the architectural shifts they made, and the measurable outcomes they achieved in production.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-challenge-scale-latency-and-complexity">The Challenge: Scale, Latency, and Complexity<a href="https://hudi.apache.org/blog/2025/06/30/uber-hudi#the-challenge-scale-latency-and-complexity" class="hash-link" aria-label="Direct link to The Challenge: Scale, Latency, and Complexity" title="Direct link to The Challenge: Scale, Latency, and Complexity">​</a></h2>
<img src="https://hudi.apache.org/assets/images/blog/figure2_uber.png" alt="challenge" width="800" align="middle">
<p>Our original data pipelines were processing nearly 90 million records a day, but the nature of updates made them inefficient. For instance, a trip taken three years ago could still be updated due to a late settlement. Our statistical analysis showed most updates occur within 180 days, so we designed the system to read and write a 180-day window every day - leading to severe read and write amplification.</p>
<p>The pipeline itself was a massive DAG with over 50–60 tasks, taking close to 20 hours to complete. These long runtimes made recovery difficult and introduced operational risks. Making a change meant tracing the logic across this sprawling DAG, which affected developer productivity and increased the chances of regressions.</p>
<p>Despite the large window, we still missed updates that fell outside the 180-day mark, leading to data quality issues. The long development cycles and heavy debugging effort further hindered our ability to iterate and maintain the system.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="rigid-sql-and-tight-coupling">Rigid SQL and Tight Coupling<a href="https://hudi.apache.org/blog/2025/06/30/uber-hudi#rigid-sql-and-tight-coupling" class="hash-link" aria-label="Direct link to Rigid SQL and Tight Coupling" title="Direct link to Rigid SQL and Tight Coupling">​</a></h2>
<p>Digging deeper, we identified multiple underlying causes. The pipeline relied heavily on SQL for all transformations. But expressing the evolving business rules for different Uber products in SQL was limiting. The logic had grown too complex to be managed effectively, and granular transformations led to a proliferation of intermediate stages. This made unit testing and debugging difficult, and the absence of structured logging made observability poor.</p>
<img src="https://hudi.apache.org/assets/images/blog/figure3_uber.png" alt="redshift" width="800" align="middle">
<p>Additionally, data and logic were tightly coupled. The system often required joining tables at very fine granularities, introducing redundancy and making logic harder to reason about. Complex joins, table scans, and late-arriving data amplified processing costs. It was also difficult to trace how a specific row was transformed through the DAG, making explainability a real challenge.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-we-solved-it">How We Solved It?<a href="https://hudi.apache.org/blog/2025/06/30/uber-hudi#how-we-solved-it" class="hash-link" aria-label="Direct link to How We Solved It?" title="Direct link to How We Solved It?">​</a></h2>
<ol>
<li><strong>Solving Read Amplification</strong></li>
</ol>
<p>The first step in addressing inefficiencies was eliminating the brute-force strategy of scanning and processing a 180-day window of data on every pipeline run. With the help of Apache Hudi’s <a href="https://hudi.apache.org/docs/table_types#incremental-queries" target="_blank" rel="noopener noreferrer"><em>incremental</em> <em>read</em></a> capabilities, we restructured the ingestion layer to read only the records that had mutated since the last checkpoint.</p>
<img src="https://hudi.apache.org/assets/images/blog/fig4_uber.png" alt="redshift" width="800" align="middle">
<p>We introduced an intermediate Hudi table that consolidated all related records for a trip or order into a single row, using complex data types such as structs, lists, and maps. This model allowed us to capture the complete state of a trip - including all updates, tips, disputes, and refunds in one place, without scattering information across multiple joins.</p>
<p>By using this intermediate table as the foundation, all downstream logic could operate on change-driven inputs. The result was a pipeline that avoided unnecessary scans, improved correctness by processing all real changes (not just those in a time window), and reduced overall I/O dramatically.</p>
<ol start="2">
<li><strong>Eliminating Self Joins with Custom Payloads</strong></li>
</ol>
<p>Self joins - especially for reconciling updates to the same trip were one of the costliest operations in our original pipeline.</p>
<img src="https://hudi.apache.org/assets/images/blog/fig5_uber.png" alt="redshift" width="800" align="middle">
<p>To solve this, we implemented a custom Hudi payload class that allows us to control how updates are applied during the merge phase. This class overrides methods such as <code>combineAndGetUpdateValue</code> and <code>getInsertValue</code>, and executes the merge logic as part of the write path, eliminating the need for a full table scan or shuffle.</p>
<p>This approach helped us efficiently handle updates to complex, nested records in the intermediate Hudi table, and dramatically reduced the cost associated with self joins.</p>
<ol start="3">
<li><strong>Simplifying Processing with a Rule-Based Framework</strong></li>
</ol>
<p>To move away from the rigidity of SQL, we designed a rule engine framework based on functional programming principles.</p>
<p>Instead of expressing business logic as large, monolithic SQL queries, we cast each input row (from the intermediate table) into a strongly typed object (e.g., a Trip object). These objects were then passed through a series of declarative rules - each consisting of a condition and an action.</p>
<img src="https://hudi.apache.org/assets/images/blog/fig6_uber.png" alt="redshift" width="800" align="middle">
<p>This framework was implemented as a custom <a href="https://hudi.apache.org/docs/hoodie_streaming_ingestion#transformers" target="_blank" rel="noopener noreferrer"><em>transformer</em></a> plugged into <a href="https://hudi.apache.org/docs/hoodie_streaming_ingestion" target="_blank" rel="noopener noreferrer">HudiStreamer</a>. The transformer intercepts the ingested data, applies the rule engine logic, and emits the transformed object to the final Hudi output table. We also built in capabilities for:</p>
<ul>
<li>Logging and observability (for metrics and debugging)</li>
<li>Unreachable state detection (flagging invalid rows)</li>
<li>Unit testing support for each rule independently</li>
</ul>
<p>This architecture replaced the huge DAG with modular, testable, and composable rule definitions, dramatically improving developer productivity and data pipeline clarity.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="final-architecture">Final Architecture<a href="https://hudi.apache.org/blog/2025/06/30/uber-hudi#final-architecture" class="hash-link" aria-label="Direct link to Final Architecture" title="Direct link to Final Architecture">​</a></h2>
<img src="https://hudi.apache.org/assets/images/blog/fig7_uber.png" alt="redshift" width="800" align="middle">
<p>The redesigned system follows a clean, composable structure:</p>
<ul>
<li>Incremental ingestion from the data lake is done using HudiStreamer, which writes to an intermediate Hudi table.</li>
<li>The intermediate table consolidates all records for a trip using complex types, serving as the central input for downstream processing.</li>
<li>A custom Transformer intercepts the records, casts them into typed domain objects, and passes them through a rule engine.</li>
<li>The rule engine applies business logic declaratively and emits fully processed objects.</li>
<li>The output is written to a final Hudi table that supports efficient, incremental consumption.</li>
</ul>
<p>This design eliminates redundant scans, reduces shuffle overhead, enables full test coverage, and offers detailed observability across all transformation stages.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-wins-with-hudi">The Wins with Hudi<a href="https://hudi.apache.org/blog/2025/06/30/uber-hudi#the-wins-with-hudi" class="hash-link" aria-label="Direct link to The Wins with Hudi" title="Direct link to The Wins with Hudi">​</a></h2>
<p>The improvements were substantial and measurable:</p>
<ul>
<li>Runtime reduced from ~20 hours to ~4 hours (~75% improvement)</li>
<li>Test coverage increased to 95% for transformation logic</li>
<li>Single run cost reduced by 60%</li>
<li>Improved data completeness, processing all updates—not just those in a statistical window</li>
<li>Reusable and modular logic, reducing DAG complexity</li>
<li>Higher developer productivity, with isolated unit testing and simplified debugging</li>
<li>Improved self-join performance through custom payloads</li>
<li>A generic rule engine design, portable across Spark and Flink</li>
</ul>
<p>Apache Hudi has been central to Nexus’ success, providing the core data lake storage layer for scalable ingestion, updates, and metadata management. It enables fast, incremental updates at massive scale while maintaining transactional guarantees on top of Amazon S3.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://hudi.apache.org/blog/2025/06/30/uber-hudi#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>By redesigning the system around Apache Hudi and adopting functional, rule-based processing, Uber was able to transform a brittle, long-running pipeline into a maintainable and efficient architecture. The changes allowed them to scale their data workflows to meet the needs of complex, multi-product use cases without compromising on performance, observability, or data quality.</p>
<p>This work highlights the power of pairing the right storage format with a principled architectural approach. Apache Hudi was instrumental in helping achieve these outcomes and continues to play a key role in Uber’s evolving data platform.</p>
<p>This blog is based on Uber’s presentation at the Apache Hudi Community Sync. If you are interested in watching the recorded version of the video, you can find it <a href="https://www.youtube.com/watch?v=VpdimpH_nsI" target="_blank" rel="noopener noreferrer">here</a>.</p>
<hr>]]></content:encoded>
            <category>Apache Hudi</category>
            <category>Uber</category>
            <category>Community</category>
        </item>
        <item>
            <title><![CDATA[Apache Hudi does XYZ (1/10): File pruning with multi-modal index]]></title>
            <link>https://hudi.apache.org/blog/2025/06/16/Apache-Hudi-does-XYZ-110</link>
            <guid>https://hudi.apache.org/blog/2025/06/16/Apache-Hudi-does-XYZ-110</guid>
            <pubDate>Mon, 16 Jun 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://blog.datumagic.ai/p/apache-hudi-does-xyz-110">here</a></span>]]></content:encoded>
            <category>hudi</category>
            <category>spark</category>
            <category>blog</category>
            <category>course</category>
            <category>tutorial</category>
            <category>datumagic</category>
            <category>data lake</category>
            <category>lakehouse</category>
            <category>apache hudi</category>
            <category>apache spark</category>
        </item>
        <item>
            <title><![CDATA[Optimizing Apache Hudi Workflows: Automation for Clustering, Resizing & Concurrency]]></title>
            <link>https://hudi.apache.org/blog/2025/06/13/Optimizing-Apache-Hudi-Workflows-Automation-for-Clustering-Resizing-Concurrency</link>
            <guid>https://hudi.apache.org/blog/2025/06/13/Optimizing-Apache-Hudi-Workflows-Automation-for-Clustering-Resizing-Concurrency</guid>
            <pubDate>Fri, 13 Jun 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://blog.datumagic.ai/p/apache-hudi-does-xyz-110">here</a></span>]]></content:encoded>
            <category>hudi</category>
            <category>blog</category>
            <category>halodoc</category>
            <category>data lake</category>
            <category>lakehouse</category>
            <category>apache hudi</category>
        </item>
        <item>
            <title><![CDATA[Exploring Apache Hudi’s New Log-Structured Merge (LSM) Timeline]]></title>
            <link>https://hudi.apache.org/blog/2025/05/29/lsm-timeline</link>
            <guid>https://hudi.apache.org/blog/2025/05/29/lsm-timeline</guid>
            <pubDate>Thu, 29 May 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Apache Hudi 1.0 introduces a new LSM Timeline to scale metadata management for long-lived tables. By restructuring timeline storage into a compacted, versioned tree layout, Hudi enables faster metadata access, snapshot isolation, and support for Non-Blocking Concurrency Control.]]></description>
            <content:encoded><![CDATA[<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>TL;DR</div><div class="admonitionContent_BuS1"><p>Apache Hudi 1.0 introduces a new LSM Timeline to scale metadata management for long-lived tables. By restructuring timeline storage into a compacted, versioned tree layout, Hudi enables faster metadata access, snapshot isolation, and support for Non-Blocking Concurrency Control.</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="apache-hudis-timeline">Apache Hudi’s Timeline<a href="https://hudi.apache.org/blog/2025/05/29/lsm-timeline#apache-hudis-timeline" class="hash-link" aria-label="Direct link to Apache Hudi’s Timeline" title="Direct link to Apache Hudi’s Timeline">​</a></h2>
<p>At the heart of Apache Hudi’s architecture is the <a href="https://hudi.apache.org/docs/timeline" target="_blank" rel="noopener noreferrer">Timeline</a> - a log-structured system that acts as the single source of truth for the table’s state at any point in time. The timeline records every change and operation performed on a Hudi table, encompassing writes, schema evolutions, compactions, cleanings, and clustering operations. This meticulous record-keeping empowers Hudi to deliver <a href="https://www.onehouse.ai/blog/acid-transactions-in-an-open-data-lakehouse" target="_blank" rel="noopener noreferrer">ACID guarantees</a>, robust <a href="https://hudi.apache.org/blog/2025/01/28/concurrency-control" target="_blank" rel="noopener noreferrer">concurrency control</a>, and advanced capabilities such as incremental processing, rollback/recovery, and time travel.</p>
<p>In essence, the timeline functions like a <a href="https://en.wikipedia.org/wiki/Write-ahead_logging" target="_blank" rel="noopener noreferrer">Write-Ahead Log (WAL)</a>, maintaining a sequence of immutable actions. Each action is recorded as a unique <em>instant</em> - a unit of work identified by its action type (e.g., commit, clean, compaction), a timestamp that marks when the action was initiated, and its lifecycle state. In Hudi, an <em>instant</em> refers to this combination of action, timestamp, and state (REQUESTED, INFLIGHT, or COMPLETED), and serves as the atomic unit of change on the timeline. These timeline entries are the backbone of Hudi’s transactional integrity, ensuring that every table change is atomically recorded and timeline-consistent. Every operation progresses through a lifecycle of <em>states</em>:</p>
<ul>
<li>REQUESTED: The action is planned and registered but not yet started.</li>
<li>INFLIGHT: The action is actively being performed, modifying table state.</li>
<li>COMPLETED: The action has successfully executed, and all data/metadata updates are finalized.</li>
</ul>
<p>These <em>instants</em> serve as both log entries and transaction markers, defining exactly what data is valid and visible at any given time. Whether you're issuing a snapshot query for the latest view, running an incremental query to fetch changes since the last checkpoint, or rolling back to a prior state, the timeline ensures that each action’s impact is precisely tracked. Every <em>action</em>, such as commit, clean, compaction, or rollback is explicitly recorded, allowing compute engines and tools to reason precisely about the table’s state transitions and history. This strict sequencing and lifecycle management also underpin Hudi’s ability to provide serializable isolation (the “I” in ACID) guarantees, ensuring that readers only observe committed data and consistent snapshots.</p>
<p>To optimize both performance and long-term storage scalability, Apache Hudi splits the timeline into two distinct components that work together to provide fast access to recent actions while ensuring historical records are retained efficiently. Let’s understand these in detail.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="active-timeline">Active Timeline<a href="https://hudi.apache.org/blog/2025/05/29/lsm-timeline#active-timeline" class="hash-link" aria-label="Direct link to Active Timeline" title="Direct link to Active Timeline">​</a></h3>
<p>The <a href="https://hudi.apache.org/docs/timeline#active-timeline" target="_blank" rel="noopener noreferrer">Active timeline</a> is the front line of Hudi’s transaction log. It contains the most recent and in-progress actions that are critical for building a consistent and up-to-date view of the table. Every time a new operation, such as a data write, compaction, clean, or rollback is initiated, it is immediately recorded here as a new instant file under the <code>.hoodie/</code> directory. Each of these files holds metadata about the action’s lifecycle, moving through the standard states of REQUESTED → INFLIGHT → COMPLETED.</p>
<p>The active timeline is consulted constantly - whether you are issuing a query, running compaction, or planning a new write operation. Compute engines read from the active timeline to determine what data files are valid and visible, making it the source of truth for the table’s latest state. To maintain performance, Hudi enforces a retention policy on the active timeline, i.e. it deliberately keeps only a window of the most recent actions, ensuring the timeline remains lightweight and quick to scan.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="archived-timeline">Archived Timeline<a href="https://hudi.apache.org/blog/2025/05/29/lsm-timeline#archived-timeline" class="hash-link" aria-label="Direct link to Archived Timeline" title="Direct link to Archived Timeline">​</a></h3>
<p>Tables naturally accumulate many more actions over time, especially in high-ingestion or update environments. As the number of instants grows, the active timeline can become bloated if left unchecked, introducing latency and performance penalties during reads and writes.</p>
<p>To solve this, Hudi implements an archival process. Once the number of active instants crosses a configured threshold, older actions are offloaded from the active timeline into the Archived Timeline stored in the <code>.hoodie/archive/</code> directory. This design ensures that while the active timeline remains lean and fast for day-to-day operations, the complete transactional history of the table is still preserved for auditing, recovery, and time travel purposes.</p>
<p>Although the archived timeline is optimized for long-term retention, accessing deep history can incur higher latency and overhead, especially in workloads with a large number of archived instants. This limitation is precisely what set the stage for the LSM Timeline innovation introduced in Hudi 1.0.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="problem-statement---why-move-to-an-lsm-timeline">Problem Statement - Why move to an LSM Timeline?<a href="https://hudi.apache.org/blog/2025/05/29/lsm-timeline#problem-statement---why-move-to-an-lsm-timeline" class="hash-link" aria-label="Direct link to Problem Statement - Why move to an LSM Timeline?" title="Direct link to Problem Statement - Why move to an LSM Timeline?">​</a></h2>
<p>Apache Hudi’s original timeline design served well for many workloads. By maintaining a lightweight active timeline for fast operations and offloading historical instants to the archive, Hudi struck a balance between performance and durability. However, there were some aspects to think about with the previous timeline design.</p>
<ul>
<li>
<p><strong>Linear Growth</strong>: The timeline grows linearly with each table action, whether it’s a commit, compaction, clustering, or rollback. Although Hudi’s archival process offloads older instants to keep the active timeline lean, the total number of instants (active + archived) continues to grow unbounded in long-lived tables. Over time, the accumulation of these instants can inflate metadata size, leading to slower scans and degraded query planning performance, especially for use cases like time travel and incremental queries.</p>
</li>
<li>
<p><strong>Latency &amp; Cost</strong>: Accessing the archived timeline, which is often required for time-travel, or recovery operations introduces high read latencies. This is because the archival format was optimized for durability and storage efficiency (many small Avro files), not for fast access. As the number of archived instants balloons, reading deep history involves scanning and deserializing large volumes of metadata, increasing both latency and compute cost. This can noticeably slow down operations like incremental syncs and historical audits.</p>
</li>
<li>
<p><strong>Cloud Storage Limitations</strong>: In cloud object stores like S3 or GCS, appending to existing files is not supported (or is highly inefficient). As a result, every new archival batch creates new small files, leading to a small-file problem. Over time, these fragmented archives accumulate, creating operational challenges in storage management and performance bottlenecks during metadata access, especially when files must be scanned individually across large object stores.</p>
</li>
<li>
<p><strong>Emerging Use Cases</strong>: Apache Hudi has evolved to support next-generation features such as non-blocking concurrency control (NBCC), infinite time travel, and fine-grained transaction metadata. These capabilities place heavier demands on the timeline architecture, requiring high-throughput writes and faster lookups across both recent and historical data.</p>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introducing-the-lsm-timeline">Introducing the LSM Timeline<a href="https://hudi.apache.org/blog/2025/05/29/lsm-timeline#introducing-the-lsm-timeline" class="hash-link" aria-label="Direct link to Introducing the LSM Timeline" title="Direct link to Introducing the LSM Timeline">​</a></h2>
<p>To overcome the scaling challenges of the original timeline architecture, Apache Hudi 1.0 introduced the <a href="https://hudi.apache.org/docs/timeline#timeline-components" target="_blank" rel="noopener noreferrer">LSM (Log-Structured Merge)</a> Timeline - a fundamentally new way to store and manage timeline metadata. This redesign brings together principles of <a href="https://en.wikipedia.org/wiki/Log-structured_merge-tree" target="_blank" rel="noopener noreferrer">log-structured storage</a>, tiered compaction, and snapshot versioning to deliver a highly scalable, cloud-native solution for tracking table history.</p>
<p>Hudi introduces a critical change in how time is represented on the timeline. Previously, Hudi treated time as instantaneous, i.e. each action appeared to take effect at a single instant. While effective for basic operations, this model proved limiting when implementing certain advanced features like <a href="https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control/" target="_blank" rel="noopener noreferrer">Non-Blocking Concurrency Control (NBCC)</a>, which require reasoning about actions as intervals of time to detect overlaps and resolve conflicts.</p>
<img src="https://hudi.apache.org/assets/images/blog/lsm_1.png" alt="index" width="800" align="middle">
<p>To address this, every action on the Hudi timeline now records both a <em>requested time</em> (when the action is initiated) and a <em>completion time</em> (when it finishes). This allows Hudi to track not just when an action was scheduled, but also how it interacts with other concurrent actions over time. To ensure global consistency across distributed processes, Hudi formalized the use of <a href="https://hudi.apache.org/docs/timeline#truetime-generation" target="_blank" rel="noopener noreferrer">TrueTime semantics</a>, guaranteeing that all instant times are monotonically increasing and globally ordered. This is a foundational requirement for precise conflict detection and robust transaction isolation.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-it-works--design">How It Works / Design<a href="https://hudi.apache.org/blog/2025/05/29/lsm-timeline#how-it-works--design" class="hash-link" aria-label="Direct link to How It Works / Design" title="Direct link to How It Works / Design">​</a></h3>
<img src="https://hudi.apache.org/assets/images/blog/lsm_2.png" alt="index" width="800" align="middle">
<p>At its core, the LSM timeline replaces the flat archival model with a layered tree structure, allowing Hudi to manage metadata for millions of historical instants efficiently, without compromising on read performance or consistency. Here’s how it’s designed:</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="file-organization">File Organization<a href="https://hudi.apache.org/blog/2025/05/29/lsm-timeline#file-organization" class="hash-link" aria-label="Direct link to File Organization" title="Direct link to File Organization">​</a></h4>
<ul>
<li>Metadata files are organized into layers (L0, L1, L2, …) following a Log-Structured Merge (LSM) tree layout.</li>
<li>Each file is a Parquet file that stores a batch of timeline instants. Their metadata entries are sorted chronologically by timestamp.</li>
<li>The files follow a precise naming convention: <code>${min_instant}_${max_instant}_${level}.parquet</code> where <code>min_instant</code> and <code>max_instant</code> represent the range of instants in the file and <code>level</code> denotes the layer (e.g., L0, L1, L2).</li>
<li>Files in the same layer may have overlapping time ranges, but the system tracks them via manifest files (more on that below).</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="compaction-strategy">Compaction Strategy<a href="https://hudi.apache.org/blog/2025/05/29/lsm-timeline#compaction-strategy" class="hash-link" aria-label="Direct link to Compaction Strategy" title="Direct link to Compaction Strategy">​</a></h4>
<ul>
<li>The LSM timeline uses a universal compaction strategy, similar to designs seen in modern databases.</li>
<li>Whenever N files (default: 10) accumulate in a given layer (e.g., L0), they are merged and flushed into the next layer (e.g., L1).</li>
<li>Compaction is governed by a size-based policy (default max file size ~1 GB), ensuring that write amplification is controlled and files stay within optimal size limits.</li>
<li>There’s no hard limit on the number of layers. The LSM tree naturally scales to handle massive tables with deep histories.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="version--manifest-management-snapshot-isolation">Version &amp; Manifest Management: Snapshot Isolation<a href="https://hudi.apache.org/blog/2025/05/29/lsm-timeline#version--manifest-management-snapshot-isolation" class="hash-link" aria-label="Direct link to Version &amp; Manifest Management: Snapshot Isolation" title="Direct link to Version &amp; Manifest Management: Snapshot Isolation">​</a></h4>
<ul>
<li>The LSM timeline introduces manifest files that record the current valid set of Parquet files representing the latest snapshot of the timeline.</li>
<li>Version files are generated alongside manifest files to maintain snapshot isolation, ensuring that readers and writers do not conflict.</li>
<li>This system supports multiple valid snapshot versions simultaneously (default: 3), enabling:<!-- -->
<ul>
<li>Consistent reads even during compaction.</li>
<li>Seamless evolution of the timeline without impacting query correctness.</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="reader-workflow">Reader Workflow<a href="https://hudi.apache.org/blog/2025/05/29/lsm-timeline#reader-workflow" class="hash-link" aria-label="Direct link to Reader Workflow" title="Direct link to Reader Workflow">​</a></h4>
<ul>
<li>When a query is made on the timeline:<!-- -->
<ul>
<li>The engine first fetches the latest version file.</li>
<li>It reads the corresponding manifest file to get the list of valid data files.</li>
<li>It scans only the relevant Parquet files, often using timestamp-based filtering to skip irrelevant data early.</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="cleaning-strategy">Cleaning Strategy<a href="https://hudi.apache.org/blog/2025/05/29/lsm-timeline#cleaning-strategy" class="hash-link" aria-label="Direct link to Cleaning Strategy" title="Direct link to Cleaning Strategy">​</a></h4>
<ul>
<li>The LSM timeline performs cleaning only after successful compaction, ensuring that no active snapshot is disrupted.</li>
<li>By default, Hudi retains 3 valid snapshot versions to support concurrent readers/writers.</li>
<li>Files are retained for at least 3 archival trigger intervals, providing a grace period before old data is purged.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="what-it-brings-to-the-table-benefits">What It Brings to the Table (Benefits)<a href="https://hudi.apache.org/blog/2025/05/29/lsm-timeline#what-it-brings-to-the-table-benefits" class="hash-link" aria-label="Direct link to What It Brings to the Table (Benefits)" title="Direct link to What It Brings to the Table (Benefits)">​</a></h3>
<p>The LSM timeline unlocks significant advancements in how Apache Hudi handles metadata, providing both performance improvements and new capabilities.</p>
<ul>
<li>
<p><strong>Scalability:</strong> The LSM timeline architecture allows Hudi to manage virtually infinite timeline history while keeping both read and write performance predictable. Whether it's thousands or millions of instants, the layered compaction model ensures stable metadata performance over time, supporting efficient query and metadata access even as tables grow in size and history length.</p>
</li>
<li>
<p><strong>Efficient Reads:</strong> Readers benefit from manifest-guided lookups, allowing them to scan only the specific set of files relevant to their query. By using Parquet’s columnar format and timestamp-based filtering, Hudi dramatically reduces the overhead of accessing deep historical metadata.</p>
</li>
<li>
<p><strong>Non-Blocking Concurrency Control (NBCC):</strong> One of the most powerful capabilities enabled by the LSM timeline is Non-Blocking Concurrency Control, allowing multiple writers to operate concurrently on the same table (and even the same file group) without the need for explicit locks - except during final commit metadata updates.</p>
</li>
<li>
<p><strong>Cloud-Native Optimization</strong>: By compacting small files into large Parquet files, the LSM timeline avoids the small-file problem common in cloud storage systems like Amazon S3 or Google Cloud Storage. This improves both query performance and storage cost efficiency.</p>
</li>
<li>
<p><strong>Snapshot Isolation &amp; Consistency</strong>: The manifest + version file mechanism ensures that concurrent operations remain isolated and consistent, even as background compaction and cleaning occur. This provides strong transactional guarantees without sacrificing performance.</p>
</li>
<li>
<p><strong>Maintenance-Free Scalability</strong>: The universal compaction and smart cleaning strategies keep the timeline healthy over time, requiring minimal manual tuning, while ensuring that old data is cleaned up safely only after valid snapshots are no longer in use.</p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="performance">Performance<a href="https://hudi.apache.org/blog/2025/05/29/lsm-timeline#performance" class="hash-link" aria-label="Direct link to Performance" title="Direct link to Performance">​</a></h3>
<p>Micro-benchmarks show that the LSM Timeline scales efficiently even as the number of timeline actions grows by orders of magnitude. Reading just the instant times for <code>10,000</code> actions takes around <code>32ms</code>, while fetching full metadata takes <code>150ms</code>. At larger scales, such as <code>10 million</code> actions, metadata reads completes in about <code>162 seconds</code>.</p>
<p>These results demonstrate that Hudi's LSM timeline can handle decades of high-frequency commits (e.g., one every 30 seconds for 10+ years) while keeping metadata access performant.</p>
<p>The LSM timeline represents a natural progression in Apache Hudi’s timeline architecture, designed to address the growing demands of large-scale and long-lived tables. Hudi’s timeline has been foundational for transactional integrity, time travel, and incremental processing capabilities. The new LSM-based design enhances scalability and operational efficiency by introducing a layered, compacted structure with manifest-driven snapshot isolation. This improvement allows Hudi to manage extensive metadata histories more efficiently, maintain predictable performance, and better support advanced use cases such as non-blocking concurrency control.</p>
<hr>]]></content:encoded>
            <category>Apache Hudi</category>
            <category>LSM Tree</category>
            <category>Performance</category>
            <category>Non-Blocking Concurrency Control</category>
        </item>
        <item>
            <title><![CDATA[How Doris + Hudi Turned the Impossible Into the Everyday]]></title>
            <link>https://hudi.apache.org/blog/2025/04/14/doris-hudi-making-impossible-possible</link>
            <guid>https://hudi.apache.org/blog/2025/04/14/doris-hudi-making-impossible-possible</guid>
            <pubDate>Mon, 14 Apr 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://dzone.com/articles/doris-hudi-making-impossible-possible">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>Apache Hudi</category>
            <category>Apache Doris</category>
            <category>use-case</category>
            <category>federated querying</category>
            <category>dzone</category>
        </item>
        <item>
            <title><![CDATA[Why Walmart Chose Apache Hudi for Their Lakehouse]]></title>
            <link>https://hudi.apache.org/blog/2025/04/09/why-walmart-chose-apache-hudi-for-their-lakehouse</link>
            <guid>https://hudi.apache.org/blog/2025/04/09/why-walmart-chose-apache-hudi-for-their-lakehouse</guid>
            <pubDate>Wed, 09 Apr 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://blog.det.life/why-walmart-chose-apache-hudi-for-their-lakehouse-c0a3574db0ba">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>Apache Hudi</category>
            <category>use-case</category>
            <category>det</category>
        </item>
        <item>
            <title><![CDATA[ From Swamp to Stream: How Apache Hudi Transforms the Modern Data Lake]]></title>
            <link>https://hudi.apache.org/blog/2025/04/06/from-swamp-to-stream-how-apache-hudi-transforms-the-modern-data-lake</link>
            <guid>https://hudi.apache.org/blog/2025/04/06/from-swamp-to-stream-how-apache-hudi-transforms-the-modern-data-lake</guid>
            <pubDate>Sun, 06 Apr 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://medium.com/aimonks/from-swamp-to-stream-how-apache-hudi-transforms-the-modern-data-lake-8a938f517ea1">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>Apache Hudi</category>
            <category>real-time datalake</category>
            <category>incremental processing</category>
            <category>upserts</category>
            <category>medium</category>
        </item>
        <item>
            <title><![CDATA[Integrating Apache Doris and Hudi for Data Querying and Migration]]></title>
            <link>https://hudi.apache.org/blog/2025/04/03/integrate-apache-doris-hudi-data-querying-migration</link>
            <guid>https://hudi.apache.org/blog/2025/04/03/integrate-apache-doris-hudi-data-querying-migration</guid>
            <pubDate>Thu, 03 Apr 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://dzone.com/articles/integrate-apache-doris-hudi-data-querying-migration">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>Apache Hudi</category>
            <category>Apache Doris</category>
            <category>real-time query</category>
            <category>how-to</category>
            <category>dzone</category>
        </item>
        <item>
            <title><![CDATA[Introducing Secondary Index in Apache Hudi Lakehouse Platform]]></title>
            <link>https://hudi.apache.org/blog/2025/04/02/secondary-index</link>
            <guid>https://hudi.apache.org/blog/2025/04/02/secondary-index</guid>
            <pubDate>Wed, 02 Apr 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Apache Hudi 1.0 introduces Secondary Indexes, enabling faster queries on non-primary key fields. This improves data retrieval in Lakehouse architectures by reducing data scans. Hudi also offers asynchronous indexing for scalability and efficient index maintenance without disrupting data ingestion. By the end of this blog, you'll understand how these features enhance Hudi's capabilities as a high-performance lakehouse platform.]]></description>
            <content:encoded><![CDATA[<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>TL;DR</div><div class="admonitionContent_BuS1"><p>Apache Hudi 1.0 introduces Secondary Indexes, enabling faster queries on non-primary key fields. This improves data retrieval in Lakehouse architectures by reducing data scans. Hudi also offers asynchronous indexing for scalability and efficient index maintenance without disrupting data ingestion. By the end of this blog, you'll understand how these features enhance Hudi's capabilities as a high-performance lakehouse platform.</p></div></div>
<p>Indexes are a fundamental data structure that enables efficient data retrieval by eliminating the need to scan the entire dataset for every query. In the context of a Lakehouse, where records are written as immutable data files (such as Parquet) at scale, indexing becomes crucial in reducing lookup times. Otherwise, a lot of time will be spent by the compute engine on finding out where exactly a particular record exists amongst thousands of files in the data lake storage, which is computationally expensive at scale. Indexing is not only important for <em>reads</em> in a lakehouse architecture, but also for <em>writes</em>, such as upserts and deletes, as you need to know where the record is to update it.</p>
<p>One of the standout design choices in Apache Hudi that separates it from other lakehouse formats is its <a href="https://hudi.apache.org/docs/next/indexes/" target="_blank" rel="noopener noreferrer">indexing</a> capability, which has been central to its architecture from the beginning. Hudi is heavily optimized to handle mutable change streams with varying write patterns, and indexing plays a pivotal role in making upserts and deletes efficient.</p>
<p>Hudi's indexing mechanism is designed to efficiently manage record lookups and updates by maintaining a structured mapping between records and file groups. Here's how it works:</p>
<ul>
<li>
<p>The first time a record is ingested into Hudi, it is assigned to a <a href="https://hudi.apache.org/tech-specs/#file-layout-hierarchy" target="_blank" rel="noopener noreferrer">File Group</a> - a logical grouping of files. This assignment typically remains unchanged throughout the record's lifecycle. However, in cases such as clustering or cross-partition updates, the record may be remapped to a different file group. Even in such scenarios, Hudi ensures that a given record key is associated with exactly one file group at any completed instant on the timeline</p>
</li>
<li>
<p>Hudi maintains a mapping between the incoming <a href="https://hudi.apache.org/docs/key_generation" target="_blank" rel="noopener noreferrer">record’s key</a> (unique identifier) and the File Group where it resides.</p>
</li>
<li>
<p>The index is responsible for quickly locating records based on this File Group mapping, eliminating the need for full dataset scans.</p>
</li>
</ul>
<p>This strategy allows Hudi to determine whether a record exists and pinpoint its exact location, enabling faster upserts and deletes.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="apache-hudis-multi-modal-indexing-system">Apache Hudi's Multi-Modal Indexing System<a href="https://hudi.apache.org/blog/2025/04/02/secondary-index#apache-hudis-multi-modal-indexing-system" class="hash-link" aria-label="Direct link to Apache Hudi's Multi-Modal Indexing System" title="Direct link to Apache Hudi's Multi-Modal Indexing System">​</a></h2>
<p>While Hudi’s indexes have set a benchmark for fast writes, bringing those advantages to queries was equally important. This led to the design of a generalized indexing subsystem that enhances performance in the lakehouse. Hudi’s <a href="https://www.onehouse.ai/blog/introducing-multi-modal-index-for-the-lakehouse-in-apache-hudi" target="_blank" rel="noopener noreferrer">multi-modal indexing</a> redefines indexing in data lakes by employing multiple index types, each optimized for different workloads and query patterns. It is built on scalable metadata that supports multiple index types without extra overhead, ACID-compliant updates to keep indexes in sync with the data table, and optimized lookups that minimize full scans for low-latency queries on large datasets.</p>
<p>At the core of Hudi’s indexing design is its <a href="https://hudi.apache.org/docs/metadata" target="_blank" rel="noopener noreferrer">metadata table</a>, a specialized Merge-on-Read table that houses multiple index types as separate partitions. These indexes serve various purposes, improving the efficiency of reads, writes, and upserts.</p>
<img src="https://hudi.apache.org/assets/images/blog/hudi-stack-indexes.png" alt="index" width="800" align="middle">
<p>Some key indexes within Hudi’s metadata table include:</p>
<ul>
<li>File Index - Stores a compact listing of files, reducing the overhead of expensive file system operations.</li>
<li>Column Stats Index - Tracks min/max statistics for each column, enabling more effective data pruning.</li>
<li>Bloom Filter Index - Stores precomputed bloom filters for all data files, optimizing record lookups.</li>
<li>Partition Stats Index - Stores aggregated partition-related information which helps in efficient partition pruning by skipping entire folders very quickly.</li>
<li>Record-Level Index - Maintains direct mappings to individual records, facilitating faster upserts and deletes.</li>
<li>Secondary Index - Allow users to create indexes on columns that are not part of record key columns in Hudi tables.</li>
</ul>
<p>By structuring these indexes as individual partitions within the metadata table, Hudi ensures efficient retrieval, quick lookups, and scalability, even as the data volume grows. In this blog, we will focus on secondary indexes and understand how it can help accelerate query performance in a lakehouse.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introducing-secondary-index">Introducing Secondary Index<a href="https://hudi.apache.org/blog/2025/04/02/secondary-index#introducing-secondary-index" class="hash-link" aria-label="Direct link to Introducing Secondary Index" title="Direct link to Introducing Secondary Index">​</a></h2>
<p>A secondary index is an indexing mechanism commonly used in database systems to provide efficient access to records based on non-primary key attributes. Unlike primary indexes, which enforce uniqueness and define the main data layout, secondary indexes serve as auxiliary data structures that accelerate lookups on fields that are frequently queried but are not the primary key.</p>
<p>For example, in an OLTP (Online Transaction Processing) database, a primary index might be defined on a unique <code>order_id</code>, whereas a secondary index could be created on <code>customer_id</code> to quickly fetch all orders placed by a specific customer. Secondary indexes enhance query performance by reducing the need for full table scans, especially in analytical workloads that involve complex filtering or joins.</p>
<p>With <a href="https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0/" target="_blank" rel="noopener noreferrer">Hudi 1.0</a>, Apache Hudi introduces <a href="https://hudi.apache.org/docs/next/indexes#secondary-index" target="_blank" rel="noopener noreferrer">secondary indexes</a>, bringing database-style indexing capabilities to the Lakehouse. Secondary indexes allow queries to scan significantly fewer files, reducing query latency and compute costs. This is especially beneficial for cloud-based query engines (such as AWS Athena), where pricing is based on the amount of data scanned. A secondary index in Hudi allows users to index any column beyond the record key (primary key), making queries on non-primary key fields much faster. This extends Hudi’s existing <a href="https://hudi.apache.org/blog/2023/11/01/record-level-index/" target="_blank" rel="noopener noreferrer">record-level index</a>, which optimizes writes and reads based on the record key.</p>
<img src="https://hudi.apache.org/assets/images/blog/secondary_index.png" alt="sec_index" width="800" align="middle">
<p>Here is how the secondary index works in Hudi.</p>
<ul>
<li>Indexes Non-Primary Key Columns: Unlike the record-level index, which tracks record keys, secondary indexes help accelerate queries on fields outside the primary key.</li>
<li>Stores Mappings Between Secondary and Primary Keys: Hudi maintains a mapping between secondary keys (e.g., city, driver) and record keys, enabling fast lookups for non-primary key queries.</li>
<li>Minimizes Data Scans via Index-Aware Query Execution: During query execution, the secondary index enables data skipping, allowing Hudi to prune unnecessary files before scanning.</li>
<li>SQL-Based Index Management: Users can create, drop, and manage indexes using SQL, making secondary indexes easily accessible.</li>
</ul>
<p>Hudi supports hash-based secondary indexes, which are horizontally scalable by distributing keys across shards for fast writes and lookups.</p>
<p>If you are interested in the implementation details of secondary indexes, you can read more <a href="https://hudi.apache.org/tech-specs-1point0/#secondary-index" target="_blank" rel="noopener noreferrer">here</a>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="creating-a-secondary-index-in-hudi">Creating a Secondary Index in Hudi<a href="https://hudi.apache.org/blog/2025/04/02/secondary-index#creating-a-secondary-index-in-hudi" class="hash-link" aria-label="Direct link to Creating a Secondary Index in Hudi" title="Direct link to Creating a Secondary Index in Hudi">​</a></h3>
<p>In Hudi 1.0, secondary indexes are supported currently in Apache Spark, with future support planned for Flink, Presto, and Trino in Hudi 1.1.</p>
<p>Let’s see an example of creating a Hudi table with a secondary index.</p>
<p>First, let’s create a table with a record index enabled. The record index maintains mappings of record keys (<code>id</code>) to file groups, enabling fast updates, deletes, and lookups.</p>
<div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">DROP</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">TABLE</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">IF</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">EXISTS</span><span class="token plain"> hudi_table</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">TABLE</span><span class="token plain"> hudi_table </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ts </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">BIGINT</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    id STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    rider STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    driver STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    fare </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">DOUBLE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    city STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    state STRING</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">USING</span><span class="token plain"> hudi</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">OPTIONS </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    primaryKey </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'id'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    hoodie</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">metadata</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">record</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">index</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">enable</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'true'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)">-- Enable record index</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    hoodie</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">write</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">record</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">merge</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">mode</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"COMMIT_TIME_ORDERING"</span><span class="token plain"> </span><span class="token comment" style="color:rgb(98, 114, 164)">-- Only Required for 1.0.0 version</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">PARTITIONED </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">BY</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">city</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> state</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">LOCATION </span><span class="token string" style="color:rgb(255, 121, 198)">'file:///tmp/hudi_test_table'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><br></span></code></pre></div></div>
<p>Now we can create a secondary index on the <code>city</code> field to optimize queries filtering on this column.</p>
<div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INDEX</span><span class="token plain"> idx_city </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">ON</span><span class="token plain"> hudi_table</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">city</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><br></span></code></pre></div></div>
<p>Now, when executing a query such as:</p>
<div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">SELECT</span><span class="token plain"> rider </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">FROM</span><span class="token plain"> hudi_table </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">WHERE</span><span class="token plain"> city </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'SFO'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><br></span></code></pre></div></div>
<p>✅ Hudi first checks the secondary index to determine which records match the filter condition.<br>
<!-- -->✅ It then uses the record index to locate the exact file group for retrieval.<br>
<!-- -->✅ Data skipping is applied, reducing the number of files read from cloud storage.</p>
<p>Users can also create secondary indexes using the Spark DataSource API by setting the following configurations:</p>
<table><thead><tr><th style="text-align:left">Config Name</th><th style="text-align:left">Default</th><th style="text-align:left">Description</th></tr></thead><tbody><tr><td style="text-align:left"><code>hoodie.metadata.index.secondary.enable</code></td><td style="text-align:left">true</td><td style="text-align:left">Enables secondary index maintenance. When true, Hudi writers automatically maintain all secondary indexes within the metadata table. When disabled, secondary indexes must be created manually using SQL.</td></tr><tr><td style="text-align:left"><code>hoodie.datasource.write.secondarykey.column</code></td><td style="text-align:left">(N/A)</td><td style="text-align:left">Specifies the columns to be used as secondary keys. Supports dot notation for nested fields (e.g., <code>customer.region</code>).</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="asynchronous-indexing-in-hudi">Asynchronous Indexing in Hudi<a href="https://hudi.apache.org/blog/2025/04/02/secondary-index#asynchronous-indexing-in-hudi" class="hash-link" aria-label="Direct link to Asynchronous Indexing in Hudi" title="Direct link to Asynchronous Indexing in Hudi">​</a></h3>
<p>A notable thing about Hudi’s indexing system is that it offers <a href="https://www.onehouse.ai/blog/asynchronous-indexing-using-hudi" target="_blank" rel="noopener noreferrer">asynchronous indexing</a> as a service. Traditional indexing approaches often introduce performance bottlenecks, as index maintenance needs to be performed synchronously with writes. Hudi’s asynchronous indexing service eliminates the performance bottlenecks of traditional indexing by decoupling index maintenance from ingestion. Instead of requiring synchronous updates that slow down writes, Hudi builds indexes in the background, ensuring ingestion remains uninterrupted.</p>
<p>A key aspect of this design is timeline-consistent indexing, where a new indexing action is introduced in Hudi’s transactional <a href="https://hudi.apache.org/docs/timeline" target="_blank" rel="noopener noreferrer">timeline</a>. The indexer service schedules indexing by adding an <code>indexing.requested</code> instant, moves it to <code>inflight</code> during execution, and finally marks it <code>completed</code> once indexing is done, without locking index file writes. This enables a scalable indexing framework, allowing indexes to be dynamically added or removed without downtime as datasets grow. Async indexing also supports multiple index types, including secondary indexes.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="benchmarking">Benchmarking<a href="https://hudi.apache.org/blog/2025/04/02/secondary-index#benchmarking" class="hash-link" aria-label="Direct link to Benchmarking" title="Direct link to Benchmarking">​</a></h2>
<p>We ran a simple benchmark using the TPCDS 1TB dataset, created the index on one of the fact table <code>web_sales</code> and ran a complex join query with lookup on customer id.</p>
<p><strong>Setup:</strong></p>
<ul>
<li>Uses 1TB TPCDS public dataset.</li>
<li>Apache Spark version -  3.5.5 installed on EMR cluster</li>
<li>Apache Hudi version - 1.0.1</li>
<li>Table on which secondary index is created - <code>web_sales</code></li>
<li>Column on which Secondary Index is created - <code>ws_ship_customer_sk</code></li>
<li>Cluster Configurations<!-- -->
<ul>
<li>Nodes: m5.xlarge (10 executors)</li>
</ul>
</li>
</ul>
<p>To evaluate performance, we executed the same query multiple times within the same Spark session. The table below demonstrates an approximately <strong>33%</strong> improvement in the first run and a <strong>58%</strong> improvement in the second run. Additionally, the amount of data scanned was reduced by <strong>90%</strong> when using the secondary index.</p>
<table><thead><tr><th style="text-align:left"></th><th style="text-align:left">Run 1</th><th style="text-align:left">Run 2</th><th style="text-align:left">Files Read</th><th style="text-align:left">File Size Read</th><th style="text-align:left">Rows Scanned</th></tr></thead><tbody><tr><td style="text-align:left">Without Secondary index</td><td style="text-align:left">32 sec</td><td style="text-align:left">14 sec</td><td style="text-align:left">5000</td><td style="text-align:left">67 GB</td><td style="text-align:left">719 M</td></tr><tr><td style="text-align:left">With Secondary Index</td><td style="text-align:left">22 sec</td><td style="text-align:left">6 sec</td><td style="text-align:left">521</td><td style="text-align:left">7 GB</td><td style="text-align:left">75 M</td></tr></tbody></table>
<p><strong>Read Query used for Benchmarking:</strong></p>
<div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">SELECT</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   ws</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ws_order_number</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   ws</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ws_item_sk</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   ws</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ws_quantity</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   ws</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ws_sales_price</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   c</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">c_customer_id</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   c</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">c_first_name</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   c</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">c_last_name</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   d</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">d_date</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   wp</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">wp_web_page_id</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">FROM</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   web_sales ws</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">JOIN</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   tpcds_hudi_1tb</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">customer c </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">ON</span><span class="token plain"> ws</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ws_ship_customer_sk </span><span class="token operator">=</span><span class="token plain"> c</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">c_customer_sk</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">JOIN</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   tpcds_hudi_1tb</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">date_dim d </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">ON</span><span class="token plain"> ws</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ws_ship_date_sk </span><span class="token operator">=</span><span class="token plain"> d</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">d_date_sk</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">JOIN</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   tpcds_hudi_1tb</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">web_page wp </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">ON</span><span class="token plain"> ws</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ws_web_page_sk </span><span class="token operator">=</span><span class="token plain"> wp</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">wp_web_page_sk</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">WHERE</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   ws</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ws_ship_customer_sk </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'647632'</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">ORDER</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">BY</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   ws</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ws_order_number</span><br></span></code></pre></div></div>
<p>As shown in the DAG below, there is a significant difference in the amount of data scanned and other metrics (see the highlighted part) for the websales table, both with and without the secondary index.</p>
<p><strong>Spark SQL Stats  with Secondary index</strong></p>
<img src="https://hudi.apache.org/assets/images/blog/sec_index_spark1.png" alt="orch" width="600" align="middle">
<p><strong>Spark SQL Stats  without Secondary index</strong></p>
<img src="https://hudi.apache.org/assets/images/blog/sec_index_spark2.png" alt="orch" width="600" align="middle">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://hudi.apache.org/blog/2025/04/02/secondary-index#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>Indexing has been a core component of Apache Hudi since its inception, enabling efficient upserts and deletes at scale. With Hudi 1.0, the introduction of secondary indexing expands these capabilities by allowing queries to efficiently filter and retrieve records based on <em>non-primary key</em> fields, significantly reducing data scans and improving query performance. Looking ahead, secondary indexing in Hudi opens new possibilities for further optimizations, such as accelerating complex joins and MERGE INTO operations.</p>
<p>Additionally, to ensure that index maintenance does not introduce bottlenecks, Hudi’s <em>asynchronous indexing</em> service decouples index updates from ingestion, enabling seamless scaling while keeping indexes timeline-consistent and ACID-compliant. These advancements further solidify Hudi’s role as a high-performance lakehouse platform, making data structures such as secondary indexes more accessible.</p>
<hr>]]></content:encoded>
            <category>Apache Hudi</category>
            <category>Indexing</category>
            <category>Performance</category>
        </item>
        <item>
            <title><![CDATA[Powering Amazon Unit Economics at Scale Using Apache Hudi]]></title>
            <link>https://hudi.apache.org/blog/2025/03/31/amazon-hudi</link>
            <guid>https://hudi.apache.org/blog/2025/03/31/amazon-hudi</guid>
            <pubDate>Mon, 31 Mar 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Amazon’s Profit Intelligence team built Nexus, a configuration-driven platform powered by Apache Hudi, to scale unit economics across thousands of retail use cases. Nexus manages over 1,200 tables, processes hundreds of billions of rows daily, and handles ~1 petabyte of data churn each month. This blog dives into their data lakehouse journey, Nexus architecture, Hudi integration, and key operational learnings.]]></description>
            <content:encoded><![CDATA[<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>TL;DR</div><div class="admonitionContent_BuS1"><p>Amazon’s Profit Intelligence team built Nexus, a configuration-driven platform powered by Apache Hudi, to scale unit economics across thousands of retail use cases. Nexus manages over 1,200 tables, processes hundreds of billions of rows daily, and handles ~1 petabyte of data churn each month. This blog dives into their data lakehouse journey, Nexus architecture, Hudi integration, and key operational learnings.</p></div></div>
<p>Understanding and improving unit-level profitability at Amazon's scale is a massive challenge - one that requires flexibility, precision, and operational efficiency. In this blog, we walk through how Amazon’s Profit Intelligence team built a scalable, configuration-driven platform called Nexus, and how Apache Hudi became the cornerstone of its data lake architecture.</p>
<p>By combining declarative configuration with Hudi's advanced table management capabilities, the team has enabled thousands of retail business use cases to run seamlessly, allowing finance and pricing teams to self-serve insights on cost and profitability, without constantly relying on engineering intervention.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-business-need-profit-intelligence-and-unit-economics">The Business Need: Profit Intelligence and Unit Economics<a href="https://hudi.apache.org/blog/2025/03/31/amazon-hudi#the-business-need-profit-intelligence-and-unit-economics" class="hash-link" aria-label="Direct link to The Business Need: Profit Intelligence and Unit Economics" title="Direct link to The Business Need: Profit Intelligence and Unit Economics">​</a></h2>
<p>Within Amazon’s Worldwide Stores, the Selling Partner Services (SPS) team supports seller-facing operations. A key part of this effort is computing <strong>Contribution Profit</strong> - a granular metric that captures revenue, costs, and profitability at the unit level, such as <em>a shipped item to the customer</em>.</p>
<p>Contribution Profit powers decision-making for a range of downstream teams including:</p>
<ul>
<li>Pricing</li>
<li>Forecasting</li>
<li>Finance</li>
</ul>
<p>The challenge? Supporting the scale and diversity of retail use cases across Amazon's global business, while maintaining a data platform that's both extensible and maintainable.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="amazons-data-lakehouse-journey">Amazon’s Data Lakehouse Journey<a href="https://hudi.apache.org/blog/2025/03/31/amazon-hudi#amazons-data-lakehouse-journey" class="hash-link" aria-label="Direct link to Amazon’s Data Lakehouse Journey" title="Direct link to Amazon’s Data Lakehouse Journey">​</a></h2>
<p>Over the past decade, the architecture behind Contribution Profit has gone through several phases of evolution, driven by the need to better support Amazon’s growing and diverse retail business use cases.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="early-phase">Early Phase<a href="https://hudi.apache.org/blog/2025/03/31/amazon-hudi#early-phase" class="hash-link" aria-label="Direct link to Early Phase" title="Direct link to Early Phase">​</a></h3>
<img src="https://hudi.apache.org/assets/images/blog/fig1_amz.png" alt="redshift" width="800" align="middle">
<p>Initial implementations relied on ETL pipelines that published data to Redshift, often with unstructured job flows. Business logic could exist at various layers of the ETL and was written entirely in SQL, making it difficult to track, maintain, or modify. These pipelines lacked systematic enforcement of patterns, which led to fragmentation and technical debt.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="intermediate-phase">Intermediate Phase<a href="https://hudi.apache.org/blog/2025/03/31/amazon-hudi#intermediate-phase" class="hash-link" aria-label="Direct link to Intermediate Phase" title="Direct link to Intermediate Phase">​</a></h3>
<img src="https://hudi.apache.org/assets/images/blog/fig2_amz.png" alt="flink" width="800" align="middle">
<p>To improve scalability and support streaming workloads, the team transitioned to a setup involving Apache Flink and a custom-built data lake. Although this introduced broader data processing flexibility, it still had major drawbacks:</p>
<ul>
<li>Redshift-based ETLs remained in use.</li>
<li>Business logic and schema changes required engineering involvement.</li>
<li>There were ongoing scalability and maintainability issues with the custom data lake.</li>
<li>Flink introduced operational challenges of its own, such as handling version upgrades through AWS Managed Flink and providing done signal in batch operation.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="current-state-nexus--apache-hudi">Current State: Nexus + Apache Hudi<a href="https://hudi.apache.org/blog/2025/03/31/amazon-hudi#current-state-nexus--apache-hudi" class="hash-link" aria-label="Direct link to Current State: Nexus + Apache Hudi" title="Direct link to Current State: Nexus + Apache Hudi">​</a></h3>
<p>Each of the prior approaches came with tradeoffs, especially around business logic being tightly coupled with code, making it hard for non-engineers to simulate or change metrics for a specific retail business.</p>
<p>Recognizing the need for better abstraction and operational maturity, the team built Nexus - a configuration-driven platform for defining and orchestrating data pipelines. All lake interactions including ingestion, transformation, schema evolution, and table management now go through Nexus. Nexus is powered by <a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer"><strong>Apache Hudi</strong></a>, which provides the foundation for scalable ingestion, efficient upserts, schema evolution, and transactional guarantees on Amazon S3.</p>
<p>This new architecture enabled the team to decouple business logic from engineering code, allowing business teams to define logic declaratively. It also introduced standardization across workloads, eliminated redundant pipelines, and laid the groundwork for scaling unit economics calculations across thousands of use cases.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="key-modules-of-nexus">Key Modules of Nexus<a href="https://hudi.apache.org/blog/2025/03/31/amazon-hudi#key-modules-of-nexus" class="hash-link" aria-label="Direct link to Key Modules of Nexus" title="Direct link to Key Modules of Nexus">​</a></h4>
<img src="https://hudi.apache.org/assets/images/blog/fig3_amz.png" alt="nexus" width="800" align="middle">
<p>Nexus consists of four core components:</p>
<p><strong>Configuration Layer</strong></p>
<p>The topmost layer where users define their business logic in a declarative format. These configurations are typically generated and enriched with metadata by internal systems.</p>
<p><strong>NexusFlow (Orchestration)</strong></p>
<img src="https://hudi.apache.org/assets/images/blog/fig4_amz.png" alt="orch" width="1000" align="middle">
<p align="center"><em>Figure: Sample NexusFlow Config</em></p>
<p>Responsible for generating and executing workflows. It operates on two levels:</p>
<ul>
<li>Logical Layer: Comprising NexusETL jobs and other tasks.</li>
<li>Physical Layer: Implemented via AWS Step Functions to orchestrate EMR jobs and related dependencies. NexusFlow supports extensibility through a federated model and can execute diverse task types like Spark jobs, Redshift queries, wait conditions, and legacy ETLs.</li>
</ul>
<p><strong>NexusETL (Execution)</strong></p>
<img src="https://hudi.apache.org/assets/images/blog/fig5_amz.png" alt="etl" width="1000" align="middle">
<p align="center"><em>Figure: Sample NexusETL Config</em></p>
<p>Executes Spark-based data transformation jobs. Jobs are defined entirely in configuration, with support for:</p>
<ul>
<li>Built-in transforms like joins and filters</li>
<li>Custom UDFs</li>
<li>Source/Sink/Transform operators: It operates at the job abstraction level and is typically invoked by NexusFlow during orchestration.</li>
</ul>
<p><strong>NexusDataLake (Storage)</strong></p>
<img src="https://hudi.apache.org/assets/images/blog/fig5_amz.png" alt="datalake" width="1000" align="middle">
<p align="center"><em>Figure: Sample NexusDataLake Config</em></p>
<p>A storage abstraction layer built on Apache Hudi. NexusDataLake manages:</p>
<ul>
<li>Table creation</li>
<li>Schema inference and evolution</li>
<li>Catalog integration: All interactions with Hudi, such as inserts, upserts, table schema changes, and metadata syncs are funneled through NexusETL and NexusFlow, maintaining consistency across the platform.</li>
</ul>
<p>By standardizing how data is defined, processed, and stored, Nexus has enabled a scalable, maintainable, and extensible architecture. Every data lake interaction - from ingestion to table maintenance, is performed through this configuration-first model, which now powers hundreds of use cases across Amazon retail.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-apache-hudi">Why Apache Hudi?<a href="https://hudi.apache.org/blog/2025/03/31/amazon-hudi#why-apache-hudi" class="hash-link" aria-label="Direct link to Why Apache Hudi?" title="Direct link to Why Apache Hudi?">​</a></h2>
<p>Apache Hudi has been central to Nexus’ success, providing the core data lake storage layer for scalable ingestion, updates, and metadata management. It enables fast, incremental updates at massive scale while maintaining transactional guarantees on top of Amazon S3.</p>
<p>In Amazon’s current architecture:</p>
<ul>
<li>Copy-on-Write (COW) table type is used for all Hudi tables.</li>
<li>Workloads generate hundreds of billions of row updates daily, with write patterns spanning concentrated single-partition updates and wide-range backfills across up to 90 partitions.</li>
<li>All Hudi interactions, including inserts, schema changes, and metadata syncs, are managed through Nexus.</li>
</ul>
<p><strong>Key Capabilities used with Apache Hudi</strong></p>
<ul>
<li>
<p><strong>Efficient Upserts</strong><br>
<!-- -->Hudi’s design primitives such as <a href="https://hudi.apache.org/docs/indexes" target="_blank" rel="noopener noreferrer">indexes</a> for Copy-on-Write (CoW) tables enable high-throughput update patterns by avoiding the need to join against the entire dataset to determine which files to rewrite, which is particularly critical for our daily workloads.</p>
</li>
<li>
<p><strong>Incremental Processing</strong><br>
<!-- -->By using Hudi’s native <a href="https://www.onehouse.ai/blog/getting-started-incrementally-process-data-with-apache-hudi" target="_blank" rel="noopener noreferrer">incremental pull</a> capabilities, downstream systems are able to consume only the changes between commits. This is essential for efficiently updating Contribution Profit metrics that power business decision-making.</p>
</li>
<li>
<p><strong>Metadata Table</strong><br>
<!-- -->Enabling the <a href="https://hudi.apache.org/docs/metadata" target="_blank" rel="noopener noreferrer">metadata table</a> (<code>hoodie.metadata.enable=true</code>) significantly reduced job runtimes by avoiding expensive file listings on S3. This is an important optimization given the scale at which we process updates across more than 1200 Hudi tables.</p>
</li>
<li>
<p><strong>Schema Evolution</strong><br>
<!-- -->Table creation and evolution are fully managed through configuration in Nexus. Hudi’s built-in support for <a href="https://hudi.apache.org/docs/schema_evolution" target="_blank" rel="noopener noreferrer">schema evolution</a> has allowed the team to onboard new use cases and adapt to changing schemas without requiring expensive rewrites or manual interventions.</p>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-learnings-from-operating-hudi-at-amazon-scale">Key Learnings from Operating Hudi at Amazon Scale<a href="https://hudi.apache.org/blog/2025/03/31/amazon-hudi#key-learnings-from-operating-hudi-at-amazon-scale" class="hash-link" aria-label="Direct link to Key Learnings from Operating Hudi at Amazon Scale" title="Direct link to Key Learnings from Operating Hudi at Amazon Scale">​</a></h2>
<p>Operating Apache Hudi at the scale and velocity required by Amazon’s Profit Intelligence workloads surfaced a set of hard-earned lessons, especially around concurrency, metadata handling, and cost optimization. These learnings reflect both architectural refinements and operational trade-offs that others adopting Hudi at large scale may find useful.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-concurrency-control">1. Concurrency Control<a href="https://hudi.apache.org/blog/2025/03/31/amazon-hudi#1-concurrency-control" class="hash-link" aria-label="Direct link to 1. Concurrency Control" title="Direct link to 1. Concurrency Control">​</a></h3>
<p>At Amazon’s ingestion scale - hundreds of billions of rows per day and thousands of concurrent table updates, multi-writer concurrency is a reality, not an edge case.</p>
<p>The team initially used Optimistic Concurrency Control (OCC), which works well in environments with low write conflicts. OCC assumes that concurrent writers rarely overlap, and when they do, the job retries after detecting a conflict. However, in high-contention scenarios, like multiple jobs writing to the same partition within a short time window, this led to frequent retries and job failures.</p>
<p>To resolve this, the team pivoted to a new table structure designed to minimize concurrent insertions. This change helped reduce contention by lowering the likelihood of multiple writers operating on overlapping partitions simultaneously. The updated design enabled using OCC while avoiding the excessive retries and failures we had initially encountered.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-metadata-table-management-async-vs-sync-trade-offs">2. Metadata Table Management: Async vs Sync Trade-Offs<a href="https://hudi.apache.org/blog/2025/03/31/amazon-hudi#2-metadata-table-management-async-vs-sync-trade-offs" class="hash-link" aria-label="Direct link to 2. Metadata Table Management: Async vs Sync Trade-Offs" title="Direct link to 2. Metadata Table Management: Async vs Sync Trade-Offs">​</a></h3>
<p>Apache Hudi’s metadata table dramatically improves performance by avoiding expensive file listings on cloud object stores like S3. It maintains a persistent <em>index</em> <em>of files</em>, enabling faster operations such as file pruning, and data skipping.</p>
<p>The team enabled Hudi’s metadata table early (<code>hoodie.metadata.enable=true</code>) and started off with synchronous cleaning but switched to asynchronous cleaning to reduce job runtime. However, we ran into an issue when experimenting with asynchronous cleaning. Due to a <a href="https://github.com/apache/hudi/issues/11535" target="_blank" rel="noopener noreferrer">known issue (#11535)</a>, async cleaning wasn’t properly cleaning up metadata entries.</p>
<p>To ensure the metadata tables were properly cleaned, we switched all of  our Hudi workloads back to synchronous cleaning.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-cost-management">3. Cost Management<a href="https://hudi.apache.org/blog/2025/03/31/amazon-hudi#3-cost-management" class="hash-link" aria-label="Direct link to 3. Cost Management" title="Direct link to 3. Cost Management">​</a></h3>
<p>While Apache Hudi helped Amazon reduce data duplication and improve ingestion efficiency, we quickly realized that operational costs were not driven by storage - but by the API interaction patterns with S3.</p>
<p>Breakdown of the cost profile:</p>
<ul>
<li><strong>70% of total cost</strong> came from <code>PUT</code> requests (writes)</li>
<li>Combined <code>PUT + GET</code> operations accounted for <strong>80%</strong> of the bill</li>
<li>Storage cost remained a small fraction, even with 3+ PB of total data under management</li>
</ul>
<p>Their data ingestion patterns contributed to this:</p>
<ul>
<li>Daily workloads: Heavy concentration (99%) of updates into a single partition</li>
<li>Backfill workloads: Spread evenly across 30–90 partitions</li>
</ul>
<p>To manage this:</p>
<ul>
<li>We moved to <strong>S3 Intelligent-Tiering</strong> to reduce unused data storage costs</li>
<li>Enabled <strong>EMR cluster auto-scaling</strong> to dynamically adjust compute resources</li>
<li>Batched writes and carefully tuned Hudi configurations (e.g., <code>write.batch.size</code>, <code>compaction.small.file.limit</code>) to reduce unnecessary file churn</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="operational-scale-nexus-by-the-numbers">Operational Scale: Nexus by the Numbers<a href="https://hudi.apache.org/blog/2025/03/31/amazon-hudi#operational-scale-nexus-by-the-numbers" class="hash-link" aria-label="Direct link to Operational Scale: Nexus by the Numbers" title="Direct link to Operational Scale: Nexus by the Numbers">​</a></h2>
<table><thead><tr><th style="text-align:left">Metric</th><th style="text-align:left">Value</th></tr></thead><tbody><tr><td style="text-align:left">Tables Managed</td><td style="text-align:left">1200+ (5–15 updates/day per table)</td></tr><tr><td style="text-align:left">Legacy SQL Deprecated</td><td style="text-align:left">300,000+ lines</td></tr><tr><td style="text-align:left">Total Data Managed</td><td style="text-align:left">~3 Petabytes</td></tr><tr><td style="text-align:left">Monthly Data Changes</td><td style="text-align:left">~1 Petabyte added/deleted</td></tr><tr><td style="text-align:left">Daily Record Updates</td><td style="text-align:left">Hundreds of billions</td></tr><tr><td style="text-align:left">Developer Time Saved</td><td style="text-align:left">300+ days</td></tr></tbody></table>
<p>Nexus with Apache Hudi as the foundation has significantly improved the scale, modularity, and maintainability of the data lake operations at Amazon. As the business use cases scale, the team is also focused on managing the increasing complexity of the data lake, while ensuring that both technical and non-technical stakeholders can interact with Nexus effectively.</p>
<p>This blog is based on Amazon’s presentation at the Hudi Community Sync. If you are interested in watching the recorded version of the video, you can find it <a href="https://www.youtube.com/watch?v=rMXhlb7Uci8" target="_blank" rel="noopener noreferrer">here</a>.</p>
<hr>]]></content:encoded>
            <category>Apache Hudi</category>
            <category>Amazon</category>
            <category>Community</category>
        </item>
        <item>
            <title><![CDATA[ACID Transactions in an Open Data Lakehouse]]></title>
            <link>https://hudi.apache.org/blog/2025/03/26/acid-transactions</link>
            <guid>https://hudi.apache.org/blog/2025/03/26/acid-transactions</guid>
            <pubDate>Wed, 26 Mar 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://www.onehouse.ai/blog/acid-transactions-in-an-open-data-lakehouse">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>Apache Hudi</category>
            <category>Apache Iceberg</category>
            <category>Delta Lake</category>
            <category>ACID</category>
        </item>
    </channel>
</rss>