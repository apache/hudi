<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-0.14.1 docs-doc-page docs-doc-id-hoodie_streaming_ingestion" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Streaming Ingestion | Apache Hudi</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://hudi.apache.org/docs/0.14.1/hoodie_streaming_ingestion"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="0.14.1"><meta data-rh="true" name="docusaurus_tag" content="docs-default-0.14.1"><meta data-rh="true" name="docsearch:version" content="0.14.1"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-0.14.1"><meta data-rh="true" property="og:title" content="Streaming Ingestion | Apache Hudi"><meta data-rh="true" name="description" content="Hudi Streamer"><meta data-rh="true" property="og:description" content="Hudi Streamer"><meta data-rh="true" name="keywords" content="hudi,streamer,hoodiestreamer,spark_streaming"><link data-rh="true" rel="icon" href="/assets/images/favicon.ico"><link data-rh="true" rel="canonical" href="https://hudi.apache.org/docs/0.14.1/hoodie_streaming_ingestion"><link data-rh="true" rel="alternate" href="https://hudi.apache.org/docs/0.14.1/hoodie_streaming_ingestion" hreflang="en"><link data-rh="true" rel="alternate" href="https://hudi.apache.org/docs/0.14.1/hoodie_streaming_ingestion" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://BH4D9OD16A-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Streaming Ingestion","item":"https://hudi.apache.org/docs/0.14.1/hoodie_streaming_ingestion"}]}</script><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Apache Hudi: User-Facing Analytics RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Apache Hudi: User-Facing Analytics Atom Feed">
<link rel="alternate" type="application/json" href="/blog/feed.json" title="Apache Hudi: User-Facing Analytics JSON Feed">




<link rel="search" type="application/opensearchdescription+xml" title="Apache Hudi" href="/opensearch.xml">
<link rel="alternate" type="application/rss+xml" href="/videos/rss.xml" title="Apache Hudi RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/videos/atom.xml" title="Apache Hudi Atom Feed">





<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Comfortaa|Ubuntu|Roboto|Source+Code+Pro">
<link rel="stylesheet" href="https://at-ui.github.io/feather-font/css/iconfont.css">
<script src="https://widget.kapa.ai/kapa-widget.bundle.js" data-website-id="9e4444ba-93cc-45ea-b143-783ae0fbeb6f" data-project-name="Apache Hudi" data-project-color="#FFFFFF" data-project-logo="https://hudi.apache.org/assets/images/logo-big.png" data-modal-disclaimer="This AI assistant answers Apache Hudi questions using your [documentation](https://hudi.apache.org/docs/quick-start-guide/), [dev setup](https://hudi.apache.org/contribute/developer-setup/), the [tech specs](https://hudi.apache.org/tech-specs-1point0/) and open GitHub Issues from the last year." data-modal-title="Apache Hudi AI Assistant" data-modal-example-questions-title="Try asking me..." data-modal-example-questions="How can I convert an existing COW table to MOR?,How do I set up incremental queries with Hudi tables?" data-modal-image="https://hudi.apache.org/assets/images/logo-big-2.png" data-modal-image-ask-ai="https://hudi.apache.org/assets/images/logo-big-2.png" data-modal-header-min-height="64px" data-modal-image-height="40" data-modal-image-width="40" data-modal-header-bg-color="#f8f9fa" data-modal-title-color="#0db1f9" data-button-text-color="#29557a" data-button-text="Ask AI" data-consent-required="true" data-consent-screen-title="Help us improve our AI assistant" data-consent-screen-disclaimer="By clicking &amp;quot;Allow tracking&amp;quot;, you consent to the use of the AI assistant in accordance with kapa.ai&#39;s [Privacy Policy](https://www.kapa.ai/content/privacy-policy). This service uses reCAPTCHA, which requires your consent to Google&#39;s [Privacy Policy](https://policies.google.com/privacy) and [Terms of Service](https://policies.google.com/terms). By proceeding, you explicitly agree to both kapa.ai&#39;s and Google&#39;s privacy policies." data-consent-screen-accept-button-text="Allow tracking" data-modal-disclaimer-font-size="0.80rem" data-query-input-placeholder-text-color="#29557a" data-submit-query-button-bg-color="#0db1f9" data-query-input-text-color="#29557a" data-user-analytics-cookie-enabled="false" data-query-input-border-color="#211b0e" data-question-text-color="#0db1f9" data-answer-text-color="#000" data-thread-clear-button-bg-color="#000000" data-thread-clear-button-text-color="#FFFFFF" data-answer-feedback-button-bg-color="#000000" data-answer-feedback-button-text-color="#FFFFFF" data-answer-feedback-button-active-bg-color="#000000" data-answer-feedback-button-active-text-color="#FFFFFF" data-answer-copy-button-bg-color="#000000" data-answer-copy-button-text-color="#FFFFFF" data-example-question-button-text-color="#29557a" data-modal-disclaimer-bg-color="#f8f9fa" data-modal-disclaimer-text-color="#0db1f9" data-deep-thinking-button-bg-color="#0db1f9" data-deep-thinking-button-text-color="#FFFFFF" data-deep-thinking-button-active-bg-color="#0db1f9" data-deep-thinking-button-hover-bg-color="#FFFFFF" data-deep-thinking-button-active-hover-bg-color="#29557a" data-deep-thinking-button-active-text-color="#FFFFFF" async></script><link rel="stylesheet" href="/assets/css/styles.daf03245.css">
<script src="/assets/js/runtime~main.0d661000.js" defer="defer"></script>
<script src="/assets/js/main.0a7b0457.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme","light"),document.documentElement.setAttribute("data-theme-choice","light"),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="theme-announcement-bar announcementBar_mb4j" role="banner"><div class="content_knG7 announcementBarContent_xLdY">⭐️ If you like <b>Apache Hudi</b>, give it a star on <a target="_blank" rel="noopener noreferrer" href="https://github.com/apache/hudi"><b>GitHub!<svg xmlns="http://www.w3.org/2000/svg\" width="16" height="16" fill="currentColor" class="bi bi-github" viewBox="0 -2 16 16"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"/></svg></b></a> ⭐</div></div><nav aria-label="Main" class="navbar navbar--fixed-top navbarWrapper_j_uY"><div class="navbar__inner navbarInnerStyle_KoMw"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo navbarLogo_aghy"><img src="/assets/images/hudi.png" alt="Apache Hudi" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/assets/images/hudi.png" alt="Apache Hudi" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a class="navbar__item navbar__link" href="/docs/overview">Docs</a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Learn</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/learn/tutorial-series">Tutorial Series</a></li><li><a class="dropdown__link" href="/talks">Talks</a></li><li><a class="dropdown__link" href="/blog">Blog</a></li><li><a class="dropdown__link" href="/videos">Video Guides</a></li><li><a class="dropdown__link" href="/faq">FAQ</a></li><li><a class="dropdown__link" href="/tech-specs">Tech Specs</a></li><li><a class="dropdown__link" href="/tech-specs-1point0">Tech Specs 1.0</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Contribute</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/contribute/developer-sync-call">Developer Sync Call</a></li><li><a class="dropdown__link" href="/contribute/how-to-contribute">How to Contribute</a></li><li><a class="dropdown__link" href="/contribute/developer-setup">Developer Setup</a></li><li><a class="dropdown__link" href="/contribute/rfc-process">RFC Process</a></li><li><a class="dropdown__link" href="/contribute/report-security-issues">Report Issues</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Community</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/community/get-involved">Get Involved</a></li><li><a class="dropdown__link" href="/community/syncs">Community Syncs</a></li><li><a class="dropdown__link" href="/community/office_hours">Office Hours</a></li><li><a class="dropdown__link" href="/community/team">Team</a></li><li><a href="https://join.slack.com/t/apache-hudi/shared_invite/zt-33fabmxb7-Q7QSUtNOHYCwUdYM8LbauA" target="_blank" rel="noopener noreferrer" class="dropdown__link">Join Our Slack Space<svg width="12" height="12" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><a class="navbar__item navbar__link" href="/ecosystem">Ecosystem</a><a class="navbar__item navbar__link" href="/powered-by">Who&#x27;s Using</a><a class="navbar__item navbar__link" href="/roadmap">Roadmap</a><a class="navbar__item navbar__link" href="/releases/download">Download</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a class="navbar__link" aria-haspopup="true" aria-expanded="false" role="button" href="/docs/0.14.1/hoodie_streaming_ingestion">0.14.1</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/next/hoodie_streaming_ingestion">Current</a></li><li><a class="dropdown__link" href="/docs/hoodie_streaming_ingestion">1.0.2</a></li><li><a class="dropdown__link" href="/docs/1.0.1/hoodie_streaming_ingestion">1.0.1</a></li><li><a class="dropdown__link" href="/docs/1.0.0/hoodie_streaming_ingestion">1.0.0</a></li><li><a class="dropdown__link" href="/docs/0.15.0/hoodie_streaming_ingestion">0.15.0</a></li><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/docs/0.14.1/hoodie_streaming_ingestion">0.14.1</a></li><li><a class="dropdown__link" href="/docs/0.14.0/hoodie_streaming_ingestion">0.14.0</a></li><li><a class="dropdown__link" href="/docs/0.13.1/overview">0.13.1</a></li><li><a class="dropdown__link" href="/docs/0.13.0/overview">0.13.0</a></li><li><a class="dropdown__link" href="/docs/0.12.3/overview">0.12.3</a></li><li><a class="dropdown__link" href="/docs/0.12.2/overview">0.12.2</a></li><li><a class="dropdown__link" href="/docs/0.12.1/overview">0.12.1</a></li><li><a class="dropdown__link" href="/docs/0.12.0/overview">0.12.0</a></li><li><a class="dropdown__link" href="/docs/0.11.1/overview">0.11.1</a></li><li><a class="dropdown__link" href="/docs/0.11.0/overview">0.11.0</a></li><li><a class="dropdown__link" href="/docs/0.10.1/overview">0.10.1</a></li><li><a class="dropdown__link" href="/docs/0.10.0/overview">0.10.0</a></li><li><a class="dropdown__link" href="/docs/0.9.0/overview">0.9.0</a></li><li><a class="dropdown__link" href="/docs/0.8.0/overview">0.8.0</a></li><li><a class="dropdown__link" href="/docs/0.7.0/overview">0.7.0</a></li><li><a class="dropdown__link" href="/docs/0.6.0/quick-start-guide">0.6.0</a></li><li><a class="dropdown__link" href="/docs/0.5.3/quick-start-guide">0.5.3</a></li><li><a class="dropdown__link" href="/docs/0.5.2/quick-start-guide">0.5.2</a></li><li><a class="dropdown__link" href="/docs/0.5.1/quick-start-guide">0.5.1</a></li><li><a class="dropdown__link" href="/docs/0.5.0/quick-start-guide">0.5.0</a></li></ul></div><a href="https://github.com/apache/hudi" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><a href="https://x.com/ApacheHudi" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-twitter-link" aria-label="Hudi Twitter Handle"></a><a href="https://join.slack.com/t/apache-hudi/shared_invite/zt-33fabmxb7-Q7QSUtNOHYCwUdYM8LbauA" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-slack-link" aria-label="Hudi Slack Channel"></a><a href="https://www.youtube.com/channel/UCs7AhE0BWaEPZSChrBR-Muw" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-youtube-link" aria-label="Hudi YouTube Channel"></a><a href="https://www.linkedin.com/company/apache-hudi/?viewAsMember=true" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-linkedin-link" aria-label="Hudi Linkedin Page"></a><div class="navbarSearchContainer_Bca1"><div><div role="button" class="searchButton_o6KI" aria-label="Search"><span class="searchText_sHOJ">Search</span><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" fill="none" viewBox="0 0 14 14"><circle cx="6.864" cy="6.864" r="5.243" stroke="#1C1E21" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5"></circle><path stroke="#1C1E21" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m10.51 10.783 2.056 2.05"></path></svg></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar navbarSideMenu_TODO"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/"><div class="navbar__logo navbarLogo_aghy"><img src="/assets/images/hudi.png" alt="Apache Hudi" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/assets/images/hudi.png" alt="Apache Hudi" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><button type="button" aria-label="Close navigation bar" class="clean-btn navbar-sidebar__close"><svg viewBox="0 0 15 15" width="21" height="21"><g stroke="var(--ifm-color-emphasis-600)" stroke-width="1.2"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><div class="navbar-sidebar__items"><div class="navbar-sidebar__item menu"><ul class="menu__list"><li class="menu__list-item"><a class="menu__link navbarFontSize_pR5Q" href="/docs/overview">Docs</a></li><li class="menu__list-item menu__list-item--collapsed"><a role="button" class="dropdownNavbarItemMobile_JUhd menu__link menu__link--sublist menu__link--sublist-caret navbarFontSize_pR5Q">Learn</a></li><li class="menu__list-item menu__list-item--collapsed"><a role="button" class="dropdownNavbarItemMobile_JUhd menu__link menu__link--sublist menu__link--sublist-caret navbarFontSize_pR5Q">Contribute</a></li><li class="menu__list-item menu__list-item--collapsed"><a role="button" class="dropdownNavbarItemMobile_JUhd menu__link menu__link--sublist menu__link--sublist-caret navbarFontSize_pR5Q">Community</a></li><li class="menu__list-item"><a class="menu__link navbarFontSize_pR5Q" href="/ecosystem">Ecosystem</a></li><li class="menu__list-item"><a class="menu__link navbarFontSize_pR5Q" href="/powered-by">Who&#x27;s Using</a></li><li class="menu__list-item"><a class="menu__link navbarFontSize_pR5Q" href="/roadmap">Roadmap</a></li><li class="menu__list-item"><a class="menu__link navbarFontSize_pR5Q" href="/releases/download">Download</a></li><li class="menu__list-item"><a role="button" class="dropdownNavbarItemMobile_JUhd menu__link menu__link--sublist menu__link--sublist-caret navbarFontSize_pR5Q">Versions</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/docs/next/hoodie_streaming_ingestion">Current</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/hoodie_streaming_ingestion">1.0.2</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/1.0.1/hoodie_streaming_ingestion">1.0.1</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/1.0.0/hoodie_streaming_ingestion">1.0.0</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/0.15.0/hoodie_streaming_ingestion">0.15.0</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active" href="/docs/0.14.1/hoodie_streaming_ingestion">0.14.1</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/0.14.0/hoodie_streaming_ingestion">0.14.0</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/0.13.1/overview">0.13.1</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/0.13.0/overview">0.13.0</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/0.12.3/overview">0.12.3</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/0.12.2/overview">0.12.2</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/0.12.1/overview">0.12.1</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/0.12.0/overview">0.12.0</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/0.11.1/overview">0.11.1</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/0.11.0/overview">0.11.0</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/0.10.1/overview">0.10.1</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/0.10.0/overview">0.10.0</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/0.9.0/overview">0.9.0</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/0.8.0/overview">0.8.0</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/0.7.0/overview">0.7.0</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/0.6.0/quick-start-guide">0.6.0</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/0.5.3/quick-start-guide">0.5.3</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/0.5.2/quick-start-guide">0.5.2</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/0.5.1/quick-start-guide">0.5.1</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/0.5.0/quick-start-guide">0.5.0</a></li></ul></li><li class="menu__list-item"><a href="https://github.com/apache/hudi" target="_blank" rel="noopener noreferrer" class="menu__link header-github-link" aria-label="GitHub repository"></a></li><li class="menu__list-item"><a href="https://x.com/ApacheHudi" target="_blank" rel="noopener noreferrer" class="menu__link header-twitter-link" aria-label="Hudi Twitter Handle"></a></li><li class="menu__list-item"><a href="https://join.slack.com/t/apache-hudi/shared_invite/zt-33fabmxb7-Q7QSUtNOHYCwUdYM8LbauA" target="_blank" rel="noopener noreferrer" class="menu__link header-slack-link" aria-label="Hudi Slack Channel"></a></li><li class="menu__list-item"><a href="https://www.youtube.com/channel/UCs7AhE0BWaEPZSChrBR-Muw" target="_blank" rel="noopener noreferrer" class="menu__link header-youtube-link" aria-label="Hudi YouTube Channel"></a></li><li class="menu__list-item"><a href="https://www.linkedin.com/company/apache-hudi/?viewAsMember=true" target="_blank" rel="noopener noreferrer" class="menu__link header-linkedin-link" aria-label="Hudi Linkedin Page"></a></li></ul></div><div class="navbar-sidebar__item menu"><button type="button" class="clean-btn navbar-sidebar__back">← Back to main menu</button></div></div></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_bSxm"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_cWv0"><aside class="theme-doc-sidebar-container docSidebarContainer_RSuS"><div class="sidebarViewport_pYEE"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/0.14.1/overview"><span title="Overview" class="linkLabel_WmDU">Overview</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" href="/docs/0.14.1/quick-start-guide"><span title="Quick Start" class="categoryLinkLabel_W154">Quick Start</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/0.14.1/quick-start-guide"><span title="Spark Guide" class="linkLabel_WmDU">Spark Guide</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/0.14.1/flink-quick-start-guide"><span title="Flink Guide" class="linkLabel_WmDU">Flink Guide</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/0.14.1/docker_demo"><span title="Docker Demo" class="linkLabel_WmDU">Docker Demo</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/0.14.1/timeline"><span title="Concepts" class="categoryLinkLabel_W154">Concepts</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/docs/0.14.1/sql_ddl"><span title="How To" class="categoryLinkLabel_W154">How To</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/docs/0.14.1/sql_ddl"><span title="SQL" class="categoryLinkLabel_W154">SQL</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/0.14.1/writing_data"><span title="Writing Data" class="linkLabel_WmDU">Writing Data</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/0.14.1/hoodie_streaming_ingestion"><span title="Streaming Ingestion" class="linkLabel_WmDU">Streaming Ingestion</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/docs/0.14.1/syncing_aws_glue_data_catalog"><span title="Syncing to Catalogs" class="categoryLinkLabel_W154">Syncing to Catalogs</span></a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/0.14.1/migration_guide"><span title="Services" class="categoryLinkLabel_W154">Services</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/0.14.1/basic_configurations"><span title="Configurations" class="categoryLinkLabel_W154">Configurations</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/0.14.1/performance"><span title="Guides" class="categoryLinkLabel_W154">Guides</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/0.14.1/faq"><span title="Frequently Asked Questions(FAQs)" class="categoryLinkLabel_W154">Frequently Asked Questions(FAQs)</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/0.14.1/use_cases"><span title="Use Cases" class="linkLabel_WmDU">Use Cases</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/0.14.1/faq"><span title="Overview" class="linkLabel_WmDU">Overview</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/0.14.1/privacy"><span title="Privacy Policy" class="linkLabel_WmDU">Privacy Policy</span></a></li></ul></nav></div></div></aside><main class="docMainContainer_hjYf"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="theme-doc-version-banner alert alert--warning margin-bottom--md" role="alert"><div>This is documentation for <!-- -->Apache Hudi<!-- --> <b>0.14.1</b>, which is no longer actively maintained.</div><div class="margin-top--md">For up-to-date documentation, see the <b><a href="/docs/hoodie_streaming_ingestion">latest version</a></b> (<!-- -->1.0.2<!-- -->).</div></div><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">How To</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Streaming Ingestion</span></li></ul></nav><span class="theme-doc-version-badge badge badge--secondary">Version: 0.14.1</span><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Streaming Ingestion</h1></header><h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="hudi-streamer">Hudi Streamer<a href="#hudi-streamer" class="hash-link" aria-label="Direct link to Hudi Streamer" title="Direct link to Hudi Streamer" translate="no">​</a></h2>
<div class="theme-admonition theme-admonition-danger admonition_xJq3 alert alert--danger"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"></path></svg></span>Important</div><div class="admonitionContent_BuS1"><p>The following classes were renamed and relocated to <code>org.apache.hudi.utilities.streamer</code> package.</p><ul>
<li class=""><code>DeltastreamerMultiWriterCkptUpdateFunc</code> is renamed to <code>StreamerMultiWriterCkptUpdateFunc</code></li>
<li class=""><code>DeltaSync</code> is renamed to <code>StreamSync</code></li>
<li class=""><code>HoodieDeltaStreamer</code> is renamed to <code>HoodieStreamer</code></li>
<li class=""><code>HoodieDeltaStreamerMetrics</code> is renamed to <code>HoodieStreamerMetrics</code></li>
<li class=""><code>HoodieMultiTableDeltaStreamer</code> is renamed to <code>HoodieMultiTableStreamer</code></li>
</ul><p>To maintain backward compatiblity, the original classes are still present in the org.apache.hudi.utilities.deltastreamer
package, but have been deprecated.</p></div></div>
<p>The <code>HoodieStreamer</code> utility (part of <code>hudi-utilities-bundle</code>) provides the way to ingest from different sources such as DFS or Kafka, with the following capabilities.</p>
<ul>
<li class="">Exactly once ingestion of new events from Kafka, <a href="https://sqoop.apache.org/docs/1.4.2/SqoopUserGuide#_incremental_imports" target="_blank" rel="noopener noreferrer" class="">incremental imports</a> from Sqoop or output of <code>HiveIncrementalPuller</code> or files under a DFS folder</li>
<li class="">Support json, avro or a custom record types for the incoming data</li>
<li class="">Manage checkpoints, rollback &amp; recovery</li>
<li class="">Leverage Avro schemas from DFS or Confluent <a href="https://github.com/confluentinc/schema-registry" target="_blank" rel="noopener noreferrer" class="">schema registry</a>.</li>
<li class="">Support for plugging in transformations</li>
</ul>
<p>Command line options describe capabilities in more detail</p>
<div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">[hoodie]$ spark-submit --class org.apache.hudi.utilities.streamer.HoodieStreamer `ls packaging/hudi-utilities-bundle/target/hudi-utilities-bundle-*.jar` --help</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Usage: &lt;main class&gt; [options]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Options:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --checkpoint</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Resume Hudi Streamer from this checkpoint.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --commit-on-errors</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Commit even when some records failed to be written</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Default: false</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --compact-scheduling-minshare</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Minshare for compaction as defined in</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      https://spark.apache.org/docs/latest/job-scheduling</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Default: 0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --compact-scheduling-weight</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Scheduling weight for compaction as defined in</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      https://spark.apache.org/docs/latest/job-scheduling</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Default: 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --continuous</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Hudi Streamer runs in continuous mode running source-fetch -&gt; Transform</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      -&gt; Hudi Write in loop</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Default: false</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --delta-sync-scheduling-minshare</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Minshare for delta sync as defined in</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      https://spark.apache.org/docs/latest/job-scheduling</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Default: 0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --delta-sync-scheduling-weight</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Scheduling weight for delta sync as defined in</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      https://spark.apache.org/docs/latest/job-scheduling</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Default: 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --disable-compaction</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Compaction is enabled for MoR table by default. This flag disables it</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Default: false</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --enable-hive-sync</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Enable syncing to hive (Deprecated in favor of --enable-sync and --sync-tool-classes)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Default: false</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --enable-sync</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Enable syncing meta</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Default: false</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --sync-tool-classes</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Classes (comma-separated) to be used for syncing meta. Shall be used only when --enable-sync or --enable-hive-sync is set to true</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Note: When used with deprecated --enable-hive-sync flag, HiveSyncTool will always be run along with any other classes mentioned in here.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Default: org.apache.hudi.hive.HiveSyncTool</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --filter-dupes</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Should duplicate records from source be dropped/filtered out before</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      insert/bulk-insert</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Default: false</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --help, -h</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --hoodie-conf</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Any configuration that can be set in the properties file (using the CLI</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      parameter &quot;--propsFilePath&quot;) can also be passed command line using this</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      parameter</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Default: []</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --max-pending-compactions</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Maximum number of outstanding inflight/requested compactions. Delta Sync</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      will not happen unlessoutstanding compactions is less than this number</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Default: 5</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --min-sync-interval-seconds</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      the min sync interval of each sync in continuous mode</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Default: 0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --op</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Takes one of these values : UPSERT (default), INSERT (use when input is</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      purely new data/inserts to gain speed)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Default: UPSERT</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Possible Values: [UPSERT, INSERT, BULK_INSERT]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --payload-class</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      subclass of HoodieRecordPayload, that works off a GenericRecord.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Implement your own, if you want to do something other than overwriting</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      existing value</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Default: org.apache.hudi.common.model.OverwriteWithLatestAvroPayload</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --props</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      path to properties file on localfs or dfs, with configurations for</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      hoodie client, schema provider, key generator and data source. For</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      hoodie client props, sane defaults are used, but recommend use to</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      provide basic things like metrics endpoints, hive configs etc. For</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      sources, referto individual classes, for supported properties.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Default: file:///Users/vinoth/bin/hoodie/src/test/resources/streamer-config/dfs-source.properties</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --schemaprovider-class</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      subclass of org.apache.hudi.utilities.schema.SchemaProvider to attach</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      schemas to input &amp; target table data, built in options:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      org.apache.hudi.utilities.schema.FilebasedSchemaProvider.Source (See</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      org.apache.hudi.utilities.sources.Source) implementation can implement</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      their own SchemaProvider. For Sources that return Dataset&lt;Row&gt;, the</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      schema is obtained implicitly. However, this CLI option allows</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      overriding the schemaprovider returned by Source.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --source-class</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Subclass of org.apache.hudi.utilities.sources to read data. Built-in</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      options: org.apache.hudi.utilities.sources.{JsonDFSSource (default), </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      AvroDFSSource, AvroKafkaSource, CsvDFSSource, HiveIncrPullSource, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      JdbcSource, JsonKafkaSource, ORCDFSSource, ParquetDFSSource, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      S3EventsHoodieIncrSource, S3EventsSource, SqlSource}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Default: org.apache.hudi.utilities.sources.JsonDFSSource</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --source-limit</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Maximum amount of data to read from source. Default: No limit For e.g:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      DFS-Source =&gt; max bytes to read, Kafka-Source =&gt; max events to read</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Default: 9223372036854775807</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --source-ordering-field</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Field within source record to decide how to break ties between records</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      with same key in input data. Default: &#x27;ts&#x27; holding unix timestamp of</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      record</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Default: ts</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --spark-master</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      spark master to use.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Default: local[2]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  * --table-type</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      Type of table. COPY_ON_WRITE (or) MERGE_ON_READ</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  * --target-base-path</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      base path for the target hoodie table. (Will be created if did not exist</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      first time around. If exists, expected to be a hoodie table)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  * --target-table</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      name of the target table in Hive</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --transformer-class</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      subclass of org.apache.hudi.utilities.transform.Transformer. Allows</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      transforming raw source Dataset to a target Dataset (conforming to</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      target schema) before writing. Default : Not set. E:g -</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      org.apache.hudi.utilities.transform.SqlQueryBasedTransformer (which</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      allows a SQL query templated to be passed as a transformation function)</span><br></span></code></pre></div></div>
<p>The tool takes a hierarchically composed property file and has pluggable interfaces for extracting data, key generation and providing schema. Sample configs for ingesting from kafka and dfs are
provided under <code>hudi-utilities/src/test/resources/streamer-config</code>.</p>
<p>For e.g: once you have Confluent Kafka, Schema registry up &amp; running, produce some test data using (<a href="https://docs.confluent.io/current/ksql/docs/tutorials/generate-custom-test-data" target="_blank" rel="noopener noreferrer" class="">impressions.avro</a> provided by schema-registry repo)</p>
<div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">[confluent-5.0.0]$ bin/ksql-datagen schema=../impressions.avro format=avro topic=impressions key=impressionid</span><br></span></code></pre></div></div>
<p>and then ingest it as follows.</p>
<div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">[hoodie]$ spark-submit --class org.apache.hudi.utilities.streamer.HoodieStreamer `ls packaging/hudi-utilities-bundle/target/hudi-utilities-bundle-*.jar` \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --props file://${PWD}/hudi-utilities/src/test/resources/streamer-config/kafka-source.properties \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --source-class org.apache.hudi.utilities.sources.AvroKafkaSource \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --source-ordering-field impresssiontime \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --target-base-path file:\/\/\/tmp/hudi-streamer-op \ </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --target-table uber.impressions \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --op BULK_INSERT</span><br></span></code></pre></div></div>
<p>In some cases, you may want to migrate your existing table into Hudi beforehand. Please refer to <a class="" href="/docs/migration_guide">migration guide</a>.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="note-on-hudi-utilities-bundle-usage-for-different-spark-versions">Note on hudi utilities bundle usage for different spark versions<a href="#note-on-hudi-utilities-bundle-usage-for-different-spark-versions" class="hash-link" aria-label="Direct link to Note on hudi utilities bundle usage for different spark versions" title="Direct link to Note on hudi utilities bundle usage for different spark versions" translate="no">​</a></h3>
<p>From 0.11.0 release, we start to provide a new <code>hudi-utilities-slim-bundle</code> which aims to exclude dependencies that can
cause conflicts and compatibility issues with different versions of Spark.  The <code>hudi-utilities-slim-bundle</code> should be
used along with a Hudi Spark bundle corresponding the Spark version used to make utilities work with Spark, e.g.,
<code>--packages org.apache.hudi:hudi-utilities-slim-bundle_2.12:0.13.0,org.apache.hudi:hudi-spark3.1-bundle_2.12:0.13.0</code>,
if using <code>hudi-utilities-bundle</code> solely to run <code>HoodieStreamer</code> in Spark encounters compatibility issues.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="multitablestreamer">MultiTableStreamer<a href="#multitablestreamer" class="hash-link" aria-label="Direct link to MultiTableStreamer" title="Direct link to MultiTableStreamer" translate="no">​</a></h4>
<p><code>HoodieMultiTableStreamer</code>, a wrapper on top of <code>HoodieStreamer</code>, enables one to ingest multiple tables at a single go into hudi datasets. Currently it only supports sequential processing of tables to be ingested and COPY_ON_WRITE storage type. The command line options for <code>HoodieMultiTableStreamer</code> are pretty much similar to <code>HoodieStreamer</code> with the only exception that you are required to provide table wise configs in separate files in a dedicated config folder. The following command line options are introduced</p>
<div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">  * --config-folder</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    the path to the folder which contains all the table wise config files</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --base-path-prefix</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    this is added to enable users to create all the hudi datasets for related tables under one path in FS. The datasets are then created under the path - &lt;base_path_prefix&gt;/&lt;database&gt;/&lt;table_to_be_ingested&gt;. However you can override the paths for every table by setting the property hoodie.streamer.ingestion.targetBasePath</span><br></span></code></pre></div></div>
<p>The following properties are needed to be set properly to ingest data using <code>HoodieMultiTableStreamer</code>.</p>
<div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">hoodie.streamer.ingestion.tablesToBeIngested</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  comma separated names of tables to be ingested in the format &lt;database&gt;.&lt;table&gt;, for example db1.table1,db1.table2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">hoodie.streamer.ingestion.targetBasePath</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  if you wish to ingest a particular table in a separate path, you can mention that path here</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">hoodie.streamer.ingestion.&lt;database&gt;.&lt;table&gt;.configFile</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  path to the config file in dedicated config folder which contains table overridden properties for the particular table to be ingested.</span><br></span></code></pre></div></div>
<p>Sample config files for table wise overridden properties can be found under <code>hudi-utilities/src/test/resources/streamer-config</code>. The command to run <code>HoodieMultiTableStreamer</code> is also similar to how you run <code>HoodieStreamer</code>.</p>
<div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">[hoodie]$ spark-submit --class org.apache.hudi.utilities.streamer.HoodieMultiTableStreamer `ls packaging/hudi-utilities-bundle/target/hudi-utilities-bundle-*.jar` \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --props file://${PWD}/hudi-utilities/src/test/resources/streamer-config/kafka-source.properties \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --config-folder file://tmp/hudi-ingestion-config \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --source-class org.apache.hudi.utilities.sources.AvroKafkaSource \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --source-ordering-field impresssiontime \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --base-path-prefix file:\/\/\/tmp/hudi-streamer-op \ </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --target-table uber.impressions \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --op BULK_INSERT</span><br></span></code></pre></div></div>
<p>For detailed information on how to configure and use <code>HoodieMultiTableStreamer</code>, please refer <a class="" href="/blog/2020/08/22/ingest-multiple-tables-using-hudi">blog section</a>.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="concurrency-control">Concurrency Control<a href="#concurrency-control" class="hash-link" aria-label="Direct link to Concurrency Control" title="Direct link to Concurrency Control" translate="no">​</a></h3>
<p>The <code>HoodieStreamer</code> utility (part of hudi-utilities-bundle) provides ways to ingest from different sources such as DFS or Kafka, with the following capabilities.</p>
<p>Using optimistic_concurrency_control via Hudi Streamer requires adding the above configs to the properties file that can be passed to the
job. For example below, adding the configs to kafka-source.properties file and passing them to Hudi Streamer will enable optimistic concurrency.
A Hudi Streamer job can then be triggered as follows:</p>
<div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">[hoodie]$ spark-submit --class org.apache.hudi.utilities.streamer.HoodieStreamer `ls packaging/hudi-utilities-bundle/target/hudi-utilities-bundle-*.jar` \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --props file://${PWD}/hudi-utilities/src/test/resources/streamer-config/kafka-source.properties \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --source-class org.apache.hudi.utilities.sources.AvroKafkaSource \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --source-ordering-field impresssiontime \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --target-base-path file:\/\/\/tmp/hudi-streamer-op \ </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --target-table uber.impressions \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  --op BULK_INSERT</span><br></span></code></pre></div></div>
<p>Read more in depth about concurrency control in the <a class="" href="/docs/concurrency_control">concurrency control concepts</a> section</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="checkpointing">Checkpointing<a href="#checkpointing" class="hash-link" aria-label="Direct link to Checkpointing" title="Direct link to Checkpointing" translate="no">​</a></h2>
<p><code>HoodieStreamer</code> uses checkpoints to keep track of what data has been read already so it can resume without needing to reprocess all data.
When using a Kafka source, the checkpoint is the <a href="https://cwiki.apache.org/confluence/display/KAFKA/Offset+Management" target="_blank" rel="noopener noreferrer" class="">Kafka Offset</a>
When using a DFS source, the checkpoint is the &#x27;last modified&#x27; timestamp of the latest file read.
Checkpoints are saved in the .hoodie commit file as <code>streamer.checkpoint.key</code>.</p>
<p>If you need to change the checkpoints for reprocessing or replaying data you can use the following options:</p>
<ul>
<li class=""><code>--checkpoint</code> will set <code>streamer.checkpoint.reset_key</code> in the commit file to overwrite the current checkpoint.</li>
<li class=""><code>--source-limit</code> will set a maximum amount of data to read from the source. For DFS sources, this is max # of bytes read.
For Kafka, this is the max # of events to read.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="schema-providers">Schema Providers<a href="#schema-providers" class="hash-link" aria-label="Direct link to Schema Providers" title="Direct link to Schema Providers" translate="no">​</a></h2>
<p>By default, Spark will infer the schema of the source and use that inferred schema when writing to a table. If you need
to explicitly define the schema you can use one of the following Schema Providers below.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="schema-registry-provider">Schema Registry Provider<a href="#schema-registry-provider" class="hash-link" aria-label="Direct link to Schema Registry Provider" title="Direct link to Schema Registry Provider" translate="no">​</a></h3>
<p>You can obtain the latest schema from an online registry. You pass a URL to the registry and if needed, you can also
pass userinfo and credentials in the url like: <code>https://foo:bar@schemaregistry.org</code> The credentials are then extracted
and are set on the request as an Authorization Header.</p>
<p>When fetching schemas from a registry, you can specify both the source schema and the target schema separately.</p>
<table><thead><tr><th>Config</th><th>Description</th><th>Example</th></tr></thead><tbody><tr><td>hoodie.streamer.schemaprovider.registry.url</td><td>The schema of the source you are reading from</td><td><a href="https://foo:bar@schemaregistry.org" target="_blank" rel="noopener noreferrer" class="">https://foo:bar@schemaregistry.org</a></td></tr><tr><td>hoodie.streamer.schemaprovider.registry.targetUrl</td><td>The schema of the target you are writing to</td><td><a href="https://foo:bar@schemaregistry.org" target="_blank" rel="noopener noreferrer" class="">https://foo:bar@schemaregistry.org</a></td></tr></tbody></table>
<p>The above configs are passed to Hudi Streamer spark-submit command like:
<code>--hoodie-conf hoodie.streamer.schemaprovider.registry.url=https://foo:bar@schemaregistry.org</code></p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="jdbc-schema-provider">JDBC Schema Provider<a href="#jdbc-schema-provider" class="hash-link" aria-label="Direct link to JDBC Schema Provider" title="Direct link to JDBC Schema Provider" translate="no">​</a></h3>
<p>You can obtain the latest schema through a JDBC connection.</p>
<table><thead><tr><th>Config</th><th>Description</th><th>Example</th></tr></thead><tbody><tr><td>hoodie.streamer.schemaprovider.source.schema.jdbc.connection.url</td><td>The JDBC URL to connect to. You can specify source specific connection properties in the URL</td><td>jdbc:postgresql://localhost/test?user=fred&amp;password=secret</td></tr><tr><td>hoodie.streamer.schemaprovider.source.schema.jdbc.driver.type</td><td>The class name of the JDBC driver to use to connect to this URL</td><td>org.h2.Driver</td></tr><tr><td>hoodie.streamer.schemaprovider.source.schema.jdbc.username</td><td>username for the connection</td><td>fred</td></tr><tr><td>hoodie.streamer.schemaprovider.source.schema.jdbc.password</td><td>password for the connection</td><td>secret</td></tr><tr><td>hoodie.streamer.schemaprovider.source.schema.jdbc.dbtable</td><td>The table with the schema to reference</td><td>test_database.test1_table or test1_table</td></tr><tr><td>hoodie.streamer.schemaprovider.source.schema.jdbc.timeout</td><td>The number of seconds the driver will wait for a Statement object to execute to the given number of seconds. Zero means there is no limit. In the write path, this option depends on how JDBC drivers implement the API setQueryTimeout, e.g., the h2 JDBC driver checks the timeout of each query instead of an entire JDBC batch. It defaults to 0.</td><td>0</td></tr><tr><td>hoodie.streamer.schemaprovider.source.schema.jdbc.nullable</td><td>If true, all columns are nullable</td><td>true</td></tr></tbody></table>
<p>The above configs are passed to Hudi Streamer spark-submit command like:
<code>--hoodie-conf hoodie.streamer.jdbcbasedschemaprovider.connection.url=jdbc:postgresql://localhost/test?user=fred&amp;password=secret</code></p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="file-based-schema-provider">File Based Schema Provider<a href="#file-based-schema-provider" class="hash-link" aria-label="Direct link to File Based Schema Provider" title="Direct link to File Based Schema Provider" translate="no">​</a></h3>
<p>You can use a .avsc file to define your schema. You can then point to this file on DFS as a schema provider.</p>
<table><thead><tr><th>Config</th><th>Description</th><th>Example</th></tr></thead><tbody><tr><td>hoodie.streamer.schemaprovider.source.schema.file</td><td>The schema of the source you are reading from</td><td><a href="https://github.com/apache/hudi/blob/a8fb69656f522648233f0310ca3756188d954281/docker/demo/config/test-suite/source.avsc" target="_blank" rel="noopener noreferrer" class="">example schema file</a></td></tr><tr><td>hoodie.streamer.schemaprovider.target.schema.file</td><td>The schema of the target you are writing to</td><td><a href="https://github.com/apache/hudi/blob/a8fb69656f522648233f0310ca3756188d954281/docker/demo/config/test-suite/target.avsc" target="_blank" rel="noopener noreferrer" class="">example schema file</a></td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="hive-schema-provider">Hive Schema Provider<a href="#hive-schema-provider" class="hash-link" aria-label="Direct link to Hive Schema Provider" title="Direct link to Hive Schema Provider" translate="no">​</a></h3>
<p>You can use hive tables to fetch source and target schema.</p>
<table><thead><tr><th>Config</th><th>Description</th></tr></thead><tbody><tr><td>hoodie.streamer.schemaprovider.source.schema.hive.database</td><td>Hive database from where source schema can be fetched</td></tr><tr><td>hoodie.streamer.schemaprovider.source.schema.hive.table</td><td>Hive table from where source schema can be fetched</td></tr><tr><td>hoodie.streamer.schemaprovider.target.schema.hive.database</td><td>Hive database from where target schema can be fetched</td></tr><tr><td>hoodie.streamer.schemaprovider.target.schema.hive.table</td><td>Hive table from where target schema can be fetched</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="schema-provider-with-post-processor">Schema Provider with Post Processor<a href="#schema-provider-with-post-processor" class="hash-link" aria-label="Direct link to Schema Provider with Post Processor" title="Direct link to Schema Provider with Post Processor" translate="no">​</a></h3>
<p>The SchemaProviderWithPostProcessor, will extract the schema from one of the previously mentioned Schema Providers and
then will apply a post processor to change the schema before it is used. You can write your own post processor by extending
this class: <a href="https://github.com/apache/hudi/blob/master/hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaPostProcessor.java" target="_blank" rel="noopener noreferrer" class="">https://github.com/apache/hudi/blob/master/hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaPostProcessor.java</a></p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="sources">Sources<a href="#sources" class="hash-link" aria-label="Direct link to Sources" title="Direct link to Sources" translate="no">​</a></h2>
<p>Hoodie Streamer can read data from a wide variety of sources. The following are a list of supported sources:</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="distributed-file-system-dfs">Distributed File System (DFS)<a href="#distributed-file-system-dfs" class="hash-link" aria-label="Direct link to Distributed File System (DFS)" title="Direct link to Distributed File System (DFS)" translate="no">​</a></h3>
<p>See the storage configurations page to see some examples of DFS applications Hudi can read from. The following are the
supported file formats Hudi can read/write with on DFS Sources. (Note: you can still use Spark/Flink readers to read from
other formats and then write data as Hudi format.)</p>
<ul>
<li class="">CSV</li>
<li class="">AVRO</li>
<li class="">JSON</li>
<li class="">PARQUET</li>
<li class="">ORC</li>
<li class="">HUDI</li>
</ul>
<p>For DFS sources the following behaviors are expected:</p>
<ul>
<li class="">For JSON DFS source, you always need to set a schema. If the target Hudi table follows the same schema as from the source file, you just need to set the source schema. If not, you need to set schemas for both source and target.</li>
<li class=""><code>HoodieStreamer</code> reads the files under the source base path (<code>hoodie.streamer.source.dfs.root</code>) directly, and it won&#x27;t use the partition paths under this base path as fields of the dataset. Detailed examples can be found <a href="https://github.com/apache/hudi/issues/5485" target="_blank" rel="noopener noreferrer" class="">here</a>.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="kafka">Kafka<a href="#kafka" class="hash-link" aria-label="Direct link to Kafka" title="Direct link to Kafka" translate="no">​</a></h3>
<p>Hudi can read directly from Kafka clusters. See more details on <code>HoodieStreamer</code> to learn how to setup streaming
ingestion with exactly once semantics, checkpointing, and plugin transformations. The following formats are supported
when reading data from Kafka:</p>
<ul>
<li class="">AVRO</li>
<li class="">JSON</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="s3-events">S3 Events<a href="#s3-events" class="hash-link" aria-label="Direct link to S3 Events" title="Direct link to S3 Events" translate="no">​</a></h3>
<p>AWS S3 storage provides an event notification service which will post notifications when certain events happen in your S3 bucket:
<a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html" target="_blank" rel="noopener noreferrer" class="">https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html</a>
AWS will put these events in a Simple Queue Service (SQS). Apache Hudi provides an S3EventsSource that can read from SQS
to trigger/processing of new or changed data as soon as it is available on S3.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="setup">Setup<a href="#setup" class="hash-link" aria-label="Direct link to Setup" title="Direct link to Setup" translate="no">​</a></h4>
<ol>
<li class="">Enable S3 Event Notifications <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html" target="_blank" rel="noopener noreferrer" class="">https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html</a></li>
<li class="">Download the aws-java-sdk-sqs jar.</li>
<li class="">Find the queue URL and Region to set these configurations:<!-- -->
<ol>
<li class="">hoodie.streamer.s3.source.queue.url=<a href="https://sqs.us-west-2.amazonaws.com/queue/url" target="_blank" rel="noopener noreferrer" class="">https://sqs.us-west-2.amazonaws.com/queue/url</a></li>
<li class="">hoodie.streamer.s3.source.queue.region=us-west-2</li>
</ol>
</li>
<li class="">start the S3EventsSource and S3EventsHoodieIncrSource using the <code>HoodieStreamer</code> utility as shown in sample commands below:</li>
</ol>
<p>Insert code sample from this blog: <a href="https://hudi.apache.org/blog/2021/08/23/s3-events-source/#configuration-and-setup" target="_blank" rel="noopener noreferrer" class="">https://hudi.apache.org/blog/2021/08/23/s3-events-source/#configuration-and-setup</a></p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="jdbc-source">JDBC Source<a href="#jdbc-source" class="hash-link" aria-label="Direct link to JDBC Source" title="Direct link to JDBC Source" translate="no">​</a></h3>
<p>Hudi can read from a JDBC source with a full fetch of a table, or Hudi can even read incrementally with checkpointing from a JDBC source.</p>
<table><thead><tr><th>Config</th><th>Description</th><th>Example</th></tr></thead><tbody><tr><td>hoodie.streamer.jdbc.url</td><td>URL of the JDBC connection</td><td>jdbc:postgresql://localhost/test</td></tr><tr><td>hoodie.streamer.jdbc.user</td><td>User to use for authentication of the JDBC connection</td><td>fred</td></tr><tr><td>hoodie.streamer.jdbc.password</td><td>Password to use for authentication of the JDBC connection</td><td>secret</td></tr><tr><td>hoodie.streamer.jdbc.password.file</td><td>If you prefer to use a password file for the connection</td><td></td></tr><tr><td>hoodie.streamer.jdbc.driver.class</td><td>Driver class to use for the JDBC connection</td><td></td></tr><tr><td>hoodie.streamer.jdbc.table.name</td><td></td><td>my_table</td></tr><tr><td>hoodie.streamer.jdbc.table.incr.column.name</td><td>If run in incremental mode, this field will be used to pull new data incrementally</td><td></td></tr><tr><td>hoodie.streamer.jdbc.incr.pull</td><td>Will the JDBC connection perform an incremental pull?</td><td></td></tr><tr><td>hoodie.streamer.jdbc.extra.options.</td><td>How you pass extra configurations that would normally by specified as spark.read.option()</td><td>hoodie.streamer.jdbc.extra.options.fetchSize=100 hoodie.streamer.jdbc.extra.options.upperBound=1 hoodie.streamer.jdbc.extra.options.lowerBound=100</td></tr><tr><td>hoodie.streamer.jdbc.storage.level</td><td>Used to control the persistence level</td><td>Default = MEMORY_AND_DISK_SER</td></tr><tr><td>hoodie.streamer.jdbc.incr.fallback.to.full.fetch</td><td>Boolean which if set true makes an incremental fetch fallback to a full fetch if there is any error in the incremental read</td><td>FALSE</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="sql-source">SQL Source<a href="#sql-source" class="hash-link" aria-label="Direct link to SQL Source" title="Direct link to SQL Source" translate="no">​</a></h3>
<p>SQL Source that reads from any table, used mainly for backfill jobs which will process specific partition dates.
This won&#x27;t update the streamer.checkpoint.key to the processed commit, instead it will fetch the latest successful
checkpoint key and set that value as this backfill commits checkpoint so that it won&#x27;t interrupt the regular incremental
processing. To fetch and use the latest incremental checkpoint, you need to also set this hoodie_conf for Hudi Streamer
jobs: <code>hoodie.write.meta.key.prefixes = &#x27;streamer.checkpoint.key&#x27;</code></p>
<p>Spark SQL should be configured using this hoodie config:
hoodie.streamer.source.sql.sql.query = &#x27;select * from source_table&#x27;</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="structured-streaming">Structured Streaming<a href="#structured-streaming" class="hash-link" aria-label="Direct link to Structured Streaming" title="Direct link to Structured Streaming" translate="no">​</a></h2>
<p>Hudi supports Spark Structured Streaming reads and writes.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="streaming-write">Streaming Write<a href="#streaming-write" class="hash-link" aria-label="Direct link to Streaming Write" title="Direct link to Streaming Write" translate="no">​</a></h3>
<p>You can write Hudi tables using spark&#x27;s structured streaming.</p>
<div class="theme-tabs-container tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Scala</li><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Python</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-scala codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-scala codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">// spark-shell</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">// prepare to stream write to new table</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">import org.apache.spark.sql.streaming.Trigger</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">val streamingTableName = &quot;hudi_trips_cow_streaming&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">val baseStreamingPath = &quot;file:///tmp/hudi_trips_cow_streaming&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">val checkpointLocation = &quot;file:///tmp/checkpoints/hudi_trips_cow_streaming&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">// create streaming df</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">val df = spark.readStream.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        format(&quot;hudi&quot;).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        load(basePath)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">// write stream to new hudi table</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">df.writeStream.format(&quot;hudi&quot;).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  options(getQuickstartWriteConfigs).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  option(PRECOMBINE_FIELD_OPT_KEY, &quot;ts&quot;).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  option(RECORDKEY_FIELD_OPT_KEY, &quot;uuid&quot;).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  option(PARTITIONPATH_FIELD_OPT_KEY, &quot;partitionpath&quot;).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  option(TABLE_NAME, streamingTableName).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  outputMode(&quot;append&quot;).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  option(&quot;path&quot;, baseStreamingPath).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  option(&quot;checkpointLocation&quot;, checkpointLocation).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  trigger(Trigger.Once()).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  start()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6"><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># pyspark</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># prepare to stream write to new table</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">streamingTableName </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;hudi_trips_cow_streaming&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">baseStreamingPath </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;file:///tmp/hudi_trips_cow_streaming&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">checkpointLocation </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;file:///tmp/checkpoints/hudi_trips_cow_streaming&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">hudi_streaming_options </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;hoodie.table.name&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> streamingTableName</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;hoodie.datasource.write.recordkey.field&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;uuid&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;hoodie.datasource.write.partitionpath.field&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;partitionpath&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;hoodie.datasource.write.table.name&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> streamingTableName</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;hoodie.datasource.write.operation&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;upsert&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;hoodie.datasource.write.precombine.field&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;ts&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;hoodie.upsert.shuffle.parallelism&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;hoodie.insert.shuffle.parallelism&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">2</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># create streaming df</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">df </span><span class="token operator">=</span><span class="token plain"> spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">readStream \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token builtin" style="color:rgb(189, 147, 249)">format</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;hudi&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">load</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">basePath</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># write stream to new hudi table</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">df</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">writeStream</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token builtin" style="color:rgb(189, 147, 249)">format</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;hudi&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">options</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token operator">**</span><span class="token plain">hudi_streaming_options</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">outputMode</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;append&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">option</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;path&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> baseStreamingPath</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">option</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;checkpointLocation&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> checkpointLocation</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">trigger</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">once</span><span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">start</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span></code></pre></div></div></div></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="streaming-read">Streaming Read<a href="#streaming-read" class="hash-link" aria-label="Direct link to Streaming Read" title="Direct link to Streaming Read" translate="no">​</a></h3>
<p>Structured Streaming reads are based on Hudi&#x27;s Incremental Query feature, therefore streaming read can return data for which
commits and base files were not yet removed by the cleaner. You can control commits retention time.</p>
<div class="theme-tabs-container tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Scala</li><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Python</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-scala codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-scala codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">// spark-shell</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">// reload data</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">df.write.format(&quot;hudi&quot;).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  options(getQuickstartWriteConfigs).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  option(PRECOMBINE_FIELD_OPT_KEY, &quot;ts&quot;).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  option(RECORDKEY_FIELD_OPT_KEY, &quot;uuid&quot;).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  option(PARTITIONPATH_FIELD_OPT_KEY, &quot;partitionpath&quot;).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  option(TABLE_NAME, tableName).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  mode(Overwrite).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  save(basePath)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">// read stream and output results to console</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark.readStream.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  format(&quot;hudi&quot;).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  load(basePath).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  writeStream.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  format(&quot;console&quot;).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  start()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">// read stream to streaming df</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">val df = spark.readStream.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        format(&quot;hudi&quot;).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        load(basePath)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6"><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># pyspark</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># reload data</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">inserts </span><span class="token operator">=</span><span class="token plain"> sc</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">_jvm</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">org</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">apache</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">hudi</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">QuickstartUtils</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">convertToStringList</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    dataGen</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">generateInserts</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">10</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">df </span><span class="token operator">=</span><span class="token plain"> spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">read</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">json</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">sparkContext</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">parallelize</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">inserts</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">hudi_options </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;hoodie.table.name&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> tableName</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;hoodie.datasource.write.recordkey.field&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;uuid&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;hoodie.datasource.write.partitionpath.field&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;partitionpath&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;hoodie.datasource.write.table.name&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> tableName</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;hoodie.datasource.write.operation&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;upsert&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;hoodie.datasource.write.precombine.field&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;ts&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;hoodie.upsert.shuffle.parallelism&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;hoodie.insert.shuffle.parallelism&#x27;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">2</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">df</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">write</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token builtin" style="color:rgb(189, 147, 249)">format</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;hudi&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    options</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token operator">**</span><span class="token plain">hudi_options</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    mode</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;overwrite&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    save</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">basePath</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># read stream to streaming df</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">df </span><span class="token operator">=</span><span class="token plain"> spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">readStream \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token builtin" style="color:rgb(189, 147, 249)">format</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;hudi&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">load</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">basePath</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># ead stream and output results to console</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">readStream \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token builtin" style="color:rgb(189, 147, 249)">format</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;hudi&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">load</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">basePath</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">writeStream \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token builtin" style="color:rgb(189, 147, 249)">format</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;console&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">start</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span></code></pre></div></div></div></div></div>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>Spark SQL can be used within ForeachBatch sink to do INSERT, UPDATE, DELETE and MERGE INTO.
Target table must exist before write.</p></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="flink-ingestion">Flink Ingestion<a href="#flink-ingestion" class="hash-link" aria-label="Direct link to Flink Ingestion" title="Direct link to Flink Ingestion" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="cdc-ingestion">CDC Ingestion<a href="#cdc-ingestion" class="hash-link" aria-label="Direct link to CDC Ingestion" title="Direct link to CDC Ingestion" translate="no">​</a></h3>
<p>CDC(change data capture) keep track of the data changes evolving in a source system so a downstream process or system can action that change.
We recommend two ways for syncing CDC data into Hudi:</p>
<p><img decoding="async" loading="lazy" alt="slide1 title" src="/assets/images/cdc-2-hudi-d151389758f4ce3fd873c1258b0a8ce5.png" width="1440" height="626" class="img_ev3q"></p>
<ol>
<li class="">Using the Ververica <a href="https://github.com/ververica/flink-cdc-connectors" target="_blank" rel="noopener noreferrer" class="">flink-cdc-connectors</a> directly connect to DB Server to sync the binlog data into Hudi.
The advantage is that it does not rely on message queues, but the disadvantage is that it puts pressure on the db server;</li>
<li class="">Consume data from a message queue (for e.g, the Kafka) using the flink cdc format, the advantage is that it is highly scalable,
but the disadvantage is that it relies on message queues.</li>
</ol>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><ul>
<li class="">If the upstream data cannot guarantee the order, you need to specify option <code>write.precombine.field</code> explicitly;</li>
</ul></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="bulk-insert">Bulk Insert<a href="#bulk-insert" class="hash-link" aria-label="Direct link to Bulk Insert" title="Direct link to Bulk Insert" translate="no">​</a></h3>
<p>For the demand of snapshot data import. If the snapshot data comes from other data sources, use the <code>bulk_insert</code> mode to quickly
import the snapshot data into Hudi.</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p><code>bulk_insert</code> eliminates the serialization and data merging. The data deduplication is skipped, so the user need to guarantee the uniqueness of the data.</p></div></div>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p><code>bulk_insert</code> is more efficient in the <code>batch execution mode</code>. By default, the <code>batch execution mode</code> sorts the input records
by the partition path and writes these records to Hudi, which can avoid write performance degradation caused by
frequent <code>file handle</code> switching.</p></div></div>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>The parallelism of <code>bulk_insert</code> is specified by <code>write.tasks</code>. The parallelism will affect the number of small files.
In theory, the parallelism of <code>bulk_insert</code> is the number of <code>bucket</code>s (In particular, when each bucket writes to maximum file size, it
will rollover to the new file handle. Finally, <code>the number of files</code> &gt;= <a class="" href="/docs/configurations#writebucket_assigntasks"><code>write.bucket_assign.tasks</code></a>.</p></div></div>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="options">Options<a href="#options" class="hash-link" aria-label="Direct link to Options" title="Direct link to Options" translate="no">​</a></h4>
<table><thead><tr><th>Option Name</th><th>Required</th><th>Default</th><th>Remarks</th></tr></thead><tbody><tr><td><code>write.operation</code></td><td><code>true</code></td><td><code>upsert</code></td><td>Setting as <code>bulk_insert</code> to open this function</td></tr><tr><td><code>write.tasks</code></td><td><code>false</code></td><td><code>4</code></td><td>The parallelism of <code>bulk_insert</code>, <code>the number of files</code> &gt;= <a class="" href="/docs/configurations#writebucket_assigntasks"><code>write.bucket_assign.tasks</code></a></td></tr><tr><td><code>write.bulk_insert.shuffle_input</code></td><td><code>false</code></td><td><code>true</code></td><td>Whether to shuffle data according to the input field before writing. Enabling this option will reduce the number of small files, but there may be a risk of data skew</td></tr><tr><td><code>write.bulk_insert.sort_input</code></td><td><code>false</code></td><td><code>true</code></td><td>Whether to sort data according to the input field before writing. Enabling this option will reduce the number of small files when a write task writes multiple partitions</td></tr><tr><td><code>write.sort.memory</code></td><td><code>false</code></td><td><code>128</code></td><td>Available managed memory of sort operator. default  <code>128</code> MB</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="index-bootstrap">Index Bootstrap<a href="#index-bootstrap" class="hash-link" aria-label="Direct link to Index Bootstrap" title="Direct link to Index Bootstrap" translate="no">​</a></h3>
<p>For the demand of <code>snapshot data</code> + <code>incremental data</code> import. If the <code>snapshot data</code> already insert into Hudi by  <a href="#bulk-insert" class="">bulk insert</a>.
User can insert <code>incremental data</code> in real time and ensure the data is not repeated by using the index bootstrap function.</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>If you think this process is very time-consuming, you can add resources to write in streaming mode while writing <code>snapshot data</code>,
and then reduce the resources to write <code>incremental data</code> (or open the rate limit function).</p></div></div>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="options-1">Options<a href="#options-1" class="hash-link" aria-label="Direct link to Options" title="Direct link to Options" translate="no">​</a></h4>
<table><thead><tr><th>Option Name</th><th>Required</th><th>Default</th><th>Remarks</th></tr></thead><tbody><tr><td><code>index.bootstrap.enabled</code></td><td><code>true</code></td><td><code>false</code></td><td>When index bootstrap is enabled, the remain records in Hudi table will be loaded into the Flink state at one time</td></tr><tr><td><code>index.partition.regex</code></td><td><code>false</code></td><td><code>*</code></td><td>Optimize option. Setting regular expressions to filter partitions. By default, all partitions are loaded into flink state</td></tr></tbody></table>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="how-to-use">How To Use<a href="#how-to-use" class="hash-link" aria-label="Direct link to How To Use" title="Direct link to How To Use" translate="no">​</a></h4>
<ol>
<li class=""><code>CREATE TABLE</code> creates a statement corresponding to the Hudi table. Note that the <code>table.type</code> must be correct.</li>
<li class="">Setting <code>index.bootstrap.enabled</code> = <code>true</code> to enable the index bootstrap function.</li>
<li class="">Setting Flink checkpoint failure tolerance in <code>flink-conf.yaml</code> : <code>execution.checkpointing.tolerable-failed-checkpoints = n</code> (depending on Flink checkpoint scheduling times).</li>
<li class="">Waiting until the first checkpoint succeeds, indicating that the index bootstrap completed.</li>
<li class="">After the index bootstrap completed, user can exit and save the savepoint (or directly use the externalized checkpoint).</li>
<li class="">Restart the job, setting <code>index.bootstrap.enable</code> as <code>false</code>.</li>
</ol>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><ol>
<li class="">Index bootstrap is blocking, so checkpoint cannot be completed during index bootstrap.</li>
<li class="">Index bootstrap triggers by the input data. User need to ensure that there is at least one record in each partition.</li>
<li class="">Index bootstrap executes concurrently. User can search in log by <code>finish loading the index under partition</code> and <code>Load record form file</code> to observe the progress of index bootstrap.</li>
<li class="">The first successful checkpoint indicates that the index bootstrap completed. There is no need to load the index again when recovering from the checkpoint.</li>
</ol></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="changelog-mode">Changelog Mode<a href="#changelog-mode" class="hash-link" aria-label="Direct link to Changelog Mode" title="Direct link to Changelog Mode" translate="no">​</a></h3>
<p>Hudi can keep all the intermediate changes (I / -U / U / D) of messages, then consumes through stateful computing of flink to have a near-real-time
data warehouse ETL pipeline (Incremental computing). Hudi MOR table stores messages in the forms of rows, which supports the retention of all change logs (Integration at the format level).
All changelog records can be consumed with Flink streaming reader.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="options-2">Options<a href="#options-2" class="hash-link" aria-label="Direct link to Options" title="Direct link to Options" translate="no">​</a></h4>
<table><thead><tr><th>Option Name</th><th>Required</th><th>Default</th><th>Remarks</th></tr></thead><tbody><tr><td><code>changelog.enabled</code></td><td><code>false</code></td><td><code>false</code></td><td>It is turned off by default, to have the <code>upsert</code> semantics, only the merged messages are ensured to be kept, intermediate changes may be merged. Setting to true to support consumption of all changes</td></tr></tbody></table>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>Batch (Snapshot) read still merge all the intermediate changes, regardless of whether the format has stored the intermediate changelog messages.</p></div></div>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>After setting <code>changelog.enable</code> as <code>true</code>, the retention of changelog records are only best effort: the asynchronous compaction task will merge the changelog records into one record, so if the
stream source does not consume timely, only the merged record for each key can be read after compaction. The solution is to reserve some buffer time for the reader by adjusting the compaction strategy, such as
the compaction options: <a href="#compaction" class=""><code>compaction.delta_commits</code></a> and <a href="#compaction" class=""><code>compaction.delta_seconds</code></a>.</p></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="append-mode">Append Mode<a href="#append-mode" class="hash-link" aria-label="Direct link to Append Mode" title="Direct link to Append Mode" translate="no">​</a></h3>
<p>For <code>INSERT</code> mode write operation, the current work flow is:</p>
<ul>
<li class="">For Merge_On_Read table, the small file strategies are by default applied: tries to append to the small avro log files first</li>
<li class="">For Copy_On_Write table, write new parquet files directly, no small file strategies are applied</li>
</ul>
<p>Hudi supports rich clustering strategies to optimize the files layout for <code>INSERT</code> mode:</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="inline-clustering">Inline Clustering<a href="#inline-clustering" class="hash-link" aria-label="Direct link to Inline Clustering" title="Direct link to Inline Clustering" translate="no">​</a></h4>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>Only Copy_On_Write table is supported.</p></div></div>
<table><thead><tr><th>Option Name</th><th>Required</th><th>Default</th><th>Remarks</th></tr></thead><tbody><tr><td><code>write.insert.cluster</code></td><td><code>false</code></td><td><code>false</code></td><td>Whether to merge small files while ingesting, for COW table, open the option to enable the small file merging strategy(no deduplication for keys but the throughput will be affected)</td></tr></tbody></table>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="async-clustering">Async Clustering<a href="#async-clustering" class="hash-link" aria-label="Direct link to Async Clustering" title="Direct link to Async Clustering" translate="no">​</a></h4>
<table><thead><tr><th>Option Name</th><th>Required</th><th>Default</th><th>Remarks</th></tr></thead><tbody><tr><td><code>clustering.schedule.enabled</code></td><td><code>false</code></td><td><code>false</code></td><td>Whether to schedule clustering plan during write process, by default false</td></tr><tr><td><code>clustering.delta_commits</code></td><td><code>false</code></td><td><code>4</code></td><td>Delta commits to schedule the clustering plan, only valid when <code>clustering.schedule.enabled</code> is true</td></tr><tr><td><code>clustering.async.enabled</code></td><td><code>false</code></td><td><code>false</code></td><td>Whether to execute clustering plan asynchronously, by default false</td></tr><tr><td><code>clustering.tasks</code></td><td><code>false</code></td><td><code>4</code></td><td>Parallelism of the clustering tasks</td></tr><tr><td><code>clustering.plan.strategy.target.file.max.bytes</code></td><td><code>false</code></td><td><code>1024*1024*1024</code></td><td>The target file size for clustering group, by default 1GB</td></tr><tr><td><code>clustering.plan.strategy.small.file.limit</code></td><td><code>false</code></td><td><code>600</code></td><td>The file that has less size than the threshold (unit MB) are candidates for clustering</td></tr><tr><td><code>clustering.plan.strategy.sort.columns</code></td><td><code>false</code></td><td><code>N/A</code></td><td>The columns to sort by when clustering</td></tr></tbody></table>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="clustering-plan-strategy">Clustering Plan Strategy<a href="#clustering-plan-strategy" class="hash-link" aria-label="Direct link to Clustering Plan Strategy" title="Direct link to Clustering Plan Strategy" translate="no">​</a></h4>
<p>Custom clustering strategy is supported.</p>
<table><thead><tr><th>Option Name</th><th>Required</th><th>Default</th><th>Remarks</th></tr></thead><tbody><tr><td><code>clustering.plan.partition.filter.mode</code></td><td><code>false</code></td><td><code>NONE</code></td><td>Valid options 1) <code>NONE</code>: no limit; 2) <code>RECENT_DAYS</code>: choose partitions that represent recent days; 3) <code>SELECTED_PARTITIONS</code>: specific partitions</td></tr><tr><td><code>clustering.plan.strategy.daybased.lookback.partitions</code></td><td><code>false</code></td><td><code>2</code></td><td>Valid for <code>RECENT_DAYS</code> mode</td></tr><tr><td><code>clustering.plan.strategy.cluster.begin.partition</code></td><td><code>false</code></td><td><code>N/A</code></td><td>Valid for <code>SELECTED_PARTITIONS</code> mode, specify the partition to begin with(inclusive)</td></tr><tr><td><code>clustering.plan.strategy.cluster.end.partition</code></td><td><code>false</code></td><td><code>N/A</code></td><td>Valid for <code>SELECTED_PARTITIONS</code> mode, specify the partition to end with(inclusive)</td></tr><tr><td><code>clustering.plan.strategy.partition.regex.pattern</code></td><td><code>false</code></td><td><code>N/A</code></td><td>The regex to filter the partitions</td></tr><tr><td><code>clustering.plan.strategy.partition.selected</code></td><td><code>false</code></td><td><code>N/A</code></td><td>Specific partitions separated by comma <code>,</code></td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="bucket-index">Bucket Index<a href="#bucket-index" class="hash-link" aria-label="Direct link to Bucket Index" title="Direct link to Bucket Index" translate="no">​</a></h3>
<p>By default, flink uses the state-backend to keep the file index: the mapping from primary key to fileId. When the input data set is large,
there is possibility the cost of the state be a bottleneck, the bucket index use deterministic hash algorithm for shuffling the records into
buckets, thus can avoid the storage and query overhead of indexes.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="options-3">Options<a href="#options-3" class="hash-link" aria-label="Direct link to Options" title="Direct link to Options" translate="no">​</a></h4>
<table><thead><tr><th>Option Name</th><th>Required</th><th>Default</th><th>Remarks</th></tr></thead><tbody><tr><td><code>index.type</code></td><td><code>false</code></td><td><code>FLINK_STATE</code></td><td>Set up as <code>BUCKET</code> to use bucket index</td></tr><tr><td><code>hoodie.bucket.index.hash.field</code></td><td><code>false</code></td><td>Primary key</td><td>Can be a subset of the primary key</td></tr><tr><td><code>hoodie.bucket.index.num.buckets</code></td><td><code>false</code></td><td><code>4</code></td><td>The number of buckets per-partition, it is immutable once set up</td></tr></tbody></table>
<p>Comparing to state index:</p>
<ul>
<li class="">Bucket index has no computing and storage cost of state-backend index, thus has better performance</li>
<li class="">Bucket index can not expand the buckets dynamically, the state-backend index can expand the buckets dynamically based on current file layout</li>
<li class="">Bucket index can not handle changes among partitions(no limit if the input itself is CDC stream), state-backend index has no limit</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="rate-limit">Rate Limit<a href="#rate-limit" class="hash-link" aria-label="Direct link to Rate Limit" title="Direct link to Rate Limit" translate="no">​</a></h3>
<p>There are many use cases that user put the full history data set onto the message queue together with the realtime incremental data. Then they consume the data from the queue into the hudi from the earliest offset using flink. Consuming history data set has these characteristics:
1). The instant throughput is huge 2). It has serious disorder (with random writing partitions). It will lead to degradation of writing performance and throughput glitches. For this case, the speed limit parameter can be turned on to ensure smooth writing of the flow.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="options-4">Options<a href="#options-4" class="hash-link" aria-label="Direct link to Options" title="Direct link to Options" translate="no">​</a></h4>
<table><thead><tr><th>Option Name</th><th>Required</th><th>Default</th><th>Remarks</th></tr></thead><tbody><tr><td><code>write.rate.limit</code></td><td><code>false</code></td><td><code>0</code></td><td>Default disable the rate limit</td></tr></tbody></table>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="kafka-connect-sink">Kafka Connect Sink<a href="#kafka-connect-sink" class="hash-link" aria-label="Direct link to Kafka Connect Sink" title="Direct link to Kafka Connect Sink" translate="no">​</a></h2>
<p>If you want to perform streaming ingestion into Hudi format similar to <code>HoodieStreamer</code>, but you don&#x27;t want to depend on Spark,
try out the new experimental release of Hudi Kafka Connect Sink. Read the <a href="https://github.com/apache/hudi/tree/master/hudi-kafka-connect" target="_blank" rel="noopener noreferrer" class="">ReadMe</a>
for full documentation.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/apache/hudi/tree/asf-site/website/versioned_docs/version-0.14.1/hoodie_streaming_ingestion.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/0.14.1/writing_data"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Writing Data</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/0.14.1/syncing_aws_glue_data_catalog"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">AWS Glue Data Catalog</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#hudi-streamer" class="table-of-contents__link toc-highlight">Hudi Streamer</a><ul><li><a href="#note-on-hudi-utilities-bundle-usage-for-different-spark-versions" class="table-of-contents__link toc-highlight">Note on hudi utilities bundle usage for different spark versions</a></li><li><a href="#concurrency-control" class="table-of-contents__link toc-highlight">Concurrency Control</a></li></ul></li><li><a href="#checkpointing" class="table-of-contents__link toc-highlight">Checkpointing</a></li><li><a href="#schema-providers" class="table-of-contents__link toc-highlight">Schema Providers</a><ul><li><a href="#schema-registry-provider" class="table-of-contents__link toc-highlight">Schema Registry Provider</a></li><li><a href="#jdbc-schema-provider" class="table-of-contents__link toc-highlight">JDBC Schema Provider</a></li><li><a href="#file-based-schema-provider" class="table-of-contents__link toc-highlight">File Based Schema Provider</a></li><li><a href="#hive-schema-provider" class="table-of-contents__link toc-highlight">Hive Schema Provider</a></li><li><a href="#schema-provider-with-post-processor" class="table-of-contents__link toc-highlight">Schema Provider with Post Processor</a></li></ul></li><li><a href="#sources" class="table-of-contents__link toc-highlight">Sources</a><ul><li><a href="#distributed-file-system-dfs" class="table-of-contents__link toc-highlight">Distributed File System (DFS)</a></li><li><a href="#kafka" class="table-of-contents__link toc-highlight">Kafka</a></li><li><a href="#s3-events" class="table-of-contents__link toc-highlight">S3 Events</a></li><li><a href="#jdbc-source" class="table-of-contents__link toc-highlight">JDBC Source</a></li><li><a href="#sql-source" class="table-of-contents__link toc-highlight">SQL Source</a></li></ul></li><li><a href="#structured-streaming" class="table-of-contents__link toc-highlight">Structured Streaming</a><ul><li><a href="#streaming-write" class="table-of-contents__link toc-highlight">Streaming Write</a></li><li><a href="#streaming-read" class="table-of-contents__link toc-highlight">Streaming Read</a></li></ul></li><li><a href="#flink-ingestion" class="table-of-contents__link toc-highlight">Flink Ingestion</a><ul><li><a href="#cdc-ingestion" class="table-of-contents__link toc-highlight">CDC Ingestion</a></li><li><a href="#bulk-insert" class="table-of-contents__link toc-highlight">Bulk Insert</a></li><li><a href="#index-bootstrap" class="table-of-contents__link toc-highlight">Index Bootstrap</a></li><li><a href="#changelog-mode" class="table-of-contents__link toc-highlight">Changelog Mode</a></li><li><a href="#append-mode" class="table-of-contents__link toc-highlight">Append Mode</a></li><li><a href="#bucket-index" class="table-of-contents__link toc-highlight">Bucket Index</a></li><li><a href="#rate-limit" class="table-of-contents__link toc-highlight">Rate Limit</a></li></ul></li><li><a href="#kafka-connect-sink" class="table-of-contents__link toc-highlight">Kafka Connect Sink</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">About</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog/2021/07/21/streaming-data-lake-platform">Our Vision</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/concepts">Concepts</a></li><li class="footer__item"><a class="footer__link-item" href="/community/team">Team</a></li><li class="footer__item"><a class="footer__link-item" href="/releases/release-1.0.2">Releases</a></li><li class="footer__item"><a class="footer__link-item" href="/releases/download">Download</a></li><li class="footer__item"><a class="footer__link-item" href="/powered-by">Who&#x27;s Using</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Learn</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/quick-start-guide">Quick Start</a></li><li class="footer__item"><a class="footer__link-item" href="/learn/tutorial-series">Tutorial Series</a></li><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a class="footer__link-item" href="/talks">Talks</a></li><li class="footer__item"><a class="footer__link-item" href="/videos">Video Guides</a></li><li class="footer__item"><a class="footer__link-item" href="/faq">FAQ</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Hudi On Cloud</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/s3_hoodie">AWS</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/gcs_hoodie">Google Cloud</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/oss_hoodie">Alibaba Cloud</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/azure_hoodie">Microsoft Azure</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/cos_hoodie">Tencent Cloud</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/ibm_cos_hoodie">IBM Cloud</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/oci_hoodie">Oracle Cloud</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/community/get-involved">Get Involved</a></li><li class="footer__item"><a href="https://join.slack.com/t/apache-hudi/shared_invite/zt-33fabmxb7-Q7QSUtNOHYCwUdYM8LbauA" target="_blank" rel="noopener noreferrer" class="footer__link-item">Slack<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://github.com/apache/hudi" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://x.com/ApacheHudi" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://www.youtube.com/channel/UCs7AhE0BWaEPZSChrBR-Muw" target="_blank" rel="noopener noreferrer" class="footer__link-item">YouTube<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://www.linkedin.com/company/apache-hudi/?viewAsMember=true" target="_blank" rel="noopener noreferrer" class="footer__link-item">Linkedin<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="mailto:dev-subscribe@hudi.apache.org?Subject=SubscribeToHudi" target="_blank" rel="noopener noreferrer" class="footer__link-item">Mailing List</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Apache</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.apache.org/events/current-event" target="_blank" rel="noopener noreferrer" class="footer__link-item">Events</a></li><li class="footer__item"><a href="https://www.apache.org/foundation/thanks.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Thanks</a></li><li class="footer__item"><a href="https://www.apache.org/licenses" target="_blank" rel="noopener noreferrer" class="footer__link-item">License</a></li><li class="footer__item"><a href="https://www.apache.org/security" target="_blank" rel="noopener noreferrer" class="footer__link-item">Security</a></li><li class="footer__item"><a class="footer__link-item" href="/asf/privacy">Privacy</a></li><li class="footer__item"><a class="footer__link-item" href="/asf/telemetry">Telemetry</a></li><li class="footer__item"><a href="https://www.apache.org/foundation/sponsorship.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Sponsorship</a></li><li class="footer__item"><a href="https://www.apache.org" target="_blank" rel="noopener noreferrer" class="footer__link-item">Foundation</a></li></ul></div></div><div class="footer__bottom text--center"><div class="margin-bottom--sm"><a href="https://hudi.apache.org/" rel="noopener noreferrer" class="footerLogoLink_BH7S"><img src="/assets/images/logo-big.png" alt="Apache Hudi™" class="footer__logo themedComponent_mlkZ themedComponent--light_NVdE"><img src="/assets/images/logo-big.png" alt="Apache Hudi™" class="footer__logo themedComponent_mlkZ themedComponent--dark_xIcU"></a></div><div class="footer__copyright">Copyright © 2021 <a href="https://apache.org">The Apache Software Foundation</a>, Licensed under the <a href="https://www.apache.org/licenses/LICENSE-2.0"> Apache License, Version 2.0</a>. <br>Hudi, Apache and the Apache feather logo are trademarks of The Apache Software Foundation.<img referrerpolicy="no-referrer-when-downgrade" src="https://static.scarf.sh/a.png?x-pxid=8f594acf-9b77-44fb-9475-3e82ead1910c"><img referrerpolicy="no-referrer-when-downgrade" src="https://analytics.apache.org/matomo.php?idsite=47&amp;rec=1"></div></div></div></footer></div>
</body>
</html>