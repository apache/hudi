<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.14">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Apache Hudi: User-Facing Analytics RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Apache Hudi: User-Facing Analytics Atom Feed">
<link rel="alternate" type="application/json" href="/blog/feed.json" title="Apache Hudi: User-Facing Analytics JSON Feed">
<link rel="search" type="application/opensearchdescription+xml" title="Apache Hudi" href="/opensearch.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Comfortaa|Ubuntu|Roboto|Source+Code+Pro">
<link rel="stylesheet" href="https://at-ui.github.io/feather-font/css/iconfont.css"><title data-react-helmet="true">Apache Hudi</title><meta data-react-helmet="true" property="og:title" content="Apache Hudi"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://hudi.apache.org/tech-specs"><meta data-react-helmet="true" name="docsearch:language" content="en"><meta data-react-helmet="true" name="docsearch:docusaurus_tag" content="default"><meta data-react-helmet="true" name="keywords" content="apache hudi, data lake, lakehouse, big data, apache spark, apache flink, presto, trino, analytics, data engineering"><link data-react-helmet="true" rel="icon" href="/assets/images/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://hudi.apache.org/tech-specs"><link data-react-helmet="true" rel="alternate" href="https://hudi.apache.org/tech-specs" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://hudi.apache.org/cn/tech-specs" hreflang="cn"><link data-react-helmet="true" rel="alternate" href="https://hudi.apache.org/tech-specs" hreflang="x-default"><link data-react-helmet="true" rel="preconnect" href="https://BH4D9OD16A-dsn.algolia.net" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.635c4f43.css">
<link rel="preload" href="/assets/js/runtime~main.1da50198.js" as="script">
<link rel="preload" href="/assets/js/main.31205107.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><div class="announcementBar_axC9" role="banner"><div class="announcementBarPlaceholder_xYHE"></div><div class="announcementBarContent_6uhP">⭐️ If you like Apache Hudi, give it a star on <a target="_blank" rel="noopener noreferrer" href="https://github.com/apache/hudi">GitHub</a>! ⭐</div><button type="button" class="clean-btn close announcementBarClose_A3A1" aria-label="Close"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/assets/images/hudi.png" alt="Apache Hudi" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/assets/images/hudi.png" alt="Apache Hudi" class="themedImage_TMUO themedImage--dark_uzRr"></div></a><a class="navbar__item navbar__link" href="/docs/overview">Docs</a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" class="navbar__link">Learn</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/talks">Talks</a></li><li><a class="dropdown__link" href="/docs/faq">FAQ</a></li><li><a class="dropdown__link" href="/tech-specs">Tech Specs</a></li><li><a href="https://cwiki.apache.org/confluence/display/HUDI" target="_blank" rel="noopener noreferrer" class="dropdown__link"><span>Technical Wiki<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" class="navbar__link">Contribute</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/contribute/how-to-contribute">How to Contribute</a></li><li><a class="dropdown__link" href="/contribute/developer-setup">Developer Setup</a></li><li><a class="dropdown__link" href="/contribute/rfc-process">RFC Process</a></li><li><a class="dropdown__link" href="/contribute/report-security-issues">Report Security Issues</a></li><li><a href="https://issues.apache.org/jira/projects/HUDI/summary" target="_blank" rel="noopener noreferrer" class="dropdown__link"><span>Report Issues<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" class="navbar__link">Community</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/community/get-involved">Get Involved</a></li><li><a class="dropdown__link" href="/community/syncs">Community Syncs</a></li><li><a class="dropdown__link" href="/community/office_hours">Office Hours</a></li><li><a class="dropdown__link" href="/community/team">Team</a></li></ul></div><a class="navbar__item navbar__link" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/powered-by">Who&#x27;s Using</a><a class="navbar__item navbar__link" href="/roadmap">Roadmap</a><a class="navbar__item navbar__link" href="/releases/download">Download</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a class="navbar__link" href="/docs/overview">0.12.0</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/next/overview">Current</a></li><li><a class="dropdown__link" href="/docs/overview">0.12.0</a></li><li><a class="dropdown__link" href="/docs/0.11.1/overview">0.11.1</a></li><li><a class="dropdown__link" href="/docs/0.11.0/overview">0.11.0</a></li><li><a class="dropdown__link" href="/docs/0.10.1/overview">0.10.1</a></li><li><a class="dropdown__link" href="/docs/0.10.0/overview">0.10.0</a></li><li><a class="dropdown__link" href="/docs/0.9.0/overview">0.9.0</a></li><li><a class="dropdown__link" href="/docs/0.8.0/overview">0.8.0</a></li><li><a class="dropdown__link" href="/docs/0.7.0/overview">0.7.0</a></li><li><a class="dropdown__link" href="/docs/0.6.0/quick-start-guide">0.6.0</a></li><li><a class="dropdown__link" href="/docs/0.5.3/quick-start-guide">0.5.3</a></li><li><a class="dropdown__link" href="/docs/0.5.2/quick-start-guide">0.5.2</a></li><li><a class="dropdown__link" href="/docs/0.5.1/quick-start-guide">0.5.1</a></li><li><a class="dropdown__link" href="/docs/0.5.0/quick-start-guide">0.5.0</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" class="navbar__link"><span><svg viewBox="0 0 20 20" width="20" height="20" aria-hidden="true" class="iconLanguage_EbrZ"><path fill="currentColor" d="M19.753 10.909c-.624-1.707-2.366-2.726-4.661-2.726-.09 0-.176.002-.262.006l-.016-2.063 3.525-.607c.115-.019.133-.119.109-.231-.023-.111-.167-.883-.188-.976-.027-.131-.102-.127-.207-.109-.104.018-3.25.461-3.25.461l-.013-2.078c-.001-.125-.069-.158-.194-.156l-1.025.016c-.105.002-.164.049-.162.148l.033 2.307s-3.061.527-3.144.543c-.084.014-.17.053-.151.143.019.09.19 1.094.208 1.172.018.08.072.129.188.107l2.924-.504.035 2.018c-1.077.281-1.801.824-2.256 1.303-.768.807-1.207 1.887-1.207 2.963 0 1.586.971 2.529 2.328 2.695 3.162.387 5.119-3.06 5.769-4.715 1.097 1.506.256 4.354-2.094 5.98-.043.029-.098.129-.033.207l.619.756c.08.096.206.059.256.023 2.51-1.73 3.661-4.515 2.869-6.683zm-7.386 3.188c-.966-.121-.944-.914-.944-1.453 0-.773.327-1.58.876-2.156a3.21 3.21 0 011.229-.799l.082 4.277a2.773 2.773 0 01-1.243.131zm2.427-.553l.046-4.109c.084-.004.166-.01.252-.01.773 0 1.494.145 1.885.361.391.217-1.023 2.713-2.183 3.758zm-8.95-7.668a.196.196 0 00-.196-.145h-1.95a.194.194 0 00-.194.144L.008 16.916c-.017.051-.011.076.062.076h1.733c.075 0 .099-.023.114-.072l1.008-3.318h3.496l1.008 3.318c.016.049.039.072.113.072h1.734c.072 0 .078-.025.062-.076-.014-.05-3.083-9.741-3.494-11.04zm-2.618 6.318l1.447-5.25 1.447 5.25H3.226z"></path></svg><span>English</span></span></a><ul class="dropdown__menu"><li><a href="/tech-specs" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active">English</a></li><li><a href="/cn/tech-specs" target="_self" rel="noopener noreferrer" class="dropdown__link">Chinese</a></li></ul></div><a href="https://github.com/apache/hudi" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><a href="https://twitter.com/ApacheHudi" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-twitter-link" aria-label="Hudi Twitter Handle"></a><a href="https://join.slack.com/t/apache-hudi/shared_invite/zt-1e94d3xro-JvlNO1kSeIHJBTVfLPlI5w" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-slack-link" aria-label="Hudi Slack Channel"></a><a href="https://www.youtube.com/channel/UCs7AhE0BWaEPZSChrBR-Muw" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-youtube-link" aria-label="Hudi YouTube Channel"></a><div class="searchBox_Utm0"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper mdx-wrapper mdx-page"><main class="container container--fluid margin-vert--lg"><div class="row mdxPageWrapper_eQvw"><div class="col col--8"><header><h1>Apache Hudi Technical Specification</h1></header><table><thead><tr><th align="left">Syntax</th><th align="center">Description</th></tr></thead><tbody><tr><td align="left">Last Updated</td><td align="center">Aug 2022</td></tr><tr><td align="left"><a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableVersion.java#L29" target="_blank" rel="noopener noreferrer">Table Version</a></td><td align="center">4</td></tr></tbody></table><p>This document is a specification for the Hudi&#x27;s Storage Format which transforms immutable cloud/file storage systems into transactional data lakes. </p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="overview">Overview<a class="hash-link" href="#overview" title="Direct link to heading">​</a></h2><p>Hudi Storage Format enables the following features over very large collections of files/objects</p><ul><li>Stream processing primitives like incremental merges, change stream etc</li><li>Database primitives like tables, transactions, mutability, indexes and query performance optimizations </li></ul><p>Apache Hudi is an open source data lake platform that is built on top of the Hudi Storage Format and it unlocks the following capabilities. </p><ul><li><strong>Unified Computation Model</strong> - a unified way to combine large batch style operations and frequent near real time streaming operations over large datasets.</li><li><strong>Self-Optimized Storage</strong> - automatically handle all the table storage maintenance such as compaction, clustering, vacuuming asynchronously and in most cases non-blocking to actual data changes.</li><li><strong>Cloud Native Database</strong> - abstracts Table/Schema from actual storage and ensures up-to-date metadata and indexes unlocking multi-fold read and write performance optimizations.</li><li><strong>Engine Neutrality</strong> - designed to be neutral and without any assumptions on the preferred computation engine. Apache Hudi will manage metadata, and provide common abstractions and pluggable interfaces to most/all common compute/query engines.</li></ul><p>This document is intended as reference guide for any compute engines, that aim to write/read Hudi tables, by interacting with the storage format directly.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="storage-format">Storage Format<a class="hash-link" href="#storage-format" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_y2LR" id="data-layout">Data Layout<a class="hash-link" href="#data-layout" title="Direct link to heading">​</a></h3><p>At a high level, Hudi organizes data into a directory structure under the base path (root directory for the Hudi table). The directory structure can be flat (non-partitioned) or based on coarse-grained partitioning values set for the table. Non-partitioned tables store all the data files under the base path.
Note that, unlike Hive style partitioning, partition columns are not removed from data files and partitioning is a mere organization of data files. A special reserved <em>.hoodie</em> directory under the base path is used to store transaction logs and metadata.
A special file <code>hoodie.properties</code> persists table level configurations, shared by writers and readers of the table. These configurations are explained <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java" target="_blank" rel="noopener noreferrer">here</a>,
and any config without a default value needs to be specified during table creation.</p><div class="codeBlockContainer_J+bg theme-code-block"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">/data/hudi_trips/                   &lt;== Base Path</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├── .hoodie/                        &lt;== Meta Path</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">|   └── hoodie.properties           &lt;== Table Configs</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   └── metadata/                   &lt;== Table Metadata</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├── americas/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   ├── brazil/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   │   └── sao_paulo/              &lt;== Partition Path </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   │       ├── &lt;data_files&gt;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   └── united_states/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│       └── san_francisco/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│           ├── &lt;data_files&gt;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">└── asia/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    └── india/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        └── chennai/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            ├── &lt;data_files&gt;</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><h3 class="anchor anchorWithStickyNavbar_y2LR" id="table-types">Table Types<a class="hash-link" href="#table-types" title="Direct link to heading">​</a></h3><p>Hudi storage format supports two table types offering different trade-offs between ingest and query performance and the data files are stored differently based on the chosen table type.
Broadly, there can be two types of data files </p><ol><li><strong>Base files</strong> - Files that contain a set of records in columnar file formats like Apache Parquet/Orc or indexed formats like HFile format.</li><li><strong>log files</strong> - Log files contain inserts, updates, deletes issued against a base file, encoded as a series of blocks. More on this <a href="#log-file-format">below</a>.</li></ol><table><thead><tr><th>Table Type</th><th>Trade-off</th></tr></thead><tbody><tr><td>Copy-on-Write (CoW)</td><td>Data is stored entirely in base files, optimized for read performance and ideal for slow changing datasets</td></tr><tr><td>Merge-on-read (MoR)</td><td>Data is stored in a combination of base and log files, optimized to <a href="##balancing-write-and-query-performance">balance the write and read performance</a> and ideal for frequently changing datasets</td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_y2LR" id="data-model">Data Model<a class="hash-link" href="#data-model" title="Direct link to heading">​</a></h3><p>Hudi&#x27;s data model is designed like an update-able database like a key-value store. Within each partition, data is organized into key-value model, where every record is uniquely identified with a record key. </p><h4 class="anchor anchorWithStickyNavbar_y2LR" id="user-fields">User fields<a class="hash-link" href="#user-fields" title="Direct link to heading">​</a></h4><p>To write a record into a Hudi table, each record must specify the following user fields.</p><table><thead><tr><th>User fields</th><th>Description</th></tr></thead><tbody><tr><td>Partitioning key <!-- -->[Optional]</td><td>Value of this field defines the directory hierarchy within the table base path. This essentially provides an hierarchy isolation for managing data and related metadata</td></tr><tr><td>Record key(s)</td><td>Record keys uniquely identify a record within each partition if partitioning is enabled</td></tr><tr><td>Ordering field(s)</td><td>Hudi guarantees the uniqueness constraint of record key and the conflict resolution configuration manages strategies on how to disambiguate when multiple records with the same keys are to be merged into the table. The resolution logic can be based on an ordering field or can be custom, specific to the table. To ensure consistent behaviour dealing with duplicate records, the resolution logic should be commutative, associative and idempotent. This is also referred to as ‘precombine field’.</td></tr></tbody></table><h4 class="anchor anchorWithStickyNavbar_y2LR" id="meta-fields">Meta fields<a class="hash-link" href="#meta-fields" title="Direct link to heading">​</a></h4><p>In addition to the fields specified by the table&#x27;s schema, the following meta fields are added to each record, to unlock incremental processing and ease of debugging. These meta fields are part of the table schema and
stored with the actual record to avoid re-computation. </p><table><thead><tr><th>Hudi meta-fields</th><th>Description</th></tr></thead><tbody><tr><td>_<!-- -->hoodie<!-- -->_<!-- -->commit<!-- -->_<!-- -->time</td><td>This field contains the commit timestamp in the <a href="#transaction-log-timeline">timeline</a> that created this record. This enables granular, record-level history tracking on the table, much like database change-data-capture.</td></tr><tr><td>_<!-- -->hoodie<!-- -->_<!-- -->commit<!-- -->_<!-- -->seqno</td><td>This field contains a unique sequence number for each record within each transaction. This serves much like offsets in Apache Kafka topics, to enable generating streams out of tables.</td></tr><tr><td>_<!-- -->hoodie<!-- -->_<!-- -->record<!-- -->_<!-- -->key</td><td>Unique record key identifying the record within the partition. Key is materialized to avoid changes to key field(s) resulting in violating unique constraints maintained within a table.</td></tr><tr><td>_<!-- -->hoodie<!-- -->_<!-- -->partition<!-- -->_<!-- -->path</td><td>Partition path under which the record is organized into.</td></tr><tr><td>_<!-- -->hoodie<!-- -->_<!-- -->file<!-- -->_<!-- -->name</td><td>The data file name this record belongs to.</td></tr></tbody></table><p>Within a given file, all records share the same values for <code>_hoodie_partition_path</code> and <code>_hoodie_file_name</code>, thus easily compressed away without any overheads with columnar file formats. The other fields can also be optional for writers
depending on whether protection against key field changes or incremental processing is desired. More on how to populate these fields in the sections below.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="transaction-log-timeline">Transaction Log (Timeline)<a class="hash-link" href="#transaction-log-timeline" title="Direct link to heading">​</a></h2><p>Hudi serializes all actions performed on a table into an event log - called the <strong>Timeline</strong>. Every transactional action on the Hudi table creates a new entry (instant) in the timeline.
Data consistency in Hudi is provided using Multi-version Concurrency Control (MVCC) and all transactional actions follow the state transitions below, to move each <a href="#file-layout-hierarchy">file group</a> from one consistent state to another.</p><ul><li><strong>requested</strong> - Action is planned and requested to start on the timeline. </li><li><strong>inflight</strong> - Action has started running and is currently in-flight. Actions are idempotent, and could fail many times in this state.</li><li><strong>completed</strong> - Action has completed running</li></ul><p>All actions and the state transitions are registered with the timeline using an atomic write of a special meta-file inside the  <em>.hoodie</em> directory. The requirement from the underlying storage system is to support an atomic-put and read-after-write consistency.
The meta file naming structure is as follows</p><div class="codeBlockContainer_J+bg theme-code-block"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">[Action timestamp].[Action type].[Action state] </span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p><strong>Action timestamp:</strong>
Monotonically increasing value to denote strict ordering of actions in the timeline. This could be provided by an external token provider or rely on the system epoch time at millisecond granularity.</p><p> <strong>Action type:</strong>
Type of action. The following are the actions on the Hudi timeline.</p><table><thead><tr><th>Action type</th><th>Description</th></tr></thead><tbody><tr><td>commit</td><td>Commit denotes an <strong>atomic write (inserts, updates and deletes)</strong> of records in a table. A commit in Hudi is an atomic way of updating data, metadata and indexes. The guarantee is that all or none the changes within a commit will be visible to the readers</td></tr><tr><td>deltacommit</td><td>Special version of <code>commit</code> which is applicable only on a Merge-on-Read storage engine. The writes are accumulated and batched to improve write performance</td></tr><tr><td>rollback</td><td>Rollback denotes that the changes made by the corresponding commit/delta commit were unsuccessful &amp; hence rolled back, removing any partial files produced during such a write</td></tr><tr><td>savepoint</td><td>Savepoint is a special marker to ensure a particular commit is not automatically cleaned. It helps restore the table to a point on the timeline, in case of disaster/data recovery scenarios</td></tr><tr><td>restore</td><td>Restore denotes that the table was restored to a particular savepoint.</td></tr><tr><td>clean</td><td>Management activity that cleans up versions of data files that no longer will be accessed</td></tr><tr><td>compaction</td><td>Management activity to optimize the storage for query performance. This action applies the batched up updates from <code>deltacommit</code> and re-optimizes data files for query performance</td></tr><tr><td>replacecommit</td><td>Management activity to replace a set of data files atomically with another. It can be used to cluster the data for better query performance. This action is different from a <code>commit</code> in that the table state before and after are logically equivalent</td></tr><tr><td>indexing</td><td>Management activity to update the index with the data. This action does not change data, only updates the index aynchronously to data changes</td></tr></tbody></table><p><strong>Action state:</strong>
Denotes the state transition identifier (requested -<!-- -->&gt;<!-- --> inflight -<!-- -->&gt;<!-- --> completed)</p><p>Meta-files with requested transaction state contain any planning details, If an action requires generating a plan of execution, this is done before requesting and is persisted in the Meta-file. The data is serialized as Json/Avro, and the schema for each of these actions are as follows</p><ul><li><code>replacecommit</code> - <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/avro/HoodieRequestedReplaceMetadata.avsc" target="_blank" rel="noopener noreferrer">HoodieRequestedReplaceMetadata</a></li><li><code>restore</code> - <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/avro/HoodieRestorePlan.avsc" target="_blank" rel="noopener noreferrer">HoodieRestorePlan</a></li><li><code>rollback</code>- <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/avro/HoodieRollbackPlan.avsc" target="_blank" rel="noopener noreferrer">HoodieRollbackPlan</a></li><li><code>clean</code>  - <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/avro/HoodieCleanerPlan.avsc" target="_blank" rel="noopener noreferrer">HoodieCleanerPlan</a></li><li><code>indexing</code> - <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/avro/HoodieIndexPlan.avsc" target="_blank" rel="noopener noreferrer">HoodieIndexPlan</a></li></ul><p>Meta-files with completed transaction state contain details about the transaction completed such as the number of inserts/updates/deletes per file ID, file size, and some extra metadata such as checkpoint and schema for the batch of records written. Similar to the requested action state, the data is serialized as Json/Avro, and the schema as follows</p><ul><li><code>commit</code> - <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/avro/HoodieCommitMetadata.avsc" target="_blank" rel="noopener noreferrer">HoodieCommitMetadata</a></li><li><code>deltacommit</code> -  <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/avro/HoodieCommitMetadata.avsc" target="_blank" rel="noopener noreferrer">HoodieCommitMetadata</a></li><li><code>rollback</code>- <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/avro/HoodieRollbackMetadata.avsc" target="_blank" rel="noopener noreferrer">HoodieRollbackMetadata</a></li><li><code>savepoint</code> - <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/avro/HoodieSavePointMetadata.avsc" target="_blank" rel="noopener noreferrer">HoodieSavepointMetadata</a></li><li><code>restore</code> - <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/avro/HoodieRestoreMetadata.avsc" target="_blank" rel="noopener noreferrer">HoodieRestoreMetadata</a></li><li><code>clean</code>  - <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/avro/HoodieCleanMetadata.avsc" target="_blank" rel="noopener noreferrer">HoodieCleanMetadata</a> </li><li><code>compaction</code> - <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/avro/HoodieCompactionMetadata.avsc" target="_blank" rel="noopener noreferrer">HoodieCompactionMetadata</a></li><li><code>replacecommit</code> - <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/avro/HoodieReplaceCommitMetadata.avsc" target="_blank" rel="noopener noreferrer">HoodieReplaceCommitMetadata</a></li><li><code>indexing</code> - <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/avro/HoodieIndexCommitMetadata.avsc" target="_blank" rel="noopener noreferrer">HoodieIndexCommitMetadata</a></li></ul><p>By reconciling all the actions in the timeline, the state of the Hudi table can be re-created as of any instant of time.  </p><p>​	</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="metadata">Metadata<a class="hash-link" href="#metadata" title="Direct link to heading">​</a></h2><p>Hudi automatically extracts the physical data statistics and stores the metadata along with the data to improve write and query performance. Hudi Metadata is an internally-managed table which organizes the table metadata under the base path <em>.hoodie/metadata.</em> The metadata is in itself a Hudi table, organized with the Hudi merge-on-read storage format. Every record stored in the metadata table is a Hudi record and hence has partitioning key and record key specified. Following are the metadata table partitions</p><ul><li><strong>files</strong> - Partition path to file name index. Key for the Hudi record is the partition path and the actual record is a map of file name to an instance of <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/avro/HoodieMetadata.avsc#L34" target="_blank" rel="noopener noreferrer">HoodieMetadataFileInfo</a>. The files index can be used to do file listing and do filter based pruning of the scanset during query</li><li><strong>bloom<!-- -->_<!-- -->filters</strong> - Bloom filter index to help map a record key to the actual file. The Hudi key is <code>str_concat(hash(partition name), hash(file name))</code> and the actual payload is an instance of <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/avro/HoodieMetadata.avsc#L66" target="_blank" rel="noopener noreferrer">HudiMetadataBloomFilter</a>. Bloom filter is used to accelerate &#x27;presence checks&#x27; validating whether particular record is present in the file, which is used during merging, hash-based joins, point-lookup queries, etc.</li><li><strong>column<!-- -->_<!-- -->stats</strong> - contains statistics of columns for all the records in the table. This enables fine grained file pruning for filters and join conditions in the query. The actual payload is an instance of <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/avro/HoodieMetadata.avsc#L101" target="_blank" rel="noopener noreferrer">HoodieMetadataColumnStats</a>. </li></ul><p>Apache Hudi platform employs HFile format, to store metadata and indexes, to ensure high performance, though different implementations are free to choose their own. </p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="file-layout-hierarchy">File Layout Hierarchy<a class="hash-link" href="#file-layout-hierarchy" title="Direct link to heading">​</a></h2><p>As mentioned in the data model, data is partitioned coarsely through a directory hierarchy based on the partition path configured. Within each partition the data is physically stored as <strong>base and log files</strong> and organized into logical concepts as <strong>File groups and File slices</strong>. These logical concepts will be referred to by the writer / reader requirements. </p><p><strong>File group</strong> - Groups multiple versions of a base file. File group is uniquely identified by a File id. Each version corresponds to the commit&#x27;s timestamp recording updates to records in the file. The base files are stored in open source data formats like  Apache Parquet, Apache ORC, Apache HBase HFile etc.</p><p><strong>File slice</strong> - A File group can further be split into multiple slices. Each file slice within the file-group is uniquely identified by commit&#x27;s timestamp that created it. In case of COW, file-slice is simply just another version of the base-file. In case of MOR it&#x27;s a combination of the base-file along with log-files attached to it. Each log-file corresponds to the delta commit in the timeline. </p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="base-file"><strong>Base file</strong><a class="hash-link" href="#base-file" title="Direct link to heading">​</a></h3><p>The base file name format is:</p><div class="codeBlockContainer_J+bg theme-code-block"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">[File Id]_[File Write Token]_[Transaction timestamp].[File Extension]</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><ul><li><strong>File Id</strong> - Uniquely identify a base file within the table. Multiple versions of the base file share the same file id.</li><li><strong>File Write Token</strong> - Monotonically increasing token for every attempt to write the base file. This should help uniquely identifying the base file when there are failures and retries. Cleaning can remove partial/uncommitted base files if the write token is not the latest in the file group </li><li><strong>Commit timestamp</strong> - Timestamp matching the commit instant in the timeline that created this base file</li><li><strong>File Extension</strong> - base file extension to denote the open source file format such as .parquet, .orc</li></ul><h3 class="anchor anchorWithStickyNavbar_y2LR" id="log-file-format">Log File Format<a class="hash-link" href="#log-file-format" title="Direct link to heading">​</a></h3><p>The log file name format is:</p><div class="codeBlockContainer_J+bg theme-code-block"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">[File Id]_[Base Transaction timestamp].[Log File Extension].[Log File Version]_[File Write Token]</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><ul><li><strong>File Id</strong> - File Id of the base file in the slice</li><li><strong>Base Transaction timestamp</strong> - Commit timestamp on the base file for which the log file is updating the deletes/updates for</li><li><strong>Log File Extension</strong> - Extension defines the format used for the log file (e.g. Hudi proprietary log format)</li><li><strong>Log File Version</strong> - Current version of the log file format</li><li><strong>File Write Token</strong> - Monotonically increasing token for every attempt to write the log file. This should help uniquely identifying the log file when there are failures and retries. Cleaner can clean-up partial log files if the write token is not the latest in the file slice.</li></ul><p>The Log file format structure is a Hudi native format. The actual content bytes are serialized into one of Apache Avro, Apache Parquet or Apache HFile file formats based on configuration and the other metadata in the block is serialized using the Java DataOutputStream (DOS) serializer.</p><p>Hudi Log format specification is as follows. </p><p><img src="/assets/images/hudi_log_format_v2.png" alt="hudi_log_format_v2"></p><table><thead><tr><th>Section</th><th>#<!-- -->Bytes</th><th>Description</th></tr></thead><tbody><tr><td><strong>magic</strong></td><td>6</td><td>6 Characters &#x27;#HUDI#&#x27; stored as a byte array. Sanity check for block corruption to assert start 6 bytes matches the magic byte[].</td></tr><tr><td><strong>LogBlock length</strong></td><td>8</td><td>Length of the block excluding the magic.</td></tr><tr><td><strong>version</strong></td><td>4</td><td>Version of the Log file format, monotonically increasing to support backwards compatibility</td></tr><tr><td><strong>type</strong></td><td>4</td><td>Represents the type of the log block. Id of the type is serialized as an Integer.</td></tr><tr><td><strong>header length</strong></td><td>8</td><td>Length of the header section to follow</td></tr><tr><td><strong>header</strong></td><td>variable</td><td>Map of header metadata entries. The entries are encoded with key as a metadata Id and the value is the String representation of the metadata value.</td></tr><tr><td><strong>content length</strong></td><td>8</td><td>Length of the actual content serialized</td></tr><tr><td><strong>content</strong></td><td>variable</td><td>The content contains the serialized records in one of the supported file formats (Apache Avro, Apache Parquet or Apache HFile)</td></tr><tr><td><strong>footer length</strong></td><td>8</td><td>Length of the footer section to follow</td></tr><tr><td><strong>footer</strong></td><td>variable</td><td>Similar to Header. Map of footer metadata entries. The entries are encoded with key as a metadata Id and the value is the String representation of the metadata value.</td></tr><tr><td><strong>total block length</strong></td><td>8</td><td>Total size of the block including the magic bytes. This is used to determine if a block is corrupt by comparing to the block size in the header. Each log block assumes that the block size will be last data written in a block. Any data if written after is just ignored.</td></tr></tbody></table><p>Metadata key mapping from Integer to actual metadata is as follows</p><ol><li>Instant Time (encoding id: 1)</li><li>Target Instant Time (encoding id: 2)</li><li>Command Block Type (encoding id: 3)</li></ol><h4 class="anchor anchorWithStickyNavbar_y2LR" id="log-file-format-block-types">Log file format block types<a class="hash-link" href="#log-file-format-block-types" title="Direct link to heading">​</a></h4><p>The following are the possible block types used in Hudi Log Format:</p><h5 class="anchor anchorWithStickyNavbar_y2LR" id="command-block-id-1">Command Block (Id: 1)<a class="hash-link" href="#command-block-id-1" title="Direct link to heading">​</a></h5><p>Encodes a command to the log reader. The Command block must be 0 byte content block which only populates the metadata Command Block Type. Only possible values in the current version of the log format is ROLLBACK<!-- -->_<!-- -->PREVIOUS<!-- -->_<!-- -->BLOCK, which lets the reader to undo the previous block written in the log file. This denotes that the previous action that wrote the log block was unsuccessful. </p><h5 class="anchor anchorWithStickyNavbar_y2LR" id="delete-block-id-2">Delete Block (Id: 2)<a class="hash-link" href="#delete-block-id-2" title="Direct link to heading">​</a></h5><p><img src="/assets/images/spec/spec_log_format_delete_block.png" alt="spec_log_format_delete_block"></p><table><thead><tr><th>Section</th><th>#<!-- -->bytes</th><th>Description</th></tr></thead><tbody><tr><td>format version</td><td>4</td><td>version of the log file format</td></tr><tr><td>length</td><td>8</td><td>length of the deleted keys section to follow</td></tr><tr><td>deleted keys</td><td>variable</td><td>Tombstone of the record to encode a delete.  The following 3 fields are serialized using the KryoSerializer.  <strong>Record Key</strong> - Unique record key within the partition to deleted <strong>Partition Path</strong> - Partition path of the record deleted <strong>Ordering Value</strong> - In a particular batch of updates, the delete block is always written after the data (Avro/HFile/Parquet) block. This field would preserve the ordering of deletes and inserts within the same batch.</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_y2LR" id="corrupted-block-id-3">Corrupted Block (Id: 3)<a class="hash-link" href="#corrupted-block-id-3" title="Direct link to heading">​</a></h5><p>This block type is never written to persistent storage. While reading a log block, if the block is corrupted, then the reader gets an instance of the Corrupted Block instead of a Data block. </p><h5 class="anchor anchorWithStickyNavbar_y2LR" id="avro-block-id-4">Avro Block (Id: 4)<a class="hash-link" href="#avro-block-id-4" title="Direct link to heading">​</a></h5><p>Data block serializes the actual records written into the log file</p><p><img src="/assets/images/spec/spec_log_format_avro_block.png" alt="spec_log_format_avro_block"></p><table><thead><tr><th>Section</th><th>#<!-- -->bytes</th><th>Description</th></tr></thead><tbody><tr><td>format version</td><td>4</td><td>version of the log file format</td></tr><tr><td>record count</td><td>4</td><td>total number of records in this block</td></tr><tr><td>record length</td><td>8</td><td>length of the record content to follow</td></tr><tr><td>record content</td><td>variable</td><td>Record represented as an Avro record serialized using BinaryEncoder</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_y2LR" id="hfile-block-id-5">HFile Block (Id: 5)<a class="hash-link" href="#hfile-block-id-5" title="Direct link to heading">​</a></h5><p>The HFile data block serializes the records using the HFile file format. HFile data model is a key value pair and both are encoded as byte arrays. Hudi record key is encoded as Avro string and the Avro record serialized using BinaryEncoder is stored as the value. HFile file format stores the records in sorted order and with index to enable quick point reads and range scans. </p><h5 class="anchor anchorWithStickyNavbar_y2LR" id="parquet-block-id-6">Parquet Block (Id: 6)<a class="hash-link" href="#parquet-block-id-6" title="Direct link to heading">​</a></h5><p>The Parquet Block serializes the records using the Apache Parquet file format. The serialization layout is similar to the Avro block except for the byte array content encoded in columnar Parquet format. This log block type enables efficient columnar scans and better compression. </p><blockquote><p>Different data block types offers different trade-offs and picking the right block is based on the workload requirements and is critical for merge and read performance. </p></blockquote><h2 class="anchor anchorWithStickyNavbar_y2LR" id="reader-expectations">Reader Expectations<a class="hash-link" href="#reader-expectations" title="Direct link to heading">​</a></h2><p>Readers will use snapshot isolation to query a Hudi table at a consistent point in time in the Hudi timeline.  The reader constructs the snapshot state using the following steps</p><ol><li>Pick an instant in the timeline (last successful commit or a specific commit version explicitly queried) and set that the commit time to compute the list of files to read from. </li><li>For the picked commit time, compute all the file slices that belong to that specific commit time. For all the partition paths involved in the query, the file slices that belong to a successful commit before the picked commit should be included. The lookup on the filesystem could be slow and inefficient and can be further optimized by caching in memory or using the files (mapping partition path to filenames) index or with the support of an external timeline serving system.  </li><li>For the MoR table type, ensure the appropriate merging rules are applied to apply the updates queued for the base in the log files.<ol><li>Contents of the log files should be loaded into an effective point lookup data structure (in-memory or persisted)</li><li>Duplicate record keys should be merged based on the ordering field specified. It is important to order the inserts and deletes in the right order to be consistent and idempotent. </li><li>When the base file is scanned, for every record block, the reader has to lookup if there is a newer version of the data available for the record keys in the block and merge them into the record iterator. </li></ol></li><li>In addition, all the &quot;replacecommit&quot; metadata needs to read to filter out flle groups, that have been replaced with newer file groups, by actions like clustering.</li></ol><h2 class="anchor anchorWithStickyNavbar_y2LR" id="writer-expectations">Writer Expectations<a class="hash-link" href="#writer-expectations" title="Direct link to heading">​</a></h2><p>Writer into Hudi will have to ingest new records, updates to existing records or delete records into the table. All transactional actions follow the same state transition as described in the transaction log (timeline) section. Writers will optimistically create new base and log files and will finally transition the action state to completed to register all the modifications to the table atomically. Writer merges the data using the following steps</p><ol><li>Writer will pick a monotonically increasing instant time from the latest state of the Hudi timeline (<strong>action commit time</strong>) and will pick the last successful commit instant (<strong>merge commit time</strong>) to merge the changes to. If the merge succeeds, then action commit time will be the next successful commit in the timeline. </li><li>For all the incoming records, the writer will have to efficiently determine if this is an update or insert. This is done by a process called tagging - which is a batched point lookups of the record key and partition path pairs in the entire table. The efficiency of tagging is critical to the merge performance. This can be optimized with indexes (bloom, global key value based index) and caching. New records will not have a tag. </li><li>Once records are tagged, the writer can apply them onto the specific file slice. <ol><li>For CoW, writer will create a new slice (action commit time) of the base file in the file group</li><li>For MoR, writer will create a new log file with the action commit time on the merge commit time file slice</li></ol></li><li>Deletes are encoded as special form of updates where only the meta fields and the operation is populated. See the delete block type in log format block types. </li><li>Once all the writes into the file system are complete, concurrency control checks happen to ensure there are no overlapping writes and if that succeeds, the commit action is completed in the timeline atomically making the changes merged visible for the next reader.  </li><li>Synchronizing Indexes and metadata needs to be done in the same transaction that commits the modifications to the table. </li></ol><h2 class="anchor anchorWithStickyNavbar_y2LR" id="compatibility">Compatibility<a class="hash-link" href="#compatibility" title="Direct link to heading">​</a></h2><p>Compatibility between different readers and writers is enforced through the <code>hoodie.table.version</code> table config. Hudi storage format evolves in a backwards compatible way for readers, where newer readers
can read older table versions correctly. However older readers may be required to upgrade in order to read higher table versions. Hence, we recommend upgrading readers first, before upgrading writers when
the table version evolves.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="balancing-write-and-query-performance">Balancing write and query performance<a class="hash-link" href="#balancing-write-and-query-performance" title="Direct link to heading">​</a></h2><p>A critical design choice for any table is to pick the right trade-offs in the data freshness and query performance spectrum. Hudi storage format lets the users decide on this trade-off by picking the table type, record merging and file sizing. </p><h4 class="anchor anchorWithStickyNavbar_y2LR" id="table-types-1">Table types<a class="hash-link" href="#table-types-1" title="Direct link to heading">​</a></h4><table><thead><tr><th></th><th>Merge Efficiency</th><th>Query Efficiency</th></tr></thead><tbody><tr><td>Copy on Write (COW)</td><td><strong>Tunable</strong> <br>COW table type creates a new File slice in the file-group for every batch of updates. Write amplification can be quite high when the update is spread across multiple file groups. The cost involved can be high over a time period especially on tables with low data latency requirements.</td><td><strong>Optimal</strong> <br>COW table types create whole readable data files in open source columnar file formats on each merge batch, there is minimal overhead per record in the query engine. Query engines are fairly optimized for accessing files directly in cloud storage.</td></tr><tr><td>Merge on Read (MOR)</td><td><strong>Optimal</strong> <br>MOR table type batches the updates to the file slice in a separate optimized Log file, write amplification is amortized over time when sufficient updates are batched. The merge cost involved will be lower than COW since the churn on the records re-written for every update is much lower.</td><td><strong>Tunable</strong><br>MOR Table type required record level merging during query. Although there are techniques to make this merge as efficient as possible, there is still a record level overhead to apply the updates batched up for the file slice. The merge cost applies on every query until the compaction applies the updates and creates a new file slice.</td></tr></tbody></table><blockquote><p>Interesting observation on the MOR table format is that, by providing a special view of the table which only serves the base files in the file slice (read optimized query of MOR table), query can pick between query efficiency and data freshness dynamically during query time. Compaction frequency determines the data freshness of the read optimized view. With this, the MOR has all the levers required to balance the merge and query performance dynamically. </p></blockquote><h4 class="anchor anchorWithStickyNavbar_y2LR" id="record-merging">Record merging<a class="hash-link" href="#record-merging" title="Direct link to heading">​</a></h4><p>Hudi data model ensures record key uniqueness constraint, to maintain this constraint every single record merged into the table needs to be checked if the same record key already exists in the table. If it does exist, the conflict resolution strategy is applied to create a new merged record to be persisted. This check is done at the file group level and every record merged needs to be tagged to a single file group. By default, record merging is done during the merge which makes it efficient for queries but for certain real-time streaming requirements, it can also be deferred to the query as long as there is a consistent way to mapping the record key to a certain file group using consistent hashing techniques. </p><h4 class="anchor anchorWithStickyNavbar_y2LR" id="file-sizing">File sizing<a class="hash-link" href="#file-sizing" title="Direct link to heading">​</a></h4><p>Sizing the file group is extremely critical to balance the merge and query performance. Larger the file size, the more the write amplification when new file slices are being created. So to balance the merge cost, compaction or merge frequency should be tuned accordingly and this has an impact on the query performance or data freshness. </p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="table-management">Table Management<a class="hash-link" href="#table-management" title="Direct link to heading">​</a></h2><p>All table services can be run synchronous with the Table client that merges modifications to the data or can be run asynchronously to the table client. Asynchronous is default mode in the Apache Hudi platform.  Any client can trigger table management by registering a &#x27;requested&#x27; state action in the Hudi timeline. Process in charge of running the table management tasks asynchronously looks for the presence of this trigger in the timeline. </p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="compaction">Compaction<a class="hash-link" href="#compaction" title="Direct link to heading">​</a></h3><p>Compaction is the process that efficiently updates a file slice (base and log files) for efficient querying. It applies all the batched up updates in the log files and writes a new file slice. The logic to apply the updates to the base file follows the same set of rules listed in the Reader expectations.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="clustering">Clustering<a class="hash-link" href="#clustering" title="Direct link to heading">​</a></h3><p>If the natural ingestion ordering does not match the query patterns, then data skipping does not work efficiently. It is important for query efficiency to be able to skip as much data on filter and join predicates with column level statistics. Clustering columns need to be specified on the Hudi table. The goal of the clustering table service, is to group data often accessed together and consolidate small files to the optimum target file size for the table. </p><ol><li>Identify file groups that are eligible for clustering - this is chosen based on the clustering strategy (file size based, time based etc)</li><li>Identify clustering groups (file groups that should be clustered together) and each group should expect data sizes in multiples of the target file size. </li><li>Persist the clustering plan in the Hudi timeline as a replacecommit, when clustering is requested.</li><li>Clustering execution can then read the individual clustering groups, write back new file groups with target size with base files sorted by the specified clustering columns. </li></ol><h3 class="anchor anchorWithStickyNavbar_y2LR" id="cleaning">Cleaning<a class="hash-link" href="#cleaning" title="Direct link to heading">​</a></h3><p>Cleaning is a process to free up storage space. Apache Hudi maintains a timeline and multiple versions of the files written as file slices. It is important to specify a cleaning protocol which deletes older versions and reclaims the storage space. Cleaner cannot delete versions that are currently in use or will be required in future. Snapshot reconstruction on a commit instant which has been cleaned is not possible. </p><p>For e.g, there are a couple of retention policies supported in Apache Hudi platform</p><ul><li><strong>keep<!-- -->_<!-- -->latest<!-- -->_<!-- -->commits</strong>: This is a temporal cleaning policy that ensures the effect of having look-back into all the changes that happened in the last X commits. </li><li><strong>keep<!-- -->_<!-- -->latest<!-- -->_<!-- -->file<!-- -->_<!-- -->versions</strong>: This policy has the effect of keeping a maximum of N number of file versions irrespective of time. </li></ul><p>Apache Hudi provides snapshot isolation between writers and readers by managing multiple files with MVCC concurrency. These file versions provide history and enable time travel and rollbacks, but it is important to manage how much history you keep to balance your storage costs.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="concurrency-control">Concurrency Control<a class="hash-link" href="#concurrency-control" title="Direct link to heading">​</a></h2><p>Apache Hudi storage format enables transactional consistencies for reads and writes. </p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="multiple-concurrent-readers">Multiple concurrent readers<a class="hash-link" href="#multiple-concurrent-readers" title="Direct link to heading">​</a></h3><p>Hudi storage format supports snapshot isolation for concurrent readers. A reader loads the Hudi timeline and picks the latest commit and constructs the snapshot state as of the picked commit. Two concurrent readers are never in contention even in the presence of concurrent writes happening. </p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="concurrency-control-with-writes">Concurrency control with writes<a class="hash-link" href="#concurrency-control-with-writes" title="Direct link to heading">​</a></h3><p>If there are only inserts to the table, then there will be no conflicts. To better illustrate scenarios with update conflicts, let&#x27;s categorize writers are 2 types. </p><ul><li>a <em>Table write client</em> merges new changes to the table, from external sources or as output another computation. </li><li>a <em>Table service client</em> does table management services like clustering, compaction, and cleaning et which does not logically change the state of the table. </li></ul><p>Let us look at the various write conflict scenarios</p><p><strong>Multiple table write client conflicts</strong></p><p>Conflicts can occur if two or more writers update the same file group and in that case the first transaction to commit succeeds while the rest will need to be aborted and all changes cleaned up. To be able to detect concurrent updates to the same file group, external locking has to be configured. Conflict detection can be optimistic or pessimistic. </p><ul><li><p>Under optimistic locking, the table writer makes all new base and log files and before committing the transaction, a table level lock is acquired and if there is a newer slice (version) on any of the file groups modified by the current transaction, the transaction has conflicts and needs to be retried. This works well for highly concurrent unrelated updates. Bulk changes to the tables may starve in the presence of multiple concurrent smaller updates.</p></li><li><p>Table clients can also hold pessimistic locks on all the file id groups before they write any new data. They will be required to hold on to the file id locks until the transaction commits. This is not a good fit for highly concurrent workloads, as lock contention may be prohibitively high. Optimistic locking works better for these scenarios.</p></li></ul><p>It is also worth noting that, if multiple writers originate from the same JVM client, a simple locking at the client level would serialize the writes and no external locking needs to be configured. </p><blockquote><p>Apache Hudi platform uses optimistic locking and provides a pluggable LockProvider interface and multiple implementations are available out of the box (Apache Zookeeper, DynamoDB, Apache Hive and JVM Level in-process lock).  </p></blockquote><p><strong>Table service client and Table write client conflicts</strong></p><p>Concurrent updates to the same file group between the Table client and Table Service client can be managed with some additional complexity without the need for external locking. The table service client will be creating a new file slice within the file group and the table client will be creating a new log entry on the current file slice in the file group. Since Hudi maintains a strict ordering of operations in the timeline, When reading a file group, Hudi reader can reconcile all the changes to the previous file slice that are not part of the current file slice. </p><p><strong>Multiple Table service client conflicts</strong></p><p>Since Table service client commits are not opaque modifications to the table, concurrency control can be more efficient and intelligent. Concurrent updates to the same file group can be detected early and conflicting table service clients can be aborted.
Planning table service actions need to be serialized by short-lived locks.</p><h4 class="anchor anchorWithStickyNavbar_y2LR" id="optimistic-concurrency-efficiency">Optimistic concurrency efficiency<a class="hash-link" href="#optimistic-concurrency-efficiency" title="Direct link to heading">​</a></h4><p>The efficiency of Optimistic concurrency is inversely proportional to the possibility of a conflict, which in turn depends on the running time and the files overlapping between the concurrent writers. Apache Hudi storage format makes design choices that make it possible to configure the system to have a low possibility of conflict with regular workloads</p><ul><li>All records with the same record key are present in a single file group. In other words, there is a 1-1 mapping between a record key and a file group id, at all times.</li><li>Unit of concurrency is a single file group and this file group size is configurable. If the table needs to be optimized for concurrent updates, the file group size can be smaller than default which could mean lower collision rates. </li><li>Merge-on-read storage engine has the option to store the contents in record oriented file formats which reduces write latencies (often up to 10 times compared to columnar storage) which results in less collision with other concurrent writers</li><li>Merge-on-read storage engine combined with scalable metadata table ensures that the system can handle frequent updates efficiently which means ingest jobs can be frequent and quick, reducing the chance of conflicts </li></ul></div><div class="col col--2"><div class="tableOfContents_vrFS thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#overview" class="table-of-contents__link toc-highlight">Overview</a></li><li><a href="#storage-format" class="table-of-contents__link toc-highlight">Storage Format</a><ul><li><a href="#data-layout" class="table-of-contents__link toc-highlight">Data Layout</a></li><li><a href="#table-types" class="table-of-contents__link toc-highlight">Table Types</a></li><li><a href="#data-model" class="table-of-contents__link toc-highlight">Data Model</a></li></ul></li><li><a href="#transaction-log-timeline" class="table-of-contents__link toc-highlight">Transaction Log (Timeline)</a></li><li><a href="#metadata" class="table-of-contents__link toc-highlight">Metadata</a></li><li><a href="#file-layout-hierarchy" class="table-of-contents__link toc-highlight">File Layout Hierarchy</a><ul><li><a href="#base-file" class="table-of-contents__link toc-highlight"><strong>Base file</strong></a></li><li><a href="#log-file-format" class="table-of-contents__link toc-highlight">Log File Format</a></li></ul></li><li><a href="#reader-expectations" class="table-of-contents__link toc-highlight">Reader Expectations</a></li><li><a href="#writer-expectations" class="table-of-contents__link toc-highlight">Writer Expectations</a></li><li><a href="#compatibility" class="table-of-contents__link toc-highlight">Compatibility</a></li><li><a href="#balancing-write-and-query-performance" class="table-of-contents__link toc-highlight">Balancing write and query performance</a></li><li><a href="#table-management" class="table-of-contents__link toc-highlight">Table Management</a><ul><li><a href="#compaction" class="table-of-contents__link toc-highlight">Compaction</a></li><li><a href="#clustering" class="table-of-contents__link toc-highlight">Clustering</a></li><li><a href="#cleaning" class="table-of-contents__link toc-highlight">Cleaning</a></li></ul></li><li><a href="#concurrency-control" class="table-of-contents__link toc-highlight">Concurrency Control</a><ul><li><a href="#multiple-concurrent-readers" class="table-of-contents__link toc-highlight">Multiple concurrent readers</a></li><li><a href="#concurrency-control-with-writes" class="table-of-contents__link toc-highlight">Concurrency control with writes</a></li></ul></li></ul></div></div></div></main></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">About</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/blog/2021/07/21/streaming-data-lake-platform">Our Vision</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/concepts">Concepts</a></li><li class="footer__item"><a class="footer__link-item" href="/community/team">Team</a></li><li class="footer__item"><a class="footer__link-item" href="/releases/release-0.12.0">Releases</a></li><li class="footer__item"><a class="footer__link-item" href="/releases/download">Download</a></li><li class="footer__item"><a class="footer__link-item" href="/powered-by">Who&#x27;s Using</a></li></ul></div><div class="col footer__col"><div class="footer__title">Learn</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/quick-start-guide">Quick Start</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/docker_demo">Docker Demo</a></li><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a class="footer__link-item" href="/talks">Talks</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/faq">FAQ</a></li><li class="footer__item"><a href="https://cwiki.apache.org/confluence/display/HUDI" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Technical Wiki<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">Hudi On Cloud</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/s3_hoodie">AWS</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/gcs_hoodie">Google Cloud</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/oss_hoodie">Alibaba Cloud</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/azure_hoodie">Microsoft Azure</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/cos_hoodie">Tencent Cloud</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/ibm_cos_hoodie">IBM Cloud</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/contribute/get-involved">Get Involved</a></li><li class="footer__item"><a href="https://join.slack.com/t/apache-hudi/shared_invite/zt-1e94d3xro-JvlNO1kSeIHJBTVfLPlI5w" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Slack<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://github.com/apache/hudi" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://twitter.com/ApacheHudi" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://www.youtube.com/channel/UCs7AhE0BWaEPZSChrBR-Muw" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>YouTube<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="mailto:dev-subscribe@hudi.apache.org?Subject=SubscribeToHudi" target="_blank" rel="noopener noreferrer" class="footer__link-item">Mailing List</a></li></ul></div><div class="col footer__col"><div class="footer__title">Apache</div><ul class="footer__items"><li class="footer__item"><a href="https://www.apache.org/events/current-event" target="_blank" rel="noopener noreferrer" class="footer__link-item">Events</a></li><li class="footer__item"><a href="https://www.apache.org/foundation/thanks.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Thanks</a></li><li class="footer__item"><a href="https://www.apache.org/licenses" target="_blank" rel="noopener noreferrer" class="footer__link-item">License</a></li><li class="footer__item"><a href="https://www.apache.org/security" target="_blank" rel="noopener noreferrer" class="footer__link-item">Security</a></li><li class="footer__item"><a href="https://www.apache.org/foundation/sponsorship.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Sponsorship</a></li><li class="footer__item"><a href="https://www.apache.org" target="_blank" rel="noopener noreferrer" class="footer__link-item">Foundation</a></li></ul></div></div><div class="footer__bottom text--center"><div class="margin-bottom--sm"><a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer" class="footerLogoLink_SRtH"><img src="/assets/images/logo-big.png" alt="Apache Hudi™" class="themedImage_TMUO themedImage--light_4Vu1 footer__logo"><img src="/assets/images/logo-big.png" alt="Apache Hudi™" class="themedImage_TMUO themedImage--dark_uzRr footer__logo"></a></div><div class="footer__copyright">Copyright © 2021 <a href="https://apache.org">The Apache Software Foundation</a>, Licensed under the <a href="https://www.apache.org/licenses/LICENSE-2.0"> Apache License, Version 2.0</a>. <br>Hudi, Apache and the Apache feather logo are trademarks of The Apache Software Foundation.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.1da50198.js"></script>
<script src="/assets/js/main.31205107.js"></script>
</body>
</html>