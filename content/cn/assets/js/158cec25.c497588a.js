"use strict";(globalThis.webpackChunkhudi=globalThis.webpackChunkhudi||[]).push([[73473],{28453:(e,o,n)=>{n.d(o,{R:()=>a,x:()=>r});var s=n(96540);const t={},i=s.createContext(t);function a(e){const o=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(o):{...o,...e}},[o,e])}function r(e){let o;return o=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(i.Provider,{value:o},e.children)}},40936:(e,o,n)=>{n.r(o),n.d(o,{assets:()=>c,contentTitle:()=>r,default:()=>m,frontMatter:()=>a,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"docker_demo","title":"Docker Demo","description":"\u4e00\u4e2a\u4f7f\u7528 Docker \u5bb9\u5668\u7684 Demo","source":"@site/i18n/cn/docusaurus-plugin-content-docs/current/docker_demo.md","sourceDirName":".","slug":"/docker_demo","permalink":"/cn/docs/next/docker_demo","draft":false,"unlisted":false,"editUrl":"https://github.com/apache/hudi/tree/asf-site/website/docs/docker_demo.md","tags":[],"version":"current","frontMatter":{"title":"Docker Demo","keywords":["hudi","docker","demo"],"toc":true,"last_modified_at":"2019-12-30T19:59:57.000Z","language":"cn"},"sidebar":"docs","previous":{"title":"Python/Rust Quick Start","permalink":"/cn/docs/next/python-rust-quick-start-guide"},"next":{"title":"Notebooks","permalink":"/cn/docs/next/notebooks"}}');var t=n(74848),i=n(28453);const a={title:"Docker Demo",keywords:["hudi","docker","demo"],toc:!0,last_modified_at:new Date("2019-12-30T19:59:57.000Z"),language:"cn"},r=void 0,c={},l=[{value:"\u4e00\u4e2a\u4f7f\u7528 Docker \u5bb9\u5668\u7684 Demo",id:"\u4e00\u4e2a\u4f7f\u7528-docker-\u5bb9\u5668\u7684-demo",level:2},{value:"\u524d\u63d0\u6761\u4ef6",id:"\u524d\u63d0\u6761\u4ef6",level:3},{value:"\u8bbe\u7f6e Docker \u96c6\u7fa4",id:"\u8bbe\u7f6e-docker-\u96c6\u7fa4",level:2},{value:"\u6784\u5efa Hudi",id:"\u6784\u5efa-hudi",level:3},{value:"\u7ec4\u5efa Demo \u96c6\u7fa4",id:"\u7ec4\u5efa-demo-\u96c6\u7fa4",level:3},{value:"Demo",id:"demo",level:2},{value:"Step 1 : \u5c06\u7b2c 1 \u6279\u6570\u636e\u53d1\u5e03\u5230 Kafka",id:"step-1--\u5c06\u7b2c-1-\u6279\u6570\u636e\u53d1\u5e03\u5230-kafka",level:3},{value:"Step 2: \u4ece Kafka Topic \u4e2d\u589e\u91cf\u91c7\u96c6\u6570\u636e",id:"step-2-\u4ece-kafka-topic-\u4e2d\u589e\u91cf\u91c7\u96c6\u6570\u636e",level:3},{value:"Step 3: \u4e0e Hive \u540c\u6b65",id:"step-3-\u4e0e-hive-\u540c\u6b65",level:3},{value:"Step 4 (a): \u8fd0\u884c Hive \u67e5\u8be2",id:"step-4-a-\u8fd0\u884c-hive-\u67e5\u8be2",level:3},{value:"Step 4 (b): \u6267\u884c Spark-SQL \u67e5\u8be2",id:"step-4-b-\u6267\u884c-spark-sql-\u67e5\u8be2",level:3},{value:"Step 4 (c): \u6267\u884c Presto \u67e5\u8be2",id:"step-4-c-\u6267\u884c-presto-\u67e5\u8be2",level:3},{value:"Step 5: \u5c06\u7b2c 2 \u6279\u6b21\u4e0a\u4f20\u5230 Kafka \u5e76\u8fd0\u884c DeltaStreamer \u8fdb\u884c\u91c7\u96c6",id:"step-5-\u5c06\u7b2c-2-\u6279\u6b21\u4e0a\u4f20\u5230-kafka-\u5e76\u8fd0\u884c-deltastreamer-\u8fdb\u884c\u91c7\u96c6",level:3},{value:"Step 6 (a): \u6267\u884c Hive \u67e5\u8be2",id:"step-6-a-\u6267\u884c-hive-\u67e5\u8be2",level:3},{value:"Step 6 (b): \u6267\u884c Spark SQL \u67e5\u8be2",id:"step-6-b-\u6267\u884c-spark-sql-\u67e5\u8be2",level:3},{value:"Step 6 (c): \u6267\u884c Presto \u67e5\u8be2",id:"step-6-c-\u6267\u884c-presto-\u67e5\u8be2",level:3},{value:"Step 7 : \u5199\u65f6\u590d\u5236\u8868\u7684\u589e\u91cf\u67e5\u8be2",id:"step-7--\u5199\u65f6\u590d\u5236\u8868\u7684\u589e\u91cf\u67e5\u8be2",level:3},{value:"\u4f7f\u7528 Spark SQL \u505a\u589e\u91cf\u67e5\u8be2",id:"\u4f7f\u7528-spark-sql-\u505a\u589e\u91cf\u67e5\u8be2",level:3},{value:"Step 8: \u4e3a\u8bfb\u65f6\u5408\u5e76\u6570\u636e\u96c6\u7684\u8c03\u5ea6\u5e76\u6267\u884c\u538b\u7f29",id:"step-8-\u4e3a\u8bfb\u65f6\u5408\u5e76\u6570\u636e\u96c6\u7684\u8c03\u5ea6\u5e76\u6267\u884c\u538b\u7f29",level:3},{value:"Step 9: \u6267\u884c\u5305\u542b\u589e\u91cf\u67e5\u8be2\u7684 Hive \u67e5\u8be2",id:"step-9-\u6267\u884c\u5305\u542b\u589e\u91cf\u67e5\u8be2\u7684-hive-\u67e5\u8be2",level:3},{value:"Step 10: \u538b\u7f29\u540e\u5728 MOR \u7684\u8bfb\u4f18\u5316\u89c6\u56fe\u4e0e\u5b9e\u65f6\u89c6\u56fe\u4e0a\u4f7f\u7528 Spark-SQL",id:"step-10-\u538b\u7f29\u540e\u5728-mor-\u7684\u8bfb\u4f18\u5316\u89c6\u56fe\u4e0e\u5b9e\u65f6\u89c6\u56fe\u4e0a\u4f7f\u7528-spark-sql",level:3},{value:"Step 11:  \u538b\u7f29\u540e\u5728 MOR \u6570\u636e\u96c6\u7684\u8bfb\u4f18\u5316\u89c6\u56fe\u4e0a\u8fdb\u884c Presto \u67e5\u8be2",id:"step-11--\u538b\u7f29\u540e\u5728-mor-\u6570\u636e\u96c6\u7684\u8bfb\u4f18\u5316\u89c6\u56fe\u4e0a\u8fdb\u884c-presto-\u67e5\u8be2",level:3},{value:"\u5728\u672c\u5730 Docker \u73af\u5883\u4e2d\u6d4b\u8bd5 Hudi",id:"\u5728\u672c\u5730-docker-\u73af\u5883\u4e2d\u6d4b\u8bd5-hudi",level:2},{value:"\u6784\u5efa\u672c\u5730 Docker \u5bb9\u5668:",id:"\u6784\u5efa\u672c\u5730-docker-\u5bb9\u5668",level:3}];function d(e){const o={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(o.h2,{id:"\u4e00\u4e2a\u4f7f\u7528-docker-\u5bb9\u5668\u7684-demo",children:"\u4e00\u4e2a\u4f7f\u7528 Docker \u5bb9\u5668\u7684 Demo"}),"\n",(0,t.jsx)(o.p,{children:"\u6211\u4eec\u6765\u4f7f\u7528\u4e00\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u6848\u4f8b\uff0c\u6765\u770b\u770b Hudi \u662f\u5982\u4f55\u95ed\u73af\u8fd0\u8f6c\u7684\u3002 \u4e3a\u4e86\u8fd9\u4e2a\u76ee\u7684\uff0c\u5728\u4f60\u7684\u8ba1\u7b97\u673a\u4e2d\u7684\u672c\u5730 Docker \u96c6\u7fa4\u4e2d\u7ec4\u5efa\u4e86\u4e00\u4e2a\u81ea\u5305\u542b\u7684\u6570\u636e\u57fa\u7840\u8bbe\u65bd\u3002"}),"\n",(0,t.jsx)(o.p,{children:"\u4ee5\u4e0b\u6b65\u9aa4\u5df2\u7ecf\u5728\u4e00\u53f0 Mac \u7b14\u8bb0\u672c\u7535\u8111\u4e0a\u6d4b\u8bd5\u8fc7\u4e86\u3002"}),"\n",(0,t.jsx)(o.h3,{id:"\u524d\u63d0\u6761\u4ef6",children:"\u524d\u63d0\u6761\u4ef6"}),"\n",(0,t.jsxs)(o.ul,{children:["\n",(0,t.jsxs)(o.li,{children:["Docker \u5b89\u88c5 :  \u5bf9\u4e8e Mac \uff0c\u8bf7\u4f9d\u7167 [",(0,t.jsx)(o.a,{href:"https://docs.docker.com/v17.12/docker-for-mac/install/",children:"https://docs.docker.com/v17.12/docker-for-mac/install/"}),"]"," \u5f53\u4e2d\u5b9a\u4e49\u7684\u6b65\u9aa4\u3002 \u4e3a\u4e86\u8fd0\u884c Spark-SQL \u67e5\u8be2\uff0c\u8bf7\u786e\u4fdd\u81f3\u5c11\u5206\u914d\u7ed9 Docker 6 GB \u548c 4 \u4e2a CPU \u3002\uff08\u53c2\u89c1 Docker -> Preferences -> Advanced\uff09\u3002\u5426\u5219\uff0cSpark-SQL \u67e5\u8be2\u53ef\u80fd\u88ab\u56e0\u4e3a\u5185\u5b58\u95ee\u9898\u800c\u88ab\u6740\u505c\u3002"]}),"\n",(0,t.jsxs)(o.li,{children:["kafkacat : \u4e00\u4e2a\u7528\u4e8e\u53d1\u5e03/\u6d88\u8d39 Kafka Topic \u7684\u547d\u4ee4\u884c\u5de5\u5177\u96c6\u3002\u4f7f\u7528 ",(0,t.jsx)(o.code,{children:"brew install kafkacat"})," \u6765\u5b89\u88c5 kafkacat \u3002"]}),"\n",(0,t.jsx)(o.li,{children:"/etc/hosts : Demo \u901a\u8fc7\u4e3b\u673a\u540d\u5f15\u7528\u4e86\u591a\u4e2a\u8fd0\u884c\u5728\u5bb9\u5668\u4e2d\u7684\u670d\u52a1\u3002\u5c06\u4e0b\u5217\u8bbe\u7f6e\u6dfb\u52a0\u5230 /etc/hosts \uff1a"}),"\n"]}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-java",children:"   127.0.0.1 adhoc-1\n   127.0.0.1 adhoc-2\n   127.0.0.1 namenode\n   127.0.0.1 datanode1\n   127.0.0.1 hiveserver\n   127.0.0.1 hivemetastore\n   127.0.0.1 kafkabroker\n   127.0.0.1 sparkmaster\n   127.0.0.1 zookeeper\n"})}),"\n",(0,t.jsx)(o.p,{children:"\u6b64\u5916\uff0c\u8fd9\u672a\u5728\u5176\u5b83\u4e00\u4e9b\u73af\u5883\u4e2d\u8fdb\u884c\u6d4b\u8bd5\uff0c\u4f8b\u5982 Windows \u4e0a\u7684 Docker \u3002"}),"\n",(0,t.jsx)(o.h2,{id:"\u8bbe\u7f6e-docker-\u96c6\u7fa4",children:"\u8bbe\u7f6e Docker \u96c6\u7fa4"}),"\n",(0,t.jsx)(o.h3,{id:"\u6784\u5efa-hudi",children:"\u6784\u5efa Hudi"}),"\n",(0,t.jsx)(o.p,{children:"\u6784\u5efa Hudi \u7684\u7b2c\u4e00\u6b65\uff1a"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-java",children:"cd <HUDI_WORKSPACE>\nmvn package -DskipTests\n"})}),"\n",(0,t.jsx)(o.h3,{id:"\u7ec4\u5efa-demo-\u96c6\u7fa4",children:"\u7ec4\u5efa Demo \u96c6\u7fa4"}),"\n",(0,t.jsx)(o.p,{children:"\u4e0b\u4e00\u6b65\u662f\u8fd0\u884c Docker \u5b89\u88c5\u811a\u672c\u5e76\u8bbe\u7f6e\u914d\u7f6e\u9879\u4ee5\u4fbf\u7ec4\u5efa\u96c6\u7fa4\u3002\n\u8fd9\u9700\u8981\u4ece Docker \u955c\u50cf\u5e93\u62c9\u53d6 Docker \u955c\u50cf\uff0c\u5e76\u8bbe\u7f6e Docker \u96c6\u7fa4\u3002"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-java",children:'cd docker\n./setup_demo.sh\n....\n....\n....\nStopping spark-worker-1            ... done\nStopping hiveserver                ... done\nStopping hivemetastore             ... done\nStopping historyserver             ... done\n.......\n......\nCreating network "hudi_demo" with the default driver\nCreating hive-metastore-postgresql ... done\nCreating namenode                  ... done\nCreating zookeeper                 ... done\nCreating kafkabroker               ... done\nCreating hivemetastore             ... done\nCreating historyserver             ... done\nCreating hiveserver                ... done\nCreating datanode1                 ... done\nCreating presto-coordinator-1      ... done\nCreating sparkmaster               ... done\nCreating presto-worker-1           ... done\nCreating adhoc-1                   ... done\nCreating adhoc-2                   ... done\nCreating spark-worker-1            ... done\nCopying spark default config and setting up configs\nCopying spark default config and setting up configs\nCopying spark default config and setting up configs\n$ docker ps\n'})}),"\n",(0,t.jsx)(o.p,{children:"\u81f3\u6b64\uff0c Docker \u96c6\u7fa4\u5c06\u4f1a\u542f\u52a8\u5e76\u8fd0\u884c\u3002 Demo \u96c6\u7fa4\u63d0\u4f9b\u4e86\u4e0b\u5217\u670d\u52a1\uff1a"}),"\n",(0,t.jsxs)(o.ul,{children:["\n",(0,t.jsx)(o.li,{children:"HDFS \u670d\u52a1\uff08 NameNode, DataNode \uff09"}),"\n",(0,t.jsx)(o.li,{children:"Spark Master \u548c Worker"}),"\n",(0,t.jsx)(o.li,{children:"Hive \u670d\u52a1\uff08 Metastore, HiveServer2 \u4ee5\u53ca PostgresDB \uff09"}),"\n",(0,t.jsx)(o.li,{children:"Kafka Broker \u548c\u4e00\u4e2a Zookeeper Node \uff08 Kafka \u5c06\u88ab\u7528\u6765\u5f53\u505a Demo \u7684\u4e0a\u6e38\u6570\u636e\u6e90 \uff09"}),"\n",(0,t.jsx)(o.li,{children:"\u7528\u6765\u8fd0\u884c Hudi/Hive CLI \u547d\u4ee4\u7684 Adhoc \u5bb9\u5668"}),"\n"]}),"\n",(0,t.jsx)(o.h2,{id:"demo",children:"Demo"}),"\n",(0,t.jsx)(o.p,{children:"Stock Tracker \u6570\u636e\u5c06\u7528\u6765\u5c55\u793a\u4e0d\u540c\u7684 Hudi \u89c6\u56fe\u4ee5\u53ca\u538b\u7f29\u5e26\u6765\u7684\u5f71\u54cd\u3002"}),"\n",(0,t.jsxs)(o.p,{children:["\u770b\u4e00\u4e0b ",(0,t.jsx)(o.code,{children:"docker/demo/data"})," \u76ee\u5f55\u3002\u90a3\u91cc\u6709 2 \u6279\u80a1\u7968\u6570\u636e\u2014\u2014\u90fd\u662f 1 \u5206\u949f\u7c92\u5ea6\u7684\u3002\n\u7b2c 1 \u6279\u6570\u636e\u5305\u542b\u4e00\u4e9b\u80a1\u7968\u4ee3\u7801\u5728\u4ea4\u6613\u7a97\u53e3\uff089:30 a.m \u81f3 10:30 a.m\uff09\u7684\u7b2c\u4e00\u4e2a\u5c0f\u65f6\u91cc\u7684\u884c\u60c5\u6570\u636e\u6570\u636e\u3002\u7b2c 2 \u6279\u5305\u542b\u63a5\u4e0b\u6765 30 \u5206\u949f\uff0810:30 - 11 a.m\uff09\u7684\u4ea4\u6613\u6570\u636e\u3002 Hudi \u5c06\u88ab\u7528\u6765\u5c06\u4e24\u4e2a\u6279\u6b21\u7684\u6570\u636e\u91c7\u96c6\u5230\u4e00\u4e2a\u6570\u636e\u96c6\u4e2d\uff0c\u8fd9\u4e2a\u6570\u636e\u96c6\u5c06\u4f1a\u5305\u542b\u6700\u65b0\u7684\u5c0f\u65f6\u7ea7\u80a1\u7968\u884c\u60c5\u6570\u636e\u3002\n\u4e24\u4e2a\u6279\u6b21\u88ab\u6709\u610f\u5730\u6309\u7a97\u53e3\u5207\u5206\uff0c\u8fd9\u6837\u5728\u7b2c 2 \u6279\u6570\u636e\u4e2d\u5305\u542b\u4e86\u4e00\u4e9b\u9488\u5bf9\u7b2c 1 \u6279\u6570\u636e\u6761\u76ee\u7684\u66f4\u65b0\u6570\u636e\u3002"]}),"\n",(0,t.jsx)(o.h3,{id:"step-1--\u5c06\u7b2c-1-\u6279\u6570\u636e\u53d1\u5e03\u5230-kafka",children:"Step 1 : \u5c06\u7b2c 1 \u6279\u6570\u636e\u53d1\u5e03\u5230 Kafka"}),"\n",(0,t.jsxs)(o.p,{children:["\u5c06\u7b2c 1 \u6279\u6570\u636e\u4e0a\u4f20\u5230 Kafka \u7684 Topic \u201cstock ticks\u201d \u4e2d ",(0,t.jsx)(o.code,{children:"cat docker/demo/data/batch_1.json | kafkacat -b kafkabroker -t stock_ticks -P"})]}),"\n",(0,t.jsx)(o.p,{children:"\u4e3a\u4e86\u68c0\u67e5\u65b0\u7684 Topic \u662f\u5426\u51fa\u73b0\uff0c\u4f7f\u7528"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-java",children:'kafkacat -b kafkabroker -L -J | jq .\n{\n  "originating_broker": {\n    "id": 1001,\n    "name": "kafkabroker:9092/1001"\n  },\n  "query": {\n    "topic": "*"\n  },\n  "brokers": [\n    {\n      "id": 1001,\n      "name": "kafkabroker:9092"\n    }\n  ],\n  "topics": [\n    {\n      "topic": "stock_ticks",\n      "partitions": [\n        {\n          "partition": 0,\n          "leader": 1001,\n          "replicas": [\n            {\n              "id": 1001\n            }\n          ],\n          "isrs": [\n            {\n              "id": 1001\n            }\n          ]\n        }\n      ]\n    }\n  ]\n}\n\n'})}),"\n",(0,t.jsx)(o.h3,{id:"step-2-\u4ece-kafka-topic-\u4e2d\u589e\u91cf\u91c7\u96c6\u6570\u636e",children:"Step 2: \u4ece Kafka Topic \u4e2d\u589e\u91cf\u91c7\u96c6\u6570\u636e"}),"\n",(0,t.jsx)(o.p,{children:"Hudi \u81ea\u5e26\u4e00\u4e2a\u540d\u4e3a DeltaStreamer \u7684\u5de5\u5177\u3002 \u8fd9\u4e2a\u5de5\u5177\u80fd\u8fde\u63a5\u591a\u79cd\u6570\u636e\u6e90\uff08\u5305\u62ec Kafka\uff09\uff0c\u4ee5\u4fbf\u62c9\u53d6\u53d8\u66f4\uff0c\u5e76\u901a\u8fc7 upsert/insert \u64cd\u4f5c\u5e94\u7528\u5230 Hudi \u6570\u636e\u96c6\u3002\u6b64\u5904\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u8fd9\u4e2a\u5de5\u5177\u4ece Kafka Topic \u4e0b\u8f7d JSON \u6570\u636e\uff0c\u5e76\u91c7\u96c6\u5230\u524d\u9762\u6b65\u9aa4\u4e2d\u521d\u59cb\u5316\u7684 COW \u548c MOR \u8868\u4e2d\u3002\u5982\u679c\u6570\u636e\u96c6\u4e0d\u5b58\u5728\uff0c\u8fd9\u4e2a\u5de5\u5177\u5c06\u81ea\u52a8\u521d\u59cb\u5316\u6570\u636e\u96c6\u5230\u6587\u4ef6\u7cfb\u7edf\u4e2d\u3002"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-java",children:"docker exec -it adhoc-2 /bin/bash\n\n# Run the following spark-submit command to execute the delta-streamer and ingest to stock_ticks_cow dataset in HDFS\nspark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer $HUDI_UTILITIES_BUNDLE --storage-type COPY_ON_WRITE --source-class org.apache.hudi.utilities.sources.JsonKafkaSource --source-ordering-field ts  --target-base-path /user/hive/warehouse/stock_ticks_cow --target-table stock_ticks_cow --props /var/demo/config/kafka-source.properties --schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider\n\n\n# Run the following spark-submit command to execute the delta-streamer and ingest to stock_ticks_mor dataset in HDFS\nspark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer $HUDI_UTILITIES_BUNDLE --storage-type MERGE_ON_READ --source-class org.apache.hudi.utilities.sources.JsonKafkaSource --source-ordering-field ts  --target-base-path /user/hive/warehouse/stock_ticks_mor --target-table stock_ticks_mor --props /var/demo/config/kafka-source.properties --schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider --disable-compaction\n\n\n# As part of the setup (Look at setup_demo.sh), the configs needed for DeltaStreamer is uploaded to HDFS. The configs\n# contain mostly Kafa connectivity settings, the avro-schema to be used for ingesting along with key and partitioning fields.\n\nexit\n"})}),"\n",(0,t.jsxs)(o.p,{children:["\u4f60\u53ef\u4ee5\u4f7f\u7528 HDFS \u7684 Web \u6d4f\u89c8\u5668\u6765\u67e5\u770b\u6570\u636e\u96c6\n",(0,t.jsx)(o.code,{children:"http://namenode:50070/explorer#/user/hive/warehouse/stock_ticks_cow"}),"."]}),"\n",(0,t.jsx)(o.p,{children:"\u4f60\u53ef\u4ee5\u6d4f\u89c8\u5728\u6570\u636e\u96c6\u4e2d\u65b0\u521b\u5efa\u7684\u5206\u533a\u6587\u4ef6\u5939\uff0c\u540c\u65f6\u8fd8\u6709\u4e00\u4e2a\u5728 .hoodie \u76ee\u5f55\u4e0b\u7684 deltacommit \u6587\u4ef6\u3002"}),"\n",(0,t.jsxs)(o.p,{children:["\u5728 MOR \u6570\u636e\u96c6\u4e2d\u4e5f\u6709\u7c7b\u4f3c\u7684\u8bbe\u7f6e\n",(0,t.jsx)(o.code,{children:"http://namenode:50070/explorer#/user/hive/warehouse/stock_ticks_mor"})]}),"\n",(0,t.jsx)(o.h3,{id:"step-3-\u4e0e-hive-\u540c\u6b65",children:"Step 3: \u4e0e Hive \u540c\u6b65"}),"\n",(0,t.jsx)(o.p,{children:"\u5230\u4e86\u8fd9\u4e00\u6b65\uff0c\u6570\u636e\u96c6\u5728 HDFS \u4e2d\u53ef\u7528\u3002\u6211\u4eec\u9700\u8981\u4e0e Hive \u540c\u6b65\u6765\u521b\u5efa\u65b0 Hive \u8868\u5e76\u6dfb\u52a0\u5206\u533a\uff0c\u4ee5\u4fbf\u5728\u90a3\u4e9b\u6570\u636e\u96c6\u4e0a\u6267\u884c Hive \u67e5\u8be2\u3002"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-java",children:"docker exec -it adhoc-2 /bin/bash\n\n# THis command takes in HIveServer URL and COW Hudi Dataset location in HDFS and sync the HDFS state to Hive\n/var/hoodie/ws/hudi-sync/hudi-hive-sync/run_sync_tool.sh  --jdbc-url jdbc:hive2://hiveserver:10000 --user hive --pass hive --partitioned-by dt --base-path /user/hive/warehouse/stock_ticks_cow --database default --table stock_ticks_cow\n.....\n2018-09-24 22:22:45,568 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(112)) - Sync complete for stock_ticks_cow\n.....\n\n# Now run hive-sync for the second data-set in HDFS using Merge-On-Read (MOR storage)\n/var/hoodie/ws/hudi-sync/hudi-hive-sync/run_sync_tool.sh  --jdbc-url jdbc:hive2://hiveserver:10000 --user hive --pass hive --partitioned-by dt --base-path /user/hive/warehouse/stock_ticks_mor --database default --table stock_ticks_mor\n...\n2018-09-24 22:23:09,171 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(112)) - Sync complete for stock_ticks_mor\n...\n2018-09-24 22:23:09,559 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(112)) - Sync complete for stock_ticks_mor_rt\n....\nexit\n"})}),"\n",(0,t.jsx)(o.p,{children:"\u6267\u884c\u4e86\u4ee5\u4e0a\u547d\u4ee4\u540e\uff0c\u4f60\u4f1a\u53d1\u73b0\uff1a"}),"\n",(0,t.jsxs)(o.ol,{children:["\n",(0,t.jsxs)(o.li,{children:["\u4e00\u4e2a\u540d\u4e3a ",(0,t.jsx)(o.code,{children:"stock_ticks_cow"})," \u7684 Hive \u8868\u88ab\u521b\u5efa\uff0c\u5b83\u4e3a\u5199\u65f6\u590d\u5236\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u8bfb\u4f18\u5316\u89c6\u56fe\u3002"]}),"\n",(0,t.jsxs)(o.li,{children:["\u4e24\u4e2a\u65b0\u8868 ",(0,t.jsx)(o.code,{children:"stock_ticks_mor"})," \u548c ",(0,t.jsx)(o.code,{children:"stock_ticks_mor_rt"})," \u88ab\u521b\u5efa\u7528\u4e8e\u8bfb\u65f6\u5408\u5e76\u6570\u636e\u96c6\u3002 \u524d\u8005\u4e3a Hudi \u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u8bfb\u4f18\u5316\u89c6\u56fe\uff0c\u800c\u540e\u8005\u4e3a\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u5b9e\u65f6\u89c6\u56fe\u3002"]}),"\n"]}),"\n",(0,t.jsx)(o.h3,{id:"step-4-a-\u8fd0\u884c-hive-\u67e5\u8be2",children:"Step 4 (a): \u8fd0\u884c Hive \u67e5\u8be2"}),"\n",(0,t.jsx)(o.p,{children:"\u6267\u884c\u4e00\u4e2a Hive \u67e5\u8be2\u6765\u4e3a\u80a1\u7968 GOOG \u627e\u5230\u91c7\u96c6\u5230\u7684\u6700\u65b0\u65f6\u95f4\u6233\u3002\u4f60\u4f1a\u6ce8\u610f\u5230\u8bfb\u4f18\u5316\u89c6\u56fe\uff08 COW \u548c MOR \u6570\u636e\u96c6\u90fd\u662f\u5982\u6b64\uff09\u548c\u5b9e\u65f6\u89c6\u56fe\uff08\u4ec5\u5bf9 MOR \u6570\u636e\u96c6\uff09\u7ed9\u51fa\u4e86\u76f8\u540c\u7684\u503c \u201c10:29 a.m\u201d\uff0c\u8fd9\u662f\u56e0\u4e3a Hudi \u4e3a\u6bcf\u4e2a\u6279\u6b21\u7684\u6570\u636e\u521b\u5efa\u4e86\u4e00\u4e2a Parquet \u6587\u4ef6\u3002"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-java",children:"docker exec -it adhoc-2 /bin/bash\nbeeline -u jdbc:hive2://hiveserver:10000 --hiveconf hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat --hiveconf hive.stats.autogather=false\n# List Tables\n0: jdbc:hive2://hiveserver:10000> show tables;\n+---------------------+--+\n|      tab_name       |\n+---------------------+--+\n| stock_ticks_cow     |\n| stock_ticks_mor     |\n| stock_ticks_mor_rt  |\n+---------------------+--+\n2 rows selected (0.801 seconds)\n0: jdbc:hive2://hiveserver:10000>\n\n\n# Look at partitions that were added\n0: jdbc:hive2://hiveserver:10000> show partitions stock_ticks_mor_rt;\n+----------------+--+\n|   partition    |\n+----------------+--+\n| dt=2018-08-31  |\n+----------------+--+\n1 row selected (0.24 seconds)\n\n\n# COPY-ON-WRITE Queries:\n=========================\n\n\n0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_cow group by symbol HAVING symbol = 'GOOG';\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:29:00  |\n+---------+----------------------+--+\n\nNow, run a projection query:\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_cow where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924221953       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924221953       | GOOG    | 2018-08-31 10:29:00  | 3391    | 1230.1899  | 1230.085  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\n\n# Merge-On-Read Queries:\n==========================\n\nLets run similar queries against M-O-R dataset. Lets look at both\nReadOptimized and Realtime views supported by M-O-R dataset\n\n# Run against ReadOptimized View. Notice that the latest timestamp is 10:29\n0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_mor group by symbol HAVING symbol = 'GOOG';\nWARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:29:00  |\n+---------+----------------------+--+\n1 row selected (6.326 seconds)\n\n\n# Run against Realtime View. Notice that the latest timestamp is again 10:29\n\n0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_mor_rt group by symbol HAVING symbol = 'GOOG';\nWARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:29:00  |\n+---------+----------------------+--+\n1 row selected (1.606 seconds)\n\n\n# Run projection query against Read Optimized and Realtime tables\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924222155       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924222155       | GOOG    | 2018-08-31 10:29:00  | 3391    | 1230.1899  | 1230.085  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_rt where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924222155       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924222155       | GOOG    | 2018-08-31 10:29:00  | 3391    | 1230.1899  | 1230.085  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\nexit\nexit\n"})}),"\n",(0,t.jsx)(o.h3,{id:"step-4-b-\u6267\u884c-spark-sql-\u67e5\u8be2",children:"Step 4 (b): \u6267\u884c Spark-SQL \u67e5\u8be2"}),"\n",(0,t.jsx)(o.p,{children:"Hudi \u652f\u6301\u4ee5 Spark \u4f5c\u4e3a\u7c7b\u4f3c Hive \u7684\u67e5\u8be2\u5f15\u64ce\u3002\u8fd9\u662f\u5728 Spartk-SQL \u4e2d\u6267\u884c\u4e0e Hive \u76f8\u540c\u7684\u67e5\u8be2"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-java",children:'docker exec -it adhoc-1 /bin/bash\n$SPARK_INSTALL/bin/spark-shell --jars $HUDI_SPARK_BUNDLE --master local[2] --driver-class-path $HADOOP_CONF_DIR --conf spark.sql.hive.convertMetastoreParquet=false --deploy-mode client  --driver-memory 1G --executor-memory 3G --num-executors 1  --packages com.databricks:spark-avro_2.11:4.0.0\n...\n\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  \'_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.3.1\n      /_/\n\nUsing Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_181)\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala>\nscala> spark.sql("show tables").show(100, false)\n+--------+------------------+-----------+\n|database|tableName         |isTemporary|\n+--------+------------------+-----------+\n|default |stock_ticks_cow   |false      |\n|default |stock_ticks_mor   |false      |\n|default |stock_ticks_mor_rt|false      |\n+--------+------------------+-----------+\n\n# Copy-On-Write Table\n\n## Run max timestamp query against COW table\n\nscala> spark.sql("select symbol, max(ts) from stock_ticks_cow group by symbol HAVING symbol = \'GOOG\'").show(100, false)\n[Stage 0:>                                                          (0 + 1) / 1]SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes#StaticLoggerBinder for further details.\n+------+-------------------+\n|symbol|max(ts)            |\n+------+-------------------+\n|GOOG  |2018-08-31 10:29:00|\n+------+-------------------+\n\n## Projection Query\n\nscala> spark.sql("select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_cow where  symbol = \'GOOG\'").show(100, false)\n+-------------------+------+-------------------+------+---------+--------+\n|_hoodie_commit_time|symbol|ts                 |volume|open     |close   |\n+-------------------+------+-------------------+------+---------+--------+\n|20180924221953     |GOOG  |2018-08-31 09:59:00|6330  |1230.5   |1230.02 |\n|20180924221953     |GOOG  |2018-08-31 10:29:00|3391  |1230.1899|1230.085|\n+-------------------+------+-------------------+------+---------+--------+\n\n# Merge-On-Read Queries:\n==========================\n\nLets run similar queries against M-O-R dataset. Lets look at both\nReadOptimized and Realtime views supported by M-O-R dataset\n\n# Run against ReadOptimized View. Notice that the latest timestamp is 10:29\nscala> spark.sql("select symbol, max(ts) from stock_ticks_mor group by symbol HAVING symbol = \'GOOG\'").show(100, false)\n+------+-------------------+\n|symbol|max(ts)            |\n+------+-------------------+\n|GOOG  |2018-08-31 10:29:00|\n+------+-------------------+\n\n\n# Run against Realtime View. Notice that the latest timestamp is again 10:29\n\nscala> spark.sql("select symbol, max(ts) from stock_ticks_mor_rt group by symbol HAVING symbol = \'GOOG\'").show(100, false)\n+------+-------------------+\n|symbol|max(ts)            |\n+------+-------------------+\n|GOOG  |2018-08-31 10:29:00|\n+------+-------------------+\n\n# Run projection query against Read Optimized and Realtime tables\n\nscala> spark.sql("select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor where  symbol = \'GOOG\'").show(100, false)\n+-------------------+------+-------------------+------+---------+--------+\n|_hoodie_commit_time|symbol|ts                 |volume|open     |close   |\n+-------------------+------+-------------------+------+---------+--------+\n|20180924222155     |GOOG  |2018-08-31 09:59:00|6330  |1230.5   |1230.02 |\n|20180924222155     |GOOG  |2018-08-31 10:29:00|3391  |1230.1899|1230.085|\n+-------------------+------+-------------------+------+---------+--------+\n\nscala> spark.sql("select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_rt where  symbol = \'GOOG\'").show(100, false)\n+-------------------+------+-------------------+------+---------+--------+\n|_hoodie_commit_time|symbol|ts                 |volume|open     |close   |\n+-------------------+------+-------------------+------+---------+--------+\n|20180924222155     |GOOG  |2018-08-31 09:59:00|6330  |1230.5   |1230.02 |\n|20180924222155     |GOOG  |2018-08-31 10:29:00|3391  |1230.1899|1230.085|\n+-------------------+------+-------------------+------+---------+--------+\n\n'})}),"\n",(0,t.jsx)(o.h3,{id:"step-4-c-\u6267\u884c-presto-\u67e5\u8be2",children:"Step 4 (c): \u6267\u884c Presto \u67e5\u8be2"}),"\n",(0,t.jsx)(o.p,{children:"\u8fd9\u662f Presto \u67e5\u8be2\uff0c\u5b83\u4eec\u4e0e Hive \u548c Spark \u7684\u67e5\u8be2\u7c7b\u4f3c\u3002\u76ee\u524d Hudi \u7684\u5b9e\u65f6\u89c6\u56fe\u4e0d\u652f\u6301 Presto \u3002"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-java",children:"docker exec -it presto-worker-1 presto --server presto-coordinator-1:8090\npresto> show catalogs;\n  Catalog\n-----------\n hive\n jmx\n localfile\n system\n(4 rows)\n\nQuery 20190817_134851_00000_j8rcz, FINISHED, 1 node\nSplits: 19 total, 19 done (100.00%)\n0:04 [0 rows, 0B] [0 rows/s, 0B/s]\n\npresto> use hive.default;\nUSE\npresto:default> show tables;\n       Table\n--------------------\n stock_ticks_cow\n stock_ticks_mor\n stock_ticks_mor_rt\n(3 rows)\n\nQuery 20190822_181000_00001_segyw, FINISHED, 2 nodes\nSplits: 19 total, 19 done (100.00%)\n0:05 [3 rows, 99B] [0 rows/s, 18B/s]\n\n\n# COPY-ON-WRITE Queries:\n=========================\n\n\npresto:default> select symbol, max(ts) from stock_ticks_cow group by symbol HAVING symbol = 'GOOG';\n symbol |        _col1\n--------+---------------------\n GOOG   | 2018-08-31 10:29:00\n(1 row)\n\nQuery 20190822_181011_00002_segyw, FINISHED, 1 node\nSplits: 49 total, 49 done (100.00%)\n0:12 [197 rows, 613B] [16 rows/s, 50B/s]\n\npresto:default> select \"_hoodie_commit_time\", symbol, ts, volume, open, close from stock_ticks_cow where symbol = 'GOOG';\n _hoodie_commit_time | symbol |         ts          | volume |   open    |  close\n---------------------+--------+---------------------+--------+-----------+----------\n 20190822180221      | GOOG   | 2018-08-31 09:59:00 |   6330 |    1230.5 |  1230.02\n 20190822180221      | GOOG   | 2018-08-31 10:29:00 |   3391 | 1230.1899 | 1230.085\n(2 rows)\n\nQuery 20190822_181141_00003_segyw, FINISHED, 1 node\nSplits: 17 total, 17 done (100.00%)\n0:02 [197 rows, 613B] [109 rows/s, 341B/s]\n\n\n# Merge-On-Read Queries:\n==========================\n\nLets run similar queries against M-O-R dataset. \n\n# Run against ReadOptimized View. Notice that the latest timestamp is 10:29\npresto:default> select symbol, max(ts) from stock_ticks_mor group by symbol HAVING symbol = 'GOOG';\n symbol |        _col1\n--------+---------------------\n GOOG   | 2018-08-31 10:29:00\n(1 row)\n\nQuery 20190822_181158_00004_segyw, FINISHED, 1 node\nSplits: 49 total, 49 done (100.00%)\n0:02 [197 rows, 613B] [110 rows/s, 343B/s]\n\n\npresto:default>  select \"_hoodie_commit_time\", symbol, ts, volume, open, close  from stock_ticks_mor where  symbol = 'GOOG';\n _hoodie_commit_time | symbol |         ts          | volume |   open    |  close\n---------------------+--------+---------------------+--------+-----------+----------\n 20190822180250      | GOOG   | 2018-08-31 09:59:00 |   6330 |    1230.5 |  1230.02\n 20190822180250      | GOOG   | 2018-08-31 10:29:00 |   3391 | 1230.1899 | 1230.085\n(2 rows)\n\nQuery 20190822_181256_00006_segyw, FINISHED, 1 node\nSplits: 17 total, 17 done (100.00%)\n0:02 [197 rows, 613B] [92 rows/s, 286B/s]\n\npresto:default> exit\n"})}),"\n",(0,t.jsx)(o.h3,{id:"step-5-\u5c06\u7b2c-2-\u6279\u6b21\u4e0a\u4f20\u5230-kafka-\u5e76\u8fd0\u884c-deltastreamer-\u8fdb\u884c\u91c7\u96c6",children:"Step 5: \u5c06\u7b2c 2 \u6279\u6b21\u4e0a\u4f20\u5230 Kafka \u5e76\u8fd0\u884c DeltaStreamer \u8fdb\u884c\u91c7\u96c6"}),"\n",(0,t.jsx)(o.p,{children:"\u4e0a\u4f20\u7b2c 2 \u6279\u6b21\u6570\u636e\uff0c\u5e76\u4f7f\u7528 DeltaStreamer \u91c7\u96c6\u3002\u7531\u4e8e\u8fd9\u4e2a\u6279\u6b21\u4e0d\u4f1a\u5f15\u5165\u4efb\u4f55\u65b0\u5206\u533a\uff0c\u56e0\u6b64\u4e0d\u9700\u8981\u6267\u884c Hive \u540c\u6b65\u3002"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-java",children:"cat docker/demo/data/batch_2.json | kafkacat -b kafkabroker -t stock_ticks -P\n\n# Within Docker container, run the ingestion command\ndocker exec -it adhoc-2 /bin/bash\n\n# Run the following spark-submit command to execute the delta-streamer and ingest to stock_ticks_cow dataset in HDFS\nspark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer $HUDI_UTILITIES_BUNDLE --storage-type COPY_ON_WRITE --source-class org.apache.hudi.utilities.sources.JsonKafkaSource --source-ordering-field ts  --target-base-path /user/hive/warehouse/stock_ticks_cow --target-table stock_ticks_cow --props /var/demo/config/kafka-source.properties --schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider\n\n\n# Run the following spark-submit command to execute the delta-streamer and ingest to stock_ticks_mor dataset in HDFS\nspark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer $HUDI_UTILITIES_BUNDLE --storage-type MERGE_ON_READ --source-class org.apache.hudi.utilities.sources.JsonKafkaSource --source-ordering-field ts  --target-base-path /user/hive/warehouse/stock_ticks_mor --target-table stock_ticks_mor --props /var/demo/config/kafka-source.properties --schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider --disable-compaction\n\nexit\n"})}),"\n",(0,t.jsxs)(o.p,{children:["\u4f7f\u7528\u5199\u65f6\u590d\u5236\u8868\uff0c DeltaStreamer \u7684\u7b2c 2 \u6279\u6570\u636e\u91c7\u96c6\u5c06\u5bfc\u81f4 Parquet \u6587\u4ef6\u521b\u5efa\u4e00\u4e2a\u65b0\u7248\u672c\u3002\n\u53c2\u8003\uff1a ",(0,t.jsx)(o.code,{children:"http://namenode:50070/explorer#/user/hive/warehouse/stock_ticks_cow/2018/08/31"})]}),"\n",(0,t.jsxs)(o.p,{children:["\u4f7f\u7528\u8bfb\u65f6\u5408\u5e76\u8868, \u7b2c 2 \u6279\u6570\u636e\u91c7\u96c6\u4ec5\u4ec5\u5c06\u6570\u636e\u8ffd\u52a0\u5230\u6ca1\u6709\u5408\u5e76\u7684 delta \uff08\u65e5\u5fd7\uff09 \u6587\u4ef6\u4e2d\u3002\u770b\u4e00\u4e0b HDFS \u6587\u4ef6\u7cfb\u7edf\u6765\u4e86\u89e3\u8fd9\u4e00\u70b9\uff1a ",(0,t.jsx)(o.code,{children:"http://namenode:50070/explorer#/user/hive/warehouse/stock_ticks_mor/2018/08/31"})]}),"\n",(0,t.jsx)(o.h3,{id:"step-6-a-\u6267\u884c-hive-\u67e5\u8be2",children:"Step 6 (a): \u6267\u884c Hive \u67e5\u8be2"}),"\n",(0,t.jsx)(o.p,{children:"\u4f7f\u7528\u5199\u65f6\u590d\u5236\u8868\uff0c\u5728\u6bcf\u4e00\u4e2a\u6279\u6b21\u88ab\u63d0\u4ea4\u91c7\u96c6\u5e76\u521b\u5efa\u65b0\u7248\u672c\u7684 Parquet \u6587\u4ef6\u65f6\uff0c\u8bfb\u4f18\u5316\u89c6\u56fe\u4f1a\u7acb\u5373\u53d1\u73b0\u53d8\u66f4\uff0c\u8fd9\u4e9b\u53d8\u66f4\u88ab\u5f53\u7b2c 2 \u6279\u6b21\u7684\u4e00\u90e8\u5206\u3002"}),"\n",(0,t.jsx)(o.p,{children:"\u4f7f\u7528\u8bfb\u65f6\u5408\u5e76\u8868\uff0c\u7b2c 2 \u6279\u6570\u636e\u91c7\u96c6\u4ec5\u4ec5\u5c06\u6570\u636e\u8ffd\u52a0\u5230\u6ca1\u6709\u5408\u5e76\u7684 delta \uff08\u65e5\u5fd7\uff09 \u6587\u4ef6\u4e2d\u3002\n\u6b64\u65f6\uff0c\u8bfb\u4f18\u5316\u89c6\u56fe\u548c\u5b9e\u65f6\u89c6\u56fe\u5c06\u63d0\u4f9b\u4e0d\u540c\u7684\u7ed3\u679c\u3002\u8bfb\u4f18\u5316\u89c6\u56fe\u4ecd\u4f1a\u8fd4\u56de\u201c10:29 am\u201d\uff0c\u56e0\u4e3a\u5b83\u4f1a\u53ea\u4f1a\u4ece Parquet \u6587\u4ef6\u4e2d\u8bfb\u53d6\u3002\u5b9e\u65f6\u89c6\u56fe\u4f1a\u505a\u5373\u65f6\u5408\u5e76\u5e76\u8fd4\u56de\u6700\u65b0\u63d0\u4ea4\u7684\u6570\u636e\uff0c\u5373\u201c10:59 a.m\u201d\u3002"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-java",children:"docker exec -it adhoc-2 /bin/bash\nbeeline -u jdbc:hive2://hiveserver:10000 --hiveconf hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat --hiveconf hive.stats.autogather=false\n\n# Copy On Write Table:\n\n0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_cow group by symbol HAVING symbol = 'GOOG';\nWARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:59:00  |\n+---------+----------------------+--+\n1 row selected (1.932 seconds)\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_cow where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924221953       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924224524       | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\nAs you can notice, the above queries now reflect the changes that came as part of ingesting second batch.\n\n\n# Merge On Read Table:\n\n# Read Optimized View\n0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_mor group by symbol HAVING symbol = 'GOOG';\nWARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:29:00  |\n+---------+----------------------+--+\n1 row selected (1.6 seconds)\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924222155       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924222155       | GOOG    | 2018-08-31 10:29:00  | 3391    | 1230.1899  | 1230.085  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\n# Realtime View\n0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_mor_rt group by symbol HAVING symbol = 'GOOG';\nWARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:59:00  |\n+---------+----------------------+--+\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_rt where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924222155       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924224537       | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\nexit\nexit\n"})}),"\n",(0,t.jsx)(o.h3,{id:"step-6-b-\u6267\u884c-spark-sql-\u67e5\u8be2",children:"Step 6 (b): \u6267\u884c Spark SQL \u67e5\u8be2"}),"\n",(0,t.jsx)(o.p,{children:"\u4ee5 Spark SQL \u6267\u884c\u7c7b\u4f3c\u7684\u67e5\u8be2\uff1a"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-java",children:"docker exec -it adhoc-1 /bin/bash\nbash-4.4# $SPARK_INSTALL/bin/spark-shell --jars $HUDI_SPARK_BUNDLE --driver-class-path $HADOOP_CONF_DIR --conf spark.sql.hive.convertMetastoreParquet=false --deploy-mode client  --driver-memory 1G --master local[2] --executor-memory 3G --num-executors 1  --packages com.databricks:spark-avro_2.11:4.0.0\n\n# Copy On Write Table:\n\nscala> spark.sql(\"select symbol, max(ts) from stock_ticks_cow group by symbol HAVING symbol = 'GOOG'\").show(100, false)\n+------+-------------------+\n|symbol|max(ts)            |\n+------+-------------------+\n|GOOG  |2018-08-31 10:59:00|\n+------+-------------------+\n\nscala> spark.sql(\"select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_cow where  symbol = 'GOOG'\").show(100, false)\n\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924221953       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924224524       | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\nAs you can notice, the above queries now reflect the changes that came as part of ingesting second batch.\n\n\n# Merge On Read Table:\n\n# Read Optimized View\nscala> spark.sql(\"select symbol, max(ts) from stock_ticks_mor group by symbol HAVING symbol = 'GOOG'\").show(100, false)\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:29:00  |\n+---------+----------------------+--+\n1 row selected (1.6 seconds)\n\nscala> spark.sql(\"select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor where  symbol = 'GOOG'\").show(100, false)\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924222155       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924222155       | GOOG    | 2018-08-31 10:29:00  | 3391    | 1230.1899  | 1230.085  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\n# Realtime View\nscala> spark.sql(\"select symbol, max(ts) from stock_ticks_mor_rt group by symbol HAVING symbol = 'GOOG'\").show(100, false)\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:59:00  |\n+---------+----------------------+--+\n\nscala> spark.sql(\"select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_rt where  symbol = 'GOOG'\").show(100, false)\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924222155       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924224537       | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\nexit\nexit\n"})}),"\n",(0,t.jsx)(o.h3,{id:"step-6-c-\u6267\u884c-presto-\u67e5\u8be2",children:"Step 6 (c): \u6267\u884c Presto \u67e5\u8be2"}),"\n",(0,t.jsx)(o.p,{children:"\u5728 Presto \u4e2d\u4e3a\u8bfb\u4f18\u5316\u89c6\u56fe\u6267\u884c\u7c7b\u4f3c\u7684\u67e5\u8be2\uff1a"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-java",children:"docker exec -it presto-worker-1 presto --server presto-coordinator-1:8090\npresto> use hive.default;\nUSE\n\n# Copy On Write Table:\n\npresto:default>select symbol, max(ts) from stock_ticks_cow group by symbol HAVING symbol = 'GOOG';\n symbol |        _col1\n--------+---------------------\n GOOG   | 2018-08-31 10:59:00\n(1 row)\n\nQuery 20190822_181530_00007_segyw, FINISHED, 1 node\nSplits: 49 total, 49 done (100.00%)\n0:02 [197 rows, 613B] [125 rows/s, 389B/s]\n\npresto:default>select \"_hoodie_commit_time\", symbol, ts, volume, open, close  from stock_ticks_cow where  symbol = 'GOOG';\n _hoodie_commit_time | symbol |         ts          | volume |   open    |  close\n---------------------+--------+---------------------+--------+-----------+----------\n 20190822180221      | GOOG   | 2018-08-31 09:59:00 |   6330 |    1230.5 |  1230.02\n 20190822181433      | GOOG   | 2018-08-31 10:59:00 |   9021 | 1227.1993 | 1227.215\n(2 rows)\n\nQuery 20190822_181545_00008_segyw, FINISHED, 1 node\nSplits: 17 total, 17 done (100.00%)\n0:02 [197 rows, 613B] [106 rows/s, 332B/s]\n\nAs you can notice, the above queries now reflect the changes that came as part of ingesting second batch.\n\n\n# Merge On Read Table:\n\n# Read Optimized View\npresto:default> select symbol, max(ts) from stock_ticks_mor group by symbol HAVING symbol = 'GOOG';\n symbol |        _col1\n--------+---------------------\n GOOG   | 2018-08-31 10:29:00\n(1 row)\n\nQuery 20190822_181602_00009_segyw, FINISHED, 1 node\nSplits: 49 total, 49 done (100.00%)\n0:01 [197 rows, 613B] [139 rows/s, 435B/s]\n\npresto:default>select \"_hoodie_commit_time\", symbol, ts, volume, open, close  from stock_ticks_mor where  symbol = 'GOOG';\n _hoodie_commit_time | symbol |         ts          | volume |   open    |  close\n---------------------+--------+---------------------+--------+-----------+----------\n 20190822180250      | GOOG   | 2018-08-31 09:59:00 |   6330 |    1230.5 |  1230.02\n 20190822180250      | GOOG   | 2018-08-31 10:29:00 |   3391 | 1230.1899 | 1230.085\n(2 rows)\n\nQuery 20190822_181615_00010_segyw, FINISHED, 1 node\nSplits: 17 total, 17 done (100.00%)\n0:01 [197 rows, 613B] [154 rows/s, 480B/s]\n\npresto:default> exit\n"})}),"\n",(0,t.jsx)(o.h3,{id:"step-7--\u5199\u65f6\u590d\u5236\u8868\u7684\u589e\u91cf\u67e5\u8be2",children:"Step 7 : \u5199\u65f6\u590d\u5236\u8868\u7684\u589e\u91cf\u67e5\u8be2"}),"\n",(0,t.jsx)(o.p,{children:"\u4f7f\u7528\u91c7\u96c6\u7684\u4e24\u4e2a\u6279\u6b21\u7684\u6570\u636e\uff0c\u6211\u4eec\u5c55\u793a Hudi \u5199\u65f6\u590d\u5236\u6570\u636e\u96c6\u4e2d\u652f\u6301\u7684\u589e\u91cf\u67e5\u8be2\u3002"}),"\n",(0,t.jsx)(o.p,{children:"\u6211\u4eec\u4f7f\u7528\u7c7b\u4f3c\u7684\u5de5\u7a0b\u67e5\u8be2\u6837\u4f8b\uff1a"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-java",children:"docker exec -it adhoc-2 /bin/bash\nbeeline -u jdbc:hive2://hiveserver:10000 --hiveconf hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat --hiveconf hive.stats.autogather=false\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_cow where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924064621       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924065039       | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n"})}),"\n",(0,t.jsx)(o.p,{children:"\u6b63\u5982\u4f60\u5728\u4e0a\u9762\u7684\u67e5\u8be2\u4e2d\u770b\u5230\u7684\uff0c\u6709\u4e24\u4e2a\u63d0\u4ea4\u2014\u2014\u6309\u65f6\u95f4\u7ebf\u6392\u5217\u662f 20180924064621 \u548c 20180924065039 \u3002\n\u5f53\u4f60\u6309\u7167\u8fd9\u4e9b\u6b65\u9aa4\u6267\u884c\u540e\uff0c\u4f60\u7684\u63d0\u4ea4\u4f1a\u5f97\u5230\u4e0d\u540c\u7684\u65f6\u95f4\u6233\u3002\u5c06\u5b83\u4eec\u66ff\u6362\u5230\u4e0a\u9762\u65f6\u95f4\u6233\u7684\u4f4d\u7f6e\u3002"}),"\n",(0,t.jsx)(o.p,{children:"\u4e3a\u4e86\u5c55\u793a\u589e\u91cf\u67e5\u8be2\u7684\u5f71\u54cd\uff0c\u6211\u4eec\u5047\u8bbe\u6709\u4e00\u4f4d\u8bfb\u8005\u5df2\u7ecf\u5728\u7b2c 1 \u6279\u6570\u636e\u4e2d\u4e00\u90e8\u5206\u770b\u5230\u4e86\u53d8\u5316\u3002\u90a3\u4e48\uff0c\u4e3a\u4e86\u8ba9\u8bfb\u8005\u770b\u5230\u7b2c 2 \u6279\u6570\u636e\u7684\u5f71\u54cd\uff0c\u4ed6/\u5979\u9700\u8981\u4fdd\u7559\u7b2c 1 \u6279\u6b21\u63d0\u4ea4\u65f6\u95f4\u4e2d\u7684\u5f00\u59cb\u65f6\u95f4\uff08 20180924064621 \uff09\u5e76\u6267\u884c\u589e\u91cf\u67e5\u8be2\uff1a"}),"\n",(0,t.jsx)(o.p,{children:"Hudi \u7684\u589e\u91cf\u6a21\u5f0f\u4e3a\u589e\u91cf\u67e5\u8be2\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u626b\u63cf\uff0c\u901a\u8fc7 Hudi \u7ba1\u7406\u7684\u5143\u6570\u636e\uff0c\u8fc7\u6ee4\u6389\u4e86\u90a3\u4e9b\u4e0d\u5305\u542b\u5019\u9009\u8bb0\u5f55\u7684\u6587\u4ef6\u3002"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-java",children:"docker exec -it adhoc-2 /bin/bash\nbeeline -u jdbc:hive2://hiveserver:10000 --hiveconf hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat --hiveconf hive.stats.autogather=false\n0: jdbc:hive2://hiveserver:10000> set hoodie.stock_ticks_cow.consume.mode=INCREMENTAL;\nNo rows affected (0.009 seconds)\n0: jdbc:hive2://hiveserver:10000>  set hoodie.stock_ticks_cow.consume.max.commits=3;\nNo rows affected (0.009 seconds)\n0: jdbc:hive2://hiveserver:10000> set hoodie.stock_ticks_cow.consume.start.timestamp=20180924064621;\n"})}),"\n",(0,t.jsx)(o.p,{children:"\u4f7f\u7528\u4e0a\u9762\u7684\u8bbe\u7f6e\uff0c\u90a3\u4e9b\u5728\u63d0\u4ea4 20180924065039 \u4e4b\u540e\u6ca1\u6709\u4efb\u4f55\u66f4\u65b0\u7684\u6587\u4ef6ID\u5c06\u88ab\u8fc7\u6ee4\u6389\uff0c\u4e0d\u8fdb\u884c\u626b\u63cf\u3002\n\u4ee5\u4e0b\u662f\u589e\u91cf\u67e5\u8be2\uff1a"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-java",children:"0: jdbc:hive2://hiveserver:10000>\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_cow where  symbol = 'GOOG' and `_hoodie_commit_time` > '20180924064621';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924065039       | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n1 row selected (0.83 seconds)\n0: jdbc:hive2://hiveserver:10000>\n"})}),"\n",(0,t.jsx)(o.h3,{id:"\u4f7f\u7528-spark-sql-\u505a\u589e\u91cf\u67e5\u8be2",children:"\u4f7f\u7528 Spark SQL \u505a\u589e\u91cf\u67e5\u8be2"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-java",children:'docker exec -it adhoc-1 /bin/bash\nbash-4.4# $SPARK_INSTALL/bin/spark-shell --jars $HUDI_SPARK_BUNDLE --driver-class-path $HADOOP_CONF_DIR --conf spark.sql.hive.convertMetastoreParquet=false --deploy-mode client  --driver-memory 1G --master local[2] --executor-memory 3G --num-executors 1  --packages com.databricks:spark-avro_2.11:4.0.0\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  \'_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.3.1\n      /_/\n\nUsing Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_181)\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> import org.apache.hudi.DataSourceReadOptions\nimport org.apache.hudi.DataSourceReadOptions\n\n# In the below query, 20180925045257 is the first commit\'s timestamp\nscala> val hoodieIncViewDF =  spark.read.format("org.apache.hudi").option(DataSourceReadOptions.VIEW_TYPE_OPT_KEY, DataSourceReadOptions.VIEW_TYPE_INCREMENTAL_OPT_VAL).option(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY, "20180924064621").load("/user/hive/warehouse/stock_ticks_cow")\nSLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes#StaticLoggerBinder for further details.\nhoodieIncViewDF: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 15 more fields]\n\nscala> hoodieIncViewDF.registerTempTable("stock_ticks_cow_incr_tmp1")\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nscala> spark.sql("select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_cow_incr_tmp1 where  symbol = \'GOOG\'").show(100, false);\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924065039       | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\n'})}),"\n",(0,t.jsx)(o.h3,{id:"step-8-\u4e3a\u8bfb\u65f6\u5408\u5e76\u6570\u636e\u96c6\u7684\u8c03\u5ea6\u5e76\u6267\u884c\u538b\u7f29",children:"Step 8: \u4e3a\u8bfb\u65f6\u5408\u5e76\u6570\u636e\u96c6\u7684\u8c03\u5ea6\u5e76\u6267\u884c\u538b\u7f29"}),"\n",(0,t.jsx)(o.p,{children:"\u6211\u4eec\u6765\u8c03\u5ea6\u5e76\u8fd0\u884c\u4e00\u4e2a\u538b\u7f29\u6765\u521b\u5efa\u4e00\u4e2a\u65b0\u7248\u672c\u7684\u5217\u5f0f\u6587\u4ef6\uff0c\u4ee5\u4fbf\u8bfb\u4f18\u5316\u8bfb\u53d6\u5668\u80fd\u770b\u5230\u65b0\u6570\u636e\u3002\n\u518d\u6b21\u5f3a\u8c03\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528 Hudi CLI \u6765\u4eba\u5de5\u8c03\u5ea6\u5e76\u6267\u884c\u538b\u7f29\u3002"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-java",children:'docker exec -it adhoc-1 /bin/bash\nroot@adhoc-1:/opt#   /var/hoodie/ws/hudi-cli/hudi-cli.sh\n============================================\n*                                          *\n*     _    _           _   _               *\n*    | |  | |         | | (_)              *\n*    | |__| |       __| |  -               *\n*    |  __  ||   | / _` | ||               *\n*    | |  | ||   || (_| | ||               *\n*    |_|  |_|\\___/ \\____/ ||               *\n*                                          *\n============================================\n\nWelcome to Hoodie CLI. Please type help if you are looking for help.\nhudi->connect --path /user/hive/warehouse/stock_ticks_mor\n18/09/24 06:59:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n18/09/24 06:59:35 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor\n18/09/24 06:59:35 INFO util.FSUtils: Hadoop Configuration: fs.defaultFS: [hdfs://namenode:8020], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1261652683_11, ugi=root (auth:SIMPLE)]]]\n18/09/24 06:59:35 INFO table.HoodieTableConfig: Loading dataset properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties\n18/09/24 06:59:36 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ from /user/hive/warehouse/stock_ticks_mor\nMetadata for table stock_ticks_mor loaded\n\n# Ensure no compactions are present\n\nhoodie:stock_ticks_mor->compactions show all\n18/09/24 06:59:54 INFO timeline.HoodieActiveTimeline: Loaded instants [[20180924064636__clean__COMPLETED], [20180924064636__deltacommit__COMPLETED], [20180924065057__clean__COMPLETED], [20180924065057__deltacommit__COMPLETED]]\n    ___________________________________________________________________\n    | Compaction Instant Time| State    | Total FileIds to be Compacted|\n    |==================================================================|\n\n\n\n\n# Schedule a compaction. This will use Spark Launcher to schedule compaction\nhoodie:stock_ticks_mor->compaction schedule\n....\nCompaction successfully completed for 20180924070031\n\n# Now refresh and check again. You will see that there is a new compaction requested\n\nhoodie:stock_ticks->connect --path /user/hive/warehouse/stock_ticks_mor\n18/09/24 07:01:16 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor\n18/09/24 07:01:16 INFO util.FSUtils: Hadoop Configuration: fs.defaultFS: [hdfs://namenode:8020], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1261652683_11, ugi=root (auth:SIMPLE)]]]\n18/09/24 07:01:16 INFO table.HoodieTableConfig: Loading dataset properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties\n18/09/24 07:01:16 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ from /user/hive/warehouse/stock_ticks_mor\nMetadata for table stock_ticks_mor loaded\n\n\n\nhoodie:stock_ticks_mor->compactions show all\n18/09/24 06:34:12 INFO timeline.HoodieActiveTimeline: Loaded instants [[20180924041125__clean__COMPLETED], [20180924041125__deltacommit__COMPLETED], [20180924042735__clean__COMPLETED], [20180924042735__deltacommit__COMPLETED], [==>20180924063245__compaction__REQUESTED]]\n    ___________________________________________________________________\n    | Compaction Instant Time| State    | Total FileIds to be Compacted|\n    |==================================================================|\n    | 20180924070031         | REQUESTED| 1                            |\n\n\n\n\n# Execute the compaction. The compaction instant value passed below must be the one displayed in the above "compactions show all" query\nhoodie:stock_ticks_mor->compaction run --compactionInstant  20180924070031 --parallelism 2 --sparkMemory 1G  --schemaFilePath /var/demo/config/schema.avsc --retry 1  \n....\nCompaction successfully completed for 20180924070031\n\n\n## Now check if compaction is completed\n\nhoodie:stock_ticks_mor->connect --path /user/hive/warehouse/stock_ticks_mor\n18/09/24 07:03:00 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor\n18/09/24 07:03:00 INFO util.FSUtils: Hadoop Configuration: fs.defaultFS: [hdfs://namenode:8020], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1261652683_11, ugi=root (auth:SIMPLE)]]]\n18/09/24 07:03:00 INFO table.HoodieTableConfig: Loading dataset properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties\n18/09/24 07:03:00 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ from /user/hive/warehouse/stock_ticks_mor\nMetadata for table stock_ticks_mor loaded\n\n\n\nhoodie:stock_ticks->compactions show all\n18/09/24 07:03:15 INFO timeline.HoodieActiveTimeline: Loaded instants [[20180924064636__clean__COMPLETED], [20180924064636__deltacommit__COMPLETED], [20180924065057__clean__COMPLETED], [20180924065057__deltacommit__COMPLETED], [20180924070031__commit__COMPLETED]]\n    ___________________________________________________________________\n    | Compaction Instant Time| State    | Total FileIds to be Compacted|\n    |==================================================================|\n    | 20180924070031         | COMPLETED| 1                            |\n\n'})}),"\n",(0,t.jsx)(o.h3,{id:"step-9-\u6267\u884c\u5305\u542b\u589e\u91cf\u67e5\u8be2\u7684-hive-\u67e5\u8be2",children:"Step 9: \u6267\u884c\u5305\u542b\u589e\u91cf\u67e5\u8be2\u7684 Hive \u67e5\u8be2"}),"\n",(0,t.jsx)(o.p,{children:"\u4f60\u5c06\u770b\u5230\u8bfb\u4f18\u5316\u89c6\u56fe\u548c\u5b9e\u65f6\u89c6\u56fe\u90fd\u4f1a\u5c55\u793a\u6700\u65b0\u63d0\u4ea4\u7684\u6570\u636e\u3002\n\u8ba9\u6211\u4eec\u4e5f\u5bf9 MOR \u8868\u6267\u884c\u589e\u91cf\u67e5\u8be2\u3002\n\u901a\u8fc7\u67e5\u770b\u4e0b\u65b9\u7684\u67e5\u8be2\u8f93\u51fa\uff0c\u80fd\u591f\u660e\u786e MOR \u8868\u7684\u7b2c\u4e00\u6b21\u63d0\u4ea4\u65f6\u95f4\u662f 20180924064636 \u800c\u7b2c\u4e8c\u6b21\u63d0\u4ea4\u65f6\u95f4\u662f 20180924070031 \u3002"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-java",children:"docker exec -it adhoc-2 /bin/bash\nbeeline -u jdbc:hive2://hiveserver:10000 --hiveconf hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat --hiveconf hive.stats.autogather=false\n\n# Read Optimized View\n0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_mor group by symbol HAVING symbol = 'GOOG';\nWARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:59:00  |\n+---------+----------------------+--+\n1 row selected (1.6 seconds)\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924064636       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924070031       | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\n# Realtime View\n0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_mor_rt group by symbol HAVING symbol = 'GOOG';\nWARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:59:00  |\n+---------+----------------------+--+\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_rt where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924064636       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924070031       | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\n# Incremental View:\n\n0: jdbc:hive2://hiveserver:10000> set hoodie.stock_ticks_mor.consume.mode=INCREMENTAL;\nNo rows affected (0.008 seconds)\n# Max-Commits covers both second batch and compaction commit\n0: jdbc:hive2://hiveserver:10000> set hoodie.stock_ticks_mor.consume.max.commits=3;\nNo rows affected (0.007 seconds)\n0: jdbc:hive2://hiveserver:10000> set hoodie.stock_ticks_mor.consume.start.timestamp=20180924064636;\nNo rows affected (0.013 seconds)\n# Query:\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor where  symbol = 'GOOG' and `_hoodie_commit_time` > '20180924064636';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924070031       | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\nexit\nexit\n"})}),"\n",(0,t.jsx)(o.h3,{id:"step-10-\u538b\u7f29\u540e\u5728-mor-\u7684\u8bfb\u4f18\u5316\u89c6\u56fe\u4e0e\u5b9e\u65f6\u89c6\u56fe\u4e0a\u4f7f\u7528-spark-sql",children:"Step 10: \u538b\u7f29\u540e\u5728 MOR \u7684\u8bfb\u4f18\u5316\u89c6\u56fe\u4e0e\u5b9e\u65f6\u89c6\u56fe\u4e0a\u4f7f\u7528 Spark-SQL"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-java",children:"docker exec -it adhoc-1 /bin/bash\nbash-4.4# $SPARK_INSTALL/bin/spark-shell --jars $HUDI_SPARK_BUNDLE --driver-class-path $HADOOP_CONF_DIR --conf spark.sql.hive.convertMetastoreParquet=false --deploy-mode client  --driver-memory 1G --master local[2] --executor-memory 3G --num-executors 1  --packages com.databricks:spark-avro_2.11:4.0.0\n\n# Read Optimized View\nscala> spark.sql(\"select symbol, max(ts) from stock_ticks_mor group by symbol HAVING symbol = 'GOOG'\").show(100, false)\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:59:00  |\n+---------+----------------------+--+\n1 row selected (1.6 seconds)\n\nscala> spark.sql(\"select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor where  symbol = 'GOOG'\").show(100, false)\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924064636       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924070031       | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\n# Realtime View\nscala> spark.sql(\"select symbol, max(ts) from stock_ticks_mor_rt group by symbol HAVING symbol = 'GOOG'\").show(100, false)\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:59:00  |\n+---------+----------------------+--+\n\nscala> spark.sql(\"select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_rt where  symbol = 'GOOG'\").show(100, false)\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924064636       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924070031       | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n"})}),"\n",(0,t.jsx)(o.h3,{id:"step-11--\u538b\u7f29\u540e\u5728-mor-\u6570\u636e\u96c6\u7684\u8bfb\u4f18\u5316\u89c6\u56fe\u4e0a\u8fdb\u884c-presto-\u67e5\u8be2",children:"Step 11:  \u538b\u7f29\u540e\u5728 MOR \u6570\u636e\u96c6\u7684\u8bfb\u4f18\u5316\u89c6\u56fe\u4e0a\u8fdb\u884c Presto \u67e5\u8be2"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-java",children:"docker exec -it presto-worker-1 presto --server presto-coordinator-1:8090\npresto> use hive.default;\nUSE\n\n# Read Optimized View\nresto:default> select symbol, max(ts) from stock_ticks_mor group by symbol HAVING symbol = 'GOOG';\n  symbol |        _col1\n--------+---------------------\n GOOG   | 2018-08-31 10:59:00\n(1 row)\n\nQuery 20190822_182319_00011_segyw, FINISHED, 1 node\nSplits: 49 total, 49 done (100.00%)\n0:01 [197 rows, 613B] [133 rows/s, 414B/s]\n\npresto:default> select \"_hoodie_commit_time\", symbol, ts, volume, open, close  from stock_ticks_mor where  symbol = 'GOOG';\n _hoodie_commit_time | symbol |         ts          | volume |   open    |  close\n---------------------+--------+---------------------+--------+-----------+----------\n 20190822180250      | GOOG   | 2018-08-31 09:59:00 |   6330 |    1230.5 |  1230.02\n 20190822181944      | GOOG   | 2018-08-31 10:59:00 |   9021 | 1227.1993 | 1227.215\n(2 rows)\n\nQuery 20190822_182333_00012_segyw, FINISHED, 1 node\nSplits: 17 total, 17 done (100.00%)\n0:02 [197 rows, 613B] [98 rows/s, 307B/s]\n\npresto:default>\n\n"})}),"\n",(0,t.jsx)(o.p,{children:"Demo \u5230\u6b64\u7ed3\u675f\u3002"}),"\n",(0,t.jsx)(o.h2,{id:"\u5728\u672c\u5730-docker-\u73af\u5883\u4e2d\u6d4b\u8bd5-hudi",children:"\u5728\u672c\u5730 Docker \u73af\u5883\u4e2d\u6d4b\u8bd5 Hudi"}),"\n",(0,t.jsx)(o.p,{children:"\u4f60\u53ef\u4ee5\u7ec4\u5efa\u4e00\u4e2a\u5305\u542b Hadoop \u3001 Hive \u548c Spark \u670d\u52a1\u7684 Hadoop Docker \u73af\u5883\uff0c\u5e76\u652f\u6301 Hudi \u3002"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-java",children:"$ mvn pre-integration-test -DskipTests\n"})}),"\n",(0,t.jsx)(o.p,{children:"\u4e0a\u9762\u7684\u547d\u4ee4\u4e3a\u6240\u6709\u7684\u670d\u52a1\u6784\u5efa\u4e86 Docker \u955c\u50cf\uff0c\u5b83\u5e26\u6709\u5f53\u524d\u5b89\u88c5\u5728 /var/hoodie/ws \u7684 Hudi \u6e90\uff0c\u5e76\u4f7f\u7528\u4e00\u4e2a\u90e8\u7f72\u6587\u4ef6\u5f15\u5165\u4e86\u8fd9\u4e9b\u670d\u52a1\u3002\u6211\u4eec\u5f53\u524d\u5728 Docker \u955c\u50cf\u4e2d\u4f7f\u7528 Hadoop \uff08v2.8.4\uff09\u3001 Hive \uff08v2.3.3\uff09\u548c Spark \uff08v2.3.1\uff09\u3002"}),"\n",(0,t.jsx)(o.p,{children:"\u8981\u9500\u6bc1\u5bb9\u5668\uff1a"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-java",children:"$ cd hudi-integ-test\n$ mvn docker-compose:down\n"})}),"\n",(0,t.jsx)(o.p,{children:"\u5982\u679c\u4f60\u60f3\u8981\u7ec4\u5efa Docker \u5bb9\u5668\uff0c\u4f7f\u7528\uff1a"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-java",children:"$ cd hudi-integ-test\n$  mvn docker-compose:up -DdetachedMode=true\n"})}),"\n",(0,t.jsxs)(o.p,{children:["Hudi \u662f\u4e00\u4e2a\u5728\u5305\u542b Hadoop \u3001 Hive \u548c Spark \u7684\u6d77\u91cf\u6570\u636e\u5206\u6790/\u91c7\u96c6\u73af\u5883\u4e2d\u4f7f\u7528\u7684\u5e93\u3002\u4e0e\u8fd9\u4e9b\u7cfb\u7edf\u7684\u4e92\u7528\u6027\u662f\u6211\u4eec\u7684\u4e00\u4e2a\u5173\u952e\u76ee\u6807\u3002 \u6211\u4eec\u5728\u79ef\u6781\u5730\u5411 ",(0,t.jsx)(o.strong,{children:"hudi-integ-test/src/test/java"})," \u6dfb\u52a0\u96c6\u6210\u6d4b\u8bd5\uff0c\u8fd9\u4e9b\u6d4b\u8bd5\u5229\u7528\u4e86\u8fd9\u4e2a Docker \u73af\u5883\uff08\u53c2\u8003\uff1a ",(0,t.jsx)(o.strong,{children:"hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestHoodieSanity.java"})," \uff09\u3002"]}),"\n",(0,t.jsx)(o.h3,{id:"\u6784\u5efa\u672c\u5730-docker-\u5bb9\u5668",children:"\u6784\u5efa\u672c\u5730 Docker \u5bb9\u5668:"}),"\n",(0,t.jsx)(o.p,{children:"Demo \u548c\u6267\u884c\u96c6\u6210\u6d4b\u8bd5\u6240\u9700\u8981\u7684 Docker \u955c\u50cf\u5df2\u7ecf\u5728 Docker \u6e90\u4e2d\u3002 Docker \u955c\u50cf\u548c\u90e8\u7f72\u811a\u672c\u7ecf\u8fc7\u4e86\u8c28\u614e\u7684\u5b9e\u73b0\u4ee5\u4fbf\u670d\u52a1\u4e0e\u591a\u79cd\u76ee\u7684\uff1a"}),"\n",(0,t.jsxs)(o.ol,{children:["\n",(0,t.jsx)(o.li,{children:"Docker \u955c\u50cf\u6709\u5185\u5efa\u7684 Hudi jar \u5305\uff0c\u5b83\u5305\u542b\u4e00\u4e9b\u6307\u5411\u5176\u4ed6 jar \u5305\u7684\u73af\u5883\u53d8\u91cf\uff08 HUDI_HADOOP_BUNDLE \u7b49\uff09"}),"\n",(0,t.jsxs)(o.li,{children:["\u4e3a\u4e86\u6267\u884c\u96c6\u6210\u6d4b\u8bd5\uff0c\u6211\u4eec\u9700\u8981\u4f7f\u7528\u672c\u5730\u751f\u6210\u7684 jar \u5305\u5728 Docker \u4e2d\u8fd0\u884c\u670d\u52a1\u3002 Docker \u90e8\u7f72\u811a\u672c\uff08\u53c2\u8003 ",(0,t.jsx)(o.code,{children:"docker/compose/docker-compose_hadoop284_hive233_spark231.yml"}),"\uff09\u80fd\u786e\u4fdd\u672c\u5730 jar \u5305\u901a\u8fc7\u6302\u8f7d Docker \u5730\u5740\u4e0a\u6302\u8f7d\u672c\u5730 Hudi \u5de5\u4f5c\u7a7a\u95f4\uff0c\u4ece\u800c\u8986\u76d6\u4e86\u5185\u5efa\u7684 jar \u5305\u3002"]}),"\n",(0,t.jsx)(o.li,{children:"\u5f53\u8fd9\u4e9b Docker \u5bb9\u5668\u6302\u8f7d\u5230\u672c\u5730 Hudi \u5de5\u4f5c\u7a7a\u95f4\u4e4b\u540e\uff0c\u4efb\u4f55\u53d1\u751f\u5728\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u7684\u53d8\u66f4\u5c06\u4f1a\u81ea\u52a8\u53cd\u6620\u5230\u5bb9\u5668\u4e2d\u3002\u8fd9\u5bf9\u4e8e\u5f00\u53d1\u8005\u6765\u8bf4\u662f\u4e00\u79cd\u5f00\u53d1\u548c\u9a8c\u8bc1 Hudi \u7684\u7b80\u4fbf\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u5f00\u53d1\u8005\u6ca1\u6709\u5206\u5e03\u5f0f\u7684\u73af\u5883\u3002\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u662f\u96c6\u6210\u6d4b\u8bd5\u7684\u6267\u884c\u65b9\u5f0f\u3002"}),"\n"]}),"\n",(0,t.jsxs)(o.p,{children:["\u8fd9\u907f\u514d\u4e86\u7ef4\u62a4\u5206\u79bb\u7684 Docker \u955c\u50cf\uff0c\u4e5f\u907f\u514d\u4e86\u672c\u5730\u6784\u5efa Docker \u955c\u50cf\u7684\u5404\u4e2a\u6b65\u9aa4\u7684\u6d88\u8017\u3002\n\u4f46\u662f\u5982\u679c\u7528\u6237\u60f3\u8981\u5728\u6709\u66f4\u4f4e\u7f51\u7edc\u5e26\u5bbd\u7684\u5730\u65b9\u6d4b\u8bd5 Hudi \uff0c\u4ed6\u4eec\u4ecd\u53ef\u4ee5\u6784\u5efa\u672c\u5730\u955c\u50cf\u3002\n\u5728\u6267\u884c ",(0,t.jsx)(o.code,{children:"docker/setup_demo.sh"})," \u4e4b\u524d\u6267\u884c\u811a\u672c ",(0,t.jsx)(o.code,{children:"docker/build_local_docker_images.sh"})," \u6765\u6784\u5efa\u672c\u5730 Docker \u955c\u50cf\u3002"]}),"\n",(0,t.jsx)(o.p,{children:"\u4ee5\u4e0b\u662f\u6267\u884c\u7684\u547d\u4ee4:"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-java",children:"cd docker\n./build_local_docker_images.sh\n.....\n\n[INFO] Reactor Summary:\n[INFO]\n[INFO] hoodie ............................................. SUCCESS [  1.709 s]\n[INFO] hudi-common ...................................... SUCCESS [  9.015 s]\n[INFO] hudi-hadoop-mr ................................... SUCCESS [  1.108 s]\n[INFO] hudi-client ...................................... SUCCESS [  4.409 s]\n[INFO] hudi-hive ........................................ SUCCESS [  0.976 s]\n[INFO] hudi-spark ....................................... SUCCESS [ 26.522 s]\n[INFO] hudi-utilities ................................... SUCCESS [ 16.256 s]\n[INFO] hudi-cli ......................................... SUCCESS [ 11.341 s]\n[INFO] hudi-hadoop-mr-bundle ............................ SUCCESS [  1.893 s]\n[INFO] hudi-hive-bundle ................................. SUCCESS [ 14.099 s]\n[INFO] hudi-spark-bundle ................................ SUCCESS [ 58.252 s]\n[INFO] hudi-hadoop-docker ............................... SUCCESS [  0.612 s]\n[INFO] hudi-hadoop-base-docker .......................... SUCCESS [04:04 min]\n[INFO] hudi-hadoop-namenode-docker ...................... SUCCESS [  6.142 s]\n[INFO] hudi-hadoop-datanode-docker ...................... SUCCESS [  7.763 s]\n[INFO] hudi-hadoop-history-docker ....................... SUCCESS [  5.922 s]\n[INFO] hudi-hadoop-hive-docker .......................... SUCCESS [ 56.152 s]\n[INFO] hudi-hadoop-sparkbase-docker ..................... SUCCESS [01:18 min]\n[INFO] hudi-hadoop-sparkmaster-docker ................... SUCCESS [  2.964 s]\n[INFO] hudi-hadoop-sparkworker-docker ................... SUCCESS [  3.032 s]\n[INFO] hudi-hadoop-sparkadhoc-docker .................... SUCCESS [  2.764 s]\n[INFO] hudi-integ-test .................................. SUCCESS [  1.785 s]\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 09:15 min\n[INFO] Finished at: 2018-09-10T17:47:37-07:00\n[INFO] Final Memory: 236M/1848M\n[INFO] ------------------------------------------------------------------------\n"})})]})}function m(e={}){const{wrapper:o}={...(0,i.R)(),...e.components};return o?(0,t.jsx)(o,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);