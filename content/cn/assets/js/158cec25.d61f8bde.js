"use strict";(self.webpackChunkhudi=self.webpackChunkhudi||[]).push([[1874],{3905:function(e,o,t){t.d(o,{Zo:function(){return m},kt:function(){return p}});var n=t(67294);function s(e,o,t){return o in e?Object.defineProperty(e,o,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[o]=t,e}function a(e,o){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);o&&(n=n.filter((function(o){return Object.getOwnPropertyDescriptor(e,o).enumerable}))),t.push.apply(t,n)}return t}function i(e){for(var o=1;o<arguments.length;o++){var t=null!=arguments[o]?arguments[o]:{};o%2?a(Object(t),!0).forEach((function(o){s(e,o,t[o])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(o){Object.defineProperty(e,o,Object.getOwnPropertyDescriptor(t,o))}))}return e}function r(e,o){if(null==e)return{};var t,n,s=function(e,o){if(null==e)return{};var t,n,s={},a=Object.keys(e);for(n=0;n<a.length;n++)t=a[n],o.indexOf(t)>=0||(s[t]=e[t]);return s}(e,o);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(n=0;n<a.length;n++)t=a[n],o.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(s[t]=e[t])}return s}var c=n.createContext({}),l=function(e){var o=n.useContext(c),t=o;return e&&(t="function"==typeof e?e(o):i(i({},o),e)),t},m=function(e){var o=l(e.components);return n.createElement(c.Provider,{value:o},e.children)},d={inlineCode:"code",wrapper:function(e){var o=e.children;return n.createElement(n.Fragment,{},o)}},_=n.forwardRef((function(e,o){var t=e.components,s=e.mdxType,a=e.originalType,c=e.parentName,m=r(e,["components","mdxType","originalType","parentName"]),_=l(t),p=s,u=_["".concat(c,".").concat(p)]||_[p]||d[p]||a;return t?n.createElement(u,i(i({ref:o},m),{},{components:t})):n.createElement(u,i({ref:o},m))}));function p(e,o){var t=arguments,s=o&&o.mdxType;if("string"==typeof e||s){var a=t.length,i=new Array(a);i[0]=_;var r={};for(var c in o)hasOwnProperty.call(o,c)&&(r[c]=o[c]);r.originalType=e,r.mdxType="string"==typeof e?e:s,i[1]=r;for(var l=2;l<a;l++)i[l]=t[l];return n.createElement.apply(null,i)}return n.createElement.apply(null,t)}_.displayName="MDXCreateElement"},12801:function(e,o,t){t.r(o),t.d(o,{frontMatter:function(){return r},contentTitle:function(){return c},metadata:function(){return l},toc:function(){return m},default:function(){return _}});var n=t(87462),s=t(63366),a=(t(67294),t(3905)),i=["components"],r={title:"Docker Demo",keywords:["hudi","docker","demo"],toc:!0,last_modified_at:new Date("2019-12-30T19:59:57.000Z"),language:"cn"},c=void 0,l={unversionedId:"docker_demo",id:"docker_demo",isDocsHomePage:!1,title:"Docker Demo",description:"\u4e00\u4e2a\u4f7f\u7528 Docker \u5bb9\u5668\u7684 Demo",source:"@site/i18n/cn/docusaurus-plugin-content-docs/current/docker_demo.md",sourceDirName:".",slug:"/docker_demo",permalink:"/cn/docs/next/docker_demo",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/docs/docs/docker_demo.md",version:"current",frontMatter:{title:"Docker Demo",keywords:["hudi","docker","demo"],toc:!0,last_modified_at:"2019-12-30T19:59:57.000Z",language:"cn"},sidebar:"docs",previous:{title:"JuiceFS",permalink:"/cn/docs/next/jfs_hoodie"},next:{title:"Metrics",permalink:"/cn/docs/next/metrics"}},m=[{value:"\u4e00\u4e2a\u4f7f\u7528 Docker \u5bb9\u5668\u7684 Demo",id:"\u4e00\u4e2a\u4f7f\u7528-docker-\u5bb9\u5668\u7684-demo",children:[{value:"\u524d\u63d0\u6761\u4ef6",id:"\u524d\u63d0\u6761\u4ef6",children:[]}]},{value:"\u8bbe\u7f6e Docker \u96c6\u7fa4",id:"\u8bbe\u7f6e-docker-\u96c6\u7fa4",children:[{value:"\u6784\u5efa Hudi",id:"\u6784\u5efa-hudi",children:[]},{value:"\u7ec4\u5efa Demo \u96c6\u7fa4",id:"\u7ec4\u5efa-demo-\u96c6\u7fa4",children:[]}]},{value:"Demo",id:"demo",children:[{value:"Step 1 : \u5c06\u7b2c 1 \u6279\u6570\u636e\u53d1\u5e03\u5230 Kafka",id:"step-1--\u5c06\u7b2c-1-\u6279\u6570\u636e\u53d1\u5e03\u5230-kafka",children:[]},{value:"Step 2: \u4ece Kafka Topic \u4e2d\u589e\u91cf\u91c7\u96c6\u6570\u636e",id:"step-2-\u4ece-kafka-topic-\u4e2d\u589e\u91cf\u91c7\u96c6\u6570\u636e",children:[]},{value:"Step 3: \u4e0e Hive \u540c\u6b65",id:"step-3-\u4e0e-hive-\u540c\u6b65",children:[]},{value:"Step 4 (a): \u8fd0\u884c Hive \u67e5\u8be2",id:"step-4-a-\u8fd0\u884c-hive-\u67e5\u8be2",children:[]},{value:"Step 4 (b): \u6267\u884c Spark-SQL \u67e5\u8be2",id:"step-4-b-\u6267\u884c-spark-sql-\u67e5\u8be2",children:[]},{value:"Step 4 (c): \u6267\u884c Presto \u67e5\u8be2",id:"step-4-c-\u6267\u884c-presto-\u67e5\u8be2",children:[]},{value:"Step 5: \u5c06\u7b2c 2 \u6279\u6b21\u4e0a\u4f20\u5230 Kafka \u5e76\u8fd0\u884c DeltaStreamer \u8fdb\u884c\u91c7\u96c6",id:"step-5-\u5c06\u7b2c-2-\u6279\u6b21\u4e0a\u4f20\u5230-kafka-\u5e76\u8fd0\u884c-deltastreamer-\u8fdb\u884c\u91c7\u96c6",children:[]},{value:"Step 6 (a): \u6267\u884c Hive \u67e5\u8be2",id:"step-6-a-\u6267\u884c-hive-\u67e5\u8be2",children:[]},{value:"Step 6 (b): \u6267\u884c Spark SQL \u67e5\u8be2",id:"step-6-b-\u6267\u884c-spark-sql-\u67e5\u8be2",children:[]},{value:"Step 6 (c): \u6267\u884c Presto \u67e5\u8be2",id:"step-6-c-\u6267\u884c-presto-\u67e5\u8be2",children:[]},{value:"Step 7 : \u5199\u65f6\u590d\u5236\u8868\u7684\u589e\u91cf\u67e5\u8be2",id:"step-7--\u5199\u65f6\u590d\u5236\u8868\u7684\u589e\u91cf\u67e5\u8be2",children:[]},{value:"\u4f7f\u7528 Spark SQL \u505a\u589e\u91cf\u67e5\u8be2",id:"\u4f7f\u7528-spark-sql-\u505a\u589e\u91cf\u67e5\u8be2",children:[]},{value:"Step 8: \u4e3a\u8bfb\u65f6\u5408\u5e76\u6570\u636e\u96c6\u7684\u8c03\u5ea6\u5e76\u6267\u884c\u538b\u7f29",id:"step-8-\u4e3a\u8bfb\u65f6\u5408\u5e76\u6570\u636e\u96c6\u7684\u8c03\u5ea6\u5e76\u6267\u884c\u538b\u7f29",children:[]},{value:"Step 9: \u6267\u884c\u5305\u542b\u589e\u91cf\u67e5\u8be2\u7684 Hive \u67e5\u8be2",id:"step-9-\u6267\u884c\u5305\u542b\u589e\u91cf\u67e5\u8be2\u7684-hive-\u67e5\u8be2",children:[]},{value:"Step 10: \u538b\u7f29\u540e\u5728 MOR \u7684\u8bfb\u4f18\u5316\u89c6\u56fe\u4e0e\u5b9e\u65f6\u89c6\u56fe\u4e0a\u4f7f\u7528 Spark-SQL",id:"step-10-\u538b\u7f29\u540e\u5728-mor-\u7684\u8bfb\u4f18\u5316\u89c6\u56fe\u4e0e\u5b9e\u65f6\u89c6\u56fe\u4e0a\u4f7f\u7528-spark-sql",children:[]},{value:"Step 11:  \u538b\u7f29\u540e\u5728 MOR \u6570\u636e\u96c6\u7684\u8bfb\u4f18\u5316\u89c6\u56fe\u4e0a\u8fdb\u884c Presto \u67e5\u8be2",id:"step-11--\u538b\u7f29\u540e\u5728-mor-\u6570\u636e\u96c6\u7684\u8bfb\u4f18\u5316\u89c6\u56fe\u4e0a\u8fdb\u884c-presto-\u67e5\u8be2",children:[]}]},{value:"\u5728\u672c\u5730 Docker \u73af\u5883\u4e2d\u6d4b\u8bd5 Hudi",id:"\u5728\u672c\u5730-docker-\u73af\u5883\u4e2d\u6d4b\u8bd5-hudi",children:[{value:"\u6784\u5efa\u672c\u5730 Docker \u5bb9\u5668:",id:"\u6784\u5efa\u672c\u5730-docker-\u5bb9\u5668",children:[]}]}],d={toc:m};function _(e){var o=e.components,t=(0,s.Z)(e,i);return(0,a.kt)("wrapper",(0,n.Z)({},d,t,{components:o,mdxType:"MDXLayout"}),(0,a.kt)("h2",{id:"\u4e00\u4e2a\u4f7f\u7528-docker-\u5bb9\u5668\u7684-demo"},"\u4e00\u4e2a\u4f7f\u7528 Docker \u5bb9\u5668\u7684 Demo"),(0,a.kt)("p",null,"\u6211\u4eec\u6765\u4f7f\u7528\u4e00\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u6848\u4f8b\uff0c\u6765\u770b\u770b Hudi \u662f\u5982\u4f55\u95ed\u73af\u8fd0\u8f6c\u7684\u3002 \u4e3a\u4e86\u8fd9\u4e2a\u76ee\u7684\uff0c\u5728\u4f60\u7684\u8ba1\u7b97\u673a\u4e2d\u7684\u672c\u5730 Docker \u96c6\u7fa4\u4e2d\u7ec4\u5efa\u4e86\u4e00\u4e2a\u81ea\u5305\u542b\u7684\u6570\u636e\u57fa\u7840\u8bbe\u65bd\u3002"),(0,a.kt)("p",null,"\u4ee5\u4e0b\u6b65\u9aa4\u5df2\u7ecf\u5728\u4e00\u53f0 Mac \u7b14\u8bb0\u672c\u7535\u8111\u4e0a\u6d4b\u8bd5\u8fc7\u4e86\u3002"),(0,a.kt)("h3",{id:"\u524d\u63d0\u6761\u4ef6"},"\u524d\u63d0\u6761\u4ef6"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Docker \u5b89\u88c5 :  \u5bf9\u4e8e Mac \uff0c\u8bf7\u4f9d\u7167 ","[https://docs.docker.com/v17.12/docker-for-mac/install/]"," \u5f53\u4e2d\u5b9a\u4e49\u7684\u6b65\u9aa4\u3002 \u4e3a\u4e86\u8fd0\u884c Spark-SQL \u67e5\u8be2\uff0c\u8bf7\u786e\u4fdd\u81f3\u5c11\u5206\u914d\u7ed9 Docker 6 GB \u548c 4 \u4e2a CPU \u3002\uff08\u53c2\u89c1 Docker -> Preferences -> Advanced\uff09\u3002\u5426\u5219\uff0cSpark-SQL \u67e5\u8be2\u53ef\u80fd\u88ab\u56e0\u4e3a\u5185\u5b58\u95ee\u9898\u800c\u88ab\u6740\u505c\u3002"),(0,a.kt)("li",{parentName:"ul"},"kafkacat : \u4e00\u4e2a\u7528\u4e8e\u53d1\u5e03/\u6d88\u8d39 Kafka Topic \u7684\u547d\u4ee4\u884c\u5de5\u5177\u96c6\u3002\u4f7f\u7528 ",(0,a.kt)("inlineCode",{parentName:"li"},"brew install kafkacat")," \u6765\u5b89\u88c5 kafkacat \u3002"),(0,a.kt)("li",{parentName:"ul"},"/etc/hosts : Demo \u901a\u8fc7\u4e3b\u673a\u540d\u5f15\u7528\u4e86\u591a\u4e2a\u8fd0\u884c\u5728\u5bb9\u5668\u4e2d\u7684\u670d\u52a1\u3002\u5c06\u4e0b\u5217\u8bbe\u7f6e\u6dfb\u52a0\u5230 /etc/hosts \uff1a")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},"   127.0.0.1 adhoc-1\n   127.0.0.1 adhoc-2\n   127.0.0.1 namenode\n   127.0.0.1 datanode1\n   127.0.0.1 hiveserver\n   127.0.0.1 hivemetastore\n   127.0.0.1 kafkabroker\n   127.0.0.1 sparkmaster\n   127.0.0.1 zookeeper\n")),(0,a.kt)("p",null,"\u6b64\u5916\uff0c\u8fd9\u672a\u5728\u5176\u5b83\u4e00\u4e9b\u73af\u5883\u4e2d\u8fdb\u884c\u6d4b\u8bd5\uff0c\u4f8b\u5982 Windows \u4e0a\u7684 Docker \u3002"),(0,a.kt)("h2",{id:"\u8bbe\u7f6e-docker-\u96c6\u7fa4"},"\u8bbe\u7f6e Docker \u96c6\u7fa4"),(0,a.kt)("h3",{id:"\u6784\u5efa-hudi"},"\u6784\u5efa Hudi"),(0,a.kt)("p",null,"\u6784\u5efa Hudi \u7684\u7b2c\u4e00\u6b65\uff1a"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},"cd <HUDI_WORKSPACE>\nmvn package -DskipTests\n")),(0,a.kt)("h3",{id:"\u7ec4\u5efa-demo-\u96c6\u7fa4"},"\u7ec4\u5efa Demo \u96c6\u7fa4"),(0,a.kt)("p",null,"\u4e0b\u4e00\u6b65\u662f\u8fd0\u884c Docker \u5b89\u88c5\u811a\u672c\u5e76\u8bbe\u7f6e\u914d\u7f6e\u9879\u4ee5\u4fbf\u7ec4\u5efa\u96c6\u7fa4\u3002\n\u8fd9\u9700\u8981\u4ece Docker \u955c\u50cf\u5e93\u62c9\u53d6 Docker \u955c\u50cf\uff0c\u5e76\u8bbe\u7f6e Docker \u96c6\u7fa4\u3002"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},'cd docker\n./setup_demo.sh\n....\n....\n....\nStopping spark-worker-1            ... done\nStopping hiveserver                ... done\nStopping hivemetastore             ... done\nStopping historyserver             ... done\n.......\n......\nCreating network "hudi_demo" with the default driver\nCreating hive-metastore-postgresql ... done\nCreating namenode                  ... done\nCreating zookeeper                 ... done\nCreating kafkabroker               ... done\nCreating hivemetastore             ... done\nCreating historyserver             ... done\nCreating hiveserver                ... done\nCreating datanode1                 ... done\nCreating presto-coordinator-1      ... done\nCreating sparkmaster               ... done\nCreating presto-worker-1           ... done\nCreating adhoc-1                   ... done\nCreating adhoc-2                   ... done\nCreating spark-worker-1            ... done\nCopying spark default config and setting up configs\nCopying spark default config and setting up configs\nCopying spark default config and setting up configs\n$ docker ps\n')),(0,a.kt)("p",null,"\u81f3\u6b64\uff0c Docker \u96c6\u7fa4\u5c06\u4f1a\u542f\u52a8\u5e76\u8fd0\u884c\u3002 Demo \u96c6\u7fa4\u63d0\u4f9b\u4e86\u4e0b\u5217\u670d\u52a1\uff1a"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"HDFS \u670d\u52a1\uff08 NameNode, DataNode \uff09"),(0,a.kt)("li",{parentName:"ul"},"Spark Master \u548c Worker"),(0,a.kt)("li",{parentName:"ul"},"Hive \u670d\u52a1\uff08 Metastore, HiveServer2 \u4ee5\u53ca PostgresDB \uff09"),(0,a.kt)("li",{parentName:"ul"},"Kafka Broker \u548c\u4e00\u4e2a Zookeeper Node \uff08 Kafka \u5c06\u88ab\u7528\u6765\u5f53\u505a Demo \u7684\u4e0a\u6e38\u6570\u636e\u6e90 \uff09"),(0,a.kt)("li",{parentName:"ul"},"\u7528\u6765\u8fd0\u884c Hudi/Hive CLI \u547d\u4ee4\u7684 Adhoc \u5bb9\u5668")),(0,a.kt)("h2",{id:"demo"},"Demo"),(0,a.kt)("p",null,"Stock Tracker \u6570\u636e\u5c06\u7528\u6765\u5c55\u793a\u4e0d\u540c\u7684 Hudi \u89c6\u56fe\u4ee5\u53ca\u538b\u7f29\u5e26\u6765\u7684\u5f71\u54cd\u3002"),(0,a.kt)("p",null,"\u770b\u4e00\u4e0b ",(0,a.kt)("inlineCode",{parentName:"p"},"docker/demo/data")," \u76ee\u5f55\u3002\u90a3\u91cc\u6709 2 \u6279\u80a1\u7968\u6570\u636e\u2014\u2014\u90fd\u662f 1 \u5206\u949f\u7c92\u5ea6\u7684\u3002\n\u7b2c 1 \u6279\u6570\u636e\u5305\u542b\u4e00\u4e9b\u80a1\u7968\u4ee3\u7801\u5728\u4ea4\u6613\u7a97\u53e3\uff089:30 a.m \u81f3 10:30 a.m\uff09\u7684\u7b2c\u4e00\u4e2a\u5c0f\u65f6\u91cc\u7684\u884c\u60c5\u6570\u636e\u6570\u636e\u3002\u7b2c 2 \u6279\u5305\u542b\u63a5\u4e0b\u6765 30 \u5206\u949f\uff0810:30 - 11 a.m\uff09\u7684\u4ea4\u6613\u6570\u636e\u3002 Hudi \u5c06\u88ab\u7528\u6765\u5c06\u4e24\u4e2a\u6279\u6b21\u7684\u6570\u636e\u91c7\u96c6\u5230\u4e00\u4e2a\u6570\u636e\u96c6\u4e2d\uff0c\u8fd9\u4e2a\u6570\u636e\u96c6\u5c06\u4f1a\u5305\u542b\u6700\u65b0\u7684\u5c0f\u65f6\u7ea7\u80a1\u7968\u884c\u60c5\u6570\u636e\u3002\n\u4e24\u4e2a\u6279\u6b21\u88ab\u6709\u610f\u5730\u6309\u7a97\u53e3\u5207\u5206\uff0c\u8fd9\u6837\u5728\u7b2c 2 \u6279\u6570\u636e\u4e2d\u5305\u542b\u4e86\u4e00\u4e9b\u9488\u5bf9\u7b2c 1 \u6279\u6570\u636e\u6761\u76ee\u7684\u66f4\u65b0\u6570\u636e\u3002"),(0,a.kt)("h3",{id:"step-1--\u5c06\u7b2c-1-\u6279\u6570\u636e\u53d1\u5e03\u5230-kafka"},"Step 1 : \u5c06\u7b2c 1 \u6279\u6570\u636e\u53d1\u5e03\u5230 Kafka"),(0,a.kt)("p",null,"\u5c06\u7b2c 1 \u6279\u6570\u636e\u4e0a\u4f20\u5230 Kafka \u7684 Topic \u201cstock ticks\u201d \u4e2d ",(0,a.kt)("inlineCode",{parentName:"p"},"cat docker/demo/data/batch_1.json | kafkacat -b kafkabroker -t stock_ticks -P")),(0,a.kt)("p",null,"\u4e3a\u4e86\u68c0\u67e5\u65b0\u7684 Topic \u662f\u5426\u51fa\u73b0\uff0c\u4f7f\u7528"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},'kafkacat -b kafkabroker -L -J | jq .\n{\n  "originating_broker": {\n    "id": 1001,\n    "name": "kafkabroker:9092/1001"\n  },\n  "query": {\n    "topic": "*"\n  },\n  "brokers": [\n    {\n      "id": 1001,\n      "name": "kafkabroker:9092"\n    }\n  ],\n  "topics": [\n    {\n      "topic": "stock_ticks",\n      "partitions": [\n        {\n          "partition": 0,\n          "leader": 1001,\n          "replicas": [\n            {\n              "id": 1001\n            }\n          ],\n          "isrs": [\n            {\n              "id": 1001\n            }\n          ]\n        }\n      ]\n    }\n  ]\n}\n\n')),(0,a.kt)("h3",{id:"step-2-\u4ece-kafka-topic-\u4e2d\u589e\u91cf\u91c7\u96c6\u6570\u636e"},"Step 2: \u4ece Kafka Topic \u4e2d\u589e\u91cf\u91c7\u96c6\u6570\u636e"),(0,a.kt)("p",null,"Hudi \u81ea\u5e26\u4e00\u4e2a\u540d\u4e3a DeltaStreamer \u7684\u5de5\u5177\u3002 \u8fd9\u4e2a\u5de5\u5177\u80fd\u8fde\u63a5\u591a\u79cd\u6570\u636e\u6e90\uff08\u5305\u62ec Kafka\uff09\uff0c\u4ee5\u4fbf\u62c9\u53d6\u53d8\u66f4\uff0c\u5e76\u901a\u8fc7 upsert/insert \u64cd\u4f5c\u5e94\u7528\u5230 Hudi \u6570\u636e\u96c6\u3002\u6b64\u5904\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u8fd9\u4e2a\u5de5\u5177\u4ece Kafka Topic \u4e0b\u8f7d JSON \u6570\u636e\uff0c\u5e76\u91c7\u96c6\u5230\u524d\u9762\u6b65\u9aa4\u4e2d\u521d\u59cb\u5316\u7684 COW \u548c MOR \u8868\u4e2d\u3002\u5982\u679c\u6570\u636e\u96c6\u4e0d\u5b58\u5728\uff0c\u8fd9\u4e2a\u5de5\u5177\u5c06\u81ea\u52a8\u521d\u59cb\u5316\u6570\u636e\u96c6\u5230\u6587\u4ef6\u7cfb\u7edf\u4e2d\u3002"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},"docker exec -it adhoc-2 /bin/bash\n\n# Run the following spark-submit command to execute the delta-streamer and ingest to stock_ticks_cow dataset in HDFS\nspark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer $HUDI_UTILITIES_BUNDLE --storage-type COPY_ON_WRITE --source-class org.apache.hudi.utilities.sources.JsonKafkaSource --source-ordering-field ts  --target-base-path /user/hive/warehouse/stock_ticks_cow --target-table stock_ticks_cow --props /var/demo/config/kafka-source.properties --schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider\n\n\n# Run the following spark-submit command to execute the delta-streamer and ingest to stock_ticks_mor dataset in HDFS\nspark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer $HUDI_UTILITIES_BUNDLE --storage-type MERGE_ON_READ --source-class org.apache.hudi.utilities.sources.JsonKafkaSource --source-ordering-field ts  --target-base-path /user/hive/warehouse/stock_ticks_mor --target-table stock_ticks_mor --props /var/demo/config/kafka-source.properties --schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider --disable-compaction\n\n\n# As part of the setup (Look at setup_demo.sh), the configs needed for DeltaStreamer is uploaded to HDFS. The configs\n# contain mostly Kafa connectivity settings, the avro-schema to be used for ingesting along with key and partitioning fields.\n\nexit\n")),(0,a.kt)("p",null,"\u4f60\u53ef\u4ee5\u4f7f\u7528 HDFS \u7684 Web \u6d4f\u89c8\u5668\u6765\u67e5\u770b\u6570\u636e\u96c6\n",(0,a.kt)("inlineCode",{parentName:"p"},"http://namenode:50070/explorer#/user/hive/warehouse/stock_ticks_cow"),"."),(0,a.kt)("p",null,"\u4f60\u53ef\u4ee5\u6d4f\u89c8\u5728\u6570\u636e\u96c6\u4e2d\u65b0\u521b\u5efa\u7684\u5206\u533a\u6587\u4ef6\u5939\uff0c\u540c\u65f6\u8fd8\u6709\u4e00\u4e2a\u5728 .hoodie \u76ee\u5f55\u4e0b\u7684 deltacommit \u6587\u4ef6\u3002"),(0,a.kt)("p",null,"\u5728 MOR \u6570\u636e\u96c6\u4e2d\u4e5f\u6709\u7c7b\u4f3c\u7684\u8bbe\u7f6e\n",(0,a.kt)("inlineCode",{parentName:"p"},"http://namenode:50070/explorer#/user/hive/warehouse/stock_ticks_mor")),(0,a.kt)("h3",{id:"step-3-\u4e0e-hive-\u540c\u6b65"},"Step 3: \u4e0e Hive \u540c\u6b65"),(0,a.kt)("p",null,"\u5230\u4e86\u8fd9\u4e00\u6b65\uff0c\u6570\u636e\u96c6\u5728 HDFS \u4e2d\u53ef\u7528\u3002\u6211\u4eec\u9700\u8981\u4e0e Hive \u540c\u6b65\u6765\u521b\u5efa\u65b0 Hive \u8868\u5e76\u6dfb\u52a0\u5206\u533a\uff0c\u4ee5\u4fbf\u5728\u90a3\u4e9b\u6570\u636e\u96c6\u4e0a\u6267\u884c Hive \u67e5\u8be2\u3002"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},"docker exec -it adhoc-2 /bin/bash\n\n# THis command takes in HIveServer URL and COW Hudi Dataset location in HDFS and sync the HDFS state to Hive\n/var/hoodie/ws/hudi-sync/hudi-hive-sync/run_sync_tool.sh  --jdbc-url jdbc:hive2://hiveserver:10000 --user hive --pass hive --partitioned-by dt --base-path /user/hive/warehouse/stock_ticks_cow --database default --table stock_ticks_cow\n.....\n2018-09-24 22:22:45,568 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(112)) - Sync complete for stock_ticks_cow\n.....\n\n# Now run hive-sync for the second data-set in HDFS using Merge-On-Read (MOR storage)\n/var/hoodie/ws/hudi-sync/hudi-hive-sync/run_sync_tool.sh  --jdbc-url jdbc:hive2://hiveserver:10000 --user hive --pass hive --partitioned-by dt --base-path /user/hive/warehouse/stock_ticks_mor --database default --table stock_ticks_mor\n...\n2018-09-24 22:23:09,171 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(112)) - Sync complete for stock_ticks_mor\n...\n2018-09-24 22:23:09,559 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(112)) - Sync complete for stock_ticks_mor_rt\n....\nexit\n")),(0,a.kt)("p",null,"\u6267\u884c\u4e86\u4ee5\u4e0a\u547d\u4ee4\u540e\uff0c\u4f60\u4f1a\u53d1\u73b0\uff1a"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"\u4e00\u4e2a\u540d\u4e3a ",(0,a.kt)("inlineCode",{parentName:"li"},"stock_ticks_cow")," \u7684 Hive \u8868\u88ab\u521b\u5efa\uff0c\u5b83\u4e3a\u5199\u65f6\u590d\u5236\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u8bfb\u4f18\u5316\u89c6\u56fe\u3002"),(0,a.kt)("li",{parentName:"ol"},"\u4e24\u4e2a\u65b0\u8868 ",(0,a.kt)("inlineCode",{parentName:"li"},"stock_ticks_mor")," \u548c ",(0,a.kt)("inlineCode",{parentName:"li"},"stock_ticks_mor_rt")," \u88ab\u521b\u5efa\u7528\u4e8e\u8bfb\u65f6\u5408\u5e76\u6570\u636e\u96c6\u3002 \u524d\u8005\u4e3a Hudi \u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u8bfb\u4f18\u5316\u89c6\u56fe\uff0c\u800c\u540e\u8005\u4e3a\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u5b9e\u65f6\u89c6\u56fe\u3002")),(0,a.kt)("h3",{id:"step-4-a-\u8fd0\u884c-hive-\u67e5\u8be2"},"Step 4 (a): \u8fd0\u884c Hive \u67e5\u8be2"),(0,a.kt)("p",null,"\u6267\u884c\u4e00\u4e2a Hive \u67e5\u8be2\u6765\u4e3a\u80a1\u7968 GOOG \u627e\u5230\u91c7\u96c6\u5230\u7684\u6700\u65b0\u65f6\u95f4\u6233\u3002\u4f60\u4f1a\u6ce8\u610f\u5230\u8bfb\u4f18\u5316\u89c6\u56fe\uff08 COW \u548c MOR \u6570\u636e\u96c6\u90fd\u662f\u5982\u6b64\uff09\u548c\u5b9e\u65f6\u89c6\u56fe\uff08\u4ec5\u5bf9 MOR \u6570\u636e\u96c6\uff09\u7ed9\u51fa\u4e86\u76f8\u540c\u7684\u503c \u201c10:29 a.m\u201d\uff0c\u8fd9\u662f\u56e0\u4e3a Hudi \u4e3a\u6bcf\u4e2a\u6279\u6b21\u7684\u6570\u636e\u521b\u5efa\u4e86\u4e00\u4e2a Parquet \u6587\u4ef6\u3002"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},"docker exec -it adhoc-2 /bin/bash\nbeeline -u jdbc:hive2://hiveserver:10000 --hiveconf hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat --hiveconf hive.stats.autogather=false\n# List Tables\n0: jdbc:hive2://hiveserver:10000> show tables;\n+---------------------+--+\n|      tab_name       |\n+---------------------+--+\n| stock_ticks_cow     |\n| stock_ticks_mor     |\n| stock_ticks_mor_rt  |\n+---------------------+--+\n2 rows selected (0.801 seconds)\n0: jdbc:hive2://hiveserver:10000>\n\n\n# Look at partitions that were added\n0: jdbc:hive2://hiveserver:10000> show partitions stock_ticks_mor_rt;\n+----------------+--+\n|   partition    |\n+----------------+--+\n| dt=2018-08-31  |\n+----------------+--+\n1 row selected (0.24 seconds)\n\n\n# COPY-ON-WRITE Queries:\n=========================\n\n\n0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_cow group by symbol HAVING symbol = 'GOOG';\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:29:00  |\n+---------+----------------------+--+\n\nNow, run a projection query:\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_cow where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924221953       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924221953       | GOOG    | 2018-08-31 10:29:00  | 3391    | 1230.1899  | 1230.085  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\n\n# Merge-On-Read Queries:\n==========================\n\nLets run similar queries against M-O-R dataset. Lets look at both\nReadOptimized and Realtime views supported by M-O-R dataset\n\n# Run against ReadOptimized View. Notice that the latest timestamp is 10:29\n0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_mor group by symbol HAVING symbol = 'GOOG';\nWARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:29:00  |\n+---------+----------------------+--+\n1 row selected (6.326 seconds)\n\n\n# Run against Realtime View. Notice that the latest timestamp is again 10:29\n\n0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_mor_rt group by symbol HAVING symbol = 'GOOG';\nWARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:29:00  |\n+---------+----------------------+--+\n1 row selected (1.606 seconds)\n\n\n# Run projection query against Read Optimized and Realtime tables\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924222155       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924222155       | GOOG    | 2018-08-31 10:29:00  | 3391    | 1230.1899  | 1230.085  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_rt where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924222155       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924222155       | GOOG    | 2018-08-31 10:29:00  | 3391    | 1230.1899  | 1230.085  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\nexit\nexit\n")),(0,a.kt)("h3",{id:"step-4-b-\u6267\u884c-spark-sql-\u67e5\u8be2"},"Step 4 (b): \u6267\u884c Spark-SQL \u67e5\u8be2"),(0,a.kt)("p",null,"Hudi \u652f\u6301\u4ee5 Spark \u4f5c\u4e3a\u7c7b\u4f3c Hive \u7684\u67e5\u8be2\u5f15\u64ce\u3002\u8fd9\u662f\u5728 Spartk-SQL \u4e2d\u6267\u884c\u4e0e Hive \u76f8\u540c\u7684\u67e5\u8be2"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},'docker exec -it adhoc-1 /bin/bash\n$SPARK_INSTALL/bin/spark-shell --jars $HUDI_SPARK_BUNDLE --master local[2] --driver-class-path $HADOOP_CONF_DIR --conf spark.sql.hive.convertMetastoreParquet=false --deploy-mode client  --driver-memory 1G --executor-memory 3G --num-executors 1  --packages com.databricks:spark-avro_2.11:4.0.0\n...\n\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  \'_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.3.1\n      /_/\n\nUsing Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_181)\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala>\nscala> spark.sql("show tables").show(100, false)\n+--------+------------------+-----------+\n|database|tableName         |isTemporary|\n+--------+------------------+-----------+\n|default |stock_ticks_cow   |false      |\n|default |stock_ticks_mor   |false      |\n|default |stock_ticks_mor_rt|false      |\n+--------+------------------+-----------+\n\n# Copy-On-Write Table\n\n## Run max timestamp query against COW table\n\nscala> spark.sql("select symbol, max(ts) from stock_ticks_cow group by symbol HAVING symbol = \'GOOG\'").show(100, false)\n[Stage 0:>                                                          (0 + 1) / 1]SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes#StaticLoggerBinder for further details.\n+------+-------------------+\n|symbol|max(ts)            |\n+------+-------------------+\n|GOOG  |2018-08-31 10:29:00|\n+------+-------------------+\n\n## Projection Query\n\nscala> spark.sql("select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_cow where  symbol = \'GOOG\'").show(100, false)\n+-------------------+------+-------------------+------+---------+--------+\n|_hoodie_commit_time|symbol|ts                 |volume|open     |close   |\n+-------------------+------+-------------------+------+---------+--------+\n|20180924221953     |GOOG  |2018-08-31 09:59:00|6330  |1230.5   |1230.02 |\n|20180924221953     |GOOG  |2018-08-31 10:29:00|3391  |1230.1899|1230.085|\n+-------------------+------+-------------------+------+---------+--------+\n\n# Merge-On-Read Queries:\n==========================\n\nLets run similar queries against M-O-R dataset. Lets look at both\nReadOptimized and Realtime views supported by M-O-R dataset\n\n# Run against ReadOptimized View. Notice that the latest timestamp is 10:29\nscala> spark.sql("select symbol, max(ts) from stock_ticks_mor group by symbol HAVING symbol = \'GOOG\'").show(100, false)\n+------+-------------------+\n|symbol|max(ts)            |\n+------+-------------------+\n|GOOG  |2018-08-31 10:29:00|\n+------+-------------------+\n\n\n# Run against Realtime View. Notice that the latest timestamp is again 10:29\n\nscala> spark.sql("select symbol, max(ts) from stock_ticks_mor_rt group by symbol HAVING symbol = \'GOOG\'").show(100, false)\n+------+-------------------+\n|symbol|max(ts)            |\n+------+-------------------+\n|GOOG  |2018-08-31 10:29:00|\n+------+-------------------+\n\n# Run projection query against Read Optimized and Realtime tables\n\nscala> spark.sql("select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor where  symbol = \'GOOG\'").show(100, false)\n+-------------------+------+-------------------+------+---------+--------+\n|_hoodie_commit_time|symbol|ts                 |volume|open     |close   |\n+-------------------+------+-------------------+------+---------+--------+\n|20180924222155     |GOOG  |2018-08-31 09:59:00|6330  |1230.5   |1230.02 |\n|20180924222155     |GOOG  |2018-08-31 10:29:00|3391  |1230.1899|1230.085|\n+-------------------+------+-------------------+------+---------+--------+\n\nscala> spark.sql("select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_rt where  symbol = \'GOOG\'").show(100, false)\n+-------------------+------+-------------------+------+---------+--------+\n|_hoodie_commit_time|symbol|ts                 |volume|open     |close   |\n+-------------------+------+-------------------+------+---------+--------+\n|20180924222155     |GOOG  |2018-08-31 09:59:00|6330  |1230.5   |1230.02 |\n|20180924222155     |GOOG  |2018-08-31 10:29:00|3391  |1230.1899|1230.085|\n+-------------------+------+-------------------+------+---------+--------+\n\n')),(0,a.kt)("h3",{id:"step-4-c-\u6267\u884c-presto-\u67e5\u8be2"},"Step 4 (c): \u6267\u884c Presto \u67e5\u8be2"),(0,a.kt)("p",null,"\u8fd9\u662f Presto \u67e5\u8be2\uff0c\u5b83\u4eec\u4e0e Hive \u548c Spark \u7684\u67e5\u8be2\u7c7b\u4f3c\u3002\u76ee\u524d Hudi \u7684\u5b9e\u65f6\u89c6\u56fe\u4e0d\u652f\u6301 Presto \u3002"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},"docker exec -it presto-worker-1 presto --server presto-coordinator-1:8090\npresto> show catalogs;\n  Catalog\n-----------\n hive\n jmx\n localfile\n system\n(4 rows)\n\nQuery 20190817_134851_00000_j8rcz, FINISHED, 1 node\nSplits: 19 total, 19 done (100.00%)\n0:04 [0 rows, 0B] [0 rows/s, 0B/s]\n\npresto> use hive.default;\nUSE\npresto:default> show tables;\n       Table\n--------------------\n stock_ticks_cow\n stock_ticks_mor\n stock_ticks_mor_rt\n(3 rows)\n\nQuery 20190822_181000_00001_segyw, FINISHED, 2 nodes\nSplits: 19 total, 19 done (100.00%)\n0:05 [3 rows, 99B] [0 rows/s, 18B/s]\n\n\n# COPY-ON-WRITE Queries:\n=========================\n\n\npresto:default> select symbol, max(ts) from stock_ticks_cow group by symbol HAVING symbol = 'GOOG';\n symbol |        _col1\n--------+---------------------\n GOOG   | 2018-08-31 10:29:00\n(1 row)\n\nQuery 20190822_181011_00002_segyw, FINISHED, 1 node\nSplits: 49 total, 49 done (100.00%)\n0:12 [197 rows, 613B] [16 rows/s, 50B/s]\n\npresto:default> select \"_hoodie_commit_time\", symbol, ts, volume, open, close from stock_ticks_cow where symbol = 'GOOG';\n _hoodie_commit_time | symbol |         ts          | volume |   open    |  close\n---------------------+--------+---------------------+--------+-----------+----------\n 20190822180221      | GOOG   | 2018-08-31 09:59:00 |   6330 |    1230.5 |  1230.02\n 20190822180221      | GOOG   | 2018-08-31 10:29:00 |   3391 | 1230.1899 | 1230.085\n(2 rows)\n\nQuery 20190822_181141_00003_segyw, FINISHED, 1 node\nSplits: 17 total, 17 done (100.00%)\n0:02 [197 rows, 613B] [109 rows/s, 341B/s]\n\n\n# Merge-On-Read Queries:\n==========================\n\nLets run similar queries against M-O-R dataset. \n\n# Run against ReadOptimized View. Notice that the latest timestamp is 10:29\npresto:default> select symbol, max(ts) from stock_ticks_mor group by symbol HAVING symbol = 'GOOG';\n symbol |        _col1\n--------+---------------------\n GOOG   | 2018-08-31 10:29:00\n(1 row)\n\nQuery 20190822_181158_00004_segyw, FINISHED, 1 node\nSplits: 49 total, 49 done (100.00%)\n0:02 [197 rows, 613B] [110 rows/s, 343B/s]\n\n\npresto:default>  select \"_hoodie_commit_time\", symbol, ts, volume, open, close  from stock_ticks_mor where  symbol = 'GOOG';\n _hoodie_commit_time | symbol |         ts          | volume |   open    |  close\n---------------------+--------+---------------------+--------+-----------+----------\n 20190822180250      | GOOG   | 2018-08-31 09:59:00 |   6330 |    1230.5 |  1230.02\n 20190822180250      | GOOG   | 2018-08-31 10:29:00 |   3391 | 1230.1899 | 1230.085\n(2 rows)\n\nQuery 20190822_181256_00006_segyw, FINISHED, 1 node\nSplits: 17 total, 17 done (100.00%)\n0:02 [197 rows, 613B] [92 rows/s, 286B/s]\n\npresto:default> exit\n")),(0,a.kt)("h3",{id:"step-5-\u5c06\u7b2c-2-\u6279\u6b21\u4e0a\u4f20\u5230-kafka-\u5e76\u8fd0\u884c-deltastreamer-\u8fdb\u884c\u91c7\u96c6"},"Step 5: \u5c06\u7b2c 2 \u6279\u6b21\u4e0a\u4f20\u5230 Kafka \u5e76\u8fd0\u884c DeltaStreamer \u8fdb\u884c\u91c7\u96c6"),(0,a.kt)("p",null,"\u4e0a\u4f20\u7b2c 2 \u6279\u6b21\u6570\u636e\uff0c\u5e76\u4f7f\u7528 DeltaStreamer \u91c7\u96c6\u3002\u7531\u4e8e\u8fd9\u4e2a\u6279\u6b21\u4e0d\u4f1a\u5f15\u5165\u4efb\u4f55\u65b0\u5206\u533a\uff0c\u56e0\u6b64\u4e0d\u9700\u8981\u6267\u884c Hive \u540c\u6b65\u3002"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},"cat docker/demo/data/batch_2.json | kafkacat -b kafkabroker -t stock_ticks -P\n\n# Within Docker container, run the ingestion command\ndocker exec -it adhoc-2 /bin/bash\n\n# Run the following spark-submit command to execute the delta-streamer and ingest to stock_ticks_cow dataset in HDFS\nspark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer $HUDI_UTILITIES_BUNDLE --storage-type COPY_ON_WRITE --source-class org.apache.hudi.utilities.sources.JsonKafkaSource --source-ordering-field ts  --target-base-path /user/hive/warehouse/stock_ticks_cow --target-table stock_ticks_cow --props /var/demo/config/kafka-source.properties --schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider\n\n\n# Run the following spark-submit command to execute the delta-streamer and ingest to stock_ticks_mor dataset in HDFS\nspark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer $HUDI_UTILITIES_BUNDLE --storage-type MERGE_ON_READ --source-class org.apache.hudi.utilities.sources.JsonKafkaSource --source-ordering-field ts  --target-base-path /user/hive/warehouse/stock_ticks_mor --target-table stock_ticks_mor --props /var/demo/config/kafka-source.properties --schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider --disable-compaction\n\nexit\n")),(0,a.kt)("p",null,"\u4f7f\u7528\u5199\u65f6\u590d\u5236\u8868\uff0c DeltaStreamer \u7684\u7b2c 2 \u6279\u6570\u636e\u91c7\u96c6\u5c06\u5bfc\u81f4 Parquet \u6587\u4ef6\u521b\u5efa\u4e00\u4e2a\u65b0\u7248\u672c\u3002\n\u53c2\u8003\uff1a ",(0,a.kt)("inlineCode",{parentName:"p"},"http://namenode:50070/explorer#/user/hive/warehouse/stock_ticks_cow/2018/08/31")),(0,a.kt)("p",null,"\u4f7f\u7528\u8bfb\u65f6\u5408\u5e76\u8868, \u7b2c 2 \u6279\u6570\u636e\u91c7\u96c6\u4ec5\u4ec5\u5c06\u6570\u636e\u8ffd\u52a0\u5230\u6ca1\u6709\u5408\u5e76\u7684 delta \uff08\u65e5\u5fd7\uff09 \u6587\u4ef6\u4e2d\u3002\u770b\u4e00\u4e0b HDFS \u6587\u4ef6\u7cfb\u7edf\u6765\u4e86\u89e3\u8fd9\u4e00\u70b9\uff1a ",(0,a.kt)("inlineCode",{parentName:"p"},"http://namenode:50070/explorer#/user/hive/warehouse/stock_ticks_mor/2018/08/31")),(0,a.kt)("h3",{id:"step-6-a-\u6267\u884c-hive-\u67e5\u8be2"},"Step 6 (a): \u6267\u884c Hive \u67e5\u8be2"),(0,a.kt)("p",null,"\u4f7f\u7528\u5199\u65f6\u590d\u5236\u8868\uff0c\u5728\u6bcf\u4e00\u4e2a\u6279\u6b21\u88ab\u63d0\u4ea4\u91c7\u96c6\u5e76\u521b\u5efa\u65b0\u7248\u672c\u7684 Parquet \u6587\u4ef6\u65f6\uff0c\u8bfb\u4f18\u5316\u89c6\u56fe\u4f1a\u7acb\u5373\u53d1\u73b0\u53d8\u66f4\uff0c\u8fd9\u4e9b\u53d8\u66f4\u88ab\u5f53\u7b2c 2 \u6279\u6b21\u7684\u4e00\u90e8\u5206\u3002"),(0,a.kt)("p",null,"\u4f7f\u7528\u8bfb\u65f6\u5408\u5e76\u8868\uff0c\u7b2c 2 \u6279\u6570\u636e\u91c7\u96c6\u4ec5\u4ec5\u5c06\u6570\u636e\u8ffd\u52a0\u5230\u6ca1\u6709\u5408\u5e76\u7684 delta \uff08\u65e5\u5fd7\uff09 \u6587\u4ef6\u4e2d\u3002\n\u6b64\u65f6\uff0c\u8bfb\u4f18\u5316\u89c6\u56fe\u548c\u5b9e\u65f6\u89c6\u56fe\u5c06\u63d0\u4f9b\u4e0d\u540c\u7684\u7ed3\u679c\u3002\u8bfb\u4f18\u5316\u89c6\u56fe\u4ecd\u4f1a\u8fd4\u56de\u201c10:29 am\u201d\uff0c\u56e0\u4e3a\u5b83\u4f1a\u53ea\u4f1a\u4ece Parquet \u6587\u4ef6\u4e2d\u8bfb\u53d6\u3002\u5b9e\u65f6\u89c6\u56fe\u4f1a\u505a\u5373\u65f6\u5408\u5e76\u5e76\u8fd4\u56de\u6700\u65b0\u63d0\u4ea4\u7684\u6570\u636e\uff0c\u5373\u201c10:59 a.m\u201d\u3002"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},"docker exec -it adhoc-2 /bin/bash\nbeeline -u jdbc:hive2://hiveserver:10000 --hiveconf hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat --hiveconf hive.stats.autogather=false\n\n# Copy On Write Table:\n\n0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_cow group by symbol HAVING symbol = 'GOOG';\nWARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:59:00  |\n+---------+----------------------+--+\n1 row selected (1.932 seconds)\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_cow where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924221953       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924224524       | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\nAs you can notice, the above queries now reflect the changes that came as part of ingesting second batch.\n\n\n# Merge On Read Table:\n\n# Read Optimized View\n0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_mor group by symbol HAVING symbol = 'GOOG';\nWARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:29:00  |\n+---------+----------------------+--+\n1 row selected (1.6 seconds)\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924222155       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924222155       | GOOG    | 2018-08-31 10:29:00  | 3391    | 1230.1899  | 1230.085  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\n# Realtime View\n0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_mor_rt group by symbol HAVING symbol = 'GOOG';\nWARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:59:00  |\n+---------+----------------------+--+\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_rt where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924222155       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924224537       | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\nexit\nexit\n")),(0,a.kt)("h3",{id:"step-6-b-\u6267\u884c-spark-sql-\u67e5\u8be2"},"Step 6 (b): \u6267\u884c Spark SQL \u67e5\u8be2"),(0,a.kt)("p",null,"\u4ee5 Spark SQL \u6267\u884c\u7c7b\u4f3c\u7684\u67e5\u8be2\uff1a"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},"docker exec -it adhoc-1 /bin/bash\nbash-4.4# $SPARK_INSTALL/bin/spark-shell --jars $HUDI_SPARK_BUNDLE --driver-class-path $HADOOP_CONF_DIR --conf spark.sql.hive.convertMetastoreParquet=false --deploy-mode client  --driver-memory 1G --master local[2] --executor-memory 3G --num-executors 1  --packages com.databricks:spark-avro_2.11:4.0.0\n\n# Copy On Write Table:\n\nscala> spark.sql(\"select symbol, max(ts) from stock_ticks_cow group by symbol HAVING symbol = 'GOOG'\").show(100, false)\n+------+-------------------+\n|symbol|max(ts)            |\n+------+-------------------+\n|GOOG  |2018-08-31 10:59:00|\n+------+-------------------+\n\nscala> spark.sql(\"select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_cow where  symbol = 'GOOG'\").show(100, false)\n\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924221953       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924224524       | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\nAs you can notice, the above queries now reflect the changes that came as part of ingesting second batch.\n\n\n# Merge On Read Table:\n\n# Read Optimized View\nscala> spark.sql(\"select symbol, max(ts) from stock_ticks_mor group by symbol HAVING symbol = 'GOOG'\").show(100, false)\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:29:00  |\n+---------+----------------------+--+\n1 row selected (1.6 seconds)\n\nscala> spark.sql(\"select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor where  symbol = 'GOOG'\").show(100, false)\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924222155       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924222155       | GOOG    | 2018-08-31 10:29:00  | 3391    | 1230.1899  | 1230.085  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\n# Realtime View\nscala> spark.sql(\"select symbol, max(ts) from stock_ticks_mor_rt group by symbol HAVING symbol = 'GOOG'\").show(100, false)\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:59:00  |\n+---------+----------------------+--+\n\nscala> spark.sql(\"select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_rt where  symbol = 'GOOG'\").show(100, false)\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924222155       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924224537       | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\nexit\nexit\n")),(0,a.kt)("h3",{id:"step-6-c-\u6267\u884c-presto-\u67e5\u8be2"},"Step 6 (c): \u6267\u884c Presto \u67e5\u8be2"),(0,a.kt)("p",null,"\u5728 Presto \u4e2d\u4e3a\u8bfb\u4f18\u5316\u89c6\u56fe\u6267\u884c\u7c7b\u4f3c\u7684\u67e5\u8be2\uff1a"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},"docker exec -it presto-worker-1 presto --server presto-coordinator-1:8090\npresto> use hive.default;\nUSE\n\n# Copy On Write Table:\n\npresto:default>select symbol, max(ts) from stock_ticks_cow group by symbol HAVING symbol = 'GOOG';\n symbol |        _col1\n--------+---------------------\n GOOG   | 2018-08-31 10:59:00\n(1 row)\n\nQuery 20190822_181530_00007_segyw, FINISHED, 1 node\nSplits: 49 total, 49 done (100.00%)\n0:02 [197 rows, 613B] [125 rows/s, 389B/s]\n\npresto:default>select \"_hoodie_commit_time\", symbol, ts, volume, open, close  from stock_ticks_cow where  symbol = 'GOOG';\n _hoodie_commit_time | symbol |         ts          | volume |   open    |  close\n---------------------+--------+---------------------+--------+-----------+----------\n 20190822180221      | GOOG   | 2018-08-31 09:59:00 |   6330 |    1230.5 |  1230.02\n 20190822181433      | GOOG   | 2018-08-31 10:59:00 |   9021 | 1227.1993 | 1227.215\n(2 rows)\n\nQuery 20190822_181545_00008_segyw, FINISHED, 1 node\nSplits: 17 total, 17 done (100.00%)\n0:02 [197 rows, 613B] [106 rows/s, 332B/s]\n\nAs you can notice, the above queries now reflect the changes that came as part of ingesting second batch.\n\n\n# Merge On Read Table:\n\n# Read Optimized View\npresto:default> select symbol, max(ts) from stock_ticks_mor group by symbol HAVING symbol = 'GOOG';\n symbol |        _col1\n--------+---------------------\n GOOG   | 2018-08-31 10:29:00\n(1 row)\n\nQuery 20190822_181602_00009_segyw, FINISHED, 1 node\nSplits: 49 total, 49 done (100.00%)\n0:01 [197 rows, 613B] [139 rows/s, 435B/s]\n\npresto:default>select \"_hoodie_commit_time\", symbol, ts, volume, open, close  from stock_ticks_mor where  symbol = 'GOOG';\n _hoodie_commit_time | symbol |         ts          | volume |   open    |  close\n---------------------+--------+---------------------+--------+-----------+----------\n 20190822180250      | GOOG   | 2018-08-31 09:59:00 |   6330 |    1230.5 |  1230.02\n 20190822180250      | GOOG   | 2018-08-31 10:29:00 |   3391 | 1230.1899 | 1230.085\n(2 rows)\n\nQuery 20190822_181615_00010_segyw, FINISHED, 1 node\nSplits: 17 total, 17 done (100.00%)\n0:01 [197 rows, 613B] [154 rows/s, 480B/s]\n\npresto:default> exit\n")),(0,a.kt)("h3",{id:"step-7--\u5199\u65f6\u590d\u5236\u8868\u7684\u589e\u91cf\u67e5\u8be2"},"Step 7 : \u5199\u65f6\u590d\u5236\u8868\u7684\u589e\u91cf\u67e5\u8be2"),(0,a.kt)("p",null,"\u4f7f\u7528\u91c7\u96c6\u7684\u4e24\u4e2a\u6279\u6b21\u7684\u6570\u636e\uff0c\u6211\u4eec\u5c55\u793a Hudi \u5199\u65f6\u590d\u5236\u6570\u636e\u96c6\u4e2d\u652f\u6301\u7684\u589e\u91cf\u67e5\u8be2\u3002"),(0,a.kt)("p",null,"\u6211\u4eec\u4f7f\u7528\u7c7b\u4f3c\u7684\u5de5\u7a0b\u67e5\u8be2\u6837\u4f8b\uff1a"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},"docker exec -it adhoc-2 /bin/bash\nbeeline -u jdbc:hive2://hiveserver:10000 --hiveconf hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat --hiveconf hive.stats.autogather=false\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_cow where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924064621       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924065039       | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n")),(0,a.kt)("p",null,"\u6b63\u5982\u4f60\u5728\u4e0a\u9762\u7684\u67e5\u8be2\u4e2d\u770b\u5230\u7684\uff0c\u6709\u4e24\u4e2a\u63d0\u4ea4\u2014\u2014\u6309\u65f6\u95f4\u7ebf\u6392\u5217\u662f 20180924064621 \u548c 20180924065039 \u3002\n\u5f53\u4f60\u6309\u7167\u8fd9\u4e9b\u6b65\u9aa4\u6267\u884c\u540e\uff0c\u4f60\u7684\u63d0\u4ea4\u4f1a\u5f97\u5230\u4e0d\u540c\u7684\u65f6\u95f4\u6233\u3002\u5c06\u5b83\u4eec\u66ff\u6362\u5230\u4e0a\u9762\u65f6\u95f4\u6233\u7684\u4f4d\u7f6e\u3002"),(0,a.kt)("p",null,"\u4e3a\u4e86\u5c55\u793a\u589e\u91cf\u67e5\u8be2\u7684\u5f71\u54cd\uff0c\u6211\u4eec\u5047\u8bbe\u6709\u4e00\u4f4d\u8bfb\u8005\u5df2\u7ecf\u5728\u7b2c 1 \u6279\u6570\u636e\u4e2d\u4e00\u90e8\u5206\u770b\u5230\u4e86\u53d8\u5316\u3002\u90a3\u4e48\uff0c\u4e3a\u4e86\u8ba9\u8bfb\u8005\u770b\u5230\u7b2c 2 \u6279\u6570\u636e\u7684\u5f71\u54cd\uff0c\u4ed6/\u5979\u9700\u8981\u4fdd\u7559\u7b2c 1 \u6279\u6b21\u63d0\u4ea4\u65f6\u95f4\u4e2d\u7684\u5f00\u59cb\u65f6\u95f4\uff08 20180924064621 \uff09\u5e76\u6267\u884c\u589e\u91cf\u67e5\u8be2\uff1a"),(0,a.kt)("p",null,"Hudi \u7684\u589e\u91cf\u6a21\u5f0f\u4e3a\u589e\u91cf\u67e5\u8be2\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u626b\u63cf\uff0c\u901a\u8fc7 Hudi \u7ba1\u7406\u7684\u5143\u6570\u636e\uff0c\u8fc7\u6ee4\u6389\u4e86\u90a3\u4e9b\u4e0d\u5305\u542b\u5019\u9009\u8bb0\u5f55\u7684\u6587\u4ef6\u3002"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},"docker exec -it adhoc-2 /bin/bash\nbeeline -u jdbc:hive2://hiveserver:10000 --hiveconf hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat --hiveconf hive.stats.autogather=false\n0: jdbc:hive2://hiveserver:10000> set hoodie.stock_ticks_cow.consume.mode=INCREMENTAL;\nNo rows affected (0.009 seconds)\n0: jdbc:hive2://hiveserver:10000>  set hoodie.stock_ticks_cow.consume.max.commits=3;\nNo rows affected (0.009 seconds)\n0: jdbc:hive2://hiveserver:10000> set hoodie.stock_ticks_cow.consume.start.timestamp=20180924064621;\n")),(0,a.kt)("p",null,"\u4f7f\u7528\u4e0a\u9762\u7684\u8bbe\u7f6e\uff0c\u90a3\u4e9b\u5728\u63d0\u4ea4 20180924065039 \u4e4b\u540e\u6ca1\u6709\u4efb\u4f55\u66f4\u65b0\u7684\u6587\u4ef6ID\u5c06\u88ab\u8fc7\u6ee4\u6389\uff0c\u4e0d\u8fdb\u884c\u626b\u63cf\u3002\n\u4ee5\u4e0b\u662f\u589e\u91cf\u67e5\u8be2\uff1a"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},"0: jdbc:hive2://hiveserver:10000>\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_cow where  symbol = 'GOOG' and `_hoodie_commit_time` > '20180924064621';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924065039       | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n1 row selected (0.83 seconds)\n0: jdbc:hive2://hiveserver:10000>\n")),(0,a.kt)("h3",{id:"\u4f7f\u7528-spark-sql-\u505a\u589e\u91cf\u67e5\u8be2"},"\u4f7f\u7528 Spark SQL \u505a\u589e\u91cf\u67e5\u8be2"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},'docker exec -it adhoc-1 /bin/bash\nbash-4.4# $SPARK_INSTALL/bin/spark-shell --jars $HUDI_SPARK_BUNDLE --driver-class-path $HADOOP_CONF_DIR --conf spark.sql.hive.convertMetastoreParquet=false --deploy-mode client  --driver-memory 1G --master local[2] --executor-memory 3G --num-executors 1  --packages com.databricks:spark-avro_2.11:4.0.0\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  \'_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.3.1\n      /_/\n\nUsing Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_181)\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> import org.apache.hudi.DataSourceReadOptions\nimport org.apache.hudi.DataSourceReadOptions\n\n# In the below query, 20180925045257 is the first commit\'s timestamp\nscala> val hoodieIncViewDF =  spark.read.format("org.apache.hudi").option(DataSourceReadOptions.VIEW_TYPE_OPT_KEY, DataSourceReadOptions.VIEW_TYPE_INCREMENTAL_OPT_VAL).option(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY, "20180924064621").load("/user/hive/warehouse/stock_ticks_cow")\nSLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes#StaticLoggerBinder for further details.\nhoodieIncViewDF: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 15 more fields]\n\nscala> hoodieIncViewDF.registerTempTable("stock_ticks_cow_incr_tmp1")\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nscala> spark.sql("select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_cow_incr_tmp1 where  symbol = \'GOOG\'").show(100, false);\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924065039       | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\n')),(0,a.kt)("h3",{id:"step-8-\u4e3a\u8bfb\u65f6\u5408\u5e76\u6570\u636e\u96c6\u7684\u8c03\u5ea6\u5e76\u6267\u884c\u538b\u7f29"},"Step 8: \u4e3a\u8bfb\u65f6\u5408\u5e76\u6570\u636e\u96c6\u7684\u8c03\u5ea6\u5e76\u6267\u884c\u538b\u7f29"),(0,a.kt)("p",null,"\u6211\u4eec\u6765\u8c03\u5ea6\u5e76\u8fd0\u884c\u4e00\u4e2a\u538b\u7f29\u6765\u521b\u5efa\u4e00\u4e2a\u65b0\u7248\u672c\u7684\u5217\u5f0f\u6587\u4ef6\uff0c\u4ee5\u4fbf\u8bfb\u4f18\u5316\u8bfb\u53d6\u5668\u80fd\u770b\u5230\u65b0\u6570\u636e\u3002\n\u518d\u6b21\u5f3a\u8c03\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528 Hudi CLI \u6765\u4eba\u5de5\u8c03\u5ea6\u5e76\u6267\u884c\u538b\u7f29\u3002"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},'docker exec -it adhoc-1 /bin/bash\nroot@adhoc-1:/opt#   /var/hoodie/ws/hudi-cli/hudi-cli.sh\n============================================\n*                                          *\n*     _    _           _   _               *\n*    | |  | |         | | (_)              *\n*    | |__| |       __| |  -               *\n*    |  __  ||   | / _` | ||               *\n*    | |  | ||   || (_| | ||               *\n*    |_|  |_|\\___/ \\____/ ||               *\n*                                          *\n============================================\n\nWelcome to Hoodie CLI. Please type help if you are looking for help.\nhudi->connect --path /user/hive/warehouse/stock_ticks_mor\n18/09/24 06:59:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n18/09/24 06:59:35 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor\n18/09/24 06:59:35 INFO util.FSUtils: Hadoop Configuration: fs.defaultFS: [hdfs://namenode:8020], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1261652683_11, ugi=root (auth:SIMPLE)]]]\n18/09/24 06:59:35 INFO table.HoodieTableConfig: Loading dataset properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties\n18/09/24 06:59:36 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ from /user/hive/warehouse/stock_ticks_mor\nMetadata for table stock_ticks_mor loaded\n\n# Ensure no compactions are present\n\nhoodie:stock_ticks_mor->compactions show all\n18/09/24 06:59:54 INFO timeline.HoodieActiveTimeline: Loaded instants [[20180924064636__clean__COMPLETED], [20180924064636__deltacommit__COMPLETED], [20180924065057__clean__COMPLETED], [20180924065057__deltacommit__COMPLETED]]\n    ___________________________________________________________________\n    | Compaction Instant Time| State    | Total FileIds to be Compacted|\n    |==================================================================|\n\n\n\n\n# Schedule a compaction. This will use Spark Launcher to schedule compaction\nhoodie:stock_ticks_mor->compaction schedule\n....\nCompaction successfully completed for 20180924070031\n\n# Now refresh and check again. You will see that there is a new compaction requested\n\nhoodie:stock_ticks->connect --path /user/hive/warehouse/stock_ticks_mor\n18/09/24 07:01:16 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor\n18/09/24 07:01:16 INFO util.FSUtils: Hadoop Configuration: fs.defaultFS: [hdfs://namenode:8020], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1261652683_11, ugi=root (auth:SIMPLE)]]]\n18/09/24 07:01:16 INFO table.HoodieTableConfig: Loading dataset properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties\n18/09/24 07:01:16 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ from /user/hive/warehouse/stock_ticks_mor\nMetadata for table stock_ticks_mor loaded\n\n\n\nhoodie:stock_ticks_mor->compactions show all\n18/09/24 06:34:12 INFO timeline.HoodieActiveTimeline: Loaded instants [[20180924041125__clean__COMPLETED], [20180924041125__deltacommit__COMPLETED], [20180924042735__clean__COMPLETED], [20180924042735__deltacommit__COMPLETED], [==>20180924063245__compaction__REQUESTED]]\n    ___________________________________________________________________\n    | Compaction Instant Time| State    | Total FileIds to be Compacted|\n    |==================================================================|\n    | 20180924070031         | REQUESTED| 1                            |\n\n\n\n\n# Execute the compaction. The compaction instant value passed below must be the one displayed in the above "compactions show all" query\nhoodie:stock_ticks_mor->compaction run --compactionInstant  20180924070031 --parallelism 2 --sparkMemory 1G  --schemaFilePath /var/demo/config/schema.avsc --retry 1  \n....\nCompaction successfully completed for 20180924070031\n\n\n## Now check if compaction is completed\n\nhoodie:stock_ticks_mor->connect --path /user/hive/warehouse/stock_ticks_mor\n18/09/24 07:03:00 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor\n18/09/24 07:03:00 INFO util.FSUtils: Hadoop Configuration: fs.defaultFS: [hdfs://namenode:8020], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1261652683_11, ugi=root (auth:SIMPLE)]]]\n18/09/24 07:03:00 INFO table.HoodieTableConfig: Loading dataset properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties\n18/09/24 07:03:00 INFO table.HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ from /user/hive/warehouse/stock_ticks_mor\nMetadata for table stock_ticks_mor loaded\n\n\n\nhoodie:stock_ticks->compactions show all\n18/09/24 07:03:15 INFO timeline.HoodieActiveTimeline: Loaded instants [[20180924064636__clean__COMPLETED], [20180924064636__deltacommit__COMPLETED], [20180924065057__clean__COMPLETED], [20180924065057__deltacommit__COMPLETED], [20180924070031__commit__COMPLETED]]\n    ___________________________________________________________________\n    | Compaction Instant Time| State    | Total FileIds to be Compacted|\n    |==================================================================|\n    | 20180924070031         | COMPLETED| 1                            |\n\n')),(0,a.kt)("h3",{id:"step-9-\u6267\u884c\u5305\u542b\u589e\u91cf\u67e5\u8be2\u7684-hive-\u67e5\u8be2"},"Step 9: \u6267\u884c\u5305\u542b\u589e\u91cf\u67e5\u8be2\u7684 Hive \u67e5\u8be2"),(0,a.kt)("p",null,"\u4f60\u5c06\u770b\u5230\u8bfb\u4f18\u5316\u89c6\u56fe\u548c\u5b9e\u65f6\u89c6\u56fe\u90fd\u4f1a\u5c55\u793a\u6700\u65b0\u63d0\u4ea4\u7684\u6570\u636e\u3002\n\u8ba9\u6211\u4eec\u4e5f\u5bf9 MOR \u8868\u6267\u884c\u589e\u91cf\u67e5\u8be2\u3002\n\u901a\u8fc7\u67e5\u770b\u4e0b\u65b9\u7684\u67e5\u8be2\u8f93\u51fa\uff0c\u80fd\u591f\u660e\u786e MOR \u8868\u7684\u7b2c\u4e00\u6b21\u63d0\u4ea4\u65f6\u95f4\u662f 20180924064636 \u800c\u7b2c\u4e8c\u6b21\u63d0\u4ea4\u65f6\u95f4\u662f 20180924070031 \u3002"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},"docker exec -it adhoc-2 /bin/bash\nbeeline -u jdbc:hive2://hiveserver:10000 --hiveconf hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat --hiveconf hive.stats.autogather=false\n\n# Read Optimized View\n0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_mor group by symbol HAVING symbol = 'GOOG';\nWARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:59:00  |\n+---------+----------------------+--+\n1 row selected (1.6 seconds)\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924064636       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924070031       | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\n# Realtime View\n0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_mor_rt group by symbol HAVING symbol = 'GOOG';\nWARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:59:00  |\n+---------+----------------------+--+\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_rt where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924064636       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924070031       | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\n# Incremental View:\n\n0: jdbc:hive2://hiveserver:10000> set hoodie.stock_ticks_mor.consume.mode=INCREMENTAL;\nNo rows affected (0.008 seconds)\n# Max-Commits covers both second batch and compaction commit\n0: jdbc:hive2://hiveserver:10000> set hoodie.stock_ticks_mor.consume.max.commits=3;\nNo rows affected (0.007 seconds)\n0: jdbc:hive2://hiveserver:10000> set hoodie.stock_ticks_mor.consume.start.timestamp=20180924064636;\nNo rows affected (0.013 seconds)\n# Query:\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor where  symbol = 'GOOG' and `_hoodie_commit_time` > '20180924064636';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924070031       | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\nexit\nexit\n")),(0,a.kt)("h3",{id:"step-10-\u538b\u7f29\u540e\u5728-mor-\u7684\u8bfb\u4f18\u5316\u89c6\u56fe\u4e0e\u5b9e\u65f6\u89c6\u56fe\u4e0a\u4f7f\u7528-spark-sql"},"Step 10: \u538b\u7f29\u540e\u5728 MOR \u7684\u8bfb\u4f18\u5316\u89c6\u56fe\u4e0e\u5b9e\u65f6\u89c6\u56fe\u4e0a\u4f7f\u7528 Spark-SQL"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},"docker exec -it adhoc-1 /bin/bash\nbash-4.4# $SPARK_INSTALL/bin/spark-shell --jars $HUDI_SPARK_BUNDLE --driver-class-path $HADOOP_CONF_DIR --conf spark.sql.hive.convertMetastoreParquet=false --deploy-mode client  --driver-memory 1G --master local[2] --executor-memory 3G --num-executors 1  --packages com.databricks:spark-avro_2.11:4.0.0\n\n# Read Optimized View\nscala> spark.sql(\"select symbol, max(ts) from stock_ticks_mor group by symbol HAVING symbol = 'GOOG'\").show(100, false)\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:59:00  |\n+---------+----------------------+--+\n1 row selected (1.6 seconds)\n\nscala> spark.sql(\"select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor where  symbol = 'GOOG'\").show(100, false)\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924064636       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924070031       | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\n# Realtime View\nscala> spark.sql(\"select symbol, max(ts) from stock_ticks_mor_rt group by symbol HAVING symbol = 'GOOG'\").show(100, false)\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:59:00  |\n+---------+----------------------+--+\n\nscala> spark.sql(\"select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_rt where  symbol = 'GOOG'\").show(100, false)\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20180924064636       | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20180924070031       | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n")),(0,a.kt)("h3",{id:"step-11--\u538b\u7f29\u540e\u5728-mor-\u6570\u636e\u96c6\u7684\u8bfb\u4f18\u5316\u89c6\u56fe\u4e0a\u8fdb\u884c-presto-\u67e5\u8be2"},"Step 11:  \u538b\u7f29\u540e\u5728 MOR \u6570\u636e\u96c6\u7684\u8bfb\u4f18\u5316\u89c6\u56fe\u4e0a\u8fdb\u884c Presto \u67e5\u8be2"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},"docker exec -it presto-worker-1 presto --server presto-coordinator-1:8090\npresto> use hive.default;\nUSE\n\n# Read Optimized View\nresto:default> select symbol, max(ts) from stock_ticks_mor group by symbol HAVING symbol = 'GOOG';\n  symbol |        _col1\n--------+---------------------\n GOOG   | 2018-08-31 10:59:00\n(1 row)\n\nQuery 20190822_182319_00011_segyw, FINISHED, 1 node\nSplits: 49 total, 49 done (100.00%)\n0:01 [197 rows, 613B] [133 rows/s, 414B/s]\n\npresto:default> select \"_hoodie_commit_time\", symbol, ts, volume, open, close  from stock_ticks_mor where  symbol = 'GOOG';\n _hoodie_commit_time | symbol |         ts          | volume |   open    |  close\n---------------------+--------+---------------------+--------+-----------+----------\n 20190822180250      | GOOG   | 2018-08-31 09:59:00 |   6330 |    1230.5 |  1230.02\n 20190822181944      | GOOG   | 2018-08-31 10:59:00 |   9021 | 1227.1993 | 1227.215\n(2 rows)\n\nQuery 20190822_182333_00012_segyw, FINISHED, 1 node\nSplits: 17 total, 17 done (100.00%)\n0:02 [197 rows, 613B] [98 rows/s, 307B/s]\n\npresto:default>\n\n")),(0,a.kt)("p",null,"Demo \u5230\u6b64\u7ed3\u675f\u3002"),(0,a.kt)("h2",{id:"\u5728\u672c\u5730-docker-\u73af\u5883\u4e2d\u6d4b\u8bd5-hudi"},"\u5728\u672c\u5730 Docker \u73af\u5883\u4e2d\u6d4b\u8bd5 Hudi"),(0,a.kt)("p",null,"\u4f60\u53ef\u4ee5\u7ec4\u5efa\u4e00\u4e2a\u5305\u542b Hadoop \u3001 Hive \u548c Spark \u670d\u52a1\u7684 Hadoop Docker \u73af\u5883\uff0c\u5e76\u652f\u6301 Hudi \u3002"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},"$ mvn pre-integration-test -DskipTests\n")),(0,a.kt)("p",null,"\u4e0a\u9762\u7684\u547d\u4ee4\u4e3a\u6240\u6709\u7684\u670d\u52a1\u6784\u5efa\u4e86 Docker \u955c\u50cf\uff0c\u5b83\u5e26\u6709\u5f53\u524d\u5b89\u88c5\u5728 /var/hoodie/ws \u7684 Hudi \u6e90\uff0c\u5e76\u4f7f\u7528\u4e00\u4e2a\u90e8\u7f72\u6587\u4ef6\u5f15\u5165\u4e86\u8fd9\u4e9b\u670d\u52a1\u3002\u6211\u4eec\u5f53\u524d\u5728 Docker \u955c\u50cf\u4e2d\u4f7f\u7528 Hadoop \uff08v2.8.4\uff09\u3001 Hive \uff08v2.3.3\uff09\u548c Spark \uff08v2.3.1\uff09\u3002"),(0,a.kt)("p",null,"\u8981\u9500\u6bc1\u5bb9\u5668\uff1a"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},"$ cd hudi-integ-test\n$ mvn docker-compose:down\n")),(0,a.kt)("p",null,"\u5982\u679c\u4f60\u60f3\u8981\u7ec4\u5efa Docker \u5bb9\u5668\uff0c\u4f7f\u7528\uff1a"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},"$ cd hudi-integ-test\n$  mvn docker-compose:up -DdetachedMode=true\n")),(0,a.kt)("p",null,"Hudi \u662f\u4e00\u4e2a\u5728\u5305\u542b Hadoop \u3001 Hive \u548c Spark \u7684\u6d77\u91cf\u6570\u636e\u5206\u6790/\u91c7\u96c6\u73af\u5883\u4e2d\u4f7f\u7528\u7684\u5e93\u3002\u4e0e\u8fd9\u4e9b\u7cfb\u7edf\u7684\u4e92\u7528\u6027\u662f\u6211\u4eec\u7684\u4e00\u4e2a\u5173\u952e\u76ee\u6807\u3002 \u6211\u4eec\u5728\u79ef\u6781\u5730\u5411 ",(0,a.kt)("strong",{parentName:"p"},"hudi-integ-test/src/test/java")," \u6dfb\u52a0\u96c6\u6210\u6d4b\u8bd5\uff0c\u8fd9\u4e9b\u6d4b\u8bd5\u5229\u7528\u4e86\u8fd9\u4e2a Docker \u73af\u5883\uff08\u53c2\u8003\uff1a ",(0,a.kt)("strong",{parentName:"p"},"hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestHoodieSanity.java")," \uff09\u3002"),(0,a.kt)("h3",{id:"\u6784\u5efa\u672c\u5730-docker-\u5bb9\u5668"},"\u6784\u5efa\u672c\u5730 Docker \u5bb9\u5668:"),(0,a.kt)("p",null,"Demo \u548c\u6267\u884c\u96c6\u6210\u6d4b\u8bd5\u6240\u9700\u8981\u7684 Docker \u955c\u50cf\u5df2\u7ecf\u5728 Docker \u6e90\u4e2d\u3002 Docker \u955c\u50cf\u548c\u90e8\u7f72\u811a\u672c\u7ecf\u8fc7\u4e86\u8c28\u614e\u7684\u5b9e\u73b0\u4ee5\u4fbf\u670d\u52a1\u4e0e\u591a\u79cd\u76ee\u7684\uff1a"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Docker \u955c\u50cf\u6709\u5185\u5efa\u7684 Hudi jar \u5305\uff0c\u5b83\u5305\u542b\u4e00\u4e9b\u6307\u5411\u5176\u4ed6 jar \u5305\u7684\u73af\u5883\u53d8\u91cf\uff08 HUDI_HADOOP_BUNDLE \u7b49\uff09"),(0,a.kt)("li",{parentName:"ol"},"\u4e3a\u4e86\u6267\u884c\u96c6\u6210\u6d4b\u8bd5\uff0c\u6211\u4eec\u9700\u8981\u4f7f\u7528\u672c\u5730\u751f\u6210\u7684 jar \u5305\u5728 Docker \u4e2d\u8fd0\u884c\u670d\u52a1\u3002 Docker \u90e8\u7f72\u811a\u672c\uff08\u53c2\u8003 ",(0,a.kt)("inlineCode",{parentName:"li"},"docker/compose/docker-compose_hadoop284_hive233_spark231.yml"),"\uff09\u80fd\u786e\u4fdd\u672c\u5730 jar \u5305\u901a\u8fc7\u6302\u8f7d Docker \u5730\u5740\u4e0a\u6302\u8f7d\u672c\u5730 Hudi \u5de5\u4f5c\u7a7a\u95f4\uff0c\u4ece\u800c\u8986\u76d6\u4e86\u5185\u5efa\u7684 jar \u5305\u3002"),(0,a.kt)("li",{parentName:"ol"},"\u5f53\u8fd9\u4e9b Docker \u5bb9\u5668\u6302\u8f7d\u5230\u672c\u5730 Hudi \u5de5\u4f5c\u7a7a\u95f4\u4e4b\u540e\uff0c\u4efb\u4f55\u53d1\u751f\u5728\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u7684\u53d8\u66f4\u5c06\u4f1a\u81ea\u52a8\u53cd\u6620\u5230\u5bb9\u5668\u4e2d\u3002\u8fd9\u5bf9\u4e8e\u5f00\u53d1\u8005\u6765\u8bf4\u662f\u4e00\u79cd\u5f00\u53d1\u548c\u9a8c\u8bc1 Hudi \u7684\u7b80\u4fbf\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u5f00\u53d1\u8005\u6ca1\u6709\u5206\u5e03\u5f0f\u7684\u73af\u5883\u3002\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u662f\u96c6\u6210\u6d4b\u8bd5\u7684\u6267\u884c\u65b9\u5f0f\u3002")),(0,a.kt)("p",null,"\u8fd9\u907f\u514d\u4e86\u7ef4\u62a4\u5206\u79bb\u7684 Docker \u955c\u50cf\uff0c\u4e5f\u907f\u514d\u4e86\u672c\u5730\u6784\u5efa Docker \u955c\u50cf\u7684\u5404\u4e2a\u6b65\u9aa4\u7684\u6d88\u8017\u3002\n\u4f46\u662f\u5982\u679c\u7528\u6237\u60f3\u8981\u5728\u6709\u66f4\u4f4e\u7f51\u7edc\u5e26\u5bbd\u7684\u5730\u65b9\u6d4b\u8bd5 Hudi \uff0c\u4ed6\u4eec\u4ecd\u53ef\u4ee5\u6784\u5efa\u672c\u5730\u955c\u50cf\u3002\n\u5728\u6267\u884c ",(0,a.kt)("inlineCode",{parentName:"p"},"docker/setup_demo.sh")," \u4e4b\u524d\u6267\u884c\u811a\u672c ",(0,a.kt)("inlineCode",{parentName:"p"},"docker/build_local_docker_images.sh")," \u6765\u6784\u5efa\u672c\u5730 Docker \u955c\u50cf\u3002"),(0,a.kt)("p",null,"\u4ee5\u4e0b\u662f\u6267\u884c\u7684\u547d\u4ee4:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},"cd docker\n./build_local_docker_images.sh\n.....\n\n[INFO] Reactor Summary:\n[INFO]\n[INFO] hoodie ............................................. SUCCESS [  1.709 s]\n[INFO] hudi-common ...................................... SUCCESS [  9.015 s]\n[INFO] hudi-hadoop-mr ................................... SUCCESS [  1.108 s]\n[INFO] hudi-client ...................................... SUCCESS [  4.409 s]\n[INFO] hudi-hive ........................................ SUCCESS [  0.976 s]\n[INFO] hudi-spark ....................................... SUCCESS [ 26.522 s]\n[INFO] hudi-utilities ................................... SUCCESS [ 16.256 s]\n[INFO] hudi-cli ......................................... SUCCESS [ 11.341 s]\n[INFO] hudi-hadoop-mr-bundle ............................ SUCCESS [  1.893 s]\n[INFO] hudi-hive-bundle ................................. SUCCESS [ 14.099 s]\n[INFO] hudi-spark-bundle ................................ SUCCESS [ 58.252 s]\n[INFO] hudi-hadoop-docker ............................... SUCCESS [  0.612 s]\n[INFO] hudi-hadoop-base-docker .......................... SUCCESS [04:04 min]\n[INFO] hudi-hadoop-namenode-docker ...................... SUCCESS [  6.142 s]\n[INFO] hudi-hadoop-datanode-docker ...................... SUCCESS [  7.763 s]\n[INFO] hudi-hadoop-history-docker ....................... SUCCESS [  5.922 s]\n[INFO] hudi-hadoop-hive-docker .......................... SUCCESS [ 56.152 s]\n[INFO] hudi-hadoop-sparkbase-docker ..................... SUCCESS [01:18 min]\n[INFO] hudi-hadoop-sparkmaster-docker ................... SUCCESS [  2.964 s]\n[INFO] hudi-hadoop-sparkworker-docker ................... SUCCESS [  3.032 s]\n[INFO] hudi-hadoop-sparkadhoc-docker .................... SUCCESS [  2.764 s]\n[INFO] hudi-integ-test .................................. SUCCESS [  1.785 s]\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 09:15 min\n[INFO] Finished at: 2018-09-10T17:47:37-07:00\n[INFO] Final Memory: 236M/1848M\n[INFO] ------------------------------------------------------------------------\n")))}_.isMDXComponent=!0}}]);