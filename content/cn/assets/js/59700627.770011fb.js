"use strict";(globalThis.webpackChunkhudi=globalThis.webpackChunkhudi||[]).push([[95187],{11470:(e,a,n)=>{n.d(a,{A:()=>k});var t=n(96540),r=n(34164),i=n(23104),s=n(56347),o=n(205),l=n(57485),d=n(31682),c=n(70679);function h(e){return t.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,t.isValidElement)(e)&&function(e){const{props:a}=e;return!!a&&"object"==typeof a&&"value"in a}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function p(e){const{values:a,children:n}=e;return(0,t.useMemo)(()=>{const e=a??function(e){return h(e).map(({props:{value:e,label:a,attributes:n,default:t}})=>({value:e,label:a,attributes:n,default:t}))}(n);return function(e){const a=(0,d.XI)(e,(e,a)=>e.value===a.value);if(a.length>0)throw new Error(`Docusaurus error: Duplicate values "${a.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[a,n])}function u({value:e,tabValues:a}){return a.some(a=>a.value===e)}function m({queryString:e=!1,groupId:a}){const n=(0,s.W6)(),r=function({queryString:e=!1,groupId:a}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:e,groupId:a});return[(0,l.aZ)(r),(0,t.useCallback)(e=>{if(!r)return;const a=new URLSearchParams(n.location.search);a.set(r,e),n.replace({...n.location,search:a.toString()})},[r,n])]}function g(e){const{defaultValue:a,queryString:n=!1,groupId:r}=e,i=p(e),[s,l]=(0,t.useState)(()=>function({defaultValue:e,tabValues:a}){if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!u({value:e,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${a.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const n=a.find(e=>e.default)??a[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:a,tabValues:i})),[d,h]=m({queryString:n,groupId:r}),[g,f]=function({groupId:e}){const a=function(e){return e?`docusaurus.tab.${e}`:null}(e),[n,r]=(0,c.Dv)(a);return[n,(0,t.useCallback)(e=>{a&&r.set(e)},[a,r])]}({groupId:r}),b=(()=>{const e=d??g;return u({value:e,tabValues:i})?e:null})();(0,o.A)(()=>{b&&l(b)},[b]);return{selectedValue:s,selectValue:(0,t.useCallback)(e=>{if(!u({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);l(e),h(e),f(e)},[h,f,i]),tabValues:i}}var f=n(92303);const b={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var x=n(74848);function _({className:e,block:a,selectedValue:n,selectValue:t,tabValues:s}){const o=[],{blockElementScrollPositionUntilNextRender:l}=(0,i.a_)(),d=e=>{const a=e.currentTarget,r=o.indexOf(a),i=s[r].value;i!==n&&(l(a),t(i))},c=e=>{let a=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const n=o.indexOf(e.currentTarget)+1;a=o[n]??o[0];break}case"ArrowLeft":{const n=o.indexOf(e.currentTarget)-1;a=o[n]??o[o.length-1];break}}a?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":a},e),children:s.map(({value:e,label:a,attributes:t})=>(0,x.jsx)("li",{role:"tab",tabIndex:n===e?0:-1,"aria-selected":n===e,ref:e=>{o.push(e)},onKeyDown:c,onClick:d,...t,className:(0,r.A)("tabs__item",b.tabItem,t?.className,{"tabs__item--active":n===e}),children:a??e},e))})}function j({lazy:e,children:a,selectedValue:n}){const i=(Array.isArray(a)?a:[a]).filter(Boolean);if(e){const e=i.find(e=>e.props.value===n);return e?(0,t.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:i.map((e,a)=>(0,t.cloneElement)(e,{key:a,hidden:e.props.value!==n}))})}function y(e){const a=g(e);return(0,x.jsxs)("div",{className:(0,r.A)("tabs-container",b.tabList),children:[(0,x.jsx)(_,{...a,...e}),(0,x.jsx)(j,{...a,...e})]})}function k(e){const a=(0,f.A)();return(0,x.jsx)(y,{...e,children:h(e.children)},String(a))}},19365:(e,a,n)=>{n.d(a,{A:()=>s});n(96540);var t=n(34164);const r={tabItem:"tabItem_Ymn6"};var i=n(74848);function s({children:e,hidden:a,className:n}){return(0,i.jsx)("div",{role:"tabpanel",className:(0,t.A)(r.tabItem,n),hidden:a,children:e})}},24136:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>c,contentTitle:()=>d,default:()=>u,frontMatter:()=>l,metadata:()=>t,toc:()=>h});const t=JSON.parse('{"id":"quick-start-guide","title":"Spark Guide","description":"This guide provides a quick peek at Hudi\'s capabilities using spark-shell. Using Spark datasources, we will walk through","source":"@site/versioned_docs/version-0.13.0/quick-start-guide.md","sourceDirName":".","slug":"/quick-start-guide","permalink":"/cn/docs/0.13.0/quick-start-guide","draft":false,"unlisted":false,"editUrl":"https://github.com/apache/hudi/tree/asf-site/website/versioned_docs/version-0.13.0/quick-start-guide.md","tags":[],"version":"0.13.0","sidebarPosition":2,"frontMatter":{"title":"Spark Guide","sidebar_position":2,"toc":true,"last_modified_at":"2019-12-30T19:59:57.000Z"},"sidebar":"docs","previous":{"title":"Overview","permalink":"/cn/docs/0.13.0/overview"},"next":{"title":"Flink Guide","permalink":"/cn/docs/0.13.0/flink-quick-start-guide"}}');var r=n(74848),i=n(28453),s=n(11470),o=n(19365);const l={title:"Spark Guide",sidebar_position:2,toc:!0,last_modified_at:new Date("2019-12-30T19:59:57.000Z")},d=void 0,c={},h=[{value:"Setup",id:"setup",level:2},{value:"Create Table",id:"create-table",level:2},{value:"Insert data",id:"insert-data",level:2},{value:"Query data",id:"query-data",level:2},{value:"Time Travel Query",id:"time-travel-query",level:3},{value:"Update data",id:"update-data",level:2},{value:"Update",id:"update",level:3},{value:"MergeInto",id:"mergeinto",level:3},{value:"Incremental query",id:"incremental-query",level:2},{value:"Structured Streaming",id:"structured-streaming",level:2},{value:"Streaming Read",id:"streaming-read",level:3},{value:"Streaming Write",id:"streaming-write",level:3},{value:"Table maintenance",id:"table-maintenance",level:3},{value:"Point in time query",id:"point-in-time-query",level:2},{value:"Delete data",id:"deletes",level:2},{value:"Soft Deletes",id:"soft-deletes",level:3},{value:"Hard Deletes",id:"hard-deletes",level:3},{value:"Insert Overwrite",id:"insert-overwrite",level:2},{value:"More Spark SQL Commands",id:"more-spark-sql-commands",level:2},{value:"Alter Table",id:"alter-table",level:3},{value:"Partition SQL Command",id:"partition-sql-command",level:3},{value:"Procedures",id:"procedures",level:3},{value:"Where to go from here?",id:"where-to-go-from-here",level:2}];function p(e){const a={a:"a",admonition:"admonition",br:"br",code:"code",em:"em",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(a.p,{children:["This guide provides a quick peek at Hudi's capabilities using spark-shell. Using Spark datasources, we will walk through\ncode snippets that allows you to insert and update a Hudi table of default table type:\n",(0,r.jsx)(a.a,{href:"/docs/table_types#copy-on-write-table",children:"Copy on Write"}),". After each write operation we will also show how to read the\ndata both snapshot and incrementally."]}),"\n",(0,r.jsx)(a.h2,{id:"setup",children:"Setup"}),"\n",(0,r.jsxs)(a.p,{children:["Hudi works with Spark-2.4.3+ & Spark 3.x versions. You can follow instructions ",(0,r.jsx)(a.a,{href:"https://spark.apache.org/downloads",children:"here"})," for setting up Spark."]}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"Spark 3 Support Matrix"})}),"\n",(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{style:{textAlign:"left"},children:"Hudi"}),(0,r.jsx)(a.th,{style:{textAlign:"left"},children:"Supported Spark 3 version"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{style:{textAlign:"left"},children:"0.13.x"}),(0,r.jsx)(a.td,{style:{textAlign:"left"},children:"3.3.x (default build), 3.2.x, 3.1.x"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{style:{textAlign:"left"},children:"0.12.x"}),(0,r.jsx)(a.td,{style:{textAlign:"left"},children:"3.3.x (default build), 3.2.x, 3.1.x"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{style:{textAlign:"left"},children:"0.11.x"}),(0,r.jsx)(a.td,{style:{textAlign:"left"},children:"3.2.x (default build, Spark bundle only), 3.1.x"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{style:{textAlign:"left"},children:"0.10.x"}),(0,r.jsx)(a.td,{style:{textAlign:"left"},children:"3.1.x (default build), 3.0.x"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{style:{textAlign:"left"},children:"0.7.0 - 0.9.0"}),(0,r.jsx)(a.td,{style:{textAlign:"left"},children:"3.0.x"})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{style:{textAlign:"left"},children:"0.6.0 and prior"}),(0,r.jsx)(a.td,{style:{textAlign:"left"},children:"not supported"})]})]})]}),"\n",(0,r.jsxs)(a.p,{children:["The ",(0,r.jsx)(a.em,{children:"default build"})," Spark version indicates that it is used to build the ",(0,r.jsx)(a.code,{children:"hudi-spark3-bundle"}),"."]}),"\n",(0,r.jsx)(a.admonition,{type:"note",children:(0,r.jsxs)(a.p,{children:["In 0.12.0, we introduce the experimental support for Spark 3.3.0.\nIn 0.11.0, there are changes on using Spark bundles, please refer\nto ",(0,r.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-0.11.0/#spark-versions-and-bundles",children:"0.11.0 release notes"})," for detailed\ninstructions."]})}),"\n",(0,r.jsxs)(s.A,{groupId:"programming-language",defaultValue:"python",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"},{label:"Spark SQL",value:"sparksql"}],children:[(0,r.jsxs)(o.A,{value:"scala",children:[(0,r.jsx)(a.p,{children:"From the extracted directory run spark-shell with Hudi:"}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-shell",children:"# Spark 3.3\nspark-shell \\\n  --packages org.apache.hudi:hudi-spark3.3-bundle_2.12:0.13.0 \\\n  --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \\\n  --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog' \\\n  --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' \\\n  --conf 'spark.kryo.registrator=org.apache.spark.HoodieSparkKryoRegistrar'\n"})}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-shell",children:"# Spark 3.2\nspark-shell \\\n  --packages org.apache.hudi:hudi-spark3.2-bundle_2.12:0.13.0 \\\n  --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \\\n  --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog' \\\n  --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' \\\n  --conf 'spark.kryo.registrator=org.apache.spark.HoodieSparkKryoRegistrar'\n"})}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-shell",children:"# Spark 3.1\nspark-shell \\\n  --packages org.apache.hudi:hudi-spark3.1-bundle_2.12:0.13.0 \\\n  --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \\\n  --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' \\\n  --conf 'spark.kryo.registrator=org.apache.spark.HoodieSparkKryoRegistrar'\n"})}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-shell",children:"# Spark 2.4\nspark-shell \\\n  --packages org.apache.hudi:hudi-spark2.4-bundle_2.11:0.13.0 \\\n  --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \\\n  --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' \\\n  --conf 'spark.kryo.registrator=org.apache.spark.HoodieSparkKryoRegistrar'\n"})})]}),(0,r.jsxs)(o.A,{value:"python",children:[(0,r.jsx)(a.p,{children:"From the extracted directory run pyspark with Hudi:"}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-shell",children:"# Spark 3.3\nexport PYSPARK_PYTHON=$(which python3)\npyspark \\\n--packages org.apache.hudi:hudi-spark3.3-bundle_2.12:0.13.0 \\\n--conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \\\n--conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog' \\\n--conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension'\n"})}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-shell",children:"# Spark 3.2\nexport PYSPARK_PYTHON=$(which python3)\npyspark \\\n--packages org.apache.hudi:hudi-spark3.2-bundle_2.12:0.13.0 \\\n--conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \\\n--conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog' \\\n--conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension'\n"})}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-shell",children:"# Spark 3.1\nexport PYSPARK_PYTHON=$(which python3)\npyspark \\\n--packages org.apache.hudi:hudi-spark3.1-bundle_2.12:0.13.0 \\\n--conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \\\n--conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension'\n"})}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-shell",children:"# Spark 2.4\nexport PYSPARK_PYTHON=$(which python3)\npyspark \\\n--packages org.apache.hudi:hudi-spark2.4-bundle_2.11:0.13.0 \\\n--conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \\\n--conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension'\n"})})]}),(0,r.jsxs)(o.A,{value:"sparksql",children:[(0,r.jsxs)(a.p,{children:["Hudi support using Spark SQL to write and read data with the ",(0,r.jsx)(a.strong,{children:"HoodieSparkSessionExtension"})," sql extension.\nFrom the extracted directory run Spark SQL with Hudi:"]}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-shell",children:"# Spark 3.3\nspark-sql --packages org.apache.hudi:hudi-spark3.3-bundle_2.12:0.13.0 \\\n--conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \\\n--conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' \\\n--conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog'\n"})}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-shell",children:"# Spark 3.2\nspark-sql --packages org.apache.hudi:hudi-spark3.2-bundle_2.12:0.13.0 \\\n--conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \\\n--conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' \\\n--conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog'\n"})}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-shell",children:"# Spark 3.1\nspark-sql --packages org.apache.hudi:hudi-spark3.1-bundle_2.12:0.13.0 \\\n--conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \\\n--conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension'\n"})}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-shell",children:"# Spark 2.4\nspark-sql --packages org.apache.hudi:hudi-spark2.4-bundle_2.11:0.13.0 \\\n--conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \\\n--conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension'\n"})})]})]}),"\n",(0,r.jsx)(a.admonition,{title:"Please note the following",type:"note",children:(0,r.jsxs)("ul",{children:[(0,r.jsx)("li",{children:" For Spark 3.2 and above, the additional spark_catalog config is required:\n--conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog' "}),(0,r.jsx)("li",{children:" We have used hudi-spark-bundle built for scala 2.12 since the spark-avro module used can also depend on 2.12. "})]})}),"\n",(0,r.jsx)(a.p,{children:"Setup table name, base path and a data generator to generate records for this guide."}),"\n",(0,r.jsxs)(s.A,{groupId:"programming-language",defaultValue:"python",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"}],children:[(0,r.jsx)(o.A,{value:"scala",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-scala",children:'// spark-shell\nimport org.apache.hudi.QuickstartUtils._\nimport scala.collection.JavaConversions._\nimport org.apache.spark.sql.SaveMode._\nimport org.apache.hudi.DataSourceReadOptions._\nimport org.apache.hudi.DataSourceWriteOptions._\nimport org.apache.hudi.config.HoodieWriteConfig._\nimport org.apache.hudi.common.model.HoodieRecord\n\nval tableName = "hudi_trips_cow"\nval basePath = "file:///tmp/hudi_trips_cow"\nval dataGen = new DataGenerator\n'})})}),(0,r.jsx)(o.A,{value:"python",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:'# pyspark\ntableName = "hudi_trips_cow"\nbasePath = "file:///tmp/hudi_trips_cow"\ndataGen = sc._jvm.org.apache.hudi.QuickstartUtils.DataGenerator()\n'})})})]}),"\n",(0,r.jsx)(a.admonition,{type:"tip",children:(0,r.jsxs)(a.p,{children:["The ",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L51",children:"DataGenerator"}),"\ncan generate sample inserts and updates based on the the sample trip schema ",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L58",children:"here"})]})}),"\n",(0,r.jsx)(a.h2,{id:"create-table",children:"Create Table"}),"\n",(0,r.jsxs)(s.A,{groupId:"programming-language",defaultValue:"python",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"},{label:"Spark SQL",value:"sparksql"}],children:[(0,r.jsx)(o.A,{value:"scala",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-scala",children:"// scala\n// No separate create table command required in spark. First batch of write to a table will create the table if not exists. \n"})})}),(0,r.jsx)(o.A,{value:"python",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:"# pyspark\n# No separate create table command required in spark. First batch of write to a table will create the table if not exists.\n"})})}),(0,r.jsxs)(o.A,{value:"sparksql",children:[(0,r.jsx)(a.p,{children:"Spark SQL needs an explicit create table command."}),(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"Table Concepts"})}),(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:["\n",(0,r.jsx)(a.p,{children:"Table types"}),"\n",(0,r.jsxs)(a.p,{children:["Both Hudi's table types, Copy-On-Write (COW) and Merge-On-Read (MOR), can be created using Spark SQL.\nWhile creating the table, table type can be specified using ",(0,r.jsx)(a.strong,{children:"type"})," option: ",(0,r.jsx)(a.strong,{children:"type = 'cow'"})," or ",(0,r.jsx)(a.strong,{children:"type = 'mor'"}),"."]}),"\n"]}),"\n",(0,r.jsxs)(a.li,{children:["\n",(0,r.jsx)(a.p,{children:"Partitioned & Non-Partitioned tables"}),"\n",(0,r.jsxs)(a.p,{children:["Users can create a partitioned table or a non-partitioned table in Spark SQL. To create a partitioned table, one needs\nto use ",(0,r.jsx)(a.strong,{children:"partitioned by"})," statement to specify the partition columns to create a partitioned table. When there is\nno ",(0,r.jsx)(a.strong,{children:"partitioned by"})," statement with create table command, table is considered to be a non-partitioned table."]}),"\n"]}),"\n",(0,r.jsxs)(a.li,{children:["\n",(0,r.jsx)(a.p,{children:"Managed & External tables"}),"\n",(0,r.jsxs)(a.p,{children:["In general, Spark SQL supports two kinds of tables, namely managed and external. If one specifies a location using **\nlocation** statement or use ",(0,r.jsx)(a.code,{children:"create external table"})," to create table explicitly, it is an external table, else its\nconsidered a managed table. You can read more about external vs managed\ntables ",(0,r.jsx)(a.a,{href:"https://sparkbyexamples.com/apache-hive/difference-between-hive-internal-tables-and-external-tables/",children:"here"}),"."]}),"\n"]}),"\n"]}),(0,r.jsx)(a.p,{children:(0,r.jsxs)(a.em,{children:["Read more in the ",(0,r.jsx)(a.a,{href:"/docs/0.13.0/table_management",children:"table management"})," guide."]})}),(0,r.jsx)(a.admonition,{type:"note",children:(0,r.jsxs)(a.ol,{children:["\n",(0,r.jsxs)(a.li,{children:["Since Hudi 0.10.0, ",(0,r.jsx)(a.code,{children:"primaryKey"})," is required. It aligns with Hudi DataSource writer\u2019s and resolves behavioural\ndiscrepancies reported in previous versions. Non-primary-key tables are no longer supported. Any Hudi table created\npre-0.10.0 without a ",(0,r.jsx)(a.code,{children:"primaryKey"})," needs to be re-created with a ",(0,r.jsx)(a.code,{children:"primaryKey"})," field with 0.10.0."]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.code,{children:"primaryKey"}),", ",(0,r.jsx)(a.code,{children:"preCombineField"}),", and ",(0,r.jsx)(a.code,{children:"type"})," are case-sensitive."]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.code,{children:"preCombineField"})," is required for MOR tables."]}),"\n",(0,r.jsxs)(a.li,{children:["When set ",(0,r.jsx)(a.code,{children:"primaryKey"}),", ",(0,r.jsx)(a.code,{children:"preCombineField"}),", ",(0,r.jsx)(a.code,{children:"type"})," or other Hudi configs, ",(0,r.jsx)(a.code,{children:"tblproperties"})," is preferred over ",(0,r.jsx)(a.code,{children:"options"}),"."]}),"\n",(0,r.jsxs)(a.li,{children:["A new Hudi table created by Spark SQL will by default set ",(0,r.jsx)(a.code,{children:"hoodie.datasource.write.hive_style_partitioning=true"}),"."]}),"\n"]})}),(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"Create a Non-Partitioned Table"})}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"-- create a cow table, with primaryKey 'uuid' and without preCombineField provided\ncreate table hudi_cow_nonpcf_tbl (\n  uuid int,\n  name string,\n  price double\n) using hudi\ntblproperties (\n  primaryKey = 'uuid'\n);\n\n\n-- create a mor non-partitioned table with preCombineField provided\ncreate table hudi_mor_tbl (\n  id int,\n  name string,\n  price double,\n  ts bigint\n) using hudi\ntblproperties (\n  type = 'mor',\n  primaryKey = 'id',\n  preCombineField = 'ts'\n);\n"})}),(0,r.jsx)(a.p,{children:"Here is an example of creating an external COW partitioned table."}),(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"Create Partitioned Table"})}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"-- create a partitioned, preCombineField-provided cow table\ncreate table hudi_cow_pt_tbl (\n  id bigint,\n  name string,\n  ts bigint,\n  dt string,\n  hh string\n) using hudi\ntblproperties (\n  type = 'cow',\n  primaryKey = 'id',\n  preCombineField = 'ts'\n )\npartitioned by (dt, hh)\nlocation '/tmp/hudi/hudi_cow_pt_tbl';\n"})}),(0,r.jsx)(a.admonition,{type:"note",children:(0,r.jsxs)(a.p,{children:["You can also create a table partitioned by multiple fields by supplying comma-separated field names.\nWhen creating a table partitioned by multiple fields, ensure that you specify the columns in the ",(0,r.jsx)(a.code,{children:"PARTITIONED BY"})," clause\nin the same order as they appear in the ",(0,r.jsx)(a.code,{children:"CREATE TABLE"})," schema. For example, for the above table, the partition fields\nshould be specified as ",(0,r.jsx)(a.code,{children:"PARTITIONED BY (dt, hh)"}),"."]})}),(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"Create Table for an existing Hudi Table"})}),(0,r.jsx)(a.p,{children:"We can create a table on an existing hudi table(created with spark-shell or deltastreamer). This is useful to\nread/write to/from a pre-existing hudi table."}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"create table hudi_existing_tbl using hudi\nlocation '/tmp/hudi/hudi_existing_table';\n"})}),(0,r.jsx)(a.admonition,{type:"tip",children:(0,r.jsx)(a.p,{children:"You don't need to specify schema and any properties except the partitioned columns if existed. Hudi can automatically recognize the schema and configurations."})}),(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"CTAS"})}),(0,r.jsxs)(a.p,{children:["Hudi supports CTAS (Create Table As Select) on Spark SQL. ",(0,r.jsx)("br",{}),"\nNote: For better performance to load data to hudi table, CTAS uses the ",(0,r.jsx)(a.strong,{children:"bulk insert"})," as the write operation."]}),(0,r.jsx)(a.p,{children:"Example CTAS command to create a non-partitioned COW table without preCombineField."}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"-- CTAS: create a non-partitioned cow table without preCombineField\ncreate table hudi_ctas_cow_nonpcf_tbl\nusing hudi\ntblproperties (primaryKey = 'id')\nas\nselect 1 as id, 'a1' as name, 10 as price;\n"})}),(0,r.jsx)(a.p,{children:"Example CTAS command to create a partitioned, primary key COW table."}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"-- CTAS: create a partitioned, preCombineField-provided cow table\ncreate table hudi_ctas_cow_pt_tbl\nusing hudi\ntblproperties (type = 'cow', primaryKey = 'id', preCombineField = 'ts')\npartitioned by (dt)\nas\nselect 1 as id, 'a1' as name, 10 as price, 1000 as ts, '2021-12-01' as dt;\n\n"})}),(0,r.jsx)(a.p,{children:"Example CTAS command to load data from another table."}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"# create managed parquet table\ncreate table parquet_mngd using parquet location 'file:///tmp/parquet_dataset/*.parquet';\n\n# CTAS by loading data into hudi table\ncreate table hudi_ctas_cow_pt_tbl2 using hudi location 'file:/tmp/hudi/hudi_tbl/' options (\n  type = 'cow',\n  primaryKey = 'id',\n  preCombineField = 'ts'\n )\npartitioned by (datestr) as select * from parquet_mngd;\n"})}),(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"Create Table Properties"})}),(0,r.jsx)(a.p,{children:"Users can set table properties while creating a hudi table. Critical options are listed here."}),(0,r.jsxs)(a.table,{children:[(0,r.jsx)(a.thead,{children:(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.th,{children:"Parameter Name"}),(0,r.jsx)(a.th,{children:"Default"}),(0,r.jsx)(a.th,{children:"Introduction"})]})}),(0,r.jsxs)(a.tbody,{children:[(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"primaryKey"}),(0,r.jsx)(a.td,{children:"uuid"}),(0,r.jsxs)(a.td,{children:["The primary key names of the table, multiple fields separated by commas. Same as ",(0,r.jsx)(a.code,{children:"hoodie.datasource.write.recordkey.field"})]})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"preCombineField"}),(0,r.jsx)(a.td,{}),(0,r.jsxs)(a.td,{children:["The pre-combine field of the table. Same as ",(0,r.jsx)(a.code,{children:"hoodie.datasource.write.precombine.field"})]})]}),(0,r.jsxs)(a.tr,{children:[(0,r.jsx)(a.td,{children:"type"}),(0,r.jsx)(a.td,{children:"cow"}),(0,r.jsxs)(a.td,{children:["The table type to create. type = 'cow' means a COPY-ON-WRITE table, while type = 'mor' means a MERGE-ON-READ table. Same as ",(0,r.jsx)(a.code,{children:"hoodie.datasource.write.table.type"})]})]})]})]}),(0,r.jsx)(a.p,{children:'To set any custom hudi config(like index type, max parquet size, etc), see the  "Set hudi config section" .'})]})]}),"\n",(0,r.jsx)(a.h2,{id:"insert-data",children:"Insert data"}),"\n",(0,r.jsxs)(s.A,{groupId:"programming-language",defaultValue:"python",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"},{label:"Spark SQL",value:"sparksql"}],children:[(0,r.jsxs)(o.A,{value:"scala",children:[(0,r.jsx)(a.p,{children:"Generate some new trips, load them into a DataFrame and write the DataFrame into the Hudi table as below."}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-scala",children:'// spark-shell\nval inserts = convertToStringList(dataGen.generateInserts(10))\nval df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\ndf.write.format("hudi").\n  options(getQuickstartWriteConfigs).\n  option(PRECOMBINE_FIELD_OPT_KEY, "ts").\n  option(RECORDKEY_FIELD_OPT_KEY, "uuid").\n  option(PARTITIONPATH_FIELD_OPT_KEY, "partitionpath").\n  option(TABLE_NAME, tableName).\n  mode(Overwrite).\n  save(basePath)\n'})}),(0,r.jsx)(a.admonition,{type:"info",children:(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.code,{children:"mode(Overwrite)"})," overwrites and recreates the table if it already exists.\nYou can check the data generated under ",(0,r.jsx)(a.code,{children:"/tmp/hudi_trips_cow/<region>/<country>/<city>/"}),". We provided a record key\n(",(0,r.jsx)(a.code,{children:"uuid"})," in ",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/2e6e302efec2fa848ded4f88a95540ad2adb7798/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L60",children:"schema"}),"), partition field (",(0,r.jsx)(a.code,{children:"region/country/city"}),") and combine logic (",(0,r.jsx)(a.code,{children:"ts"})," in\n",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/2e6e302efec2fa848ded4f88a95540ad2adb7798/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L60",children:"schema"}),") to ensure trip records are unique within each partition. For more info, refer to\n",(0,r.jsx)(a.a,{href:"https://hudi.apache.org/learn/faq/#how-do-i-model-the-data-stored-in-hudi",children:"Modeling data stored in Hudi"}),"\nand for info on ways to ingest data into Hudi, refer to ",(0,r.jsx)(a.a,{href:"/docs/writing_data",children:"Writing Hudi Tables"}),".\nHere we are using the default write operation : ",(0,r.jsx)(a.code,{children:"upsert"}),". If you have a workload without updates, you can also issue\n",(0,r.jsx)(a.code,{children:"insert"})," or ",(0,r.jsx)(a.code,{children:"bulk_insert"})," operations which could be faster. To know more, refer to ",(0,r.jsx)(a.a,{href:"/docs/write_operations",children:"Write operations"})]})})]}),(0,r.jsxs)(o.A,{value:"python",children:[(0,r.jsx)(a.p,{children:"Generate some new trips, load them into a DataFrame and write the DataFrame into the Hudi table as below."}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:"# pyspark\ninserts = sc._jvm.org.apache.hudi.QuickstartUtils.convertToStringList(dataGen.generateInserts(10))\ndf = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\n\nhudi_options = {\n    'hoodie.table.name': tableName,\n    'hoodie.datasource.write.recordkey.field': 'uuid',\n    'hoodie.datasource.write.partitionpath.field': 'partitionpath',\n    'hoodie.datasource.write.table.name': tableName,\n    'hoodie.datasource.write.operation': 'upsert',\n    'hoodie.datasource.write.precombine.field': 'ts',\n    'hoodie.upsert.shuffle.parallelism': 2,\n    'hoodie.insert.shuffle.parallelism': 2\n}\n\ndf.write.format(\"hudi\"). \\\n    options(**hudi_options). \\\n    mode(\"overwrite\"). \\\n    save(basePath)\n"})}),(0,r.jsx)(a.admonition,{type:"info",children:(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.code,{children:"mode(Overwrite)"})," overwrites and recreates the table if it already exists.\nYou can check the data generated under ",(0,r.jsx)(a.code,{children:"/tmp/hudi_trips_cow/<region>/<country>/<city>/"}),". We provided a record key\n(",(0,r.jsx)(a.code,{children:"uuid"})," in ",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L58",children:"schema"}),"), partition field (",(0,r.jsx)(a.code,{children:"region/country/city"}),") and combine logic (",(0,r.jsx)(a.code,{children:"ts"})," in\n",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L58",children:"schema"}),") to ensure trip records are unique within each partition. For more info, refer to\n",(0,r.jsx)(a.a,{href:"https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=113709185#FAQ-HowdoImodelthedatastoredinHudi",children:"Modeling data stored in Hudi"}),"\nand for info on ways to ingest data into Hudi, refer to ",(0,r.jsx)(a.a,{href:"/docs/writing_data",children:"Writing Hudi Tables"}),".\nHere we are using the default write operation : ",(0,r.jsx)(a.code,{children:"upsert"}),". If you have a workload without updates, you can also issue\n",(0,r.jsx)(a.code,{children:"insert"})," or ",(0,r.jsx)(a.code,{children:"bulk_insert"})," operations which could be faster. To know more, refer to ",(0,r.jsx)(a.a,{href:"/docs/write_operations",children:"Write operations"})]})})]}),(0,r.jsxs)(o.A,{value:"sparksql",children:[(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"-- insert into non-partitioned table\ninsert into hudi_cow_nonpcf_tbl select 1, 'a1', 20;\ninsert into hudi_mor_tbl select 1, 'a1', 20, 1000;\n\n-- insert dynamic partition\ninsert into hudi_cow_pt_tbl partition (dt, hh)\nselect 1 as id, 'a1' as name, 1000 as ts, '2021-12-09' as dt, '10' as hh;\n\n-- insert static partition\ninsert into hudi_cow_pt_tbl partition(dt = '2021-12-09', hh='11') select 2, 'a2', 1000;\n"})}),(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"NOTICE"})}),(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:["By default,  if ",(0,r.jsx)(a.code,{children:"preCombineKey "}),"  is provided,  ",(0,r.jsx)(a.code,{children:"insert into"})," use ",(0,r.jsx)(a.code,{children:"upsert"})," as the type of write operation, otherwise use ",(0,r.jsx)(a.code,{children:"insert"}),"."]}),"\n",(0,r.jsxs)(a.li,{children:["We support to use ",(0,r.jsx)(a.code,{children:"bulk_insert"})," as the type of write operation, just need to set two configs: ",(0,r.jsx)(a.code,{children:"hoodie.sql.bulk.insert.enable"})," and ",(0,r.jsx)(a.code,{children:"hoodie.sql.insert.mode"}),". Example as follow:"]}),"\n"]}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"-- upsert mode for preCombineField-provided table\ninsert into hudi_mor_tbl select 1, 'a1_1', 20, 1001;\nselect id, name, price, ts from hudi_mor_tbl;\n1\ta1_1\t20.0\t1001\n\n-- bulk_insert mode for preCombineField-provided table\nset hoodie.sql.bulk.insert.enable=true;\nset hoodie.sql.insert.mode=non-strict;\n\ninsert into hudi_mor_tbl select 1, 'a1_2', 20, 1002;\nselect id, name, price, ts from hudi_mor_tbl;\n1\ta1_1\t20.0\t1001\n1\ta1_2\t20.0\t1002\n"})})]})]}),"\n",(0,r.jsxs)(a.p,{children:["Checkout ",(0,r.jsx)(a.a,{href:"https://hudi.apache.org/blog/2021/02/13/hudi-key-generators",children:"https://hudi.apache.org/blog/2021/02/13/hudi-key-generators"})," for various key generator options, like Timestamp based,\ncomplex, custom, NonPartitioned Key gen, etc."]}),"\n",(0,r.jsx)(a.admonition,{type:"tip",children:(0,r.jsxs)(a.p,{children:["With ",(0,r.jsx)(a.a,{href:"/docs/configurations#externalized-config-file",children:"externalized config file"}),",\ninstead of directly passing configuration settings to every Hudi job,\nyou can also centrally set them in a configuration file ",(0,r.jsx)(a.code,{children:"hudi-defaults.conf"}),"."]})}),"\n",(0,r.jsx)(a.h2,{id:"query-data",children:"Query data"}),"\n",(0,r.jsx)(a.p,{children:"Load the data files into a DataFrame."}),"\n",(0,r.jsxs)(s.A,{groupId:"programming-language",defaultValue:"python",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"},{label:"Spark SQL",value:"sparksql"}],children:[(0,r.jsx)(o.A,{value:"scala",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-scala",children:'// spark-shell\nval tripsSnapshotDF = spark.\n  read.\n  format("hudi").\n  load(basePath)\ntripsSnapshotDF.createOrReplaceTempView("hudi_trips_snapshot")\n\nspark.sql("select fare, begin_lon, begin_lat, ts from  hudi_trips_snapshot where fare > 20.0").show()\nspark.sql("select _hoodie_commit_time, _hoodie_record_key, _hoodie_partition_path, rider, driver, fare from  hudi_trips_snapshot").show()\n'})})}),(0,r.jsx)(o.A,{value:"python",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:'# pyspark\ntripsSnapshotDF = spark. \\\n  read. \\\n  format("hudi"). \\\n  load(basePath)\n# load(basePath) use "/partitionKey=partitionValue" folder structure for Spark auto partition discovery\n\ntripsSnapshotDF.createOrReplaceTempView("hudi_trips_snapshot")\n\nspark.sql("select fare, begin_lon, begin_lat, ts from  hudi_trips_snapshot where fare > 20.0").show()\nspark.sql("select _hoodie_commit_time, _hoodie_record_key, _hoodie_partition_path, rider, driver, fare from  hudi_trips_snapshot").show()\n'})})}),(0,r.jsx)(o.A,{value:"sparksql",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:" select fare, begin_lon, begin_lat, ts from  hudi_trips_snapshot where fare > 20.0\n"})})})]}),"\n",(0,r.jsx)(a.admonition,{type:"info",children:(0,r.jsxs)(a.p,{children:["Since 0.9.0 hudi has support a hudi built-in FileIndex: ",(0,r.jsx)(a.strong,{children:"HoodieFileIndex"}),' to query hudi table,\nwhich supports partition pruning and metatable for query. This will help improve query performance.\nIt also supports non-global query path which means users can query the table by the base path without\nspecifing the "*" in the query path. This feature has enabled by default for the non-global query path.\nFor the global query path, hudi uses the old query path.\nRefer to ',(0,r.jsx)(a.a,{href:"/docs/concepts#table-types--queries",children:"Table types and queries"})," for more info on all table types and query types supported."]})}),"\n",(0,r.jsx)(a.h3,{id:"time-travel-query",children:"Time Travel Query"}),"\n",(0,r.jsx)(a.p,{children:"Hudi supports time travel query since 0.9.0. Currently three query time formats are supported as given below."}),"\n",(0,r.jsxs)(s.A,{groupId:"programming-language",defaultValue:"python",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"},{label:"Spark SQL",value:"sparksql"}],children:[(0,r.jsx)(o.A,{value:"scala",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-scala",children:'spark.read.\n  format("hudi").\n  option("as.of.instant", "20210728141108100").\n  load(basePath)\n\nspark.read.\n  format("hudi").\n  option("as.of.instant", "2021-07-28 14:11:08.200").\n  load(basePath)\n\n// It is equal to "as.of.instant = 2021-07-28 00:00:00"\nspark.read.\n  format("hudi").\n  option("as.of.instant", "2021-07-28").\n  load(basePath)\n\n'})})}),(0,r.jsx)(o.A,{value:"python",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:'#pyspark\nspark.read. \\\n  format("hudi"). \\\n  option("as.of.instant", "20210728141108"). \\\n  load(basePath)\n\nspark.read. \\\n  format("hudi"). \\\n  option("as.of.instant", "2021-07-28 14:11:08.000"). \\\n  load(basePath)\n\n# It is equal to "as.of.instant = 2021-07-28 00:00:00"\nspark.read. \\\n  format("hudi"). \\\n  option("as.of.instant", "2021-07-28"). \\\n  load(basePath)\n'})})}),(0,r.jsxs)(o.A,{value:"sparksql",children:[(0,r.jsx)(a.admonition,{type:"note",children:(0,r.jsx)(a.p,{children:"Requires Spark 3.2+"})}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"create table hudi_cow_pt_tbl (\n  id bigint,\n  name string,\n  ts bigint,\n  dt string,\n  hh string\n) using hudi\ntblproperties (\n  type = 'cow',\n  primaryKey = 'id',\n  preCombineField = 'ts'\n )\npartitioned by (dt, hh)\nlocation '/tmp/hudi/hudi_cow_pt_tbl';\n\ninsert into hudi_cow_pt_tbl select 1, 'a0', 1000, '2021-12-09', '10';\nselect * from hudi_cow_pt_tbl;\n\n-- record id=1 changes `name`\ninsert into hudi_cow_pt_tbl select 1, 'a1', 1001, '2021-12-09', '10';\nselect * from hudi_cow_pt_tbl;\n\n-- time travel based on first commit time, assume `20220307091628793`\nselect * from hudi_cow_pt_tbl timestamp as of '20220307091628793' where id = 1;\n-- time travel based on different timestamp formats\nselect * from hudi_cow_pt_tbl timestamp as of '2022-03-07 09:16:28.100' where id = 1;\nselect * from hudi_cow_pt_tbl timestamp as of '2022-03-08' where id = 1;\n"})})]})]}),"\n",(0,r.jsx)(a.h2,{id:"update-data",children:"Update data"}),"\n",(0,r.jsx)(a.p,{children:"This is similar to inserting new data. Generate updates to existing trips using the data generator, load into a DataFrame\nand write DataFrame into the hudi table."}),"\n",(0,r.jsxs)(s.A,{groupId:"programming-language",defaultValue:"python",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"},{label:"Spark SQL",value:"sparksql"}],children:[(0,r.jsxs)(o.A,{value:"scala",children:[(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-scala",children:'// spark-shell\nval updates = convertToStringList(dataGen.generateUpdates(10))\nval df = spark.read.json(spark.sparkContext.parallelize(updates, 2))\ndf.write.format("hudi").\n  options(getQuickstartWriteConfigs).\n  option(PRECOMBINE_FIELD_OPT_KEY, "ts").\n  option(RECORDKEY_FIELD_OPT_KEY, "uuid").\n  option(PARTITIONPATH_FIELD_OPT_KEY, "partitionpath").\n  option(TABLE_NAME, tableName).\n  mode(Append).\n  save(basePath)\n'})}),(0,r.jsx)(a.admonition,{type:"note",children:(0,r.jsxs)(a.p,{children:["Notice that the save mode is now ",(0,r.jsx)(a.code,{children:"Append"}),". In general, always use append mode unless you are trying to create the table for the first time.\n",(0,r.jsx)(a.a,{href:"#query-data",children:"Querying"})," the data again will now show updated trips. Each write operation generates a new ",(0,r.jsx)(a.a,{href:"/docs/concepts",children:"commit"}),"\ndenoted by the timestamp. Look for changes in ",(0,r.jsx)(a.code,{children:"_hoodie_commit_time"}),", ",(0,r.jsx)(a.code,{children:"rider"}),", ",(0,r.jsx)(a.code,{children:"driver"})," fields for the same ",(0,r.jsx)(a.code,{children:"_hoodie_record_key"}),"s in previous commit."]})})]}),(0,r.jsxs)(o.A,{value:"sparksql",children:[(0,r.jsx)(a.p,{children:"Spark SQL supports two kinds of DML to update hudi table: Merge-Into and Update."}),(0,r.jsx)(a.h3,{id:"update",children:"Update"}),(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"Syntax"})}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"UPDATE tableIdentifier SET column = EXPRESSION(,column = EXPRESSION) [ WHERE boolExpression]\n"})}),(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"Case"})}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"update hudi_mor_tbl set price = price * 2, ts = 1111 where id = 1;\n\nupdate hudi_cow_pt_tbl set name = 'a1_1', ts = 1001 where id = 1;\n\n-- update using non-PK field\nupdate hudi_cow_pt_tbl set ts = 1001 where name = 'a1';\n"})}),(0,r.jsx)(a.admonition,{type:"note",children:(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.code,{children:"Update"})," operation requires ",(0,r.jsx)(a.code,{children:"preCombineField"})," specified."]})}),(0,r.jsx)(a.h3,{id:"mergeinto",children:"MergeInto"}),(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"Syntax"})}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"MERGE INTO tableIdentifier AS target_alias\nUSING (sub_query | tableIdentifier) AS source_alias\nON <merge_condition>\n[ WHEN MATCHED [ AND <condition> ] THEN <matched_action> ]\n[ WHEN NOT MATCHED [ AND <condition> ]  THEN <not_matched_action> ]\n\n<merge_condition> =A equal bool condition \n<matched_action>  =\n  DELETE  |\n  UPDATE SET *  |\n  UPDATE SET column1 = expression1 [, column2 = expression2 ...]\n<not_matched_action>  =\n  INSERT *  |\n  INSERT (column1 [, column2 ...]) VALUES (value1 [, value2 ...])\n"})}),(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"Example"})}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"-- source table using hudi for testing merging into non-partitioned table\ncreate table merge_source (id int, name string, price double, ts bigint) using hudi\ntblproperties (primaryKey = 'id', preCombineField = 'ts');\ninsert into merge_source values (1, \"old_a1\", 22.22, 900), (2, \"new_a2\", 33.33, 2000), (3, \"new_a3\", 44.44, 2000);\n\nmerge into hudi_mor_tbl as target\nusing merge_source as source\non target.id = source.id\nwhen matched then update set *\nwhen not matched then insert *\n;\n\n-- source table using parquet for testing merging into partitioned table\ncreate table merge_source2 (id int, name string, flag string, dt string, hh string) using parquet;\ninsert into merge_source2 values (1, \"new_a1\", 'update', '2021-12-09', '10'), (2, \"new_a2\", 'delete', '2021-12-09', '11'), (3, \"new_a3\", 'insert', '2021-12-09', '12');\n\nmerge into hudi_cow_pt_tbl as target\nusing (\n  select id, name, '1000' as ts, flag, dt, hh from merge_source2\n) source\non target.id = source.id\nwhen matched and flag != 'delete' then\n update set id = source.id, name = source.name, ts = source.ts, dt = source.dt, hh = source.hh\nwhen matched and flag = 'delete' then delete\nwhen not matched then\n insert (id, name, ts, dt, hh) values(source.id, source.name, source.ts, source.dt, source.hh)\n;\n\n"})})]}),(0,r.jsxs)(o.A,{value:"python",children:[(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:'# pyspark\nupdates = sc._jvm.org.apache.hudi.QuickstartUtils.convertToStringList(dataGen.generateUpdates(10))\ndf = spark.read.json(spark.sparkContext.parallelize(updates, 2))\ndf.write.format("hudi"). \\\n  options(**hudi_options). \\\n  mode("append"). \\\n  save(basePath)\n'})}),(0,r.jsx)(a.admonition,{type:"note",children:(0,r.jsxs)(a.p,{children:["Notice that the save mode is now ",(0,r.jsx)(a.code,{children:"Append"}),". In general, always use append mode unless you are trying to create the table for the first time.\n",(0,r.jsx)(a.a,{href:"#query-data",children:"Querying"})," the data again will now show updated trips. Each write operation generates a new ",(0,r.jsx)(a.a,{href:"/docs/concepts",children:"commit"}),"\ndenoted by the timestamp. Look for changes in ",(0,r.jsx)(a.code,{children:"_hoodie_commit_time"}),", ",(0,r.jsx)(a.code,{children:"rider"}),", ",(0,r.jsx)(a.code,{children:"driver"})," fields for the same ",(0,r.jsx)(a.code,{children:"_hoodie_record_key"}),"s in previous commit."]})})]})]}),"\n",(0,r.jsx)(a.h2,{id:"incremental-query",children:"Incremental query"}),"\n",(0,r.jsx)(a.p,{children:"Hudi also provides capability to obtain a stream of records that changed since given commit timestamp.\nThis can be achieved using Hudi's incremental querying and providing a begin time from which changes need to be streamed.\nWe do not need to specify endTime, if we want all changes after the given commit (as is the common case)."}),"\n",(0,r.jsxs)(s.A,{groupId:"programming-language",defaultValue:"python",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"}],children:[(0,r.jsx)(o.A,{value:"scala",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-scala",children:'// spark-shell\n// reload data\nspark.\n  read.\n  format("hudi").\n  load(basePath).\n  createOrReplaceTempView("hudi_trips_snapshot")\n\nval commits = spark.sql("select distinct(_hoodie_commit_time) as commitTime from  hudi_trips_snapshot order by commitTime").map(k => k.getString(0)).take(50)\nval beginTime = commits(commits.length - 2) // commit time we are interested in\n\n// incrementally query data\nval tripsIncrementalDF = spark.read.format("hudi").\n  option(QUERY_TYPE_OPT_KEY, QUERY_TYPE_INCREMENTAL_OPT_VAL).\n  option(BEGIN_INSTANTTIME_OPT_KEY, beginTime).\n  load(basePath)\ntripsIncrementalDF.createOrReplaceTempView("hudi_trips_incremental")\n\nspark.sql("select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from  hudi_trips_incremental where fare > 20.0").show()\n'})})}),(0,r.jsx)(o.A,{value:"python",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:'# pyspark\n# reload data\nspark. \\\n  read. \\\n  format("hudi"). \\\n  load(basePath). \\\n  createOrReplaceTempView("hudi_trips_snapshot")\n\ncommits = list(map(lambda row: row[0], spark.sql("select distinct(_hoodie_commit_time) as commitTime from  hudi_trips_snapshot order by commitTime").limit(50).collect()))\nbeginTime = commits[len(commits) - 2] # commit time we are interested in\n\n# incrementally query data\nincremental_read_options = {\n  \'hoodie.datasource.query.type\': \'incremental\',\n  \'hoodie.datasource.read.begin.instanttime\': beginTime,\n}\n\ntripsIncrementalDF = spark.read.format("hudi"). \\\n  options(**incremental_read_options). \\\n  load(basePath)\ntripsIncrementalDF.createOrReplaceTempView("hudi_trips_incremental")\n\nspark.sql("select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from  hudi_trips_incremental where fare > 20.0").show()\n'})})})]}),"\n",(0,r.jsx)(a.admonition,{type:"info",children:(0,r.jsx)(a.p,{children:"This will give all changes that happened after the beginTime commit with the filter of fare > 20.0. The unique thing about this\nfeature is that it now lets you author streaming pipelines on batch data."})}),"\n",(0,r.jsx)(a.h2,{id:"structured-streaming",children:"Structured Streaming"}),"\n",(0,r.jsx)(a.p,{children:"Hudi supports Spark Structured Streaming reads and writes.\nStructured Streaming reads are based on Hudi Incremental Query feature, therefore streaming read can return data for which commits and base files were not yet removed by the cleaner. You can control commits retention time."}),"\n",(0,r.jsx)(a.h3,{id:"streaming-read",children:"Streaming Read"}),"\n",(0,r.jsxs)(s.A,{groupId:"programming-language",defaultValue:"python",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"}],children:[(0,r.jsx)(o.A,{value:"scala",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-scala",children:'// spark-shell\n// reload data\ndf.write.format("hudi").\n  options(getQuickstartWriteConfigs).\n  option(PRECOMBINE_FIELD_OPT_KEY, "ts").\n  option(RECORDKEY_FIELD_OPT_KEY, "uuid").\n  option(PARTITIONPATH_FIELD_OPT_KEY, "partitionpath").\n  option(TABLE_NAME, tableName).\n  mode(Overwrite).\n  save(basePath)\n\n// read stream and output results to console\nspark.readStream.\n  format("hudi").\n  load(basePath).\n  writeStream.\n  format("console").\n  start()\n\n// read stream to streaming df\nval df = spark.readStream.\n        format("hudi").\n        load(basePath)\n\n'})})}),(0,r.jsx)(o.A,{value:"python",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:"# pyspark\n# reload data\ninserts = sc._jvm.org.apache.hudi.QuickstartUtils.convertToStringList(\n    dataGen.generateInserts(10))\ndf = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\n\nhudi_options = {\n    'hoodie.table.name': tableName,\n    'hoodie.datasource.write.recordkey.field': 'uuid',\n    'hoodie.datasource.write.partitionpath.field': 'partitionpath',\n    'hoodie.datasource.write.table.name': tableName,\n    'hoodie.datasource.write.operation': 'upsert',\n    'hoodie.datasource.write.precombine.field': 'ts',\n    'hoodie.upsert.shuffle.parallelism': 2,\n    'hoodie.insert.shuffle.parallelism': 2\n}\n\ndf.write.format(\"hudi\"). \\\n    options(**hudi_options). \\\n    mode(\"overwrite\"). \\\n    save(basePath)\n\n# read stream to streaming df\ndf = spark.readStream \\\n    .format(\"hudi\") \\\n    .load(basePath)\n\n# ead stream and output results to console\nspark.readStream \\\n    .format(\"hudi\") \\\n    .load(basePath) \\\n    .writeStream \\\n    .format(\"console\") \\\n    .start()\n\n"})})})]}),"\n",(0,r.jsx)(a.h3,{id:"streaming-write",children:"Streaming Write"}),"\n",(0,r.jsxs)(s.A,{groupId:"programming-language",defaultValue:"python",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"}],children:[(0,r.jsx)(o.A,{value:"scala",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-scala",children:'// spark-shell\n// prepare to stream write to new table\nimport org.apache.spark.sql.streaming.Trigger\n\nval streamingTableName = "hudi_trips_cow_streaming"\nval baseStreamingPath = "file:///tmp/hudi_trips_cow_streaming"\nval checkpointLocation = "file:///tmp/checkpoints/hudi_trips_cow_streaming"\n\n// create streaming df\nval df = spark.readStream.\n        format("hudi").\n        load(basePath)\n\n// write stream to new hudi table\ndf.writeStream.format("hudi").\n  options(getQuickstartWriteConfigs).\n  option(PRECOMBINE_FIELD_OPT_KEY, "ts").\n  option(RECORDKEY_FIELD_OPT_KEY, "uuid").\n  option(PARTITIONPATH_FIELD_OPT_KEY, "partitionpath").\n  option(TABLE_NAME, streamingTableName).\n  outputMode("append").\n  option("path", baseStreamingPath).\n  option("checkpointLocation", checkpointLocation).\n  trigger(Trigger.Once()).\n  start()\n\n'})})}),(0,r.jsx)(o.A,{value:"python",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:"# pyspark\n# prepare to stream write to new table\nstreamingTableName = \"hudi_trips_cow_streaming\"\nbaseStreamingPath = \"file:///tmp/hudi_trips_cow_streaming\"\ncheckpointLocation = \"file:///tmp/checkpoints/hudi_trips_cow_streaming\"\n\nhudi_streaming_options = {\n    'hoodie.table.name': streamingTableName,\n    'hoodie.datasource.write.recordkey.field': 'uuid',\n    'hoodie.datasource.write.partitionpath.field': 'partitionpath',\n    'hoodie.datasource.write.table.name': streamingTableName,\n    'hoodie.datasource.write.operation': 'upsert',\n    'hoodie.datasource.write.precombine.field': 'ts',\n    'hoodie.upsert.shuffle.parallelism': 2,\n    'hoodie.insert.shuffle.parallelism': 2\n}\n\n# create streaming df\ndf = spark.readStream \\\n    .format(\"hudi\") \\\n    .load(basePath)\n\n# write stream to new hudi table\ndf.writeStream.format(\"hudi\") \\\n    .options(**hudi_streaming_options) \\\n    .outputMode(\"append\") \\\n    .option(\"path\", baseStreamingPath) \\\n    .option(\"checkpointLocation\", checkpointLocation) \\\n    .trigger(once=True) \\\n    .start()\n\n"})})})]}),"\n",(0,r.jsx)(a.admonition,{type:"info",children:(0,r.jsx)(a.p,{children:"Spark SQL can be used within ForeachBatch sink to do INSERT, UPDATE, DELETE and MERGE INTO.\nTarget table must exist before write."})}),"\n",(0,r.jsx)(a.h3,{id:"table-maintenance",children:"Table maintenance"}),"\n",(0,r.jsxs)(a.p,{children:["Hudi can run async or inline table services while running Structured Streaming query and takes care of cleaning, compaction and clustering. There's no operational overhead for the user.",(0,r.jsx)(a.br,{}),"\n","For CoW tables, table services work in inline mode by default.",(0,r.jsx)(a.br,{}),"\n","For MoR tables, some async services are enabled by default."]}),"\n",(0,r.jsxs)(a.admonition,{type:"note",children:[(0,r.jsxs)(a.p,{children:["Since Hudi 0.11 Metadata Table is enabled by default. When using async table services with Metadata Table enabled you must use Optimistic Concurrency Control to avoid the risk of data loss (even in single writer scenario). See ",(0,r.jsx)(a.a,{href:"metadata#deployment-considerations",children:"Metadata Table deployment considerations"})," for detailed instructions."]}),(0,r.jsx)(a.p,{children:"If you're using Foreach or ForeachBatch streaming sink you must use inline table services, async table services are not supported."})]}),"\n",(0,r.jsx)(a.p,{children:"Hive Sync works with Structured Streaming, it will create table if not exists and synchronize table to metastore aftear each streaming write."}),"\n",(0,r.jsx)(a.h2,{id:"point-in-time-query",children:"Point in time query"}),"\n",(0,r.jsx)(a.p,{children:'Lets look at how to query data as of a specific time. The specific time can be represented by pointing endTime to a\nspecific commit time and beginTime to "000" (denoting earliest possible commit time).'}),"\n",(0,r.jsxs)(s.A,{groupId:"programming-language",defaultValue:"python",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"}],children:[(0,r.jsx)(o.A,{value:"scala",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-scala",children:'// spark-shell\nval beginTime = "000" // Represents all commits > this time.\nval endTime = commits(commits.length - 2) // commit time we are interested in\n\n//incrementally query data\nval tripsPointInTimeDF = spark.read.format("hudi").\n  option(QUERY_TYPE_OPT_KEY, QUERY_TYPE_INCREMENTAL_OPT_VAL).\n  option(BEGIN_INSTANTTIME_OPT_KEY, beginTime).\n  option(END_INSTANTTIME_OPT_KEY, endTime).\n  load(basePath)\ntripsPointInTimeDF.createOrReplaceTempView("hudi_trips_point_in_time")\nspark.sql("select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from hudi_trips_point_in_time where fare > 20.0").show()\n'})})}),(0,r.jsx)(o.A,{value:"python",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:"# pyspark\nbeginTime = \"000\" # Represents all commits > this time.\nendTime = commits[len(commits) - 2]\n\n# query point in time data\npoint_in_time_read_options = {\n  'hoodie.datasource.query.type': 'incremental',\n  'hoodie.datasource.read.end.instanttime': endTime,\n  'hoodie.datasource.read.begin.instanttime': beginTime\n}\n\ntripsPointInTimeDF = spark.read.format(\"hudi\"). \\\n  options(**point_in_time_read_options). \\\n  load(basePath)\n\ntripsPointInTimeDF.createOrReplaceTempView(\"hudi_trips_point_in_time\")\nspark.sql(\"select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from hudi_trips_point_in_time where fare > 20.0\").show()\n"})})})]}),"\n",(0,r.jsx)(a.h2,{id:"deletes",children:"Delete data"}),"\n",(0,r.jsxs)(a.p,{children:["Apache Hudi supports two types of deletes: ",(0,r.jsx)("br",{})]}),"\n",(0,r.jsxs)(a.ol,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Soft Deletes"}),": This retains the record key and just nulls out the values for all the other fields. The records with nulls in soft deletes are always persisted in storage and never removed."]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.strong,{children:"Hard Deletes"}),": This physically removes any trace of the record from the table. Check out the\n",(0,r.jsx)(a.a,{href:"/docs/writing_data#deletes",children:"deletion section"})," for more details."]}),"\n"]}),"\n",(0,r.jsx)(a.h3,{id:"soft-deletes",children:"Soft Deletes"}),"\n",(0,r.jsxs)(a.p,{children:["Soft deletes retain the record key and null out the values for all the other fields. For example, records with nulls in soft deletes are always persisted in storage and never removed.",(0,r.jsx)("br",{}),(0,r.jsx)("br",{})]}),"\n",(0,r.jsxs)(s.A,{groupId:"programming-language",defaultValue:"python",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"}],children:[(0,r.jsxs)(o.A,{value:"scala",children:[(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-scala",children:'// spark-shell\nspark.\n  read.\n  format("hudi").\n  load(basePath).\n  createOrReplaceTempView("hudi_trips_snapshot")\n// fetch total records count\nspark.sql("select uuid, partitionpath from hudi_trips_snapshot").count()\nspark.sql("select uuid, partitionpath from hudi_trips_snapshot where rider is not null").count()\n// fetch two records for soft deletes\nval softDeleteDs = spark.sql("select * from hudi_trips_snapshot").limit(2)\n\n// prepare the soft deletes by ensuring the appropriate fields are nullified\nval nullifyColumns = softDeleteDs.schema.fields.\n  map(field => (field.name, field.dataType.typeName)).\n  filter(pair => (!HoodieRecord.HOODIE_META_COLUMNS.contains(pair._1)\n    && !Array("ts", "uuid", "partitionpath").contains(pair._1)))\n\nval softDeleteDf = nullifyColumns.\n  foldLeft(softDeleteDs.drop(HoodieRecord.HOODIE_META_COLUMNS: _*))(\n    (ds, col) => ds.withColumn(col._1, lit(null).cast(col._2)))\n\n// simply upsert the table after setting these fields to null\nsoftDeleteDf.write.format("hudi").\n  options(getQuickstartWriteConfigs).\n  option(OPERATION_OPT_KEY, "upsert").\n  option(PRECOMBINE_FIELD_OPT_KEY, "ts").\n  option(RECORDKEY_FIELD_OPT_KEY, "uuid").\n  option(PARTITIONPATH_FIELD_OPT_KEY, "partitionpath").\n  option(TABLE_NAME, tableName).\n  mode(Append).\n  save(basePath)\n\n// reload data\nspark.\n  read.\n  format("hudi").\n  load(basePath).\n  createOrReplaceTempView("hudi_trips_snapshot")\n\n// This should return the same total count as before\nspark.sql("select uuid, partitionpath from hudi_trips_snapshot").count()\n// This should return (total - 2) count as two records are updated with nulls\nspark.sql("select uuid, partitionpath from hudi_trips_snapshot where rider is not null").count()\n'})}),(0,r.jsx)(a.admonition,{type:"note",children:(0,r.jsxs)(a.p,{children:["Notice that the save mode is ",(0,r.jsx)(a.code,{children:"Append"}),"."]})})]}),(0,r.jsxs)(o.A,{value:"python",children:[(0,r.jsx)(a.admonition,{type:"note",children:(0,r.jsxs)(a.p,{children:["Notice that the save mode is ",(0,r.jsx)(a.code,{children:"Append"}),"."]})}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:'# pyspark\nfrom pyspark.sql.functions import lit\nfrom functools import reduce\n\nspark.read.format("hudi"). \\\n  load(basePath). \\\n  createOrReplaceTempView("hudi_trips_snapshot")\n\n# fetch total records count\nspark.sql("select uuid, partitionpath from hudi_trips_snapshot").count()\nspark.sql("select uuid, partitionpath from hudi_trips_snapshot where rider is not null").count()\n\n# fetch two records for soft deletes\nsoft_delete_ds = spark.sql("select * from hudi_trips_snapshot").limit(2)\n\n# prepare the soft deletes by ensuring the appropriate fields are nullified\nmeta_columns = ["_hoodie_commit_time", "_hoodie_commit_seqno", "_hoodie_record_key", \\\n  "_hoodie_partition_path", "_hoodie_file_name"]\nexcluded_columns = meta_columns + ["ts", "uuid", "partitionpath"]\n'})}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:"nullify_columns = list(filter(lambda field: field[0] not in excluded_columns, \\\n  list(map(lambda field: (field.name, field.dataType), soft_delete_ds.schema.fields))))\n\nhudi_soft_delete_options = {\n  'hoodie.table.name': tableName,\n  'hoodie.datasource.write.recordkey.field': 'uuid',\n  'hoodie.datasource.write.partitionpath.field': 'partitionpath',\n  'hoodie.datasource.write.table.name': tableName,\n  'hoodie.datasource.write.operation': 'upsert',\n  'hoodie.datasource.write.precombine.field': 'ts',\n  'hoodie.upsert.shuffle.parallelism': 2, \n  'hoodie.insert.shuffle.parallelism': 2\n}\n\nsoft_delete_df = reduce(lambda df,col: df.withColumn(col[0], lit(None).cast(col[1])), \\\n  nullify_columns, reduce(lambda df,col: df.drop(col[0]), meta_columns, soft_delete_ds))\n\n# simply upsert the table after setting these fields to null\nsoft_delete_df.write.format(\"hudi\"). \\\n  options(**hudi_soft_delete_options). \\\n  mode(\"append\"). \\\n  save(basePath)\n\n# reload data\nspark.read.format(\"hudi\"). \\\n  load(basePath). \\\n  createOrReplaceTempView(\"hudi_trips_snapshot\")\n\n# This should return the same total count as before\nspark.sql(\"select uuid, partitionpath from hudi_trips_snapshot\").count()\n# This should return (total - 2) count as two records are updated with nulls\nspark.sql(\"select uuid, partitionpath from hudi_trips_snapshot where rider is not null\").count()\n"})})]})]}),"\n",(0,r.jsx)(a.h3,{id:"hard-deletes",children:"Hard Deletes"}),"\n",(0,r.jsxs)(a.p,{children:["Hard deletes physically remove any trace of the record from the table. For example, this deletes records for the HoodieKeys passed in.",(0,r.jsx)("br",{}),(0,r.jsx)("br",{})]}),"\n",(0,r.jsxs)(s.A,{groupId:"programming-language",defaultValue:"python",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"},{label:"Spark SQL",value:"sparksql"}],children:[(0,r.jsxs)(o.A,{value:"scala",children:[(0,r.jsxs)(a.p,{children:["Delete records for the HoodieKeys passed in.",(0,r.jsx)("br",{})]}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-scala",children:'// spark-shell\n// fetch total records count\nspark.sql("select uuid, partitionpath from hudi_trips_snapshot").count()\n// fetch two records to be deleted\nval ds = spark.sql("select uuid, partitionpath from hudi_trips_snapshot").limit(2)\n\n// issue deletes\nval deletes = dataGen.generateDeletes(ds.collectAsList())\nval hardDeleteDf = spark.read.json(spark.sparkContext.parallelize(deletes, 2))\n\nhardDeleteDf.write.format("hudi").\n  options(getQuickstartWriteConfigs).\n  option(OPERATION_OPT_KEY, "delete").\n  option(PRECOMBINE_FIELD_OPT_KEY, "ts").\n  option(RECORDKEY_FIELD_OPT_KEY, "uuid").\n  option(PARTITIONPATH_FIELD_OPT_KEY, "partitionpath").\n  option(TABLE_NAME, tableName).\n  mode(Append).\n  save(basePath)\n\n// run the same read query as above.\nval roAfterDeleteViewDF = spark.\n  read.\n  format("hudi").\n  load(basePath)\n\nroAfterDeleteViewDF.registerTempTable("hudi_trips_snapshot")\n// fetch should return (total - 2) records\nspark.sql("select uuid, partitionpath from hudi_trips_snapshot").count()\n'})}),(0,r.jsx)(a.admonition,{type:"note",children:(0,r.jsxs)(a.p,{children:["Only ",(0,r.jsx)(a.code,{children:"Append"})," mode is supported for delete operation."]})})]}),(0,r.jsxs)(o.A,{value:"sparksql",children:[(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"Syntax"})}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"DELETE FROM tableIdentifier [ WHERE BOOL_EXPRESSION]\n"})}),(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"Example"})}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"delete from hudi_cow_nonpcf_tbl where uuid = 1;\n\ndelete from hudi_mor_tbl where id % 2 = 0;\n\n-- delete using non-PK field\ndelete from hudi_cow_pt_tbl where name = 'a1';\n"})})]}),(0,r.jsxs)(o.A,{value:"python",children:[(0,r.jsx)(a.admonition,{type:"note",children:(0,r.jsxs)(a.p,{children:["Only ",(0,r.jsx)(a.code,{children:"Append"})," mode is supported for delete operation."]})}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:"# pyspark\n# fetch total records count\nspark.sql(\"select uuid, partitionpath from hudi_trips_snapshot\").count()\n# fetch two records to be deleted\nds = spark.sql(\"select uuid, partitionpath from hudi_trips_snapshot\").limit(2)\n\n# issue deletes\nhudi_hard_delete_options = {\n  'hoodie.table.name': tableName,\n  'hoodie.datasource.write.recordkey.field': 'uuid',\n  'hoodie.datasource.write.partitionpath.field': 'partitionpath',\n  'hoodie.datasource.write.table.name': tableName,\n  'hoodie.datasource.write.operation': 'delete',\n  'hoodie.datasource.write.precombine.field': 'ts',\n  'hoodie.upsert.shuffle.parallelism': 2, \n  'hoodie.insert.shuffle.parallelism': 2\n}\n\nfrom pyspark.sql.functions import lit\ndeletes = list(map(lambda row: (row[0], row[1]), ds.collect()))\nhard_delete_df = spark.sparkContext.parallelize(deletes).toDF(['uuid', 'partitionpath']).withColumn('ts', lit(0.0))\nhard_delete_df.write.format(\"hudi\"). \\\n  options(**hudi_hard_delete_options). \\\n  mode(\"append\"). \\\n  save(basePath)\n\n# run the same read query as above.\nroAfterDeleteViewDF = spark. \\\n  read. \\\n  format(\"hudi\"). \\\n  load(basePath)\n"})}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:'roAfterDeleteViewDF.createOrReplaceTempView("hudi_trips_snapshot") \n\n# fetch should return (total - 2) records\nspark.sql("select uuid, partitionpath from hudi_trips_snapshot").count()\n'})})]})]}),"\n",(0,r.jsx)(a.h2,{id:"insert-overwrite",children:"Insert Overwrite"}),"\n",(0,r.jsxs)(a.p,{children:["Generate some new trips, overwrite the all the partitions that are present in the input. This operation can be faster\nthan ",(0,r.jsx)(a.code,{children:"upsert"})," for batch ETL jobs, that are recomputing entire target partitions at once (as opposed to incrementally\nupdating the target tables). This is because, we are able to bypass indexing, precombining and other repartitioning\nsteps in the upsert write path completely."]}),"\n",(0,r.jsxs)(s.A,{groupId:"programming-language",defaultValue:"python",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"},{label:"Spark SQL",value:"sparksql"}],children:[(0,r.jsx)(o.A,{value:"scala",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-scala",children:'// spark-shell\nspark.\n  read.format("hudi").\n  load(basePath).\n  select("uuid","partitionpath").\n  sort("partitionpath","uuid").\n  show(100, false)\n\nval inserts = convertToStringList(dataGen.generateInserts(10))\nval df = spark.\n  read.json(spark.sparkContext.parallelize(inserts, 2)).\n  filter("partitionpath = \'americas/united_states/san_francisco\'")\ndf.write.format("hudi").\n  options(getQuickstartWriteConfigs).\n  option(OPERATION.key(),"insert_overwrite").\n  option(PRECOMBINE_FIELD.key(), "ts").\n  option(RECORDKEY_FIELD.key(), "uuid").\n  option(PARTITIONPATH_FIELD.key(), "partitionpath").\n  option(TBL_NAME.key(), tableName).\n  mode(Append).\n  save(basePath)\n\n// Should have different keys now for San Francisco alone, from query before.\nspark.\n  read.format("hudi").\n  load(basePath).\n  select("uuid","partitionpath").\n  sort("partitionpath","uuid").\n  show(100, false)\n'})})}),(0,r.jsx)(o.A,{value:"python",children:(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:"# pyspark\nspark.read.format(\"hudi\"). \\\n    load(basePath). \\\n    select([\"uuid\", \"partitionpath\"]). \\\n    sort([\"partitionpath\", \"uuid\"]). \\\n    show(n=100, truncate=False)\n    \ninserts = sc._jvm.org.apache.hudi.QuickstartUtils.convertToStringList(dataGen.generateInserts(10)) \ndf = spark.read.json(spark.sparkContext.parallelize(inserts, 2)). \\\n    filter(\"partitionpath = 'americas/united_states/san_francisco'\")\nhudi_insert_overwrite_options = {\n    'hoodie.table.name': tableName,\n    'hoodie.datasource.write.recordkey.field': 'uuid',\n    'hoodie.datasource.write.partitionpath.field': 'partitionpath',\n    'hoodie.datasource.write.table.name': tableName,\n    'hoodie.datasource.write.operation': 'insert_overwrite',\n    'hoodie.datasource.write.precombine.field': 'ts',\n    'hoodie.upsert.shuffle.parallelism': 2,\n    'hoodie.insert.shuffle.parallelism': 2\n}\ndf.write.format(\"hudi\").options(**hudi_insert_overwrite_options).mode(\"append\").save(basePath)\nspark.read.format(\"hudi\"). \\\n    load(basePath). \\\n    select([\"uuid\", \"partitionpath\"]). \\\n    sort([\"partitionpath\", \"uuid\"]). \\\n    show(n=100, truncate=False)\n"})})}),(0,r.jsxs)(o.A,{value:"sparksql",children:[(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.code,{children:"insert overwrite"})," a partitioned table use the ",(0,r.jsx)(a.code,{children:"INSERT_OVERWRITE"})," type of write operation, while a non-partitioned table to ",(0,r.jsx)(a.code,{children:"INSERT_OVERWRITE_TABLE"}),"."]}),(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"-- insert overwrite non-partitioned table\ninsert overwrite hudi_mor_tbl select 99, 'a99', 20.0, 900;\ninsert overwrite hudi_cow_nonpcf_tbl select 99, 'a99', 20.0;\n\n-- insert overwrite partitioned table with dynamic partition\ninsert overwrite table hudi_cow_pt_tbl select 10, 'a10', 1100, '2021-12-09', '10';\n\n-- insert overwrite partitioned table with static partition\ninsert overwrite hudi_cow_pt_tbl partition(dt = '2021-12-09', hh='12') select 13, 'a13', 1100;\n"})})]})]}),"\n",(0,r.jsx)(a.h2,{id:"more-spark-sql-commands",children:"More Spark SQL Commands"}),"\n",(0,r.jsx)(a.h3,{id:"alter-table",children:"Alter Table"}),"\n",(0,r.jsxs)(a.p,{children:["Schema evolution can be achieved via ",(0,r.jsx)(a.code,{children:"ALTER TABLE"})," commands. Below shows some basic examples."]}),"\n",(0,r.jsx)(a.admonition,{type:"note",children:(0,r.jsxs)(a.p,{children:["For more detailed examples, please prefer to ",(0,r.jsx)(a.a,{href:"schema_evolution",children:"schema evolution"})]})}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"Syntax"})}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"-- Alter table name\nALTER TABLE oldTableName RENAME TO newTableName\n\n-- Alter table add columns\nALTER TABLE tableIdentifier ADD COLUMNS(colAndType (,colAndType)*)\n\n-- Alter table column type\nALTER TABLE tableIdentifier CHANGE COLUMN colName colName colType\n\n-- Alter table properties\nALTER TABLE tableIdentifier SET TBLPROPERTIES (key = 'value')\n"})}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"Examples"})}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"--rename to:\nALTER TABLE hudi_cow_nonpcf_tbl RENAME TO hudi_cow_nonpcf_tbl2;\n\n--add column:\nALTER TABLE hudi_cow_nonpcf_tbl2 add columns(remark string);\n\n--change column:\nALTER TABLE hudi_cow_nonpcf_tbl2 change column uuid uuid bigint;\n\n--set properties;\nalter table hudi_cow_nonpcf_tbl2 set tblproperties (hoodie.keep.max.commits = '10');\n"})}),"\n",(0,r.jsx)(a.h3,{id:"partition-sql-command",children:"Partition SQL Command"}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"Syntax"})}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"-- Drop Partition\nALTER TABLE tableIdentifier DROP PARTITION ( partition_col_name = partition_col_val [ , ... ] )\n\n-- Show Partitions\nSHOW PARTITIONS tableIdentifier\n"})}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"Examples"})}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"--show partition:\nshow partitions hudi_cow_pt_tbl;\n\n--drop partition\uff1a\nalter table hudi_cow_pt_tbl drop partition (dt='2021-12-09', hh='10');\n"})}),"\n",(0,r.jsx)(a.admonition,{type:"note",children:(0,r.jsxs)(a.p,{children:["Currently,  the result of ",(0,r.jsx)(a.code,{children:"show partitions"})," is based on the filesystem table path. It's not precise when delete the whole partition data or drop certain partition directly."]})}),"\n",(0,r.jsx)(a.h3,{id:"procedures",children:"Procedures"}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"Syntax"})}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"--Call procedure by positional arguments\nCALL system.procedure_name(arg_1, arg_2, ... arg_n)\n\n--Call procedure by named arguments\nCALL system.procedure_name(arg_name_2 => arg_2, arg_name_1 => arg_1, ... arg_name_n => arg_n)\n"})}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"Examples"})}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-sql",children:"--show commit's info\ncall show_commits(table => 'test_hudi_table', limit => 10);\n"})}),"\n",(0,r.jsxs)(a.p,{children:["Call command has already support some commit procedures and table optimization procedures,\nmore details please refer to ",(0,r.jsx)(a.a,{href:"procedures",children:"procedures"}),"."]}),"\n",(0,r.jsx)(a.h2,{id:"where-to-go-from-here",children:"Where to go from here?"}),"\n",(0,r.jsxs)(a.p,{children:["You can also do the quickstart by ",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi#building-apache-hudi-from-source",children:"building hudi yourself"}),",\nand using ",(0,r.jsx)(a.code,{children:"--jars <path to hudi_code>/packaging/hudi-spark-bundle/target/hudi-spark3.2-bundle_2.1?-*.*.*-SNAPSHOT.jar"})," in the spark-shell command above\ninstead of ",(0,r.jsx)(a.code,{children:"--packages org.apache.hudi:hudi-spark3.2-bundle_2.12:0.13.0"}),". Hudi also supports scala 2.12. Refer ",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi#build-with-different-spark-versions",children:"build with scala 2.12"}),"\nfor more info."]}),"\n",(0,r.jsxs)(a.p,{children:["Also, we used Spark here to show case the capabilities of Hudi. However, Hudi can support multiple table types/query types and\nHudi tables can be queried from query engines like Hive, Spark, Presto and much more. We have put together a\n",(0,r.jsx)(a.a,{href:"https://www.youtube.com/watch?v=VhNgUsxdrD0",children:"demo video"})," that show cases all of this on a docker based setup with all\ndependent systems running locally. We recommend you replicate the same setup and run the demo yourself, by following\nsteps ",(0,r.jsx)(a.a,{href:"/docs/docker_demo",children:"here"})," to get a taste for it. Also, if you are looking for ways to migrate your existing data\nto Hudi, refer to ",(0,r.jsx)(a.a,{href:"/docs/migration_guide",children:"migration guide"}),"."]})]})}function u(e={}){const{wrapper:a}={...(0,i.R)(),...e.components};return a?(0,r.jsx)(a,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}},28453:(e,a,n)=>{n.d(a,{R:()=>s,x:()=>o});var t=n(96540);const r={},i=t.createContext(r);function s(e){const a=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function o(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(i.Provider,{value:a},e.children)}}}]);