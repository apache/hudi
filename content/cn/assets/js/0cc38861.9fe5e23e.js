"use strict";(self.webpackChunkhudi=self.webpackChunkhudi||[]).push([[5738],{3905:function(e,r,t){t.d(r,{Zo:function(){return d},kt:function(){return f}});var a=t(67294);function n(e,r,t){return r in e?Object.defineProperty(e,r,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[r]=t,e}function i(e,r){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);r&&(a=a.filter((function(r){return Object.getOwnPropertyDescriptor(e,r).enumerable}))),t.push.apply(t,a)}return t}function s(e){for(var r=1;r<arguments.length;r++){var t=null!=arguments[r]?arguments[r]:{};r%2?i(Object(t),!0).forEach((function(r){n(e,r,t[r])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(r){Object.defineProperty(e,r,Object.getOwnPropertyDescriptor(t,r))}))}return e}function o(e,r){if(null==e)return{};var t,a,n=function(e,r){if(null==e)return{};var t,a,n={},i=Object.keys(e);for(a=0;a<i.length;a++)t=i[a],r.indexOf(t)>=0||(n[t]=e[t]);return n}(e,r);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)t=i[a],r.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var l=a.createContext({}),u=function(e){var r=a.useContext(l),t=r;return e&&(t="function"==typeof e?e(r):s(s({},r),e)),t},d=function(e){var r=u(e.components);return a.createElement(l.Provider,{value:r},e.children)},c={inlineCode:"code",wrapper:function(e){var r=e.children;return a.createElement(a.Fragment,{},r)}},p=a.forwardRef((function(e,r){var t=e.components,n=e.mdxType,i=e.originalType,l=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),p=u(t),f=n,h=p["".concat(l,".").concat(f)]||p[f]||c[f]||i;return t?a.createElement(h,s(s({ref:r},d),{},{components:t})):a.createElement(h,s({ref:r},d))}));function f(e,r){var t=arguments,n=r&&r.mdxType;if("string"==typeof e||n){var i=t.length,s=new Array(i);s[0]=p;var o={};for(var l in r)hasOwnProperty.call(r,l)&&(o[l]=r[l]);o.originalType=e,o.mdxType="string"==typeof e?e:n,s[1]=o;for(var u=2;u<i;u++)s[u]=t[u];return a.createElement.apply(null,s)}return a.createElement.apply(null,t)}p.displayName="MDXCreateElement"},10038:function(e,r,t){t.r(r),t.d(r,{frontMatter:function(){return o},contentTitle:function(){return l},metadata:function(){return u},toc:function(){return d},default:function(){return p}});var a=t(87462),n=t(63366),i=(t(67294),t(3905)),s=["components"],o={title:"Release 0.5.3",sidebar_position:6,layout:"releases",toc:!0,last_modified_at:new Date("2020-05-28T15:40:00.000Z")},l="[Release 0.5.3](https://github.com/apache/hudi/releases/tag/release-0.5.3) ([docs](/docs/quick-start-guide))",u={unversionedId:"release-0.5.3",id:"release-0.5.3",isDocsHomePage:!1,title:"Release 0.5.3",description:"Migration Guide for this release",source:"@site/releases/release-0.5.3.md",sourceDirName:".",slug:"/release-0.5.3",permalink:"/cn/releases/release-0.5.3",version:"current",sidebarPosition:6,frontMatter:{title:"Release 0.5.3",sidebar_position:6,layout:"releases",toc:!0,last_modified_at:"2020-05-28T15:40:00.000Z"},sidebar:"releases",previous:{title:"Release 0.6.0",permalink:"/cn/releases/release-0.6.0"},next:{title:"Older Releases",permalink:"/cn/releases/older-releases"}},d=[{value:"Migration Guide for this release",id:"migration-guide-for-this-release",children:[]},{value:"Release Highlights",id:"release-highlights",children:[]},{value:"Raw Release Notes",id:"raw-release-notes",children:[]}],c={toc:d};function p(e){var r=e.components,t=(0,n.Z)(e,s);return(0,i.kt)("wrapper",(0,a.Z)({},c,t,{components:r,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"release-053-docs"},(0,i.kt)("a",{parentName:"h1",href:"https://github.com/apache/hudi/releases/tag/release-0.5.3"},"Release 0.5.3")," (",(0,i.kt)("a",{parentName:"h1",href:"/docs/quick-start-guide"},"docs"),")"),(0,i.kt)("h2",{id:"migration-guide-for-this-release"},"Migration Guide for this release"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"This is a bug fix only release and no special migration steps needed when upgrading from 0.5.2. If you are upgrading from earlier releases \u201cX\u201d, please make sure you read the migration guide for each subsequent release between \u201cX\u201d and 0.5.3"),(0,i.kt)("li",{parentName:"ul"},'0.5.3 is the first hudi release after graduation. As a result, all hudi jars will no longer have "-incubating" in the version name. In all the places where hudi version is referred, please make sure "-incubating" is no longer present.')),(0,i.kt)("p",null,"For example hudi-spark-bundle pom dependency would look like:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml"},"    <dependency>\n        <groupId>org.apache.hudi</groupId>\n        <artifactId>hudi-spark-bundle_2.12</artifactId>\n        <version>0.5.3</version>\n    </dependency>\n")),(0,i.kt)("h2",{id:"release-highlights"},"Release Highlights"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Hudi now supports ",(0,i.kt)("inlineCode",{parentName:"li"},"aliyun OSS")," storage service."),(0,i.kt)("li",{parentName:"ul"},"Embedded Timeline Server is enabled by default for both delta-streamer and spark datasource writes. This feature was in experimental mode before this release. Embedded Timeline Server caches file listing calls in Spark driver and serves them to Spark writer tasks. This reduces the number of file listings needed to be performed for each write."),(0,i.kt)("li",{parentName:"ul"},"Incremental Cleaning is enabled by default for both delta-streamer and spark datasource writes. This feature was also in experimental mode before this release. In the steady state, incremental cleaning avoids the costly step of scanning all partitions and instead uses Hudi metadata to find files to be cleaned up."),(0,i.kt)("li",{parentName:"ul"},"Delta-streamer config files can now be placed in different filesystem than actual data."),(0,i.kt)("li",{parentName:"ul"},"Hudi Hive Sync now supports tables partitioned by date type column."),(0,i.kt)("li",{parentName:"ul"},"Hudi Hive Sync now supports syncing directly via Hive MetaStore. You simply need to set hoodie.datasource.hive_sync.use_jdbc\n=false. Hive Metastore Uri will be read implicitly from environment. For example, when writing through Spark Data Source,    ")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-Scala"}," spark.write.format(\u201chudi\u201d)\n .option(\u2026)\n .option(\u201choodie.datasource.hive_sync.username\u201d, \u201c<user>\u201d)\n .option(\u201choodie.datasource.hive_sync.password\u201d, \u201c<password>\u201d)\n .option(\u201choodie.datasource.hive_sync.partition_fields\u201d, \u201c<partition_fields>\u201d)\n .option(\u201choodie.datasource.hive_sync.database\u201d, \u201c<db_name>\u201d)\n .option(\u201choodie.datasource.hive_sync.table\u201d, \u201c<table_name>\u201d)\n .option(\u201choodie.datasource.hive_sync.use_jdbc\u201d, \u201cfalse\u201d)\n .mode(APPEND)\n .save(\u201c/path/to/dataset\u201d)\n")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Other Writer Performance related fixes:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"DataSource Writer now avoids unnecessary loading of data after write."),(0,i.kt)("li",{parentName:"ul"},"Hudi Writer now leverages spark parallelism when searching for existing files for writing new records.")))),(0,i.kt)("h2",{id:"raw-release-notes"},"Raw Release Notes"),(0,i.kt)("p",null,"   The raw release notes are available ",(0,i.kt)("a",{parentName:"p",href:"https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12322822&version=12348256"},"here")),(0,i.kt)("p",null,"For releases older than these versions, please see ",(0,i.kt)("a",{parentName:"p",href:"/releases/older-releases"},"here"),"."))}p.isMDXComponent=!0}}]);