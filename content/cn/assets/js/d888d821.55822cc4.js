"use strict";(self.webpackChunkhudi=self.webpackChunkhudi||[]).push([[9653],{3905:function(e,a,t){t.d(a,{Zo:function(){return p},kt:function(){return h}});var i=t(67294);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function n(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);a&&(i=i.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,i)}return t}function o(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?n(Object(t),!0).forEach((function(a){r(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):n(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function s(e,a){if(null==e)return{};var t,i,r=function(e,a){if(null==e)return{};var t,i,r={},n=Object.keys(e);for(i=0;i<n.length;i++)t=n[i],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(i=0;i<n.length;i++)t=n[i],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=i.createContext({}),d=function(e){var a=i.useContext(l),t=a;return e&&(t="function"==typeof e?e(a):o(o({},a),e)),t},p=function(e){var a=d(e.components);return i.createElement(l.Provider,{value:a},e.children)},u={inlineCode:"code",wrapper:function(e){var a=e.children;return i.createElement(i.Fragment,{},a)}},c=i.forwardRef((function(e,a){var t=e.components,r=e.mdxType,n=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),c=d(t),h=r,m=c["".concat(l,".").concat(h)]||c[h]||u[h]||n;return t?i.createElement(m,o(o({ref:a},p),{},{components:t})):i.createElement(m,o({ref:a},p))}));function h(e,a){var t=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var n=t.length,o=new Array(n);o[0]=c;var s={};for(var l in a)hasOwnProperty.call(a,l)&&(s[l]=a[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var d=2;d<n;d++)o[d]=t[d];return i.createElement.apply(null,o)}return i.createElement.apply(null,t)}c.displayName="MDXCreateElement"},53268:function(e,a,t){t.r(a),t.d(a,{contentTitle:function(){return l},default:function(){return c},frontMatter:function(){return s},metadata:function(){return d},toc:function(){return p}});var i=t(87462),r=t(63366),n=(t(67294),t(3905)),o=["components"],s={title:"Release 0.9.0",sidebar_position:5,layout:"releases",toc:!0,last_modified_at:new Date("2021-08-26T15:40:00.000Z")},l="[Release 0.9.0](https://github.com/apache/hudi/releases/tag/release-0.9.0) ([docs](/docs/quick-start-guide))",d={unversionedId:"release-0.9.0",id:"release-0.9.0",title:"Release 0.9.0",description:"Migration Guide for this release",source:"@site/releases/release-0.9.0.md",sourceDirName:".",slug:"/release-0.9.0",permalink:"/cn/releases/release-0.9.0",tags:[],version:"current",sidebarPosition:5,frontMatter:{title:"Release 0.9.0",sidebar_position:5,layout:"releases",toc:!0,last_modified_at:"2021-08-26T15:40:00.000Z"},sidebar:"releases",previous:{title:"Release 0.10.0",permalink:"/cn/releases/release-0.10.0"},next:{title:"Release 0.8.0",permalink:"/cn/releases/release-0.8.0"}},p=[{value:"Migration Guide for this release",id:"migration-guide-for-this-release",children:[],level:2},{value:"Release Highlights",id:"release-highlights",children:[{value:"Spark SQL DML and DDL Support",id:"spark-sql-dml-and-ddl-support",children:[],level:3},{value:"Query side improvements",id:"query-side-improvements",children:[],level:3},{value:"Writer side improvements",id:"writer-side-improvements",children:[],level:3},{value:"Flink Integration Improvements",id:"flink-integration-improvements",children:[],level:3},{value:"DeltaStreamer",id:"deltastreamer",children:[],level:3}],level:2},{value:"Raw Release Notes",id:"raw-release-notes",children:[],level:2}],u={toc:p};function c(e){var a=e.components,t=(0,r.Z)(e,o);return(0,n.kt)("wrapper",(0,i.Z)({},u,t,{components:a,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"release-090-docs"},(0,n.kt)("a",{parentName:"h1",href:"https://github.com/apache/hudi/releases/tag/release-0.9.0"},"Release 0.9.0")," (",(0,n.kt)("a",{parentName:"h1",href:"/docs/quick-start-guide"},"docs"),")"),(0,n.kt)("h2",{id:"migration-guide-for-this-release"},"Migration Guide for this release"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"If migrating from an older release, please also check the upgrade instructions for each subsequent release below."),(0,n.kt)("li",{parentName:"ul"},"With 0.9.0, Hudi is adding more table properties to aid in using an existing hudi table with spark-sql.\nTo smoothly aid this transition these properties added to ",(0,n.kt)("inlineCode",{parentName:"li"},"hoodie.properties")," file. Whenever Hudi is launched with\nnewer table version i.e 2 (or moving from pre 0.9.0 to 0.9.0), an upgrade step will be executed automatically.\nThis automatic upgrade step will happen just once per Hudi table as the ",(0,n.kt)("inlineCode",{parentName:"li"},"hoodie.table.version")," will be updated in\nproperty file after upgrade is completed."),(0,n.kt)("li",{parentName:"ul"},"Similarly, a command line tool for Downgrading (command - ",(0,n.kt)("inlineCode",{parentName:"li"},"downgrade"),") is added if in case some users want to\ndowngrade Hudi from table version ",(0,n.kt)("inlineCode",{parentName:"li"},"2")," to ",(0,n.kt)("inlineCode",{parentName:"li"},"1")," or move from Hudi 0.9.0 to pre 0.9.0. This needs to be executed from a\n0.9.0 ",(0,n.kt)("inlineCode",{parentName:"li"},"hudi-cli")," binary/script."),(0,n.kt)("li",{parentName:"ul"},"With this release, we added a new framework to track config properties in code, moving away from string variables that\nhold property names and values. This move helps us automate configuration doc generation and much more. While we still\nsupport the older configs string variables, users are encouraged to use the new ",(0,n.kt)("inlineCode",{parentName:"li"},"ConfigProperty")," equivalents, as noted\nin the deprecation notices. In most cases, it is as simple as calling ",(0,n.kt)("inlineCode",{parentName:"li"},".key()")," and ",(0,n.kt)("inlineCode",{parentName:"li"},".defaultValue()")," on the corresponding\nalternative. e.g ",(0,n.kt)("inlineCode",{parentName:"li"},"RECORDKEY_FIELD_OPT_KEY")," can be replaced by ",(0,n.kt)("inlineCode",{parentName:"li"},"RECORDKEY_FIELD_NAME.key()"))),(0,n.kt)("h2",{id:"release-highlights"},"Release Highlights"),(0,n.kt)("h3",{id:"spark-sql-dml-and-ddl-support"},"Spark SQL DML and DDL Support"),(0,n.kt)("p",null,"0.9.0 adds ",(0,n.kt)("strong",{parentName:"p"},"experimental")," support for DDL/DMLs using Spark SQL, taking a huge step towards making Hudi more easily accessible and\noperable by all personas (non-engineers, analysts etc). Users can now use ",(0,n.kt)("inlineCode",{parentName:"p"},"CREATE TABLE....USING HUDI")," and ",(0,n.kt)("inlineCode",{parentName:"p"},"CREATE TABLE .. AS SELECT"),"\nstatements to directly create and manage tables in catalogs like Hive. Users can then use ",(0,n.kt)("inlineCode",{parentName:"p"},"INSERT"),", ",(0,n.kt)("inlineCode",{parentName:"p"},"UPDATE"),", ",(0,n.kt)("inlineCode",{parentName:"p"},"MERGE INTO")," and ",(0,n.kt)("inlineCode",{parentName:"p"},"DELETE"),"\nsql statements to manipulate data. In addition, ",(0,n.kt)("inlineCode",{parentName:"p"},"INSERT OVERWRITE")," statement can be used to overwrite existing data in the table or partition\nfor existing batch ETL pipelines. For more information, checkout our docs ",(0,n.kt)("a",{parentName:"p",href:"/docs/quick-start-guide"},"here")," clicking on ",(0,n.kt)("inlineCode",{parentName:"p"},"SparkSQL")," tab.\nPlease see ",(0,n.kt)("a",{parentName:"p",href:"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+25%3A+Spark+SQL+Extension+For+Hudi"},"RFC-25"),"\nfor more implementation details."),(0,n.kt)("h3",{id:"query-side-improvements"},"Query side improvements"),(0,n.kt)("p",null,"Hudi tables are now registered with Hive as spark datasource tables, meaning Spark SQL on these tables now uses the datasource as well,\ninstead of relying on the Hive fallbacks within Spark, which are ill-maintained/cumbersome. This unlocks many optimizations such as the\nuse of Hudi's own ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/bf5a52e51bbeaa089995335a0a4c55884792e505/hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieFileIndex.scala#L46"},"FileIndex"),"\nimplementation for optimized caching and the use of the Hudi metadata table, for faster listing of large tables. We have also added support for\n",(0,n.kt)("a",{parentName:"p",href:"/docs/quick-start-guide#time-travel-query"},"timetravel query"),", for spark datasource."),(0,n.kt)("h3",{id:"writer-side-improvements"},"Writer side improvements"),(0,n.kt)("p",null,"Virtual keys support has been added where users can avoid adding meta fields to hudi table and leverage existing fields to populate record keys and partition paths.\nOne needs to disable ",(0,n.kt)("a",{parentName:"p",href:"/docs/configurations#hoodiepopulatemetafields"},"this")," config to enable virtual keys. "),(0,n.kt)("p",null,"Bulk Insert operations using ",(0,n.kt)("a",{parentName:"p",href:"/docs/configurations#hoodiedatasourcewriterowwriterenable"},"row writer enabled")," now supports pre-combining,\nsort modes and user defined partitioners and now turned on by default for fast inserts."),(0,n.kt)("p",null,"Hudi performs automatic cleanup of uncommitted data, which has now been enhanced to be performant over cloud storage, even for\nextremely large tables. Specifically, a new marker mechanism has been implemented leveraging the timeline server to perform\ncentrally co-ordinated batched read/write of file markers to underlying storage. You can turn this using this ",(0,n.kt)("a",{parentName:"p",href:"/docs/configurations#hoodiewritemarkerstype"},"config")," and learn more\nabout it on this ",(0,n.kt)("a",{parentName:"p",href:"/blog/2021/08/18/improving-marker-mechanism"},"blog"),"."),(0,n.kt)("p",null,"Async Clustering support has been added to both DeltaStreamer and Spark Structured Streaming Sink. More on this can be found in this\n",(0,n.kt)("a",{parentName:"p",href:"/blog/2021/08/23/async-clustering"},"blog post"),". In addition, we have added a new utility class ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/bf5a52e51bbeaa089995335a0a4c55884792e505/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java"},"HoodieClusteringJob"),"\nto assist in building and executing a clustering plan together as a standalone spark job."),(0,n.kt)("p",null,"Users can choose to drop fields used to generate partition paths, using ",(0,n.kt)("inlineCode",{parentName:"p"},"hoodie.datasource.write.drop.partition.columns=true"),", to support\nquerying of Hudi snapshots using systems like BigQuery, which cannot handle this."),(0,n.kt)("p",null,"Hudi uses different ",(0,n.kt)("a",{parentName:"p",href:"/docs/configurations#hoodiecommonspillablediskmaptype"},"types of spillable maps"),", for internally handling merges (compaction, updates or even MOR snapshot queries). In 0.9.0, we have added\nsupport for ",(0,n.kt)("a",{parentName:"p",href:"/docs/configurations#hoodiecommondiskmapcompressionenabled"},"compression")," for the bitcask style default option and introduced a new spillable map backed by rocksDB, which can be more performant for large\nbulk updates or working with large base file sizes."),(0,n.kt)("p",null,"Added a new write operation ",(0,n.kt)("inlineCode",{parentName:"p"},"delete_partition")," operation, with support in spark. Users can leverage this to delete older partitions in bulk, in addition to\nrecord level deletes. Deletion of specific partitions can be done using this ",(0,n.kt)("a",{parentName:"p",href:"/docs/configurations#hoodiedatasourcewritepartitionstodelete"},"config"),".    "),(0,n.kt)("p",null,"Support for Huawei Cloud Object Storage, BAIDU AFS storage format, Baidu BOS storage in Hudi. "),(0,n.kt)("p",null,"A ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/bf5a52e51bbeaa089995335a0a4c55884792e505/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/validator/SparkPreCommitValidator.java"},"pre commit validator framework"),"\nhas been added for spark engine, which can used for DeltaStreamer and Spark Datasource writers. Users can leverage this to add any validations to be executed before committing writes to Hudi.\nThree validators come out-of-box "),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/apache/hudi/blob/bf5a52e51bbeaa089995335a0a4c55884792e505/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/validator/SqlQueryEqualityPreCommitValidator.java"},"org.apache.hudi.client.validator.SqlQueryEqualityPreCommitValidator")," can be used to validate for equality of rows before and after the commit. "),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/apache/hudi/blob/bf5a52e51bbeaa089995335a0a4c55884792e505/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/validator/SqlQueryInequalityPreCommitValidator.java"},"org.apache.hudi.client.validator.SqlQueryInequalityPreCommitValidator")," can be used to validate for inequality of rows before and after the commit. "),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/apache/hudi/blob/bf5a52e51bbeaa089995335a0a4c55884792e505/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/validator/SqlQuerySingleResultPreCommitValidator.java"},"org.apache.hudi.client.validator.SqlQuerySingleResultPreCommitValidator")," can be used to validate that a query on the table results in a specific value. ")),(0,n.kt)("p",null,"These can be configured by setting ",(0,n.kt)("inlineCode",{parentName:"p"},"hoodie.precommit.validators=<comma separated list of validator class names>"),". Users can also provide their own implementations by extending the abstract class ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/bf5a52e51bbeaa089995335a0a4c55884792e505/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/validator/SparkPreCommitValidator.java"},"SparkPreCommitValidator"),"\nand overriding this method "),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},"void validateRecordsBeforeAndAfter(Dataset<Row> before, \n                                   Dataset<Row> after, \n                                   Set<String> partitionsAffected)\n")),(0,n.kt)("h3",{id:"flink-integration-improvements"},"Flink Integration Improvements"),(0,n.kt)("p",null,"The Flink writer now supports propagation of CDC format for MOR table, by turning on the option ",(0,n.kt)("inlineCode",{parentName:"p"},"changelog.enabled=true"),". Hudi would then persist all change flags of each record,\nusing the streaming reader of Flink, user can do stateful computation based on these change logs. Note that when the table is compacted with async compaction service, all the\nintermediate changes are merged into one(last record), to only have UPSERT semantics."),(0,n.kt)("p",null,"Flink writing now also has most feature parity with spark writing, with addition of write operations like ",(0,n.kt)("inlineCode",{parentName:"p"},"bulk_insert"),", ",(0,n.kt)("inlineCode",{parentName:"p"},"insert_overwrite"),", support for non-partitioned tables,\nautomatic cleanup of uncommitted data, global indexing support, hive style partitioning and handling of partition path updates. Writing also supports a new log append mode, where\nno records are de-duplicated and base files are directly written for each flush. To use this mode, set ",(0,n.kt)("inlineCode",{parentName:"p"},"write.insert.deduplicate=false"),"."),(0,n.kt)("p",null,"Flink readers now support streaming reads from COW/MOR tables. Deletions are emitted by default in streaming read mode, the downstream receives the DELETE message as a Hoodie record with empty payload."),(0,n.kt)("p",null,"Hive sync has been greatly improved by support different Hive versions(1.x, 2.x, 3.x). Hive sync can also now be done asynchronously."),(0,n.kt)("p",null,"Flink Streamer tool now supports transformers."),(0,n.kt)("h3",{id:"deltastreamer"},"DeltaStreamer"),(0,n.kt)("p",null,"We have enhanced Deltastreamer utility with 3 new sources. "),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/bf5a52e51bbeaa089995335a0a4c55884792e505/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/JdbcSource.java"},"JDBC source")," can take a extraction SQL statement and\nincrementally fetch data out of sources supporting JDBC. This can be useful for e.g when reading data from RDBMS sources. Note that, this approach may need periodic re-bootstrapping to ensure data consistency, although being much simpler to operate over CDC based approaches."),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/bf5a52e51bbeaa089995335a0a4c55884792e505/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/SqlSource.java"},"SQLSource")," takes a Spark SQL statement to fetch data out of existing tables and\ncan be very useful for easy SQL based backfills use-cases e.g: backfilling just one column for the past N months. "),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/bf5a52e51bbeaa089995335a0a4c55884792e505/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/S3EventsHoodieIncrSource.java"},"S3EventsHoodieIncrSource")," and ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/bf5a52e51bbeaa089995335a0a4c55884792e505/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/S3EventsSource.java"},"S3EventsSource"),"\nassist in reading data from S3 reliably and efficiently ingesting that to Hudi. Existing approach using ",(0,n.kt)("inlineCode",{parentName:"p"},"*DFSSource")," source classes uses last modification time of files as checkpoint to pull in new files.\nBut, if large number of files have the same modification time, this might miss some files to be read from the source.  These two sources (S3EventsHoodieIncrSource and S3EventsSource) together ensures data\nis reliably ingested from S3 into Hudi by leveraging AWS SNS and SQS services that subscribes to file events from the source bucket. ",(0,n.kt)("a",{parentName:"p",href:"/blog/2021/08/23/s3-events-source"},"This blog post")," presents a model for\nscalable, reliable incremental ingestion by using these two sources in tandem."),(0,n.kt)("p",null,"In addition to pulling events from kafka using regular offset format, we also added support for timestamp based fetches, that can\nhelp with initial backfill/bootstrap scenarios. We have also added support for passing in basic auth credentials in schema registry provider url with schema provider."),(0,n.kt)("h2",{id:"raw-release-notes"},"Raw Release Notes"),(0,n.kt)("p",null,"The raw release notes are available ",(0,n.kt)("a",{parentName:"p",href:"https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12322822&version=12350027"},"here")))}c.isMDXComponent=!0}}]);