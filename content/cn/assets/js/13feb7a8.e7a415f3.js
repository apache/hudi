"use strict";(self.webpackChunkhudi=self.webpackChunkhudi||[]).push([[94538],{28453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>o});var t=r(96540);const s={},i=t.createContext(s);function a(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(i.Provider,{value:n},e.children)}},45345:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>t,toc:()=>h});const t=JSON.parse('{"id":"disaster_recovery","title":"Disaster Recovery","description":"Disaster Recovery is very much mission-critical for any software. Especially when it comes to data systems, the impact could be very serious","source":"@site/docs/disaster_recovery.md","sourceDirName":".","slug":"/disaster_recovery","permalink":"/cn/docs/next/disaster_recovery","draft":false,"unlisted":false,"editUrl":"https://github.com/apache/hudi/tree/asf-site/website/docs/disaster_recovery.md","tags":[],"version":"current","frontMatter":{"title":"Disaster Recovery","toc":true},"sidebar":"docs","previous":{"title":"Post-commit Callback","permalink":"/cn/docs/next/platform_services_post_commit_callback"},"next":{"title":"\u8fc1\u79fb\u6307\u5357","permalink":"/cn/docs/next/migration_guide"}}');var s=r(74848),i=r(28453);const a={title:"Disaster Recovery",toc:!0},o=void 0,l={},h=[{value:"Savepoint",id:"savepoint",level:2},{value:"Restore",id:"restore",level:2},{value:"Runbook",id:"runbook",level:2},{value:"Savepoint Example",id:"savepoint-example",level:3},{value:"Using Hudi CLI",id:"using-hudi-cli",level:4},{value:"Using Spark SQL Procedures",id:"using-spark-sql-procedures",level:4},{value:"Restore Example",id:"restore-example",level:3},{value:"Using Hudi CLI",id:"using-hudi-cli-1",level:4},{value:"Using Spark SQL Procedures",id:"using-spark-sql-procedures-1",level:4},{value:"Related Resources",id:"related-resources",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(n.p,{children:["Disaster Recovery is very much mission-critical for any software. Especially when it comes to data systems, the impact could be very serious\nleading to delay in business decisions or even wrong business decisions at times. Apache Hudi has two operations to assist you in recovering\ndata from a previous state: ",(0,s.jsx)(n.code,{children:"savepoint"})," and ",(0,s.jsx)(n.code,{children:"restore"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"savepoint",children:"Savepoint"}),"\n",(0,s.jsxs)(n.p,{children:["As the name suggest, ",(0,s.jsx)(n.code,{children:"savepoint"})," saves the table as of the commit time, so that it lets you restore the table to this\nsavepoint at a later point in time if need be. Care is taken to ensure cleaner will not clean up any files that are savepointed.\nOn similar lines, savepoint cannot be triggered on a commit that is already cleaned up. In simpler terms, this is synonymous\nto taking a backup, just that we don't make a new copy of the table, but just save the state of the table elegantly so that\nwe can restore it later when in need."]}),"\n",(0,s.jsx)(n.h2,{id:"restore",children:"Restore"}),"\n",(0,s.jsx)(n.p,{children:"This operation lets you restore your table to one of the savepoint commit. This operation cannot be undone (or reversed) and so care\nshould be taken before doing a restore. Hudi will delete all data files and commit files (timeline files) greater than the\nsavepoint commit to which the table is being restored. You should pause all writes to the table when performing\na restore since they are likely to fail while the restore is in progress. Also, reads could also fail since snapshot queries\nwill be hitting latest files which has high possibility of getting deleted with restore."}),"\n",(0,s.jsx)(n.h2,{id:"runbook",children:"Runbook"}),"\n",(0,s.jsxs)(n.p,{children:["Savepoint and restore can be triggered via ",(0,s.jsx)(n.a,{href:"/docs/cli",children:"Hudi CLI"})," and ",(0,s.jsx)(n.a,{href:"/docs/procedures",children:"SQL Procedures"}),". Let's walk through an example of how\none can take savepoint and later restore the state of the table."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Note:"})," When using the Hudi CLI, we need to specify the ",(0,s.jsx)(n.em,{children:"table path"}),", whereas when using SQL procedures, we need to provide the ",(0,s.jsx)(n.em,{children:"table name"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["Let's create a hudi table via ",(0,s.jsx)(n.code,{children:"spark-shell"})," and trigger a batch of inserts."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-scala",children:'import org.apache.hudi.QuickstartUtils._\nimport scala.collection.JavaConversions._\nimport org.apache.spark.sql.SaveMode._\nimport org.apache.hudi.DataSourceReadOptions._\nimport org.apache.hudi.DataSourceWriteOptions._\nimport org.apache.hudi.config.HoodieWriteConfig._\n\nval tableName = "hudi_trips_cow"\nval basePath = "file:///tmp/hudi_trips_cow"\nval dataGen = new DataGenerator\n\nval inserts = convertToStringList(dataGen.generateInserts(10))\nval df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\ndf.write.format("hudi").\n  options(getQuickstartWriteConfigs).\n  option("hoodie.datasource.write.precombine.field", "ts").\n  option("hoodie.datasource.write.recordkey.field", "uuid").\n  option("hoodie.datasource.write.partitionpath.field", "partitionpath").\n  option("hoodie.table.name", tableName).\n  mode(Overwrite).\n  save(basePath)\n'})}),"\n",(0,s.jsx)(n.p,{children:"Let's add four more batches of inserts."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-scala",children:'for (_ <- 1 to 4) {\n  val inserts = convertToStringList(dataGen.generateInserts(10))\n  val df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\n  df.write.format("hudi").\n    options(getQuickstartWriteConfigs).\n    option("hoodie.datasource.write.precombine.field", "ts").\n    option("hoodie.datasource.write.recordkey.field", "uuid").\n    option("hoodie.datasource.write.partitionpath.field", "partitionpath").\n    option("hoodie.table.name", tableName).\n    mode(Append).\n    save(basePath)\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:"Total record count should be 50."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-scala",children:'val tripsSnapshotDF = spark.\n  read.\n  format("hudi").\n  load(basePath)\ntripsSnapshotDF.createOrReplaceTempView("hudi_trips_snapshot")\n\nspark.sql("select count(partitionpath, uuid) from  hudi_trips_snapshot").show()\n\n+--------------------------+\n|count(partitionpath, uuid)|\n+--------------------------+\n|                        50|\n+--------------------------+\n'})}),"\n",(0,s.jsx)(n.p,{children:"Let's take a look at the timeline after 5 batches of inserts."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"ls -ltr /tmp/hudi_trips_cow/.hoodie \ntotal 128\ndrwxr-xr-x  2 nsb  wheel    64 Jan 28 16:00 archived\n-rw-r--r--  1 nsb  wheel   546 Jan 28 16:00 hoodie.properties\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:00 20220128160040171.commit.requested\n-rw-r--r--  1 nsb  wheel  2594 Jan 28 16:00 20220128160040171.inflight\n-rw-r--r--  1 nsb  wheel  4374 Jan 28 16:00 20220128160040171.commit\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:01 20220128160124637.commit.requested\n-rw-r--r--  1 nsb  wheel  2594 Jan 28 16:01 20220128160124637.inflight\n-rw-r--r--  1 nsb  wheel  4414 Jan 28 16:01 20220128160124637.commit\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:02 20220128160226172.commit.requested\n-rw-r--r--  1 nsb  wheel  2594 Jan 28 16:02 20220128160226172.inflight\n-rw-r--r--  1 nsb  wheel  4427 Jan 28 16:02 20220128160226172.commit\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:02 20220128160229636.commit.requested\n-rw-r--r--  1 nsb  wheel  2594 Jan 28 16:02 20220128160229636.inflight\n-rw-r--r--  1 nsb  wheel  4428 Jan 28 16:02 20220128160229636.commit\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:02 20220128160245447.commit.requested\n-rw-r--r--  1 nsb  wheel  2594 Jan 28 16:02 20220128160245447.inflight\n-rw-r--r--  1 nsb  wheel  4428 Jan 28 16:02 20220128160245447.commit\n"})}),"\n",(0,s.jsx)(n.h3,{id:"savepoint-example",children:"Savepoint Example"}),"\n",(0,s.jsxs)(n.p,{children:["Savepoints can be created via ",(0,s.jsx)(n.a,{href:"/docs/cli",children:"Hudi CLI"})," or ",(0,s.jsx)(n.a,{href:"/docs/procedures",children:"SQL Procedures"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"Let's trigger a savepoint as of the latest commit."}),"\n",(0,s.jsx)(n.h4,{id:"using-hudi-cli",children:"Using Hudi CLI"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Launch the Hudi CLI."}),"\n",(0,s.jsx)(n.li,{children:"Specify the SPARK_HOME if it's not specified."}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"cd hudi-cli\n./hudi-cli.sh\nset --conf SPARK_HOME=<SPARK_HOME>\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"3",children:["\n",(0,s.jsxs)(n.li,{children:["Connect to the table using the table path for example ",(0,s.jsx)(n.code,{children:"/tmp/hudi_trips_cow/"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Run the ",(0,s.jsx)(n.code,{children:"commits show"})," command to display the commits from the table."]}),"\n",(0,s.jsxs)(n.li,{children:["Run the ",(0,s.jsx)(n.code,{children:"savepoint create"})," command by specifying the ",(0,s.jsx)(n.code,{children:"commit_time"})," to create the Savepoint."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"connect --path /tmp/hudi_trips_cow/\ncommits show\nsavepoint create --commit 20220128160245447 --sparkMaster local[2]\n"})}),"\n",(0,s.jsx)(n.admonition,{title:"NOTE:",type:"note",children:(0,s.jsx)(n.p,{children:"Make sure you replace 20220128160245447 with the latest commit in your table."})}),"\n",(0,s.jsx)(n.h4,{id:"using-spark-sql-procedures",children:"Using Spark SQL Procedures"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Launch the ",(0,s.jsx)(n.code,{children:"spark-sql"})," shell by specifying Spark version and Hudi version. For example,"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"export SPARK_VERSION=3.5\nexport HUDI_VERSION=1.0.2\n\nspark-sql --packages org.apache.hudi:hudi-spark$SPARK_VERSION-bundle_2.12:$HUDI_VERSION \\\n--conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \\\n--conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' \\\n--conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog' \\\n--conf 'spark.kryo.registrator=org.apache.spark.HoodieSparkKryoRegistrar'\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsxs)(n.li,{children:["Run the ",(0,s.jsx)(n.code,{children:"show_commits"})," command to display the commits from the table."]}),"\n",(0,s.jsxs)(n.li,{children:["Run the ",(0,s.jsx)(n.code,{children:"create_savepoint"})," command by specifying the commit_time to create the Savepoint."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"call show_commits(table => 'hudi_trips_cow');\ncall create_savepoint(table => 'hudi_trips_cow', commit_time => '20220128160245447');\n"})}),"\n",(0,s.jsx)(n.admonition,{title:"NOTE:",type:"note",children:(0,s.jsx)(n.p,{children:"Make sure you replace 20220128160245447 with the latest commit in your table."})}),"\n",(0,s.jsx)(n.p,{children:"Let's check the timeline after savepoint."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"ls -ltr /tmp/hudi_trips_cow/.hoodie\ntotal 136\ndrwxr-xr-x  2 nsb  wheel    64 Jan 28 16:00 archived\n-rw-r--r--  1 nsb  wheel   546 Jan 28 16:00 hoodie.properties\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:00 20220128160040171.commit.requested\n-rw-r--r--  1 nsb  wheel  2594 Jan 28 16:00 20220128160040171.inflight\n-rw-r--r--  1 nsb  wheel  4374 Jan 28 16:00 20220128160040171.commit\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:01 20220128160124637.commit.requested\n-rw-r--r--  1 nsb  wheel  2594 Jan 28 16:01 20220128160124637.inflight\n-rw-r--r--  1 nsb  wheel  4414 Jan 28 16:01 20220128160124637.commit\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:02 20220128160226172.commit.requested\n-rw-r--r--  1 nsb  wheel  2594 Jan 28 16:02 20220128160226172.inflight\n-rw-r--r--  1 nsb  wheel  4427 Jan 28 16:02 20220128160226172.commit\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:02 20220128160229636.commit.requested\n-rw-r--r--  1 nsb  wheel  2594 Jan 28 16:02 20220128160229636.inflight\n-rw-r--r--  1 nsb  wheel  4428 Jan 28 16:02 20220128160229636.commit\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:02 20220128160245447.commit.requested\n-rw-r--r--  1 nsb  wheel  2594 Jan 28 16:02 20220128160245447.inflight\n-rw-r--r--  1 nsb  wheel  4428 Jan 28 16:02 20220128160245447.commit\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:05 20220128160245447.savepoint.inflight\n-rw-r--r--  1 nsb  wheel  1168 Jan 28 16:05 20220128160245447.savepoint\n"})}),"\n",(0,s.jsx)(n.p,{children:"You could notice that savepoint meta files are added which keeps track of the files that are part of the latest table snapshot."}),"\n",(0,s.jsx)(n.p,{children:"Now, let's continue adding three more batches of inserts."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-scala",children:'for (_ <- 1 to 3) {\n  val inserts = convertToStringList(dataGen.generateInserts(10))\n  val df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\n  df.write.format("hudi").\n    options(getQuickstartWriteConfigs).\n    option("hoodie.datasource.write.precombine.field", "ts").\n    option("hoodie.datasource.write.recordkey.field", "uuid").\n    option("hoodie.datasource.write.partitionpath.field", "partitionpath").\n    option("hoodie.table.name", tableName).\n    mode(Append).\n    save(basePath)\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:"Total record count will be 80 since we have done 8 batches in total. (5 until savepoint and 3 after savepoint)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-scala",children:'val tripsSnapshotDF = spark.\n  read.\n  format("hudi").\n  load(basePath)\ntripsSnapshotDF.createOrReplaceTempView("hudi_trips_snapshot")\n\nspark.sql("select count(partitionpath, uuid) from  hudi_trips_snapshot").show()\n+--------------------------+\n|count(partitionpath, uuid)|\n+--------------------------+\n|                        80|\n+--------------------------+\n'})}),"\n",(0,s.jsx)(n.h3,{id:"restore-example",children:"Restore Example"}),"\n",(0,s.jsxs)(n.p,{children:["Let's say something bad happened, and you want to restore your table to an older snapshot. We can perform a restore operation via ",(0,s.jsx)(n.a,{href:"/docs/cli",children:"Hudi CLI"})," or ",(0,s.jsx)(n.a,{href:"/docs/procedures",children:"SQL Procedures"}),". And do remember to bring down all of your writer processes while doing a restore."]}),"\n",(0,s.jsx)(n.p,{children:"Let's checkout timeline once, before we trigger the restore."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"ls -ltr /tmp/hudi_trips_cow/.hoodie\ntotal 208\ndrwxr-xr-x  2 nsb  wheel    64 Jan 28 16:00 archived\n-rw-r--r--  1 nsb  wheel   546 Jan 28 16:00 hoodie.properties\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:00 20220128160040171.commit.requested\n-rw-r--r--  1 nsb  wheel  2594 Jan 28 16:00 20220128160040171.inflight\n-rw-r--r--  1 nsb  wheel  4374 Jan 28 16:00 20220128160040171.commit\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:01 20220128160124637.commit.requested\n-rw-r--r--  1 nsb  wheel  2594 Jan 28 16:01 20220128160124637.inflight\n-rw-r--r--  1 nsb  wheel  4414 Jan 28 16:01 20220128160124637.commit\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:02 20220128160226172.commit.requested\n-rw-r--r--  1 nsb  wheel  2594 Jan 28 16:02 20220128160226172.inflight\n-rw-r--r--  1 nsb  wheel  4427 Jan 28 16:02 20220128160226172.commit\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:02 20220128160229636.commit.requested\n-rw-r--r--  1 nsb  wheel  2594 Jan 28 16:02 20220128160229636.inflight\n-rw-r--r--  1 nsb  wheel  4428 Jan 28 16:02 20220128160229636.commit\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:02 20220128160245447.commit.requested\n-rw-r--r--  1 nsb  wheel  2594 Jan 28 16:02 20220128160245447.inflight\n-rw-r--r--  1 nsb  wheel  4428 Jan 28 16:02 20220128160245447.commit\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:05 20220128160245447.savepoint.inflight\n-rw-r--r--  1 nsb  wheel  1168 Jan 28 16:05 20220128160245447.savepoint\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:06 20220128160620557.commit.requested\n-rw-r--r--  1 nsb  wheel  2594 Jan 28 16:06 20220128160620557.inflight\n-rw-r--r--  1 nsb  wheel  4428 Jan 28 16:06 20220128160620557.commit\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:06 20220128160627501.commit.requested\n-rw-r--r--  1 nsb  wheel  2594 Jan 28 16:06 20220128160627501.inflight\n-rw-r--r--  1 nsb  wheel  4428 Jan 28 16:06 20220128160627501.commit\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:06 20220128160630785.commit.requested\n-rw-r--r--  1 nsb  wheel  2594 Jan 28 16:06 20220128160630785.inflight\n-rw-r--r--  1 nsb  wheel  4428 Jan 28 16:06 20220128160630785.commit\n"})}),"\n",(0,s.jsx)(n.h4,{id:"using-hudi-cli-1",children:"Using Hudi CLI"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Launch the Hudi CLI or use the existing Hudi CLI."}),"\n",(0,s.jsx)(n.li,{children:"Specify the SPARK_HOME if it's not specified."}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"cd hudi-cli\n./hudi-cli.sh\nset --conf SPARK_HOME=<SPARK_HOME>\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"3",children:["\n",(0,s.jsxs)(n.li,{children:["Connect to the Hudi table using the specified table path, for example ",(0,s.jsx)(n.code,{children:"/tmp/hudi_trips_cow/"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Execute the ",(0,s.jsx)(n.code,{children:"refresh"})," command to update the table state to its latest version."]}),"\n",(0,s.jsxs)(n.li,{children:["Run the ",(0,s.jsx)(n.code,{children:"savepoints show"})," command to display the all savepoints."]}),"\n",(0,s.jsxs)(n.li,{children:["Run the ",(0,s.jsx)(n.code,{children:"savepoint rollback"})," specifying the savepoint ",(0,s.jsx)(n.strong,{children:"instant_time"})," to perform the rollback."]}),"\n",(0,s.jsxs)(n.li,{children:["(Optional) Run the ",(0,s.jsx)(n.code,{children:"savepoint delete"})," command to delete the savepoint ",(0,s.jsx)(n.strong,{children:"instant_time"})," from the existing savepoints."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"connect --path /tmp/hudi_trips_cow/\nrefresh\nsavepoints show\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 SavepointTime     \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 20220128160245447 \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\nsavepoint rollback --savepoint 20220128160245447 --sparkMaster local[2]\nsavepoint delete --commit 20220128160245447 --sparkMaster local[2]\n"})}),"\n",(0,s.jsx)(n.admonition,{title:"NOTE:",type:"note",children:(0,s.jsx)(n.p,{children:"Make sure you replace 20220128160245447 with the latest savepoint in your table."})}),"\n",(0,s.jsx)(n.h4,{id:"using-spark-sql-procedures-1",children:"Using Spark SQL Procedures"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Launch the ",(0,s.jsx)(n.code,{children:"spark-sql"})," shell by specifying Spark version and Hudi version or use the existing ",(0,s.jsx)(n.code,{children:"spark-sql"})," shell."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"export SPARK_VERSION=3.5\nexport HUDI_VERSION=1.0.2\n\nspark-sql --packages org.apache.hudi:hudi-spark$SPARK_VERSION-bundle_2.12:$HUDI_VERSION \\\n--conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \\\n--conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' \\\n--conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog' \\\n--conf 'spark.kryo.registrator=org.apache.spark.HoodieSparkKryoRegistrar'\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsxs)(n.li,{children:["Run the ",(0,s.jsx)(n.code,{children:"show_savepoints"})," command to display all the savepoints from the table."]}),"\n",(0,s.jsxs)(n.li,{children:["Run the ",(0,s.jsx)(n.code,{children:"rollback_to_savepoint"})," command by specifying the savepoint ",(0,s.jsx)(n.strong,{children:"instant_time"})," to rollback."]}),"\n",(0,s.jsxs)(n.li,{children:["(Optional) Run the ",(0,s.jsx)(n.code,{children:"delete_savepoint"})," command to delete the savepoint ",(0,s.jsx)(n.strong,{children:"instant_time"})," from the existing savepoints."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"call show_savepoints(table => 'hudi_trips_cow');\ncall rollback_to_savepoint(table => 'hudi_trips_cow', instant_time => '20220128160245447');\ncall delete_savepoint(table => 'hudi_trips_cow', instant_time => '20220128160245447');\n"})}),"\n",(0,s.jsx)(n.admonition,{title:"NOTE:",type:"note",children:(0,s.jsx)(n.p,{children:"Make sure you replace 20220128160245447 with the latest savepoint in your table."})}),"\n",(0,s.jsx)(n.p,{children:"Hudi table should have been restored to the savepointed commit 20220128160245447. Both data files and timeline files should have\nbeen deleted."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"ls -ltr /tmp/hudi_trips_cow/.hoodie\ntotal 152\ndrwxr-xr-x  2 nsb  wheel    64 Jan 28 16:00 archived\n-rw-r--r--  1 nsb  wheel   546 Jan 28 16:00 hoodie.properties\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:00 20220128160040171.commit.requested\n-rw-r--r--  1 nsb  wheel  2594 Jan 28 16:00 20220128160040171.inflight\n-rw-r--r--  1 nsb  wheel  4374 Jan 28 16:00 20220128160040171.commit\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:01 20220128160124637.commit.requested\n-rw-r--r--  1 nsb  wheel  2594 Jan 28 16:01 20220128160124637.inflight\n-rw-r--r--  1 nsb  wheel  4414 Jan 28 16:01 20220128160124637.commit\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:02 20220128160226172.commit.requested\n-rw-r--r--  1 nsb  wheel  2594 Jan 28 16:02 20220128160226172.inflight\n-rw-r--r--  1 nsb  wheel  4427 Jan 28 16:02 20220128160226172.commit\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:02 20220128160229636.commit.requested\n-rw-r--r--  1 nsb  wheel  2594 Jan 28 16:02 20220128160229636.inflight\n-rw-r--r--  1 nsb  wheel  4428 Jan 28 16:02 20220128160229636.commit\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:02 20220128160245447.commit.requested\n-rw-r--r--  1 nsb  wheel  2594 Jan 28 16:02 20220128160245447.inflight\n-rw-r--r--  1 nsb  wheel  4428 Jan 28 16:02 20220128160245447.commit\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:05 20220128160245447.savepoint.inflight\n-rw-r--r--  1 nsb  wheel  1168 Jan 28 16:05 20220128160245447.savepoint\n-rw-r--r--  1 nsb  wheel     0 Jan 28 16:07 20220128160732437.restore.inflight\n-rw-r--r--  1 nsb  wheel  4152 Jan 28 16:07 20220128160732437.restore\n"})}),"\n",(0,s.jsx)(n.p,{children:"Let's check the total record count in the table. Should match the records we had, just before we triggered the savepoint."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-scala",children:'val tripsSnapshotDF = spark.\n  read.\n  format("hudi").\n  load(basePath)\ntripsSnapshotDF.createOrReplaceTempView("hudi_trips_snapshot")\n\nspark.sql("select count(partitionpath, uuid) from  hudi_trips_snapshot").show()\n+--------------------------+\n|count(partitionpath, uuid)|\n+--------------------------+\n|                        50|\n+--------------------------+\n'})}),"\n",(0,s.jsx)(n.p,{children:"As you could see, entire table state is restored back to the commit which was savepointed. Users can choose to trigger savepoint\nat regular cadence and keep deleting older savepoints when new ones are created. Please do remember that cleaner may not clean the files\nthat are savepointed. And so users should ensure they delete the savepoints from time to time. If not, the storage reclamation may not happen."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Note:"})," Savepoint and restore for ",(0,s.jsx)(n.strong,{children:"MOR"})," table is available only from ",(0,s.jsx)(n.strong,{children:"0.11"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"related-resources",children:"Related Resources"}),"\n",(0,s.jsx)("h3",{children:"Videos"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.youtube.com/watch?v=VgIMPSK7rFAa",children:"Use Glue 4.0 to take regular save points for your Hudi tables for backup or disaster Recovery"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.youtube.com/watch?v=Vi25q4vzogs",children:"How to Rollback to Previous Checkpoint during Disaster in Apache Hudi using Glue 4.0 Demo"})}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}}}]);