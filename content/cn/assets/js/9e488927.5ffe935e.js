"use strict";(self.webpackChunkhudi=self.webpackChunkhudi||[]).push([[57379],{3905:(_,e,t)=>{t.d(e,{Zo:()=>c,kt:()=>u});var a=t(67294);function n(_,e,t){return e in _?Object.defineProperty(_,e,{value:t,enumerable:!0,configurable:!0,writable:!0}):_[e]=t,_}function i(_,e){var t=Object.keys(_);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(_);e&&(a=a.filter((function(e){return Object.getOwnPropertyDescriptor(_,e).enumerable}))),t.push.apply(t,a)}return t}function o(_){for(var e=1;e<arguments.length;e++){var t=null!=arguments[e]?arguments[e]:{};e%2?i(Object(t),!0).forEach((function(e){n(_,e,t[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(_,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(e){Object.defineProperty(_,e,Object.getOwnPropertyDescriptor(t,e))}))}return _}function r(_,e){if(null==_)return{};var t,a,n=function(_,e){if(null==_)return{};var t,a,n={},i=Object.keys(_);for(a=0;a<i.length;a++)t=i[a],e.indexOf(t)>=0||(n[t]=_[t]);return n}(_,e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(_);for(a=0;a<i.length;a++)t=i[a],e.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(_,t)&&(n[t]=_[t])}return n}var l=a.createContext({}),s=function(_){var e=a.useContext(l),t=e;return _&&(t="function"==typeof _?_(e):o(o({},e),_)),t},c=function(_){var e=s(_.components);return a.createElement(l.Provider,{value:e},_.children)},d="mdxType",p={inlineCode:"code",wrapper:function(_){var e=_.children;return a.createElement(a.Fragment,{},e)}},m=a.forwardRef((function(_,e){var t=_.components,n=_.mdxType,i=_.originalType,l=_.parentName,c=r(_,["components","mdxType","originalType","parentName"]),d=s(t),m=n,u=d["".concat(l,".").concat(m)]||d[m]||p[m]||i;return t?a.createElement(u,o(o({ref:e},c),{},{components:t})):a.createElement(u,o({ref:e},c))}));function u(_,e){var t=arguments,n=e&&e.mdxType;if("string"==typeof _||n){var i=t.length,o=new Array(i);o[0]=m;var r={};for(var l in e)hasOwnProperty.call(e,l)&&(r[l]=e[l]);r.originalType=_,r[d]="string"==typeof _?_:n,o[1]=r;for(var s=2;s<i;s++)o[s]=t[s];return a.createElement.apply(null,o)}return a.createElement.apply(null,t)}m.displayName="MDXCreateElement"},71873:(_,e,t)=>{t.r(e),t.d(e,{contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>r,toc:()=>l});var a=t(87462),n=(t(67294),t(3905));const i={title:"CLI",keywords:["hudi","cli"],last_modified_at:new Date("2021-08-18T19:59:57.000Z")},o=void 0,r={unversionedId:"cli",id:"version-0.12.2/cli",title:"CLI",description:"Local set up",source:"@site/versioned_docs/version-0.12.2/cli.md",sourceDirName:".",slug:"/cli",permalink:"/cn/docs/0.12.2/cli",editUrl:"https://github.com/apache/hudi/tree/asf-site/website/versioned_docs/version-0.12.2/cli.md",tags:[],version:"0.12.2",frontMatter:{title:"CLI",keywords:["hudi","cli"],last_modified_at:"2021-08-18T19:59:57.000Z"},sidebar:"docs",previous:{title:"Deployment",permalink:"/cn/docs/0.12.2/deployment"},next:{title:"Metrics",permalink:"/cn/docs/0.12.2/metrics"}},l=[{value:"Local set up",id:"local-set-up",children:[],level:3},{value:"Using Hudi-cli in S3",id:"using-hudi-cli-in-s3",children:[{value:"Note: These AWS jar versions below are specific to Spark 3.2.0",id:"note-these-aws-jar-versions-below-are-specific-to-spark-320",children:[],level:4}],level:3},{value:"Connect to a Kerberized cluster",id:"connect-to-a-kerberized-cluster",children:[],level:2},{value:"Using hudi-cli",id:"using-hudi-cli",children:[{value:"Inspecting Commits",id:"inspecting-commits",children:[],level:3},{value:"Drilling Down to a specific Commit",id:"drilling-down-to-a-specific-commit",children:[],level:3},{value:"FileSystem View",id:"filesystem-view",children:[],level:3},{value:"Statistics",id:"statistics",children:[],level:3},{value:"Archived Commits",id:"archived-commits",children:[],level:3},{value:"Compactions",id:"compactions",children:[],level:3},{value:"Validate Compaction",id:"validate-compaction",children:[],level:3},{value:"Unscheduling Compaction",id:"unscheduling-compaction",children:[],level:3},{value:"Repair Compaction",id:"repair-compaction",children:[],level:3},{value:"Savepoint and Restore",id:"savepoint-and-restore",children:[],level:3},{value:"Upgrade and Downgrade Table",id:"upgrade-and-downgrade-table",children:[],level:3}],level:2}],s={toc:l},c="wrapper";function d(_){let{components:e,...t}=_;return(0,n.kt)(c,(0,a.Z)({},s,t,{components:e,mdxType:"MDXLayout"}),(0,n.kt)("h3",{id:"local-set-up"},"Local set up"),(0,n.kt)("p",null,"Once hudi has been built, the shell can be fired by via  ",(0,n.kt)("inlineCode",{parentName:"p"},"cd hudi-cli && ./hudi-cli.sh"),". A hudi table resides on DFS, in a location referred to as the ",(0,n.kt)("inlineCode",{parentName:"p"},"basePath")," and\nwe would need this location in order to connect to a Hudi table. Hudi library effectively manages this table internally, using ",(0,n.kt)("inlineCode",{parentName:"p"},".hoodie")," subfolder to track all metadata."),(0,n.kt)("h3",{id:"using-hudi-cli-in-s3"},"Using Hudi-cli in S3"),(0,n.kt)("p",null,"If you are using hudi that comes packaged with AWS EMR, you can find instructions to use hudi-cli ",(0,n.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi-cli.html"},"here"),".\nIf you are not using EMR, or would like to use latest hudi-cli from master, you can follow the below steps to access S3 dataset in your local environment (laptop).  "),(0,n.kt)("p",null,"Build Hudi with corresponding Spark version, for eg, -Dspark3.1.x"),(0,n.kt)("p",null,"Set the following environment variables. "),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"export AWS_REGION=us-east-2\nexport AWS_ACCESS_KEY_ID=<key_id>\nexport AWS_SECRET_ACCESS_KEY=<secret_key>\nexport SPARK_HOME=<spark_home>\n")),(0,n.kt)("p",null,"Ensure you set the SPARK_HOME to your local spark home compatible to compiled hudi spark version above."),(0,n.kt)("p",null,"Apart from these, we might need to add aws jars to class path so that accessing S3 is feasible from local.\nWe need two jars, namely, aws-java-sdk-bundle jar and hadoop-aws jar which you can find online.\nFor eg:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.0/hadoop-aws-3.2.0.jar -o /lib/spark-3.2.0-bin-hadoop3.2/jars/hadoop-aws-3.2.0.jar\nwget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.375/aws-java-sdk-bundle-1.11.375.jar -o /lib/spark-3.2.0-bin-hadoop3.2/jars/aws-java-sdk-bundle-1.11.375.jar\n")),(0,n.kt)("h4",{id:"note-these-aws-jar-versions-below-are-specific-to-spark-320"},"Note: These AWS jar versions below are specific to Spark 3.2.0"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"export CLIENT_JAR=/lib/spark-3.2.0-bin-hadoop3.2/jars/aws-java-sdk-bundle-1.12.48.jar:/lib/spark-3.2.0-bin-hadoop3.2/jars/hadoop-aws-3.3.1.jar\n")),(0,n.kt)("p",null,"Once these are set, you are good to launch hudi-cli and access S3 dataset. "),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"./hudi-cli/hudi-cli.sh\n")),(0,n.kt)("h2",{id:"connect-to-a-kerberized-cluster"},"Connect to a Kerberized cluster"),(0,n.kt)("p",null,"Before connecting to a Kerberized cluster, you can use ",(0,n.kt)("strong",{parentName:"p"},"kerberos kinit")," command. Following is the usage of this command."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"hudi->help kerberos kinit\nNAME\n       kerberos kinit - Perform Kerberos authentication\n\nSYNOPSIS\n       kerberos kinit --krb5conf String [--principal String] [--keytab String]\n\nOPTIONS\n       --krb5conf String\n       Path to krb5.conf\n       [Optional, default = /etc/krb5.conf]\n\n       --principal String\n       Kerberos principal\n       [Mandatory]\n\n       --keytab String\n       Path to keytab\n       [Mandatory]\n")),(0,n.kt)("p",null,"For example:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"hudi->kerberos kinit --principal user/host@DOMAIN --keytab /etc/security/keytabs/user.keytab\nPerform Kerberos authentication\nParameters:\n--krb5conf: /etc/krb5.conf\n--principal: user/host@DOMAIN\n--keytab: /etc/security/keytabs/user.keytab\nKerberos current user: user/host@DOMAIN (auth:KERBEROS)\nKerberos login user: user/host@DOMAIN (auth:KERBEROS)\nKerberos authentication success\n")),(0,n.kt)("p",null,'If you see "Kerberos authentication success" in the command output, it means Kerberos authentication has been successful.'),(0,n.kt)("h2",{id:"using-hudi-cli"},"Using hudi-cli"),(0,n.kt)("p",null,"To initialize a hudi table, use the following command."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},"===================================================================\n*         ___                          ___                        *\n*        /\\__\\          ___           /\\  \\           ___         *\n*       / /  /         /\\__\\         /  \\  \\         /\\  \\        *\n*      / /__/         / /  /        / /\\ \\  \\        \\ \\  \\       *\n*     /  \\  \\ ___    / /  /        / /  \\ \\__\\       /  \\__\\      *\n*    / /\\ \\  /\\__\\  / /__/  ___   / /__/ \\ |__|     / /\\/__/      *\n*    \\/  \\ \\/ /  /  \\ \\  \\ /\\__\\  \\ \\  \\ / /  /  /\\/ /  /         *\n*         \\  /  /    \\ \\  / /  /   \\ \\  / /  /   \\  /__/          *\n*         / /  /      \\ \\/ /  /     \\ \\/ /  /     \\ \\__\\          *\n*        / /  /        \\  /  /       \\  /  /       \\/__/          *\n*        \\/__/          \\/__/         \\/__/    Apache Hudi CLI    *\n*                                                                 *\n===================================================================\n\nhudi->create --path /user/hive/warehouse/table1 --tableName hoodie_table_1 --tableType COPY_ON_WRITE\n.....\n")),(0,n.kt)("p",null,"To see the description of hudi table, use the command:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},"hudi:hoodie_table_1->desc\n18/09/06 15:57:19 INFO timeline.HoodieActiveTimeline: Loaded instants []\n    _________________________________________________________\n    | Property                | Value                        |\n    |========================================================|\n    | basePath                | ...                          |\n    | metaPath                | ...                          |\n    | fileSystem              | hdfs                         |\n    | hoodie.table.name       | hoodie_table_1               |\n    | hoodie.table.type       | COPY_ON_WRITE                |\n    | hoodie.archivelog.folder|                              |\n")),(0,n.kt)("p",null,"Following is a sample command to connect to a Hudi table contains uber trips."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},"hudi:trips->connect --path /app/uber/trips\n\n16/10/05 23:20:37 INFO model.HoodieTableMetadata: All commits :HoodieCommits{commitList=[20161002045850, 20161002052915, 20161002055918, 20161002065317, 20161002075932, 20161002082904, 20161002085949, 20161002092936, 20161002105903, 20161002112938, 20161002123005, 20161002133002, 20161002155940, 20161002165924, 20161002172907, 20161002175905, 20161002190016, 20161002192954, 20161002195925, 20161002205935, 20161002215928, 20161002222938, 20161002225915, 20161002232906, 20161003003028, 20161003005958, 20161003012936, 20161003022924, 20161003025859, 20161003032854, 20161003042930, 20161003052911, 20161003055907, 20161003062946, 20161003065927, 20161003075924, 20161003082926, 20161003085925, 20161003092909, 20161003100010, 20161003102913, 20161003105850, 20161003112910, 20161003115851, 20161003122929, 20161003132931, 20161003142952, 20161003145856, 20161003152953, 20161003155912, 20161003162922, 20161003165852, 20161003172923, 20161003175923, 20161003195931, 20161003210118, 20161003212919, 20161003215928, 20161003223000, 20161003225858, 20161004003042, 20161004011345, 20161004015235, 20161004022234, 20161004063001, 20161004072402, 20161004074436, 20161004080224, 20161004082928, 20161004085857, 20161004105922, 20161004122927, 20161004142929, 20161004163026, 20161004175925, 20161004194411, 20161004203202, 20161004211210, 20161004214115, 20161004220437, 20161004223020, 20161004225321, 20161004231431, 20161004233643, 20161005010227, 20161005015927, 20161005022911, 20161005032958, 20161005035939, 20161005052904, 20161005070028, 20161005074429, 20161005081318, 20161005083455, 20161005085921, 20161005092901, 20161005095936, 20161005120158, 20161005123418, 20161005125911, 20161005133107, 20161005155908, 20161005163517, 20161005165855, 20161005180127, 20161005184226, 20161005191051, 20161005193234, 20161005203112, 20161005205920, 20161005212949, 20161005223034, 20161005225920]}\nMetadata for table trips loaded\n")),(0,n.kt)("p",null,"Once connected to the table, a lot of other commands become available. The shell has contextual autocomplete help (press TAB) and below is a list of all commands, few of which are reviewed in this section"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"hudi:trips->help\n* ! - Allows execution of operating system (OS) commands\n* // - Inline comment markers (start of line only)\n* ; - Inline comment markers (start of line only)\n* bootstrap index showmapping - Show bootstrap index mapping\n* bootstrap index showpartitions - Show bootstrap indexed partitions\n* bootstrap run - Run a bootstrap action for current Hudi table\n* clean showpartitions - Show partition level details of a clean\n* cleans refresh - Refresh table metadata\n* cleans run - run clean\n* cleans show - Show the cleans\n* clear - Clears the console\n* cls - Clears the console\n* clustering run - Run Clustering\n* clustering schedule - Schedule Clustering\n* clustering scheduleAndExecute - Run Clustering. Make a cluster plan first and execute that plan immediately\n* commit rollback - Rollback a commit\n* commits compare - Compare commits with another Hoodie table\n* commit show_write_stats - Show write stats of a commit\n* commit showfiles - Show file level details of a commit\n* commit showpartitions - Show partition level details of a commit\n* commits refresh - Refresh table metadata\n* commits show - Show the commits\n* commits showarchived - Show the archived commits\n* commits sync - Sync commits with another Hoodie table\n* compaction repair - Renames the files to make them consistent with the timeline as dictated by Hoodie metadata. Use when compaction unschedule fails partially.\n* compaction run - Run Compaction for given instant time\n* compaction schedule - Schedule Compaction\n* compaction scheduleAndExecute - Schedule compaction plan and execute this plan\n* compaction show - Shows compaction details for a specific compaction instant\n* compaction showarchived - Shows compaction details for a specific compaction instant\n* compactions show all - Shows all compactions that are in active timeline\n* compactions showarchived - Shows compaction details for specified time window\n* compaction unschedule - Unschedule Compaction\n* compaction unscheduleFileId - UnSchedule Compaction for a fileId\n* compaction validate - Validate Compaction\n* connect - Connect to a hoodie table\n* create - Create a hoodie table if not present\n* date - Displays the local date and time\n* desc - Describe Hoodie Table properties\n* downgrade table - Downgrades a table\n* exit - Exits the shell\n* export instants - Export Instants and their metadata from the Timeline\n* fetch table schema - Fetches latest table schema\n* hdfsparquetimport - Imports Parquet table to a hoodie table\n* help - List all commands usage\n* marker delete - Delete the marker\n* metadata create - Create the Metadata Table if it does not exist\n* metadata delete - Remove the Metadata Table\n* metadata init - Update the metadata table from commits since the creation\n* metadata list-files - Print a list of all files in a partition from the metadata\n* metadata list-partitions - List all partitions from metadata\n* metadata refresh - Refresh table metadata\n* metadata set - Set options for Metadata Table\n* metadata stats - Print stats about the metadata\n* metadata validate-files - Validate all files in all partitions from the metadata\n* quit - Exits the shell\n* refresh - Refresh table metadata\n* repair addpartitionmeta - Add partition metadata to a table, if not present\n* repair corrupted clean files - repair corrupted clean files\n* repair deduplicate - De-duplicate a partition path contains duplicates & produce repaired files to replace with\n* repair migrate-partition-meta - Migrate all partition meta file currently stored in text format to be stored in base file format. See HoodieTableConfig#PARTITION_METAFILE_USE_DATA_FORMAT.\n* repair overwrite-hoodie-props - Overwrite hoodie.properties with provided file. Risky operation. Proceed with caution!\n* savepoint create - Savepoint a commit\n* savepoint delete - Delete the savepoint\n* savepoint rollback - Savepoint a commit\n* savepoints refresh - Refresh table metadata\n* savepoints show - Show the savepoints\n* script - Parses the specified resource file and executes its commands\n* set - Set spark launcher env to cli\n* show archived commits - Read commits from archived files and show details\n* show archived commit stats - Read commits from archived files and show details\n* show env - Show spark launcher env by key\n* show envs all - Show spark launcher envs\n* show fsview all - Show entire file-system view\n* show fsview latest - Show latest file-system view\n* show logfile metadata - Read commit metadata from log files\n* show logfile records - Read records from log files\n* show rollback - Show details of a rollback instant\n* show rollbacks - List all rollback instants\n* stats filesizes - File Sizes. Display summary stats on sizes of files\n* stats wa - Write Amplification. Ratio of how many records were upserted to how many records were actually written\n* sync validate - Validate the sync by counting the number of records\n* system properties - Shows the shell's properties\n* table delete-configs - Delete the supplied table configs from the table.\n* table recover-configs - Recover table configs, from update/delete that failed midway.\n* table update-configs - Update the table configs with configs with provided file.\n* temp_delete - Delete view name\n* temp_query - query against created temp view\n* temp delete - Delete view name\n* temp query - query against created temp view\n* temps_show - Show all views name\n* temps show - Show all views name\n* upgrade table - Upgrades a table\n* utils loadClass - Load a class\n* version - Displays shell version\n\nhudi:trips->\n")),(0,n.kt)("h3",{id:"inspecting-commits"},"Inspecting Commits"),(0,n.kt)("p",null,"The task of upserting or inserting a batch of incoming records is known as a ",(0,n.kt)("strong",{parentName:"p"},"commit")," in Hudi. A commit provides basic atomicity guarantees such that only committed data is available for querying.\nEach commit has a monotonically increasing string/number called the ",(0,n.kt)("strong",{parentName:"p"},"commit number"),". Typically, this is the time at which we started the commit."),(0,n.kt)("p",null,"To view some basic information about the last 10 commits,"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},'hudi:trips->commits show --sortBy "Total Bytes Written" --desc true --limit 10\n    ________________________________________________________________________________________________________________________________________________________________________\n    | CommitTime    | Total Bytes Written| Total Files Added| Total Files Updated| Total Partitions Written| Total Records Written| Total Update Records Written| Total Errors|\n    |=======================================================================================================================================================================|\n    ....\n    ....\n    ....\n')),(0,n.kt)("p",null,"At the start of each write, Hudi also writes a .inflight commit to the .hoodie folder. You can use the timestamp there to estimate how long the commit has been inflight"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},"$ hdfs dfs -ls /app/uber/trips/.hoodie/*.inflight\n-rw-r--r--   3 vinoth supergroup     321984 2016-10-05 23:18 /app/uber/trips/.hoodie/20161005225920.inflight\n")),(0,n.kt)("h3",{id:"drilling-down-to-a-specific-commit"},"Drilling Down to a specific Commit"),(0,n.kt)("p",null,"To understand how the writes spread across specific partiions,"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},'hudi:trips->commit showpartitions --commit 20161005165855 --sortBy "Total Bytes Written" --desc true --limit 10\n    __________________________________________________________________________________________________________________________________________\n    | Partition Path| Total Files Added| Total Files Updated| Total Records Inserted| Total Records Updated| Total Bytes Written| Total Errors|\n    |=========================================================================================================================================|\n     ....\n     ....\n')),(0,n.kt)("p",null,"If you need file level granularity , we can do the following"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},'hudi:trips->commit showfiles --commit 20161005165855 --sortBy "Partition Path"\n    ________________________________________________________________________________________________________________________________________________________\n    | Partition Path| File ID                             | Previous Commit| Total Records Updated| Total Records Written| Total Bytes Written| Total Errors|\n    |=======================================================================================================================================================|\n    ....\n    ....\n')),(0,n.kt)("h3",{id:"filesystem-view"},"FileSystem View"),(0,n.kt)("p",null,"Hudi views each partition as a collection of file-groups with each file-group containing a list of file-slices in commit order (See concepts).\nThe below commands allow users to view the file-slices for a data-set."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},'hudi:stock_ticks_mor->show fsview all\n ....\n  _______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n | Partition | FileId | Base-Instant | Data-File | Data-File Size| Num Delta Files| Total Delta File Size| Delta Files |\n |==============================================================================================================================================================================================================================================================================================================================================================================================================|\n | 2018/08/31| 111415c3-f26d-4639-86c8-f9956f245ac3| 20181002180759| hdfs://namenode:8020/user/hive/warehouse/stock_ticks_mor/2018/08/31/111415c3-f26d-4639-86c8-f9956f245ac3_0_20181002180759.parquet| 432.5 KB | 1 | 20.8 KB | [HoodieLogFile {hdfs://namenode:8020/user/hive/warehouse/stock_ticks_mor/2018/08/31/.111415c3-f26d-4639-86c8-f9956f245ac3_20181002180759.log.1}]|\n\n\n\nhudi:stock_ticks_mor->show fsview latest --partitionPath "2018/08/31"\n ......\n __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n | Partition | FileId | Base-Instant | Data-File | Data-File Size| Num Delta Files| Total Delta Size| Delta Size - compaction scheduled| Delta Size - compaction unscheduled| Delta To Base Ratio - compaction scheduled| Delta To Base Ratio - compaction unscheduled| Delta Files - compaction scheduled | Delta Files - compaction unscheduled|\n |=================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================|\n | 2018/08/31| 111415c3-f26d-4639-86c8-f9956f245ac3| 20181002180759| hdfs://namenode:8020/user/hive/warehouse/stock_ticks_mor/2018/08/31/111415c3-f26d-4639-86c8-f9956f245ac3_0_20181002180759.parquet| 432.5 KB | 1 | 20.8 KB | 20.8 KB | 0.0 B | 0.0 B | 0.0 B | [HoodieLogFile {hdfs://namenode:8020/user/hive/warehouse/stock_ticks_mor/2018/08/31/.111415c3-f26d-4639-86c8-f9956f245ac3_20181002180759.log.1}]| [] |\n\n')),(0,n.kt)("h3",{id:"statistics"},"Statistics"),(0,n.kt)("p",null,"Since Hudi directly manages file sizes for DFS table, it might be good to get an overall picture"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},'hudi:trips->stats filesizes --partitionPath 2016/09/01 --sortBy "95th" --desc true --limit 10\n    ________________________________________________________________________________________________\n    | CommitTime    | Min     | 10th    | 50th    | avg     | 95th    | Max     | NumFiles| StdDev  |\n    |===============================================================================================|\n    | <COMMIT_ID>   | 93.9 MB | 93.9 MB | 93.9 MB | 93.9 MB | 93.9 MB | 93.9 MB | 2       | 2.3 KB  |\n    ....\n    ....\n')),(0,n.kt)("p",null,"In case of Hudi write taking much longer, it might be good to see the write amplification for any sudden increases"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},"hudi:trips->stats wa\n    __________________________________________________________________________\n    | CommitTime    | Total Upserted| Total Written| Write Amplifiation Factor|\n    |=========================================================================|\n    ....\n    ....\n")),(0,n.kt)("h3",{id:"archived-commits"},"Archived Commits"),(0,n.kt)("p",null,"In order to limit the amount of growth of .commit files on DFS, Hudi archives older .commit files (with due respect to the cleaner policy) into a commits.archived file.\nThis is a sequence file that contains a mapping from commitNumber => json with raw information about the commit (same that is nicely rolled up above)."),(0,n.kt)("h3",{id:"compactions"},"Compactions"),(0,n.kt)("p",null,"To get an idea of the lag between compaction and writer applications, use the below command to list down all\npending compactions."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},"hudi:trips->compactions show all\n     ___________________________________________________________________\n    | Compaction Instant Time| State    | Total FileIds to be Compacted|\n    |==================================================================|\n    | <INSTANT_1>            | REQUESTED| 35                           |\n    | <INSTANT_2>            | INFLIGHT | 27                           |\n")),(0,n.kt)("p",null,"To inspect a specific compaction plan, use"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},"hudi:trips->compaction show --instant <INSTANT_1>\n    _________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n    | Partition Path| File Id | Base Instant  | Data File Path                                    | Total Delta Files| getMetrics                                                                                                                    |\n    |================================================================================================================================================================================================================================================\n    | 2018/07/17    | <UUID>  | <INSTANT_1>   | viewfs://ns-default/.../../UUID_<INSTANT>.parquet | 1                | {TOTAL_LOG_FILES=1.0, TOTAL_IO_READ_MB=1230.0, TOTAL_LOG_FILES_SIZE=2.51255751E8, TOTAL_IO_WRITE_MB=991.0, TOTAL_IO_MB=2221.0}|\n\n")),(0,n.kt)("p",null,"To manually schedule or run a compaction, use the below command. This command uses spark launcher to perform compaction\noperations."),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},"NOTE:")," Make sure no other application is scheduling compaction for this table concurrently\n{: .notice--info}"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},"hudi:trips->help compaction schedule\nKeyword:                   compaction schedule\nDescription:               Schedule Compaction\n Keyword:                  sparkMemory\n   Help:                   Spark executor memory\n   Mandatory:              false\n   Default if specified:   '__NULL__'\n   Default if unspecified: '1G'\n\n* compaction schedule - Schedule Compaction\n")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},"hudi:trips->help compaction run\nKeyword:                   compaction run\nDescription:               Run Compaction for given instant time\n Keyword:                  tableName\n   Help:                   Table name\n   Mandatory:              true\n   Default if specified:   '__NULL__'\n   Default if unspecified: '__NULL__'\n\n Keyword:                  parallelism\n   Help:                   Parallelism for hoodie compaction\n   Mandatory:              true\n   Default if specified:   '__NULL__'\n   Default if unspecified: '__NULL__'\n\n Keyword:                  schemaFilePath\n   Help:                   Path for Avro schema file\n   Mandatory:              true\n   Default if specified:   '__NULL__'\n   Default if unspecified: '__NULL__'\n\n Keyword:                  sparkMemory\n   Help:                   Spark executor memory\n   Mandatory:              true\n   Default if specified:   '__NULL__'\n   Default if unspecified: '__NULL__'\n\n Keyword:                  retry\n   Help:                   Number of retries\n   Mandatory:              true\n   Default if specified:   '__NULL__'\n   Default if unspecified: '__NULL__'\n\n Keyword:                  compactionInstant\n   Help:                   Base path for the target hoodie table\n   Mandatory:              true\n   Default if specified:   '__NULL__'\n   Default if unspecified: '__NULL__'\n\n* compaction run - Run Compaction for given instant time\n")),(0,n.kt)("h3",{id:"validate-compaction"},"Validate Compaction"),(0,n.kt)("p",null,"Validating a compaction plan : Check if all the files necessary for compactions are present and are valid"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},"hudi:stock_ticks_mor->compaction validate --instant 20181005222611\n...\n\n   COMPACTION PLAN VALID\n\n    ___________________________________________________________________________________________________________________________________________________________________________________________________________________________\n    | File Id                             | Base Instant Time| Base Data File                                                                                                                   | Num Delta Files| Valid| Error|\n    |==========================================================================================================================================================================================================================|\n    | 05320e98-9a57-4c38-b809-a6beaaeb36bd| 20181005222445   | hdfs://namenode:8020/user/hive/warehouse/stock_ticks_mor/2018/08/31/05320e98-9a57-4c38-b809-a6beaaeb36bd_0_20181005222445.parquet| 1              | true |      |\n\n\n\nhudi:stock_ticks_mor->compaction validate --instant 20181005222601\n\n   COMPACTION PLAN INVALID\n\n    _______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n    | File Id                             | Base Instant Time| Base Data File                                                                                                                   | Num Delta Files| Valid| Error                                                                           |\n    |=====================================================================================================================================================================================================================================================================================================|\n    | 05320e98-9a57-4c38-b809-a6beaaeb36bd| 20181005222445   | hdfs://namenode:8020/user/hive/warehouse/stock_ticks_mor/2018/08/31/05320e98-9a57-4c38-b809-a6beaaeb36bd_0_20181005222445.parquet| 1              | false| All log files specified in compaction operation is not present. Missing ....    |\n")),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},"NOTE:")," The following commands must be executed without any other writer/ingestion application running.\n{: .notice--warning}"),(0,n.kt)("p",null,"Sometimes, it becomes necessary to remove a fileId from a compaction-plan inorder to speed-up or unblock compaction\noperation. Any new log-files that happened on this file after the compaction got scheduled will be safely renamed\nso that are preserved. Hudi provides the following CLI to support it"),(0,n.kt)("h3",{id:"unscheduling-compaction"},"Unscheduling Compaction"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},"hudi:trips->compaction unscheduleFileId --fileId <FileUUID>\n....\nNo File renames needed to unschedule file from pending compaction. Operation successful.\n")),(0,n.kt)("p",null,"In other cases, an entire compaction plan needs to be reverted. This is supported by the following CLI"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},"hudi:trips->compaction unschedule --instant <compactionInstant>\n.....\nNo File renames needed to unschedule pending compaction. Operation successful.\n")),(0,n.kt)("h3",{id:"repair-compaction"},"Repair Compaction"),(0,n.kt)("p",null,"The above compaction unscheduling operations could sometimes fail partially (e:g -> DFS temporarily unavailable). With\npartial failures, the compaction operation could become inconsistent with the state of file-slices. When you run\n",(0,n.kt)("inlineCode",{parentName:"p"},"compaction validate"),", you can notice invalid compaction operations if there is one.  In these cases, the repair\ncommand comes to the rescue, it will rearrange the file-slices so that there is no loss and the file-slices are\nconsistent with the compaction plan"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},"hudi:stock_ticks_mor->compaction repair --instant 20181005222611\n......\nCompaction successfully repaired\n.....\n")),(0,n.kt)("h3",{id:"savepoint-and-restore"},"Savepoint and Restore"),(0,n.kt)("p",null,'As the name suggest, "savepoint" saves the table as of the commit time, so that it lets you restore the table to this\nsavepoint at a later point in time if need be. You can read more about savepoints and restore ',(0,n.kt)("a",{parentName:"p",href:"/docs/disaster_recovery"},"here")),(0,n.kt)("p",null,"To trigger savepoint for a hudi table"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},"connect --path /tmp/hudi_trips_cow/\ncommits show\nset --conf SPARK_HOME=<SPARK_HOME>\nsavepoint create --commit 20220128160245447 --sparkMaster local[2]\n")),(0,n.kt)("p",null,"To restore the table to one of the savepointed commit:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},"connect --path /tmp/hudi_trips_cow/\ncommits show\nset --conf SPARK_HOME=<SPARK_HOME>\nsavepoints show\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 SavepointTime     \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 20220128160245447 \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\nsavepoint rollback --savepoint 20220128160245447 --sparkMaster local[2]\n")),(0,n.kt)("h3",{id:"upgrade-and-downgrade-table"},"Upgrade and Downgrade Table"),(0,n.kt)("p",null,"In case the user needs to downgrade the version of Hudi library used, the Hudi table needs to be manually downgraded\non the newer version of Hudi CLI before library downgrade.  To downgrade a Hudi table through CLI, user needs to specify\nthe target Hudi table version as follows:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"connect --path <table_path>\ndowngrade table --toVersion <target_version>\n")),(0,n.kt)("p",null,"The following table shows the Hudi table versions corresponding to the Hudi release versions:"),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",{parentName:"tr",align:"left"},"Hudi Table Version"),(0,n.kt)("th",{parentName:"tr",align:"left"},"Hudi Release Version(s)"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:"left"},(0,n.kt)("inlineCode",{parentName:"td"},"FIVE")," or ",(0,n.kt)("inlineCode",{parentName:"td"},"5")),(0,n.kt)("td",{parentName:"tr",align:"left"},"0.12.x")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:"left"},(0,n.kt)("inlineCode",{parentName:"td"},"FOUR")," or ",(0,n.kt)("inlineCode",{parentName:"td"},"4")),(0,n.kt)("td",{parentName:"tr",align:"left"},"0.11.x")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:"left"},(0,n.kt)("inlineCode",{parentName:"td"},"THREE")," or ",(0,n.kt)("inlineCode",{parentName:"td"},"3")),(0,n.kt)("td",{parentName:"tr",align:"left"},"0.10.x")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:"left"},(0,n.kt)("inlineCode",{parentName:"td"},"TWO")," or ",(0,n.kt)("inlineCode",{parentName:"td"},"2")),(0,n.kt)("td",{parentName:"tr",align:"left"},"0.9.x")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:"left"},(0,n.kt)("inlineCode",{parentName:"td"},"ONE")," or ",(0,n.kt)("inlineCode",{parentName:"td"},"1")),(0,n.kt)("td",{parentName:"tr",align:"left"},"0.6.x - 0.8.x")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:"left"},(0,n.kt)("inlineCode",{parentName:"td"},"ZERO")," or ",(0,n.kt)("inlineCode",{parentName:"td"},"0")),(0,n.kt)("td",{parentName:"tr",align:"left"},"0.5.x and below")))),(0,n.kt)("p",null,"For example, to downgrade a table from version ",(0,n.kt)("inlineCode",{parentName:"p"},"FIVE"),"(",(0,n.kt)("inlineCode",{parentName:"p"},"5"),") (current version) to ",(0,n.kt)("inlineCode",{parentName:"p"},"TWO"),"(",(0,n.kt)("inlineCode",{parentName:"p"},"2"),"), you should run (use proper Spark master based\non your environment)"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"downgrade table --toVersion TWO --sparkMaster local[2]\n")),(0,n.kt)("p",null,"or"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"downgrade table --toVersion 2 --sparkMaster local[2]\n")),(0,n.kt)("p",null,"You can verify the table version by looking at the ",(0,n.kt)("inlineCode",{parentName:"p"},"hoodie.table.version")," property in ",(0,n.kt)("inlineCode",{parentName:"p"},".hoodie/hoodie.properties")," under\nthe table path:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-properties"},"hoodie.table.version=2\n")),(0,n.kt)("p",null,"Hudi CLI also provides the ability to manually upgrade a Hudi table.  To upgrade a Hudi table through CLI:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"upgrade table --toVersion <target_version>\n")),(0,n.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,n.kt)("div",{parentName:"div",className:"admonition-heading"},(0,n.kt)("h5",{parentName:"div"},(0,n.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,n.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,n.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,n.kt)("div",{parentName:"div",className:"admonition-content"},(0,n.kt)("p",{parentName:"div"},"Table upgrade is automatically handled by the Hudi write client in different deployment modes such as DeltaStreamer\nafter upgrading the Hudi library so that the user does not have to do manual upgrade.  Such automatic table upgrade\nis the ",(0,n.kt)("strong",{parentName:"p"},"recommended")," way in general, instead of using ",(0,n.kt)("inlineCode",{parentName:"p"},"upgrade")," CLI command."),(0,n.kt)("p",{parentName:"div"},'Table upgrade from table version ONE to TWO requires key generator related configs such as\n"hoodie.datasource.write.recordkey.field", which is only available when user configures the write job. So the table\nupgrade from version ONE to TWO through CLI is not supported, and user should rely on the automatic upgrade in the write\nclient instead.'))),(0,n.kt)("p",null,"You may also run the upgrade command without specifying the target version.  In such a case, the latest table version\ncorresponding to the library release version is used:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-shell"},"upgrade table\n")))}d.isMDXComponent=!0}}]);