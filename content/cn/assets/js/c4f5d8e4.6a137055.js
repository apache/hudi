(self.webpackChunkhudi=self.webpackChunkhudi||[]).push([[52634],{43152:(e,a,t)=>{var i={"./2016-08-04-The-Case-for-incremental-processing-on-Hadoop.mdx":5070,"./2016-12-30-strata-talk-2017.md":57192,"./2017-03-12-Hoodie-Uber-Engineerings-Incremental-Processing-Framework-on-Hadoop.mdx":41678,"./2019-01-18-asf-incubation.md":20543,"./2019-03-07-batch-vs-incremental.md":95101,"./2019-05-14-registering-dataset-to-hive.md":18410,"./2019-09-09-ingesting-database-changes.md":75878,"./2019-10-22-Hudi-On-Hops.mdx":78818,"./2019-11-15-New-Insert-Update-Delete-Data-on-S3-with-Amazon-EMR-and-Apache-Hudi.mdx":17607,"./2020-01-15-delete-support-in-hudi.md":3877,"./2020-01-20-change-capture-using-aws.md":70085,"./2020-03-22-exporting-hudi-datasets.md":99046,"./2020-04-27-apache-hudi-apache-zepplin.md":81900,"./2020-05-28-monitoring-hudi-metrics-with-datadog.md":52516,"./2020-06-04-The-Apache-Software-Foundation-Announces-Apache-Hudi-as-a-Top-Level-Project.mdx":5711,"./2020-06-09-Building-a-Large-scale-Transactional-Data-Lake-at-Uber-Using-Apache-Hudi.mdx":31205,"./2020-06-16-Apache-Hudi-grows-cloud-data-lake-maturity.mdx":80674,"./2020-08-04-PrestoDB-and-Apache-Hudi.mdx":40965,"./2020-08-18-hudi-incremental-processing-on-data-lakes.md":1718,"./2020-08-20-efficient-migration-of-large-parquet-tables.md":57373,"./2020-08-21-async-compaction-deployment-model.md":85075,"./2020-08-22-ingest-multiple-tables-using-hudi.md":17857,"./2020-10-06-cdc-solution-using-hudi-by-nclouds.md":62898,"./2020-10-15-apache-hudi-meets-apache-flink.md":67035,"./2020-10-19-Origins-of-Data-Lake-at-Grofers.mdx":29692,"./2020-10-19-hudi-meets-aws-emr-and-aws-dms.md":1356,"./2020-10-21-Architecting-Data-Lakes-for-the-Modern-Enterprise-at-Data-Summit-Connect-Fall-2020.mdx":95978,"./2020-10-21-Data-Lake-Change-Capture-using-Apache-Hudi-and-Amazon-AMS-EMR.mdx":94747,"./2020-11-11-hudi-indexing-mechanisms.md":66030,"./2020-11-29-Can-Big-Data-Solutions-Be-Affordable.mdx":16935,"./2020-12-01-high-perf-data-lake-with-hudi-and-alluxio-t3go.md":33616,"./2021-01-27-hudi-clustering-intro.md":91107,"./2021-02-13-hudi-key-generators.md":18058,"./2021-02-24-Time-travel-operations-in-Hopsworks-Feature-Store.mdx":42340,"./2021-03-01-Data-Lakehouse-Building-the-Next-Generation-of-Data-Lakes-using-Apache-Hudi.mdx":50836,"./2021-03-01-hudi-file-sizing.md":77713,"./2021-03-04-Build-a-data-lake-using-amazon-kinesis-data-stream-for-amazon-dynamodb-and-apache-hudi.mdx":76030,"./2021-03-11-New-features-from-Apache-hudi-in-Amazon-EMR.mdx":66447,"./2021-04-12-Build-Slowly-Changing-Dimensions-Type-2-SCD2-with-Apache-Spark-and-Apache-Hudi-on-Amazon-EMR.mdx":17921,"./2021-05-12-Experts-primer-on-Apache-Hudi.mdx":7074,"./2021-06-04-Apache-Hudi-How-Uber-gets-data-a-ride-to-its-destination.mdx":71488,"./2021-06-10-employing-right-configurations-for-hudi-cleaner.md":19135,"./2021-07-16-Amazon-Athena-expands-Apache-Hudi-support.mdx":24639,"./2021-07-16-Query-apache-hudi-dataset-in-an-amazon-S3-data-lake-with-amazon-athena-Read-optimized-queries.mdx":9001,"./2021-07-21-streaming-data-lake-platform.md":30562,"./2021-07-26-Baixin-banksreal-time-data-lake-evolution-scheme-based-on-Apache-Hudi.mdx":43988,"./2021-08-03-MLOps-Wars-Versioned-Feature-Data-with-a-Lakehouse.mdx":12550,"./2021-08-11-Cost-Efficient-Open-Source-Big-Data-Platform-at-Uber.mdx":39998,"./2021-08-16-kafka-custom-deserializer.md":91254,"./2021-08-18-improving-marker-mechanism.md":56944,"./2021-08-18-virtual-keys.md":44870,"./2021-08-23-async-clustering.md":54355,"./2021-08-23-s3-events-source.md":9160,"./2021-09-01-building-eb-level-data-lake-using-hudi-at-bytedance.md":46013,"./2021-10-05-Data-Platform-2.0-Part-I.mdx":35612,"./2021-10-14-How-Amazon-Transportation-Service-enabled-near-real-time-event-analytics-at-petabyte-scale-using-AWS-Glue-with-Apache-Hudi.mdx":84129,"./2021-10-21-Practice-of-Apache-Hudi-in-building-real-time-data-lake-at-station-B.mdx":90023,"./2021-11-16-How-GE-Aviation-built-cloud-native-data-pipelines-at-enterprise-scale-using-the-AWS-platform.mdx":18577,"./2021-11-22-Apache-Hudi-Architecture-Tools-and-Best-Practices.mdx":28696,"./2021-12-16-lakehouse-concurrency-control-are-we-too-optimistic.md":42243,"./2021-12-20-New-features-from-Apache-Hudi-0.7.0-and-0.8.0-available-on-Amazon-EMR.mdx":36710,"./2021-12-29-hudi-zorder-and-hilbert-space-filling-curves.md":34890,"./2021-12-31-The-Art-of-Building-Open-Data-Lakes-with-Apache-Hudi-Kafka-Hive-and-Debezium.mdx":11470,"./2022-01-06-apache-hudi-2021-a-year-in-review.md":21015,"./2022-01-14-change-data-capture-with-debezium-and-apache-hudi.md":30661,"./2022-01-18-Why-and-How-I-Integrated-Airbyte-and-Apache-Hudi.mdx":18920,"./2022-01-20-Hudi-powering-data-lake-efforts-at-Walmart-and-Disney-Hotstar.mdx":10082,"./2022-01-25-Cost-Efficiency-Scale-in-Big-Data-File-Format.mdx":22951,"./2022-02-02-Onehouse-Commitment-to-Openness.mdx":80318,"./2022-02-03-Onehouse-brings-a-fully-managed-lakehouse-to-Apache-Hudi.mdx":41836,"./2022-02-09-ACID-transformations-on-Distributed-file-system.mdx":79014,"./2022-02-12-Open-Source-Data-Lake-Table-Formats-Evaluating-Current-Interest-and-Rate-of-Adoption.mdx":89906,"./2022-02-17-Fresher-Data-Lake-on-AWS-S3.mdx":8155,"./2022-02-20-Understanding-its-core-concepts-from-hudi-persistence-files.mdx":89449,"./2022-03-01-Create-a-low-latency-source-to-data-lake-pipeline-using-Amazon-MSK-Connect-Apache-Flink-and-Apache-Hudi.mdx":6511,"./2022-03-09-Build-a-serverless-pipeline-to-analyze-streaming-data-using-AWS-Glue-Apache-Hudi-and-Amazon-S3.mdx":28718,"./2022-03-24-Zendesk-Insights-for-CTOs-Part-3-Growing-your-business-with-modern-data-capabilities.mdx":98357,"./2022-04-04-Key-Learnings-on-Using-Apache-HUDI-in-building-Lakehouse-Architecture-at-Halodoc.mdx":48644,"./2022-04-04-New-features-from-Apache-Hudi-0.9.0-on-Amazon-EMR.mdx":25075,"./2022-04-19-Corrections-in-data-lakehouse-table-format-comparisons.mdx":21592,"./2022-05-17-Introducing-Multi-Modal-Index-for-the-Lakehouse-in-Apache-Hudi.mdx":93386,"./2022-05-25-Record-by-record-deletable-data-lake-using-Apache-Hudi.mdx":37092,"./2022-06-04-Asynchronous-Indexing-Using-Hudi.mdx":86229,"./2022-06-09-Singificant-queries-speedup-from-Hudi-Column-Stats-Index-and-Data-Skipping-features.mdx":22229,"./2022-06-29-Apache-Hudi-vs-Delta-Lake-transparent-tpc-ds-lakehouse-performance-benchmarks.mdx":61574,"./2022-07-11-build-open-lakehouse-using-apache-hudi-and-dbt.md":96814,"./2022-08-09-How-NerdWallet-uses-AWS-and-Apache-Hudi-to-build-a-serverless-real-time-analytics-platform.mdx":12029,"./2022-08-12-Use-Flink-Hudi-to-Build-a-Streaming-Data-Lake-Platform.mdx":1135,"./2022-08-24-Implementation-of-SCD-2-with-Apache-Hudi-and-Spark.mdx":8215,"./2022-08-25-Data-Lake-Lakehouse-Guide-Powered-by-Data-Lake-Table-Formats-Delta-Lake-Iceberg-Hudi.mdx":83264,"./2022-09-20-Building-Streaming-Data-Lakes-with-Hudi-and-MinIO.mdx":63441,"./2022-09-28-Data-processing-with-Spark-time-traveling.mdx":90465,"./2022-10-06-Ingest-streaming-data-to-Apache-Hudi-using-AWS-Glue-and-DeltaStreamer.mdx":22896,"./2022-10-08-what-why-and-how-apache-hudis-bloom-index.mdx":74731,"./2022-10-17-Get-started-with-Apache-Hudi-using-AWS.mdx":12161,"./2022-11-10-How-Hudl-built-a-cost-optimized-AWS-Glue-pipeline-with-Apache-Hudi-datasets.mdx":54424,"./2022-11-22-Build-your-Apache-Hudi-data-lake-on-AWS-using-Amazon-EMR-Part-1.mdx":77010,"./2022-12-01-Run-apache-hudi-at-scale-on-aws.mdx":80012,"./2022-12-19-Build-Your-First-Hudi-Lakehouse-with-AWS-Glue-and-AWS-S3.md":62099,"./2022-12-29-Apache-Hudi-2022-A-Year-In-Review.md":68611,"./2023-01-11-Apache-Hudi-vs-Delta-Lake-vs-Apache-Iceberg-Lakehouse-Feature-Comparison.mdx":67072,"./2023-01-27-Introducing-native-support-for-Apache-Hudi-Delta-Lake-Apache-Iceberg-on-AWS-Glue-for-Apache-Spark.mdx":11472,"./2023-02-07-automate-schema-evolution-at-scale-with-apache-hudi-in-aws-glue.mdx":44662,"./2023-02-12-table-service-deployment-models-in-apache-hudi.mdx":65321,"./2023-02-19-bulk-insert-sort-modes-with-apache-hudi.mdx":14926,"./2023-02-22-Getting-Started-Manage-your-Hudi-tables-with-the-admin-Hudi-CLI-tool.mdx":90932,"./2023-03-16-Setting-Uber-Transactional-Data-Lake-in-Motion-with-Incremental-ETL-Using-Apache-Hudi.mdx":97143,"./2023-03-17-introduction-to-apache-hudi.mdx":16488,"./2023-03-20-Introducing-native-support-for-Apache Hudi-Delta-Lake-and-Apache-Iceberg-on-AWS-Glue-for-Apache-Spark-Part-2-AWS-Glue-Studio-Visual-Editor.mdx":77947,"./2023-03-23-Spark-ETL-Chapter-8-with-Lakehouse-Apache-HUDI.mdx":9528,"./2023-04-02-global-vs-non-global-index-in-apache-hudi.mdx":12741,"./2023-04-07-Speed-up-your-write-latencies-using-Bucket-Index-in-Apache-Hudi.mdx":22239,"./2023-04-18-getting-started-incrementally-process-data-with-apache-hudi.mdx":57100,"./2023-04-26-the-lakehouse-trifecta.mdx":36547,"./2023-04-29-can-you-concurrently-write-data-to-apache-hudi-w-o-any-lock-provider.mdx":76121,"./2023-05-02-intro-to-hudi-and-flink.mdx":9374,"./2023-05-03-lakehouse-at-fortune-1-scale.mdx":10283,"./2023-05-09-amazon-athena-apache-hudi.mdx":64193,"./2023-05-10-top-3-things-you-can-do-to-get-fast-upsert-performance-in-apache-hudi.mdx":19688,"./2023-05-12-ingesting-data-to-apache-hudi-using-spark-sql.mdx":31552,"./2023-05-16-how-zoom-implemented-streaming-log-ingestion-and-efficient-gdpr-deletes-using-apache-hudi-on-amazon-emr.mdx":27151,"./2023-05-19-hudi-metafields-demystified.mdx":36891,"./2023-05-29-different-query-types-with-apache-hudi.mdx":90362,"./2023-06-03-text-based-search-from-elastic-search-to-vector-search.mdx":49999,"./2023-06-11-cleaner-and-archival-in-apache-hudi.mdx":77448,"./2023-06-16-Exploring-New-Frontiers-How-Apache-Flink-Apache-Hudi-and-Presto-Power-New-Insights-at-Scale.mdx":63939,"./2023-06-20-How-to-query-data-in-Apache-Hudi-using-StarRocks.mdx":36969,"./2023-06-20-timeline-server-in-apache-hudi.mdx":84730,"./2023-06-24-multi-writer-support-in-apache-hudi.mdx":55702,"./2023-06-26-Unlimited-Big-Data-Exchange-A-Wonderful-Review-of-Apache-DolphinScheduler-and-Hudi-Hangzhou-Meetup.mdx":26910,"./2023-06-30-What-about-Apache-Hudi-Apache-Iceberg-and-Delta-Lake.mdx":1310,"./2023-07-01-monitoring-table-size-stats.mdx":47364,"./2023-07-02-Hudi-Best-Practices-Handling-Failed-Inserts-Upserts-with-Error-Tables.mdx":84458,"./2023-07-07-Skip-rocks-and-files-Turbocharge-Trino-queries-with-Hudi-multi-modal-indexing-subsystem.mdx":60881,"./2023-07-08-Quickly-start-using-Apache-Hudi-on-AWS-EMR.mdx":82317,"./2023-07-09-Hoodie-Timeline-Foundational-pillar-for-ACID-transactions.mdx":2507,"./2023-07-20-Backfilling-Apache-Hudi-Tables-in-Production-Techniques-and-Approaches-Using-AWS-Glue-by-Job-Target-LLC.mdx":14390,"./2023-07-21-AWS-Glue-Crawlers-now-supports-Apache-Hudi-Tables.mdx":57924,"./2023-07-27-Apache-Hudi-Revolutionizing-Big-Data-Management-for-Real-Time-Analytics.mdx":71533,"./2023-08-03-Apache-Hudi-on-AWS-Glue-A-Step-by-Step-Guide.mdx":39520,"./2023-08-03-Create-an-Apache-Hudi-based-near-real-time-transactional-data lake-using-AWS-DMS-Amazon-Kinesis-AWS-Glue-streaming-ETL-and-data-visualization-using-Amazon-QuickSight.mdx":72440,"./2023-08-03-Data-lake-Table-formats-Apache-Iceberg-vs-Apache-Hudi-vs-Delta-lake.mdx":4578,"./2023-08-05-Data-Lakehouse-Architecture-for-Big-Data-with-Apache-Hudi.mdx":73927,"./2023-08-09-Lakehouse-Trifecta-Delta-Lake-Apache-Iceberg-and-Apache-Hudi.mdx":38940,"./2023-08-22-Exploring-various-storage-types-in-Apache-Hudi.mdx":75859,"./2023-08-25-Delta-Hudi-Iceberg-Which-is-most-popular.mdx":46218,"./2023-08-28-Apache-Hudi-From-Zero-To-One.mdx":86793,"./2023-08-28-Delta-Hudi-Iceberg-A-Benchmark-Compilation.mdx":31776,"./2023-08-31-Incremental-Queries-with-Apache-Hudi-and-Apache-Flink.mdx":61105,"./2023-09-06-Apache-Hudi-From-Zero-To-One-blog-2.mdx":83342,"./2023-09-06-Lakehouse-or-Warehouse-Part-1-of-2.mdx":30282,"./2023-09-10-Demystifying-Copy-on-Write-in-Apache-Hudi-Understanding-Read-and-Write-Operations.mdx":31974,"./2023-09-12-Lakehouse-or-Warehouse-Part-2-of-2.mdx":38494,"./2023-09-13-Simplify-operational-data-processing-in-data-lakes-using-AWS-Glue-and-Apache-Hudi.mdx":7746,"./2023-09-15-Apache-Hudi-From-Zero-To-One-blog-3.mdx":22811,"./2023-09-19-A-Beginners-Guide-to-Apache-Hudi-with-PySpark-Part-1-of-2.mdx":34565,"./2023-09-22-Exploring-the-Architecture-of-Apache-Iceberg-Delta-Lake-and-Apache-Hudi.mdx":12114,"./2023-09-27-Apache-Hudi-From-Zero-To-One-blog-4.mdx":34811,"./2023-10-06-Apache-Hudi-Copy-on-Write-CoW-Table.mdx":18512,"./2023-10-11-starrocks-query-performance-with-apache-hudi-and-onehouse.mdx":10762,"./2023-10-17-Get-started-with-Apache-Hudi-using-AWS-Glue-by-implementing-key-design-concepts-Part-1.mdx":62981,"./2023-10-18-Apache-Hudi-From-Zero-To-One-blog-5.mdx":42166,"./2023-10-19-load-data-incrementally-from-transactional-data-lakes-to-data-warehouses.mdx":10116,"./2023-10-20-Its-Time-for-the-Universal-Data-Lakehouse.mdx":47237,"./2023-10-22-Tipico-Facilitates-Faster-Data-Access-with-a-Modern-Data-Strategy-on-AWS.mdx":46040,"./2023-10-29-UPSERT-Performance-Evaluation-of-Hudi-0-14-and-Spark-3-4-1-Record-Level-Index-Global-Bloom-Global-Simple-Indexes.mdx":91989,"./2023-11-01-record-level-index.md":92329,"./2023-11-13-Apache-Hudi-From-Zero-To-One-blog-6.mdx":42361,"./2023-11-19-Hudi-Streamer-DeltaStreamer-Hands-On-Guide-Local-Ingestion-from-Parquet-Source.mdx":96470,"./2023-11-22-Introducing-Apache-Hudi-support-with-AWS-Glue-crawlers.mdx":50472,"./2023-11-26-Real-Time-Data-Processing-with-Postgres-Debezium-Kafka-Schema-Registry-and-DeltaStreamer-Guide-for-Begineers.mdx":30090,"./2023-11-28-Apache-Hudi-Part-1-History-Getting-Started.mdx":9158,"./2023-11-30-Mastering-Data-Lakes-A-Deep-Dive-into-MINIO-Hudi-and-Delta-Streamer.mdx":80584,"./2023-12-01-Getting-started-with-Apache-Hudi.mdx":69216,"./2023-12-06-Apache-Hudi-From-Zero-To-One-blog-7.mdx":20040,"./2023-12-09-Getting-started-with-Apache-Hudi.mdx":40104,"./2023-12-13-what-is-apache-hudi.mdx":38399,"./2023-12-28-apache-hudi-2023-a-year-in-review.md":23298,"./2024-01-01-From-Data-lake-to-Microservices-Unleashing-the-Power-of-Apache-Hudi-Record-Level-Index-with-FastAPI-and-Spark-Connect.mdx":58112,"./2024-01-02-Build-a-federated-query-solution-with-Apache-Doris-Apache-Flink-and-Apache-Hudi.mdx":79725,"./2024-01-05-Small-Talk-about-Apache-Hudi.mdx":17876,"./2024-01-09-introduction-to-apache-hudi.mdx":75868,"./2024-01-11-In-House-Data-Lake-with-CDC-Processing-Hudi-Docker.mdx":20216,"./2024-01-17-Enforce-fine-grained-access-control-on-Open-Table-Formats-via-Amazon-EMR-integrated-with-AWS-Lake-Formation.mdx":9495,"./2024-01-18-Deleting-Items-from-Apache-Hudi-using-Delta-Streamer-in-UPSERT-Mode-with-Kafka-Avro-Messages.mdx":24326,"./2024-01-20-Data-Engineering-Bootstrapping-Data-lake-with-Apache-Hudi.mdx":10244,"./2024-01-20-Learn-How-to-Move-Data-From-MongoDB-to-Apache-Hudi-Using-PySpark.mdx":36371,"./2024-01-24-Use-Amazon-Athena-with-Spark-SQL-for-your-open-source-transactional-table-formats.mdx":85538,"./2024-01-30-Leverage-Partition-Paths-of-your-data-lake-tables-to-Optimize-Data-Retrieval-Costs-on-the-cloud.mdx":37909,"./2024-02-04-Apache-Hudi-Managing-Partition-on-a-petabyte-scale-table.mdx":1740,"./2024-02-06-Building-an-Open-Source-Data-Lake-House-with-Hudi-Postgres-Hive-Metastore-Minio-and-StarRocks.mdx":55711,"./2024-02-06-Combine-Transactional-Integrity-and-Data-Lake-Operations-with-YugabyteDB-and-Apache-Hudi.mdx":38859,"./2024-02-12-How-a-POC-became-a-production-ready-Hudi-data-lakehouse-through-close-team-collaboration.mdx":55505,"./2024-02-23-Enabling-near-real-time-data-analytics-on-the-data-lake.mdx":6052,"./2024-02-27-Building-Data-Lakes-on-AWS-with-Kafka-Connect-Debezium-Apicurio-Registry-and-Apache-Hudi.mdx":57339,"./2024-02-27-empowering-data-driven-excellence-how-the-bluestone-data-platform-embraced-data-mesh-for-success.mdx":48392,"./2024-03-05-Apache-Hudi-From-Zero-To-One-blog-9.mdx":90943,"./2024-03-10-navigating-the-future-the-evolutionary-journey-of-upstoxs-data-platform.mdx":14113,"./2024-03-14-Modern-Datalakes-with-Hudi--MinIO--and-HMS.mdx":78948,"./2024-03-16-Open-Table-Formats-part-1-Apache-Hudi-Hadoop-Upserts-Deletes-and-Incrementals.mdx":16304,"./2024-03-22-data-lake-cost-optimisation-strategies.mdx":32606,"./2024-03-23-options-on-kafka-sink-to-open-table-formats-apache-iceberg-and-apache-hudi.mdx":87593,"./2024-03-30-record-level-indexing-apache-hudi-delivers-70-faster-point.mdx":73919,"./2024-04-03-hands-on-guide-reading-data-from-hudi-tables-joining-delta.mdx":40966,"./2024-04-21-build-real-time-streaming-pipeline-with-kinesis-apache-flink-and-apache-hudi.mdx":40968,"./2024-04-24-understanding-apache-hudi-consistency-model-part-1.mdx":24779,"./2024-04-24-understanding-apache-hudi-consistency-model-part-2.mdx":16804,"./2024-04-24-understanding-apache-hudi-consistency-model-part-3.mdx":63577,"./2024-04-25-apache-hudi-vs-apache-iceberg-a-comprehensive-comparison.mdx":45598,"./2024-05-02-how-query-apache-hudi-tables-python-using-daft-spark-free.mdx":33987,"./2024-05-07-learn-how-read-hudi-data-aws-glue-ray-using-daft-spark.mdx":27468,"./2024-05-10-building-analytical-apps-on-the-lakehouse-using-apache-hudi-daft-streamlit.mdx":66217,"./2024-05-19-apache-hudi-on-aws-glue.mdx":25734,"./2024-05-22-use-aws-data-exchange-to-seamlessly-share-apache-hudi-datasets.mdx":87602,"./2024-05-27-apache-hudi-vs-delta-lake-choosing-the-right-tool-for-your-data-lake-on-aws.mdx":8432,"./2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.mdx":65944,"./2024-06-18-how-to-use-apache-hudi-with-databricks.mdx":76535,"./2024-07-11-what-is-a-data-lakehouse.md":78555,"./2024-07-30-data-lake-cdc.md":36637,"./2024-07-31-hudi-file-formats.md":70435};function n(e){var a=o(e);return t(a)}function o(e){if(!t.o(i,e)){var a=new Error("Cannot find module '"+e+"'");throw a.code="MODULE_NOT_FOUND",a}return i[e]}n.keys=function(){return Object.keys(i)},n.resolve=o,e.exports=n,n.id=43152},68234:(e,a,t)=>{"use strict";t.d(a,{A:()=>o});var i=t(96540),n=t(75489);const o=e=>{let{authors:a=[],className:t,withLink:o=!0}=e;const r=e=>i.createElement("span",{className:t,itemProp:"name"},e.name);return i.createElement(i.Fragment,null,a.map(((e,t)=>i.createElement("div",{key:t},i.createElement("div",null,e.name&&i.createElement("div",null,0!==t?t!==a.length-1?",":"and":"",o?i.createElement(n.A,{href:e.url,itemProp:"url"},r(e)):r(e)))))))}},9230:(e,a,t)=>{"use strict";t.d(a,{A:()=>o});var i=t(96540),n=t(92303);function o(e){let{children:a,url:o}=e;return(0,n.A)()&&(t.g.window.location.href=o),i.createElement("span",null,a,"or click ",i.createElement("a",{href:o},"here"))}},27301:(e,a,t)=>{"use strict";t.r(a),t.d(a,{default:()=>ot});var i=t(96540),n=t(27300),o=t(44586);const r="titleWrapper_KIWM",s="primaryTxt_31pU",l="secondaryTxt_YgDv";const d=(0,t(50039).A)("h2"),c=e=>{let{primaryText:a,secondaryText:t,id:n}=e;return i.createElement(d,{className:r,id:n},i.createElement("span",{className:s},a),"\xa0",i.createElement("span",{className:l},t))},p=()=>i.createElement("section",{className:"data-lake"},i.createElement("div",{className:"container"},i.createElement(c,{primaryText:"What is",secondaryText:"Hudi"}),i.createElement("div",{className:"sub-title text--center text--semibold margin-bottom--md"},"Apache Hudi is a transactional data lake platform that brings database and data warehouse capabilities to the data lake. Hudi reimagines slow old-school batch data processing with a powerful new incremental processing framework for low latency minute-level analytics."),i.createElement("img",{className:"hudi-lake text-center",src:t(15679).A,alt:"Hudi Data Lake"}))),g={banner:"banner_VzXK",sideMicrophone:"sideMicrophone_GFdd",sideCalendar:"sideCalendar_Yi+S",joinButton:"joinButton_8exB",registerbutton:"registerbutton_zbiq",flexContainer:"flexContainer_q5k2",flexParagraph:"flexParagraph_xEPN"};var u=t(58168),m=t(20053);const h="button_VODY",b="primary_YeCq",y="secondary_XDJr";var f=t(75489);const w=e=>{let{type:a="primary",className:t,...n}=e;return i.createElement(f.A,(0,u.A)({className:(0,m.A)(h,{[b]:"primary"===a,[y]:"secondary"===a,[t]:t})},n))};var k;function v(){return v=Object.assign?Object.assign.bind():function(e){for(var a=1;a<arguments.length;a++){var t=arguments[a];for(var i in t)({}).hasOwnProperty.call(t,i)&&(e[i]=t[i])}return e},v.apply(null,arguments)}const A=e=>{let{title:a,titleId:t,...n}=e;return i.createElement("svg",v({width:18,height:20,viewBox:"0 0 18 20",fill:"none",xmlns:"http://www.w3.org/2000/svg","aria-labelledby":t},n),a?i.createElement("title",{id:t},a):null,k||(k=i.createElement("path",{d:"M1.493 6.368h15.632a.692.692 0 0 0 .69-.732l-.008-.18a3.464 3.464 0 0 0-3.458-3.239h-.889v-.692a.692.692 0 1 0-1.384 0v.692H6.542v-.692a.692.692 0 0 0-1.384 0v.692H4.27A3.465 3.465 0 0 0 .811 5.458l-.008.179a.69.69 0 0 0 .69.73Zm16.43 2.058a.692.692 0 0 0-.69-.674H1.385a.692.692 0 0 0-.692.674 84.751 84.751 0 0 0 .131 7.342 3.461 3.461 0 0 0 3.235 3.235c3.496.219 7.002.218 10.498 0a3.462 3.462 0 0 0 3.235-3.236c.149-2.393.193-4.863.13-7.341Zm-5.155 3.476a1.038 1.038 0 1 1 0-2.075 1.038 1.038 0 0 1 0 2.075Zm1.038 2.422a1.038 1.038 0 1 1-2.075 0 1.038 1.038 0 0 1 2.075 0ZM5.85 13.286a1.038 1.038 0 1 1 0 2.076 1.038 1.038 0 0 1 0-2.076Zm-1.038-2.421a1.037 1.037 0 1 1 2.075 0 1.037 1.037 0 0 1-2.075 0Zm3.46 3.459a1.038 1.038 0 1 1 2.075 0 1.038 1.038 0 0 1-2.076 0Zm1.037-2.422a1.038 1.038 0 1 1 0-2.075 1.038 1.038 0 0 1 0 2.075Z",fill:"#fff"})))};var C;function H(){return H=Object.assign?Object.assign.bind():function(e){for(var a=1;a<arguments.length;a++){var t=arguments[a];for(var i in t)({}).hasOwnProperty.call(t,i)&&(e[i]=t[i])}return e},H.apply(null,arguments)}const D=e=>{let{title:a,titleId:t,...n}=e;return i.createElement("svg",H({width:13,height:20,viewBox:"0 0 13 20",fill:"none",xmlns:"http://www.w3.org/2000/svg","aria-labelledby":t},n),a?i.createElement("title",{id:t},a):null,C||(C=i.createElement("path",{d:"M6.606 14.257v5.029h3.46a.357.357 0 1 1 0 .714H2.432a.357.357 0 1 1 0-.714h3.46v-5.029A6.089 6.089 0 0 1 .168 8.193a.357.357 0 0 1 .714 0 5.367 5.367 0 1 0 10.736 0 .357.357 0 0 1 .714 0 6.089 6.089 0 0 1-5.725 6.064Zm-4.532-4.9V3.214A3.217 3.217 0 0 1 5.288 0H7.21a3.217 3.217 0 0 1 3.214 3.214v6.143a3.22 3.22 0 0 1-3.214 3.214H5.288a3.22 3.22 0 0 1-3.214-3.214Zm2.903-6.89a.358.358 0 0 0 .358.358h1.967a.357.357 0 0 0 0-.714H5.335a.358.358 0 0 0-.358.357Zm-.992 1.908a.354.354 0 0 0 .357.357h3.814a.357.357 0 1 0 0-.714H4.342a.356.356 0 0 0-.357.357Zm0 1.91a.356.356 0 0 0 .357.358h3.814a.357.357 0 1 0 0-.714H4.342a.354.354 0 0 0-.357.357Zm0 1.908a.356.356 0 0 0 .357.357h3.814a.357.357 0 0 0 0-.714H4.342a.356.356 0 0 0-.357.357Zm0 1.907a.356.356 0 0 0 .357.357h3.814a.357.357 0 1 0 0-.714H4.342a.356.356 0 0 0-.357.357Z",fill:"#fff"})))},S=()=>i.createElement("section",null,i.createElement("div",{className:g.banner},i.createElement("div",{className:"container"},i.createElement("div",{className:g.flexContainer},i.createElement("div",{className:g.bannercontent},i.createElement("h3",null," Scaling Complex Data Workflows Using Apache Hudi, from Uber"),i.createElement("p",{className:g.flexParagraph},i.createElement("span",{className:g.sideMicrophone},i.createElement(D,null)),"Linkedin Live Event | ",i.createElement("span",{className:g.sideCalendar},i.createElement(A,null)),"Aug 20th, 9am Pacific Time")),i.createElement("div",{className:g.joinButton},i.createElement(w,{class:g.registerbutton,type:"secondary",to:"https://www.linkedin.com/events/scalingcomplexdataworkflowsusin7227002765909057537/theater/"},"Join Now")))))),x="featuresWrapper_IU+R",T="titleWrapper_4muF",I="wrapperContainer_fvNI",N="boxWrapper_kqgO",M="featuresTitle_M4-j",E=()=>i.createElement("svg",{width:75,height:75,viewBox:"0 0 75 75",fill:"none",xmlns:"http://www.w3.org/2000/svg"},i.createElement("rect",{opacity:"0.5",width:75,height:75,rx:8,fill:"#CEEEF7"}),i.createElement("path",{d:"M22.7957 25.2945C21.9656 25.6816 21.066 25.8599 20.1277 25.8599C16.6446 25.8599 13.8056 23.0624 13.8056 19.637C13.8056 18.5112 14.1257 17.429 14.6758 16.4808C14.8504 16.1929 14.7299 15.8562 14.5063 15.6911L14.5066 15.6906L14.4967 15.6848C14.2068 15.5136 13.8664 15.6301 13.6982 15.851L13.6937 15.857L13.69 15.8635C13.0288 17.0262 12.6497 18.3303 12.6497 19.6371C12.6497 23.6941 16.0116 27.0017 20.1277 27.0017C21.3324 27.0017 22.5365 26.7117 23.6214 26.1712L23.421 27.5131C23.3616 27.8149 23.603 28.092 23.8825 28.147L23.8825 28.147L23.8859 28.1476C24.2258 28.2034 24.5147 27.98 24.5731 27.6923L24.5733 27.6924L24.5742 27.6859L24.9469 25.0707C25.0034 24.7326 24.7736 24.4498 24.4851 24.393L24.4851 24.3929L24.4788 24.392L21.8227 24.025C21.4835 23.97 21.1953 24.1932 21.137 24.4805L21.1348 24.4915V24.5027C21.1348 24.6379 21.1755 24.7876 21.2607 24.9064C21.3465 25.0261 21.4799 25.1164 21.6572 25.1194L22.7957 25.2945Z",fill:"#0DB1F9",stroke:"#0DB1F9",strokeWidth:"0.223529"}),i.createElement("path",{d:"M20.15 12.2725C19.0383 12.2725 17.9269 12.5193 16.933 12.9745L17.1366 11.5307C17.1957 11.2291 16.9544 10.9523 16.675 10.8974L16.6751 10.8973L16.6715 10.8967C16.3259 10.8401 16.0398 11.0708 15.9838 11.4015L15.9838 11.4015L15.9834 11.4044L15.6109 14.0179C15.5518 14.3196 15.793 14.5964 16.0724 14.6514L16.0724 14.6515L16.0768 14.6522L18.7343 15.0653L18.7429 15.0667H18.7515C18.9089 15.0667 19.0607 15.028 19.1825 14.9434C19.3058 14.8576 19.3923 14.7286 19.4209 14.5621C19.4822 14.2539 19.2322 13.9795 18.9097 13.9265C18.9096 13.9265 18.9095 13.9265 18.9094 13.9265L17.9828 13.7692C18.6884 13.541 19.4199 13.414 20.1501 13.414C23.6332 13.414 26.4721 16.2115 26.4721 19.637C26.4721 20.7661 26.1969 21.8467 25.6034 22.7907L25.6026 22.7921C25.4259 23.0821 25.549 23.4214 25.8229 23.5863C26.0795 23.771 26.4252 23.6304 26.5855 23.3683C27.2972 22.2469 27.6281 20.9407 27.6281 19.6371C27.6281 15.5806 24.3133 12.2725 20.15 12.2725Z",fill:"#0DB1F9",stroke:"#0DB1F9",strokeWidth:"0.223529"}),i.createElement("path",{d:"M43.1265 24.2075L43.1266 24.2067L43.4436 14.4175H44.8381C45.1202 14.4175 45.3505 14.1872 45.3505 13.9051C45.3505 13.6229 45.1202 13.3926 44.8381 13.3926H40.7425V12.6028C40.7425 11.6595 39.9712 10.8882 39.0279 10.8882H35.9826C35.0393 10.8882 34.268 11.6595 34.268 12.6028V13.3926H30.1524C29.8703 13.3926 29.6399 13.6229 29.6399 13.9051C29.6399 14.1872 29.8703 14.4175 30.1524 14.4175H31.5468L31.8639 24.2067L31.8639 24.2075C31.9264 25.7897 33.1974 27.0196 34.7804 27.0196H40.21C41.7931 27.0196 43.0641 25.7898 43.1265 24.2075ZM35.293 12.6028V12.6028C35.293 12.2238 35.6035 11.9132 35.9825 11.9132H39.0279C39.4069 11.9132 39.7175 12.2238 39.7175 12.6028V13.3926H35.2929L35.293 12.6028ZM34.8006 25.9947C33.7801 25.9947 32.9486 25.2028 32.9088 24.1628L32.9087 24.1594L32.5918 14.3975H42.4384L42.1019 24.1589C42.1019 24.159 42.1019 24.159 42.1019 24.1591C42.0639 25.1817 41.2308 25.9947 40.2101 25.9947H34.8006Z",fill:"#0DB1F9",stroke:"#0DB1F9",strokeWidth:"0.223529"}),i.createElement("path",{d:"M37.3934 23.6411V23.6536H37.5052C37.7873 23.6536 38.0176 23.4233 38.0176 23.1412V17.1507C38.0176 16.8685 37.7873 16.6382 37.5052 16.6382C37.2231 16.6382 36.9927 16.8686 36.9927 17.1507V23.1412C36.9927 23.3848 37.1647 23.5899 37.3934 23.6411Z",fill:"#0DB1F9",stroke:"#0DB1F9",strokeWidth:"0.223529"}),i.createElement("path",{d:"M34.9891 23.6411V23.6536H35.1009C35.383 23.6536 35.6133 23.4233 35.6133 23.1412V17.1507C35.6133 16.8685 35.383 16.6382 35.1009 16.6382C34.8187 16.6382 34.5884 16.8686 34.5884 17.1507V23.1412C34.5884 23.3848 34.7604 23.5899 34.9891 23.6411Z",fill:"#0DB1F9",stroke:"#0DB1F9",strokeWidth:"0.223529"}),i.createElement("path",{d:"M39.7974 23.6411V23.6536H39.9092C40.1913 23.6536 40.4216 23.4233 40.4216 23.1412V17.1507C40.4216 16.8685 40.1913 16.6382 39.9092 16.6382C39.6271 16.6382 39.3968 16.8686 39.3968 17.1507V23.1412C39.3968 23.3848 39.5687 23.5899 39.7974 23.6411Z",fill:"#0DB1F9",stroke:"#0DB1F9",strokeWidth:"0.223529"}),i.createElement("path",{d:"M60.1415 42.8475C60.1415 42.895 60.1245 42.9595 60.1347 43.0037C60.1347 43.0071 60.1619 42.8475 60.1483 42.9154C60.1449 42.9324 60.1415 42.9527 60.1381 42.9697C60.1211 43.058 60.094 43.1395 60.0668 43.2244C60.0294 43.3364 60.1313 43.0988 60.0736 43.204C60.0566 43.238 60.0396 43.2753 60.0193 43.3093C59.9751 43.3874 59.9276 43.4621 59.8766 43.5368C59.8732 43.5402 59.8359 43.5877 59.8393 43.5877C59.8359 43.5843 59.9276 43.4825 59.88 43.5368C59.8495 43.5707 59.8223 43.6047 59.7951 43.6387C59.7238 43.7202 59.6457 43.7983 59.5676 43.873C59.4963 43.9409 59.4182 44.002 59.3435 44.0665C59.2858 44.1175 59.3775 44.0427 59.3809 44.0394C59.3639 44.0563 59.3367 44.0699 59.3163 44.0869C59.2654 44.1242 59.2145 44.1616 59.1635 44.1956C58.7764 44.4604 58.3588 44.6811 57.9309 44.8747C57.8867 44.8951 57.8426 44.9121 57.8019 44.9324C57.7339 44.963 57.8528 44.9121 57.8528 44.9121C57.829 44.9189 57.8086 44.9324 57.7849 44.9392C57.6762 44.9834 57.5709 45.0275 57.4623 45.0683C57.2585 45.1464 57.0548 45.2177 56.8477 45.2856C55.8969 45.598 54.9155 45.8187 53.9308 45.9817C53.8051 46.0021 53.6761 46.0225 53.5504 46.0428C53.4893 46.053 53.4316 46.0598 53.3705 46.07C53.3297 46.0768 53.109 46.1074 53.2584 46.087C53.0105 46.1209 52.7592 46.1481 52.5114 46.1753C51.9817 46.2296 51.452 46.2703 50.9188 46.2975C49.8084 46.3552 48.6913 46.3518 47.5809 46.2941C47.0478 46.267 46.5146 46.2228 45.9815 46.1651C45.8593 46.1515 45.7404 46.1379 45.6182 46.1243C45.557 46.1175 45.4993 46.1108 45.4382 46.1006C45.4076 46.0972 45.3771 46.0938 45.3499 46.0904C45.2175 46.0734 45.4042 46.1006 45.3227 46.087C45.0816 46.0496 44.8406 46.0157 44.5995 45.9749C43.6012 45.8085 42.6096 45.581 41.6453 45.2619C41.4211 45.1872 41.2004 45.1091 40.9797 45.0242C40.9321 45.0038 40.8812 44.9868 40.8337 44.9664C40.8099 44.9562 40.7861 44.9461 40.7624 44.9359C40.6877 44.9053 40.8031 44.9562 40.8065 44.9562C40.7216 44.9529 40.5892 44.8612 40.5111 44.8238C40.0764 44.6235 39.6554 44.3926 39.2682 44.1107C39.2479 44.0938 39.2275 44.0802 39.2037 44.0632C39.1392 44.0157 39.2377 44.0904 39.2377 44.0904C39.2071 44.087 39.146 44.0157 39.1222 43.9953C39.0373 43.924 38.9558 43.8459 38.8743 43.7678C38.8098 43.7033 38.7521 43.6354 38.691 43.5674C38.6095 43.4724 38.7623 43.6693 38.691 43.5674C38.6638 43.5267 38.6332 43.4893 38.6095 43.4452C38.5653 43.3773 38.528 43.306 38.4906 43.2347C38.4838 43.2211 38.477 43.1939 38.4669 43.1803C38.4669 43.1803 38.5178 43.3196 38.4974 43.255C38.4838 43.2075 38.4635 43.16 38.4499 43.1124C38.4397 43.0751 38.4295 43.0411 38.4227 43.0038C38.4159 42.9766 38.3955 42.8442 38.4125 42.9664C38.4295 43.0886 38.4125 42.9528 38.4125 42.9223C38.4091 42.8849 38.4091 42.8442 38.4125 42.8068C38.4125 42.7932 38.4159 42.671 38.4227 42.671C38.4261 42.671 38.3854 42.8815 38.4125 42.7593C38.4193 42.7287 38.4227 42.6981 38.4329 42.6676C38.4431 42.6234 38.4567 42.5759 38.4703 42.5317C38.477 42.508 38.4872 42.4876 38.494 42.4638C38.5348 42.3484 38.4533 42.5487 38.4669 42.525C38.5212 42.4299 38.5619 42.3348 38.6197 42.2431C38.6468 42.199 38.6774 42.1582 38.708 42.1141C38.7759 42.019 38.6366 42.199 38.6672 42.1684C38.6876 42.1446 38.708 42.1175 38.7283 42.0937C38.8098 41.9986 38.8947 41.9103 38.9864 41.8254C39.0305 41.7847 39.0747 41.744 39.1222 41.7032C39.146 41.6828 39.1698 41.6625 39.1935 41.6421C39.2173 41.6217 39.3226 41.5402 39.2241 41.6149C39.1256 41.6896 39.2241 41.6183 39.2445 41.6013C39.2716 41.581 39.2988 41.564 39.326 41.5436C39.3905 41.4995 39.455 41.4519 39.5229 41.4112C39.6485 41.3297 39.7742 41.255 39.9032 41.1803C40.1953 41.0173 40.4975 40.8746 40.8031 40.7422C40.871 40.7117 40.7318 40.7694 40.7385 40.7694C40.7589 40.766 40.7793 40.7524 40.7963 40.7456C40.837 40.7286 40.8744 40.7151 40.9151 40.6981C41.0102 40.6607 41.1053 40.6234 41.2004 40.5894C41.3769 40.5249 41.5535 40.4638 41.7335 40.406C42.545 40.1412 43.3804 39.9408 44.2191 39.788C44.4534 39.7439 44.6877 39.7065 44.922 39.6692C45.034 39.6522 45.1427 39.6352 45.2548 39.6183C45.3125 39.6115 45.4212 39.5945 45.2989 39.6115C45.3634 39.6013 45.4313 39.5945 45.4993 39.5843C45.995 39.5232 46.4942 39.4722 46.9967 39.4383C49.0715 39.2855 51.1633 39.3364 53.2277 39.6047C53.3635 39.6216 53.136 39.5911 53.2719 39.6115C53.3262 39.6182 53.3839 39.6284 53.4382 39.6352C53.5673 39.6556 53.6963 39.6726 53.8253 39.6964C54.0495 39.7337 54.2736 39.7711 54.4977 39.8152C54.9289 39.8967 55.3602 39.9918 55.788 40.1004C56.1819 40.2023 56.5724 40.3144 56.9595 40.4434C57.1429 40.5045 57.3263 40.569 57.5062 40.637C57.5877 40.6675 57.6658 40.6981 57.7473 40.732C57.7711 40.7422 57.9035 40.7999 57.8017 40.7558C57.6998 40.7117 57.8458 40.7762 57.873 40.7864C58.1752 40.9188 58.4706 41.0648 58.7558 41.2278C58.8883 41.3025 59.0173 41.384 59.143 41.4689C59.1973 41.5062 59.2516 41.5436 59.306 41.5844C59.3331 41.6047 59.3569 41.6251 59.3841 41.6421C59.4418 41.6828 59.2992 41.5742 59.3093 41.5844C59.3229 41.5979 59.3433 41.6115 59.3603 41.6251C59.5538 41.7847 59.7304 41.9545 59.8934 42.1446C59.9681 42.2329 59.8323 42.0563 59.8527 42.0903C59.8662 42.1107 59.8832 42.1311 59.9002 42.1548C59.9341 42.2024 59.9681 42.2533 59.9987 42.3042C60.0258 42.3484 60.0496 42.3891 60.07 42.4333C60.0802 42.4536 60.0903 42.4774 60.1005 42.4978C60.1549 42.6065 60.0462 42.3484 60.0734 42.4333C60.1039 42.525 60.1379 42.6166 60.1549 42.7117C60.1583 42.7355 60.1617 42.7593 60.1684 42.7796C60.1786 42.8204 60.1481 42.5793 60.1549 42.6913C60.1345 42.7491 60.1379 42.8 60.1413 42.8475C60.1447 43.2041 60.4503 43.5437 60.8204 43.5267C61.1872 43.5097 61.503 43.2279 61.4996 42.8475C61.4894 41.6115 60.4979 40.7015 59.5097 40.1106C58.0971 39.2651 56.4434 38.8101 54.8407 38.5045C52.7218 38.1004 50.5486 37.9544 48.3924 38.0121C46.338 38.0698 44.2633 38.3041 42.2734 38.8305C40.8234 39.2142 39.3192 39.7541 38.1612 40.7388C37.7198 41.1124 37.3598 41.5809 37.1629 42.1277C36.9218 42.7966 37.0576 43.564 37.4379 44.1515C37.9846 44.9936 38.8777 45.5607 39.7707 45.9784C40.7046 46.4164 41.6927 46.7356 42.6944 46.9767C44.8744 47.4996 47.1394 47.7068 49.3768 47.7C51.6213 47.6932 53.8964 47.4758 56.0766 46.9224C57.0171 46.6847 57.9442 46.3723 58.8203 45.9546C59.7032 45.5335 60.603 44.9563 61.1294 44.1074C61.3637 43.7304 61.4927 43.2958 61.4995 42.851C61.5063 42.4944 61.1837 42.1549 60.8204 42.1718C60.4468 42.1854 60.1446 42.4672 60.1412 42.8476L60.1415 42.8475Z",fill:"#1C1E21"}),i.createElement("path",{d:"M60.1343 43.007V46.0122V48.104V48.8035C60.1343 48.8884 60.114 49.0038 60.1343 49.0853C60.114 49.0072 60.1615 48.9359 60.1411 49.0174C60.1309 49.065 60.1207 49.1159 60.1072 49.1634C60.0936 49.211 60.0766 49.2585 60.0596 49.3095C60.0257 49.4147 60.0766 49.2619 60.0834 49.2551C60.063 49.2755 60.046 49.3366 60.0325 49.3604C59.9815 49.4589 59.9204 49.5471 59.8593 49.6388C59.8015 49.7237 59.883 49.5981 59.8864 49.6049C59.8864 49.6083 59.8253 49.6796 59.8321 49.6728C59.788 49.7237 59.7438 49.7747 59.6963 49.8256C59.608 49.9173 59.5129 50.0056 59.4144 50.0871C59.3873 50.1074 59.3635 50.1312 59.3363 50.1516C59.3194 50.1652 59.2311 50.2331 59.3024 50.1787C59.3771 50.1244 59.282 50.1923 59.265 50.2059C59.2277 50.2331 59.1937 50.2568 59.1564 50.284C58.8813 50.4742 58.5927 50.6372 58.2973 50.7866C58.192 50.8375 58.0867 50.8884 57.9815 50.936C57.9305 50.9598 57.8796 50.9835 57.8287 51.0039C57.8049 51.0141 57.7709 51.0243 57.7506 51.0379C57.8015 51.0039 57.8728 50.9869 57.7743 51.0277C57.5468 51.1194 57.3159 51.2076 57.0816 51.2891C56.5621 51.4691 56.029 51.6185 55.4958 51.7476C54.9016 51.8902 54.3006 52.0056 53.6962 52.1007C53.5434 52.1245 53.3905 52.1482 53.2377 52.1686C53.2072 52.172 53.1053 52.1856 53.2445 52.1686C53.2072 52.172 53.1664 52.1788 53.1291 52.1822C53.0408 52.1924 52.9525 52.2026 52.8642 52.2162C52.5348 52.2535 52.2021 52.2875 51.8727 52.3146C50.4533 52.4335 49.0237 52.4539 47.6009 52.3758C46.9184 52.3384 46.2392 52.2807 45.5601 52.1958C45.5194 52.1924 45.482 52.1856 45.4447 52.1822C45.4243 52.1788 45.2986 52.1618 45.3903 52.1754C45.4786 52.189 45.3564 52.172 45.336 52.1686C45.2986 52.1618 45.2579 52.1584 45.2205 52.1516C45.0609 52.1279 44.8979 52.1041 44.7384 52.0769C44.426 52.026 44.1169 51.9683 43.808 51.9072C42.7451 51.6898 41.706 51.3944 40.6975 50.9869C40.6126 50.953 40.8231 51.0413 40.7416 51.0073C40.7145 50.9971 40.6907 50.9835 40.6635 50.9734C40.6126 50.9496 40.5617 50.9292 40.5107 50.9054C40.3376 50.8273 40.1644 50.7187 39.964 50.7187C40.0795 50.7492 40.1916 50.7798 40.307 50.8104C40.015 50.6644 39.7331 50.5048 39.4615 50.3248C39.4004 50.284 39.3426 50.2433 39.2849 50.2025C39.2577 50.1822 39.2272 50.1618 39.2 50.1414C39.1219 50.0837 39.2883 50.2161 39.217 50.1516C39.1117 50.0633 39.0065 49.975 38.9046 49.8766C38.8163 49.7917 38.7382 49.7034 38.6601 49.6117C38.5956 49.5404 38.6941 49.6151 38.6873 49.649C38.6873 49.6423 38.6363 49.5777 38.6363 49.5811C38.6024 49.5302 38.5684 49.4793 38.5379 49.4249C38.5107 49.3808 38.4869 49.3332 38.4632 49.2857C38.453 49.2619 38.4428 49.2382 38.4292 49.2144C38.3952 49.1533 38.4598 49.2891 38.4598 49.2891C38.4598 49.245 38.4224 49.1872 38.4088 49.1465C38.3919 49.0854 38.3783 49.0242 38.3647 48.9631C38.3409 48.8511 38.3647 49.082 38.3715 49.0242C38.3749 49.0107 38.3715 48.9971 38.3715 48.9869C38.3749 48.9292 38.3715 48.868 38.3715 48.8103V48.1787V44.0835V42.9562H37.0132C37.0709 43.418 37.2102 43.8458 37.4818 44.2296C37.7874 44.6608 38.161 45.0004 38.5854 45.3094C39.5056 45.9783 40.616 46.3926 41.6958 46.7186L41.2204 46.2432C41.2951 46.5488 41.5261 46.6745 41.7977 46.7695C41.9166 46.8137 41.9947 46.8239 42.1271 46.851C41.9709 46.8171 42.178 46.8714 42.195 46.8748C42.65 47.0004 43.0167 46.6541 43.0541 46.2194V46.2126C42.7689 46.43 42.4802 46.6507 42.195 46.868C42.2629 46.885 42.3308 46.8952 42.3954 46.9122C42.4497 46.9257 42.5957 46.9257 42.6263 46.9529C42.7485 47.0718 42.4497 46.9054 42.5516 46.9529C42.6229 46.9869 42.7247 46.9971 42.7994 47.0106C42.9862 47.048 43.1866 47.0548 43.3563 46.9495C43.1832 46.9733 43.0066 46.9937 42.8334 47.0174C45.5092 47.6354 48.3005 47.812 51.0408 47.6592C52.4636 47.5811 53.8898 47.4079 55.2854 47.1125C56.5282 46.851 57.7608 46.4809 58.9052 45.924C59.5402 45.615 60.1649 45.2245 60.6573 44.7151C61.1225 44.2329 61.4112 43.6794 61.4961 43.0173C61.5436 42.6641 61.1497 42.3246 60.8169 42.3381C60.406 42.3449 60.1819 42.6267 60.1344 43.007C60.165 42.7829 60.1378 42.9561 60.1276 43.0104C60.1174 43.058 60.1038 43.1089 60.0869 43.1564C60.0801 43.1802 60.0699 43.204 60.0597 43.2277C60.0393 43.2923 60.0155 43.2549 60.0835 43.1734C60.0529 43.2108 60.0359 43.2719 60.0122 43.316C59.9986 43.3432 59.8254 43.5843 59.8322 43.5945C59.8288 43.5877 59.9408 43.4654 59.8593 43.5605C59.839 43.5843 59.822 43.6047 59.8016 43.6284C59.7609 43.676 59.7201 43.7201 59.676 43.7643C59.5843 43.856 59.4892 43.9442 59.3907 44.0257C59.3636 44.0461 59.3398 44.0699 59.3126 44.0903C59.2685 44.1276 59.2719 44.1582 59.3262 44.0801C59.2923 44.131 59.2006 44.1718 59.153 44.2057C58.895 44.3823 58.6267 44.5385 58.3483 44.6811C58.226 44.7456 58.1004 44.8033 57.9747 44.8611C57.9136 44.8882 57.8525 44.9154 57.7914 44.9426C57.7235 44.9731 57.7642 44.9867 57.8151 44.9324C57.7982 44.9494 57.7438 44.9629 57.7201 44.9697C57.445 45.0818 57.17 45.1803 56.8882 45.2753C56.2837 45.4757 55.6657 45.6387 55.0443 45.7745C54.7081 45.8492 54.3686 45.9137 54.029 45.9715C53.866 45.9986 53.703 46.0258 53.54 46.0496C53.4585 46.0631 53.3804 46.0733 53.2989 46.0835C53.2616 46.0903 53.2208 46.0937 53.1835 46.1005C53.2819 46.0869 53.2208 46.0971 53.1563 46.1039C52.5043 46.1888 51.8489 46.2499 51.1902 46.2873C49.8014 46.3721 48.4092 46.3654 47.0237 46.2669C46.6502 46.2397 46.2766 46.2058 45.9065 46.165C45.7265 46.1446 45.5466 46.1243 45.3632 46.1005C45.3428 46.0971 45.2172 46.0801 45.3055 46.0937C45.3972 46.1073 45.2376 46.0835 45.2172 46.0801C45.1289 46.0665 45.0406 46.0563 44.9557 46.0428C44.6162 45.9918 44.2766 45.9307 43.937 45.8662C43.7842 45.8356 43.628 45.8051 43.4752 45.7711C43.2036 45.7134 42.9183 45.6149 42.6603 45.7745C42.8334 45.7507 43.01 45.7304 43.1832 45.7066C43.0372 45.6726 42.8946 45.6285 42.7486 45.6115C42.7044 45.6047 42.6603 45.5979 42.6161 45.5945C42.752 45.6081 42.7859 45.6115 42.7214 45.5945C42.6637 45.5776 42.6025 45.5708 42.5448 45.5572C42.0864 45.4519 41.7231 45.7643 41.6857 46.2126V46.2194C41.9709 46.002 42.2596 45.7813 42.5448 45.564C42.4939 45.5504 42.4429 45.5368 42.3886 45.5266C42.3377 45.5164 42.1577 45.4655 42.117 45.4927C42.1543 45.4689 42.3105 45.5334 42.1985 45.4791C42.1543 45.4587 42.1 45.4451 42.0524 45.4281C42.212 45.5877 42.3682 45.7439 42.5278 45.9035C42.4022 45.4451 41.9404 45.3806 41.5533 45.2516C41.2986 45.1667 41.0473 45.075 40.7994 44.9731C40.7858 44.9663 40.6025 44.8916 40.7451 44.9527C40.684 44.9256 40.6228 44.8984 40.5617 44.8712C40.4497 44.8203 40.3376 44.7694 40.229 44.7117C39.9913 44.5928 39.757 44.4638 39.5328 44.3178C39.4344 44.2532 39.3393 44.1853 39.2408 44.1174C39.1118 44.0291 39.3223 44.1887 39.2136 44.097C39.1627 44.0563 39.1118 44.0121 39.0608 43.968C39.0269 43.9374 38.6194 43.5639 38.6432 43.5367C38.7043 43.6182 38.7145 43.6284 38.6703 43.5707C38.6635 43.5605 38.6534 43.5469 38.6466 43.5367C38.616 43.4926 38.5854 43.4451 38.5549 43.3975C38.5141 43.3296 38.4768 43.2583 38.4394 43.187C38.3783 43.0715 38.4904 43.3432 38.4394 43.187C38.4258 43.1462 38.4089 43.1021 38.3987 43.0614C38.3783 42.9934 38.3342 42.7082 38.3647 42.9493C38.3206 42.5995 38.0829 42.2532 37.6856 42.2701C37.3188 42.2871 37.0064 42.5689 37.0064 42.9493V46.0394V48.1345C37.0064 48.508 36.9792 48.8917 37.0369 49.2653C37.132 49.8629 37.4954 50.4028 37.9164 50.8205C38.1745 51.0752 38.4665 51.2959 38.7654 51.4963C38.9182 51.5981 39.071 51.6898 39.2306 51.7815C39.4479 51.9037 39.7026 52.0769 39.9538 52.0769C39.8384 52.0464 39.7263 52.0158 39.6109 51.9852C41.7943 53.0379 44.2595 53.4725 46.6571 53.6729C49.4619 53.9106 52.3279 53.785 55.0886 53.2281C56.2194 53.0005 57.323 52.6814 58.3858 52.2297C59.4453 51.7815 60.6066 51.1329 61.1771 50.0871C61.3503 49.7713 61.4759 49.4113 61.4827 49.0446C61.4861 48.8918 61.4827 48.7356 61.4827 48.5794V46.6744V43.0818V43.0105C61.4827 42.6539 61.1703 42.3144 60.8035 42.3313C60.447 42.3449 60.1346 42.6267 60.1346 43.0071L60.1343 43.007Z",fill:"#1C1E21"}),i.createElement("path",{d:"M60.1339 49.085V54.1445V55.0681C60.1339 55.0953 60.1305 55.1225 60.1339 55.1496V55.1632C60.1407 55.187 59.4956 56.8407 60.1475 55.0579C60.1305 55.1021 60.1271 55.1564 60.117 55.204C60.0966 55.2753 60.0592 55.35 60.0456 55.4213C60.0796 55.2583 60.083 55.3398 60.049 55.4009C60.0321 55.4383 60.0117 55.4722 59.9913 55.5062C59.9574 55.5673 59.92 55.625 59.8793 55.6793C59.791 55.8084 59.886 55.6861 59.8827 55.6793C59.8894 55.6861 59.7774 55.805 59.7672 55.8152C59.6382 55.9578 59.4989 56.0868 59.3495 56.2091C59.3258 56.2294 59.2035 56.3449 59.3258 56.2294C59.2884 56.2634 59.2375 56.294 59.1967 56.3245C59.0915 56.3992 58.9828 56.4705 58.8707 56.5385C58.6636 56.6641 58.4531 56.7795 58.2357 56.8848C58.0795 56.9629 57.8928 57.0138 57.7467 57.1089C57.8928 57.0139 57.8045 57.0852 57.7501 57.1055C57.7094 57.1225 57.6686 57.1395 57.6279 57.1531C57.5464 57.187 57.4615 57.2176 57.38 57.2481C57.1932 57.3195 57.0031 57.384 56.8129 57.4451C56.419 57.5741 56.0217 57.6862 55.6211 57.7847C55.1864 57.8933 54.7484 57.985 54.3104 58.0665C54.0896 58.1073 53.8655 58.1446 53.6414 58.1786C53.5464 58.1922 53.4513 58.2057 53.3562 58.2227C53.2985 58.2295 53.2407 58.2397 53.183 58.2465C53.3528 58.2227 53.1015 58.2567 53.0744 58.2601C52.0522 58.3891 51.02 58.4604 49.9911 58.4842C48.9248 58.508 47.8586 58.4808 46.7958 58.3959C46.2932 58.3551 45.794 58.3042 45.2983 58.2397C45.4647 58.2601 45.2711 58.2363 45.2473 58.2329C45.1896 58.2261 45.1319 58.2159 45.0742 58.2091C44.9791 58.1956 44.884 58.182 44.7889 58.165C44.5478 58.1276 44.3067 58.0835 44.069 58.0394C43.2303 57.8798 42.3949 57.676 41.5834 57.4043C41.4102 57.3466 41.237 57.2855 41.0639 57.221C40.9824 57.1904 40.8975 57.1564 40.816 57.1259C40.782 57.1123 40.7311 57.0817 40.6971 57.075C40.7922 57.1157 40.8092 57.1225 40.7413 57.0919C40.7141 57.0817 40.6903 57.0682 40.6632 57.058C40.2659 56.8848 39.8788 56.6845 39.512 56.4468C39.4204 56.389 39.3321 56.3245 39.2438 56.2634C39.0978 56.1615 39.349 56.3551 39.2166 56.243C39.1793 56.2125 39.1385 56.1785 39.1012 56.1479C38.945 56.0155 38.8091 55.8695 38.6733 55.7167C38.5714 55.6012 38.7582 55.8423 38.6733 55.7167C38.6529 55.6861 38.6326 55.659 38.6122 55.6318C38.5714 55.5707 38.5307 55.5062 38.4967 55.4382C38.4764 55.4043 38.4628 55.3669 38.4424 55.333C38.3779 55.2073 38.4967 55.4858 38.4492 55.3533C38.4186 55.2685 38.3915 55.187 38.3711 55.0987C38.3643 55.0647 38.3609 55.024 38.3507 54.99C38.9721 56.9187 38.3609 55.1156 38.3643 55.0953V55.0817C38.3677 55.0511 38.3643 55.0206 38.3643 54.9934V54.6267V49.1596V49.0238H37.006C37.0943 49.7097 37.3999 50.2904 37.8855 50.7828C38.1436 51.0442 38.4424 51.2717 38.7446 51.4789C38.9042 51.5841 39.0672 51.686 39.2336 51.7777C39.4475 51.8999 39.7056 52.0731 39.9568 52.0731C39.8414 52.0425 39.7293 52.012 39.6139 51.9814C40.8228 52.5655 42.1199 52.9458 43.434 53.2141C45.0503 53.5435 46.704 53.7166 48.351 53.7642C50.1031 53.8151 51.8451 53.7234 53.5803 53.4823C55.0099 53.2854 56.4361 52.9798 57.7876 52.4704C59.2816 51.9067 61.1425 51.0069 61.4617 49.2683C61.5262 48.9186 61.3666 48.5179 60.9863 48.433C60.6535 48.3583 60.2189 48.5349 60.151 48.9084C60.134 49.0035 60.117 49.0951 60.0899 49.1902C60.0763 49.2378 60.0593 49.2819 60.0457 49.3294C60.0186 49.4279 60.1068 49.2276 60.0389 49.343C60.0084 49.3974 59.9778 49.4551 59.9439 49.5094C59.9269 49.54 59.9065 49.5671 59.8861 49.5977C59.8725 49.6147 59.8624 49.6317 59.8488 49.6486C59.8182 49.6894 59.8318 49.6724 59.8895 49.5977C59.8929 49.6249 59.7639 49.7471 59.7435 49.7675C59.6892 49.8252 59.6348 49.8796 59.5771 49.9339C59.5228 49.9848 59.4685 50.0358 59.4107 50.0833C59.3768 50.1105 59.3462 50.1376 59.3123 50.1648C59.2036 50.2565 59.4107 50.0969 59.2817 50.1852C58.9625 50.4127 58.6297 50.613 58.28 50.7862C58.1611 50.8473 58.0389 50.9016 57.9166 50.9594C57.9064 50.9628 57.7231 51.0545 57.7163 51.0477C57.7197 51.0545 57.8657 50.9865 57.7366 51.0375C57.6891 51.0579 57.6382 51.0782 57.5906 51.0952C56.3003 51.6046 54.9352 51.9136 53.5667 52.1207C53.4683 52.1343 53.3698 52.1513 53.2713 52.1649C53.2306 52.1717 53.1932 52.1751 53.1525 52.1818C53.2951 52.1615 53.1592 52.1818 53.1219 52.1852C52.908 52.2124 52.694 52.2362 52.4835 52.2599C52.0625 52.3041 51.6448 52.338 51.2237 52.3652C50.4155 52.4161 49.604 52.4263 48.7958 52.4161C47.9571 52.4026 47.1217 52.3584 46.2864 52.2769C46.0962 52.2599 45.9061 52.2396 45.7193 52.2158C45.631 52.2056 45.5427 52.1954 45.4545 52.1818C45.4307 52.1784 45.2371 52.1513 45.4035 52.1751C45.3458 52.1683 45.2881 52.1581 45.2303 52.1513C44.8568 52.0969 44.4833 52.0358 44.1132 51.9645C43.3661 51.8219 42.6259 51.6453 41.9026 51.4212C41.58 51.3193 41.2608 51.2107 40.945 51.0884C40.8635 51.0579 40.782 51.0239 40.7039 50.99C40.5952 50.9458 40.7378 51.0001 40.748 51.0103C40.7141 50.9866 40.6699 50.9764 40.6326 50.9594C40.422 50.8677 40.1877 50.7115 39.9568 50.7115C40.0723 50.7421 40.1843 50.7726 40.2998 50.8032C40.001 50.6538 39.7089 50.4908 39.4305 50.3006C39.3694 50.2599 39.3116 50.2191 39.2539 50.1784C39.2267 50.158 39.1894 50.141 39.169 50.1173C39.2607 50.2225 39.1996 50.141 39.1656 50.1139C39.0604 50.0256 38.9585 49.9373 38.86 49.8388C38.8396 49.8184 38.6223 49.5706 38.6223 49.5706C38.6291 49.5672 38.7208 49.7098 38.6393 49.5875C38.6223 49.5638 38.6087 49.5434 38.5917 49.5196C38.5612 49.4755 38.534 49.4279 38.5069 49.3804C38.4763 49.3295 38.4525 49.2751 38.4254 49.2208C38.3846 49.1359 38.4491 49.2548 38.4491 49.2751C38.4491 49.2548 38.4288 49.2242 38.422 49.2038C38.405 49.1495 38.388 49.0952 38.3744 49.0408C38.3608 48.9865 38.3337 48.793 38.3642 49.0205C38.3201 48.6707 38.0824 48.3243 37.6851 48.3413C37.3183 48.3583 37.0059 48.6401 37.0059 49.0205V50.9628V54.1921C37.0059 54.4773 37.0025 54.7625 37.0059 55.0478C37.0161 55.6861 37.3591 56.2804 37.77 56.749C38.3642 57.4281 39.1962 57.8967 40.0145 58.2533C40.7887 58.5895 41.5866 58.8577 42.4016 59.0648C44.3541 59.564 46.3847 59.7915 48.3984 59.8458C50.524 59.9036 52.66 59.7609 54.7517 59.3704C55.6006 59.2108 56.4427 59.0071 57.2645 58.7354C58.0727 58.4672 58.8842 58.1378 59.6075 57.6862C60.3749 57.2074 61.1015 56.5622 61.3936 55.6759C61.4921 55.3805 61.4921 55.0987 61.4921 54.7931V51.8524V49.2309V49.0815C61.4921 48.725 61.1797 48.3854 60.8129 48.4024C60.4462 48.4227 60.1338 48.7046 60.1338 49.0849L60.1339 49.085Z",fill:"#1C1E21"}),i.createElement("path",{d:"M60.1349 61.0891C60.1349 61.1265 60.1349 61.1672 60.1315 61.2046C60.1179 61.3811 60.1552 61.0755 60.1383 61.1604C60.1213 61.2453 60.1043 61.3268 60.0771 61.4083C60.0636 61.4456 60.0466 61.483 60.0364 61.5204C60.1077 61.2894 60.0771 61.4287 60.0466 61.4898C60.0058 61.5713 59.9583 61.6528 59.9074 61.7275C59.8836 61.7648 59.8564 61.7988 59.8327 61.8328C59.7783 61.9109 59.8938 61.7547 59.8904 61.758C59.87 61.775 59.8564 61.8022 59.8394 61.8226C59.7715 61.9007 59.7036 61.9788 59.6289 62.0501C59.561 62.118 59.4897 62.1791 59.4184 62.2402C59.3776 62.2742 59.3369 62.3048 59.2961 62.3387C59.2316 62.393 59.4422 62.2301 59.3437 62.3014C59.3165 62.3217 59.2927 62.3387 59.2656 62.3591C58.8479 62.6613 58.3895 62.9058 57.9209 63.1163C57.8598 63.1435 57.7919 63.1672 57.7341 63.1978C57.9209 63.1027 57.7138 63.2046 57.6628 63.225C57.5134 63.2861 57.364 63.3404 57.2146 63.3947C56.9226 63.5 56.6306 63.5917 56.3351 63.68C55.6458 63.8803 54.9463 64.0399 54.24 64.1656C54.06 64.1995 53.8801 64.2301 53.6967 64.2573C53.605 64.2708 53.5133 64.2844 53.4217 64.298C53.3843 64.3048 53.3469 64.3082 53.3096 64.315C53.2722 64.3218 53.2349 64.3252 53.1975 64.332C53.2858 64.3218 53.2892 64.3184 53.2077 64.332C52.8206 64.3829 52.4369 64.4236 52.0498 64.4576C50.335 64.6104 48.6032 64.6206 46.8884 64.4882C46.4843 64.4576 46.0836 64.4202 45.6829 64.3727C45.5879 64.3625 45.4928 64.3489 45.3977 64.3387C45.3705 64.3354 45.1804 64.315 45.3502 64.332C45.3128 64.3286 45.2755 64.3218 45.2381 64.315C45.0344 64.2878 44.834 64.2539 44.6303 64.2199C43.9172 64.1011 43.2075 63.9516 42.5114 63.7649C41.9001 63.5985 41.3025 63.4015 40.715 63.1605C40.5792 63.1061 40.8305 63.2114 40.7422 63.1706C40.7116 63.1571 40.6777 63.1435 40.6471 63.1299C40.586 63.1027 40.5249 63.0756 40.4638 63.045C40.3347 62.9839 40.2057 62.9227 40.08 62.8548C39.8627 62.7394 39.6488 62.6171 39.4417 62.4813C39.35 62.4202 39.2617 62.3523 39.17 62.2878C39.0987 62.2368 39.3126 62.4032 39.2176 62.3251C39.1972 62.3081 39.1768 62.2912 39.1564 62.2742C39.1123 62.2368 39.0715 62.2029 39.0274 62.1655C38.878 62.0297 38.749 61.8803 38.6165 61.7309C38.7761 61.9108 38.6641 61.792 38.6301 61.7411C38.6063 61.7071 38.5826 61.6698 38.5622 61.6324C38.5147 61.5543 38.4807 61.4694 38.4366 61.3913C38.4332 61.3845 38.5079 61.5747 38.4773 61.4864C38.4705 61.4626 38.4603 61.4422 38.4535 61.4185C38.44 61.3811 38.4298 61.3438 38.4196 61.3064C38.4094 61.2691 38.3992 61.2317 38.3924 61.1943C38.389 61.1808 38.389 61.1638 38.3856 61.1502C38.3618 61.0449 38.4026 61.3573 38.3958 61.2283C38.3924 61.191 38.3924 61.1502 38.389 61.1129C38.3856 61.0348 38.3788 61.0008 38.3652 60.8989C38.3449 60.7563 38.3754 60.9872 38.3754 60.9872C38.3788 60.977 38.3754 60.9634 38.3754 60.9533C38.3788 60.8786 38.3754 60.8038 38.3754 60.7291V59.8225V55.1976V55.0957H37.0172C37.0715 55.5168 37.1869 55.9039 37.4145 56.2672C37.642 56.6238 37.934 56.943 38.2634 57.2078C38.9731 57.7783 39.7812 58.1756 40.6234 58.5015C42.2669 59.1366 44.0224 59.4591 45.7644 59.6594C47.8087 59.8937 49.8834 59.9175 51.9378 59.7443C53.7511 59.5881 55.5814 59.2995 57.3131 58.7188C58.1926 58.4234 59.0789 58.0601 59.8463 57.5337C60.2945 57.2247 60.6884 56.8783 61.011 56.4403C61.2929 56.06 61.4389 55.6321 61.5 55.1669H60.1418V57.1466V60.2639C60.135 60.5389 60.135 60.814 60.135 61.089C60.135 61.4455 60.4474 61.7851 60.8141 61.7682C61.1809 61.7512 61.4933 61.4693 61.4933 61.089V59.0482V55.9038V55.1704C61.4933 54.8138 61.1809 54.4742 60.8141 54.4912C60.4066 54.5082 60.1825 54.79 60.135 55.1704C60.1655 54.9293 60.1248 55.1873 60.1078 55.2451C60.0976 55.2824 60.0874 55.3164 60.0738 55.3537C60.0467 55.4352 59.9957 55.442 60.084 55.3367C60.0467 55.3809 60.0229 55.4624 59.9923 55.5133C59.9754 55.5405 59.8226 55.7408 59.8327 55.7578C59.8497 55.734 59.8667 55.7103 59.8871 55.6899C59.8701 55.7137 59.8497 55.734 59.8293 55.7578C59.809 55.7816 59.7886 55.802 59.7716 55.8257C59.7105 55.8936 59.6426 55.9616 59.5747 56.0261C59.5034 56.094 59.4287 56.1585 59.354 56.2196C59.3506 56.223 59.2147 56.3249 59.303 56.2604C59.3811 56.2026 59.2147 56.3215 59.2011 56.3317C58.8412 56.583 58.4541 56.7935 58.0568 56.9802C57.9923 57.0108 57.9311 57.038 57.8666 57.0651C57.8293 57.0821 57.7851 57.0957 57.7512 57.1161C57.8972 57.021 57.8055 57.0923 57.7546 57.1127C57.5882 57.1806 57.4218 57.2451 57.2554 57.3028C56.522 57.5677 55.7681 57.7714 55.0075 57.9344C54.5932 58.0227 54.1756 58.1008 53.7544 58.1687C53.6424 58.1857 53.5269 58.2027 53.4149 58.2197C53.3571 58.2265 53.2994 58.2366 53.2417 58.2434C53.1093 58.2604 53.2247 58.2468 53.2485 58.2434C52.9938 58.2774 52.7357 58.308 52.4811 58.3351C50.5319 58.5389 48.5591 58.5524 46.6066 58.3861C46.3859 58.3657 46.1618 58.3453 45.941 58.3215C45.8222 58.308 45.7033 58.2944 45.5845 58.2808C45.5267 58.274 45.4656 58.2672 45.4079 58.2604C45.3705 58.257 45.3298 58.2502 45.2924 58.2468C45.3671 58.257 45.3773 58.257 45.3264 58.2502C44.885 58.1857 44.4469 58.1178 44.0089 58.0329C43.2177 57.8801 42.4367 57.6899 41.6726 57.4387C41.4825 57.3775 41.2923 57.3096 41.1056 57.2383C41.0241 57.2078 40.9392 57.1738 40.8577 57.1432C40.8169 57.1263 40.7762 57.1093 40.7354 57.0957C40.6675 57.0685 40.6811 57.0753 40.7796 57.1127C40.7524 57.1025 40.7286 57.0889 40.7015 57.0787C40.2906 56.8988 39.8899 56.695 39.513 56.4505C39.4111 56.3826 39.2922 56.3181 39.2039 56.2332C39.3058 56.3317 39.2447 56.2672 39.2175 56.2434C39.1666 56.1993 39.1157 56.1585 39.0647 56.1144C39.0308 56.0838 38.6233 55.7137 38.6471 55.6831C38.6437 55.6865 38.7387 55.8121 38.6505 55.6831C38.6199 55.639 38.5893 55.5914 38.5588 55.5439C38.518 55.476 38.4807 55.4047 38.4433 55.3334C38.3788 55.2077 38.4976 55.4862 38.4501 55.3537C38.4331 55.3062 38.4161 55.2587 38.4026 55.2077C38.3856 55.1432 38.338 54.8512 38.3686 55.0957C38.3245 54.7459 38.0868 54.3995 37.6894 54.4165C37.3227 54.4335 37.0103 54.7153 37.0103 55.0957V59.3708C37.0103 59.9074 36.9899 60.4507 37.0103 60.9838C37.0137 61.0517 37.0137 60.9533 37.034 61.1706C37.0476 61.3098 37.0137 60.9974 37.0239 61.174C37.0307 61.3098 37.051 61.449 37.0782 61.5815C37.1223 61.792 37.2106 61.9991 37.3091 62.1893C37.5094 62.5798 37.8218 62.9261 38.1546 63.2046C39.0646 63.9687 40.1784 64.4508 41.299 64.8176C42.7625 65.2998 44.2906 65.568 45.8221 65.7412C47.5674 65.9382 49.3298 65.9789 51.082 65.8804C52.7085 65.7888 54.3453 65.5782 55.9276 65.1877C57.1636 64.8855 58.4201 64.4712 59.5169 63.8091C60.3216 63.3235 61.1162 62.6478 61.3981 61.7173C61.4592 61.5136 61.4898 61.2963 61.4898 61.0823C61.4932 60.7258 61.174 60.3862 60.8106 60.4031C60.4439 60.4235 60.1383 60.7054 60.1349 61.0891L60.1349 61.0891Z",fill:"#1C1E21"}),i.createElement("line",{x1:"49.4333",y1:12,x2:"49.4333",y2:"27.25",stroke:"#1C1E21",strokeWidth:"1.2"}),i.createElement("path",{d:"M49.3415 19.1168H60.6719C64.719 19.1168 67.9998 22.3976 67.9998 26.4448V26.4448C67.9998 30.4919 64.719 33.7727 60.6719 33.7727H15.9384C11.5632 33.7727 8.01636 37.3195 8.01636 41.6948V41.6948C8.01636 46.07 11.5632 49.6169 15.9384 49.6169H20.1371H32.2578",stroke:"#1C1E21",strokeWidth:"1.2"}),i.createElement("path",{d:"M28.6596 53.6034C28.5173 53.4611 28.5044 53.2386 28.6208 53.0817L28.6596 53.0368L32.1152 49.581L28.6596 46.1252C28.5173 45.983 28.5044 45.7604 28.6208 45.6036L28.6596 45.5587C28.8018 45.4164 29.0244 45.4035 29.1812 45.5199L29.2261 45.5587L32.9652 49.2977C33.1074 49.44 33.1204 49.6625 33.004 49.8194L32.9652 49.8643L29.2261 53.6034C29.0697 53.7598 28.816 53.7598 28.6596 53.6034Z",fill:"#1C1E21",stroke:"#1C1E21",strokeWidth:"0.3"})),L=()=>i.createElement("svg",{width:75,height:75,viewBox:"0 0 75 75",fill:"none",xmlns:"http://www.w3.org/2000/svg"},i.createElement("rect",{opacity:"0.5",width:75,height:75,rx:"8.57143",fill:"#CEEEF7"}),i.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M19.6497 24C16.3665 24 13.0905 24.2772 10.5745 24.8452C9.31669 25.1293 8.25152 25.4773 7.43936 25.9489C6.68289 26.3882 6.099 27.0074 6.02118 27.7684C6.01119 27.8541 6.0069 27.8832 6 27.9375V34.5995V34.6492V41.3213V41.3611V48.0724C6 48.9134 6.62722 49.5794 7.43962 50.0511C8.25203 50.5227 9.31701 50.8807 10.5747 51.1648C13.0905 51.7328 16.3666 52 19.65 52C22.9334 52 26.2092 51.7328 28.7253 51.1648C29.983 50.8807 31.0587 50.523 31.8711 50.0511C32.6835 49.5794 33.3 48.9134 33.3 48.0724V27.9372C33.3 27.8904 33.2929 27.8495 33.2895 27.808C33.2286 27.0283 32.6409 26.3956 31.8714 25.9485C31.0589 25.4769 29.9833 25.1289 28.7255 24.8448C26.2097 24.2768 22.9332 24 19.6497 24ZM19.6497 25.2726C22.8503 25.2726 26.0578 25.5481 28.4051 26.0781C29.5786 26.3431 30.5397 26.6806 31.1458 27.0327C31.7142 27.3627 31.9125 27.6426 31.9349 27.8878V27.9376C31.9349 28.1946 31.7518 28.4807 31.1458 28.8324C30.5398 29.1842 29.5788 29.522 28.4051 29.787C26.058 30.3171 22.8506 30.5925 19.6497 30.5925C16.4492 30.5925 13.2417 30.3171 10.8944 29.787C9.72088 29.522 8.75973 29.1845 8.15365 28.8324C7.54762 28.4807 7.36459 28.1945 7.36459 27.9376C7.36459 27.6805 7.54764 27.3847 8.15365 27.0327C8.75969 26.6809 9.72088 26.3431 10.8944 26.0781C13.2414 25.5481 16.4489 25.2726 19.6497 25.2726ZM7.36495 29.8665C7.39066 29.8818 7.41375 29.9011 7.4397 29.9162C8.2521 30.3878 9.31709 30.7359 10.5748 31.0199C13.0906 31.5879 16.3666 31.8651 19.6501 31.8651C22.9335 31.8651 26.2093 31.5879 28.7253 31.0199C29.9831 30.7359 31.0588 30.3878 31.8712 29.9162C31.8933 29.9033 31.9133 29.8896 31.9352 29.8765V34.6492C31.9352 34.9062 31.7522 35.1923 31.1461 35.5441C30.5401 35.8959 29.5792 36.2337 28.4054 36.4987C26.0584 37.0287 22.8509 37.3041 19.6501 37.3041C16.4495 37.3041 13.242 37.0287 10.8947 36.4987C9.72121 36.2337 8.76006 35.8961 8.15398 35.5441C7.54795 35.1923 7.36492 34.9062 7.36492 34.6492V34.5795L7.36495 29.8665ZM7.36495 36.5778C7.39066 36.5932 7.41375 36.6127 7.4397 36.6276C8.2521 37.0992 9.31709 37.4572 10.5748 37.7413C13.0906 38.3093 16.3666 38.5765 19.6501 38.5765C22.9335 38.5765 26.2093 38.3093 28.7253 37.7413C29.9831 37.4572 31.0588 37.0992 31.8712 36.6276C31.8933 36.6149 31.9133 36.6009 31.9352 36.5878V41.3606C31.9352 41.6176 31.7522 41.9134 31.1461 42.2654C30.5401 42.6172 29.5792 42.9451 28.4054 43.2101C26.0584 43.7401 22.8509 44.0155 19.6501 44.0155C16.4495 44.0155 13.242 43.7401 10.8947 43.2101C9.72121 42.945 8.76006 42.6172 8.15398 42.2654C7.54791 41.9137 7.36492 41.6176 7.36492 41.3606V41.3009L7.36495 36.5778ZM7.36495 43.2892C7.3909 43.3047 7.41328 43.3236 7.4397 43.3389C8.2521 43.8106 9.31709 44.1686 10.5748 44.4527C13.0906 45.0209 16.3666 45.2878 19.6501 45.2878C22.9335 45.2878 26.2093 45.0206 28.7253 44.4527C29.9831 44.1686 31.0588 43.8108 31.8712 43.3389C31.8938 43.3258 31.9128 43.3125 31.9352 43.2992V48.0719C31.9352 48.3289 31.7522 48.6248 31.1461 48.9768C30.5401 49.3286 29.5792 49.6564 28.4054 49.9214C26.0584 50.4514 22.8509 50.7269 19.6501 50.7269C16.4495 50.7269 13.242 50.4514 10.8947 49.9214C9.72121 49.6564 8.76006 49.3286 8.15398 48.9768C7.54791 48.6251 7.36492 48.329 7.36492 48.0719L7.36495 43.2892Z",fill:"black"}),i.createElement("path",{d:"M7.3502 34C7.3502 34 9.05748 37.0413 20.4263 37.2C20.4263 37.2 28.7835 37.7761 31.9501 34V43.6C31.9501 43.6 29.684 45.8122 20.837 45.9709C20.837 45.9709 11.7246 46.608 7.3501 43.3L7.3502 34Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M44.3448 43.4082C44.2269 43.2903 44.2162 43.1058 44.3127 42.9758L44.3448 42.9386L47.2093 40.0739L44.3448 37.2092C44.2269 37.0913 44.2162 36.9068 44.3127 36.7768L44.3448 36.7396C44.4627 36.6217 44.6472 36.611 44.7772 36.7074L44.8145 36.7396L47.914 39.8391C48.0319 39.957 48.0426 40.1415 47.9461 40.2715L47.914 40.3087L44.8145 43.4082C44.6848 43.5379 44.4745 43.5379 44.3448 43.4082Z",fill:"#0DB1F9",stroke:"#0DB1F9",strokeWidth:"0.28209"}),i.createElement("path",{d:"M68.963 33.554C68.963 33.5504 68.9594 33.5429 68.9594 33.5393C68.9372 33.4803 68.9003 33.4284 68.8596 33.3805C68.856 33.3768 68.856 33.373 68.8521 33.3694L62.1221 26.1514C62.0187 26.0405 61.8706 25.974 61.7154 25.974L51.6277 25.9737C50.6958 25.9737 49.9341 26.7317 49.9341 27.6673V49.059C49.9341 49.9909 50.6921 50.7526 51.6277 50.7526H67.3063C68.2382 50.7526 68.9999 49.9945 68.9999 49.059L68.9996 33.7501C68.9996 33.6797 68.9849 33.617 68.9627 33.5541L68.963 33.554ZM62.2515 27.9186L67.1696 33.1916H62.8357C62.5141 33.1916 62.2515 32.929 62.2515 32.6074L62.2515 27.9186ZM67.8905 49.0587C67.8905 49.3804 67.6279 49.643 67.3063 49.643H51.6277C51.306 49.643 51.0434 49.3804 51.0434 49.0587V27.6703C51.0434 27.3487 51.306 27.0861 51.6277 27.0861H61.1422V32.6106C61.1422 33.5425 61.9003 34.3042 62.8358 34.3042H67.8906L67.8905 49.0587Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M54.8594 37.9841C54.8594 38.2911 55.1073 38.5387 55.4139 38.5387H63.5161C63.823 38.5387 64.0706 38.2908 64.0706 37.9841C64.0706 37.6772 63.8227 37.4296 63.5161 37.4296H55.4146C55.1113 37.4296 54.8598 37.6774 54.8598 37.9841H54.8594Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M63.5201 40.0476H55.4147C55.1077 40.0476 54.8601 40.2955 54.8601 40.6022C54.8601 40.9091 55.108 41.1567 55.4147 41.1567H63.5168C63.8237 41.1567 64.0713 40.9089 64.0713 40.6022C64.0713 40.2952 63.8237 40.0476 63.5204 40.0476H63.5201Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M63.5201 42.6658H55.4147C55.1077 42.6658 54.8601 42.9136 54.8601 43.2203C54.8601 43.5273 55.108 43.7749 55.4147 43.7749H63.5168C63.8237 43.7749 64.0713 43.527 64.0713 43.2203C64.0713 42.9134 63.8237 42.6658 63.5204 42.6658H63.5201Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M59.4669 45.2839H55.4142C55.1072 45.2839 54.8596 45.5318 54.8596 45.8385C54.8596 46.1454 55.1075 46.3931 55.4142 46.3931H59.4669C59.7738 46.3931 60.0215 46.1452 60.0215 45.8385C60.0215 45.5316 59.7702 45.2839 59.4669 45.2839Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M25.8948 40.0658H47.4474",stroke:"#0DB1F9",strokeWidth:"1.12836"})),R=()=>i.createElement("svg",{width:75,height:75,viewBox:"0 0 75 75",fill:"none",xmlns:"http://www.w3.org/2000/svg"},i.createElement("rect",{opacity:"0.5",width:75,height:75,rx:8,fill:"#CEEEF7"}),i.createElement("path",{d:"M17.5 52L25 45.5",stroke:"#0DB1F9",strokeWidth:"1.5"}),i.createElement("path",{d:"M57 52.5L49.5 45.5",stroke:"#0DB1F9",strokeWidth:"1.5"}),i.createElement("path",{d:"M45.5 56.5L45 46.5",stroke:"#0DB1F9",strokeWidth:"1.5"}),i.createElement("path",{d:"M27.9999 56.5001L30.5 46.5",stroke:"#0DB1F9",strokeWidth:"1.5"}),i.createElement("g",{clipPath:"url(#clip0_707_7507)"},i.createElement("path",{d:"M47.0388 11H35.4142C35.2896 11.0506 35.1804 11.1325 35.097 11.2379L23.9164 22.3392C23.8213 22.4415 23.7556 22.5675 23.726 22.704V22.7991C23.7241 22.8308 23.7241 22.8626 23.726 22.8943L23.6785 43.7172C23.6872 45.5266 25.1519 46.9913 26.9613 47H47.0388C48.8482 46.9913 50.3129 45.5266 50.3216 43.7172V14.2828C50.3216 12.4698 48.8519 11 47.0388 11ZM30.6565 17.8987L34.875 13.7119V20.4678C34.8663 21.4076 34.102 22.1648 33.1622 22.1648H26.3904L30.6565 17.8987ZM48.7357 14.2828V43.7172C48.7271 44.6508 47.9724 45.4055 47.0388 45.4141H26.9772C26.0374 45.4141 25.2731 44.657 25.2644 43.7172V23.7507H33.194C35.0034 23.742 36.4681 22.2773 36.4768 20.4678V12.5383H47.0547C47.9918 12.5467 48.7447 13.3132 48.7363 14.2503C48.7363 14.2612 48.736 14.272 48.7357 14.2828Z",fill:"#1C1E21"}),i.createElement("path",{d:"M31.164 27.4141H29.5781C29.1402 27.4141 28.7852 27.7691 28.7852 28.2071V29.7929C28.7852 30.2309 29.1402 30.5859 29.5781 30.5859H31.164C31.6019 30.5859 31.957 30.2309 31.957 29.7929V28.2071C31.9569 27.7691 31.6019 27.4141 31.164 27.4141Z",fill:"#1C1E21"}),i.createElement("path",{d:"M45.2149 28.1912H33.511V29.777H45.2149V28.1912Z",fill:"#1C1E21"}),i.createElement("path",{d:"M31.164 33.2661H29.5781C29.1402 33.2661 28.7852 33.6211 28.7852 34.059V35.6766C28.7852 36.1146 29.1402 36.4696 29.5781 36.4696H31.164C31.6019 36.4696 31.957 36.1146 31.957 35.6766V34.059C31.9569 33.6211 31.6019 33.2661 31.164 33.2661Z",fill:"#1C1E21"}),i.createElement("path",{d:"M45.2149 34.0749H33.511V35.6607H45.2149V34.0749Z",fill:"#1C1E21"}),i.createElement("path",{d:"M31.164 39.1498H29.5781C29.1402 39.1498 28.7852 39.5048 28.7852 39.9428V41.5286C28.7852 41.9666 29.1402 42.3216 29.5781 42.3216H31.164C31.6019 42.3216 31.957 41.9666 31.957 41.5286V39.9428C31.9569 39.5048 31.6019 39.1498 31.164 39.1498Z",fill:"#1C1E21"}),i.createElement("path",{d:"M45.2149 39.9268H33.511V41.5127H45.2149V39.9268Z",fill:"#1C1E21"})),i.createElement("path",{d:"M12.9998 50C11.144 50.0025 9.365 50.7408 8.05298 52.053C6.74097 53.3652 6.00253 55.1442 6 56.9998C6.00033 58.8563 6.73782 60.6366 8.05056 61.9492C9.3632 63.2619 11.1434 63.9994 12.9999 63.9997C13.1312 63.9997 13.2624 63.9954 13.3937 63.9867V63.9866C15.1783 63.8844 16.8564 63.104 18.0846 61.8052C19.3126 60.5063 19.9978 58.7872 20 56.9999C19.9975 55.1441 19.2592 53.3651 17.947 52.053C16.6348 50.741 14.8558 50.0026 13.0002 50.0001L12.9998 50ZM17.9087 60.2025C16.5593 58.9939 14.8116 58.3256 12.9998 58.3256C11.188 58.3256 9.44065 58.9939 8.09095 60.2025C7.46265 59.2528 7.12934 58.1386 7.13286 56.9999C7.13286 55.444 7.75104 53.9518 8.85122 52.8516C9.95148 51.7514 11.4437 51.1332 12.9996 51.1332C14.5554 51.1332 16.0477 51.7514 17.148 52.8516C18.2481 53.9518 18.8663 55.4441 18.8663 56.9999C18.8698 58.1385 18.5365 59.2528 17.9082 60.2025H17.9087Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M15.4585 54.9176C15.4823 55.5947 15.237 56.2537 14.7763 56.7505C14.3155 57.2473 13.6768 57.5415 12.9998 57.5688C12.3229 57.5415 11.6841 57.2473 11.2234 56.7505C10.7627 56.2537 10.5174 55.5947 10.5411 54.9176C10.5175 54.2408 10.7629 53.5823 11.2238 53.0863C11.6846 52.5902 12.3232 52.297 12.9998 52.2707C13.6765 52.2969 14.3151 52.5902 14.7758 53.0863C15.2367 53.5823 15.4821 54.2408 15.4585 54.9176H15.4585Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M28.9998 56C27.144 56.0025 25.365 56.7408 24.053 58.053C22.741 59.3652 22.0025 61.1442 22 62.9998C22.0003 64.8563 22.7378 66.6366 24.0506 67.9492C25.3632 69.2619 27.1434 69.9994 28.9999 69.9997C29.1312 69.9997 29.2624 69.9954 29.3937 69.9867V69.9866C31.1783 69.8844 32.8564 69.104 34.0846 67.8052C35.3126 66.5063 35.9978 64.7872 36 62.9999C35.9975 61.1441 35.2592 59.3651 33.947 58.053C32.6348 56.741 30.8558 56.0026 29.0002 56.0001L28.9998 56ZM33.9087 66.2025C32.5593 64.9939 30.8116 64.3256 28.9998 64.3256C27.188 64.3256 25.4407 64.9939 24.091 66.2025C23.4626 65.2528 23.1293 64.1386 23.1329 62.9999C23.1329 61.444 23.751 59.9518 24.8512 58.8516C25.9515 57.7514 27.4437 57.1332 28.9996 57.1332C30.5554 57.1332 32.0477 57.7514 33.148 58.8516C34.2481 59.9518 34.8663 61.4441 34.8663 62.9999C34.8698 64.1385 34.5365 65.2528 33.9082 66.2025H33.9087Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M31.4585 60.9176C31.4823 61.5947 31.237 62.2537 30.7763 62.7505C30.3155 63.2473 29.6768 63.5415 28.9998 63.5688C28.3229 63.5415 27.6841 63.2473 27.2234 62.7505C26.7627 62.2537 26.5174 61.5947 26.5411 60.9176C26.5175 60.2408 26.7629 59.5823 27.2238 59.0863C27.6846 58.5902 28.3232 58.297 28.9998 58.2707C29.6765 58.2969 30.3151 58.5902 30.7758 59.0863C31.2367 59.5823 31.4821 60.2408 31.4585 60.9176H31.4585Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M45.9998 56C44.144 56.0025 42.365 56.7408 41.053 58.053C39.741 59.3652 39.0025 61.1442 39 62.9998C39.0003 64.8563 39.7378 66.6366 41.0506 67.9492C42.3632 69.2619 44.1434 69.9994 45.9999 69.9997C46.1312 69.9997 46.2624 69.9954 46.3937 69.9867V69.9866C48.1783 69.8844 49.8564 69.104 51.0846 67.8052C52.3126 66.5063 52.9978 64.7872 53 62.9999C52.9975 61.1441 52.2592 59.3651 50.947 58.053C49.6348 56.741 47.8558 56.0026 46.0002 56.0001L45.9998 56ZM50.9087 66.2025C49.5593 64.9939 47.8116 64.3256 45.9998 64.3256C44.188 64.3256 42.4407 64.9939 41.091 66.2025C40.4626 65.2528 40.1293 64.1386 40.1329 62.9999C40.1329 61.444 40.751 59.9518 41.8512 58.8516C42.9515 57.7514 44.4437 57.1332 45.9996 57.1332C47.5554 57.1332 49.0477 57.7514 50.148 58.8516C51.2481 59.9518 51.8663 61.4441 51.8663 62.9999C51.8698 64.1385 51.5365 65.2528 50.9082 66.2025H50.9087Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M48.4585 60.9176C48.4823 61.5947 48.237 62.2537 47.7763 62.7505C47.3155 63.2473 46.6768 63.5415 45.9998 63.5688C45.3229 63.5415 44.6841 63.2473 44.2234 62.7505C43.7627 62.2537 43.5174 61.5947 43.5411 60.9176C43.5175 60.2408 43.7629 59.5823 44.2238 59.0863C44.6846 58.5902 45.3232 58.297 45.9998 58.2707C46.6765 58.2969 47.3151 58.5902 47.7758 59.0863C48.2367 59.5823 48.4821 60.2408 48.4585 60.9176H48.4585Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M61.9998 50C60.144 50.0025 58.365 50.7408 57.053 52.053C55.741 53.3652 55.0025 55.1442 55 56.9998C55.0003 58.8563 55.7378 60.6366 57.0506 61.9492C58.3632 63.2619 60.1434 63.9994 61.9999 63.9997C62.1312 63.9997 62.2624 63.9954 62.3937 63.9867V63.9866C64.1783 63.8844 65.8564 63.104 67.0846 61.8052C68.3126 60.5063 68.9978 58.7872 69 56.9999C68.9975 55.1441 68.2592 53.3651 66.947 52.053C65.6348 50.741 63.8558 50.0026 62.0002 50.0001L61.9998 50ZM66.9087 60.2025C65.5593 58.9939 63.8116 58.3256 61.9998 58.3256C60.188 58.3256 58.4407 58.9939 57.091 60.2025C56.4626 59.2528 56.1293 58.1386 56.1329 56.9999C56.1329 55.444 56.751 53.9518 57.8512 52.8516C58.9515 51.7514 60.4437 51.1332 61.9996 51.1332C63.5554 51.1332 65.0477 51.7514 66.148 52.8516C67.2481 53.9518 67.8663 55.4441 67.8663 56.9999C67.8698 58.1385 67.5365 59.2528 66.9082 60.2025H66.9087Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M64.4585 54.9176C64.4823 55.5947 64.237 56.2537 63.7763 56.7505C63.3155 57.2473 62.6768 57.5415 61.9998 57.5688C61.3229 57.5415 60.6841 57.2473 60.2234 56.7505C59.7627 56.2537 59.5174 55.5947 59.5411 54.9176C59.5175 54.2408 59.7629 53.5823 60.2238 53.0863C60.6846 52.5902 61.3232 52.297 61.9998 52.2707C62.6765 52.2969 63.3151 52.5902 63.7758 53.0863C64.2367 53.5823 64.4821 54.2408 64.4585 54.9176H64.4585Z",fill:"#0DB1F9"}),i.createElement("defs",null,i.createElement("clipPath",{id:"clip0_707_7507"},i.createElement("rect",{width:36,height:36,fill:"white",transform:"translate(19 11)"})))),B=()=>i.createElement("svg",{width:70,height:70,viewBox:"0 0 70 70",fill:"none",xmlns:"http://www.w3.org/2000/svg"},i.createElement("rect",{opacity:"0.5",width:70,height:70,rx:8,fill:"#CEEEF7"}),i.createElement("path",{d:"M57.7275 35.0615C57.1595 35.0615 56.6949 35.5261 56.6949 36.0941C56.6949 48.0729 46.9363 57.8832 34.9058 57.8832C22.8752 57.8832 13.1166 48.1246 13.1166 36.1457C13.1166 24.6315 22.049 15.2343 33.3568 14.4598L32.4276 15.1826C31.9629 15.5442 31.8596 16.2152 32.2212 16.6798C32.4275 16.938 32.7377 17.0929 33.0474 17.0929C33.2538 17.0929 33.512 17.0414 33.6669 16.8865L37.1777 14.2015C37.436 13.9951 37.5909 13.685 37.5909 13.3238C37.5909 12.9622 37.3845 12.6525 37.1262 12.4976L33.6154 10.1742C33.1508 9.86446 32.4794 9.96782 32.1696 10.4839C31.8599 10.9486 31.9633 11.6199 32.4794 11.9297L33.2022 12.3943C20.8103 13.2205 11 23.547 11 36.1455C11 49.312 21.6881 60 34.8545 60C48.0209 60.0004 58.7606 49.312 58.7606 36.1455C58.7606 35.5775 58.296 35.0614 57.728 35.0614L57.7275 35.0615Z",fill:"#1C1E21"}),i.createElement("path",{d:"M58.1922 30.621C58.1922 31.3338 57.6145 31.9119 56.9014 31.9119C56.1886 31.9119 55.6105 31.3338 55.6105 30.621C55.6105 29.9079 56.1886 29.3302 56.9014 29.3302C57.6145 29.3302 58.1922 29.9079 58.1922 30.621Z",fill:"#1C1E21"}),i.createElement("path",{d:"M56.5399 25.8191C56.5399 26.5323 55.9618 27.11 55.2491 27.11C54.5363 27.11 53.9583 26.5323 53.9583 25.8191C53.9583 25.1064 54.5363 24.5283 55.2491 24.5283C55.9618 24.5283 56.5399 25.1064 56.5399 25.8191Z",fill:"#1C1E21"}),i.createElement("path",{d:"M53.4419 21.3271C53.4419 22.0398 52.8642 22.6179 52.1511 22.6179C51.4383 22.6179 50.8602 22.0398 50.8602 21.3271C50.8602 20.614 51.4383 20.0363 52.1511 20.0363C52.8642 20.0363 53.4419 20.614 53.4419 21.3271Z",fill:"#1C1E21"}),i.createElement("path",{d:"M49.1564 17.4546C49.1564 18.1674 48.5787 18.7454 47.8656 18.7454C47.1528 18.7454 46.5748 18.1674 46.5748 17.4546C46.5748 16.7415 47.1528 16.1638 47.8656 16.1638C48.5787 16.1638 49.1564 16.7415 49.1564 17.4546Z",fill:"#1C1E21"}),i.createElement("path",{d:"M44.0447 14.7696C44.0447 15.4827 43.4667 16.0604 42.7539 16.0604C42.0408 16.0604 41.4631 15.4827 41.4631 14.7696C41.4631 14.0569 42.0408 13.4788 42.7539 13.4788C43.4667 13.4788 44.0447 14.0569 44.0447 14.7696Z",fill:"#1C1E21"}),i.createElement("path",{d:"M44.0448 30.9825C44.0448 30.4145 43.5802 29.898 42.9603 29.898L28.9677 29.8984L31.343 27.523C31.7561 27.1099 31.7561 26.4386 31.343 26.0259C31.1367 25.8195 30.8784 25.7161 30.5687 25.7161C30.3104 25.7161 30.0007 25.8195 29.7943 26.0259L25.6121 30.2081C25.199 30.6212 25.199 31.2925 25.6121 31.7052L29.7943 35.9393C30.0007 36.1457 30.2589 36.2491 30.5687 36.2491C30.8784 36.2491 31.1367 36.1457 31.343 35.9393C31.5494 35.733 31.6528 35.4747 31.6528 35.165C31.6528 34.8552 31.5494 34.597 31.343 34.3906L28.9677 31.9639H42.9603C43.5279 32.0669 44.0444 31.5504 44.0444 30.9824L44.0448 30.9825Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M39.0879 47.1436C39.3976 47.1436 39.6559 47.0403 39.8622 46.8339L44.0444 42.5998C44.4575 42.1867 44.4575 41.5153 44.0444 41.1026L39.8622 36.9204C39.4491 36.5073 38.7778 36.5073 38.3651 36.9204C37.952 37.3335 37.952 38.0049 38.3651 38.4176L40.7404 40.7929H26.6962C26.1282 40.7929 25.6117 41.2575 25.6117 41.8774C25.6117 42.4454 26.0763 42.9619 26.6962 42.9619H40.6887L38.3134 45.3886C38.1071 45.595 38.0037 45.8532 38.0037 46.163C38.0037 46.4727 38.1071 46.731 38.3134 46.9373C38.5198 47.0403 38.7781 47.1437 39.0878 47.1437L39.0879 47.1436Z",fill:"#0DB1F9"})),z=()=>i.createElement("svg",{width:75,height:75,viewBox:"0 0 75 75",fill:"none",xmlns:"http://www.w3.org/2000/svg"},i.createElement("rect",{opacity:"0.5",width:75,height:75,rx:"8.57143",fill:"#CEEEF7"}),i.createElement("g",{clipPath:"url(#clip0_687_7655)"},i.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M64.2412 58.0051C63.9174 57.7503 63.6289 56.849 63.1383 56.6616C62.6841 56.6236 62.5176 56.4808 61.9205 56.6024C62.1371 56.4939 62.3415 56.3551 62.5718 56.2807C62.7277 56.2439 62.874 56.2847 63.0228 56.3052C63.0595 56.2879 63.0836 56.2663 63.0746 56.2336C62.4735 55.8622 61.2404 55.8569 60.408 55.6448C61.3741 55.6771 62.443 55.6236 63.1691 55.8564C63.7529 56.3746 63.9426 57.3503 64.2412 58.0051L64.2412 58.0051Z",fill:"#1C1E21"}),i.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M66.0118 53.6586C66.1459 53.6707 66.8672 54.8117 66.9011 55.0015C66.961 55.4166 67.109 55.8818 67.1487 56.316C67.0299 55.8996 66.9005 55.4831 66.7387 55.0667C66.6913 54.952 66.5717 54.6927 66.2392 54.326C66.0844 54.0675 66.051 53.8644 66.0119 53.6586L66.0118 53.6586Z",fill:"#1C1E21"}),i.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M68.4557 59.873H68.1471L68.4882 59.9625L68.4557 59.873Z",fill:"#1C1E21"}),i.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M66.3501 59.5648C66.0398 59.5267 65.7214 59.5216 65.4111 59.5898C65.2846 59.7597 65.2759 59.9325 65.2206 60.0922C65.5978 59.6765 65.7475 59.651 66.3501 59.5648V59.5648Z",fill:"#1C1E21"}),i.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M70.5829 61.7106C70.4102 61.8485 70.393 61.9782 69.8824 62.1337C69.51 62.2217 69.2997 62.0725 69.1244 61.8718C69.3901 61.9479 69.4326 62.1487 70.1451 61.9236L70.5829 61.7106Z",fill:"#1C1E21"}),i.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M69.0351 61.7969C68.9108 62.1386 68.7769 62.484 68.6418 62.8041C68.3201 63.1813 68.4572 62.9663 67.9757 63.5523C68.1297 63.3146 68.3301 63.0861 68.435 62.8386C68.5087 62.6756 68.6005 62.5026 68.6411 62.3578C68.5134 62.2941 68.2521 62.289 68.2166 62.3119C67.7885 62.5644 67.7112 62.8316 67.4589 63.0918C67.6418 62.7994 67.798 62.4802 68.0101 62.217C68.0343 62.1828 68.1913 62.1762 68.2878 62.1561C68.1378 62.1303 67.8602 62.0764 67.8378 62.0789C67.5585 62.146 67.402 62.3893 67.1947 62.5508C67.3459 62.3326 67.4851 62.1053 67.677 61.9177C67.696 61.8957 68.2149 61.9411 68.4837 62.0484L68.4891 62.1554L68.6417 62.217L68.7566 61.9292L69.0351 61.7969Z",fill:"#1C1E21"}),i.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M65.6472 63.8573C65.5783 63.8337 64.9696 63.7555 64.9696 63.7738C64.7359 63.9402 64.7005 64.0993 64.7055 64.2371C65.0943 63.8546 65.0798 63.8536 65.6472 63.8573L65.6472 63.8573Z",fill:"#1C1E21"}),i.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M65.1851 61.1061C65.1851 61.1061 65.6243 61.0421 65.6157 61.0831C65.5839 61.2355 65.6695 61.6068 65.6789 61.555L65.432 61.7565C65.9491 61.4965 66.4935 61.5487 67.0456 61.509C67.0456 61.509 66.5807 61.3806 66.6092 61.3766C66.7128 61.3619 66.5815 60.8918 66.546 60.8816C66.6344 60.8517 66.7208 60.8292 66.8101 60.8068C66.1454 60.6329 65.761 60.7354 65.1851 61.1061L65.1851 61.1061Z",fill:"#1C1E21"}),i.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M68.9863 61.434L68.8887 61.483C68.8748 61.197 68.5677 61.218 68.3087 61.2153L68.0962 61.1636C68.1608 61.2233 68.3606 61.2187 68.28 61.3477C68.2122 61.384 68.181 61.5394 68.1422 61.67L68.0273 61.6758C68.2302 61.7551 68.4184 61.8246 68.6418 61.9175L68.7738 61.9376L69.0897 61.7679L68.9863 61.434Z",fill:"#1C1E21"}),i.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M66.0754 61.2417C66.0259 61.1439 65.7772 61.1578 65.7236 61.221C65.6801 61.2725 65.7209 61.6023 65.7768 61.5467C65.8575 61.4757 65.963 61.446 66.0897 61.4517C66.1057 61.3908 66.1001 61.325 66.0754 61.2417L66.0754 61.2417Z",fill:"white"}),i.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M68.4605 61.7336C68.4838 61.7165 68.5333 61.5511 68.5173 61.5016C68.4846 61.3848 68.2534 61.4094 68.2534 61.4094C68.1969 61.4426 68.1475 61.6247 68.1681 61.6847C68.1779 61.7356 68.4233 61.7518 68.4604 61.7336L68.4605 61.7336Z",fill:"white"}),i.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M58.5706 67.44L58.7167 67.4543C59.5827 67.8795 61.0088 68.51 63.0733 68.1712L63.3972 68.6809C62.488 69.1227 61.5101 69.0211 60.5937 69.0521L58.5706 67.44Z",fill:"#1C1E21"}),i.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M57.3271 64.0788L57.761 65.9001C58.9225 66.6262 60.9066 67.4578 62.5605 67.3907L62.195 66.5521C58.7427 65.8189 58.4507 64.8922 57.3272 64.0786L57.3271 64.0788Z",fill:"#1C1E21"}),i.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M58.4069 60.3477C58.6189 61.7591 58.902 62.8291 60.1292 63.7254C60.8815 64.2664 61.6281 64.8051 62.5363 65.2497C62.5363 65.2497 62.4567 65.664 62.3901 65.6566C59.6874 65.3564 57.7087 63.1153 57.469 62.0917C57.7011 61.3065 58.0342 60.8161 58.4068 60.3477H58.4069Z",fill:"#1C1E21"}),i.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M59.7056 59.5482C60.0658 61.1125 60.9245 62.6968 61.7781 64.1341C62.091 64.5311 62.1729 64.7063 62.6552 64.9481C63.307 65.1545 63.7679 65.1018 64.2411 65.0707C64.1114 64.8308 63.9962 64.58 63.8473 64.3545C62.7879 63.5166 63.2775 62.7538 63.5589 62.2708C62.9698 62.122 62.1893 61.8056 62.0818 61.3895C61.9089 60.0436 61.9967 59.6166 62.1697 58.9253C61.372 59.0935 60.5636 59.2602 59.7056 59.5482L59.7056 59.5482Z",fill:"#1C1E21"}),i.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M56.7794 51.2393C56.5173 51.4106 56.2647 51.6555 55.9929 51.9338C55.5424 52.4001 55.243 52.855 54.7844 53.1923C54.6927 53.2713 54.4353 53.3987 54.1118 53.4176C53.9594 53.4252 53.8601 53.4504 53.58 53.4152C53.333 53.2815 53.0983 53.3685 52.8693 53.6424C52.6182 54.0035 52.3002 54.6893 52.1679 55.121C51.8953 56.2219 52.5903 57.129 53.2567 57.828C53.8495 58.4072 54.1913 58.7691 54.4347 59.3001C54.619 59.6444 54.7555 60.117 54.9033 60.4484C54.9566 60.5534 54.9456 60.5529 55.089 60.5845C55.3988 60.6507 55.8285 60.6512 56.2111 60.6841C56.3766 60.6864 56.6039 60.6802 56.8193 60.6578C57.1161 60.6017 57.4647 60.5468 57.757 60.4457C58.0425 60.3754 58.297 60.278 58.5263 60.1884C58.4968 60.2888 58.2783 60.3854 58.1963 60.5126C57.4189 61.6255 57.206 62.6158 57.3413 64.1783C57.4179 64.8708 57.5768 65.4449 57.8166 66.1018C57.9264 66.4024 58.1599 66.8784 58.3837 67.1947C59.0491 68.135 60.5379 69.3425 62.6104 69.8073C62.9719 69.8567 63.3823 69.8303 63.7682 69.7276C64.7911 69.4175 66.8758 68.6842 66.8758 68.6842C66.8758 68.6842 65.0253 68.8373 64.1778 68.7535C63.9816 68.7228 63.7654 68.7124 63.6287 68.5787C63.6058 68.5491 63.5225 68.3813 63.5763 68.3798C63.6497 68.3778 63.8603 68.2903 64.1856 68.2613C63.4912 68.1866 63.5011 68.1874 63.4504 68.061C63.3716 67.8646 63.2509 67.5962 63.1267 67.3614C63.4318 67.3871 64.1074 67.4251 64.3367 67.1884C64.3367 67.1884 63.9369 67.2346 63.5836 67.1931C63.4646 67.1792 63.2618 67.1068 63.2008 67.0844C63.0422 67.0227 62.9116 67.0035 62.8812 66.9632C62.8261 66.823 62.7888 66.7759 62.7223 66.5881C62.6323 66.3394 62.6252 66.0621 62.608 65.8082C62.8391 66.0931 63.1336 66.3329 63.5235 66.4553C63.5285 66.4448 64.0292 66.673 64.3929 66.5557L64.4683 66.5313C64.4683 66.5381 64.2274 66.5525 64.139 66.5087C63.3994 66.1943 63.2881 65.9192 63.1686 65.7825L62.8493 65.3041C62.9464 65.1105 63.0013 65.1031 63.13 65.1013C63.5193 65.1446 63.6887 65.1778 63.9255 65.1207C64.0856 65.4501 64.1288 65.7814 64.4932 66.0211C65.7071 66.3815 65.9709 65.9141 66.2649 65.5012C67.1383 66.1422 68.5603 66.3399 69.5478 65.5123C70.8032 64.0596 71.2007 61.7695 71.0893 61.6388C70.9325 61.3728 70.7268 61.0985 70.5502 61.1364C69.9206 61.3082 69.6884 61.6276 69.0646 61.5548C69.1389 61.5506 69.2638 61.5486 69.2648 61.5415C69.3129 61.0132 69.2609 60.7576 69.2377 60.7138C69.0484 60.2963 68.7998 59.8569 68.6266 59.5276C68.5819 59.4593 68.4507 58.9405 68.2309 58.7355C68.1366 58.655 67.9058 58.4427 67.9058 58.4427L67.8863 58.6684C67.8863 58.6684 67.9765 58.682 68.0108 58.8218C68.141 59.3516 68.8037 60.6045 68.8524 60.6646C69.0873 61.0438 68.8763 61.5162 69.0548 61.7812C69.072 61.8149 69.4192 61.7782 69.6883 61.7948C70.1308 61.6996 70.1121 61.5046 70.4943 61.4753C70.7518 61.4555 70.7776 61.9391 70.7742 61.9766C70.7263 62.5106 70.5638 63.1314 70.3153 63.6983C69.7984 64.694 69.2197 65.5883 68.3988 65.713C67.3975 65.888 66.8772 65.4502 66.3345 65.1715L66.1268 65.3486C65.4214 66.051 64.5839 65.9886 64.2413 65.0708C64.0708 64.7148 63.8485 64.4966 63.6583 64.1912L62.6492 64.918C62.5652 65.0902 62.4619 65.3619 62.3364 65.6653C62.2492 65.8763 62.1761 66.2386 62.1808 66.5397C62.0515 66.7601 62.6288 67.6537 62.9937 68.1973C63.1013 68.3577 63.3023 68.6379 63.309 68.6554C63.3814 68.843 63.5398 69.0087 63.5494 69.0289C64.2267 69.9123 62.695 69.7089 62.3787 69.6555C61.7546 69.5561 61.1441 69.2954 60.5703 68.9468C60.537 68.9265 60.5038 68.906 60.4708 68.8852C59.7889 68.4558 59.1614 67.9065 58.628 67.3631C58.3098 67.0029 58.007 66.2646 57.7832 65.7412C57.4479 64.4827 56.9668 62.3014 58.2702 60.6484C58.3529 60.5539 58.442 60.3966 58.5179 60.3655C58.9052 60.1036 59.3257 59.925 59.7779 59.8259L59.7323 59.5327C59.5064 59.5841 58.746 59.8622 58.531 59.9786C58.042 60.1203 57.6365 60.2684 57.0133 60.4244C56.8107 60.4492 56.6115 60.4496 56.4172 60.4232C55.9689 60.3623 55.2048 60.4167 55.1561 60.3714C54.8526 59.9481 54.7695 59.1897 54.4925 58.7838L54.4892 58.7798L54.4857 58.7759C54.3243 58.5589 54.1361 58.3819 53.9508 58.1978C53.2871 57.5383 52.719 56.9007 52.56 56.2251C52.52 56.0256 52.417 55.819 52.4718 55.2287L52.4728 55.2258L52.4737 55.2229C52.6334 54.6715 52.8932 54.222 53.3341 53.7715C53.7929 53.779 54.247 53.789 54.6008 53.8566C54.7631 53.8838 55.1005 53.9278 55.4491 54.0577C56.3314 54.3864 57.4972 54.9267 57.4972 54.9267C56.6221 54.4462 55.6436 53.822 55.0257 53.696C54.9338 53.683 54.8773 53.6411 54.85 53.5678C55.7793 53.0218 55.9493 52.3789 56.5606 51.8023C56.84 51.6807 56.9489 51.6187 57.1863 51.5929C59.3815 51.94 60.7635 52.8143 61.8567 53.399C62.3004 53.6403 62.7037 53.8228 63.089 54.0425C63.4267 54.1536 64.4457 54.9229 64.7439 55.3377C65.0469 55.9773 65.268 56.6683 65.4698 57.335C65.6144 58.0131 65.7413 58.2882 65.7413 58.2882C65.7413 58.2882 65.6184 57.721 65.6389 57.6191C65.7663 57.6667 66.0677 57.7608 66.1926 57.7462C66.1926 57.7462 65.6346 57.4577 65.5615 57.2028C65.3269 56.3848 65.0915 55.1114 65.0281 55.045C64.8493 54.82 64.1035 54.2493 63.6461 53.9853C63.4731 53.8854 63.3795 53.8235 63.3722 53.7796C63.5184 53.632 63.6988 53.4367 63.8607 53.3126C64.0158 53.1937 64.157 53.0595 64.3732 52.9793C65.3234 52.5513 65.8667 53.1439 65.9956 53.0239C65.9956 53.0239 65.7912 52.7896 65.882 52.8282C65.975 52.8785 66.278 52.9379 66.3113 52.9689C66.6576 53.2398 67.563 54.2299 68.1087 55.2746C68.2397 55.5305 68.293 55.699 68.2322 56.0071C68.1713 56.3155 68.1241 56.4851 68.0576 56.6911C67.9976 56.8288 67.6578 57.7715 67.6591 57.8982C67.5907 58.4154 67.881 59.063 67.881 59.063C67.8837 58.8872 67.8702 58.7936 67.8861 58.6679L67.9056 58.4432C67.9056 58.4432 67.8932 58.3835 67.8955 58.3598C67.9102 58.2045 67.9483 58.0739 67.9595 57.9849C68.0686 57.3094 68.2583 56.8231 68.4734 56.2303C68.5373 56.0807 68.621 55.9975 68.6167 55.8836C68.6202 55.6815 68.4394 55.4102 68.3087 55.1452C68.1768 54.8774 68.0192 54.5789 67.8124 54.2643C67.3408 53.5861 66.9388 53.0555 66.2026 52.7235C65.9967 52.6331 65.1862 52.5447 64.9063 52.5972C64.5667 52.6679 64.2747 52.7377 64.042 52.8868C63.6756 53.1214 63.3877 53.4846 63.0501 53.6986C62.3032 53.3251 61.9426 53.0455 61.8754 53.007C61.4315 52.7691 60.8985 52.4931 60.3251 52.2313C60.0502 51.9749 58.3429 51.3617 56.7796 51.2394L56.7794 51.2393ZM66.3343 65.1711C65.8739 64.8112 65.4864 64.4407 65.2309 64.0593C65.1475 64.5079 64.8448 64.8274 64.6109 65.1417C64.5643 65.2154 64.5307 65.3159 64.7601 65.6362C64.8219 65.7215 65.0455 65.7361 65.1965 65.7283C65.0429 65.6122 64.8092 65.4894 64.7716 65.383C65.0417 65.5669 65.2919 65.6198 65.518 65.5902C65.5697 65.5844 65.6339 65.5296 65.6835 65.4452C65.7843 65.2287 65.8632 65.1765 65.943 65.1182L66.1267 65.3484L66.3343 65.1711Z",fill:"#1C1E21",stroke:"#1C1E21",strokeWidth:"0.00409071"})),i.createElement("path",{d:"M22.2143 60.6743C22.2143 61.2812 21.7223 61.7732 21.1153 61.7732C20.5084 61.7732 20.0164 61.2812 20.0164 60.6743C20.0164 60.0673 20.5084 59.5753 21.1153 59.5753C21.7223 59.5753 22.2143 60.0673 22.2143 60.6743Z",fill:"#1C1E21"}),i.createElement("path",{d:"M19.9478 62.9135C19.9478 63.5204 19.4558 64.0125 18.8488 64.0125C18.2418 64.0125 17.7498 63.5204 17.7498 62.9135C17.7498 62.3065 18.2418 61.8145 18.8488 61.8145C19.4558 61.8145 19.9478 62.3065 19.9478 62.9135Z",fill:"#1C1E21"}),i.createElement("path",{d:"M17.6784 65.1993C17.6784 65.8063 17.1864 66.2983 16.5795 66.2983C15.9725 66.2983 15.4805 65.8063 15.4805 65.1993C15.4805 64.5924 15.9725 64.1004 16.5795 64.1004C17.1864 64.1004 17.6784 64.5924 17.6784 65.1993Z",fill:"#1C1E21"}),i.createElement("path",{d:"M19.878 58.4179C19.878 58.9861 19.4174 59.4466 18.8493 59.4466C18.2811 59.4466 17.8206 58.9861 17.8206 58.4179C17.8206 57.8497 18.2811 57.3891 18.8493 57.3891C19.4174 57.3891 19.878 57.8497 19.878 58.4179Z",fill:"#1C1E21"}),i.createElement("path",{d:"M17.6167 60.6543C17.6167 61.2224 17.1562 61.683 16.5881 61.683C16.0199 61.683 15.5593 61.2224 15.5593 60.6543C15.5593 60.0861 16.0199 59.6255 16.5881 59.6255C17.1562 59.6255 17.6167 60.0861 17.6167 60.6543Z",fill:"#1C1E21"}),i.createElement("path",{d:"M15.3628 62.9228C15.3628 63.491 14.9023 63.9516 14.3342 63.9516C13.766 63.9516 13.3054 63.491 13.3054 62.9228C13.3054 62.3547 13.766 61.8941 14.3342 61.8941C14.9023 61.8941 15.3628 62.3547 15.3628 62.9228Z",fill:"#1C1E21"}),i.createElement("path",{d:"M17.5308 56.1704C17.5308 56.6864 17.1125 57.1048 16.5965 57.1048C16.0804 57.1048 15.6621 56.6864 15.6621 56.1704C15.6621 55.6544 16.0804 55.236 16.5965 55.236C17.1125 55.236 17.5308 55.6544 17.5308 56.1704Z",fill:"#1C1E21"}),i.createElement("path",{d:"M15.2726 58.4054C15.2726 58.9214 14.8542 59.3398 14.3382 59.3398C13.8222 59.3398 13.4038 58.9214 13.4038 58.4054C13.4038 57.8893 13.8222 57.4709 14.3382 57.4709C14.8542 57.4709 15.2726 57.8893 15.2726 58.4054Z",fill:"#1C1E21"}),i.createElement("path",{d:"M13.0101 60.6819C13.0101 61.1979 12.5918 61.6162 12.0757 61.6162C11.5597 61.6162 11.1414 61.1979 11.1414 60.6819C11.1414 60.1659 11.5597 59.7475 12.0757 59.7475C12.5918 59.7475 13.0101 60.1659 13.0101 60.6819Z",fill:"#1C1E21"}),i.createElement("path",{d:"M15.1954 53.9114C15.1954 54.3753 14.8194 54.7513 14.3555 54.7513C13.8916 54.7513 13.5156 54.3753 13.5156 53.9114C13.5156 53.4474 13.8916 53.0714 14.3555 53.0714C14.8194 53.0714 15.1954 53.4474 15.1954 53.9114Z",fill:"#1C1E21"}),i.createElement("path",{d:"M12.9283 56.1577C12.9283 56.6216 12.5523 56.9977 12.0884 56.9977C11.6246 56.9977 11.2485 56.6216 11.2485 56.1577C11.2485 55.6939 11.6246 55.3178 12.0884 55.3178C12.5523 55.3178 12.9283 55.6939 12.9283 56.1577Z",fill:"#1C1E21"}),i.createElement("path",{d:"M10.6686 58.4293C10.6686 58.8932 10.2925 59.2692 9.82865 59.2692C9.36479 59.2692 8.98877 58.8932 8.98877 58.4293C8.98877 57.9655 9.36479 57.5894 9.82865 57.5894C10.2925 57.5894 10.6686 57.9655 10.6686 58.4293Z",fill:"#1C1E21"}),i.createElement("path",{d:"M10.5825 62.9289C10.584 63.0279 10.5658 63.1262 10.529 63.2181C10.4921 63.31 10.4374 63.3936 10.3679 63.4642C10.2984 63.5347 10.2156 63.5907 10.1242 63.6289C10.0329 63.6672 9.93486 63.6868 9.83585 63.6868C9.73684 63.6868 9.63882 63.6671 9.54749 63.6289C9.45616 63.5906 9.37336 63.5346 9.30389 63.464C9.23442 63.3935 9.17967 63.3098 9.14283 63.2179C9.106 63.126 9.08781 63.0277 9.08933 62.9287C9.09233 62.7327 9.17232 62.5457 9.31203 62.4081C9.45174 62.2705 9.63995 62.1934 9.83602 62.1934C10.0321 62.1935 10.2203 62.2706 10.36 62.4082C10.4996 62.5458 10.5796 62.7328 10.5825 62.9289ZM12.8325 65.1789C12.834 65.2779 12.8158 65.3762 12.779 65.4681C12.7421 65.56 12.6873 65.6436 12.6179 65.7142C12.5484 65.7847 12.4656 65.8407 12.3742 65.8789C12.2829 65.9172 12.1849 65.9368 12.0858 65.9368C11.9868 65.9368 11.8888 65.9171 11.7975 65.8789C11.7062 65.8406 11.6234 65.7846 11.5539 65.714C11.4844 65.6435 11.4297 65.5598 11.3928 65.4679C11.356 65.376 11.3378 65.2777 11.3393 65.1787C11.3423 64.9827 11.4223 64.7957 11.562 64.6581C11.7017 64.5205 11.8899 64.4434 12.086 64.4434C12.2821 64.4435 12.4703 64.5206 12.61 64.6582C12.7496 64.7958 12.8296 64.9828 12.8325 65.1789ZM15.0789 67.4253C15.0789 67.5233 15.0596 67.6204 15.0221 67.711C14.9846 67.8016 14.9296 67.8839 14.8602 67.9532C14.7909 68.0225 14.7086 68.0775 14.618 68.115C14.5274 68.1526 14.4304 68.1719 14.3323 68.1719C14.2343 68.1719 14.1372 68.1526 14.0466 68.115C13.956 68.0775 13.8737 68.0225 13.8044 67.9532C13.735 67.8839 13.6801 67.8016 13.6425 67.711C13.605 67.6204 13.5857 67.5233 13.5857 67.4253C13.5857 67.3272 13.605 67.2301 13.6425 67.1395C13.6801 67.049 13.735 66.9666 13.8044 66.8973C13.8737 66.828 13.956 66.773 14.0466 66.7355C14.1372 66.698 14.2343 66.6786 14.3323 66.6786C14.4304 66.6786 14.5274 66.698 14.618 66.7355C14.7086 66.773 14.7909 66.828 14.8602 66.8973C14.9296 66.9666 14.9846 67.049 15.0221 67.1395C15.0596 67.2301 15.0789 67.3272 15.0789 67.4253ZM10.5933 67.4253C10.5944 67.524 10.5758 67.6219 10.5388 67.7135C10.5017 67.805 10.4469 67.8882 10.3775 67.9584C10.308 68.0286 10.2254 68.0843 10.1343 68.1224C10.0431 68.1604 9.94539 68.18 9.84666 68.18C9.74793 68.18 9.65017 68.1604 9.55906 68.1224C9.46794 68.0843 9.38527 68.0286 9.31584 67.9584C9.2464 67.8882 9.19157 67.805 9.15452 67.7135C9.11747 67.6219 9.09894 67.524 9.1 67.4253C9.10212 67.2286 9.18171 67.0408 9.32151 66.9025C9.4613 66.7641 9.65001 66.6866 9.84666 66.6866C10.0433 66.6866 10.232 66.7641 10.3718 66.9025C10.5116 67.0408 10.5912 67.2286 10.5933 67.4253ZM6.06455 67.4253C6.0664 67.5245 6.04845 67.623 6.01176 67.7152C5.97507 67.8074 5.92038 67.8914 5.85087 67.9622C5.78136 68.033 5.69844 68.0893 5.60694 68.1277C5.51544 68.1661 5.41721 68.1858 5.31799 68.1858C5.21876 68.1858 5.12053 68.1661 5.02903 68.1277C4.93754 68.0893 4.85461 68.033 4.7851 67.9622C4.7156 67.8914 4.6609 67.8074 4.62421 67.7152C4.58752 67.623 4.56957 67.5245 4.57142 67.4253C4.57506 67.2297 4.65531 67.0433 4.79493 66.9063C4.93454 66.7692 5.12236 66.6924 5.31799 66.6924C5.51362 66.6924 5.70144 66.7692 5.84105 66.9063C5.98066 67.0433 6.06092 67.2297 6.06455 67.4253ZM8.31455 65.1753C8.3164 65.2745 8.29845 65.3731 8.26176 65.4652C8.22507 65.5574 8.17037 65.6414 8.10087 65.7122C8.03136 65.783 7.94843 65.8393 7.85694 65.8777C7.76544 65.9161 7.66721 65.9358 7.56798 65.9358C7.46876 65.9358 7.37052 65.9161 7.27903 65.8777C7.18753 65.8393 7.10461 65.783 7.0351 65.7122C6.96559 65.6414 6.91089 65.5574 6.87421 65.4652C6.83752 65.3731 6.81957 65.2745 6.82141 65.1753C6.82505 64.9797 6.90531 64.7933 7.04492 64.6563C7.18453 64.5192 7.37235 64.4425 7.56798 64.4425C7.76361 64.4425 7.95143 64.5192 8.09104 64.6563C8.23066 64.7933 8.31091 64.9797 8.31455 65.1753Z",fill:"#1C1E21"}),i.createElement("path",{d:"M10.5912 53.9136C10.5882 54.1185 10.5047 54.314 10.3587 54.4578C10.2127 54.6016 10.016 54.6823 9.81105 54.6823C9.60611 54.6823 9.4094 54.6016 9.26342 54.4578C9.11743 54.314 9.03389 54.1185 9.03085 53.9136C9.02931 53.8101 9.04836 53.7074 9.08687 53.6114C9.12539 53.5154 9.18262 53.428 9.25522 53.3543C9.32782 53.2806 9.41435 53.2221 9.50977 53.1822C9.60519 53.1422 9.7076 53.1217 9.81105 53.1217C9.91449 53.1217 10.0169 53.1422 10.1123 53.1822C10.2077 53.2221 10.2943 53.2806 10.3669 53.3543C10.4395 53.428 10.4967 53.5154 10.5352 53.6114C10.5737 53.7074 10.5928 53.8101 10.5912 53.9136Z",fill:"#1C1E21"}),i.createElement("path",{d:"M8.32094 56.1965C8.32094 56.4034 8.23873 56.6019 8.0924 56.7482C7.94606 56.8945 7.7476 56.9767 7.54065 56.9767C7.33371 56.9767 7.13524 56.8945 6.98891 56.7482C6.84257 56.6019 6.76037 56.4034 6.76037 56.1965C6.76037 55.9895 6.84257 55.791 6.98891 55.6447C7.13524 55.4984 7.33371 55.4162 7.54065 55.4162C7.7476 55.4162 7.94606 55.4984 8.0924 55.6447C8.23873 55.791 8.32094 55.9895 8.32094 56.1965ZM5.94175 53.8819C5.94175 53.9645 5.92547 54.0464 5.89383 54.1228C5.86219 54.1992 5.81582 54.2686 5.75736 54.327C5.6989 54.3855 5.62949 54.4319 5.55311 54.4635C5.47673 54.4951 5.39486 54.5114 5.31219 54.5114C5.22951 54.5114 5.14764 54.4951 5.07126 54.4635C4.99488 54.4319 4.92547 54.3855 4.86701 54.327C4.80855 54.2686 4.76218 54.1992 4.73054 54.1228C4.6989 54.0464 4.68262 53.9645 4.68262 53.8819C4.68262 53.7149 4.74895 53.5548 4.86701 53.4367C4.98508 53.3186 5.14521 53.2523 5.31219 53.2523C5.47916 53.2523 5.63929 53.3186 5.75736 53.4367C5.87542 53.5548 5.94175 53.7149 5.94175 53.8819Z",fill:"#1C1E21"}),i.createElement("g",{clipPath:"url(#clip1_687_7655)"},i.createElement("path",{d:"M63.5897 18.3061C63.5897 18.2807 63.5972 18.2559 63.6114 18.2348C63.6256 18.2137 63.6457 18.1973 63.6693 18.1877C63.6928 18.1781 63.7187 18.1758 63.7436 18.1809C63.7685 18.1861 63.7912 18.1986 63.809 18.2168C63.8268 18.235 63.8387 18.258 63.8433 18.283C63.8479 18.308 63.8449 18.3338 63.8348 18.3572C63.8246 18.3805 63.8077 18.4002 63.7863 18.4139C63.7649 18.4276 63.7399 18.4345 63.7144 18.4339C63.6978 18.4339 63.6813 18.4306 63.666 18.4241C63.6507 18.4177 63.6368 18.4082 63.6252 18.3963C63.6135 18.3844 63.6044 18.3703 63.5983 18.3548C63.5922 18.3393 63.5893 18.3228 63.5897 18.3061ZM59.1508 18.1814C59.1169 18.1814 59.0844 18.1948 59.0605 18.2188C59.0365 18.2428 59.0231 18.2753 59.0231 18.3092C59.0231 18.343 59.0365 18.3755 59.0605 18.3995C59.0844 18.4235 59.1169 18.4369 59.1508 18.4369C59.1847 18.4369 59.2172 18.4235 59.2412 18.3995C59.2651 18.3755 59.2786 18.343 59.2786 18.3092C59.2786 18.2753 59.2651 18.2428 59.2412 18.2188C59.2172 18.1948 59.1847 18.1814 59.1508 18.1814ZM67.1736 17.6551V19.228C67.1491 19.6863 66.7588 20.0385 66.3005 20.0161H65.9963C65.7943 20.5258 65.4899 20.9887 65.1018 21.376C65.0767 21.3996 65.0567 21.4281 65.043 21.4598C65.0293 21.4914 65.0223 21.5256 65.0223 21.56C65.0223 21.5945 65.0293 21.6286 65.043 21.6603C65.0567 21.6919 65.0767 21.7205 65.1018 21.7441L65.2904 21.9176C65.315 21.9378 65.3352 21.9627 65.3501 21.9909C65.3649 22.019 65.374 22.0498 65.3769 22.0815C65.3797 22.1132 65.3762 22.1452 65.3666 22.1755C65.357 22.2059 65.3415 22.234 65.3209 22.2583C64.4295 23.3535 62.9721 23.962 61.3231 23.962C60.6186 23.9829 59.9171 23.8604 59.2613 23.6021C58.6055 23.3437 58.009 22.9548 57.508 22.459C57.5057 22.4565 57.5033 22.4541 57.5011 22.4514C57.4577 22.4024 57.4244 22.3452 57.4031 22.2832C57.3818 22.2213 57.3729 22.1557 57.3769 22.0903C57.3809 22.0249 57.3978 21.961 57.4266 21.9021C57.4553 21.8432 57.4954 21.7906 57.5445 21.7472C57.6529 21.647 57.653 21.4731 57.5445 21.373C57.1558 20.9879 56.8521 20.5257 56.6531 20.0161H56.3762C55.9173 20.0385 55.5262 19.6867 55.5 19.228V17.6551C55.5278 17.1976 55.9185 16.8476 56.3762 16.8701H56.5922C56.8545 15.894 57.4334 15.0326 58.2381 14.4211C57.9035 13.5509 57.3254 11.8441 57.2675 10.4568C57.1763 8.50347 57.3315 5.02609 58.6032 4.96514C59.875 4.90434 59.6893 8.15355 59.4734 10.265C59.361 11.4358 59.3284 12.6129 59.376 13.7881C60.0051 13.5565 60.671 13.4411 61.3414 13.4474C61.8047 13.4464 62.2665 13.5015 62.7166 13.6117C62.9235 12.6077 63.4102 10.7823 64.323 9.49835C65.6464 7.63636 67.0429 5.71357 67.9738 6.25206C68.9048 6.79055 67.7305 8.73772 66.9212 9.75691C66.1119 10.7761 64.7154 12.2973 64.2499 13.1036C64.0826 13.4078 63.9092 13.7394 63.7784 14.0163C64.3413 14.3058 64.8376 14.7097 65.2352 15.2022C65.6329 15.6947 65.9232 16.2649 66.0876 16.8762H66.1454V15.0508C66.1461 15.0034 66.1329 14.9568 66.1075 14.9169C66.0821 14.8769 66.0456 14.8451 66.0024 14.8256C65.8749 14.7715 65.7656 14.682 65.6874 14.5677C65.6092 14.4534 65.5654 14.319 65.5612 14.1806C65.5808 13.9974 65.6626 13.8264 65.7928 13.6961C65.9231 13.5659 66.0941 13.4841 66.2773 13.4646C66.7201 13.4172 67.1172 13.7379 67.1646 14.1806C67.1603 14.3186 67.1169 14.4525 67.0393 14.5668C66.9617 14.681 66.8532 14.7708 66.7265 14.8256C66.685 14.8465 66.6504 14.8787 66.6267 14.9186C66.603 14.9585 66.5911 15.0044 66.5926 15.0508V16.7546C66.5921 16.8004 66.6043 16.8454 66.628 16.8846C66.6516 16.9239 66.6857 16.9557 66.7265 16.9766C66.8569 17.0368 66.9678 17.1322 67.0469 17.2521C67.1259 17.3719 67.1698 17.5115 67.1736 17.6551ZM66.3644 14.5093C66.41 14.5121 66.4558 14.5058 66.499 14.4908C66.5422 14.4757 66.5819 14.4522 66.6159 14.4216C66.6499 14.391 66.6775 14.3539 66.697 14.3126C66.7165 14.2712 66.7275 14.2263 66.7295 14.1806C66.7291 14.1173 66.7104 14.0554 66.6757 14.0024C66.6409 13.9495 66.5916 13.9077 66.5336 13.8821C66.4757 13.8565 66.4116 13.8482 66.349 13.8582C66.2865 13.8682 66.2281 13.896 66.181 13.9384C66.134 13.9808 66.1001 14.0359 66.0836 14.097C66.0671 14.1582 66.0686 14.2228 66.088 14.2832C66.1074 14.3435 66.1438 14.3969 66.1928 14.437C66.2418 14.4771 66.3014 14.5022 66.3644 14.5093ZM66.0297 17.2626C66.1072 17.6421 66.1469 18.0283 66.1483 18.4156V18.7047H66.4739C66.5412 18.7047 66.6057 18.6782 66.6536 18.6309C66.7014 18.5836 66.7287 18.5194 66.7295 18.4521V17.655C66.7207 17.4201 66.5131 17.2389 66.2792 17.2626H66.0297ZM63.0786 13.7151L63.5623 13.9038C64.1271 12.8455 64.7571 11.8233 65.4487 10.8431C66.4678 9.47713 67.2984 8.00765 66.9303 7.77949C66.5621 7.55126 65.6494 8.753 64.7367 10.2408C64.1039 11.3589 63.5498 12.5199 63.0786 13.7151ZM59.8689 14.2476L59.8811 15.3033C59.878 15.4622 60.03 15.5873 60.1853 15.5528C60.5634 15.4654 60.9503 15.4215 61.3383 15.422C61.7258 15.4205 62.1121 15.4664 62.4884 15.5588C62.6432 15.5903 62.7934 15.4676 62.7926 15.3094V14.2476C62.7929 14.191 62.7742 14.1359 62.7394 14.0912C62.7047 14.0465 62.6559 14.0148 62.601 14.0012C62.1855 13.9013 61.7595 13.8522 61.3323 13.8551C60.905 13.8526 60.4791 13.9016 60.0636 14.0012C60.0081 14.0142 59.9586 14.0456 59.9232 14.0904C59.8879 14.1352 59.8687 14.1906 59.8689 14.2476ZM61.2197 19.6813V19.9856C61.1727 20.0472 61.1131 20.098 61.0449 20.1347C60.9767 20.1715 60.9014 20.1932 60.8242 20.1985C60.5808 20.1985 60.3983 19.8669 60.3983 19.8669C60.3947 19.86 60.3899 19.8539 60.3839 19.849C60.378 19.844 60.3712 19.8403 60.3638 19.838C60.3564 19.8358 60.3487 19.835 60.341 19.8358C60.3333 19.8366 60.3259 19.8389 60.3191 19.8426L60.3172 19.8436C60.3047 19.851 60.2956 19.8631 60.292 19.8771C60.2884 19.8912 60.2904 19.9061 60.2978 19.9186C60.3069 19.9369 60.5199 20.311 60.8242 20.311C60.9331 20.3076 61.0388 20.2727 61.1284 20.2106C61.1708 20.1825 61.2205 20.1675 61.2714 20.1675C61.3222 20.1675 61.372 20.1825 61.4143 20.2106C61.5255 20.283 61.656 20.3201 61.7886 20.3171C61.8979 20.2959 62.0012 20.2508 62.0912 20.1851C62.1811 20.1194 62.2554 20.0346 62.3088 19.9369C62.3126 19.9305 62.315 19.9235 62.316 19.9162C62.3169 19.9089 62.3164 19.9014 62.3144 19.8943C62.3124 19.8872 62.309 19.8806 62.3044 19.8748C62.2998 19.8691 62.2941 19.8643 62.2876 19.8608C62.2811 19.8566 62.2738 19.8537 62.2662 19.8525C62.2585 19.8513 62.2507 19.8517 62.2432 19.8537C62.2358 19.8557 62.2288 19.8593 62.2228 19.8642C62.2168 19.8691 62.2119 19.8752 62.2085 19.8821C62.2085 19.9004 62.035 20.1863 61.7795 20.2046C61.6929 20.2021 61.6078 20.1819 61.5293 20.1452C61.4508 20.1086 61.3806 20.0564 61.3231 19.9917V19.6874C61.323 19.6484 61.332 19.61 61.3494 19.5751C61.3668 19.5402 61.3922 19.5099 61.4235 19.4866C61.5786 19.3679 61.752 19.1733 61.752 19.0972C61.7295 18.9958 61.6706 18.9061 61.5866 18.845C61.5026 18.7839 61.3991 18.7557 61.2957 18.7655C60.9975 18.7655 60.7785 18.9633 60.7785 19.1276C60.7785 19.2341 60.9793 19.3953 61.1131 19.4927C61.1436 19.5144 61.169 19.5426 61.1874 19.5752C61.2058 19.6078 61.2168 19.644 61.2197 19.6813ZM58.5697 14.1836L59.0261 13.9403C59.0109 13.0489 58.9988 11.4516 59.0261 10.3442C59.0687 8.73171 59.1478 6.72373 58.6124 6.74503C58.077 6.76633 57.7026 8.31189 57.9095 10.5115C58.0419 11.7497 58.2626 12.9769 58.5697 14.1836ZM55.9533 18.4522C55.9533 18.5192 55.9799 18.5834 56.0273 18.6308C56.0746 18.6781 56.1389 18.7047 56.2058 18.7047H56.5223C56.5165 18.6085 56.5165 18.512 56.5223 18.4157C56.5234 18.0285 56.5621 17.6424 56.6379 17.2627C56.5989 17.2628 56.3688 17.2631 56.3319 17.261C56.2811 17.262 56.2311 17.273 56.1846 17.2934C56.1382 17.3138 56.0961 17.3431 56.061 17.3797C56.0259 17.4163 55.9983 17.4595 55.9798 17.5067C55.9613 17.554 55.9524 17.6044 55.9534 17.6552V18.4522H55.9533ZM56.6804 19.6174C56.64 19.4722 56.6075 19.3249 56.583 19.1762C56.5736 19.116 56.5429 19.0611 56.4966 19.0216C56.4503 18.982 56.3914 18.9602 56.3305 18.9602H55.938C55.9386 18.9731 55.9368 19.2294 55.9387 19.2413C55.9424 19.2951 55.9566 19.3476 55.9805 19.396C56.0045 19.4443 56.0377 19.4875 56.0783 19.5229C56.119 19.5584 56.1662 19.5856 56.2173 19.6028C56.2684 19.62 56.3224 19.627 56.3762 19.6234L56.6804 19.6174ZM64.6971 21.9267L64.6272 21.8628C64.5855 21.827 64.5334 21.8056 64.4786 21.8018C64.4238 21.7979 64.3692 21.8119 64.3229 21.8415C63.4161 22.3905 62.3706 22.6671 61.311 22.6385C60.2544 22.6648 59.2124 22.3883 58.308 21.8415C58.2618 21.8118 58.2072 21.7979 58.1524 21.8018C58.0976 21.8056 58.0455 21.827 58.0038 21.8628L57.9338 21.9267C57.9093 21.95 57.8897 21.9781 57.8763 22.0092C57.863 22.0403 57.8561 22.0738 57.8561 22.1077C57.8561 22.1415 57.863 22.175 57.8763 22.2062C57.8897 22.2373 57.9093 22.2654 57.9338 22.2887C58.856 23.1411 60.077 23.5959 61.3322 23.5544C62.5861 23.5938 63.8054 23.1393 64.7275 22.2887C64.7503 22.2634 64.7676 22.2337 64.7785 22.2014C64.7893 22.1692 64.7934 22.135 64.7906 22.1011C64.7877 22.0672 64.778 22.0342 64.7619 22.0042C64.7458 21.9742 64.7238 21.9478 64.6971 21.9267ZM65.8441 18.4308C65.8441 16.4867 64.9314 15.0051 63.4497 14.2931C63.285 14.2122 63.0798 14.3402 63.0816 14.5244V15.577C63.0826 15.6276 63.0985 15.6767 63.1271 15.7184C63.1558 15.76 63.196 15.7923 63.2429 15.8114C64.615 16.3742 65.5612 17.5424 65.5612 18.8811C65.5612 20.6792 63.8544 21.8962 61.3322 21.8962C58.81 21.8962 57.1032 20.6761 57.1032 18.8811C57.1032 17.5424 58.0494 16.3742 59.4215 15.8114C59.4688 15.793 59.5095 15.7609 59.5383 15.7191C59.567 15.6773 59.5825 15.6278 59.5828 15.577V14.5305C59.5866 14.3472 59.382 14.2167 59.2176 14.2992C57.7238 14.999 56.8111 16.4898 56.8111 18.4216C56.8111 20.7886 58.6213 22.3768 61.3321 22.3768C64.0461 22.3769 65.8441 20.7857 65.8441 18.4308ZM59.0292 18.0049H59.0261C58.9036 18.0053 58.7863 18.0543 58.6998 18.1412C58.6134 18.228 58.565 18.3456 58.5652 18.4682C58.5654 18.5907 58.6142 18.7081 58.7009 18.7947C58.7876 18.8813 58.9051 18.9299 59.0276 18.9299C59.1502 18.9299 59.2677 18.8813 59.3544 18.7947C59.4411 18.7081 59.4899 18.5907 59.4901 18.4682C59.4903 18.3456 59.4419 18.228 59.3555 18.1412C59.269 18.0543 59.1517 18.0053 59.0292 18.0049ZM63.1152 18.4735C63.1202 18.7289 63.3223 18.9318 63.5776 18.9267C63.6694 18.9267 63.7592 18.8994 63.8355 18.8482C63.9118 18.797 63.971 18.7242 64.0058 18.6391C64.0405 18.5541 64.0491 18.4606 64.0306 18.3707C64.012 18.2807 63.967 18.1983 63.9014 18.134C63.8358 18.0697 63.7525 18.0263 63.6622 18.0095C63.5719 17.9928 63.4786 18.0032 63.3943 18.0397C63.3099 18.0761 63.2383 18.1368 63.1886 18.2141C63.139 18.2913 63.1134 18.3817 63.1152 18.4735ZM66.7325 18.9663H66.337C66.2766 18.967 66.2184 18.9891 66.1727 19.0286C66.127 19.0682 66.0969 19.1226 66.0875 19.1823C66.0601 19.3192 66.0267 19.4804 65.9871 19.6235C66.0001 19.624 66.2928 19.6224 66.3048 19.6242C66.3587 19.6261 66.4124 19.6173 66.4629 19.5984C66.5135 19.5795 66.5598 19.5509 66.5992 19.5141C66.6387 19.4773 66.6705 19.4331 66.6928 19.384C66.7152 19.3349 66.7277 19.2819 66.7296 19.228L66.7325 18.9663Z",fill:"#1C1E21"})),i.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M18.1005 19.4669C18.0594 19.3789 18.0409 19.3347 18.0187 19.2924C17.4234 18.1601 16.8298 17.0278 16.2284 15.8956C16.1681 15.782 16.1758 15.715 16.2583 15.6171C17.2054 14.5108 18.1403 13.3984 19.089 12.2967C19.1225 12.2578 19.1523 12.2167 19.1649 12.1474C18.8895 12.2191 18.614 12.2901 18.3371 12.3631C17.1941 12.6661 16.0419 12.9675 14.9096 13.2751C14.8033 13.3037 14.7551 13.2726 14.7015 13.1842C14.0527 12.0993 13.3994 11.0114 12.743 9.94032C12.7094 9.88417 12.6729 9.82969 12.6013 9.78272C12.5486 10.0719 12.4954 10.3596 12.4437 10.6488C12.2616 11.6694 12.0795 12.6838 11.8989 13.709C11.8792 13.819 11.8523 13.9293 11.8428 14.0395C11.8338 14.1448 11.7794 14.1837 11.6837 14.214C10.3356 14.6378 8.99064 15.0647 7.64413 15.4916C7.58507 15.5101 7.52708 15.5346 7.46358 15.592C8.56527 16.0296 9.66695 16.4672 10.7839 16.9125C10.7432 16.9448 10.7163 16.9699 10.6865 16.989C9.99792 17.4342 9.30784 17.8795 8.62081 18.3263C8.53834 18.3802 8.47315 18.3878 8.38058 18.346C7.55738 17.9758 6.72805 17.6116 5.90178 17.2444C5.53149 17.0791 5.19946 16.8603 4.94087 16.542C4.35483 15.8259 4.47112 15.0119 5.25148 14.507C5.50701 14.3432 5.79773 14.2224 6.08693 14.1275C7.40742 13.6975 8.73404 13.2829 10.0499 12.8667C10.161 12.8313 10.2121 12.7818 10.2336 12.6616C10.4111 11.6395 10.5931 10.6113 10.7783 9.6014C10.8768 9.05515 10.9288 8.49665 11.1945 7.99477C11.2962 7.80198 11.4179 7.61377 11.5648 7.45311C12.0911 6.8686 12.8241 6.84718 13.3856 7.40292C13.5738 7.59113 13.7345 7.81146 13.8737 8.03945C14.4858 9.03862 15.0886 10.0439 15.6946 11.0538C15.7657 11.1727 15.8297 11.1966 15.9608 11.162C17.4374 10.7657 18.9139 10.3755 20.3982 9.98531C20.7042 9.90467 21.0117 9.87544 21.3254 9.93405C22.0063 10.0614 22.3047 10.5798 22.0736 11.2392C21.9691 11.5391 21.7875 11.7916 21.584 12.0318C20.5527 13.2452 19.5183 14.4647 18.4932 15.6735C18.4088 15.7727 18.4072 15.8434 18.4656 15.9551C19.0838 17.1195 19.6958 18.2809 20.3171 19.4591C20.4641 19.7375 20.5772 20.0283 20.5802 20.3481C20.5874 21.0764 20.0539 21.6731 19.3301 21.7803C18.9247 21.8395 18.5482 21.7529 18.1672 21.6357C17.2415 21.3495 16.3158 21.068 15.3824 20.788C15.2963 20.7623 15.2634 20.7281 15.2479 20.6356C15.1414 19.9822 15.0245 19.3304 14.9113 18.677C14.9084 18.6591 14.9137 18.64 14.9168 18.5999C15.9741 18.8921 17.0284 19.1798 18.1148 19.4812",fill:"#1C1E21"}),i.createElement("path",{d:"M22.4773 37.1463H16.5288C14.8527 37.1463 13.4939 35.7875 13.4939 34.1114V27.6773",stroke:"#0DB1F9",strokeWidth:"0.971178",strokeLinecap:"round"}),i.createElement("path",{d:"M17.0308 30.4193C16.9036 30.5465 16.7045 30.5581 16.5642 30.454L16.5241 30.4193L13.4331 27.3286L10.3422 30.4193C10.215 30.5465 10.0159 30.5581 9.87566 30.454L9.83548 30.4193C9.70826 30.2921 9.6967 30.093 9.80078 29.9528L9.83548 29.9126L13.1798 26.5683C13.307 26.4411 13.506 26.4295 13.6463 26.5336L13.6865 26.5683L17.0308 29.9126C17.1707 30.0525 17.1707 30.2794 17.0308 30.4193Z",fill:"#0DB1F9",stroke:"#0DB1F9",strokeWidth:"0.268325"}),i.createElement("path",{d:"M52.5835 37.1462H58.532C60.2081 37.1462 61.5669 35.7875 61.5669 34.1113V27.6773",stroke:"#0DB1F9",strokeWidth:"0.971178",strokeLinecap:"round"}),i.createElement("path",{d:"M58.0295 30.4193C58.1567 30.5465 58.3558 30.5581 58.4961 30.454L58.5362 30.4193L61.6272 27.3285L64.7181 30.4193C64.8453 30.5465 65.0444 30.5581 65.1846 30.454L65.2248 30.4193C65.352 30.2921 65.3636 30.093 65.2595 29.9527L65.2248 29.9126L61.8805 26.5683C61.7533 26.4411 61.5543 26.4295 61.414 26.5336L61.3738 26.5683L58.0295 29.9126C57.8896 30.0525 57.8896 30.2794 58.0295 30.4193Z",fill:"#0DB1F9",stroke:"#0DB1F9",strokeWidth:"0.268325"}),i.createElement("path",{d:"M51.9091 27.6865L44.9235 20.7009C44.8185 20.5878 44.6703 20.5244 44.516 20.5262H27.5521C26.9573 20.5294 26.3873 20.7671 25.9666 21.1876C25.5461 21.6084 25.3083 22.1783 25.3052 22.7732V33.9156C25.3052 34.2371 25.5658 34.4978 25.8873 34.4978C26.2088 34.4978 26.4695 34.2371 26.4695 33.9156V22.7732C26.4695 22.486 26.5835 22.2108 26.7865 22.0076C26.9897 21.8046 27.265 21.6905 27.5521 21.6905H43.9338V28.094C43.9338 28.2484 43.9952 28.3965 44.1043 28.5057C44.2135 28.6148 44.3616 28.6762 44.516 28.6762H50.9195V52.0435C50.9195 52.3307 50.8054 52.6059 50.6024 52.8091C50.3992 53.0121 50.124 53.1262 49.8368 53.1262H27.5526C27.2654 53.1262 26.9902 53.0121 26.7869 52.8091C26.584 52.6059 26.4699 52.3307 26.4699 52.0435V49.0512C26.4699 48.7297 26.2092 48.4691 25.8877 48.4691C25.5663 48.4691 25.3056 48.7297 25.3056 49.0512V52.0435C25.3087 52.6384 25.5465 53.2083 25.967 53.6291C26.3878 54.0495 26.9577 54.2873 27.5525 54.2905H49.8368C50.4317 54.2873 51.0016 54.0495 51.4223 53.6291C51.8428 53.2083 52.0806 52.6384 52.0837 52.0435V28.094C52.0856 27.9397 52.0222 27.7915 51.9091 27.6865L51.9091 27.6865ZM45.0978 22.5172L50.0924 27.5119H45.0978V22.5172Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M47.4272 45.4181V37.5476C47.4303 37.0463 47.2325 36.5647 46.8783 36.2102C46.5238 35.856 46.0423 35.6582 45.5409 35.6613H24.8633C24.362 35.6582 23.8805 35.856 23.526 36.2102C23.1717 36.5647 22.974 37.0462 22.9771 37.5476V45.4181V45.4178C22.974 45.9191 23.1717 46.4007 23.526 46.7552C23.8804 47.1094 24.362 47.3072 24.8633 47.3041H45.5409C46.0422 47.3072 46.5237 47.1094 46.8783 46.7552C47.2325 46.4007 47.4303 45.9192 47.4272 45.4178V45.4181ZM30.778 43.5552C30.3268 43.896 29.7709 44.0688 29.2062 44.0443C28.2673 44.0488 27.3614 43.6995 26.6682 43.0664L27.3203 42.2979H27.32C27.8377 42.7876 28.5173 43.0693 29.2296 43.0895C29.5054 43.1048 29.7788 43.0316 30.0096 42.88C30.1853 42.7506 30.2892 42.5453 30.2892 42.327C30.2892 42.1087 30.1853 41.9034 30.0096 41.774C29.7185 41.6001 29.3994 41.478 29.0664 41.413C28.7237 41.3353 28.3856 41.2381 28.0537 41.1219C27.832 41.0364 27.6241 40.9187 27.4364 40.7727C27.0606 40.4673 26.8566 39.9984 26.8894 39.5154C26.8652 39.0083 27.097 38.5231 27.5063 38.223C27.9437 37.901 28.4773 37.7367 29.0199 37.7572C29.4162 37.7585 29.8097 37.8253 30.1842 37.9553C30.5459 38.067 30.8822 38.2489 31.1738 38.4906L30.6265 39.2591H30.6268C30.4129 39.0832 30.1681 38.9488 29.9048 38.8633C29.6121 38.7641 29.3055 38.7129 28.9968 38.7121C28.7408 38.7006 28.4866 38.7612 28.2631 38.8867C28.0628 39.0122 27.9409 39.2321 27.9409 39.4689C27.9409 39.7054 28.0628 39.9252 28.2631 40.051C28.627 40.2503 29.0199 40.392 29.4274 40.4699C29.9376 40.5692 30.4165 40.7885 30.8245 41.1103C31.1465 41.4099 31.3209 41.8353 31.302 42.2746C31.3313 42.759 31.1385 43.2304 30.778 43.5553L30.778 43.5552ZM37.3679 45.2087C36.9882 45.2076 36.6168 45.0985 36.2966 44.8942C35.9671 44.6951 35.7132 44.3921 35.5749 44.0327H35.4468C34.5805 44.0652 33.7391 43.7411 33.1182 43.1363C32.534 42.5417 32.2183 41.734 32.245 40.9008C32.2263 40.0601 32.5597 39.2498 33.1647 38.6652C33.7804 38.0514 34.6245 37.7221 35.4933 37.7572C36.3426 37.7357 37.1635 38.0642 37.7636 38.6652C38.3777 39.2443 38.7163 40.0572 38.695 40.9008C38.707 41.545 38.5155 42.1763 38.1477 42.7054C37.7927 43.2327 37.2849 43.6389 36.6924 43.8697C36.8621 44.1132 37.1409 44.2569 37.4377 44.2538C37.6664 44.2522 37.8907 44.1922 38.0895 44.0792C38.2875 43.9801 38.4531 43.8268 38.5669 43.6368L39.149 44.4401C38.6953 44.9433 38.045 45.2236 37.3678 45.2086L37.3679 45.2087ZM43.7366 43.9862H39.941V37.9085H41.0236V43.0079H43.7945L43.7366 43.9862Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M37.5896 40.9008C37.5979 40.3197 37.3762 39.7589 36.9726 39.3407C36.57 38.9325 36.0206 38.7027 35.4473 38.7027C34.874 38.7027 34.3246 38.9325 33.9221 39.3407C33.5359 39.7685 33.3223 40.3244 33.3223 40.9008C33.3223 41.4772 33.5359 42.0331 33.9221 42.4609C34.3246 42.8691 34.8741 43.0989 35.4473 43.0989C36.0206 43.0989 36.57 42.8691 36.9726 42.4609C37.3783 42.0443 37.6005 41.4824 37.5896 40.9008Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M22.8171 37.6363H16.8687C15.1925 37.6363 13.8337 38.9951 13.8337 40.6712V47.1053",stroke:"#0DB1F9",strokeWidth:"0.971178",strokeLinecap:"round"}),i.createElement("path",{d:"M17.3706 44.3632C17.2434 44.236 17.0444 44.2245 16.9041 44.3286L16.8639 44.3632L13.773 47.454L10.6821 44.3632C10.5548 44.236 10.3558 44.2245 10.2155 44.3286L10.1753 44.3632C10.0481 44.4905 10.0365 44.6895 10.1406 44.8298L10.1753 44.87L13.5196 48.2143C13.6468 48.3415 13.8459 48.3531 13.9862 48.249L14.0263 48.2143L17.3706 44.87C17.5106 44.7301 17.5106 44.5032 17.3706 44.3632Z",fill:"#0DB1F9",stroke:"#0DB1F9",strokeWidth:"0.268325"}),i.createElement("path",{d:"M52.5835 37.6363H58.532C60.2081 37.6363 61.5669 38.995 61.5669 40.6712V47.1052",stroke:"#0DB1F9",strokeWidth:"0.971178",strokeLinecap:"round"}),i.createElement("path",{d:"M58.0295 44.3632C58.1567 44.236 58.3558 44.2244 58.4961 44.3285L58.5362 44.3632L61.6272 47.4539L64.7181 44.3632C64.8453 44.236 65.0444 44.2244 65.1846 44.3285L65.2248 44.3632C65.352 44.4904 65.3636 44.6895 65.2595 44.8297L65.2248 44.8699L61.8805 48.2142C61.7533 48.3414 61.5543 48.353 61.414 48.2489L61.3738 48.2142L58.0295 44.8699C57.8896 44.73 57.8896 44.5031 58.0295 44.3632Z",fill:"#0DB1F9",stroke:"#0DB1F9",strokeWidth:"0.268325"}),i.createElement("defs",null,i.createElement("clipPath",{id:"clip0_687_7655"},i.createElement("rect",{width:"20.3571",height:"20.3571",fill:"white",transform:"translate(51.4285 50.3571)"})),i.createElement("clipPath",{id:"clip1_687_7655"},i.createElement("rect",{width:"20.3571",height:"20.3571",fill:"white",transform:"translate(51.4285 4.28571)"})))),P=()=>i.createElement("svg",{width:75,height:75,viewBox:"0 0 75 75",fill:"none",xmlns:"http://www.w3.org/2000/svg"},i.createElement("rect",{opacity:"0.5",width:75,height:75,rx:"8.57143",fill:"#CEEEF7"}),i.createElement("path",{d:"M13.9111 48.7555H14.932V49.7764H13.9111V48.7555ZM14.932 47.7346H13.9111V46.7137H14.932V47.7346ZM14.932 45.6929H13.9111V44.672H14.932V45.6929ZM14.932 43.6511H13.9111V42.6303H14.932V43.6511ZM14.932 41.6094H13.9111V40.5885H14.932V41.6094ZM14.932 39.5676H13.9111V38.5468H14.932V39.5676ZM14.932 37.5259H13.9111V36.505H14.932V37.5259ZM14.932 35.4842H13.9111V34.4633H14.932V35.4842ZM15.9529 49.7758V48.7549H16.9737V49.7758H15.9529ZM17.9946 49.7758V48.7549H19.0155V49.7758H17.9946ZM25.8757 37.2395H26.8966V38.2603H25.8757V37.2395ZM26.8966 36.2186H25.8757V35.1977H26.8966V36.2186ZM26.8966 34.1769H25.8757V33.156H26.8966V34.1769ZM26.8966 32.1351H25.8757V31.1142H26.8966V32.1351ZM26.8966 30.0934H25.8757V29.0725H26.8966V30.0934ZM26.8966 28.0516H25.8757V27.0308H26.8966V28.0516ZM26.8966 26.0099H25.8757V24.989H26.8966V26.0099ZM26.8966 23.9681H25.8757V22.9473H26.8966V23.9681ZM26.8966 21.9264H25.8757V20.9055H26.8966V21.9264ZM26.8966 19.8847H25.8757V18.8638H26.8966V19.8847ZM26.8966 17.8429H25.8757V16.822H26.8966V17.8429ZM41.1888 34.7485H40.1679V33.7277H41.1888V34.7485ZM41.1888 32.7068H40.1679V31.6859H41.1888V32.7068ZM41.1888 30.6651H40.1679V29.6442H41.1888V30.6651ZM41.1888 28.6233H40.1679V27.6024H41.1888V28.6233ZM41.1888 26.5816H40.1679V25.5607H41.1888V26.5816ZM41.1888 24.5398H40.1679V23.519H41.1888V24.5398ZM41.1888 22.4981H40.1679V21.4772H41.1888V22.4981ZM41.1888 20.4563H40.1679V19.4355H41.1888V20.4563ZM41.1888 18.4146H40.1679V17.3937H41.1888V18.4146ZM41.1888 16.3729H40.1679V15.352H41.1888V16.3729ZM43.2306 16.3729H42.2097V15.352H43.2306V16.3729ZM45.2723 16.3729H44.2514V15.352H45.2723V16.3729ZM55.9302 48.8975H56.9511V49.9184H55.9302V48.8975ZM55.9302 46.8558H56.9511V47.8767H55.9302V46.8558ZM55.9302 44.814H56.9511V45.8349H55.9302V44.814ZM55.9302 42.7723H56.9511V43.7932H55.9302V42.7723ZM55.9302 40.7306H56.9511V41.7514H55.9302V40.7306ZM55.9302 38.6888H56.9511V39.7097H55.9302V38.6888ZM55.9302 36.6471H56.9511V37.6679H55.9302V36.6471ZM56.9511 35.6262H55.9302V34.6053H56.9511V35.6262ZM54.9093 48.8975V49.9184H53.8884V48.8975H54.9093ZM52.8676 49.9184H51.8467V48.8975H52.8676V49.9184Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M37.9682 40.1815C38.998 40.3652 39.9892 40.6578 40.929 41.0473C41.8874 41.4446 42.7954 41.9443 43.6391 42.5332L45.5598 40.6125C45.7928 40.3796 46.1702 40.3796 46.4028 40.6125L46.4048 40.6149L49.1474 43.3575C49.3804 43.5904 49.3804 43.9679 49.1474 44.2005L49.1451 44.2025L47.2265 46.1212C47.8151 46.9652 48.3149 47.8732 48.712 48.8319C49.1015 49.7715 49.3941 50.7623 49.5775 51.7917H52.2916C52.6208 51.7917 52.8879 52.0585 52.8879 52.388V56.2696C52.8879 56.5987 52.6211 56.8659 52.2916 56.8659H49.5782C49.3945 57.8956 49.1019 58.8868 48.7123 59.8267C48.3151 60.785 47.8154 61.6931 47.2265 62.5368L49.1472 64.4575C49.3801 64.6904 49.3801 65.0679 49.1472 65.3005L49.1448 65.3025L46.4022 68.0451C46.1693 68.2781 45.7918 68.2781 45.5592 68.0451L45.5572 68.0427L43.6385 66.1241C42.7945 66.7127 41.8865 67.2125 40.9278 67.6097C39.9882 67.9992 38.9974 68.2918 37.968 68.4751V71.1893C37.968 71.5184 37.7012 71.7856 37.3717 71.7856H33.4901C33.161 71.7856 32.8938 71.5187 32.8938 71.1893V68.4758C31.8641 68.2921 30.8729 67.9995 29.933 67.61C28.9747 67.2128 28.0666 66.713 27.2229 66.1241L25.3022 68.0448C25.0693 68.2778 24.6918 68.2778 24.4592 68.0448L24.4572 68.0425L21.7146 65.2999C21.4816 65.0669 21.4816 64.6895 21.7146 64.4568L21.7169 64.4548L23.6356 62.5362C23.047 61.6922 22.5472 60.7841 22.15 59.8254C21.7605 58.8859 21.4679 57.895 21.2845 56.8657H18.5704C18.2413 56.8657 17.9741 56.5988 17.9741 56.2693V52.3878C17.9741 52.0586 18.2409 51.7915 18.5704 51.7915H21.2839C21.4672 50.7614 21.7602 49.7705 22.1497 48.8303C22.5469 47.872 23.0467 46.9639 23.6352 46.1199L21.7152 44.1999C21.4823 43.9669 21.4823 43.5895 21.7152 43.3569L21.7176 43.3548L24.4602 40.6123C24.6931 40.3793 25.0706 40.3793 25.3032 40.6123L25.3052 40.6146L27.2232 42.5326C28.0673 41.944 28.9753 41.4442 29.9337 41.0471V41.0461C30.8776 40.6542 31.8885 40.3609 32.8945 40.1816V37.4682C32.8945 37.139 33.1613 36.8719 33.4908 36.8719H37.3723C37.7015 36.8719 37.9687 37.1387 37.9687 37.4682V40.1816L37.9682 40.1815ZM40.4725 42.1467C39.4787 41.7348 38.4248 41.4412 37.328 41.2833C37.0193 41.2609 36.7759 41.0031 36.7759 40.6887V38.0644H34.0866V40.6799C34.0913 40.9762 33.8744 41.2364 33.5738 41.2789L33.5721 41.2779C32.4763 41.4324 31.4242 41.7217 30.433 42.128C30.4189 42.135 30.4045 42.1417 30.3901 42.1478C30.3287 42.0029 27.7751 43.5938 27.5573 43.7564C27.3233 43.9665 26.963 43.9588 26.7377 43.7339L24.8814 41.8776L22.9798 43.7791L24.8288 45.6281C25.043 45.8339 25.0745 46.1721 24.8914 46.4155L24.8894 46.4141C24.2304 47.2903 23.677 48.2547 23.2493 49.2878C22.8374 50.2816 22.5437 51.3358 22.3859 52.4325C22.3635 52.7413 22.1057 52.9846 21.7913 52.9846H19.167V55.6739H21.7835C22.0799 55.6692 22.34 55.8861 22.3825 56.1868L22.3815 56.1885C22.5381 57.2983 22.8337 58.3649 23.25 59.3695C23.6714 60.3865 24.2144 61.3368 24.86 62.2026C25.0702 62.4366 25.0625 62.7969 24.8376 63.0221L22.9802 64.8795L24.8817 66.7811L26.7314 64.9314C26.9375 64.7176 27.2754 64.6861 27.5188 64.8691L27.5174 64.8708C28.3936 65.5298 29.358 66.0832 30.3907 66.5112C31.3846 66.9232 32.4384 67.2168 33.5353 67.3746C33.844 67.3971 34.0873 67.6549 34.0873 67.9693V70.5935H36.7766V67.977C36.7719 67.6807 36.9888 67.4206 37.2895 67.378C37.3706 67.9539 39.1418 67.0619 40.4722 66.5105C41.4892 66.0892 42.4395 65.5462 43.3053 64.9006C43.5393 64.6904 43.8996 64.6981 44.1249 64.923L45.9823 66.7804L47.8838 64.8788L46.0342 63.0292C45.8203 62.823 45.7888 62.4851 45.9718 62.2418L45.9735 62.2431C46.6325 61.3669 47.1859 60.4026 47.614 59.3698C48.0259 58.376 48.3195 57.3221 48.4774 56.2253C48.4998 55.9166 48.7576 55.6732 49.072 55.6732H51.6963V52.9839H49.0797C48.7834 52.9886 48.5233 52.7718 48.4807 52.4711C48.8538 52.4185 48.4807 52.4711 48.4807 52.4711C48.0433 51.9388 48.0259 50.2836 47.6133 49.2884C47.1919 48.2713 46.6489 47.3211 46.0033 46.4552C45.7931 46.2213 45.8008 45.8609 46.0258 45.6357L47.8831 43.7783L45.9816 41.8767L44.1319 43.7264C43.9258 43.9402 43.5879 43.9718 43.3445 43.7887L43.3459 43.7871C42.4697 43.1281 41.5053 42.5746 40.4726 42.1466L40.4725 42.1467Z",fill:"#1C1E21"}),i.createElement("path",{d:"M35.4312 45.9745C37.738 45.9745 39.827 46.9097 41.3387 48.4215C42.8505 49.9333 43.7857 52.0218 43.7857 54.329C43.7857 56.6359 42.8505 58.7248 41.3387 60.2366C39.8269 61.7483 37.7384 62.6835 35.4312 62.6835C33.1243 62.6835 31.0354 61.7484 29.5236 60.2366C28.0119 58.7247 27.0767 56.6362 27.0767 54.329C27.0767 52.0222 28.0118 49.9332 29.5236 48.4215C31.0354 46.9097 33.124 45.9745 35.4312 45.9745ZM40.4957 49.2645C39.1998 47.9686 37.4092 47.1668 35.4312 47.1668C33.4535 47.1668 31.6629 47.9686 30.3666 49.2645C29.0707 50.5604 28.269 52.351 28.269 54.329C28.269 56.3067 29.0708 58.0973 30.3666 59.3936C31.6626 60.6895 33.4532 61.4912 35.4312 61.4912C37.4089 61.4912 39.1995 60.6894 40.4957 59.3936C41.7916 58.0976 42.5934 56.307 42.5934 54.329C42.5934 52.3513 41.7916 50.5607 40.4957 49.2645Z",fill:"#1C1E21"}),i.createElement("path",{d:"M9.02748 21.4517H8.00916C7.72798 21.4517 7.5 21.2239 7.5 20.9425C7.5 20.6611 7.72798 20.4334 8.00916 20.4334H8.51832V19.9242C8.51832 19.6428 8.74631 19.415 9.02748 19.415C9.30866 19.415 9.53664 19.6428 9.53664 19.9242V20.9425C9.53664 21.2239 9.30866 21.4517 9.02748 21.4517Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M21.247 21.4516H20.2286C19.9475 21.4516 19.7195 21.2239 19.7195 20.9425V19.9241C19.7195 19.6427 19.9475 19.415 20.2286 19.415C20.5098 19.415 20.7378 19.6427 20.7378 19.9241V20.4333H21.247C21.5281 20.4333 21.7561 20.6611 21.7561 20.9425C21.7561 21.2239 21.5281 21.4516 21.247 21.4516Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M9.02748 35.7081C8.74631 35.7081 8.51832 35.4803 8.51832 35.1989V34.6898H8.00916C7.72798 34.6898 7.5 34.462 7.5 34.1806C7.5 33.8992 7.72798 33.6714 8.00916 33.6714H9.02748C9.30866 33.6714 9.53664 33.8992 9.53664 34.1806V35.1989C9.53664 35.4803 9.30866 35.7081 9.02748 35.7081Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M20.2286 35.7081C19.9475 35.7081 19.7195 35.4803 19.7195 35.1989V34.1806C19.7195 33.8992 19.9475 33.6714 20.2286 33.6714H21.247C21.5281 33.6714 21.7561 33.8992 21.7561 34.1806C21.7561 34.462 21.5281 34.6898 21.247 34.6898H20.7378V35.1989C20.7378 35.4803 20.5098 35.7081 20.2286 35.7081Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M19.2003 33.6715H10.0558C9.77462 33.6715 9.54663 33.4437 9.54663 33.1623V21.9608C9.54663 21.6794 9.77462 21.4516 10.0558 21.4516H19.2003C19.4815 21.4516 19.7095 21.6794 19.7095 21.9608V33.1623C19.7094 33.4437 19.4814 33.6715 19.2002 33.6715H19.2003ZM10.565 32.6531H18.6912L18.691 22.4699H10.5648L10.565 32.6531Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M17.2655 26.0954H11.9909C11.7097 26.0954 11.4817 25.8677 11.4817 25.5862C11.4817 25.3048 11.7097 25.0771 11.9909 25.0771H17.2655C17.5466 25.0771 17.7746 25.3048 17.7746 25.5862C17.7746 25.8677 17.5466 26.0954 17.2655 26.0954Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M17.2655 29.2164H11.9909C11.7097 29.2164 11.4817 28.9887 11.4817 28.7073C11.4817 28.4259 11.7097 28.1981 11.9909 28.1981H17.2655C17.5466 28.1981 17.7746 28.4259 17.7746 28.7073C17.7746 28.9887 17.5466 29.2164 17.2655 29.2164Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M34.9192 14.327H19.66V4.99508C19.66 4.83886 19.5336 4.71259 19.3775 4.71259C19.2213 4.71259 19.095 4.83886 19.095 4.99508V14.327H18.2566C18.1004 14.327 17.9741 14.4532 17.9741 14.6095C17.9741 14.7657 18.1004 14.892 18.2566 14.892H19.095V15.6411C19.095 15.7973 19.2213 15.9236 19.3775 15.9236C19.5334 15.9236 19.66 15.7973 19.66 15.6411V14.892H34.9192C35.0754 14.892 35.2016 14.7657 35.2016 14.6095C35.2016 14.4532 35.0754 14.3272 34.9192 14.3272V14.327Z",fill:"#0DB1F9",stroke:"#0DB1F9",strokeWidth:"0.581897"}),i.createElement("path",{d:"M22.0931 11.7635C22.0931 12.1636 21.7686 12.4881 21.3683 12.4881C20.968 12.4881 20.6436 12.1636 20.6436 11.7635C20.6436 11.3632 20.968 11.0388 21.3683 11.0388C21.7686 11.0388 22.0931 11.3632 22.0931 11.7635Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M23.5211 11.2497C24.1486 11.2497 24.659 10.7393 24.659 10.1124C24.659 9.48497 24.1484 8.97455 23.5211 8.97455C22.8939 8.97455 22.3833 9.48517 22.3833 10.1124C22.3831 10.7393 22.8937 11.2497 23.5211 11.2497Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M22.6479 12.6921C22.6479 13.1162 22.9931 13.4613 23.417 13.4613C23.8412 13.4613 24.1867 13.1162 24.1867 12.6921C24.1867 12.2681 23.8414 11.923 23.417 11.923C22.9929 11.9234 22.6479 12.2683 22.6479 12.6921Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M24.9215 6.23212C24.3185 6.23212 23.8284 6.72245 23.8284 7.32507C23.8284 7.92808 24.3187 8.41823 24.9215 8.41823C25.5247 8.41823 26.0153 7.9279 26.0153 7.32507C26.0153 6.72267 25.5244 6.23212 24.9215 6.23212Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M26.2635 8.97412C25.6362 8.97412 25.1262 9.48474 25.1262 10.112C25.1262 10.7392 25.6366 11.2492 26.2635 11.2492C26.8909 11.2492 27.4013 10.7388 27.4013 10.112C27.4013 9.48469 26.8907 8.97412 26.2635 8.97412Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M28.1065 8.71335C28.6445 8.71335 29.0822 8.27581 29.0822 7.7382C29.0822 7.2002 28.6445 6.76245 28.1065 6.76245C27.5687 6.76245 27.1313 7.20018 27.1313 7.7382C27.1313 8.27621 27.5689 8.71335 28.1065 8.71335Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M26.8754 6.79703C27.4499 6.79703 27.9173 6.32956 27.9173 5.75511C27.9173 5.18065 27.4499 4.7132 26.8754 4.7132C26.301 4.7132 25.8335 5.18067 25.8335 5.75511C25.8337 6.32978 26.301 6.79703 26.8754 6.79703Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M30.498 10.3479C30.498 10.8945 30.0546 11.338 29.5077 11.338C28.961 11.338 28.5176 10.8945 28.5176 10.3479C28.5176 9.80099 28.961 9.35754 29.5077 9.35754C30.0546 9.35754 30.498 9.80099 30.498 10.3479Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M26.3041 11.5992C25.8337 11.5992 25.4509 11.9823 25.4509 12.4535C25.4509 12.9244 25.8337 13.3069 26.3041 13.3069C26.775 13.3069 27.1579 12.9242 27.1579 12.4535C27.1581 11.9827 26.7748 11.5992 26.3041 11.5992Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M29.9326 7.47272C29.9326 8.15707 30.4895 8.71359 31.1739 8.71359C31.8582 8.71359 32.4152 8.15687 32.4152 7.47272C32.4152 6.78876 31.8582 6.23199 31.1739 6.23199C30.4895 6.23199 29.9326 6.78871 29.9326 7.47272Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M29.7882 6.35421C30.3588 6.35421 30.823 5.89028 30.823 5.31996C30.823 4.74983 30.3587 4.28571 29.7882 4.28571C29.2176 4.28571 28.7539 4.74964 28.7539 5.31996C28.7539 5.89009 29.2178 6.35421 29.7882 6.35421Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M55.059 10.5233C54.3703 10.2454 53.6397 10.1047 52.8877 10.1047C49.9558 10.1047 47.4735 12.2958 47.1136 15.2011C47.0841 15.439 47.0691 15.6822 47.0691 15.9237C47.0691 19.1322 49.6793 21.7426 52.8877 21.7426C56.0964 21.7426 58.707 19.1324 58.707 15.9237C58.7072 13.5343 57.2751 11.4142 55.059 10.5232L55.059 10.5233ZM52.8878 21.0908C50.0388 21.0908 47.721 18.773 47.721 15.9238C47.721 15.7091 47.7343 15.4928 47.7607 15.2813C48.0801 12.7016 50.2842 10.7566 52.8879 10.7566C53.556 10.7566 54.2046 10.8814 54.8157 11.1279C56.7838 11.9193 58.0554 13.8019 58.0554 15.9239C58.0553 18.7729 55.7372 21.0909 52.8877 21.0909L52.8878 21.0908ZM55.6088 15.5976H50.1675C49.9875 15.5976 49.8416 15.7435 49.8416 15.9235C49.8416 16.1036 49.9875 16.2494 50.1675 16.2494H55.6088C55.7888 16.2494 55.9347 16.1036 55.9347 15.9235C55.9347 15.7435 55.7888 15.5976 55.6088 15.5976Z",fill:"#0DB1F9",stroke:"#0DB1F9",strokeWidth:"0.349138"}),i.createElement("path",{d:"M60.6245 35.7097C60.5841 35.7097 60.5436 35.6996 60.5065 35.6793C54.9019 32.5826 52.9217 27.2418 52.9024 27.1883C52.8705 27.0998 52.8923 27.001 52.9583 26.934L53.1509 26.7388C53.3755 26.5113 53.6956 26.4102 54.0065 26.4675C55.0354 26.6592 57.8236 26.8723 61.4842 24.726C61.581 24.6693 61.704 24.6861 61.7823 24.766L63.9363 26.9726C64.0289 27.0675 64.0288 27.2191 63.9362 27.3139C61.552 29.7515 61.5167 33.3653 61.5626 34.4148C61.5746 34.6873 61.4722 34.9543 61.2818 35.1471L60.7981 35.6371C60.7512 35.6849 60.6882 35.7097 60.6244 35.7097L60.6245 35.7097ZM53.4176 27.1637C53.7647 28.008 55.7656 32.4283 60.5815 35.1619L60.9346 34.8042C61.03 34.7077 61.0812 34.5735 61.0752 34.4362C61.0281 33.3613 61.0643 29.7171 63.4234 27.1459L61.5659 25.2431C57.8439 27.37 54.9845 27.1459 53.9177 26.9474C53.7661 26.9195 53.6094 26.9695 53.4986 27.0817L53.4176 27.1637Z",fill:"#0DB1F9",stroke:"#0DB1F9",strokeWidth:"0.349138"}),i.createElement("path",{d:"M63.3726 26.9869C63.3109 26.9869 63.2493 26.9638 63.2018 26.9172C63.1054 26.8229 63.1039 26.6684 63.1981 26.5721L67.3114 22.3698C67.6033 22.0718 67.6033 21.5866 67.3114 21.2885C67.1719 21.1457 66.9865 21.067 66.7896 21.067C66.5927 21.067 66.4072 21.1457 66.2675 21.2884L62.1542 25.4904C62.06 25.5866 61.9054 25.5885 61.8092 25.4941C61.7129 25.3997 61.7112 25.2453 61.8055 25.149L65.9188 20.9467C66.1509 20.7095 66.4603 20.5788 66.7895 20.5788C67.1187 20.5788 67.4279 20.7095 67.6602 20.9468C68.1363 21.4332 68.1363 22.2248 67.6602 22.7112L63.5469 26.9132C63.4991 26.962 63.4358 26.9865 63.3725 26.9865L63.3726 26.9869Z",fill:"#0DB1F9",stroke:"#0DB1F9",strokeWidth:"0.349138"}),i.createElement("path",{d:"M56.2465 32.2481C56.174 32.2481 56.1022 32.2159 56.0541 32.1544C55.9709 32.0483 55.9898 31.8949 56.0959 31.8118C56.28 31.6676 56.4647 31.5194 56.6446 31.3712C57.4823 30.6808 58.2905 29.931 59.0472 29.1426C59.1405 29.0453 59.2949 29.042 59.3922 29.1354C59.4895 29.2289 59.4927 29.3833 59.3993 29.4804C58.6296 30.2825 57.8071 31.0452 56.955 31.7476C56.7719 31.8985 56.584 32.0495 56.3966 32.1962C56.352 32.2311 56.2991 32.2481 56.2465 32.2481L56.2465 32.2481Z",fill:"#0DB1F9",stroke:"#0DB1F9",strokeWidth:"0.349138"}),i.createElement("path",{d:"M54.3199 29.5597C54.1981 29.5597 54.0929 29.4689 54.0779 29.3449C54.0618 29.211 54.1573 29.0895 54.2912 29.0734C54.6534 29.0298 55.0173 28.9663 55.3727 28.8845C56.6588 28.5886 57.8921 28.0427 59.038 27.2623C59.1496 27.1866 59.3014 27.2153 59.3771 27.3265C59.453 27.438 59.4241 27.5897 59.3128 27.6656C58.1164 28.4804 56.8275 29.0505 55.482 29.36C55.11 29.4456 54.7289 29.5121 54.3494 29.5577C54.3396 29.5591 54.3297 29.5597 54.3199 29.5597L54.3199 29.5597Z",fill:"#0DB1F9",stroke:"#0DB1F9",strokeWidth:"0.349138"}),i.createElement("path",{d:"M59.1496 34.739C59.1401 34.739 59.1305 34.7386 59.1208 34.7373C58.9869 34.7216 58.8912 34.6002 58.907 34.4664C58.9521 34.082 59.0178 33.6964 59.1021 33.3204C59.4075 31.9589 59.9702 30.6543 60.7746 29.443C60.849 29.3306 61.0005 29.2999 61.1128 29.3747C61.2251 29.4491 61.2558 29.6008 61.1812 29.713C60.41 30.8745 59.8707 32.1241 59.5785 33.4272C59.4979 33.7869 59.4351 34.1556 59.3918 34.5233C59.3771 34.6477 59.2716 34.739 59.1496 34.739L59.1496 34.739Z",fill:"#0DB1F9",stroke:"#0DB1F9",strokeWidth:"0.349138"})),F=()=>i.createElement("svg",{width:75,height:75,viewBox:"0 0 75 75",fill:"none",xmlns:"http://www.w3.org/2000/svg"},i.createElement("rect",{opacity:"0.5",width:75,height:75,rx:"8.57143",fill:"#CEEEF7"}),i.createElement("g",{clipPath:"url(#rich_platform)"},i.createElement("path",{d:"M66.1508 49.081C66.183 48.7775 66.1443 48.4708 66.0378 48.1849C65.9313 47.899 65.7599 47.6417 65.537 47.4333C65.3108 47.2962 65.0556 47.2141 64.7919 47.1935C64.5282 47.1729 64.2633 47.2145 64.0186 47.3149C62.3726 47.8505 60.6434 48.084 58.9143 48.0041V41.5429C58.9114 41.1278 58.7445 40.7306 58.45 40.4381C58.1554 40.1456 57.7571 39.9814 57.342 39.9814H55.4791C55.0806 38.5822 54.5204 37.2343 53.8099 35.9647L55.1237 34.651C55.2679 34.5107 55.383 34.3434 55.4625 34.1587C55.542 33.9739 55.5842 33.7752 55.5867 33.5741C55.5792 33.1684 55.413 32.7818 55.1237 32.4972L53.1207 30.4942L56.5667 27.0698C56.6456 26.9905 56.7081 26.8964 56.7506 26.793C56.7931 26.6895 56.8148 26.5786 56.8144 26.4668V20.684H60.2819C60.9855 20.684 61.6604 20.4052 62.1589 19.9088C62.6574 19.4123 62.9389 18.7385 62.9417 18.0349V11.7245C62.9389 11.0209 62.6574 10.3471 62.1589 9.85063C61.6604 9.35414 60.9855 9.07538 60.2819 9.07539H57.7081C57.4825 9.07539 57.2662 9.16502 57.1066 9.32456C56.9471 9.4841 56.8574 9.7005 56.8574 9.92612C56.8574 10.1517 56.9471 10.3681 57.1066 10.5277C57.2662 10.6872 57.4825 10.7768 57.7081 10.7768H60.2819C60.5342 10.7768 60.7764 10.8763 60.9558 11.0537C61.1352 11.2311 61.2374 11.4722 61.2403 11.7245V14.029H50.687V11.7245C50.687 11.4731 50.7868 11.2321 50.9646 11.0544C51.1423 10.8767 51.3833 10.7768 51.6346 10.7768H53.6592C53.8848 10.7768 54.1011 10.6872 54.2607 10.5277C54.4202 10.3681 54.5099 10.1517 54.5099 9.92612C54.5099 9.7005 54.4202 9.4841 54.2607 9.32456C54.1011 9.16502 53.8848 9.07539 53.6592 9.07539H51.6346C50.9321 9.07539 50.2582 9.35449 49.7614 9.85129C49.2646 10.3481 48.9856 11.0219 48.9856 11.7245V18.0349C48.9856 18.7375 49.2646 19.4113 49.7614 19.9081C50.2582 20.4049 50.9321 20.684 51.6346 20.684H55.1129V26.0683L51.8823 29.2989L49.8686 27.2852C49.5726 26.9945 49.1743 26.8316 48.7594 26.8316C48.3445 26.8316 47.9462 26.9945 47.6502 27.2852L46.3795 28.5236C45.1054 27.8148 43.754 27.2548 42.3521 26.8544V25.013C42.3521 24.596 42.1864 24.1961 41.8915 23.9013C41.5967 23.6064 41.1968 23.4408 40.7798 23.4408H37.9369V20.6086H41.4044C42.108 20.6086 42.7829 20.3299 43.2814 19.8334C43.7799 19.3369 44.0614 18.6631 44.0643 17.9595V11.6491C44.0614 10.9455 43.7799 10.2717 43.2814 9.77525C42.7829 9.27875 42.108 8.99999 41.4044 9H32.7895C32.0859 8.99999 31.411 9.27875 30.9125 9.77525C30.414 10.2717 30.1325 10.9455 30.1296 11.6491V17.9595C30.1296 18.3083 30.1985 18.6537 30.3323 18.9758C30.4661 19.2979 30.6622 19.5904 30.9093 19.8365C31.1565 20.0827 31.4498 20.2776 31.7724 20.4101C32.095 20.5425 32.4407 20.61 32.7895 20.6086H36.257V23.4408H33.4141C32.9971 23.4408 32.5972 23.6064 32.3023 23.9013C32.0075 24.1961 31.8418 24.596 31.8418 25.013V26.8544C30.4392 27.2528 29.0876 27.813 27.8144 28.5236L26.5006 27.2206C26.206 26.9273 25.8072 26.7626 25.3914 26.7626C24.9757 26.7626 24.5769 26.9273 24.2823 27.2206L22.2685 29.2343L19.0379 26.0037V20.6194H22.5054C23.209 20.6194 23.8839 20.3406 24.3824 19.8441C24.8809 19.3477 25.1624 18.6738 25.1653 17.9703V11.6599C25.1624 10.9563 24.8809 10.2825 24.3824 9.78603C23.8839 9.28953 23.209 9.01077 22.5054 9.01078H19.9425C19.7169 9.01078 19.5005 9.10041 19.3409 9.25995C19.1814 9.41949 19.0918 9.63585 19.0918 9.86148C19.0918 10.0871 19.1814 10.3035 19.3409 10.463C19.5005 10.6226 19.7169 10.7122 19.9425 10.7122H22.5054C22.7577 10.7122 22.9999 10.8117 23.1793 10.9891C23.3588 11.1665 23.461 11.4075 23.4638 11.6599V13.9644H12.9106V11.6599C12.9134 11.4075 13.0156 11.1665 13.195 10.9891C13.3744 10.8117 13.6166 10.7122 13.8689 10.7122H15.8935C16.1191 10.7122 16.3355 10.6226 16.495 10.463C16.6546 10.3035 16.7442 10.0871 16.7442 9.86148C16.7442 9.63585 16.6546 9.41949 16.495 9.25995C16.3355 9.10041 16.1191 9.01078 15.8935 9.01078H13.8689C13.1654 9.01077 12.4905 9.28953 11.992 9.78603C11.4934 10.2825 11.2119 10.9563 11.2091 11.6599V17.9703C11.2119 18.6738 11.4934 19.3477 11.992 19.8441C12.4905 20.3406 13.1654 20.6194 13.8689 20.6194H17.3042V26.4022C17.3038 26.514 17.3254 26.6249 17.3679 26.7284C17.4104 26.8318 17.4729 26.9259 17.5518 27.0052L20.9978 30.4296L18.9948 32.4326C18.7055 32.7172 18.5393 33.1038 18.5318 33.5095C18.5344 33.7106 18.5766 33.9092 18.656 34.094C18.7355 34.2788 18.8506 34.4461 18.9948 34.5863L20.2978 35.9001C19.5873 37.1696 19.0272 38.5176 18.6287 39.9168H16.7765C16.3614 39.9168 15.9631 40.081 15.6685 40.3735C15.374 40.666 15.2071 41.0632 15.2043 41.4783V48.0041C13.4742 48.0863 11.7439 47.849 10.0999 47.3041C9.8537 47.2086 9.5889 47.1706 9.32576 47.193C9.06261 47.2154 8.80806 47.2976 8.58153 47.4333C8.35446 47.6385 8.18014 47.8954 8.0733 48.1821C7.96646 48.4689 7.93024 48.7772 7.96771 49.081V65.3739C7.96771 65.5818 8.00867 65.7877 8.08822 65.9797C8.16777 66.1718 8.28439 66.3463 8.43138 66.4933C8.57837 66.6403 8.75288 66.7569 8.94493 66.8364C9.13699 66.916 9.34282 66.9569 9.5507 66.9569H64.5355C64.9553 66.9569 65.358 66.7901 65.6548 66.4933C65.9517 66.1964 66.1185 65.7938 66.1185 65.3739L66.1508 49.081ZM50.687 17.9595V15.6981H61.2403V18.0026C61.2347 18.2541 61.1316 18.4935 60.9528 18.6703C60.7739 18.8471 60.5334 18.9475 60.2819 18.9502H51.6669C51.5365 18.9532 51.4069 18.9296 51.2859 18.8807C51.1649 18.8319 51.0552 18.7588 50.9635 18.6661C50.8717 18.5733 50.7999 18.4628 50.7523 18.3413C50.7048 18.2198 50.6826 18.0899 50.687 17.9595ZM13.8689 18.9072C13.7436 18.9086 13.6192 18.8851 13.503 18.8381C13.3868 18.7911 13.281 18.7216 13.1919 18.6335C13.1028 18.5453 13.032 18.4404 12.9837 18.3247C12.9354 18.209 12.9105 18.0849 12.9106 17.9595V15.6981H23.4638V18.0026C23.4638 18.128 23.439 18.2521 23.3907 18.3678C23.3424 18.4834 23.2716 18.5884 23.1825 18.6765C23.0933 18.7647 22.9876 18.8342 22.8714 18.8812C22.7551 18.9282 22.6307 18.9517 22.5054 18.9502L13.8689 18.9072ZM32.7572 10.7014H41.3721C41.6244 10.7014 41.8666 10.8009 42.046 10.9783C42.2255 11.1558 42.3277 11.3968 42.3305 11.6491V13.9536H31.7772V11.6491C31.7801 11.3968 31.8823 11.1558 32.0617 10.9783C32.2412 10.8009 32.4833 10.7014 32.7356 10.7014H32.7572ZM32.7572 18.9072C32.6318 18.9086 32.5074 18.8851 32.3912 18.8381C32.275 18.7911 32.1693 18.7216 32.0801 18.6335C31.991 18.5453 31.9202 18.4404 31.8719 18.3247C31.8236 18.209 31.7987 18.0849 31.7988 17.9595V15.6981H42.3521V18.0026C42.3521 18.128 42.3272 18.2521 42.2789 18.3678C42.2306 18.4834 42.1598 18.5884 42.0707 18.6765C41.9815 18.7647 41.8758 18.8342 41.7596 18.8812C41.6434 18.9282 41.519 18.9517 41.3936 18.9502L32.7572 18.9072ZM16.938 41.6721H19.3179C19.508 41.671 19.6925 41.607 19.8425 41.4902C19.9924 41.3733 20.0996 41.2101 20.1471 41.026C20.558 39.3433 21.2264 37.7341 22.1285 36.2555C22.2377 36.0897 22.2847 35.8905 22.261 35.6934C22.2373 35.4962 22.1446 35.3139 21.9993 35.1786L20.3194 33.4987L25.3591 28.4805L27.039 30.1604C27.1776 30.3003 27.3594 30.3893 27.5549 30.4127C27.7504 30.4362 27.9482 30.3928 28.1159 30.2896C29.5997 29.3899 31.212 28.7218 32.8972 28.3082C33.0822 28.2624 33.2464 28.1557 33.3636 28.0054C33.4808 27.855 33.544 27.6696 33.5433 27.479V25.1099H40.6722V27.5436C40.6695 27.7347 40.732 27.921 40.8495 28.0718C40.9669 28.2225 41.1323 28.3287 41.3182 28.3728C43.0027 28.7888 44.6146 29.4568 46.0995 30.3542C46.2678 30.4555 46.4651 30.4979 46.6602 30.4745C46.8552 30.451 47.0368 30.3633 47.1764 30.225L48.8563 28.5451L53.8853 33.5633L52.2162 35.2432C52.0708 35.3785 51.9781 35.5609 51.9544 35.758C51.9308 35.9551 51.9777 36.1543 52.0869 36.3201C52.9834 37.8016 53.6514 39.4099 54.0684 41.0906C54.1142 41.2756 54.2208 41.4399 54.3712 41.5571C54.5216 41.6742 54.7069 41.7375 54.8975 41.7367H57.2666V47.8964C56.4209 47.7361 55.5918 47.4982 54.7899 47.1857L54.1545 46.8949C53.1195 46.453 52.0347 46.1384 50.9239 45.958C50.6493 42.485 49.0758 39.243 46.5171 36.8785C43.9583 34.5141 40.6024 33.201 37.1185 33.201C33.6346 33.201 30.2787 34.5141 27.7199 36.8785C25.1612 39.243 23.5876 42.485 23.3131 45.958C22.101 46.1479 20.9224 46.5106 19.8132 47.0349L19.1133 47.3149C18.4141 47.5724 17.6933 47.7671 16.9595 47.8964V41.7367L16.938 41.6721ZM9.70148 65.2985V60.6034C13.2834 61.4623 17.0428 61.1991 20.4701 59.8496H20.5348C22.1944 59.1199 23.9876 58.7431 25.8006 58.7431C27.6136 58.7431 29.4068 59.1199 31.0665 59.8496C32.9299 60.7065 34.9599 61.1404 37.0108 61.1203H37.1293C39.1837 61.1413 41.2174 60.7073 43.0843 59.8496C44.7434 59.1247 46.5343 58.7505 48.3448 58.7505C50.1553 58.7505 51.9463 59.1247 53.6053 59.8496H53.6699C57.0991 61.1909 60.8559 61.4539 64.4386 60.6034V65.2985H9.70148ZM9.70148 58.8374V54.9283C13.2834 55.7872 17.0428 55.524 20.4701 54.1745H20.5348C22.1944 53.4449 23.9876 53.0681 25.8006 53.0681C27.6136 53.0681 29.4068 53.4449 31.0665 54.1745C32.9299 55.0314 34.9599 55.4653 37.0108 55.4452H37.1293C39.1837 55.4662 41.2174 55.0323 43.0843 54.1745C44.7434 53.4496 46.5343 53.0754 48.3448 53.0754C50.1553 53.0754 51.9463 53.4496 53.6053 54.1745H53.6699C57.0991 55.5158 60.8559 55.7788 64.4386 54.9283V58.8374C61.1034 59.7076 57.5799 59.5156 54.3591 58.2881C52.4745 57.4422 50.4321 57.0049 48.3664 57.0049C46.3006 57.0049 44.2582 57.4422 42.3736 58.2881H42.309C40.6536 59.0053 38.8687 59.3754 37.0646 59.3754C35.2606 59.3754 33.4757 59.0053 31.8203 58.2881H31.7557C29.8692 57.442 27.8251 57.0046 25.7576 57.0046C23.69 57.0046 21.6459 57.442 19.7594 58.2881C16.5423 59.5154 13.0222 59.7074 9.6907 58.8374H9.70148ZM64.4493 53.173C61.1155 54.0557 57.5881 53.8635 54.3699 52.6238C52.4854 51.7773 50.443 51.3396 48.3771 51.3396C46.3113 51.3396 44.2688 51.7773 42.3844 52.6238H42.3198C40.6861 53.3604 38.9103 53.7281 37.1185 53.7007C35.316 53.7311 33.529 53.3634 31.8849 52.6238H31.8203C29.9339 51.7777 27.8897 51.3402 25.8222 51.3402C23.7546 51.3402 21.7105 51.7777 19.824 52.6238C16.6069 53.8511 13.0868 54.0431 9.75531 53.173V48.9948C13.2819 50.1659 17.1168 49.9934 20.524 48.5102H20.5886C22.2483 47.7805 24.0415 47.4037 25.8545 47.4037C27.6675 47.4037 29.4607 47.7805 31.1203 48.5102C32.3798 49.0988 33.7247 49.4841 35.1047 49.6517C35.2158 49.6665 35.3286 49.6594 35.4369 49.6306C35.5451 49.6019 35.6466 49.5521 35.7356 49.4841C35.8246 49.4161 35.8993 49.3312 35.9555 49.2343C36.0117 49.1374 36.0483 49.0304 36.0632 48.9194C36.078 48.8084 36.0708 48.6956 36.0421 48.5873C36.0133 48.4791 35.9635 48.3775 35.8955 48.2886C35.8275 48.1996 35.7427 48.1248 35.6458 48.0686C35.5489 48.0124 35.4419 47.9758 35.3309 47.961C34.1382 47.8221 32.9751 47.495 31.8849 46.9918H31.8203C29.6916 46.0626 27.3778 45.6352 25.0576 45.7427C25.367 42.7564 26.7722 39.9909 29.0018 37.9803C31.2314 35.9698 34.127 34.857 37.1293 34.857C40.1315 34.857 43.0271 35.9698 45.2567 37.9803C47.4863 39.9909 48.8915 42.7564 49.2009 45.7427C46.8803 45.6289 44.5651 46.0566 42.4382 46.9918H42.3736C41.2846 47.4881 40.1262 47.8149 38.9384 47.961C38.7142 47.991 38.5111 48.1088 38.3738 48.2886C38.2364 48.4683 38.1761 48.6952 38.2061 48.9194C38.2361 49.1436 38.3539 49.3467 38.5337 49.4841C38.7134 49.6214 38.9403 49.6817 39.1645 49.6517C40.5405 49.481 41.8813 49.0959 43.1382 48.5102C44.7972 47.7853 46.5882 47.4111 48.3987 47.4111C50.2092 47.4111 52.0001 47.7853 53.6592 48.5102L54.2622 48.7579C57.5398 50.0403 61.1627 50.1355 64.5032 49.0271V53.2053L64.4493 53.173Z",fill:"black"}),i.createElement("path",{d:"M50.6869 17.9595V15.6981H61.2402V18.0026C61.2346 18.2541 61.1315 18.4935 60.9527 18.6703C60.7738 18.8471 60.5333 18.9475 60.2818 18.9502H51.6668C51.5364 18.9532 51.4068 18.9296 51.2858 18.8807C51.1648 18.8319 51.0551 18.7588 50.9634 18.6661C50.8716 18.5733 50.7998 18.4628 50.7522 18.3413C50.7047 18.2198 50.6825 18.0899 50.6869 17.9595ZM13.8688 18.9072C13.7435 18.9086 13.6191 18.8851 13.5029 18.8381C13.3867 18.7911 13.2809 18.7216 13.1918 18.6335C13.1027 18.5453 13.0319 18.4404 12.9836 18.3247C12.9353 18.209 12.9104 18.0849 12.9105 17.9595V15.6981H23.4637V18.0026C23.4637 18.128 23.4389 18.2521 23.3906 18.3678C23.3423 18.4834 23.2715 18.5884 23.1824 18.6765C23.0932 18.7647 22.9875 18.8342 22.8713 18.8812C22.755 18.9282 22.6306 18.9517 22.5053 18.9502L13.8688 18.9072ZM32.7571 10.7014H41.372C41.6243 10.7014 41.8665 10.8009 42.0459 10.9783C42.2254 11.1558 42.3276 11.3968 42.3304 11.6491V13.9536H31.7771V11.6491C31.78 11.3968 31.8822 11.1558 32.0616 10.9783C32.2411 10.8009 32.4832 10.7014 32.7355 10.7014H32.7571ZM32.7571 18.9072C32.6317 18.9086 32.5073 18.8851 32.3911 18.8381C32.2749 18.7911 32.1692 18.7216 32.08 18.6335C31.9909 18.5453 31.9201 18.4404 31.8718 18.3247C31.8235 18.209 31.7986 18.0849 31.7987 17.9595V15.6981H42.352V18.0026C42.352 18.128 42.3271 18.2521 42.2788 18.3678C42.2305 18.4834 42.1597 18.5884 42.0706 18.6765C41.9814 18.7647 41.8757 18.8342 41.7595 18.8812C41.6433 18.9282 41.5189 18.9517 41.3935 18.9502L32.7571 18.9072ZM58.9142 48.0041V41.5429C58.9113 41.1278 58.7444 40.7306 58.4499 40.4381C58.1553 40.1456 57.757 39.9814 57.3419 39.9814H55.479C55.0805 38.5822 54.5203 37.2343 53.8098 35.9647L55.1236 34.651C55.2678 34.5107 55.3829 34.3434 55.4624 34.1587C55.5419 33.9739 55.5841 33.7752 55.5866 33.5741C55.5791 33.1684 55.4129 32.7818 55.1236 32.4972L53.1206 30.4942L56.5666 27.0698C56.6455 26.9905 56.708 26.8964 56.7505 26.793C56.793 26.6895 56.8147 26.5786 56.8143 26.4668V20.684H60.2818C60.9854 20.684 61.6603 20.4052 62.1588 19.9088C62.6573 19.4123 62.9388 18.7385 62.9416 18.0349V11.7245C62.9388 11.0209 62.6573 10.3471 62.1588 9.85063C61.6603 9.35414 60.9854 9.07538 60.2818 9.07539H57.7081C57.4824 9.07539 57.2661 9.16502 57.1065 9.32456C56.947 9.4841 56.8573 9.7005 56.8573 9.92612C56.8573 10.1517 56.947 10.3681 57.1065 10.5277C57.2661 10.6872 57.4824 10.7768 57.7081 10.7768H60.2818C60.5341 10.7768 60.7763 10.8763 60.9557 11.0537C61.1351 11.2311 61.2373 11.4722 61.2402 11.7245V14.029H50.6869V11.7245C50.6869 11.4731 50.7867 11.2321 50.9645 11.0544C51.1422 10.8767 51.3832 10.7768 51.6345 10.7768H53.6591C53.8847 10.7768 54.101 10.6872 54.2606 10.5277C54.4201 10.3681 54.5098 10.1517 54.5098 9.92612C54.5098 9.7005 54.4201 9.4841 54.2606 9.32456C54.101 9.16502 53.8847 9.07539 53.6591 9.07539H51.6345C50.932 9.07539 50.2581 9.35449 49.7613 9.85129C49.2645 10.3481 48.9855 11.0219 48.9855 11.7245V18.0349C48.9855 18.7375 49.2645 19.4113 49.7613 19.9081C50.2581 20.4049 50.932 20.684 51.6345 20.684H55.1128V26.0683L51.8822 29.2989L49.8685 27.2852C49.5725 26.9945 49.1742 26.8316 48.7593 26.8316C48.3444 26.8316 47.9461 26.9945 47.6501 27.2852L46.3794 28.5236C45.1053 27.8148 43.7539 27.2548 42.352 26.8544V25.013C42.352 24.596 42.1863 24.1961 41.8915 23.9013C41.5966 23.6064 41.1967 23.4408 40.7797 23.4408H37.9368V20.6086H41.4043C42.1079 20.6086 42.7828 20.3299 43.2813 19.8334C43.7798 19.3369 44.0613 18.6631 44.0642 17.9595V11.6491C44.0613 10.9455 43.7798 10.2717 43.2813 9.77525C42.7828 9.27875 42.1079 8.99999 41.4043 9H32.7894C32.0858 8.99999 31.4109 9.27875 30.9124 9.77525C30.4139 10.2717 30.1324 10.9455 30.1295 11.6491V17.9595C30.1295 18.3083 30.1984 18.6537 30.3322 18.9758C30.466 19.2979 30.6621 19.5904 30.9092 19.8365C31.1564 20.0827 31.4497 20.2776 31.7723 20.4101C32.0949 20.5425 32.4406 20.61 32.7894 20.6086H36.2569V23.4408H33.414C32.997 23.4408 32.5971 23.6064 32.3022 23.9013C32.0074 24.1961 31.8417 24.596 31.8417 25.013V26.8544C30.4391 27.2528 29.0875 27.813 27.8143 28.5236L26.5005 27.2206C26.2059 26.9273 25.8071 26.7626 25.3913 26.7626C24.9756 26.7626 24.5768 26.9273 24.2822 27.2206L22.2684 29.2343L19.0378 26.0037V20.6194H22.5053C23.2089 20.6194 23.8838 20.3406 24.3823 19.8441C24.8808 19.3477 25.1623 18.6738 25.1652 17.9703V11.6599C25.1623 10.9563 24.8808 10.2825 24.3823 9.78603C23.8838 9.28953 23.2089 9.01077 22.5053 9.01078H19.9424C19.7168 9.01078 19.5004 9.10041 19.3408 9.25995C19.1813 9.41949 19.0917 9.63585 19.0917 9.86148C19.0917 10.0871 19.1813 10.3035 19.3408 10.463C19.5004 10.6226 19.7168 10.7122 19.9424 10.7122H22.5053C22.7576 10.7122 22.9998 10.8117 23.1792 10.9891C23.3587 11.1665 23.4609 11.4075 23.4637 11.6599V13.9644H12.9105V11.6599C12.9133 11.4075 13.0155 11.1665 13.1949 10.9891C13.3743 10.8117 13.6165 10.7122 13.8688 10.7122H15.8934C16.119 10.7122 16.3354 10.6226 16.4949 10.463C16.6545 10.3035 16.7441 10.0871 16.7441 9.86148C16.7441 9.63585 16.6545 9.41949 16.4949 9.25995C16.3354 9.10041 16.119 9.01078 15.8934 9.01078H13.8688C13.1653 9.01077 12.4904 9.28953 11.9919 9.78603C11.4933 10.2825 11.2118 10.9563 11.209 11.6599V17.9703C11.2118 18.6738 11.4933 19.3477 11.9919 19.8441C12.4904 20.3406 13.1653 20.6194 13.8688 20.6194H17.304V26.4022C17.3037 26.514 17.3253 26.6249 17.3678 26.7284C17.4103 26.8318 17.4728 26.9259 17.5517 27.0052L20.9977 30.4296L18.9947 32.4326C18.7054 32.7172 18.5392 33.1038 18.5317 33.5095C18.5343 33.7106 18.5765 33.9092 18.6559 34.094C18.7354 34.2788 18.8505 34.4461 18.9947 34.5863L20.2977 35.9001C19.5872 37.1696 19.0271 38.5176 18.6286 39.9168H16.7764C16.3613 39.9168 15.963 40.081 15.6684 40.3735C15.3739 40.666 15.207 41.0632 15.2041 41.4783V48.0041L16.9056 47.7887V41.629H19.2855C19.4756 41.6279 19.6601 41.564 19.81 41.4471C19.96 41.3302 20.0671 41.167 20.1147 40.9829C20.5256 39.3002 21.194 37.6911 22.0961 36.2124C22.2053 36.0466 22.2522 35.8475 22.2286 35.6503C22.2049 35.4532 22.1122 35.2708 21.9669 35.1355L20.287 33.4556L25.3267 28.4374L27.0066 30.1174C27.1452 30.2573 27.327 30.3462 27.5225 30.3697C27.718 30.3931 27.9158 30.3498 28.0835 30.2466C29.5673 29.3469 31.1796 28.6787 32.8648 28.2651C33.0498 28.2193 33.214 28.1126 33.3312 27.9623C33.4484 27.8119 33.5116 27.6266 33.5109 27.4359V25.0669H40.6398V27.5436C40.637 27.7347 40.6996 27.921 40.817 28.0718C40.9345 28.2225 41.0999 28.3287 41.2858 28.3728C42.9703 28.7888 44.5822 29.4568 46.0671 30.3542C46.2354 30.4555 46.4327 30.4979 46.6277 30.4745C46.8228 30.451 47.0044 30.3633 47.144 30.225L48.8239 28.5451L53.8529 33.5633L52.1838 35.2432C52.0384 35.3785 51.9457 35.5609 51.922 35.758C51.8984 35.9551 51.9453 36.1543 52.0545 36.3201C52.951 37.8016 53.6189 39.4099 54.0359 41.0906C54.0818 41.2756 54.1884 41.4399 54.3388 41.5571C54.4892 41.6742 54.6745 41.7375 54.8651 41.7367H57.2342V47.8964M50.913 45.958C50.6385 42.485 49.0649 39.243 46.5062 36.8785C43.9474 34.5141 40.5916 33.201 37.1076 33.201C33.6237 33.201 30.2678 34.5141 27.709 36.8785C25.1503 39.243 23.5768 42.485 23.3022 45.958L25.0359 45.775C25.3453 42.7887 26.7506 40.0232 28.9802 38.0126C31.2098 36.0021 34.1054 34.8893 37.1076 34.8893C40.1098 34.8893 43.0055 36.0021 45.2351 38.0126C47.4646 40.0232 48.8699 42.7887 49.1793 45.775",fill:"#0DB1F9"})),i.createElement("defs",null,i.createElement("clipPath",{id:"rich_platform"},i.createElement("rect",{width:"58.1508",height:58,fill:"white",transform:"translate(8 9)"})))),O=()=>i.createElement("svg",{width:75,height:75,viewBox:"0 0 75 75",fill:"none",xmlns:"http://www.w3.org/2000/svg"},i.createElement("rect",{opacity:"0.5",width:75,height:75,rx:"8.57143",fill:"#CEEEF7"}),i.createElement("path",{d:"M12.8003 12.7999H14.7003V14.6999H12.8003V12.7999Z",fill:"#1C1E21"}),i.createElement("path",{d:"M16.5999 12.7999H18.4999V14.6999H16.5999V12.7999Z",fill:"#1C1E21"}),i.createElement("path",{d:"M20.3999 12.7999H22.2999V14.6999H20.3999V12.7999Z",fill:"#1C1E21"}),i.createElement("path",{d:"M62.1998 42.2499C62.1998 42.7745 61.7744 43.1999 61.2498 43.1999C60.7252 43.1999 60.2998 42.7745 60.2998 42.2499C60.2998 41.7253 60.7252 41.2999 61.2498 41.2999C61.7744 41.2999 62.1998 41.7253 62.1998 42.2499Z",fill:"#1C1E21"}),i.createElement("path",{d:"M62.1998 51.75C62.1998 52.2747 61.7744 52.7 61.2498 52.7C60.7252 52.7 60.2998 52.2747 60.2998 51.75C60.2998 51.2254 60.7252 50.8 61.2498 50.8C61.7744 50.8 62.1998 51.2254 62.1998 51.75Z",fill:"#1C1E21"}),i.createElement("path",{d:"M66 38.45C66 36.835 64.765 35.6 63.15 35.6H58.4V9H9V58.4H41.3V63.15C41.3 64.765 42.535 66 44.15 66H63.15C64.765 66 66 64.765 66 63.15V57.45C66 56.69 65.715 56.025 65.24 55.55C65.715 55.075 66 54.41 66 53.65V47.95C66 47.19 65.715 46.525 65.24 46.05C65.715 45.575 66 44.91 66 44.15V38.45ZM56.5 10.9V16.6H10.9V10.9H56.5ZM10.9 18.5H56.5V35.6H44.15C42.535 35.6 41.3 36.835 41.3 38.45V44.15C41.3 44.91 41.585 45.575 42.06 46.05C41.585 46.525 41.3 47.19 41.3 47.95V53.65C41.3 54.41 41.585 55.075 42.06 55.55C41.775 55.835 41.585 56.12 41.49 56.5H10.9V18.5ZM64.1 63.15C64.1 63.72 63.72 64.1 63.15 64.1H44.15C43.58 64.1 43.2 63.72 43.2 63.15V57.45C43.2 56.88 43.58 56.5 44.15 56.5H63.15C63.72 56.5 64.1 56.88 64.1 57.45V63.15ZM64.1 53.65C64.1 54.22 63.72 54.6 63.15 54.6H44.15C43.58 54.6 43.2 54.22 43.2 53.65V47.95C43.2 47.38 43.58 47 44.15 47H63.15C63.72 47 64.1 47.38 64.1 47.95V53.65ZM64.1 44.15C64.1 44.72 63.72 45.1 63.15 45.1H44.15C43.58 45.1 43.2 44.72 43.2 44.15V38.45C43.2 37.88 43.58 37.5 44.15 37.5H63.15C63.72 37.5 64.1 37.88 64.1 38.45V44.15Z",fill:"#1C1E21"}),i.createElement("path",{d:"M62.1998 61.25C62.1998 61.7747 61.7744 62.2 61.2498 62.2C60.7252 62.2 60.2998 61.7747 60.2998 61.25C60.2998 60.7254 60.7252 60.3 61.2498 60.3C61.7744 60.3 62.1998 60.7254 62.1998 61.25Z",fill:"#1C1E21"}),i.createElement("line",{x1:14,y1:24,x2:47,y2:24,stroke:"#0DB1F9",strokeWidth:2}),i.createElement("line",{x1:14,y1:33,x2:47,y2:33,stroke:"#0DB1F9",strokeWidth:2}),i.createElement("line",{x1:14,y1:42,x2:"41.3",y2:42,stroke:"#0DB1F9",strokeWidth:2}),i.createElement("line",{x1:14,y1:51,x2:"41.3",y2:51,stroke:"#0DB1F9",strokeWidth:2}),i.createElement("g",{clipPath:"url(#clip0_705_7413)"},i.createElement("path",{d:"M12.8 12.8H14.7V14.7H12.8V12.8Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M16.6001 12.8H18.5001V14.7H16.6001V12.8Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M20.3999 12.8H22.2999V14.7H20.3999V12.8Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M56.5 10.9V16.6H10.9V10.9H56.5ZM58.4 35.6V9H9V58.4H41.3L41.49 56.5H10.9V18.5H56.5V35.6",fill:"#0DB1F9"})),i.createElement("defs",null,i.createElement("clipPath",{id:"clip0_705_7413"},i.createElement("rect",{width:"49.4",height:"49.4",fill:"white",transform:"translate(9 9)"})))),U=()=>i.createElement("svg",{width:75,height:75,viewBox:"0 0 75 75",fill:"none",xmlns:"http://www.w3.org/2000/svg"},i.createElement("rect",{opacity:"0.5",width:75,height:75,rx:"8.57143",fill:"#CEEEF7"}),i.createElement("path",{d:"M50.7302 40.7277H23.4577C22.0472 40.7277 20.9187 41.8562 20.9187 43.2667V48.5331C20.9187 49.1915 21.1069 49.6617 21.483 50.1319C21.1069 50.602 20.9187 51.1663 20.9187 51.7306V56.997C20.9187 57.6554 21.1069 58.1256 21.483 58.5958C21.1069 59.0659 20.9187 59.6302 20.9187 60.1945V65.4609C20.9187 66.8715 22.0473 68 23.4577 68H50.7302C52.1407 68 53.2692 66.8714 53.2692 65.4609V60.1945C53.2692 59.5361 53.081 59.0659 52.7049 58.5958C53.081 58.1256 53.2692 57.5613 53.2692 56.997V51.7306C53.2692 51.0722 53.081 50.602 52.7049 50.1319C53.081 49.6617 53.2692 49.0974 53.2692 48.5331V43.2667C53.2696 41.9504 52.1411 40.7277 50.7302 40.7277ZM22.7988 43.2667C22.7988 42.8907 23.0808 42.6083 23.4572 42.6083H50.7296C51.1057 42.6083 51.3881 42.8902 51.3881 43.2667V48.5331C51.3881 48.9092 51.1061 49.1915 50.7296 49.1915H23.4572C23.0812 49.1915 22.7988 48.9096 22.7988 48.5331V43.2667ZM51.3879 51.8247V57.0911C51.3879 57.4672 51.106 57.7495 50.7295 57.7495H23.4571C23.0811 57.7495 22.7987 57.4676 22.7987 57.0911V51.8247C22.7987 51.4487 23.0806 51.1663 23.4571 51.1663H50.7295C51.106 51.1663 51.3879 51.4487 51.3879 51.8247ZM51.3879 65.6493C51.3879 66.0254 51.106 66.3077 50.7295 66.3077H23.4571C23.0811 66.3077 22.7987 66.0258 22.7987 65.6493V60.3829C22.7987 60.0069 23.0806 59.7245 23.4571 59.7245H50.7295C51.1056 59.7245 51.3879 60.0064 51.3879 60.3829V65.6493Z",fill:"black"}),i.createElement("path",{d:"M26.9374 47.9688C27.5017 47.9688 27.8777 47.5928 27.8777 47.0285V44.8655C27.8777 44.3012 27.5017 43.9252 26.9374 43.9252C26.3731 43.9252 25.9971 44.3012 25.9971 44.8655V47.0285C25.9971 47.5928 26.4672 47.9688 26.9374 47.9688Z",fill:"black"}),i.createElement("path",{d:"M31.1693 47.9688C31.7336 47.9688 32.1097 47.5928 32.1097 47.0285V44.8655C32.1097 44.3012 31.7336 43.9252 31.1693 43.9252C30.6051 43.9252 30.229 44.3012 30.229 44.8655V47.0285C30.229 47.5928 30.6992 47.9688 31.1693 47.9688Z",fill:"black"}),i.createElement("path",{d:"M26.9374 56.5268C27.5017 56.5268 27.8777 56.1508 27.8777 55.5865V53.4235C27.8777 52.8592 27.5017 52.4832 26.9374 52.4832C26.3731 52.4832 25.9971 52.8592 25.9971 53.4235V55.5865C25.9971 56.0567 26.4672 56.5268 26.9374 56.5268Z",fill:"black"}),i.createElement("path",{d:"M31.1693 56.5268C31.7336 56.5268 32.1097 56.1508 32.1097 55.5865V53.4235C32.1097 52.8592 31.7336 52.4832 31.1693 52.4832C30.6051 52.4832 30.229 52.8592 30.229 53.4235V55.5865C30.229 56.0567 30.6992 56.5268 31.1693 56.5268Z",fill:"black"}),i.createElement("path",{d:"M26.9374 60.9473C26.3731 60.9473 25.9971 61.3233 25.9971 61.8876V64.0506C25.9971 64.6149 26.3731 64.9909 26.9374 64.9909C27.5017 64.9909 27.8777 64.6149 27.8777 64.0506V61.8876C27.8777 61.4174 27.5017 60.9473 26.9374 60.9473Z",fill:"black"}),i.createElement("path",{d:"M31.1693 60.9473C30.6051 60.9473 30.229 61.3233 30.229 61.8876V64.0506C30.229 64.6149 30.6051 64.9909 31.1693 64.9909C31.7336 64.9909 32.1097 64.6149 32.1097 64.0506V61.8876C32.1097 61.4174 31.7336 60.9473 31.1693 60.9473Z",fill:"black"}),i.createElement("path",{d:"M64.6485 8H53.0815C51.671 8 50.5425 9.12858 50.5425 10.539V17.1222C50.5425 18.5328 51.6711 19.6613 53.0815 19.6613H64.6485C66.059 19.6613 67.1875 18.5327 67.1875 17.1222V10.539C67.188 9.12849 66.0594 8 64.6485 8ZM65.3069 17.1224C65.3069 17.4984 65.025 17.7808 64.6485 17.7808H53.0815C52.7055 17.7808 52.4231 17.4988 52.4231 17.1224V10.5392C52.329 10.1631 52.7055 9.88119 53.0815 9.88119H64.6485C65.025 9.88119 65.3069 10.1631 65.3069 10.5396V17.1224Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M31.2632 19.6619H42.8301C44.2407 19.6619 45.3692 18.5333 45.3692 17.1228V10.5396C45.3692 9.12906 44.2406 8.00057 42.8301 8.00057L31.2632 8.00012C29.8526 8.00012 28.7241 9.1287 28.7241 10.5392V17.1224C28.7241 18.5333 29.8527 19.6619 31.2632 19.6619V19.6619ZM30.6048 10.5395C30.6048 10.1634 30.8867 9.88109 31.2632 9.88109H42.8301C43.2062 9.88109 43.4885 10.163 43.4885 10.5395V17.1227C43.4885 17.4987 43.2066 17.7811 42.8301 17.7811L31.2632 17.7807C30.8871 17.7807 30.6048 17.4987 30.6048 17.1223V10.5395Z",fill:"black"}),i.createElement("path",{d:"M9.5395 19.6619H21.1065C22.517 19.6619 23.6455 18.5333 23.6455 17.1228V10.5396C23.7396 9.12873 22.5169 8.00012 21.1065 8.00012H9.5395C8.1286 8.00012 7 9.1287 7 10.5396V17.1228C7 18.5334 8.12858 19.6619 9.5395 19.6619ZM8.88109 10.5395C8.88109 10.163 9.16302 9.8811 9.5395 9.8811H21.1065C21.4825 9.8811 21.7649 10.163 21.7649 10.5395V17.1227C21.7649 17.4988 21.4829 17.7811 21.1065 17.7811L9.5395 17.7807C9.16345 17.7807 8.88109 17.4987 8.88109 17.1223V10.5395Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M22 9.5H8.5V18H22V9.5Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M65.5 9.5H52V18H65.5V9.5Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M22 24H8.5V32.5H22V24Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M65.5 24H52V32.5H65.5V24Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M64.6485 22.4825H53.0815C51.671 22.4825 50.5425 23.6111 50.5425 25.0216V31.6048C50.5425 33.0153 51.6711 34.1438 53.0815 34.1438H64.6485C66.059 34.1438 67.1875 33.0153 67.1875 31.6048V25.0216C67.188 23.611 66.0594 22.4825 64.6485 22.4825ZM65.3069 31.6049C65.3069 31.981 65.025 32.2633 64.6485 32.2633H53.0815C52.7055 32.2633 52.4231 31.9814 52.4231 31.6049V25.0217C52.4231 24.6457 52.705 24.3633 53.0815 24.3633H64.6485C65.0245 24.3633 65.3069 24.6452 65.3069 25.0217V31.6049Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M31.2632 34.2379H42.8301C44.2407 34.2379 45.3692 33.1094 45.3692 31.6989V25.1157C45.3692 23.7052 44.2406 22.5767 42.8301 22.5767H31.2632C29.8526 22.5767 28.7241 23.7052 28.7241 25.1157V31.6989C28.7241 33.0157 29.8527 34.2379 31.2632 34.2379ZM30.6048 25.0219C30.6048 24.6459 30.8867 24.3635 31.2632 24.3635H42.8301C43.2062 24.3635 43.4885 24.6455 43.4885 25.0219V31.6051C43.4885 31.9812 43.2066 32.2636 42.8301 32.2636L31.2632 32.2631C30.8871 32.2631 30.6048 31.9812 30.6048 31.6047V25.0219Z",fill:"black"}),i.createElement("path",{d:"M18.1911 53.9882H16.3104V36.8725C16.3104 36.3082 15.9343 35.9321 15.37 35.9321C14.8057 35.9321 14.4297 36.3082 14.4297 36.8725V54.8344C14.4297 55.3987 14.8057 55.7748 15.37 55.7748H18.1915C18.7558 55.7748 19.1318 55.3987 19.1318 54.8344C19.1318 54.2706 18.7553 53.9882 18.191 53.9882L18.1911 53.9882Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M58.8175 35.9314C58.2532 35.9314 57.8772 36.3074 57.8772 36.8717V53.8939H55.9965C55.4322 53.8939 55.0562 54.2699 55.0562 54.8342C55.0562 55.3985 55.4322 55.7745 55.9965 55.7745H58.8179C59.3822 55.7745 59.7583 55.3985 59.7583 54.8342L59.7578 36.8722C59.7578 36.4021 59.3818 35.9319 58.8175 35.9319L58.8175 35.9314Z",fill:"#0DB1F9"}),i.createElement("path",{d:"M23.7398 31.6056V25.0224C23.7398 23.6119 22.6113 22.4834 21.2008 22.4834H9.53904C8.12849 22.4834 7 23.612 7 25.0224V31.6056C7 33.0162 8.12858 34.1447 9.53904 34.1447H21.106C22.5166 34.2388 23.7393 33.0165 23.7393 31.6056H23.7398ZM8.88059 31.6056V25.0224C8.88059 24.6464 9.16252 24.364 9.539 24.364L21.106 24.3645C21.482 24.3645 21.7644 24.6464 21.7644 25.0229V31.6061C21.7644 31.9821 21.4824 32.2645 21.106 32.2645L9.539 32.2641C9.1625 32.3582 8.88059 31.9817 8.88059 31.6056Z",fill:"#0DB1F9"})),W=e=>{let{data:a}=e;const t=a.icon,{title:n,description:o,link:r}=a;return i.createElement("div",{className:N},i.createElement(f.A,{to:r},a.icon&&i.createElement(t,null),i.createElement("h4",{className:M},n),i.createElement("p",null,o)))},_=()=>{const e=[{icon:E,title:"Mutability support for all data lake workloads",description:"Quickly update & delete data with Hudi\u2019s fast, pluggable indexing. This includes streaming workloads, with full support for out-of-order data, bursty traffic & data deduplication.",link:"/docs/indexing"},{icon:L,title:"Improved efficiency by incrementally processing new data",description:"Replace old-school batch pipelines with incremental streaming on your data lake. Experience faster ingestion and lower processing times for analytical workloads.",link:"/blog/2020/08/18/hudi-incremental-processing-on-data-lakes"},{icon:R,title:"ACID Transactional guarantees to your data lake",description:"Bring transactional guarantees to your data lake, with consistent, atomic writes and concurrency controls tailored for longer-running lake transactions.",link:"/docs/use_cases/#acid-transactions"},{icon:B,title:"Unlock historical data with time travel",description:"Query historical data with the ability to roll back to a table version; debug data versions to understand what changed over time; audit data changes by viewing the commit history.",link:"/docs/use_cases/#time-travel"},{icon:z,title:"Interoperable multi-cloud ecosystem support",description:"Extensive ecosystem support with plug-and-play options for popular data sources & query engines. Build future-proof architectures interoperable with your vendor of choice.",link:"/docs/cloud"},{icon:P,title:"Comprehensive table services for high-performance analytics",description:"Fully automated table services that continuously schedule & orchestrate clustering, compaction, cleaning, file sizing & indexing to ensure tables are always ready.",link:"/blog/2021/07/21/streaming-data-lake-platform/#table-services"},{icon:F,title:"A rich platform to build your lakehouse faster",description:"Effortlessly build your lakehouse with built-in tools for auto ingestion from services like Debezium and Kafka and auto catalog sync for easy discoverability & more.",link:"/blog/2022/01/14/change-data-capture-with-debezium-and-apache-hudi"},{icon:O,title:"Query acceleration through multi-modal indexes.",description:"Experience faster write transactions on huge/wide tables & faster query performance with first-of-its kind multi-modal indexing subsystem.",link:"/blog/2022/05/17/Introducing-Multi-Modal-Index-for-the-Lakehouse-in-Apache-Hudi"},{icon:U,title:"Resilient Pipelines with schema evolution & enforcement",description:"Easily change the current schema of a Hudi table to adapt to the data that is changing over time and ensure pipeline resilience by failing fast and avoiding data corruption.",link:"/docs/schema_evolution"}];return i.createElement("section",{className:x},i.createElement("div",{className:"container"},i.createElement("div",{className:T},i.createElement(c,{primaryText:"Hudi",secondaryText:"Features"})),i.createElement("div",{className:I},e.map(((e,a)=>i.createElement(W,{key:a,data:e}))))))},q="joinCommunityWrapper_vkSw",Z="communityContent_gy8z",G="communityDescription_Kql-",V="communityCardWrapper_7xyQ",j="communityCardChildWrapper_rvog",X="communityCard_nL8J",Q="mediaTitle_nHt1",K="title_JQCg",Y="linkText_2Pqi",J="leftSideWrapper_MMA7";var $;function ee(){return ee=Object.assign?Object.assign.bind():function(e){for(var a=1;a<arguments.length;a++){var t=arguments[a];for(var i in t)({}).hasOwnProperty.call(t,i)&&(e[i]=t[i])}return e},ee.apply(null,arguments)}const ae=e=>{let{title:a,titleId:t,...n}=e;return i.createElement("svg",ee({width:18,height:18,viewBox:"0 0 18 18",fill:"none",xmlns:"http://www.w3.org/2000/svg","aria-labelledby":t},n),a?i.createElement("title",{id:t},a):null,$||($=i.createElement("path",{d:"M6.375 3.75 11.625 9l-5.25 5.25",stroke:"#0DB1F9",strokeWidth:1.5,strokeLinecap:"round",strokeLinejoin:"round"})))},te=e=>{let{media:a}=e;const t=a.icon,{title:n,linkText:o,url:r}=a;return i.createElement("div",{className:X,onClick:()=>window.open(r,"_blank")},i.createElement(t,null),i.createElement("div",{className:Q},i.createElement("p",{className:K},n),i.createElement("p",{className:Y},o," ",i.createElement(ae,null))))};var ie;function ne(){return ne=Object.assign?Object.assign.bind():function(e){for(var a=1;a<arguments.length;a++){var t=arguments[a];for(var i in t)({}).hasOwnProperty.call(t,i)&&(e[i]=t[i])}return e},ne.apply(null,arguments)}const oe=e=>{let{title:a,titleId:t,...n}=e;return i.createElement("svg",ne({width:60,height:59,viewBox:"0 0 60 59",fill:"none",xmlns:"http://www.w3.org/2000/svg","aria-labelledby":t},n),a?i.createElement("title",{id:t},a):null,ie||(ie=i.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M30.015 0a30.004 30.004 0 0 0-9.485 58.474c1.49.279 2.047-.65 2.047-1.44 0-.716-.025-3.081-.04-5.59-8.347 1.818-10.106-3.538-10.106-3.538-1.366-3.478-3.334-4.392-3.334-4.392-2.722-1.863.21-1.823.21-1.823 3.01.213 4.595 3.09 4.595 3.09 2.678 4.591 7.02 3.264 8.735 2.484.268-1.937 1.043-3.259 1.903-4.01-6.663-.76-13.669-3.333-13.669-14.83a11.576 11.576 0 0 1 3.09-8.05c-.312-.76-1.34-3.81.289-7.949 0 0 2.519-.805 8.253 3.076a28.498 28.498 0 0 1 15.03 0c5.723-3.88 8.242-3.076 8.242-3.076 1.63 4.134.606 7.185.293 7.95a11.576 11.576 0 0 1 3.086 8.049c0 11.527-7.016 14.066-13.699 14.806 1.079.93 2.038 2.758 2.038 5.555 0 4.014-.035 7.249-.035 8.238 0 .794.536 1.734 2.057 1.435A30.006 30.006 0 0 0 30.015 0Z",fill:"#191717"})))};var re,se,le,de;function ce(){return ce=Object.assign?Object.assign.bind():function(e){for(var a=1;a<arguments.length;a++){var t=arguments[a];for(var i in t)({}).hasOwnProperty.call(t,i)&&(e[i]=t[i])}return e},ce.apply(null,arguments)}const pe=e=>{let{title:a,titleId:t,...n}=e;return i.createElement("svg",ce({width:61,height:60,viewBox:"0 0 61 60",fill:"none",xmlns:"http://www.w3.org/2000/svg","aria-labelledby":t},n),a?i.createElement("title",{id:t},a):null,re||(re=i.createElement("path",{d:"M14.265 37.388c0 3.237-2.645 5.882-5.882 5.882-3.238 0-5.883-2.645-5.883-5.883 0-3.237 2.645-5.882 5.883-5.882h5.882v5.883ZM17.23 37.388c0-3.238 2.645-5.883 5.883-5.883 3.237 0 5.882 2.645 5.882 5.883v14.729c0 3.238-2.645 5.883-5.883 5.883-3.237 0-5.882-2.645-5.882-5.883v-14.73Z",fill:"#E01E5A"})),se||(se=i.createElement("path",{d:"M23.113 13.765c-3.238 0-5.883-2.645-5.883-5.882C17.23 4.645 19.875 2 23.113 2c3.237 0 5.882 2.645 5.882 5.883v5.882h-5.883ZM23.112 16.729c3.238 0 5.883 2.645 5.883 5.882 0 3.238-2.645 5.883-5.883 5.883H8.382c-3.237 0-5.882-2.645-5.882-5.883 0-3.237 2.645-5.882 5.883-5.882h14.73Z",fill:"#36C5F0"})),le||(le=i.createElement("path",{d:"M46.736 22.611c0-3.237 2.645-5.882 5.882-5.882 3.238 0 5.883 2.645 5.883 5.882 0 3.238-2.645 5.883-5.883 5.883h-5.882v-5.883ZM43.771 22.612c0 3.238-2.645 5.883-5.883 5.883-3.237 0-5.882-2.645-5.882-5.883V7.882C32.006 4.646 34.65 2 37.888 2c3.238 0 5.883 2.645 5.883 5.883v14.73Z",fill:"#2EB67D"})),de||(de=i.createElement("path",{d:"M37.888 46.235c3.238 0 5.883 2.645 5.883 5.882 0 3.238-2.645 5.883-5.883 5.883-3.237 0-5.882-2.645-5.882-5.883v-5.882h5.882ZM37.888 43.27c-3.237 0-5.882-2.645-5.882-5.883 0-3.237 2.645-5.882 5.882-5.882h14.73c3.238 0 5.883 2.645 5.883 5.883 0 3.237-2.645 5.882-5.883 5.882h-14.73Z",fill:"#ECB22E"})))};var ge,ue,me,he,be;function ye(){return ye=Object.assign?Object.assign.bind():function(e){for(var a=1;a<arguments.length;a++){var t=arguments[a];for(var i in t)({}).hasOwnProperty.call(t,i)&&(e[i]=t[i])}return e},ye.apply(null,arguments)}const fe=e=>{let{title:a,titleId:t,...n}=e;return i.createElement("svg",ye({xmlns:"http://www.w3.org/2000/svg",width:60,height:60,viewBox:"0 0 255 255","aria-labelledby":t},n),a?i.createElement("title",{id:t},a):null,ge||(ge=i.createElement("path",{d:"M0 0h225v225H0V0Z",fill:"#007DBA"})),ue||(ue=i.createElement("path",{d:"M177.25 87.813C186.847 95.445 191.633 103.85 194 116c.167 2.728.255 5.346.227 8.068v2.31a1293.7 1293.7 0 0 1-.032 7.466l-.008 5.212c-.008 4.55-.027 9.1-.05 13.651-.024 5.472-.033 10.944-.045 16.415-.019 8.293-.057 16.585-.092 24.878h-34l-.113-13.902c-.03-2.922-.062-5.843-.096-8.764-.054-4.628-.105-9.256-.137-13.884-.027-3.735-.067-7.47-.117-11.204a586.665 586.665 0 0 1-.035-4.257c.02-13.428.02-13.428-6.002-24.989-5.357-4.286-11.953-3.53-18.5-3-3.783.92-6.059 2.686-8.362 5.902-4.266 7.652-4.372 14.985-4.345 23.512-.013 1.466-.028 2.931-.046 4.396-.04 3.824-.05 7.648-.054 11.472-.01 4.606-.059 9.211-.1 13.817-.06 6.967-.072 13.933-.093 20.901H89V85h33l1 12 3.25-3.625c14.02-14.094 34.284-14.885 51-5.563ZM34 85h34v109H34V85Z",fill:"#FBFCFD"})),me||(me=i.createElement("path",{d:"M61.313 33.625C66.448 36.998 69.04 40.122 71 46c.741 6.843.09 12.349-4 18-3.799 4.09-8.066 6.583-13.613 7.371C46.564 71.54 42.212 70.497 37 66c-4.142-4.36-6.067-8.324-6.375-14.313.164-5.81 2.052-10.204 6.023-14.449 7.548-6.806 15.539-8.754 24.664-3.613Z",fill:"#FBFDFD"})),he||(he=i.createElement("path",{d:"M224 204h1v21h-21c4-2 4-2 5.93-2.79 5.716-2.455 9.018-5.8 12.07-11.21.923-2.357 1.481-4.51 2-7Z",fill:"#F9FBFD"})),be||(be=i.createElement("path",{d:"M0 204c2 4 2 4 2.79 5.93 2.915 6.785 7.572 10.403 14.21 13.445l2 .625v1H0v-21ZM204 0h21v19c-2.007-2.007-2.564-3.192-3.625-5.75-2.646-5.41-6.85-8.869-12.45-11.063-1.62-.473-3.272-.843-4.925-1.187V0ZM0 0h21c-4 2-4 2-5.93 2.79C8.285 5.704 4.667 10.361 1.625 17L1 19H0V0Z",fill:"#FAFCFD"})))};var we;function ke(){return ke=Object.assign?Object.assign.bind():function(e){for(var a=1;a<arguments.length;a++){var t=arguments[a];for(var i in t)({}).hasOwnProperty.call(t,i)&&(e[i]=t[i])}return e},ke.apply(null,arguments)}const ve=e=>{let{title:a,titleId:t,...n}=e;return i.createElement("svg",ke({width:60,height:49,viewBox:"0 0 60 49",fill:"none",xmlns:"http://www.w3.org/2000/svg","aria-labelledby":t},n),a?i.createElement("title",{id:t},a):null,we||(we=i.createElement("path",{d:"M53.857 12.05a22.7 22.7 0 0 1 .037 1.58c0 16.149-12.383 34.773-35.025 34.773v-.01A35.032 35.032 0 0 1 0 42.916a24.833 24.833 0 0 0 18.218-5.065c-5.267-.1-9.886-3.51-11.5-8.487 1.845.353 3.746.28 5.557-.21C6.533 28 2.401 22.99 2.401 17.172v-.155a12.3 12.3 0 0 0 5.587 1.53C2.578 14.958.912 7.815 4.178 2.229c6.25 7.636 15.471 12.277 25.37 12.768a12.173 12.173 0 0 1 3.561-11.676c4.958-4.627 12.756-4.39 17.416.53 2.757-.54 5.4-1.544 7.818-2.967a12.283 12.283 0 0 1-5.412 6.759A24.62 24.62 0 0 0 60 5.719a24.908 24.908 0 0 1-6.143 6.33Z",fill:"#1D9BF0"})))};var Ae;function Ce(){return Ce=Object.assign?Object.assign.bind():function(e){for(var a=1;a<arguments.length;a++){var t=arguments[a];for(var i in t)({}).hasOwnProperty.call(t,i)&&(e[i]=t[i])}return e},Ce.apply(null,arguments)}const He=e=>{let{title:a,titleId:t,...n}=e;return i.createElement("svg",Ce({width:61,height:60,viewBox:"0 0 61 60",fill:"none",xmlns:"http://www.w3.org/2000/svg","aria-labelledby":t},n),a?i.createElement("title",{id:t},a):null,Ae||(Ae=i.createElement("path",{d:"M50.3 11H10.7c-3.96 0-7.2 3.24-7.2 7.2V41c0 3.96 3.24 7.2 7.2 7.2h39.6c3.96 0 7.2-3.24 7.2-7.2V18.2c0-3.96-3.24-7.2-7.2-7.2Zm-.66 7.236L36.8 27.08a11.072 11.072 0 0 1-6.3 1.95c-2.208 0-4.416-.648-6.3-1.95l-12.84-8.844a1.501 1.501 0 0 1 1.704-2.472l12.84 8.85a8.16 8.16 0 0 0 9.192 0l12.84-8.85a1.507 1.507 0 0 1 2.088.384c.468.684.3 1.62-.383 2.088Z",fill:"#079FCA"})))};var De,Se;function xe(){return xe=Object.assign?Object.assign.bind():function(e){for(var a=1;a<arguments.length;a++){var t=arguments[a];for(var i in t)({}).hasOwnProperty.call(t,i)&&(e[i]=t[i])}return e},xe.apply(null,arguments)}const Te=e=>{let{title:a,titleId:t,...n}=e;return i.createElement("svg",xe({xmlns:"http://www.w3.org/2000/svg",viewBox:"0 0 48 48",width:48,height:48,"aria-labelledby":t},n),a?i.createElement("title",{id:t},a):null,De||(De=i.createElement("path",{fill:"#FF3D00",d:"M43.2 33.9c-.4 2.1-2.1 3.7-4.2 4-3.3.5-8.8 1.1-15 1.1-6.1 0-11.6-.6-15-1.1-2.1-.3-3.8-1.9-4.2-4-.4-2.3-.8-5.7-.8-9.9s.4-7.6.8-9.9c.4-2.1 2.1-3.7 4.2-4C12.3 9.6 17.8 9 24 9c6.2 0 11.6.6 15 1.1 2.1.3 3.8 1.9 4.2 4 .4 2.3.9 5.7.9 9.9-.1 4.2-.5 7.6-.9 9.9z"})),Se||(Se=i.createElement("path",{fill:"#FFF",d:"M20 31V17l12 7z"})))},Ie=()=>{const e=[{icon:oe,title:"GitHub",linkText:"Join community",url:"https://github.com/apache/hudi"},{icon:pe,title:"Slack",linkText:"Join community",url:"https://join.slack.com/t/apache-hudi/shared_invite/zt-2ggm1fub8-_yt4Reu9djwqqVRFC7X49g"},{icon:fe,title:"Linkedin",linkText:"Join community",url:"https://www.linkedin.com/company/apache-hudi/?viewAsMember=true"},{icon:ve,title:"Twitter",linkText:"Join community",url:"https://twitter.com/ApacheHudi"},{icon:Te,title:"Youtube",linkText:"Subscribe",url:"https://www.youtube.com/channel/UCs7AhE0BWaEPZSChrBR-Muw"},{icon:He,title:"Mailing",linkText:"Subscribe",url:"mailto:dev-subscribe@hudi.apache.org?Subject=SubscribeToHudi"}],a=e.slice(0,3),t=e.slice(3,6);return i.createElement("div",{className:q},i.createElement("div",{className:"container"},i.createElement("div",{className:Z},i.createElement("div",{className:J},i.createElement(c,{primaryText:"Join our",secondaryText:"Community"}),i.createElement("p",{className:G},"Get technical help, influence the product roadmap & see what\u2019s new with Hudi!")),i.createElement("div",{className:V},i.createElement("div",{className:j},a.map(((e,a)=>i.createElement(te,{key:a,media:e})))),i.createElement("div",{className:j},t.map(((e,a)=>i.createElement(te,{key:a,media:e}))))))))},Ne={wrapper:"wrapper_w4Lv",cardsWrapper:"cardsWrapper_9krC",whyHudiTitle:"whyHudiTitle_7dsC",textWrapper:"textWrapper_N8Ff"},Me="cardWrapper_SG-j",Ee="title_9BtH",Le="subtitle_10CH",Re="iconWrapper_IGcW",Be=e=>{let{icon:a,title:t,subtitle:n}=e;const o=a;return i.createElement("div",{className:Me},i.createElement("div",{className:Re},o&&i.createElement(o,null)),i.createElement("h3",{className:Ee},t),i.createElement("div",{className:Le},n))};var ze,Pe,Fe,Oe;function Ue(){return Ue=Object.assign?Object.assign.bind():function(e){for(var a=1;a<arguments.length;a++){var t=arguments[a];for(var i in t)({}).hasOwnProperty.call(t,i)&&(e[i]=t[i])}return e},Ue.apply(null,arguments)}const We=e=>{let{title:a,titleId:t,...n}=e;return i.createElement("svg",Ue({width:101,height:100,viewBox:"0 0 101 100",fill:"none",xmlns:"http://www.w3.org/2000/svg","aria-labelledby":t},n),a?i.createElement("title",{id:t},a):null,ze||(ze=i.createElement("circle",{cx:50.5,cy:50,r:48.5,fill:"#29557A",stroke:"#CEEEF7",strokeWidth:3})),Pe||(Pe=i.createElement("path",{d:"M75.31 47.796h4.36v2.18h-4.36v-2.18ZM23 47.796h4.36v2.18H23v-2.18ZM50.245 26h2.18v4.36h-2.18V26ZM25.047 29.589l1.54-1.541 3.084 3.082-1.542 1.542-3.082-3.083ZM73 31.13l3.082-3.083 1.541 1.541-3.083 3.083L73 31.129ZM48.86 61.497l1.83 1.344c.391.286.924.28 1.308-.016l2.282-1.765a22.712 22.712 0 0 0 9.03-15.47v-.015a1.09 1.09 0 0 0-.901-1.191l-.998-.168a17.012 17.012 0 0 1-9.309-4.817 1.122 1.122 0 0 0-1.545 0 17.016 17.016 0 0 1-9.308 4.817l-.998.168a1.09 1.09 0 0 0-.903 1.199 22.653 22.653 0 0 0 9.512 15.914Zm2.475-19.825a19.161 19.161 0 0 0 9.668 4.686 20.669 20.669 0 0 1-8.053 12.98L51.316 60.6l-1.166-.858a20.616 20.616 0 0 1-8.487-13.385 19.152 19.152 0 0 0 9.672-4.686Z",fill:"#fff"})),Fe||(Fe=i.createElement("path",{d:"M30.629 35.808v30.515a2.18 2.18 0 0 0 2.18 2.18h37.053a2.18 2.18 0 0 0 2.18-2.18V35.808a2.18 2.18 0 0 0-2.18-2.18H32.808a2.18 2.18 0 0 0-2.18 2.18Zm39.233 30.515H32.808V35.808h37.054v30.515Z",fill:"#fff"})),Oe||(Oe=i.createElement("path",{d:"M49.794 54.692c.29 0 .566-.114.77-.32l5.395-5.393-1.541-1.54-4.624 4.622-1.542-1.54-1.54 1.54 2.311 2.312c.205.205.482.32.771.32Z",fill:"#fff"})))};var _e,qe,Ze,Ge;function Ve(){return Ve=Object.assign?Object.assign.bind():function(e){for(var a=1;a<arguments.length;a++){var t=arguments[a];for(var i in t)({}).hasOwnProperty.call(t,i)&&(e[i]=t[i])}return e},Ve.apply(null,arguments)}const je=e=>{let{title:a,titleId:t,...n}=e;return i.createElement("svg",Ve({width:46,height:60,viewBox:"0 0 46 60",fill:"none",xmlns:"http://www.w3.org/2000/svg","aria-labelledby":t},n),a?i.createElement("title",{id:t},a):null,_e||(_e=i.createElement("path",{d:"M21.34 60h-.98V17.832l-4.405-3.846V9.93h.978v3.636l4.406 3.846V60Z",fill:"#fff"})),qe||(qe=i.createElement("path",{d:"M16.934 60h-.98V20.07l-4.405-3.916V7.692h.979v8.042l4.406 3.846V60ZM26.584 60h-1.469V16.224l3.846-3.847V9.371h1.469v3.566l-3.846 3.916V60Z",fill:"#fff"})),Ze||(Ze=i.createElement("path",{d:"M30.71 60h-.979V19.09l3.846-3.845V8.252h.98v7.342l-3.847 3.847V60ZM16.304 22.937l.35.35-.35-.35ZM30.01 20.49l.42-.28-.42.28ZM25.605 43.077l.42-.28-.42.28ZM15.255 0h2.448v10.49h-2.448V0ZM10.85 0h2.447v10.49H10.85V0ZM28.472 0h2.448v10.49h-2.448V0Z",fill:"#fff"})),Ge||(Ge=i.createElement("path",{d:"M32.878 0h2.448v10.49h-2.448V0ZM3.227 18.251h4.545v4.545H3.227v-4.545ZM.5 42.238h4.545v4.545H.5v-4.545ZM38.472 19.23h4.545v4.546h-4.545v-4.545ZM36.094 46.364h4.546v4.545h-4.546v-4.545ZM11.34 36.153h1.398v1.399h-1.399v-1.399ZM8.053 39.161H9.45v1.399H8.053V39.16ZM.99 25.455h1.398v1.398H.989v-1.398ZM33.367 31.258h1.399v1.399h-1.399v-1.399ZM42.668 26.014h1.398v1.398h-1.398v-1.398ZM31.969 45.385h1.398v1.398h-1.398v-1.398ZM7.983 28.041h2.937v2.937H7.983v-2.937ZM7.143 48.321h2.937v2.938H7.143V48.32ZM33.017 22.657h2.938v2.938h-2.938v-2.938ZM39.451 31.608h2.937v2.937h-2.937v-2.937ZM35.395 37.832h2.937v2.937h-2.937v-2.937ZM42.878 41.748h2.937v2.937h-2.937v-2.937ZM3.018 34.196h2.937v2.937H3.018v-2.938ZM11.34 40.49h2.936v2.937H11.34V40.49Z",fill:"#fff"})))};var Xe,Qe,Ke;function Ye(){return Ye=Object.assign?Object.assign.bind():function(e){for(var a=1;a<arguments.length;a++){var t=arguments[a];for(var i in t)({}).hasOwnProperty.call(t,i)&&(e[i]=t[i])}return e},Ye.apply(null,arguments)}const Je=e=>{let{title:a,titleId:t,...n}=e;return i.createElement("svg",Ye({width:47,height:49,viewBox:"0 0 47 49",fill:"none",xmlns:"http://www.w3.org/2000/svg","aria-labelledby":t},n),a?i.createElement("title",{id:t},a):null,Xe||(Xe=i.createElement("path",{d:"M11.655 23.883H9.048a6.82 6.82 0 0 0 6.67 5.397H26.39V26.78H15.718a4.311 4.311 0 0 1-4.063-2.896ZM39.691 0H15.718a6.818 6.818 0 0 0-6.797 6.809v11.516h2.5V6.81A4.312 4.312 0 0 1 15.72 2.5H39.69A4.312 4.312 0 0 1 44 6.81v15.685a4.312 4.312 0 0 1-4.308 4.308H31.96v2.479h7.731a6.813 6.813 0 0 0 6.81-6.787V6.81A6.814 6.814 0 0 0 39.69 0Z",fill:"#fff"})),Qe||(Qe=i.createElement("path",{d:"M15.507 10.455h4.791a1.39 1.39 0 1 0 0-2.779h-4.791a1.39 1.39 0 1 0 0 2.78ZM35.123 10.455h4.79a1.39 1.39 0 1 0 0-2.779h-4.79a1.39 1.39 0 1 0 0 2.78ZM30.103 7.676h-4.785a1.39 1.39 0 1 0 0 2.78h4.791a1.39 1.39 0 1 0 0-2.78h-.006ZM15.507 16.313h4.791a1.39 1.39 0 1 0 0-2.779h-4.791a1.39 1.39 0 1 0 0 2.78ZM35.123 16.313h4.79a1.39 1.39 0 1 0 0-2.779h-4.79a1.39 1.39 0 1 0 0 2.78ZM30.103 13.534h-4.785a1.39 1.39 0 1 0 0 2.78h4.791a1.39 1.39 0 0 0 0-2.78h-.006ZM23.622 19.832H7.309A6.814 6.814 0 0 0 .5 26.663v15.085a6.814 6.814 0 0 0 6.809 6.809h16.313a6.815 6.815 0 0 0 6.809-6.81V26.664a6.814 6.814 0 0 0-6.809-6.831Zm4.325 9.449v12.45a4.313 4.313 0 0 1-4.308 4.308H7.309A4.311 4.311 0 0 1 3 41.73V26.663a4.313 4.313 0 0 1 4.307-4.308h16.314a4.312 4.312 0 0 1 4.324 4.308v2.618Z",fill:"#fff"})),Ke||(Ke=i.createElement("path",{d:"M18.142 34.522a2.679 2.679 0 1 1-5.357 0 2.679 2.679 0 0 1 5.357 0ZM13.606 40.37a2.68 2.68 0 1 1-5.358 0 2.68 2.68 0 0 1 5.358 0ZM22.683 40.37a2.68 2.68 0 1 1-5.358 0 2.68 2.68 0 0 1 5.358 0Z",fill:"#fff"})))};var $e,ea;function aa(){return aa=Object.assign?Object.assign.bind():function(e){for(var a=1;a<arguments.length;a++){var t=arguments[a];for(var i in t)({}).hasOwnProperty.call(t,i)&&(e[i]=t[i])}return e},aa.apply(null,arguments)}const ta=e=>{let{title:a,titleId:t,...n}=e;return i.createElement("svg",aa({width:72,height:42,viewBox:"0 0 72 42",fill:"none",xmlns:"http://www.w3.org/2000/svg","aria-labelledby":t},n),a?i.createElement("title",{id:t},a):null,$e||($e=i.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M35.938 18.653c-10.016 0-18.167 8.15-18.167 18.167 0 1.45.172 2.863.495 4.217a1.23 1.23 0 1 0 2.393-.572 15.654 15.654 0 0 1-.427-3.645c0-8.685 7.02-15.706 15.706-15.706 8.685 0 15.706 7.02 15.706 15.706a15.66 15.66 0 0 1-.428 3.645 1.23 1.23 0 1 0 2.393.572c.324-1.353.495-2.766.495-4.217 0-10.016-8.15-18.166-18.166-18.166ZM35.938 0c-4.614 0-8.385 3.764-8.385 8.379s3.77 8.385 8.385 8.385 8.385-3.77 8.385-8.385S40.552 0 35.938 0Z",fill:"#fff"})),ea||(ea=i.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M56.379 18.936c-2.322 0-4.531.53-6.503 1.482a1.23 1.23 0 1 0 1.07 2.216 12.444 12.444 0 0 1 5.433-1.237 12.499 12.499 0 0 1 12.519 12.519c0 .998-.118 1.972-.341 2.898a1.232 1.232 0 0 0 1.84 1.35 1.23 1.23 0 0 0 .552-.773c.269-1.117.41-2.284.41-3.476 0-8.262-6.724-14.98-14.98-14.98ZM56.374 4.325c-3.57 0-6.498 2.922-6.498 6.498 0 3.57 2.928 6.497 6.498 6.497 3.576 0 6.498-2.928 6.498-6.497 0-3.574-2.924-6.498-6.498-6.498Zm0 2.46a4.017 4.017 0 0 1 4.037 4.038 4.023 4.023 0 0 1-4.037 4.037 4.029 4.029 0 0 1-4.037-4.037 4.023 4.023 0 0 1 4.037-4.038ZM15.48 18.936c2.321 0 4.53.53 6.502 1.482a1.23 1.23 0 1 1-1.07 2.216 12.444 12.444 0 0 0-5.432-1.237A12.499 12.499 0 0 0 2.96 33.916c0 .998.119 1.972.342 2.898a1.232 1.232 0 1 1-2.393.577A14.84 14.84 0 0 1 .5 33.915c0-8.262 6.723-14.98 14.98-14.98ZM15.484 4.325c3.57 0 6.498 2.922 6.498 6.498 0 3.57-2.927 6.497-6.498 6.497-3.576 0-6.498-2.928-6.498-6.497 0-3.574 2.924-6.498 6.498-6.498Zm0 2.46a4.017 4.017 0 0 0-4.037 4.038 4.023 4.023 0 0 0 4.037 4.037 4.029 4.029 0 0 0 4.037-4.037 4.023 4.023 0 0 0-4.037-4.038Z",fill:"#fff"})))},ia=()=>{const e=[{icon:We,title:"Trusted Platform",subtitle:"Battle tested and proven in production in some of the largest data lakes on the planet."},{icon:ta,title:"Open Source",subtitle:"Hudi is a thriving & growing community that is built with contributions from people around the globe."},{icon:Je,title:"Derived tables",subtitle:"Seamlessly create and manage SQL tables on your data lake to build multi-stage incremental pipelines."},{icon:je,title:"Data streams",subtitle:"Take advantage of built-in CDC sources and tools for streaming ingestion."}];return i.createElement("section",{className:"container"},i.createElement("div",{className:Ne.wrapper},i.createElement("div",{className:Ne.title},i.createElement("div",{className:Ne.whyHudiTitle},i.createElement(c,{primaryText:"Why",secondaryText:"Hudi"})),i.createElement("div",{className:Ne.textWrapper},i.createElement("div",{className:"text--center text--semibold"},"Take advantage of Hudi\u2019s platform with rich services and tools to make your data lake actionable for applications like personalization, machine learning, customer 360 and more!"))),i.createElement("div",{className:Ne.cardsWrapper},e.map(((e,a)=>i.createElement(Be,{key:a,icon:e.icon,title:e.title,subtitle:e.subtitle}))))))},na="heroBanner_567B",oa="heroImg_UdI5",ra="contentWrapper_H1SW",sa="content_qomh",la="imageWrapper_Ztuf",da="leftContent_qOgS",ca="heroTitle_sXLF",pa="buttons_OhU5",ga="headlineWrapper_zoJj",ua="typingText_rgMd";var ma=t(9519);const ha=()=>{const{siteConfig:e}=(0,o.A)(),{taglineConfig:{prefix:a,suffix:t,content:n}}=e.customFields;return i.createElement("div",{className:ga},i.createElement("div",null,a),"\xa0",i.createElement(ma.d,{sequence:[n[0],1e3,n[1],1e3,n[2],1e3],wrapper:"div",cursor:!0,repeat:1/0,className:ua}),"\xa0",i.createElement("div",null,t))};const ba=function(){const{siteConfig:e}=(0,o.A)(),[a,n]=e.title.split(" ");return i.createElement("header",{className:(0,m.A)("hero hero--primary",na)},i.createElement("div",{className:"container"},i.createElement("div",{className:ra},i.createElement("div",{className:sa},i.createElement("div",{className:da},i.createElement("h1",{className:(0,m.A)("hero__title",ca)},i.createElement("span",null,a)," ",n),i.createElement(ha,null),i.createElement("div",{className:pa},i.createElement(w,{to:"/releases/release-0.15.0"},"Latest releases"),i.createElement(w,{type:"secondary",to:"/docs/quick-start-guide"},"Documentation"))),i.createElement("div",{className:la},i.createElement("img",{className:(0,m.A)("hero__img",oa),src:t(13683).A,alt:"Hudi banner"}))))))};var ya,fa,wa=t(82648),ka=t(46942),va=t.n(ka),Aa=t(86025),Ca=t(68234);function Ha(){return Ha=Object.assign?Object.assign.bind():function(e){for(var a=1;a<arguments.length;a++){var t=arguments[a];for(var i in t)({}).hasOwnProperty.call(t,i)&&(e[i]=t[i])}return e},Ha.apply(null,arguments)}const Da=e=>{let{title:a,titleId:t,...n}=e;return i.createElement("svg",Ha({width:34,height:34,viewBox:"0 0 34 34",fill:"none",xmlns:"http://www.w3.org/2000/svg","aria-labelledby":t},n),a?i.createElement("title",{id:t},a):null,ya||(ya=i.createElement("path",{d:"M22.286 11.326 11.68 21.933M13.749 11.343l8.537-.018-.017 8.538",stroke:"#0DB1F9",strokeWidth:1.5,strokeLinecap:"round",strokeLinejoin:"round"})),fa||(fa=i.createElement("rect",{x:5.75,y:5.75,width:22.5,height:22.5,rx:2.25,stroke:"#0DB1F9",strokeWidth:1.5})))},Sa="cardBlogWrapper_0KGH",xa="blogsWrapper_AK7c",Ta="titleWrapper_yx7W",Ia="cardTitleWrapper_aDfz",Na="cardBlogs_xPiG",Ma="link_Am1H",Ea="blogImg_iCe5",La="blogImgWrapper_4Zj3",Ra="blogContent_sn4a",Ba="authorName_A9Sf",za="date_NJkY",Pa="blogTitle_j0NU",Fa="sliderActionsWrapper_YAdg",Oa="dotsWrapper_PuKX",Ua="sliderDots_KQEn",Wa="activeSliderDots_xhp8",_a="arrowIcon_vUc-",qa="blogViewAllBtnWrapper_NKNV",Za="blogBtn_ReOC",Ga="embla_70aK",Va="embla__container_1NUB",ja="embla__slide_rGfR",Xa=e=>{let{blog:a}=e;const{withBaseUrl:t}=(0,Aa.h)(),{frontMatter:n,assets:o,metadata:r}=a,{formattedDate:s,title:l,authors:d,permalink:c}=r,p=o.image??n.image??"/assets/images/hudi.png";return i.createElement("div",{className:xa},i.createElement(f.A,{itemProp:"url",to:c,className:Ma},i.createElement("div",{className:Na},i.createElement("div",null,i.createElement("div",{className:La},i.createElement("img",{src:t(p,{absolute:!0}),alt:l,className:Ea})),i.createElement("div",{className:Ra},i.createElement(Ca.A,{authors:d,className:Ba,withLink:!1}),i.createElement("div",{className:za},s)),i.createElement("div",{className:Ia},i.createElement("h5",{className:Pa},l),i.createElement("div",null,i.createElement(Da,null)))))))};var Qa;function Ka(){return Ka=Object.assign?Object.assign.bind():function(e){for(var a=1;a<arguments.length;a++){var t=arguments[a];for(var i in t)({}).hasOwnProperty.call(t,i)&&(e[i]=t[i])}return e},Ka.apply(null,arguments)}const Ya=e=>{let{title:a,titleId:t,...n}=e;return i.createElement("svg",Ka({width:36,height:36,viewBox:"0 0 36 36",fill:"none",xmlns:"http://www.w3.org/2000/svg","aria-labelledby":t},n),a?i.createElement("title",{id:t},a):null,Qa||(Qa=i.createElement("path",{d:"M29.625 17.588h-22.5M20.55 8.552l9.075 9.036-9.075 9.037",stroke:"#0DB1F9",strokeWidth:2.25,strokeLinecap:"round",strokeLinejoin:"round"})))};var Ja;function $a(){return $a=Object.assign?Object.assign.bind():function(e){for(var a=1;a<arguments.length;a++){var t=arguments[a];for(var i in t)({}).hasOwnProperty.call(t,i)&&(e[i]=t[i])}return e},$a.apply(null,arguments)}const et=e=>{let{title:a,titleId:t,...n}=e;return i.createElement("svg",$a({width:36,height:36,viewBox:"0 0 36 36",fill:"none",xmlns:"http://www.w3.org/2000/svg","aria-labelledby":t},n),a?i.createElement("title",{id:t},a):null,Ja||(Ja=i.createElement("path",{d:"M6.375 17.588h22.5M15.45 8.552l-9.075 9.036 9.075 9.037",stroke:"#0DB1F9",strokeWidth:2.25,strokeLinecap:"round",strokeLinejoin:"round"})))};var at;const tt=[...(at=t(43152)).keys().reduce(((e,a,t)=>{const i=at(a);return[...e,{frontMatter:{...i.frontMatter},metadata:{...i.metadata},assets:{...i.assets}}]}),[]).sort(((e,a)=>new Date(e.metadata.date).getTime()-new Date(a.metadata.date).getTime())).reverse().slice(0,10)],it=()=>{const[e,a]=(0,wa.A)({loop:!0,slidesToScroll:1,align:0}),[t,n]=(0,i.useState)(0),o=(0,i.useCallback)((()=>a&&a.scrollPrev()),[a]),r=(0,i.useCallback)((()=>a&&a.scrollNext()),[a]),s=(0,i.useCallback)((()=>{a&&n(a.selectedScrollSnap())}),[a]);return(0,i.useEffect)((()=>{a&&(s(),a.on("select",s))}),[a,s]),i.createElement("section",{className:Sa},i.createElement("div",{className:"container"},i.createElement("div",{className:Ta},i.createElement(c,{primaryText:"Hudi",secondaryText:"Blogs"})),i.createElement("div",{className:Ga,ref:e},i.createElement("div",{className:Va},tt.map(((e,a)=>i.createElement("div",{className:ja,key:a},i.createElement(Xa,{blog:e})))))),i.createElement("div",{className:Fa},i.createElement(et,{onClick:()=>o(),className:_a}),i.createElement("div",{className:Oa},tt.map(((e,n)=>i.createElement("div",{key:n,role:"button",onClick:()=>{return e=n,void(a&&a.scrollTo(e));var e},className:va()(Ua,{[Wa]:t===n})})))),i.createElement(Ya,{onClick:()=>r(),className:_a})),i.createElement("div",{className:qa},i.createElement(w,{to:"/blog",className:Za},"View All"))))};function nt(){return i.createElement("div",{className:"container"},i.createElement("div",{className:"wrapper"},i.createElement("br",null)))}function ot(){const{siteConfig:e}=(0,o.A)();return i.createElement(n.A,{title:"Apache Hudi | An Open Source Data Lake Platform",shouldShowOnlyTitle:!0,description:"Description will go into a meta tag in <head />"},i.createElement(nt,null),i.createElement(ba,null),i.createElement(S,null),i.createElement("main",null,i.createElement(p,null),i.createElement(_,null),i.createElement(ia,null),i.createElement(it,null),i.createElement(Ie,null)))}},5070:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"The Case for incremental processing on Hadoop",authors:[{name:"Vinoth Chandar"}],category:"blog",image:"/assets/images/blog/2016-08-04-The-Case-for-incremental-processing-on-Hadoop.png",tags:["use-case","incremental processing","oreilly"]},s=void 0,l={permalink:"/cn/blog/2016/08/04/The-Case-for-incremental-processing-on-Hadoop",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2016-08-04-The-Case-for-incremental-processing-on-Hadoop.mdx",source:"@site/blog/2016-08-04-The-Case-for-incremental-processing-on-Hadoop.mdx",title:"The Case for incremental processing on Hadoop",description:"Redirecting... please wait!!",date:"2016-08-04T00:00:00.000Z",formattedDate:"August 4, 2016",tags:[{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"incremental processing",permalink:"/cn/blog/tags/incremental-processing"},{label:"oreilly",permalink:"/cn/blog/tags/oreilly"}],readingTime:.045,truncated:!1,authors:[{name:"Vinoth Chandar"}],prevItem:{title:"Connect with us at Strata San Jose March 2017",permalink:"/cn/blog/2016/12/30/strata-talk-2017"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.oreilly.com/ideas/ubers-case-for-incremental-processing-on-hadoop",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},57192:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Connect with us at Strata San Jose March 2017",author:"admin",date:new Date("2016-12-30T00:00:00.000Z"),category:"blog"},r=void 0,s={permalink:"/cn/blog/2016/12/30/strata-talk-2017",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2016-12-30-strata-talk-2017.md",source:"@site/blog/2016-12-30-strata-talk-2017.md",title:"Connect with us at Strata San Jose March 2017",description:"We will be presenting Hudi & general concepts around how incremental processing works at Uber.",date:"2016-12-30T00:00:00.000Z",formattedDate:"December 30, 2016",tags:[],readingTime:.12,truncated:!1,authors:[{name:"admin"}],prevItem:{title:"Hoodie: Uber Engineering's Incremental Processing Framework on Hadoop",permalink:"/cn/blog/2017/03/12/Hoodie-Uber-Engineerings-Incremental-Processing-Framework-on-Hadoop"},nextItem:{title:"The Case for incremental processing on Hadoop",permalink:"/cn/blog/2016/08/04/The-Case-for-incremental-processing-on-Hadoop"}},l={authorsImageUrls:[void 0]},d=[],c={toc:d},p="wrapper";function g(e){let{components:a,...t}=e;return(0,n.yg)(p,(0,i.A)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"We will be presenting Hudi & general concepts around how incremental processing works at Uber.\nCatch our talk ",(0,n.yg)("strong",{parentName:"p"},'"Incremental Processing on Hadoop At Uber"')))}g.isMDXComponent=!0},41678:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Hoodie: Uber Engineering's Incremental Processing Framework on Hadoop",authors:[{name:"Prasanna Rajaperumal"},{name:"Vinoth Chandar"}],category:"blog",image:"/assets/images/blog/2017-03-12-Hoodie-Uber-Engineerings-Incremental-Processing-Framework-on-Hadoop.png",tags:["use-case","incremental processing","uber"]},s=void 0,l={permalink:"/cn/blog/2017/03/12/Hoodie-Uber-Engineerings-Incremental-Processing-Framework-on-Hadoop",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2017-03-12-Hoodie-Uber-Engineerings-Incremental-Processing-Framework-on-Hadoop.mdx",source:"@site/blog/2017-03-12-Hoodie-Uber-Engineerings-Incremental-Processing-Framework-on-Hadoop.mdx",title:"Hoodie: Uber Engineering's Incremental Processing Framework on Hadoop",description:"Redirecting... please wait!!",date:"2017-03-12T00:00:00.000Z",formattedDate:"March 12, 2017",tags:[{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"incremental processing",permalink:"/cn/blog/tags/incremental-processing"},{label:"uber",permalink:"/cn/blog/tags/uber"}],readingTime:.045,truncated:!1,authors:[{name:"Prasanna Rajaperumal"},{name:"Vinoth Chandar"}],prevItem:{title:"Hudi entered Apache Incubator",permalink:"/cn/blog/2019/01/18/asf-incubation"},nextItem:{title:"Connect with us at Strata San Jose March 2017",permalink:"/cn/blog/2016/12/30/strata-talk-2017"}},d={authorsImageUrls:[void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://eng.uber.com/hoodie/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},20543:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Hudi entered Apache Incubator",author:"admin",date:new Date("2019-01-18T00:00:00.000Z"),category:"blog"},r=void 0,s={permalink:"/cn/blog/2019/01/18/asf-incubation",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2019-01-18-asf-incubation.md",source:"@site/blog/2019-01-18-asf-incubation.md",title:"Hudi entered Apache Incubator",description:"In the coming weeks, we will be moving in our new home on the Apache Incubator.",date:"2019-01-18T00:00:00.000Z",formattedDate:"January 18, 2019",tags:[],readingTime:.08,truncated:!1,authors:[{name:"admin"}],prevItem:{title:"Big Batch vs Incremental Processing",permalink:"/cn/blog/2019/03/07/batch-vs-incremental"},nextItem:{title:"Hoodie: Uber Engineering's Incremental Processing Framework on Hadoop",permalink:"/cn/blog/2017/03/12/Hoodie-Uber-Engineerings-Incremental-Processing-Framework-on-Hadoop"}},l={authorsImageUrls:[void 0]},d=[],c={toc:d},p="wrapper";function g(e){let{components:a,...t}=e;return(0,n.yg)(p,(0,i.A)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"In the coming weeks, we will be moving in our new home on the Apache Incubator."))}g.isMDXComponent=!0},95101:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Big Batch vs Incremental Processing",author:"vinoth",category:"blog",image:"/assets/images/blog/batch_vs_incremental.png"},r=void 0,s={permalink:"/cn/blog/2019/03/07/batch-vs-incremental",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2019-03-07-batch-vs-incremental.md",source:"@site/blog/2019-03-07-batch-vs-incremental.md",title:"Big Batch vs Incremental Processing",description:"",date:"2019-03-07T00:00:00.000Z",formattedDate:"March 7, 2019",tags:[],readingTime:.005,truncated:!1,authors:[{name:"vinoth"}],prevItem:{title:"Registering sample dataset to Hive via beeline",permalink:"/cn/blog/2019/05/14/registering-dataset-to-hive"},nextItem:{title:"Hudi entered Apache Incubator",permalink:"/cn/blog/2019/01/18/asf-incubation"}},l={authorsImageUrls:[void 0]},d=[],c={toc:d},p="wrapper";function g(e){let{components:a,...o}=e;return(0,n.yg)(p,(0,i.A)({},c,o,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,(0,n.yg)("img",{src:t(52237).A})))}g.isMDXComponent=!0},18410:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Registering sample dataset to Hive via beeline",excerpt:"How to manually register HUDI dataset into Hive using beeline",author:"vinoth",category:"blog",tags:["how-to","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2019/05/14/registering-dataset-to-hive",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2019-05-14-registering-dataset-to-hive.md",source:"@site/blog/2019-05-14-registering-dataset-to-hive.md",title:"Registering sample dataset to Hive via beeline",description:"Hudi hive sync tool typically handles registration of the dataset into Hive metastore. In case, there are issues with quickstart around this, following page shows commands that can be used to do this manually via beeline.",date:"2019-05-14T00:00:00.000Z",formattedDate:"May 14, 2019",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:1.32,truncated:!0,authors:[{name:"vinoth"}],prevItem:{title:"Ingesting Database changes via Sqoop/Hudi",permalink:"/cn/blog/2019/09/09/ingesting-database-changes"},nextItem:{title:"Big Batch vs Incremental Processing",permalink:"/cn/blog/2019/03/07/batch-vs-incremental"}},l={authorsImageUrls:[void 0]},d=[],c={toc:d},p="wrapper";function g(e){let{components:a,...t}=e;return(0,n.yg)(p,(0,i.A)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"Hudi hive sync tool typically handles registration of the dataset into Hive metastore. In case, there are issues with quickstart around this, following page shows commands that can be used to do this manually via beeline.  "),(0,n.yg)("p",null,"Add in the ",(0,n.yg)("em",{parentName:"p"},"packaging/hoodie-hive-bundle/target/hoodie-hive-bundle-0.4.6-SNAPSHOT.jar,")," so that Hive can read the Hudi dataset and answer the query."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"hive> set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;\nhive> set hive.stats.autogather=false;\nhive> add jar file:///path/to/hoodie-hive-bundle-0.5.2-SNAPSHOT.jar;\nAdded [file:///path/to/hoodie-hive-bundle-0.5.2-SNAPSHOT.jar] to class path\nAdded resources: [file:///path/to/hoodie-hive-bundle-0.5.2-SNAPSHOT.jar]\n")),(0,n.yg)("p",null,"Then, you need to create a ",(0,n.yg)("em",{parentName:"p"},"ReadOptimized")," Hive table as below and register the sample partitions"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"DROP TABLE hoodie_test;\nCREATE EXTERNAL TABLE hoodie_test(`_row_key` string,\n    `_hoodie_commit_time` string,\n    `_hoodie_commit_seqno` string,\n    rider string,\n    driver string,\n    begin_lat double,\n    begin_lon double,\n    end_lat double,\n    end_lon double,\n    fare double)\n    PARTITIONED BY (`datestr` string)\n    ROW FORMAT SERDE\n    'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n    STORED AS INPUTFORMAT\n    'com.uber.hoodie.hadoop.HoodieInputFormat'\n    OUTPUTFORMAT\n    'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n    LOCATION\n    'hdfs:///tmp/hoodie/sample-table';\n     \nALTER TABLE `hoodie_test` ADD IF NOT EXISTS PARTITION (datestr='2016-03-15') LOCATION 'hdfs:///tmp/hoodie/sample-table/2016/03/15';\nALTER TABLE `hoodie_test` ADD IF NOT EXISTS PARTITION (datestr='2015-03-16') LOCATION 'hdfs:///tmp/hoodie/sample-table/2015/03/16';\nALTER TABLE `hoodie_test` ADD IF NOT EXISTS PARTITION (datestr='2015-03-17') LOCATION 'hdfs:///tmp/hoodie/sample-table/2015/03/17';\n     \nset mapreduce.framework.name=yarn;\n")),(0,n.yg)("p",null,"And you can add a ",(0,n.yg)("em",{parentName:"p"},"Realtime")," Hive table, as below"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"DROP TABLE hoodie_rt;\nCREATE EXTERNAL TABLE hoodie_rt(\n    `_hoodie_commit_time` string,\n    `_hoodie_commit_seqno` string,\n    `_hoodie_record_key` string,\n    `_hoodie_partition_path` string,\n    `_hoodie_file_name` string,\n    timestamp double,\n    `_row_key` string,\n    rider string,\n    driver string,\n    begin_lat double,\n    begin_lon double,\n    end_lat double,\n    end_lon double,\n    fare double)\n    PARTITIONED BY (`datestr` string)\n    ROW FORMAT SERDE\n    'com.uber.hoodie.hadoop.realtime.HoodieParquetSerde'\n    STORED AS INPUTFORMAT\n    'com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat'\n    OUTPUTFORMAT\n    'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n    LOCATION\n    'file:///tmp/hoodie/sample-table';\n     \nALTER TABLE `hoodie_rt` ADD IF NOT EXISTS PARTITION (datestr='2016-03-15') LOCATION 'file:///tmp/hoodie/sample-table/2016/03/15';\nALTER TABLE `hoodie_rt` ADD IF NOT EXISTS PARTITION (datestr='2015-03-16') LOCATION 'file:///tmp/hoodie/sample-table/2015/03/16';\nALTER TABLE `hoodie_rt` ADD IF NOT EXISTS PARTITION (datestr='2015-03-17') LOCATION 'file:///tmp/hoodie/sample-table/2015/03/17';\n")))}g.isMDXComponent=!0},75878:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Ingesting Database changes via Sqoop/Hudi",excerpt:"Learn how to ingesting changes from a HUDI dataset using Sqoop/Hudi",author:"vinoth",category:"blog",tags:["how-to","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2019/09/09/ingesting-database-changes",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2019-09-09-ingesting-database-changes.md",source:"@site/blog/2019-09-09-ingesting-database-changes.md",title:"Ingesting Database changes via Sqoop/Hudi",description:"Very simple in just 2 steps.",date:"2019-09-09T00:00:00.000Z",formattedDate:"September 9, 2019",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:.605,truncated:!0,authors:[{name:"vinoth"}],prevItem:{title:"Hudi On Hops",permalink:"/cn/blog/2019/10/22/Hudi-On-Hops"},nextItem:{title:"Registering sample dataset to Hive via beeline",permalink:"/cn/blog/2019/05/14/registering-dataset-to-hive"}},l={authorsImageUrls:[void 0]},d=[],c={toc:d},p="wrapper";function g(e){let{components:a,...t}=e;return(0,n.yg)(p,(0,i.A)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"Very simple in just 2 steps."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Step 1"),": Extract new changes to users table in MySQL, as avro data files on DFS"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-bash"},"// Command to extract incrementals using sqoop\nbin/sqoop import \\\n  -Dmapreduce.job.user.classpath.first=true \\\n  --connect jdbc:mysql://localhost/users \\\n  --username root \\\n  --password ******* \\\n  --table users \\\n  --as-avrodatafile \\\n  --target-dir \\ \n  s3:///tmp/sqoop/import-1/users\n")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Step 2"),": Use your fav datasource to read extracted data and directly \u201cupsert\u201d the users table on DFS/Hive"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-scala"},'// Spark Datasource\nimport org.apache.hudi.DataSourceWriteOptions._\n// Use Spark datasource to read avro\nval inputDataset = spark.read.avro("s3://tmp/sqoop/import-1/users/*");\n     \n// save it as a Hudi dataset\ninputDataset.write.format("org.apache.hudi\u201d)\n  .option(HoodieWriteConfig.TABLE_NAME, "hoodie.users")\n  .option(RECORDKEY_FIELD_OPT_KEY(), "userID")\n  .option(PARTITIONPATH_FIELD_OPT_KEY(),"country")\n  .option(PRECOMBINE_FIELD_OPT_KEY(), "last_mod")\n  .option(OPERATION_OPT_KEY(), UPSERT_OPERATION_OPT_VAL())\n  .mode(SaveMode.Append)\n  .save("/path/on/dfs");\n')),(0,n.yg)("p",null,"Alternatively, you can also use the Hudi ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/writing_data#deltastreamer"},"DeltaStreamer")," tool with the DFSSource."))}g.isMDXComponent=!0},78818:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Hudi On Hops",authors:[{name:"NETSANET GEBRETSADKAN KIDANE"}],category:"blog",tags:["blog","diva-portal"]},s=void 0,l={permalink:"/cn/blog/2019/10/22/Hudi-On-Hops",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2019-10-22-Hudi-On-Hops.mdx",source:"@site/blog/2019-10-22-Hudi-On-Hops.mdx",title:"Hudi On Hops",description:"Redirecting... please wait!!",date:"2019-10-22T00:00:00.000Z",formattedDate:"October 22, 2019",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"diva-portal",permalink:"/cn/blog/tags/diva-portal"}],readingTime:.045,truncated:!1,authors:[{name:"NETSANET GEBRETSADKAN KIDANE"}],prevItem:{title:"New \u2013 Insert, Update, Delete Data on S3 with Amazon EMR and Apache Hudi",permalink:"/cn/blog/2019/11/15/New-Insert-Update-Delete-Data-on-S3-with-Amazon-EMR-and-Apache-Hudi"},nextItem:{title:"Ingesting Database changes via Sqoop/Hudi",permalink:"/cn/blog/2019/09/09/ingesting-database-changes"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.diva-portal.org/smash/get/diva2:1413103/FULLTEXT01.pdf",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},17607:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"New \u2013 Insert, Update, Delete Data on S3 with Amazon EMR and Apache Hudi",authors:[{name:"Danilo Poccia"}],category:"blog",image:"/assets/images/blog/aws.jpg",tags:["blog","amazon"]},s=void 0,l={permalink:"/cn/blog/2019/11/15/New-Insert-Update-Delete-Data-on-S3-with-Amazon-EMR-and-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2019-11-15-New-Insert-Update-Delete-Data-on-S3-with-Amazon-EMR-and-Apache-Hudi.mdx",source:"@site/blog/2019-11-15-New-Insert-Update-Delete-Data-on-S3-with-Amazon-EMR-and-Apache-Hudi.mdx",title:"New \u2013 Insert, Update, Delete Data on S3 with Amazon EMR and Apache Hudi",description:"Redirecting... please wait!!",date:"2019-11-15T00:00:00.000Z",formattedDate:"November 15, 2019",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[{name:"Danilo Poccia"}],prevItem:{title:"Delete support in Hudi",permalink:"/cn/blog/2020/01/15/delete-support-in-hudi"},nextItem:{title:"Hudi On Hops",permalink:"/cn/blog/2019/10/22/Hudi-On-Hops"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/aws/new-insert-update-delete-data-on-s3-with-amazon-emr-and-apache-hudi/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},3877:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Delete support in Hudi",excerpt:"Deletes are supported at a record level in Hudi with 0.5.1 release. This blog is a \u201chow to\u201d blog on how to delete records in hudi.",author:"shivnarayan",category:"blog",tags:["how-to","deletes","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2020/01/15/delete-support-in-hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-01-15-delete-support-in-hudi.md",source:"@site/blog/2020-01-15-delete-support-in-hudi.md",title:"Delete support in Hudi",description:'Deletes are supported at a record level in Hudi with 0.5.1 release. This blog is a "how to" blog on how to delete records in hudi. Deletes can be done with 3 flavors: Hudi RDD APIs, with Spark data source and with DeltaStreamer.',date:"2020-01-15T00:00:00.000Z",formattedDate:"January 15, 2020",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"deletes",permalink:"/cn/blog/tags/deletes"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:3.835,truncated:!0,authors:[{name:"shivnarayan"}],prevItem:{title:"Change Capture Using AWS Database Migration Service and Hudi",permalink:"/cn/blog/2020/01/20/change-capture-using-aws"},nextItem:{title:"New \u2013 Insert, Update, Delete Data on S3 with Amazon EMR and Apache Hudi",permalink:"/cn/blog/2019/11/15/New-Insert-Update-Delete-Data-on-S3-with-Amazon-EMR-and-Apache-Hudi"}},l={authorsImageUrls:[void 0]},d=[{value:"Delete using RDD Level APIs",id:"delete-using-rdd-level-apis",children:[],level:3},{value:"Deletion with Datasource",id:"deletion-with-datasource",children:[],level:3},{value:"Deletion with HoodieDeltaStreamer",id:"deletion-with-hoodiedeltastreamer",children:[],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...t}=e;return(0,n.yg)(p,(0,i.A)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,'Deletes are supported at a record level in Hudi with 0.5.1 release. This blog is a "how to" blog on how to delete records in hudi. Deletes can be done with 3 flavors: Hudi RDD APIs, with Spark data source and with DeltaStreamer.'),(0,n.yg)("h3",{id:"delete-using-rdd-level-apis"},"Delete using RDD Level APIs"),(0,n.yg)("p",null,"If you have embedded  ",(0,n.yg)("em",{parentName:"p"},"HoodieWriteClient")," , then deletion is as simple as passing in a  ",(0,n.yg)("em",{parentName:"p"},"JavaRDD","<","HoodieKey",">")," to the delete api."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"// Fetch list of HoodieKeys from elsewhere that needs to be deleted\n// convert to JavaRDD if required. JavaRDD<HoodieKey> toBeDeletedKeys\nList<WriteStatus> statuses = writeClient.delete(toBeDeletedKeys, commitTime);\n")),(0,n.yg)("h3",{id:"deletion-with-datasource"},"Deletion with Datasource"),(0,n.yg)("p",null,"Now we will walk through an example of how to perform deletes on a sample dataset using the Datasource API. Quick Start has the same example as below. Feel free to check it out."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Step 1")," : Launch spark shell"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-bash"},"bin/spark-shell --packages org.apache.hudi:hudi-spark-bundle:0.5.1-incubating \\\n  --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer'\n")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Step 2")," : Import as required and set up table name, etc for sample dataset"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-scala"},'import org.apache.hudi.QuickstartUtils._\nimport scala.collection.JavaConversions._\nimport org.apache.spark.sql.SaveMode._\nimport org.apache.hudi.DataSourceReadOptions._\nimport org.apache.hudi.DataSourceWriteOptions._\nimport org.apache.hudi.config.HoodieWriteConfig._\n     \nval tableName = "hudi_cow_table"\nval basePath = "file:///tmp/hudi_cow_table"\nval dataGen = new DataGenerator\n')),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Step 3")," : Insert data. Generate some new trips, load them into a DataFrame and write the DataFrame into the Hudi dataset as below."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-scala"},'val inserts = convertToStringList(dataGen.generateInserts(10))\nval df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\ndf.write.format("org.apache.hudi").\n  options(getQuickstartWriteConfigs).\n  option(PRECOMBINE_FIELD_OPT_KEY, "ts").\n  option(RECORDKEY_FIELD_OPT_KEY, "uuid").\n  option(PARTITIONPATH_FIELD_OPT_KEY, "partitionpath").\n  option(TABLE_NAME, tableName).\n  mode(Overwrite).\n  save(basePath);\n')),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Note:")," For non-partitioned table, set"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},'option(KEYGENERATOR_CLASS_PROP, "org.apache.hudi.keygen.NonpartitionedKeyGenerator")\n')),(0,n.yg)("p",null," Checkout ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/blog/2021/02/13/hudi-key-generators"},"https://hudi.apache.org/blog/2021/02/13/hudi-key-generators")," for more options"),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Step 4")," : Query data. Load the data files into a DataFrame."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-scala"},'val roViewDF = spark.read.\n  format("org.apache.hudi").\n  load(basePath + "/*/*/*/*")\nroViewDF.createOrReplaceTempView("hudi_ro_table")\nspark.sql("select count(*) from hudi_ro_table").show() // should return 10 (number of records inserted above)\nval riderValue = spark.sql("select distinct rider from hudi_ro_table").show()\n// copy the value displayed to be used in next step\n')),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Step 5"),' : Fetch records that needs to be deleted, with the above rider value. This example is just to illustrate how to delete. In real world, use a select query using spark sql to fetch records that needs to be deleted and from the result we could invoke deletes as given below. Example rider value used is "rider-213".'),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-scala"},"val df = spark.sql(\"select uuid, partitionPath from hudi_ro_table where rider = 'rider-213'\")\n")),(0,n.yg)("p",null,"// Replace the above query with any other query that will fetch records to be deleted."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Step 6")," : Issue deletes"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-scala"},'val deletes = dataGen.generateDeletes(df.collectAsList())\nval df = spark.read.json(spark.sparkContext.parallelize(deletes, 2));\ndf.write.format("org.apache.hudi").\n  options(getQuickstartWriteConfigs).\n  option(OPERATION_OPT_KEY,"delete").\n  option(PRECOMBINE_FIELD_OPT_KEY, "ts").\n  option(RECORDKEY_FIELD_OPT_KEY, "uuid").\n  option(PARTITIONPATH_FIELD_OPT_KEY, "partitionpath").\n  option(TABLE_NAME, tableName).\n  mode(Append).\n  save(basePath);\n')),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Note:")," For non-partitioned table, set"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},'option(KEYGENERATOR_CLASS_PROP, "org.apache.hudi.keygen.NonpartitionedKeyGenerator")\n')),(0,n.yg)("p",null," Checkout ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/blog/2021/02/13/hudi-key-generators"},"https://hudi.apache.org/blog/2021/02/13/hudi-key-generators")," for more options"),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Step 7")," : Reload the table and verify that the records are deleted"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-scala"},'val roViewDFAfterDelete = spark.\n  read.\n  format("org.apache.hudi").\n  load(basePath + "/*/*/*/*")\nroViewDFAfterDelete.createOrReplaceTempView("hudi_ro_table")\nspark.sql("select uuid, partitionPath from hudi_ro_table where rider = \'rider-213\'").show() // should not return any rows\n')),(0,n.yg)("h2",{id:"deletion-with-hoodiedeltastreamer"},"Deletion with HoodieDeltaStreamer"),(0,n.yg)("p",null,"Deletion with ",(0,n.yg)("inlineCode",{parentName:"p"},"HoodieDeltaStreamer"),' takes the same path as upsert and so it relies on a specific field called  "',(0,n.yg)("em",{parentName:"p"},"_hoodie_is_deleted"),'" of type boolean in each record.'),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"If a record has the field value set to  ",(0,n.yg)("em",{parentName:"li"},"false")," or it's not present, then it is considered a regular upsert"),(0,n.yg)("li",{parentName:"ul"},"if not (if the value is set to  ",(0,n.yg)("em",{parentName:"li"},"true")," ), then its considered to be deleted record.")),(0,n.yg)("p",null,"This essentially means that the schema has to be changed for the source, to add this field and all incoming records are expected to have this field set. We will be working to relax this in future releases."),(0,n.yg)("p",null,"Lets say the original schema is:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-json"},'{\n  "type":"record",\n  "name":"example_tbl",\n  "fields":[{\n     "name": "uuid",\n     "type": "String"\n  }, {\n     "name": "ts",\n     "type": "string"\n  },  {\n     "name": "partitionPath",\n     "type": "string"\n  }, {\n     "name": "rank",\n     "type": "long"\n  }\n]}\n')),(0,n.yg)("p",null,"To leverage deletion capabilities of ",(0,n.yg)("inlineCode",{parentName:"p"},"DeltaStreamer"),", you have to change the schema as below."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-json"},'{\n  "type":"record",\n  "name":"example_tbl",\n  "fields":[{\n     "name": "uuid",\n     "type": "String"\n  }, {\n     "name": "ts",\n     "type": "string"\n  },  {\n     "name": "partitionPath",\n     "type": "string"\n  }, {\n     "name": "rank",\n     "type": "long"\n  }, {\n    "name" : "_hoodie_is_deleted",\n    "type" : "boolean",\n    "default" : false\n  }\n]}\n')),(0,n.yg)("p",null,"Example incoming record for upsert"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-json"},'{\n  "ts": 0.0,\n  "uuid":"69cdb048-c93e-4532-adf9-f61ce6afe605",\n  "rank": 1034,\n  "partitionpath":"americas/brazil/sao_paulo",\n  "_hoodie_is_deleted":false\n}\n')),(0,n.yg)("p",null,"Example incoming record that needs to be deleted"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-json"},'{\n  "ts": 0.0,\n  "uuid": "19tdb048-c93e-4532-adf9-f61ce6afe10",\n  "rank": 1045,\n  "partitionpath":"americas/brazil/sao_paulo",\n  "_hoodie_is_deleted":true\n}\n')),(0,n.yg)("p",null,"These are one time changes. Once these are in, then the DeltaStreamer pipeline will handle both upserts and deletions within every batch. Each batch could contain a mix of upserts and deletes and no additional step or changes are required after this. Note that this is to perform hard deletion instead of soft deletion."))}g.isMDXComponent=!0},70085:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Change Capture Using AWS Database Migration Service and Hudi",excerpt:"In this blog, we will build an end-end solution for capturing changes from a MySQL instance running on AWS RDS to a Hudi table on S3, using capabilities in the Hudi 0.5.1 release.",author:"vinoth",category:"blog",image:"/assets/images/blog/change-capture-architecture.png",tags:["how-to","change data capture","cdc","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2020/01/20/change-capture-using-aws",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-01-20-change-capture-using-aws.md",source:"@site/blog/2020-01-20-change-capture-using-aws.md",title:"Change Capture Using AWS Database Migration Service and Hudi",description:"One of the core use-cases for Apache Hudi is enabling seamless, efficient database ingestion to your data lake. Even though a lot has been talked about and even users already adopting this model, content on how to go about this is sparse.",date:"2020-01-20T00:00:00.000Z",formattedDate:"January 20, 2020",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"change data capture",permalink:"/cn/blog/tags/change-data-capture"},{label:"cdc",permalink:"/cn/blog/tags/cdc"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:7.42,truncated:!0,authors:[{name:"vinoth"}],prevItem:{title:"Export Hudi datasets as a copy or as different formats",permalink:"/cn/blog/2020/03/22/exporting-hudi-datasets"},nextItem:{title:"Delete support in Hudi",permalink:"/cn/blog/2020/01/15/delete-support-in-hudi"}},l={authorsImageUrls:[void 0]},d=[{value:"Extracting Change logs from MySQL",id:"extracting-change-logs-from-mysql",children:[],level:3},{value:"Applying Change Logs using Hudi DeltaStreamer",id:"applying-change-logs-using-hudi-deltastreamer",children:[],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...o}=e;return(0,n.yg)(p,(0,i.A)({},c,o,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"One of the core use-cases for Apache Hudi is enabling seamless, efficient database ingestion to your data lake. Even though a lot has been talked about and even users already adopting this model, content on how to go about this is sparse."),(0,n.yg)("p",null,"In this blog, we will build an end-end solution for capturing changes from a MySQL instance running on AWS RDS to a Hudi table on S3, using capabilities in the Hudi  ",(0,n.yg)("strong",{parentName:"p"},"0.5.1 release")),(0,n.yg)("p",null,"We can break up the problem into two pieces."),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Extracting change logs from MySQL"),"  : Surprisingly, this is still a pretty tricky problem to solve and often Hudi users get stuck here. Thankfully, at-least for AWS users, there is a  ",(0,n.yg)("a",{parentName:"li",href:"https://aws.amazon.com/dms/"},"Database Migration service"),"  (DMS for short), that does this change capture and uploads them as parquet files on S3"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Applying these change logs to your data lake table"),"  : Once there are change logs in some form, the next step is to apply them incrementally to your table. This mundane task can be fully automated using the Hudi  ",(0,n.yg)("a",{parentName:"li",href:"http://hudi.apache.org/docs/writing_data#deltastreamer"},"DeltaStreamer"),"  tool.")),(0,n.yg)("p",null,"The actual end-end architecture looks something like this.\n",(0,n.yg)("img",{alt:"enter image description here",src:t(67527).A})),(0,n.yg)("p",null,"Let's now illustrate how one can accomplish this using a simple ",(0,n.yg)("em",{parentName:"p"},"orders")," table, stored in MySQL (these instructions should broadly apply to other database engines like Postgres, or Aurora as well, though SQL/Syntax may change)"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"CREATE DATABASE hudi_dms;\nUSE hudi_dms;\n     \nCREATE TABLE orders(\n   order_id INTEGER,\n   order_qty INTEGER,\n   customer_name VARCHAR(100),\n   updated_at TIMESTAMP DEFAULT NOW() ON UPDATE NOW(),\n   created_at TIMESTAMP DEFAULT NOW(),\n   CONSTRAINT orders_pk PRIMARY KEY(order_id)\n);\n \nINSERT INTO orders(order_id, order_qty, customer_name) VALUES(1, 10, 'victor');\nINSERT INTO orders(order_id, order_qty, customer_name) VALUES(2, 20, 'peter');\n")),(0,n.yg)("p",null,"In the table, ",(0,n.yg)("em",{parentName:"p"},"order_id")," is the primary key which will be enforced on the Hudi table as well. Since a batch of change records can contain changes to the same primary key, we also include ",(0,n.yg)("em",{parentName:"p"},"updated_at")," and ",(0,n.yg)("em",{parentName:"p"},"created_at")," fields, which are kept upto date as writes happen to the table."),(0,n.yg)("h3",{id:"extracting-change-logs-from-mysql"},"Extracting Change logs from MySQL"),(0,n.yg)("p",null,"Before we can configure DMS, we first need to ",(0,n.yg)("a",{parentName:"p",href:"https://aws.amazon.com/premiumsupport/knowledge-center/enable-binary-logging-aurora/"},"prepare the MySQL instance"),"  for change capture, by ensuring backups are enabled and binlog is turned on.\n",(0,n.yg)("img",{src:t(53261).A})),(0,n.yg)("p",null,"Now, proceed to create endpoints in DMS that capture MySQL data and  ",(0,n.yg)("a",{parentName:"p",href:"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3"},"store in S3, as parquet files"),"."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Source ",(0,n.yg)("em",{parentName:"li"},"hudi-source-db")," endpoint, points to the DB server and provides basic authentication details"),(0,n.yg)("li",{parentName:"ul"},"Target ",(0,n.yg)("em",{parentName:"li"},"parquet-s3")," endpoint, points to the bucket and folder on s3 to store the change logs records as parquet files\n",(0,n.yg)("img",{src:t(13755).A}),(0,n.yg)("img",{src:t(35424).A}),(0,n.yg)("img",{src:t(70239).A}))),(0,n.yg)("p",null,"Then proceed to create a migration task, as below. Give it a name, connect the source to the target and be sure to pick the right ",(0,n.yg)("em",{parentName:"p"},"Migration type")," as shown below, to ensure ongoing changes are continuously replicated to S3. Also make sure to specify, the rules using which DMS decides which MySQL schema/tables to replicate. In this example, we simply whitelist ",(0,n.yg)("em",{parentName:"p"},"orders")," table under the ",(0,n.yg)("em",{parentName:"p"},"hudi_dms")," schema, as specified in the table SQL above."),(0,n.yg)("p",null,(0,n.yg)("img",{src:t(98847).A}),"\n",(0,n.yg)("img",{src:t(85284).A})),(0,n.yg)("p",null,"Starting the DMS task and should result in an initial load, like below."),(0,n.yg)("p",null,(0,n.yg)("img",{src:t(73700).A})),(0,n.yg)("p",null,"Simply reading the raw initial load file, shoud give the same values as the upstream table"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-scala"},'scala> spark.read.parquet("s3://hudi-dms-demo/orders/hudi_dms/orders/*").sort("updated_at").show\n \n+--------+---------+-------------+-------------------+-------------------+\n|order_id|order_qty|customer_name|         updated_at|         created_at|\n+--------+---------+-------------+-------------------+-------------------+\n|       2|       10|        peter|2020-01-20 20:12:22|2020-01-20 20:12:22|\n|       1|       10|       victor|2020-01-20 20:12:31|2020-01-20 20:12:31|\n+--------+---------+-------------+-------------------+-------------------+\n\n')),(0,n.yg)("h2",{id:"applying-change-logs-using-hudi-deltastreamer"},"Applying Change Logs using Hudi DeltaStreamer"),(0,n.yg)("p",null,"Now, we are ready to start consuming the change logs. Hudi DeltaStreamer runs as Spark job on your favorite workflow scheduler (it also supports a continuous mode using ",(0,n.yg)("em",{parentName:"p"},"--continuous")," flag, where it runs as a long running Spark job), that tails a given path on S3 (or any DFS implementation) for new files and can issue an ",(0,n.yg)("em",{parentName:"p"},"upsert")," to a target hudi dataset. The tool automatically checkpoints itself and thus to repeatedly ingest, all one needs to do is to keep executing the DeltaStreamer periodically."),(0,n.yg)("p",null,"With an initial load already on S3, we then run the following command (deltastreamer command, here on) to ingest the full load first and create a Hudi dataset on S3."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-bash"},"spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer  \\\n  --packages org.apache.spark:spark-avro_2.11:2.4.4 \\\n  --master yarn --deploy-mode client \\\n  hudi-utilities-bundle_2.11-0.5.1-SNAPSHOT.jar \\\n  --table-type COPY_ON_WRITE \\\n  --source-ordering-field updated_at \\\n  --source-class org.apache.hudi.utilities.sources.ParquetDFSSource \\\n  --target-base-path s3://hudi-dms-demo/hudi_orders --target-table hudi_orders \\\n  --transformer-class org.apache.hudi.utilities.transform.AWSDmsTransformer \\\n  --payload-class org.apache.hudi.payload.AWSDmsAvroPayload \\\n  --hoodie-conf hoodie.datasource.write.recordkey.field=order_id,hoodie.datasource.write.partitionpath.field=customer_name,hoodie.deltastreamer.source.dfs.root=s3://hudi-dms-demo/orders/hudi_dms/orders\n")),(0,n.yg)("p",null,"A few things are going on here"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"First, we specify the ",(0,n.yg)("em",{parentName:"li"},"--table-type")," as COPY_ON_WRITE. Hudi also supports another _MERGE_ON_READ ty_pe you can use if you choose from."),(0,n.yg)("li",{parentName:"ul"},"To handle cases where the input parquet files contain multiple updates/deletes or insert/updates to the same record, we use ",(0,n.yg)("em",{parentName:"li"},"updated_at")," as the ordering field. This ensures that the change record which has the latest timestamp will be reflected in Hudi."),(0,n.yg)("li",{parentName:"ul"},"We specify a target base path and a table table, all needed for creating and writing to the Hudi table"),(0,n.yg)("li",{parentName:"ul"},"We use a special payload class - ",(0,n.yg)("em",{parentName:"li"},"AWSDMSAvroPayload")," , to handle the different change operations correctly. The parquet files generated have an ",(0,n.yg)("em",{parentName:"li"},"Op")," field, that indicates whether a given change record is an insert (I), delete (D) or update (U) and the payload implementation uses this field to decide how to handle a given change record."),(0,n.yg)("li",{parentName:"ul"},"You may also notice a special transformer class ",(0,n.yg)("em",{parentName:"li"},"AWSDmsTransformer")," , being specified. The reason here is tactical, but important. The initial load file does not contain an ",(0,n.yg)("em",{parentName:"li"},"Op")," field, so this adds one to Hudi table schema additionally."),(0,n.yg)("li",{parentName:"ul"},"Finally, we specify the record key for the Hudi table as same as the upstream table. Then we specify partitioning by ",(0,n.yg)("em",{parentName:"li"},"customer_name"),"  and also the root of the DMS output.")),(0,n.yg)("p",null,"Once the command is run, the Hudi table should be created and have same records as the upstream table (with all the _hoodie fields as well)."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-scala"},'scala> spark.read.format("org.apache.hudi").load("s3://hudi-dms-demo/hudi_orders/*/*.parquet").show\n+-------------------+--------------------+------------------+----------------------+--------------------+--------+---------+-------------+-------------------+-------------------+---+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|order_id|order_qty|customer_name|         updated_at|         created_at| Op|\n+-------------------+--------------------+------------------+----------------------+--------------------+--------+---------+-------------+-------------------+-------------------+---+\n|     20200120205028|  20200120205028_0_1|                 2|                 peter|af9a2525-a486-40e...|       2|       10|        peter|2020-01-20 20:12:22|2020-01-20 20:12:22|   |\n|     20200120205028|  20200120205028_1_1|                 1|                victor|8e431ece-d51c-4c7...|       1|       10|       victor|2020-01-20 20:12:31|2020-01-20 20:12:31|   |\n+-------------------+--------------------+------------------+----------------------+--------------------+--------+---------+-------------+-------------------+-------------------+---+\n')),(0,n.yg)("p",null,"Now, let's do an insert and an update"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"INSERT INTO orders(order_id, order_qty, customer_name) VALUES(3, 30, 'sandy');\nUPDATE orders set order_qty = 20 where order_id = 2;\n")),(0,n.yg)("p",null,"This will add a new parquet file to the DMS output folder and when the deltastreamer command is run again, it will go ahead and apply these to the Hudi table."),(0,n.yg)("p",null,"So, querying the Hudi table now would yield 3 rows and the ",(0,n.yg)("em",{parentName:"p"},"hoodie_commit_time")," accurately reflects when these writes happened. You can notice that order_qty for order_id=2, is updated from 10 to 20!"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-bash"},"+-------------------+--------------------+------------------+----------------------+--------------------+---+--------+---------+-------------+-------------------+-------------------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| Op|order_id|order_qty|customer_name|         updated_at|         created_at|\n+-------------------+--------------------+------------------+----------------------+--------------------+---+--------+---------+-------------+-------------------+-------------------+\n|     20200120211526|  20200120211526_0_1|                 2|                 peter|af9a2525-a486-40e...|  U|       2|       20|        peter|2020-01-20 21:11:47|2020-01-20 20:12:22|\n|     20200120211526|  20200120211526_1_1|                 3|                 sandy|566eb34a-e2c5-44b...|  I|       3|       30|        sandy|2020-01-20 21:11:24|2020-01-20 21:11:24|\n|     20200120205028|  20200120205028_1_1|                 1|                victor|8e431ece-d51c-4c7...|   |       1|       10|       victor|2020-01-20 20:12:31|2020-01-20 20:12:31|\n+-------------------+--------------------+------------------+----------------------+--------------------+---+--------+---------+-------------+-------------------+-------------------+\n")),(0,n.yg)("p",null,"A nice debugging aid would be read all of the DMS output now and sort it by update_at, which should give us a sequence of changes that happened on the upstream table. As we can see, the Hudi table above is a compacted snapshot of this raw change log."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-bash"},"+----+--------+---------+-------------+-------------------+-------------------+\n|  Op|order_id|order_qty|customer_name|         updated_at|         created_at|\n+----+--------+---------+-------------+-------------------+-------------------+\n|null|       2|       10|        peter|2020-01-20 20:12:22|2020-01-20 20:12:22|\n|null|       1|       10|       victor|2020-01-20 20:12:31|2020-01-20 20:12:31|\n|   I|       3|       30|        sandy|2020-01-20 21:11:24|2020-01-20 21:11:24|\n|   U|       2|       20|        peter|2020-01-20 21:11:47|2020-01-20 20:12:22|\n+----+--------+---------+-------------+-------------------+-------------------+\n")),(0,n.yg)("p",null,"Initial load with no ",(0,n.yg)("em",{parentName:"p"},"Op")," field value , followed by an insert and an update."),(0,n.yg)("p",null,"Now, lets do deletes an inserts"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"DELETE FROM orders WHERE order_id = 2;\nINSERT INTO orders(order_id, order_qty, customer_name) VALUES(4, 40, 'barry');\nINSERT INTO orders(order_id, order_qty, customer_name) VALUES(5, 50, 'nathan');\n")),(0,n.yg)("p",null,"This should result in more files on S3, written by DMS , which the DeltaStreamer command will continue to process incrementally (i.e only the newly written files are read each time)"),(0,n.yg)("p",null,(0,n.yg)("img",{src:t(23608).A})),(0,n.yg)("p",null,"Running the deltastreamer command again, would result in the follow state for the Hudi table. You can notice the two new records and that the ",(0,n.yg)("em",{parentName:"p"},"order_id=2")," is now gone"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-bash"},"+-------------------+--------------------+------------------+----------------------+--------------------+---+--------+---------+-------------+-------------------+-------------------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| Op|order_id|order_qty|customer_name|         updated_at|         created_at|\n+-------------------+--------------------+------------------+----------------------+--------------------+---+--------+---------+-------------+-------------------+-------------------+\n|     20200120212522|  20200120212522_1_1|                 5|                nathan|3da94b20-c70b-457...|  I|       5|       50|       nathan|2020-01-20 21:23:00|2020-01-20 21:23:00|\n|     20200120212522|  20200120212522_2_1|                 4|                 barry|8cc46715-8f0f-48a...|  I|       4|       40|        barry|2020-01-20 21:22:49|2020-01-20 21:22:49|\n|     20200120211526|  20200120211526_1_1|                 3|                 sandy|566eb34a-e2c5-44b...|  I|       3|       30|        sandy|2020-01-20 21:11:24|2020-01-20 21:11:24|\n|     20200120205028|  20200120205028_1_1|                 1|                victor|8e431ece-d51c-4c7...|   |       1|       10|       victor|2020-01-20 20:12:31|2020-01-20 20:12:31|\n+-------------------+--------------------+------------------+----------------------+--------------------+---+--------+---------+-------------+-------------------+-------------------+\n")),(0,n.yg)("p",null,"Our little informal change log query yields the following."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-bash"},"+----+--------+---------+-------------+-------------------+-------------------+\n|  Op|order_id|order_qty|customer_name|         updated_at|         created_at|\n+----+--------+---------+-------------+-------------------+-------------------+\n|null|       2|       10|        peter|2020-01-20 20:12:22|2020-01-20 20:12:22|\n|null|       1|       10|       victor|2020-01-20 20:12:31|2020-01-20 20:12:31|\n|   I|       3|       30|        sandy|2020-01-20 21:11:24|2020-01-20 21:11:24|\n|   U|       2|       20|        peter|2020-01-20 21:11:47|2020-01-20 20:12:22|\n|   D|       2|       20|        peter|2020-01-20 21:11:47|2020-01-20 20:12:22|\n|   I|       4|       40|        barry|2020-01-20 21:22:49|2020-01-20 21:22:49|\n|   I|       5|       50|       nathan|2020-01-20 21:23:00|2020-01-20 21:23:00|\n+----+--------+---------+-------------+-------------------+-------------------+\n")),(0,n.yg)("p",null,"Note that the delete and update have the same ",(0,n.yg)("em",{parentName:"p"},"updated_at,")," value. thus it can very well order differently here.. In short this way of looking at the changelog has its caveats. For a true changelog of the Hudi table itself, you can issue an ",(0,n.yg)("a",{parentName:"p",href:"http://hudi.apache.org/docs/querying_data"},"incremental query"),"."),(0,n.yg)("p",null,"And Life goes on ..... Hope this was useful to all the data engineers out there!"))}g.isMDXComponent=!0},99046:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Export Hudi datasets as a copy or as different formats",excerpt:"Learn how to copy or export HUDI dataset in various formats.",author:"rxu",category:"blog",tags:["how-to","snapshot exporter","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2020/03/22/exporting-hudi-datasets",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-03-22-exporting-hudi-datasets.md",source:"@site/blog/2020-03-22-exporting-hudi-datasets.md",title:"Export Hudi datasets as a copy or as different formats",description:"Copy to Hudi dataset",date:"2020-03-22T00:00:00.000Z",formattedDate:"March 22, 2020",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"snapshot exporter",permalink:"/cn/blog/tags/snapshot-exporter"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:1.695,truncated:!0,authors:[{name:"rxu"}],prevItem:{title:"Apache Hudi Support on Apache Zeppelin",permalink:"/cn/blog/2020/04/27/apache-hudi-apache-zepplin"},nextItem:{title:"Change Capture Using AWS Database Migration Service and Hudi",permalink:"/cn/blog/2020/01/20/change-capture-using-aws"}},l={authorsImageUrls:[void 0]},d=[{value:"Copy to Hudi dataset",id:"copy-to-hudi-dataset",children:[],level:3},{value:"Export to json or parquet dataset",id:"export-to-json-or-parquet-dataset",children:[],level:3},{value:"Re-partitioning",id:"re-partitioning",children:[{value:"<code>--output-partition-field</code>",id:"--output-partition-field",children:[],level:4},{value:"<code>--output-partitioner</code>",id:"--output-partitioner",children:[],level:4}],level:3}],c={toc:d},p="wrapper";function g(e){let{components:a,...t}=e;return(0,n.yg)(p,(0,i.A)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("h3",{id:"copy-to-hudi-dataset"},"Copy to Hudi dataset"),(0,n.yg)("p",null,"Similar to the existing  ",(0,n.yg)("inlineCode",{parentName:"p"},"HoodieSnapshotCopier"),", the Exporter scans the source dataset and then makes a copy of it to the target output path."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-bash"},'spark-submit \\\n  --jars "packaging/hudi-spark-bundle/target/hudi-spark-bundle_2.11-0.6.0-SNAPSHOT.jar" \\\n  --deploy-mode "client" \\\n  --class "org.apache.hudi.utilities.HoodieSnapshotExporter" \\\n      packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.6.0-SNAPSHOT.jar \\\n  --source-base-path "/tmp/" \\\n  --target-output-path "/tmp/exported/hudi/" \\\n  --output-format "hudi"\n')),(0,n.yg)("h3",{id:"export-to-json-or-parquet-dataset"},"Export to json or parquet dataset"),(0,n.yg)("p",null,'The Exporter can also convert the source dataset into other formats. Currently only "json" and "parquet" are supported.'),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-bash"},'spark-submit \\\n  --jars "packaging/hudi-spark-bundle/target/hudi-spark-bundle_2.11-0.6.0-SNAPSHOT.jar" \\\n  --deploy-mode "client" \\\n  --class "org.apache.hudi.utilities.HoodieSnapshotExporter" \\\n      packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.6.0-SNAPSHOT.jar \\\n  --source-base-path "/tmp/" \\\n  --target-output-path "/tmp/exported/json/" \\\n  --output-format "json"  # or "parquet"\n')),(0,n.yg)("h3",{id:"re-partitioning"},"Re-partitioning"),(0,n.yg)("p",null,"When export to a different format, the Exporter takes parameters to do some custom re-partitioning. By default, if neither of the 2 parameters below is given, the output dataset will have no partition."),(0,n.yg)("h4",{id:"--output-partition-field"},(0,n.yg)("inlineCode",{parentName:"h4"},"--output-partition-field")),(0,n.yg)("p",null,"This parameter uses an existing non-metadata field as the output partitions. All  ",(0,n.yg)("inlineCode",{parentName:"p"},"_hoodie_*"),"  metadata field will be stripped during export."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-bash"},'spark-submit \\\n  --jars "packaging/hudi-spark-bundle/target/hudi-spark-bundle_2.11-0.6.0-SNAPSHOT.jar" \\\n  --deploy-mode "client" \\\n  --class "org.apache.hudi.utilities.HoodieSnapshotExporter" \\\n      packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.6.0-SNAPSHOT.jar \\  \n  --source-base-path "/tmp/" \\\n  --target-output-path "/tmp/exported/json/" \\\n  --output-format "json" \\\n  --output-partition-field "symbol"  # assume the source dataset contains a field `symbol`\n')),(0,n.yg)("p",null,"The output directory will look like this"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-bash"},"`_SUCCESS symbol=AMRS symbol=AYX symbol=CDMO symbol=CRC symbol=DRNA ...`\n")),(0,n.yg)("h4",{id:"--output-partitioner"},(0,n.yg)("inlineCode",{parentName:"h4"},"--output-partitioner")),(0,n.yg)("p",null,"This parameter takes in a fully-qualified name of a class that implements  ",(0,n.yg)("inlineCode",{parentName:"p"},"HoodieSnapshotExporter.Partitioner"),". This parameter takes higher precedence than  ",(0,n.yg)("inlineCode",{parentName:"p"},"--output-partition-field"),", which will be ignored if this is provided."),(0,n.yg)("p",null,"An example implementation is shown below:"),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"MyPartitioner.java")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'package com.foo.bar;\npublic class MyPartitioner implements HoodieSnapshotExporter.Partitioner {\n\n  private static final String PARTITION_NAME = "date";\n \n  @Override\n  public DataFrameWriter<Row> partition(Dataset<Row> source) {\n    // use the current hoodie partition path as the output partition\n    return source\n        .withColumnRenamed(HoodieRecord.PARTITION_PATH_METADATA_FIELD, PARTITION_NAME)\n        .repartition(new Column(PARTITION_NAME))\n        .write()\n        .partitionBy(PARTITION_NAME);\n  }\n}\n')),(0,n.yg)("p",null,"After putting this class in ",(0,n.yg)("inlineCode",{parentName:"p"},"my-custom.jar"),", which is then placed on the job classpath, the submit command will look like this:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-bash"},'spark-submit \\\n  --jars "packaging/hudi-spark-bundle/target/hudi-spark-bundle_2.11-0.6.0-SNAPSHOT.jar,my-custom.jar" \\\n  --deploy-mode "client" \\\n  --class "org.apache.hudi.utilities.HoodieSnapshotExporter" \\\n      packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.6.0-SNAPSHOT.jar \\\n  --source-base-path "/tmp/" \\\n  --target-output-path "/tmp/exported/json/" \\\n  --output-format "json" \\\n  --output-partitioner "com.foo.bar.MyPartitioner"\n')))}g.isMDXComponent=!0},81900:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Apache Hudi Support on Apache Zeppelin",excerpt:"Integrating HUDI's real-time and read-optimized query capabilities into Apache Zeppelin\u2019s notebook",author:"leesf",category:"blog",tags:["how-to","apache zeppelin","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2020/04/27/apache-hudi-apache-zepplin",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-04-27-apache-hudi-apache-zepplin.md",source:"@site/blog/2020-04-27-apache-hudi-apache-zepplin.md",title:"Apache Hudi Support on Apache Zeppelin",description:"1. Introduction",date:"2020-04-27T00:00:00.000Z",formattedDate:"April 27, 2020",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"apache zeppelin",permalink:"/cn/blog/tags/apache-zeppelin"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:2.23,truncated:!0,authors:[{name:"leesf"}],prevItem:{title:"Monitor Hudi metrics with Datadog",permalink:"/cn/blog/2020/05/28/monitoring-hudi-metrics-with-datadog"},nextItem:{title:"Export Hudi datasets as a copy or as different formats",permalink:"/cn/blog/2020/03/22/exporting-hudi-datasets"}},l={authorsImageUrls:[void 0]},d=[{value:"1. Introduction",id:"1-introduction",children:[],level:2},{value:"2. Achieve the effect",id:"2-achieve-the-effect",children:[{value:"2.1 Hive",id:"21-hive",children:[],level:3},{value:"2.1.1 Read optimized view",id:"211-read-optimized-view",children:[],level:3},{value:"2.1.2 Real-time view",id:"212-real-time-view",children:[],level:3},{value:"2.2 Spark SQL",id:"22-spark-sql",children:[],level:3},{value:"2.2.1 Read optimized view",id:"221-read-optimized-view",children:[],level:3},{value:"2.2.2 Real-time view",id:"222-real-time-view",children:[],level:3}],level:2},{value:"3. Common problems",id:"3-common-problems",children:[{value:"3.1 Hudi package adaptation",id:"31-hudi-package-adaptation",children:[],level:3},{value:"3.2 Parquet jar package adaptation",id:"32-parquet-jar-package-adaptation",children:[],level:3},{value:"3.3 Spark Interpreter adaptation",id:"33-spark-interpreter-adaptation",children:[],level:3}],level:2},{value:"4. Hudi incremental view",id:"4-hudi-incremental-view",children:[],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...o}=e;return(0,n.yg)(p,(0,i.A)({},c,o,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("h2",{id:"1-introduction"},"1. Introduction"),(0,n.yg)("p",null,"Apache Zeppelin is a web-based notebook that provides interactive data analysis. It is convenient for you to make beautiful documents that can be data-driven, interactive, and collaborative, and supports multiple languages, including Scala (using Apache Spark), Python (Apache Spark), SparkSQL, Hive, Markdown, Shell, and so on. Hive and SparkSQL currently support querying Hudi\u2019s read-optimized view and real-time view. So in theory, Zeppelin\u2019s notebook should also have such query capabilities."),(0,n.yg)("h2",{id:"2-achieve-the-effect"},"2. Achieve the effect"),(0,n.yg)("h3",{id:"21-hive"},"2.1 Hive"),(0,n.yg)("h3",{id:"211-read-optimized-view"},"2.1.1 Read optimized view"),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Read Optimized View",src:t(91030).A})),(0,n.yg)("h3",{id:"212-real-time-view"},"2.1.2 Real-time view"),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Real-time View",src:t(94838).A})),(0,n.yg)("h3",{id:"22-spark-sql"},"2.2 Spark SQL"),(0,n.yg)("h3",{id:"221-read-optimized-view"},"2.2.1 Read optimized view"),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Read Optimized View",src:t(15256).A})),(0,n.yg)("h3",{id:"222-real-time-view"},"2.2.2 Real-time view"),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Real-time View",src:t(23940).A})),(0,n.yg)("h2",{id:"3-common-problems"},"3. Common problems"),(0,n.yg)("h3",{id:"31-hudi-package-adaptation"},"3.1 Hudi package adaptation"),(0,n.yg)("p",null,"Zeppelin will load the packages under lib by default when starting. For external dependencies such as Hudi, it is suitable to be placed directly under zeppelin / lib to avoid Hive or Spark SQL not finding the corresponding Hudi dependency on the cluster."),(0,n.yg)("h3",{id:"32-parquet-jar-package-adaptation"},"3.2 Parquet jar package adaptation"),(0,n.yg)("p",null,"The parquet version of the Hudi package is 1.10, and the current parquet version of the CDH cluster is 1.9, so when executing the Hudi table query, many jar package conflict errors will be reported."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Solution"),": upgrade the parquet package to 1.10 in the spark / jars directory of the node where zepeelin is located.\n",(0,n.yg)("strong",{parentName:"p"},"Side effects"),": The tasks of saprk jobs other than zeppelin assigned to the cluster nodes of parquet 1.10 may fail.\n",(0,n.yg)("strong",{parentName:"p"},"Suggestions"),": Clients other than zeppelin will also have jar conflicts. Therefore, it is recommended to fully upgrade the spark jar, parquet jar and related dependent jars of the cluster to better adapt to Hudi\u2019s capabilities."),(0,n.yg)("h3",{id:"33-spark-interpreter-adaptation"},"3.3 Spark Interpreter adaptation"),(0,n.yg)("p",null,"The same SQL using Spark SQL query on Zeppelin will have more records than the hive query."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Cause of the problem"),": When reading and writing Parquet tables to the Hive metastore, Spark SQL will use the Parquet SerDe (SerDe: Serialize / Deserilize for short) for Spark serialization and deserialization, not the Hive\u2019s SerDe, because Spark SQL\u2019s own SerDe has better performance."),(0,n.yg)("p",null,"This causes Spark SQL to only query Hudi\u2019s pipeline records, not the final merge result."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Solution"),": set ",(0,n.yg)("inlineCode",{parentName:"p"},"spark.sql.hive.convertMetastoreParquet=false")),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Method 1"),": Edit properties directly on the page**\n",(0,n.yg)("img",{src:t(33346).A})),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Method 2"),": Edit ",(0,n.yg)("inlineCode",{parentName:"li"},"zeppelin / conf / interpreter.json")," and add**")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-json"},'"spark.sql.hive.convertMetastoreParquet": {\n  "name": "spark.sql.hive.convertMetastoreParquet",\n  "value": false,\n  "type": "checkbox"\n}\n')),(0,n.yg)("h2",{id:"4-hudi-incremental-view"},"4. Hudi incremental view"),(0,n.yg)("p",null,"For Hudi incremental view, currently only supports pulling by writing Spark code. Considering that Zeppelin has the ability to execute code and shell commands directly on the notebook, later consider packaging these notebooks to query Hudi incremental views in a way that supports SQL."))}g.isMDXComponent=!0},52516:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Monitor Hudi metrics with Datadog",excerpt:"Introducing the feature of reporting Hudi metrics via Datadog HTTP API",author:"rxu",category:"blog",tags:["how-to","metrics","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2020/05/28/monitoring-hudi-metrics-with-datadog",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-05-28-monitoring-hudi-metrics-with-datadog.md",source:"@site/blog/2020-05-28-monitoring-hudi-metrics-with-datadog.md",title:"Monitor Hudi metrics with Datadog",description:"Availability",date:"2020-05-28T00:00:00.000Z",formattedDate:"May 28, 2020",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"metrics",permalink:"/cn/blog/tags/metrics"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:1.415,truncated:!0,authors:[{name:"rxu"}],prevItem:{title:"The Apache Software Foundation Announces Apache\xae Hudi\u2122 as a Top-Level Project",permalink:"/cn/blog/2020/06/04/The-Apache-Software-Foundation-Announces-Apache-Hudi-as-a-Top-Level-Project"},nextItem:{title:"Apache Hudi Support on Apache Zeppelin",permalink:"/cn/blog/2020/04/27/apache-hudi-apache-zepplin"}},l={authorsImageUrls:[void 0]},d=[{value:"Availability",id:"availability",children:[],level:2},{value:"Introduction",id:"introduction",children:[],level:2},{value:"Configurations",id:"configurations",children:[],level:2},{value:"Demo",id:"demo",children:[],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...o}=e;return(0,n.yg)(p,(0,i.A)({},c,o,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("h2",{id:"availability"},"Availability"),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"0.6.0 (unreleased)")),(0,n.yg)("h2",{id:"introduction"},"Introduction"),(0,n.yg)("p",null,(0,n.yg)("a",{parentName:"p",href:"https://www.datadoghq.com/"},"Datadog")," is a popular monitoring service. In the upcoming ",(0,n.yg)("inlineCode",{parentName:"p"},"0.6.0")," release of Apache Hudi, we will introduce the feature of reporting Hudi metrics via Datadog HTTP API, in addition to the current reporter types: Graphite and JMX."),(0,n.yg)("h2",{id:"configurations"},"Configurations"),(0,n.yg)("p",null,"Similar to other supported reporters, turning on Datadog reporter requires these 2 properties."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-properties"},"hoodie.metrics.on=true\nhoodie.metrics.reporter.type=DATADOG\n")),(0,n.yg)("p",null,"The following property sets the Datadog API site. It determines whether the requests will be sent to ",(0,n.yg)("inlineCode",{parentName:"p"},"api.datadoghq.eu")," (EU) or ",(0,n.yg)("inlineCode",{parentName:"p"},"api.datadoghq.com")," (US). Set this according to your Datadog account settings."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-properties"},"hoodie.metrics.datadog.api.site=EU # or US\n")),(0,n.yg)("p",null,"The property ",(0,n.yg)("inlineCode",{parentName:"p"},"hoodie.metrics.datadog.api.key")," allows you to set the api key directly from the configuration. "),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-properties"},"hoodie.metrics.datadog.api.key=<your api key>\nhoodie.metrics.datadog.api.key.supplier=<your api key supplier>\n")),(0,n.yg)("p",null,"Due to security consideration in some cases, you may prefer to return the api key at runtime. To go with this approach, implement ",(0,n.yg)("inlineCode",{parentName:"p"},"java.util.function.Supplier<String>")," and set the implementation's FQCN to ",(0,n.yg)("inlineCode",{parentName:"p"},"hoodie.metrics.datadog.api.key.supplier"),", and make sure ",(0,n.yg)("inlineCode",{parentName:"p"},"hoodie.metrics.datadog.api.key")," is ",(0,n.yg)("em",{parentName:"p"},"not")," set since it will take higher precedence."),(0,n.yg)("p",null,"The following property helps segregate metrics by setting different prefixes for different jobs. "),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-properties"},"hoodie.metrics.datadog.metric.prefix=<your metrics prefix>\n")),(0,n.yg)("p",null,"Note that it will use ",(0,n.yg)("inlineCode",{parentName:"p"},".")," to delimit the prefix and the metric name. For example, if the prefix is set to ",(0,n.yg)("inlineCode",{parentName:"p"},"foo"),", then ",(0,n.yg)("inlineCode",{parentName:"p"},"foo.")," will be prepended to the metric name."),(0,n.yg)("p",null,"There are other optional properties, which are explained in the configuration reference page."),(0,n.yg)("h2",{id:"demo"},"Demo"),(0,n.yg)("p",null,"In this demo, we ran a ",(0,n.yg)("inlineCode",{parentName:"p"},"HoodieDeltaStreamer")," job with metrics turn on and other configurations set properly. "),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"datadog metrics demo",src:t(96346).A})),(0,n.yg)("p",null,"As shown above, we were able to collect Hudi's action-related metrics like"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"<prefix>.<table name>.commit.totalScanTime")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"<prefix>.<table name>.clean.duration")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"<prefix>.<table name>.index.lookup.duration"))),(0,n.yg)("p",null,"as well as ",(0,n.yg)("inlineCode",{parentName:"p"},"HoodieDeltaStreamer"),"-specific metrics"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"<prefix>.<table name>.deltastreamer.duration")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"<prefix>.<table name>.deltastreamer.hiveSyncDuration"))))}g.isMDXComponent=!0},5711:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"The Apache Software Foundation Announces Apache\xae Hudi\u2122 as a Top-Level Project",category:"blog",image:"/assets/images/asf_logo.svg",tags:["blog","apache"]},s=void 0,l={permalink:"/cn/blog/2020/06/04/The-Apache-Software-Foundation-Announces-Apache-Hudi-as-a-Top-Level-Project",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-06-04-The-Apache-Software-Foundation-Announces-Apache-Hudi-as-a-Top-Level-Project.mdx",source:"@site/blog/2020-06-04-The-Apache-Software-Foundation-Announces-Apache-Hudi-as-a-Top-Level-Project.mdx",title:"The Apache Software Foundation Announces Apache\xae Hudi\u2122 as a Top-Level Project",description:"Redirecting... please wait!!",date:"2020-06-04T00:00:00.000Z",formattedDate:"June 4, 2020",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache",permalink:"/cn/blog/tags/apache"}],readingTime:.045,truncated:!1,authors:[],prevItem:{title:"Building a Large-scale Transactional Data Lake at Uber Using Apache Hudi",permalink:"/cn/blog/2020/06/09/Building-a-Large-scale-Transactional-Data-Lake-at-Uber-Using-Apache-Hudi"},nextItem:{title:"Monitor Hudi metrics with Datadog",permalink:"/cn/blog/2020/05/28/monitoring-hudi-metrics-with-datadog"}},d={authorsImageUrls:[]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://blogs.apache.org/foundation/entry/the-apache-software-foundation-announces64",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},31205:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Building a Large-scale Transactional Data Lake at Uber Using Apache Hudi",authors:[{name:"Nishith Agarwal"}],category:"blog",image:"/assets/images/blog/2020-06-09-Building-a-Large-scale-Transactional-Data-Lake-at-Uber-Using-Apache-Hudi.png",tags:["use-case","datalake","analytics at scale","uber"]},s=void 0,l={permalink:"/cn/blog/2020/06/09/Building-a-Large-scale-Transactional-Data-Lake-at-Uber-Using-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-06-09-Building-a-Large-scale-Transactional-Data-Lake-at-Uber-Using-Apache-Hudi.mdx",source:"@site/blog/2020-06-09-Building-a-Large-scale-Transactional-Data-Lake-at-Uber-Using-Apache-Hudi.mdx",title:"Building a Large-scale Transactional Data Lake at Uber Using Apache Hudi",description:"Redirecting... please wait!!",date:"2020-06-09T00:00:00.000Z",formattedDate:"June 9, 2020",tags:[{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"datalake",permalink:"/cn/blog/tags/datalake"},{label:"analytics at scale",permalink:"/cn/blog/tags/analytics-at-scale"},{label:"uber",permalink:"/cn/blog/tags/uber"}],readingTime:.045,truncated:!1,authors:[{name:"Nishith Agarwal"}],prevItem:{title:"Apache Hudi grows cloud data lake maturity",permalink:"/cn/blog/2020/06/16/Apache-Hudi-grows-cloud-data-lake-maturity"},nextItem:{title:"The Apache Software Foundation Announces Apache\xae Hudi\u2122 as a Top-Level Project",permalink:"/cn/blog/2020/06/04/The-Apache-Software-Foundation-Announces-Apache-Hudi-as-a-Top-Level-Project"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://eng.uber.com/apache-hudi-graduation/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},80674:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Apache Hudi grows cloud data lake maturity",authors:[{name:"Sean Michael Kerner"}],category:"blog",image:"/assets/images/blog/2020-06-16-Apache-Hudi-grows-cloud-data-lake-maturity.jpeg",tags:["blog","techtarget"]},s=void 0,l={permalink:"/cn/blog/2020/06/16/Apache-Hudi-grows-cloud-data-lake-maturity",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-06-16-Apache-Hudi-grows-cloud-data-lake-maturity.mdx",source:"@site/blog/2020-06-16-Apache-Hudi-grows-cloud-data-lake-maturity.mdx",title:"Apache Hudi grows cloud data lake maturity",description:"Redirecting... please wait!!",date:"2020-06-16T00:00:00.000Z",formattedDate:"June 16, 2020",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"techtarget",permalink:"/cn/blog/tags/techtarget"}],readingTime:.045,truncated:!1,authors:[{name:"Sean Michael Kerner"}],prevItem:{title:"PrestoDB and Apache Hudi",permalink:"/cn/blog/2020/08/04/PrestoDB-and-Apache-Hudi"},nextItem:{title:"Building a Large-scale Transactional Data Lake at Uber Using Apache Hudi",permalink:"/cn/blog/2020/06/09/Building-a-Large-scale-Transactional-Data-Lake-at-Uber-Using-Apache-Hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://searchdatamanagement.techtarget.com/news/252484740/Apache-Hudi-grows-cloud-data-lake-maturity",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},40965:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"PrestoDB and Apache Hudi",authors:[{name:"Bhavani Sudha Saktheeswaran"},{name:"Brandon Scheller"}],category:"blog",image:"/assets/images/blog/2020-08-04-PrestoDB-and-Apache-Hudi.png",tags:["blog","prestodb"]},s=void 0,l={permalink:"/cn/blog/2020/08/04/PrestoDB-and-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-08-04-PrestoDB-and-Apache-Hudi.mdx",source:"@site/blog/2020-08-04-PrestoDB-and-Apache-Hudi.mdx",title:"PrestoDB and Apache Hudi",description:"Redirecting... please wait!!",date:"2020-08-04T00:00:00.000Z",formattedDate:"August 4, 2020",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"prestodb",permalink:"/cn/blog/tags/prestodb"}],readingTime:.045,truncated:!1,authors:[{name:"Bhavani Sudha Saktheeswaran"},{name:"Brandon Scheller"}],prevItem:{title:"Incremental Processing on the Data Lake",permalink:"/cn/blog/2020/08/18/hudi-incremental-processing-on-data-lakes"},nextItem:{title:"Apache Hudi grows cloud data lake maturity",permalink:"/cn/blog/2020/06/16/Apache-Hudi-grows-cloud-data-lake-maturity"}},d={authorsImageUrls:[void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://prestodb.io/blog/2020/08/04/prestodb-and-hudi",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},1718:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Incremental Processing on the Data Lake",excerpt:"How Apache Hudi provides ability for incremental data processing.",author:"vinoyang",category:"blog",image:"/assets/images/blog/incr-processing/image7.png",tags:["blog","datalake","incremental processing","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2020/08/18/hudi-incremental-processing-on-data-lakes",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-08-18-hudi-incremental-processing-on-data-lakes.md",source:"@site/blog/2020-08-18-hudi-incremental-processing-on-data-lakes.md",title:"Incremental Processing on the Data Lake",description:"NOTE: This article is a translation of the infoq.cn article, found here, with minor edits",date:"2020-08-18T00:00:00.000Z",formattedDate:"August 18, 2020",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"datalake",permalink:"/cn/blog/tags/datalake"},{label:"incremental processing",permalink:"/cn/blog/tags/incremental-processing"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:17.005,truncated:!0,authors:[{name:"vinoyang"}],prevItem:{title:"Efficient Migration of Large Parquet Tables to Apache Hudi",permalink:"/cn/blog/2020/08/20/efficient-migration-of-large-parquet-tables"},nextItem:{title:"PrestoDB and Apache Hudi",permalink:"/cn/blog/2020/08/04/PrestoDB-and-Apache-Hudi"}},l={authorsImageUrls:[void 0]},d=[{value:"NOTE: This article is a translation of the infoq.cn article, found here, with minor edits",id:"note-this-article-is-a-translation-of-the-infoqcn-article-found-here-with-minor-edits",children:[],level:3},{value:"Traditional data lakes lack the primitives for incremental processing",id:"traditional-data-lakes-lack-the-primitives-for-incremental-processing",children:[],level:2},{value:"The significance of incremental processing for the data lake",id:"the-significance-of-incremental-processing-for-the-data-lake",children:[{value:"Streaming Semantics",id:"streaming-semantics",children:[],level:3},{value:"Warehousing needs Incremental Processing",id:"warehousing-needs-incremental-processing",children:[],level:3},{value:"Quasi-real-time scenarios, resource/efficiency trade-offs",id:"quasi-real-time-scenarios-resourceefficiency-trade-offs",children:[],level:3},{value:"Incremental processing facilitates unified data lake architecture",id:"incremental-processing-facilitates-unified-data-lake-architecture",children:[],level:3}],level:2},{value:"Hudi&#39;s support for incremental processing",id:"hudis-support-for-incremental-processing",children:[],level:2},{value:"Summary",id:"summary",children:[],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...o}=e;return(0,n.yg)(p,(0,i.A)({},c,o,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("h3",{id:"note-this-article-is-a-translation-of-the-infoqcn-article-found-here-with-minor-edits"},"NOTE: This article is a translation of the infoq.cn article, found ",(0,n.yg)("a",{parentName:"h3",href:"https://www.infoq.cn/article/CAgIDpfJBVcJHKJLSbhe"},"here"),", with minor edits"),(0,n.yg)("p",null,'Apache Hudi is a data lake framework which provides the ability to ingest, manage and query large analytical data sets on a distributed file system/cloud stores.\nHudi joined the Apache incubator for incubation in January 2019, and was promoted to the top Apache project in May 2020. This article mainly discusses the importance\nof Hudi to the data lake from the perspective of "incremental processing". More information about Apache Hudi\'s framework functions, features, usage scenarios, and\nlatest developments can be found at ',(0,n.yg)("a",{parentName:"p",href:"https://qconplus.infoq.cn/2020/shanghai/presentation/2646"},"QCon Global Software Development Conference (Shanghai Station) 2020"),"."),(0,n.yg)("p",null,"Throughout the development of big data technology, Hadoop has steadily seized the opportunities of this era and has become the de-facto standard for enterprises to build big data infrastructure.\nAmong them, the distributed file system HDFS that supports the Hadoop ecosystem almost naturally has become the standard interface for big data storage systems. In recent years, with the rise of\ncloud-native architectures, we have seen a wave of newer models embracing low-cost cloud storage emerging, a number of data lake frameworks compatible with HDFS interfaces\nembracing cloud vendor storage have emerged in the industry as well. "),(0,n.yg)("p",null,'However, we are still processing data pretty much in the same way we did 10 years ago. This article will try to talk about its importance to the data lake from the perspective of "incremental processing".'),(0,n.yg)("h2",{id:"traditional-data-lakes-lack-the-primitives-for-incremental-processing"},"Traditional data lakes lack the primitives for incremental processing"),(0,n.yg)("p",null,"In the era of mobile Internet and Internet of Things, delayed arrival of data is very common.\nHere we are involved in the definition of two time semantics: ",(0,n.yg)("a",{parentName:"p",href:"https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/"},"event time and processing time"),". "),(0,n.yg)("p",null,"As the name suggests:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Event time:")," the time when the event actually occurred;"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Processing time:")," the time when an event is observed (processed) in the system;")),(0,n.yg)("p",null,'Ideally, the event time and the processing time are the same, but in reality, they may have more or less deviation, which we often call "Time Skew".\nWhether for low-latency stream computing or common batch processing, the processing of event time and processing time and late data is a common and difficult problem.\nIn general, in order to ensure correctness, when we strictly follow the "event time" semantics, late data will trigger the\n',(0,n.yg)("a",{parentName:"p",href:"https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/stream/operators/windows#late-elements-considerations"},"recalculation of the time window"),'\n(usually Hive partitions for batch processing), although the results of these "windows" may have been calculated or even interacted with the end user.\nFor recalculation, the extensible key-value storage structure is usually used in streaming processing, which is processed incrementally at the record/event level and optimized\nbased on point queries and updates. However, in data lakes, recalculating usually means rewriting the entire (immutable) Hive partition (or simply a folder in DFS), and\nre-triggering the recalculation of cascading tasks that have consumed that Hive partition.'),(0,n.yg)("p",null,"With data lakes supporting massive amounts of data, many long-tail businesses still have a strong demand for updating cold data. However, for a long time,\nthe data in a single partition in the data lake was designed to be non-updatable. If it needs to be updated, the entire partition needs to be rewritten.\nThis will seriously damage the efficiency of the entire ecosystem. From the perspective of latency and resource utilization, these operations on Hadoop will incur expensive overhead.\nBesides, this overhead is usually also cascaded to the entire Hadoop data processing pipeline, which ultimately leads to an increase in latency by several hours."),(0,n.yg)("p",null,"In response to the two problems mentioned above, if the data lake supports fine-grained incremental processing, we can incorporate changes into existing Hive partitions\nmore effectively, and provide a way for downstream table consumers to obtain only the changed data. For effectively supporting incremental processing, we can decompose it into the\nfollowing two primitive operations:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Update insert (upsert):")," Conceptually, rewriting the entire partition can be regarded as a very inefficient upsert operation, which will eventually write much more data than the\noriginal data itself. Therefore, support for (bulk) upsert is considered a very important feature. ",(0,n.yg)("a",{parentName:"p",href:"https://research.google/pubs/pub42851/"},"Google's Mesa")," (Google's data warehouse system) also\ntalks about several techniques that can be applied to rapid data ingestion scenarios.")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Incremental consumption:")," Although upsert can solve the problem of quickly releasing new data to a partition, downstream data consumers do not know\nwhich data has been changed from which time in the past. Usually, consumers can only know the changed data by scanning the entire partition/data table and\nrecalculating all the data, which requires considerable time and resources. Therefore, we also need a mechanism to more efficiently obtain data records that\nhave changed since the last time the partition was consumed."))),(0,n.yg)("p",null,"With the above two primitive operations, you can upsert a data set, and then incrementally consume from it, and create another (also incremental) data set to solve the two problems\nwe mentioned above and support many common cases, so as to support end-to-end incremental processing and reduce end-to-end latency. These two primitives combine with each other,\nunlocking the ability of stream/incremental processing based on DFS abstraction."),(0,n.yg)("p",null,"The storage scale of the data lake far exceeds that of the data warehouse. Although the two have different focuses on the definition of functions,\nthere is still a considerable intersection (of course, there are still disputes and deviations from definition and implementation.\nThis is not the topic this article tries to discuss). In any case, the data lake will support larger analytical data sets with cheaper storage,\nso incremental processing is also very important for it. Next let's discuss the significance of incremental processing for the data lake."),(0,n.yg)("h2",{id:"the-significance-of-incremental-processing-for-the-data-lake"},"The significance of incremental processing for the data lake"),(0,n.yg)("h3",{id:"streaming-semantics"},"Streaming Semantics"),(0,n.yg)("p",null,'It has long been stated that there is a "',(0,n.yg)("a",{parentName:"p",href:"https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying"},"dualism"),'"\nbetween the change log (that is, the "flow" in the conventional sense we understand) and the table.'),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"dualism",src:t(87706).A})),(0,n.yg)("p",null,'The core of this discussion is: if there is a change log, you can use these changes to generate a data table and get the current status. If you update a table,\nyou can record these changes and publish all "change logs" to the table\'s status information. This interchangeable nature is called "stream table duality" for short.'),(0,n.yg)("p",null,'A more general understanding of "stream table duality": when the business system is modifying the data in the MySQL table, MySQL will reflect these changes as Binlog,\nif we publish these continuous Binlog (stream) to Kafka, and then let the downstream processing system subscribe to the Kafka, and use the state store to gradually\naccumulate the intermediate results. Then the current state of this intermediate result can reflects the current snapshot of the table.'),(0,n.yg)("p",null,'If the two primitives mentioned above that support incremental processing can be introduced to the data lake, the above pipeline, which can reflect the\n"stream table duality", is also applicable on the data lake. Based on the first primitive, the data lake can also ingest the Binlog log streams in Kafka,\nand then store these Binlog log streams into "tables" on the data lake. Based on the second primitive, these tables recognize the changed records as "Binlog"\nstreams to support the incremental consumption of subsequent cascading tasks.'),(0,n.yg)("p",null,"Of course, as the data in the data lake needs to be landed on the final file/object storage, considering the trade-off between throughput and write performance,\nBinlog on the data lake reacts to a small batch of change logs over a period of time on the stream. For example, the Apache Hudi community is further trying to\nprovide an incremental view similar to Binlog for different Commits (a Commit refers to a batch of data write commit),\nas shown in the following figure:"),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"idu",src:t(56803).A})),(0,n.yg)("p",null,'Remarks in the "Flag" column:'),(0,n.yg)("p",null,"I: Insert;\nD: Delete;\nU: After image of Update;\nX: Before image of Update;"),(0,n.yg)("p",null,"Based on the above discussion, we can think that incremental processing and stream are naturally compatible, and we can naturally connect them on the data lake."),(0,n.yg)("h3",{id:"warehousing-needs-incremental-processing"},"Warehousing needs Incremental Processing"),(0,n.yg)("p",null,"In the data warehouse, whether it is dimensional modeling or relational modeling theory, it is usually constructed based on the ",(0,n.yg)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Data_warehouse#Design_methods"},"layered design ideas"),".\nIn terms of technical implementation, multiple stages (steps) of a long pipeline are formed by connecting multiple levels of ETL tasks through a workflow scheduling engine,\nas shown in the following figure:"),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"image2",src:t(55176).A})),(0,n.yg)("p",null,"As the main application of the data warehouse, in the OLAP field, for the conventional business scenarios(for no or few changes), there are already some frameworks in the industry\nthat focus on the scenarios where they are good at providing efficient analysis capabilities. However, in the Hadoop data warehouse/data lake ecosystem,\nthere is still no good solution for the analysis scenario of frequent changes of business data."),(0,n.yg)("p",null,"For example, let\u2019s consider the scenario of updating the order status of a travel business. This scenario has a typical long-tail effect:\nyou cannot know whether an order will be billed tomorrow, one month later, or one year later. In this scenario, the order table is the main data table,\nbut usually we will derive other derived tables based on this table to support the modeling of various business scenarios.\nThe initial update takes place in the order table at the ODS level, but the derived tables need to be updated in cascade."),(0,n.yg)("p",null,"For this scenario, in the past, once there is a change, people usually need to find the partition where the data to be updated is located in the Hive order\ntable of the ODS layer, and update the entire partition, besides, the partition of the relevant data of the derived table needs to be updated in cascade."),(0,n.yg)("p",null,"Yes, someone will definitely think of that Kudu's support for Upsert can solve the problem of the old version of Hive missing the first incremental primitive.\nBut the Kudu storage engine has its own limitations:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Performance: additional requirements for the hardware itself;"),(0,n.yg)("li",{parentName:"ol"},"Ecologically: In terms of adapting to mainstream big data computing frameworks and machine learning frameworks, it is far less advantageous than Hive;"),(0,n.yg)("li",{parentName:"ol"},"Cost: requires special maintenance costs and expenses;"),(0,n.yg)("li",{parentName:"ol"},"Did not solve the second primitive of incremental processing mentioned above: the problem of incremental consumption.")),(0,n.yg)("p",null,"In summary, incremental processing has the following advantages on the data lake:"),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Performance improvement:")," Ingesting data usually needs to handle updates, deletes, and enforce unique key constraints. Since incremental primitives support record-level updates,\nit can bring orders of magnitude performance improvements to these operations. "),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Faster ETL/derived Pipelines:")," An ubiquitous next step, once the data has been ingested from external sources is to build derived data pipelines using\nApache Spark/Apache Hive or any other data processing framework to ETL the ingested data for a variety of use-cases like data warehouse,\nmachine learning, or even just analytics. Typically, such processes again rely on batch processing jobs expressed in code or SQL. Such data pipelines can be speed up dramatically,\nby querying one or more input tables using an incremental query instead of a regular snapshot query, resulting in only processing the incremental changes from upstream tables and\nthen upsert or delete the target derived table.Similar to raw data ingestion, in order to reduce the data delay of the modelled table, the ETL job only needs to gradually extract the\nchanged data from the original table and update the previously derived output table instead of rebuilding the entire output table every few hours ."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Unified storage:")," Based on the above two advantages, faster and lighter processing on the existing data lake means that only for the purpose of accessing near real-time data,\nno special storage or data mart is needed."),(0,n.yg)("p",null,"Next, we use two simple examples to illustrate how ",(0,n.yg)("a",{parentName:"p",href:"https://www.oreilly.com/content/ubers-case-for-incremental-processing-on-hadoop/"},"incremental processing")," can speed up the processing\nof pipelines in analytical scenarios. First of all, data projection is the most common and easy to understand case:"),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"image7",src:t(95733).A})),(0,n.yg)("p",null,"This simple example shows that: by upserting new changes into table_1 and establishing a simple projected table (projected_table) through incremental consumption, we can\noperate simpler with lower latency more efficiently projection."),(0,n.yg)("p",null,"Next, for a more complex scenario, we can use incremental processing to support the stream and batch connections supported by the stream computing framework,\nand stream-stream connections (just need to add some additional logic to align window) :"),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"image6",src:t(26636).A})),(0,n.yg)("p",null,"The example in the figure above connects a fact table to multiple dimension tables to create a connected table. This case is one of the rare scenarios where we can save hardware\ncosts while significantly reducing latency."),(0,n.yg)("h3",{id:"quasi-real-time-scenarios-resourceefficiency-trade-offs"},"Quasi-real-time scenarios, resource/efficiency trade-offs"),(0,n.yg)("p",null,"Incremental processing of new data in mini batches can use resources more efficiently. Let's refer to a specific example. We have a Kafka event stream that is pouring in\nat a rate of 10,000 per second. We want to count the number of messages in some dimensions over the past 15 minutes. Many stream processing pipelines use an external/internal\nresult state store (such as RocksDB, Cassandra, ElasticSearch) to save the aggregated count results, and run the containers in resource managers such as YARN/Mesos continuously,\nwhich is very reasonable in less than a five-minute delay window scene. In fact, the YARN container itself has some startup overhead. In addition, in order to improve the\nperformance of writing to result storage system, we usually cache the results before performing batch updates. This kind of protocol requires the container to run continuously."),(0,n.yg)("p",null,"However, in quasi-real-time processing scenarios, these options may not be optimal. To achieve the same effect, you can use short-life containers and optimize overall\nresource utilization. For example, a streaming processor may need to perform six million updates to the result storage system in 15 minutes. However, in the incremental\nbatch mode, we only need to perform an in-memory merge on the accumulated data and update the result storage system only once, then only use the resource container for\nfive minutes. Compared with the pure stream processing mode, the incremental batch processing mode has several times the CPU efficiency improvement, and there are several\norders of magnitude efficiency improvement in updating to the result storage. Basically, this processing method obtains resources on demand, instead of swallowing CPU and\nmemory while waiting for data to be calculated in real time."),(0,n.yg)("h3",{id:"incremental-processing-facilitates-unified-data-lake-architecture"},"Incremental processing facilitates unified data lake architecture"),(0,n.yg)("p",null,"Whether in the data warehouse or in the data lake, data processing is an unavoidable problem. Data processing involves the selection of computing engines and\nthe design of architectures. There are currently two mainstream architectures in the industry: Lambda and Kappa architectures. Each architecture has its own\ncharacteristics and existing problems. Derivative versions of these architectures are also ",(0,n.yg)("a",{parentName:"p",href:"https://www.infoq.cn/article/Uo4pFswlMzBVhq*Y2tB9"},"emerging endlessly"),"."),(0,n.yg)("p",null,"In reality, many enterprises still maintain the implementation of the ",(0,n.yg)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Lambda_architecture"},"Lambda architecture"),".\nThe typical Lambda architecture has two modules for the data processing part: the speed layer and the batch layer."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"image5",src:t(46695).A})),(0,n.yg)("p",null,"They are usually two independent implementations (from code to infrastructure). For example, Flink (formerly Storm) is a popular option on the speed layer,\nwhile MapReduce/Spark can serve as a batch layer. In fact, people often rely on the speed layer to provide updated results (which may not be accurate), and\nonce the data is considered complete, the results of the speed layer are corrected at a later time through the batch layer. With incremental processing,\nwe have the opportunity to implement the Lambda architecture for batch processing and quasi-real-time processing at the code level and infrastructure level in\na unified manner. It typically looks like below:"),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"image3",src:t(69393).A})),(0,n.yg)("p",null,'As we said, you can use SQL or a batch processing framework like Spark to consistently implement your processing logic. The result table is built incrementally,\nand SQL is executed on "new data" like streaming to produce a quick view of the results. The same SQL can be executed periodically on the full amount of data to\ncorrect any inaccurate results (remember, join operations are always tricky!) and produce a more "complete" view of the results. In both cases, we will use the\nsame infrastructure to perform calculations, which can reduce overall operating costs and complexity.'),(0,n.yg)("p",null,"Setting aside the Lambda architecture, even in the Kappa architecture, the first primitive of incremental processing (upsert) also plays an important role.\nUber ",(0,n.yg)("a",{parentName:"p",href:"https://www.slideshare.net/FlinkForward/flink-forward-san-francisco-2019-moving-from-lambda-and-kappa-architectures-to-kappa-at-uber-roshan-naik"},"proposed")," the Kappa + architecture\nbased on this. The Kappa architecture advocates a single stream computing layer sufficient to become a general solution\nfor data processing. Although the batch layer is removed in this model, there are still two problems in the service layer:"),(0,n.yg)("p",null,"Now days many stream processing engines support row-level data processing, which requires that our service layer should also support row-level updates;\nThe trade-offs between data ingestion delay, scanning performance and computing resources and operational complexity are unavoidable."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"image8",src:t(66578).A})),(0,n.yg)("p",null,"However, if our business scenarios have low latency requirements, for example, we can accept a delay of about 10 minutes. And if we can quickly ingest and prepare data on DFS,\neffectively connect and propagate updates to the upper-level modeling data set, Speed Serving in the service layer is unnecessary. Then the service layer can be unified,\ngreatly reducing the overall complexity and resource consumption of the system."),(0,n.yg)("p",null,"Above, we introduced the significance of incremental processing for the data lake. Next, we introduce the implementation and support of incremental processing.\nAmong the three open source data lake frameworks (Apache Hudi/Iceberg, Delta Lake), only Apache Hudi provides good support for incremental processing.\nThis is completely rooted in a framework developed by Uber at the time when it encountered the pain points of data analysis on the Hadoop data lake.\nSo, next, let's introduce how Hudi supports incremental processing."),(0,n.yg)("h2",{id:"hudis-support-for-incremental-processing"},"Hudi's support for incremental processing"),(0,n.yg)("p",null,"Apache Hudi (Hadoop Upserts Deletes and Incrementals) is a top-level project of the Apache Foundation. It allows you to process very large-scale data on\ntop of Hadoop-compatible storage, and it also provides two primitives that enable stream processing on the data lake in addition to classic batch processing."),(0,n.yg)("p",null,'From the naming of the letter "I" denotes "Incremental Processing", we can see that it will support incremental processing as a first class citizen.\nThe two primitives we mentioned at the beginning of this article that support incremental processing are reflected in the following two aspects in Apache Hudi:'),(0,n.yg)("p",null,"Update/Delete operation:Hudi provides support for updating/deleting records, using fine-grained file/record level indexes while providing transactional guarantees\nfor the write operation. Queries process the last such committed snapshot, to produce results.."),(0,n.yg)("p",null,"Change stream: Hudi also provides first-class support for obtaining an incremental stream of all the records that were updated/inserted/deleted in a given table, from a given point-in-time."),(0,n.yg)("p",null,'The specific implementation of the change flow is "incremental view". Hudi is the only one of the three open source data lake frameworks that supports\nthe incremental query feature, with support for record level change streams. The following sample code snippet shows us how to query the incremental view:'),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},'// spark-shell\n// reload data\nspark.\n  read.\n  format("hudi").\n  load(basePath + "/*/*/*/*").\n  createOrReplaceTempView("hudi_trips_snapshot")\n\nval commits = spark.sql("select distinct(_hoodie_commit_time) as commitTime from  hudi_trips_snapshot order by commitTime").map(k => k.getString(0)).take(50)\nval beginTime = commits(commits.length - 2) // commit time we are interested in\n\n// incrementally query data\nval tripsIncrementalDF = spark.read.format("hudi").\n  option(QUERY_TYPE_OPT_KEY, QUERY_TYPE_INCREMENTAL_OPT_VAL).\n  option(BEGIN_INSTANTTIME_OPT_KEY, beginTime).\n  load(basePath)\ntripsIncrementalDF.createOrReplaceTempView("hudi_trips_incremental")\n\nspark.sql("select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from  hudi_trips_incremental where fare > 20.0").show()\n\n')),(0,n.yg)("p",null,'The code snippet above creates a Hudi trip increment table (hudi_trips_incremental), and then queries all the change records in the increment table after the "beginTime" submission time\nand the "cost"  is greater than 20.0. Based on this query, you can create incremental data pipelines on batch data.'),(0,n.yg)("h2",{id:"summary"},"Summary"),(0,n.yg)("p",null,"In this article, we first elaborated many problems caused by the lack of incremental processing primitives in the traditional Hadoop data warehouse due to the trade-off between data integrity\nand latency, and some long-tail applications that rely heavily on updates. Next, we argued that to support incremental processing, we must have at least two primitives: upsert and\nincremental consumption, and explained why these two primitives can solve the problems explained above."),(0,n.yg)("p",null,'Then, we introduced why incremental processing is also important to the data lake. There are many common parts in data processing between the data lake and the data warehouse.\nIn the data warehouse, some "pain points" caused by the lack of incremental processing also exist in the data lake. We elaborated its significance to the data lake from four\naspects: incremental processing of semantics of natural fit flow, the need for analytical scenarios, quasi-real-time scene resource/efficiency trade-offs, and unified lake architecture.'),(0,n.yg)("p",null,"Finally, we introduced the open source data lake storage framework Apache Hudi's support for incremental processing and simple cases."))}g.isMDXComponent=!0},57373:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Efficient Migration of Large Parquet Tables to Apache Hudi",excerpt:"Migrating a large parquet table to Apache Hudi without having to rewrite the entire dataset.",author:"vbalaji",category:"blog",image:"/assets/images/blog/2020-08-20-skeleton.png",tags:["how-to","migration","bootstrap","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2020/08/20/efficient-migration-of-large-parquet-tables",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-08-20-efficient-migration-of-large-parquet-tables.md",source:"@site/blog/2020-08-20-efficient-migration-of-large-parquet-tables.md",title:"Efficient Migration of Large Parquet Tables to Apache Hudi",description:"We will look at how to migrate a large parquet table to Hudi without having to rewrite the entire dataset.",date:"2020-08-20T00:00:00.000Z",formattedDate:"August 20, 2020",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"migration",permalink:"/cn/blog/tags/migration"},{label:"bootstrap",permalink:"/cn/blog/tags/bootstrap"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:4.755,truncated:!0,authors:[{name:"vbalaji"}],prevItem:{title:"Async Compaction Deployment Models",permalink:"/cn/blog/2020/08/21/async-compaction-deployment-model"},nextItem:{title:"Incremental Processing on the Data Lake",permalink:"/cn/blog/2020/08/18/hudi-incremental-processing-on-data-lakes"}},l={authorsImageUrls:[void 0]},d=[{value:"Motivation:",id:"motivation",children:[],level:2},{value:"High Level Idea:",id:"high-level-idea",children:[{value:"Per Record Metadata:",id:"per-record-metadata",children:[],level:3}],level:2},{value:"Design Deep Dive:",id:"design-deep-dive",children:[],level:2},{value:"Migration:",id:"migration",children:[{value:"Query Engine Support:",id:"query-engine-support",children:[],level:3},{value:"Ways To Migrate :",id:"ways-to-migrate-",children:[],level:3},{value:"Configurations:",id:"configurations",children:[],level:3},{value:"Spark Data Source:",id:"spark-data-source",children:[],level:3},{value:"Hoodie DeltaStreamer:",id:"hoodie-deltastreamer",children:[],level:3},{value:"Known Caveats",id:"known-caveats",children:[],level:3}],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...o}=e;return(0,n.yg)(p,(0,i.A)({},c,o,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"We will look at how to migrate a large parquet table to Hudi without having to rewrite the entire dataset. "),(0,n.yg)("h2",{id:"motivation"},"Motivation:"),(0,n.yg)("p",null,"Apache Hudi maintains per record metadata to perform core operations such as upserts and incremental pull. To take advantage of Hudi\u2019s upsert and incremental processing support, users would need to rewrite their whole dataset to make it an Apache Hudi table.  Hudi 0.6.0 comes with an ",(0,n.yg)("strong",{parentName:"p"},(0,n.yg)("em",{parentName:"strong"},"experimental feature"))," to support efficient migration of large Parquet tables to Hudi without the need to rewrite the entire dataset."),(0,n.yg)("h2",{id:"high-level-idea"},"High Level Idea:"),(0,n.yg)("h3",{id:"per-record-metadata"},"Per Record Metadata:"),(0,n.yg)("p",null,"Apache Hudi maintains record level metadata for perform efficient upserts and incremental pull."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Per Record Metadata",src:t(49720).A})),(0,n.yg)("p",null,"Apache HUDI physical file contains 3 parts"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"For each record, 5 HUDI metadata fields with column indices 0 to 4"),(0,n.yg)("li",{parentName:"ol"},"For each record, the original data columns that comprises the record (Original Data)"),(0,n.yg)("li",{parentName:"ol"},"Additional Hudi Metadata at file footer for index lookup")),(0,n.yg)("p",null,"The parts (1) and (3) constitute what we term as  \u201cHudi skeleton\u201d. Hudi skeleton contains additional metadata that it maintains in each physical parquet file for supporting Hudi primitives. The conceptual idea is to decouple Hudi skeleton data from original data (2). Hudi skeleton can be stored in a Hudi file while the original data is stored in an external non-Hudi file. A migration of large parquet would result in creating only Hudi skeleton files without having to rewrite original data."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"skeleton",src:t(20976).A})),(0,n.yg)("h2",{id:"design-deep-dive"},"Design Deep Dive:"),(0,n.yg)("p",null," For a deep dive on the internals, please take a look at the ",(0,n.yg)("a",{parentName:"p",href:"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+12+%3A+Efficient+Migration+of+Large+Parquet+Tables+to+Apache+Hudi"},"RFC document")," "),(0,n.yg)("h2",{id:"migration"},"Migration:"),(0,n.yg)("p",null,"Hudi supports 2 modes when migrating parquet tables.  We will use the term bootstrap and migration interchangeably in this document.  "),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"METADATA_ONLY : In this mode, record level metadata alone is generated for each source record and stored in new bootstrap location."),(0,n.yg)("li",{parentName:"ul"},"FULL_RECORD : In this mode, record level metadata is generated for each source record and both original record and metadata for each record copied")),(0,n.yg)("p",null,'You can pick and choose these modes at partition level. One of the common strategy would be to use FULL_RECORD mode for a small set of "hot" partitions which are accessed more frequently and METADATA_ONLY for a larger set of "warm" partitions. '),(0,n.yg)("h3",{id:"query-engine-support"},"Query Engine Support:"),(0,n.yg)("p",null,"For a METADATA_ONLY bootstrapped table, Spark - data source, Spark-Hive and native Hive query engines are supported. Presto support is in the works."),(0,n.yg)("h3",{id:"ways-to-migrate-"},"Ways To Migrate :"),(0,n.yg)("p",null,"There are 2 ways to migrate a large parquet table to Hudi. "),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Spark Datasource Write"),(0,n.yg)("li",{parentName:"ul"},"Hudi DeltaStreamer")),(0,n.yg)("p",null,"We will look at how to migrate using both these approaches."),(0,n.yg)("h3",{id:"configurations"},"Configurations:"),(0,n.yg)("p",null,"These are bootstrap specific configurations that needs to be set in addition to regular hudi write configurations."),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Configuration Name"),(0,n.yg)("th",{parentName:"tr",align:null},"Default"),(0,n.yg)("th",{parentName:"tr",align:null},"Mandatory ?"),(0,n.yg)("th",{parentName:"tr",align:null},"Description"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"hoodie.bootstrap.base.path"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"Yes"),(0,n.yg)("td",{parentName:"tr",align:null},"Base Path of  source parquet table.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"hoodie.bootstrap.parallelism"),(0,n.yg)("td",{parentName:"tr",align:null},"1500"),(0,n.yg)("td",{parentName:"tr",align:null},"Yes"),(0,n.yg)("td",{parentName:"tr",align:null},"Spark Parallelism used when running bootstrap")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"hoodie.bootstrap.keygen.class"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"Yes"),(0,n.yg)("td",{parentName:"tr",align:null},"Bootstrap Index internally used by Hudi to map Hudi skeleton and source parquet files.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"hoodie.bootstrap.mode.selector"),(0,n.yg)("td",{parentName:"tr",align:null},"org.apache.hudi.client.bootstrap.selector.MetadataOnlyBootstrapModeSelector"),(0,n.yg)("td",{parentName:"tr",align:null},"Yes"),(0,n.yg)("td",{parentName:"tr",align:null},"Bootstap Mode Selector class. By default, Hudi employs METADATA_ONLY boostrap for all partitions.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"hoodie.bootstrap.partitionpath.translator.class"),(0,n.yg)("td",{parentName:"tr",align:null},"org.apache.hudi.client.bootstrap.translator. IdentityBootstrapPartitionPathTranslator"),(0,n.yg)("td",{parentName:"tr",align:null},"No"),(0,n.yg)("td",{parentName:"tr",align:null},"For METADATA_ONLY bootstrap, this class allows customization of partition paths used in Hudi target dataset. By default, no customization is done and the partition paths reflects what is available in source parquet table.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"hoodie.bootstrap.full.input.provider"),(0,n.yg)("td",{parentName:"tr",align:null},"org.apache.hudi.bootstrap.SparkParquetBootstrapDataProvider"),(0,n.yg)("td",{parentName:"tr",align:null},"No"),(0,n.yg)("td",{parentName:"tr",align:null},"For FULL_RECORD bootstrap, this class provides the input RDD of Hudi records to write.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"hoodie.bootstrap.mode.selector.regex.mode"),(0,n.yg)("td",{parentName:"tr",align:null},"METADATA_ONLY"),(0,n.yg)("td",{parentName:"tr",align:null},"No"),(0,n.yg)("td",{parentName:"tr",align:null},"Bootstrap Mode used when the partition matches the regex pattern in hoodie.bootstrap.mode.selector.regex . Used only when hoodie.bootstrap.mode.selector set to BootstrapRegexModeSelector.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"hoodie.bootstrap.mode.selector.regex"),(0,n.yg)("td",{parentName:"tr",align:null},".","*"),(0,n.yg)("td",{parentName:"tr",align:null},"No"),(0,n.yg)("td",{parentName:"tr",align:null},"Partition Regex used when  hoodie.bootstrap.mode.selector set to BootstrapRegexModeSelector.")))),(0,n.yg)("h3",{id:"spark-data-source"},"Spark Data Source:"),(0,n.yg)("p",null,"Here, we use a Spark Datasource Write to perform bootstrap.\nHere is an example code snippet to perform METADATA_ONLY bootstrap."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-properties"},'import org.apache.hudi.{DataSourceWriteOptions, HoodieDataSourceHelpers}\nimport org.apache.hudi.config.{HoodieBootstrapConfig, HoodieWriteConfig}\nimport org.apache.hudi.keygen.SimpleKeyGenerator\nimport org.apache.spark.sql.SaveMode\n \nval bootstrapDF = spark.emptyDataFrame\nbootstrapDF.write\n      .format("hudi")\n      .option(HoodieWriteConfig.TABLE_NAME, "hoodie_test")\n      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.BOOTSTRAP_OPERATION_OPT_VAL)\n      .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY, "_row_key")\n      .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY, "datestr")\n      .option(HoodieBootstrapConfig.BOOTSTRAP_BASE_PATH_PROP, srcPath)\n      .option(HoodieBootstrapConfig.BOOTSTRAP_KEYGEN_CLASS, classOf[SimpleKeyGenerator].getName)\n      .mode(SaveMode.Overwrite)\n      .save(basePath)\n')),(0,n.yg)("p",null,"Here is an example code snippet to perform METADATA_ONLY bootstrap for August 20 2020 - August 29 2020 partitions and FULL_RECORD bootstrap for other partitions."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-properties"},'import org.apache.hudi.bootstrap.SparkParquetBootstrapDataProvider\nimport org.apache.hudi.client.bootstrap.selector.BootstrapRegexModeSelector\nimport org.apache.hudi.{DataSourceWriteOptions, HoodieDataSourceHelpers}\nimport org.apache.hudi.config.{HoodieBootstrapConfig, HoodieWriteConfig}\nimport org.apache.hudi.keygen.SimpleKeyGenerator\nimport org.apache.spark.sql.SaveMode\n \nval bootstrapDF = spark.emptyDataFrame\nbootstrapDF.write\n      .format("hudi")\n      .option(HoodieWriteConfig.TABLE_NAME, "hoodie_test")\n      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.BOOTSTRAP_OPERATION_OPT_VAL)\n      .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY, "_row_key")\n      .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY, "datestr")\n      .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY, "timestamp")\n      .option(HoodieBootstrapConfig.BOOTSTRAP_BASE_PATH_PROP, srcPath)\n      .option(HoodieBootstrapConfig.BOOTSTRAP_KEYGEN_CLASS, classOf[SimpleKeyGenerator].getName)\n      .option(HoodieBootstrapConfig.BOOTSTRAP_MODE_SELECTOR, classOf[BootstrapRegexModeSelector].getName)\n      .option(HoodieBootstrapConfig.BOOTSTRAP_MODE_SELECTOR_REGEX, "2020/08/2[0-9]")\n      .option(HoodieBootstrapConfig.BOOTSTRAP_MODE_SELECTOR_REGEX_MODE, "METADATA_ONLY")\n      .option(HoodieBootstrapConfig.FULL_BOOTSTRAP_INPUT_PROVIDER, classOf[SparkParquetBootstrapDataProvider].getName)\n      .mode(SaveMode.Overwrite)\n      .save(basePath)\n')),(0,n.yg)("h3",{id:"hoodie-deltastreamer"},"Hoodie DeltaStreamer:"),(0,n.yg)("p",null,"Hoodie Deltastreamer allows bootstrap to be performed using --run-bootstrap command line option."),(0,n.yg)("p",null,"If you are planning to use delta-streamer after the initial boostrap to incrementally ingest data to the new hudi dataset, you need to pass either --checkpoint or --initial-checkpoint-provider to set the initial checkpoint for the deltastreamer."),(0,n.yg)("p",null,"Here is an example for running METADATA_ONLY bootstrap using Delta Streamer."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-properties"},"spark-submit --package org.apache.hudi:hudi-spark-bundle_2.11:0.6.0\n--conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \\\n--class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer  \\\n--run-bootstrap \\\n--target-base-path <Hudi_Base_Path> \\\n--target-table <Hudi_Table_Name> \\\n--props <props_file> \\\n--checkpoint <initial_checkpoint_if_you_are_going_to_use_deltastreamer_to_incrementally_ingest> \\\n--hoodie-conf hoodie.bootstrap.base.path=<Parquet_Source_base_Path> \\\n--hoodie-conf hoodie.datasource.write.recordkey.field=_row_key \\\n--hoodie-conf hoodie.datasource.write.partitionpath.field=datestr \\\n--hoodie-conf hoodie.bootstrap.keygen.class=org.apache.hudi.keygen.SimpleKeyGenerator\n")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-properties"},"spark-submit --package org.apache.hudi:hudi-spark-bundle_2.11:0.6.0\n--conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \\\n--class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer  \\\n--run-bootstrap \\\n--target-base-path <Hudi_Base_Path> \\\n--target-table <Hudi_Table_Name> \\\n--props <props_file> \\\n--checkpoint <initial_checkpoint_if_you_are_going_to_use_deltastreamer_to_incrementally_ingest> \\\n--hoodie-conf hoodie.bootstrap.base.path=<Parquet_Source_base_Path> \\\n--hoodie-conf hoodie.datasource.write.recordkey.field=_row_key \\\n--hoodie-conf hoodie.datasource.write.partitionpath.field=datestr \\\n--hoodie-conf hoodie.bootstrap.keygen.class=org.apache.hudi.keygen.SimpleKeyGenerator \\\n--hoodie-conf hoodie.bootstrap.full.input.provider=org.apache.hudi.bootstrap.SparkParquetBootstrapDataProvider \\\n--hoodie-conf hoodie.bootstrap.mode.selector=org.apache.hudi.client.bootstrap.selector.BootstrapRegexModeSelector \\\n--hoodie-conf hoodie.bootstrap.mode.selector.regex=\"2020/08/2[0-9]\" \\\n--hoodie-conf hoodie.bootstrap.mode.selector.regex.mode=METADATA_ONLY\n")),(0,n.yg)("h3",{id:"known-caveats"},"Known Caveats"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Need proper defaults for the bootstrap config : hoodie.bootstrap.full.input.provider. Here is the ",(0,n.yg)("a",{parentName:"li",href:"https://issues.apache.org/jira/browse/HUDI-1213"},"ticket")),(0,n.yg)("li",{parentName:"ol"},"DeltaStreamer manages checkpoints inside hoodie commit files and expects checkpoints in previously committed metadata. Users are expected to pass checkpoint or initial checkpoint provider when performing bootstrap through deltastreamer. Such support is not present when doing bootstrap using Spark Datasource. Here is the ",(0,n.yg)("a",{parentName:"li",href:"https://issues.apache.org/jira/browse/HUDI-1214"},"ticket"),".")))}g.isMDXComponent=!0},85075:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Async Compaction Deployment Models",excerpt:"Mechanisms for executing compaction jobs in Hudi asynchronously",author:"vbalaji",category:"blog",tags:["how-to","compaction","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2020/08/21/async-compaction-deployment-model",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-08-21-async-compaction-deployment-model.md",source:"@site/blog/2020-08-21-async-compaction-deployment-model.md",title:"Async Compaction Deployment Models",description:"We will look at different deployment models for executing compactions asynchronously.",date:"2020-08-21T00:00:00.000Z",formattedDate:"August 21, 2020",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"compaction",permalink:"/cn/blog/tags/compaction"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:1.925,truncated:!0,authors:[{name:"vbalaji"}],prevItem:{title:"Ingest multiple tables using Hudi",permalink:"/cn/blog/2020/08/22/ingest-multiple-tables-using-hudi"},nextItem:{title:"Efficient Migration of Large Parquet Tables to Apache Hudi",permalink:"/cn/blog/2020/08/20/efficient-migration-of-large-parquet-tables"}},l={authorsImageUrls:[void 0]},d=[{value:"Compaction",id:"compaction",children:[],level:2},{value:"Async Compaction",id:"async-compaction",children:[],level:2},{value:"Deployment Models",id:"deployment-models",children:[{value:"Spark Structured Streaming",id:"spark-structured-streaming",children:[],level:3},{value:"DeltaStreamer Continuous Mode",id:"deltastreamer-continuous-mode",children:[],level:3},{value:"Hudi CLI",id:"hudi-cli",children:[],level:3},{value:"Hudi Compactor Script",id:"hudi-compactor-script",children:[],level:3}],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...t}=e;return(0,n.yg)(p,(0,i.A)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"We will look at different deployment models for executing compactions asynchronously."),(0,n.yg)("h2",{id:"compaction"},"Compaction"),(0,n.yg)("p",null,"For Merge-On-Read table, data is stored using a combination of columnar (e.g parquet) + row based (e.g avro) file formats.\nUpdates are logged to delta files & later compacted to produce new versions of columnar files synchronously or\nasynchronously. One of th main motivations behind Merge-On-Read is to reduce data latency when ingesting records.\nHence, it makes sense to run compaction asynchronously without blocking ingestion."),(0,n.yg)("h2",{id:"async-compaction"},"Async Compaction"),(0,n.yg)("p",null,"Async Compaction is performed in 2 steps:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},(0,n.yg)("em",{parentName:"strong"},"Compaction Scheduling")),": This is done by the ingestion job. In this step, Hudi scans the partitions and selects ",(0,n.yg)("strong",{parentName:"li"},"file\nslices")," to be compacted. A compaction plan is finally written to Hudi timeline."),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},(0,n.yg)("em",{parentName:"strong"},"Compaction Execution")),": A separate process reads the compaction plan and performs compaction of file slices.")),(0,n.yg)("h2",{id:"deployment-models"},"Deployment Models"),(0,n.yg)("p",null,"There are few ways by which we can execute compactions asynchronously. "),(0,n.yg)("h3",{id:"spark-structured-streaming"},"Spark Structured Streaming"),(0,n.yg)("p",null,"With 0.6.0, we now have support for running async compactions in Spark\nStructured Streaming jobs. Compactions are scheduled and executed asynchronously inside the\nstreaming job.  Async Compactions are enabled by default for structured streaming jobs\non Merge-On-Read table."),(0,n.yg)("p",null,"Here is an example snippet in java"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-properties"},'import org.apache.hudi.DataSourceWriteOptions;\nimport org.apache.hudi.HoodieDataSourceHelpers;\nimport org.apache.hudi.config.HoodieCompactionConfig;\nimport org.apache.hudi.config.HoodieWriteConfig;\n\nimport org.apache.spark.sql.streaming.OutputMode;\nimport org.apache.spark.sql.streaming.ProcessingTime;\n\n\n DataStreamWriter<Row> writer = streamingInput.writeStream().format("org.apache.hudi")\n        .option(DataSourceWriteOptions.OPERATION_OPT_KEY(), operationType)\n        .option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY(), tableType)\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), "_row_key")\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), "partition")\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), "timestamp")\n        .option(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP, "10")\n        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE_OPT_KEY(), "true")\n        .option(HoodieWriteConfig.TABLE_NAME, tableName).option("checkpointLocation", checkpointLocation)\n        .outputMode(OutputMode.Append());\n writer.trigger(new ProcessingTime(30000)).start(tablePath);\n')),(0,n.yg)("h3",{id:"deltastreamer-continuous-mode"},"DeltaStreamer Continuous Mode"),(0,n.yg)("p",null,"Hudi DeltaStreamer provides continuous ingestion mode where a single long running spark application",(0,n.yg)("br",{parentName:"p"}),"\n","ingests data to Hudi table continuously from upstream sources. In this mode, Hudi supports managing asynchronous\ncompactions. Here is an example snippet for running in continuous mode with async compactions"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-properties"},"spark-submit --packages org.apache.hudi:hudi-utilities-bundle_2.11:0.6.0 \\\n--class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer \\\n--table-type MERGE_ON_READ \\\n--target-base-path <hudi_base_path> \\\n--target-table <hudi_table> \\\n--source-class org.apache.hudi.utilities.sources.JsonDFSSource \\\n--source-ordering-field ts \\\n--schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider \\\n--props /path/to/source.properties \\\n--continous\n")),(0,n.yg)("h3",{id:"hudi-cli"},"Hudi CLI"),(0,n.yg)("p",null,"Hudi CLI is yet another way to execute specific compactions asynchronously. Here is an example "),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-properties"},"hudi:trips->compaction run --tableName <table_name> --parallelism <parallelism> --compactionInstant <InstantTime>\n...\n")),(0,n.yg)("h3",{id:"hudi-compactor-script"},"Hudi Compactor Script"),(0,n.yg)("p",null,"Hudi provides a standalone tool to also execute specific compactions asynchronously. Here is an example"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-properties"},"spark-submit --packages org.apache.hudi:hudi-utilities-bundle_2.11:0.6.0 \\\n--class org.apache.hudi.utilities.HoodieCompactor \\\n--base-path <base_path> \\\n--table-name <table_name> \\\n--instant-time <compaction_instant> \\\n--schema-file <schema_file>\n")))}g.isMDXComponent=!0},17857:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Ingest multiple tables using Hudi",excerpt:"Ingesting multiple tables using Hudi at a single go is now possible. This blog gives a detailed explanation of how to achieve the same using `HoodieMultiTableDeltaStreamer.java`",author:"pratyakshsharma",category:"blog",tags:["how-to","multi deltastreamer","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2020/08/22/ingest-multiple-tables-using-hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-08-22-ingest-multiple-tables-using-hudi.md",source:"@site/blog/2020-08-22-ingest-multiple-tables-using-hudi.md",title:"Ingest multiple tables using Hudi",description:"When building a change data capture pipeline for already existing or newly created relational databases, one of the most common problems that one faces is simplifying the onboarding process for multiple tables. Ingesting multiple tables to Hudi dataset at a single go is now possible using HoodieMultiTableDeltaStreamer class which is a wrapper on top of the more popular HoodieDeltaStreamer class. Currently HoodieMultiTableDeltaStreamer supports COPY_ON_WRITE storage type only and the ingestion is done in a sequential way.",date:"2020-08-22T00:00:00.000Z",formattedDate:"August 22, 2020",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"multi deltastreamer",permalink:"/cn/blog/tags/multi-deltastreamer"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:3.55,truncated:!0,authors:[{name:"pratyakshsharma"}],prevItem:{title:"How nClouds Helps Accelerate Data Delivery with Apache Hudi on Amazon EMR",permalink:"/cn/blog/2020/10/06/cdc-solution-using-hudi-by-nclouds"},nextItem:{title:"Async Compaction Deployment Models",permalink:"/cn/blog/2020/08/21/async-compaction-deployment-model"}},l={authorsImageUrls:[void 0]},d=[{value:"Configuration",id:"configuration",children:[],level:3},{value:"Configuring schema providers",id:"configuring-schema-providers",children:[],level:3},{value:"Run Command",id:"run-command",children:[],level:3},{value:"Example",id:"example",children:[],level:3}],c={toc:d},p="wrapper";function g(e){let{components:a,...t}=e;return(0,n.yg)(p,(0,i.A)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"When building a change data capture pipeline for already existing or newly created relational databases, one of the most common problems that one faces is simplifying the onboarding process for multiple tables. Ingesting multiple tables to Hudi dataset at a single go is now possible using ",(0,n.yg)("inlineCode",{parentName:"p"},"HoodieMultiTableDeltaStreamer")," class which is a wrapper on top of the more popular ",(0,n.yg)("inlineCode",{parentName:"p"},"HoodieDeltaStreamer")," class. Currently ",(0,n.yg)("inlineCode",{parentName:"p"},"HoodieMultiTableDeltaStreamer")," supports ",(0,n.yg)("strong",{parentName:"p"},"COPY_ON_WRITE")," storage type only and the ingestion is done in a ",(0,n.yg)("strong",{parentName:"p"},"sequential")," way."),(0,n.yg)("p",null,"This blog will guide you through configuring and running ",(0,n.yg)("inlineCode",{parentName:"p"},"HoodieMultiTableDeltaStreamer"),"."),(0,n.yg)("h3",{id:"configuration"},"Configuration"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"HoodieMultiTableDeltaStreamer")," expects users to maintain table wise overridden properties in separate files in a dedicated config folder. Common properties can be configured via common properties file also."),(0,n.yg)("li",{parentName:"ul"},"By default, hudi datasets are created under the path ",(0,n.yg)("inlineCode",{parentName:"li"},"<base-path-prefix>/<database_name>/<name_of_table_to_be_ingested>"),". You need to provide the names of tables to be ingested via the property ",(0,n.yg)("inlineCode",{parentName:"li"},"hoodie.deltastreamer.ingestion.tablesToBeIngested")," in the format ",(0,n.yg)("inlineCode",{parentName:"li"},"<database>.<table>"),", for example ")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"hoodie.deltastreamer.ingestion.tablesToBeIngested=db1.table1,db2.table2\n")),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"If you do not provide database name, then it is assumed the table belongs to default database and the hudi dataset for the concerned table is created under the path ",(0,n.yg)("inlineCode",{parentName:"li"},"<base-path-prefix>/default/<name_of_table_to_be_ingested>"),". Also there is a provision to override the default path for hudi datasets. You can create hudi dataset for a particular table by setting the property ",(0,n.yg)("inlineCode",{parentName:"li"},"hoodie.deltastreamer.ingestion.targetBasePath")," in table level config file"),(0,n.yg)("li",{parentName:"ul"},"There are a lot of properties that one might like to override per table, for example")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"hoodie.datasource.write.recordkey.field=_row_key\nhoodie.datasource.write.partitionpath.field=created_at\nhoodie.deltastreamer.source.kafka.topic=topic2\nhoodie.deltastreamer.keygen.timebased.timestamp.type=UNIX_TIMESTAMP\nhoodie.deltastreamer.keygen.timebased.input.dateformat=yyyy-MM-dd HH:mm:ss.S\nhoodie.datasource.hive_sync.table=short_trip_uber_hive_dummy_table\nhoodie.deltastreamer.ingestion.targetBasePath=s3:///temp/hudi/table1\n")),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Properties like above need to be set for every table to be ingested. As already suggested at the beginning, users are expected to maintain separate config files for every table by setting the below property")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"hoodie.deltastreamer.ingestion.<db>.<table>.configFile=s3:///tmp/config/config1.properties\n")),(0,n.yg)("p",null,"If you do not want to set the above property for every table, you can simply create config files for every table to be ingested under the config folder with the name - ",(0,n.yg)("inlineCode",{parentName:"p"},"<database>_<table>_config.properties"),". For example if you want to ingest table1 and table2 from dummy database, where config folder is set to ",(0,n.yg)("inlineCode",{parentName:"p"},"s3:///tmp/config"),", then you need to create 2 config files on the given paths - ",(0,n.yg)("inlineCode",{parentName:"p"},"s3:///tmp/config/dummy_table1_config.properties")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"s3:///tmp/config/dummy_table2_config.properties"),"."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Finally you can specify all the common properties in a common properties file. Common properties file does not necessarily have to lie under config folder but it is advised to keep it along with other config files. This file will contain the below properties")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"hoodie.deltastreamer.ingestion.tablesToBeIngested=db1.table1,db2.table2\nhoodie.deltastreamer.ingestion.db1.table1.configFile=s3:///tmp/config_table1.properties\nhoodie.deltastreamer.ingestion.db2.table2.configFile=s3:///tmp/config_table2.properties\n")),(0,n.yg)("h3",{id:"configuring-schema-providers"},"Configuring schema providers"),(0,n.yg)("p",null,"It is possible to configure different schema providers for different tables or same schema provider class for all tables. All you need to do is configure the property ",(0,n.yg)("inlineCode",{parentName:"p"},"hoodie.deltastreamer.schemaprovider.class")," accordingly as per your use case as below - "),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"hoodie.deltastreamer.schemaprovider.class=org.apache.hudi.utilities.schema.FilebasedSchemaProvider\n")),(0,n.yg)("p",null,"Further it is also possible to configure different source and target schema registry urls with ",(0,n.yg)("inlineCode",{parentName:"p"},"SchemaRegistryProvider")," as the schemaprovider class. Originally HoodieMultiTableDeltaStreamer was designed to cater to use cases where subject naming strategy is set to ",(0,n.yg)("a",{parentName:"p",href:"https://docs.confluent.io/platform/current/schema-registry/serdes-develop/index.html#subject-name-strategy"},"TopicNameStrategy")," which is the default provided by Confluent.\nWith this default strategy in place, the subject name is same as the topic name being used in kafka. Source and target schema registry urls can be configured as below with TopicNameStrategy - "),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"hoodie.deltastreamer.schemaprovider.registry.baseUrl=http://localhost:8081/subjects/\nhoodie.deltastreamer.schemaprovider.registry.urlSuffix=-value/versions/latest\n")),(0,n.yg)("p",null,"If you want to consume different versions of your source and target subjects, you can configure as below - "),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"hoodie.deltastreamer.schemaprovider.registry.baseUrl=http://localhost:8081/subjects/\nhoodie.deltastreamer.schemaprovider.registry.sourceUrlSuffix=-value/versions/latest\nhoodie.deltastreamer.schemaprovider.registry.targetUrlSuffix=-value/versions/1\n")),(0,n.yg)("p",null,"If you are looking to configure the schema registry urls in the most straight forward way, you can do that as below"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"hoodie.deltastreamer.schemaprovider.registry.url=http://localhost:8081/subjects/random-value/versions/latest\nhoodie.deltastreamer.schemaprovider.registry.targetUrl=http://localhost:8081/subjects/random-value/versions/latest\n")),(0,n.yg)("h3",{id:"run-command"},"Run Command"),(0,n.yg)("p",null,(0,n.yg)("inlineCode",{parentName:"p"},"HoodieMultiTableDeltaStreamer")," can be run similar to how one runs ",(0,n.yg)("inlineCode",{parentName:"p"},"HoodieDeltaStreamer"),". Please refer to the example given below for the command. "),(0,n.yg)("h3",{id:"example"},"Example"),(0,n.yg)("p",null,"Suppose you want to ingest table1 and table2 from db1 and want to ingest the 2 tables under the path ",(0,n.yg)("inlineCode",{parentName:"p"},"s3:///temp/hudi"),". You can ingest them using the below command"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"[hoodie]$ spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieMultiTableDeltaStreamer `ls packaging/hudi-utilities-bundle/target/hudi-utilities-bundle-*.jar` \\\n  --props s3:///temp/hudi-ingestion-config/kafka-source.properties \\\n  --config-folder s3:///temp/hudi-ingestion-config \\\n  --schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \\\n  --source-class org.apache.hudi.utilities.sources.AvroKafkaSource \\\n  --source-ordering-field impresssiontime \\\n  --base-path-prefix s3:///temp/hudi \\ \n  --target-table dummy_table \\\n  --op UPSERT\n")),(0,n.yg)("p",null,"s3:///temp/config/kafka-source.properties"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"hoodie.deltastreamer.ingestion.tablesToBeIngested=db1.table1,db1.table2\nhoodie.deltastreamer.ingestion.db1.table1.configFile=s3:///temp/hudi-ingestion-config/config_table1.properties\nhoodie.deltastreamer.ingestion.db21.table2.configFile=s3:///temp/hudi-ingestion-config/config_table2.properties\n\n#Kafka props\nbootstrap.servers=localhost:9092\nauto.offset.reset=earliest\nschema.registry.url=http://localhost:8081\n\nhoodie.datasource.write.keygenerator.class=org.apache.hudi.keygen.CustomKeyGenerator\n")),(0,n.yg)("p",null,"s3:///temp/hudi-ingestion-config/config_table1.properties"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"hoodie.datasource.write.recordkey.field=_row_key1\nhoodie.datasource.write.partitionpath.field=created_at\nhoodie.deltastreamer.source.kafka.topic=topic1\n")),(0,n.yg)("p",null,"s3:///temp/hudi-ingestion-config/config_table2.properties"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"hoodie.datasource.write.recordkey.field=_row_key2\nhoodie.datasource.write.partitionpath.field=created_at\nhoodie.deltastreamer.source.kafka.topic=topic2\n")),(0,n.yg)("p",null,"Contributions are welcome for extending multiple tables ingestion support to ",(0,n.yg)("strong",{parentName:"p"},"MERGE_ON_READ")," storage type and enabling ",(0,n.yg)("inlineCode",{parentName:"p"},"HoodieMultiTableDeltaStreamer")," ingest multiple tables parallely. "),(0,n.yg)("p",null,"Happy ingesting!"))}g.isMDXComponent=!0},62898:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"How nClouds Helps Accelerate Data Delivery with Apache Hudi on Amazon EMR",excerpt:"Solution to set up a new data and analytics platform using Apache Hudi on Amazon EMR and other managed services, including Amazon QuickSight for data visualization.",author:"nclouds",category:"blog",image:"/assets/images/blog/2020-10-06-cdc-solution-using-hudi-by-nclouds.jpg",tags:["blog","apache flink","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2020/10/06/cdc-solution-using-hudi-by-nclouds",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-10-06-cdc-solution-using-hudi-by-nclouds.md",source:"@site/blog/2020-10-06-cdc-solution-using-hudi-by-nclouds.md",title:"How nClouds Helps Accelerate Data Delivery with Apache Hudi on Amazon EMR",description:"This blog published by nClouds in partnership with AWS shows how to build a CDC pipeline using Apache Hudi on Amazon EMR and other managed services like Amazon RDS and AWS DMS, including Amazon QuickSight for data visualization.",date:"2020-10-06T00:00:00.000Z",formattedDate:"October 6, 2020",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache flink",permalink:"/cn/blog/tags/apache-flink"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:.19,truncated:!1,authors:[{name:"nclouds"}],prevItem:{title:"Apache Hudi meets Apache Flink",permalink:"/cn/blog/2020/10/15/apache-hudi-meets-apache-flink"},nextItem:{title:"Ingest multiple tables using Hudi",permalink:"/cn/blog/2020/08/22/ingest-multiple-tables-using-hudi"}},l={authorsImageUrls:[void 0]},d=[],c={toc:d},p="wrapper";function g(e){let{components:a,...t}=e;return(0,n.yg)(p,(0,i.A)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"This ",(0,n.yg)("a",{parentName:"p",href:"https://aws.amazon.com/blogs/apn/how-nclouds-helps-accelerate-data-delivery-with-apache-hudi-on-amazon-emr/"},"blog")," published by nClouds in partnership with AWS shows how to build a CDC pipeline using Apache Hudi on Amazon EMR and other managed services like Amazon RDS and AWS DMS, including Amazon QuickSight for data visualization."))}g.isMDXComponent=!0},67035:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Apache Hudi meets Apache Flink",excerpt:"The design and latest progress of the integration of Apache Hudi and Apache Flink.",author:"wangxianghu",category:"blog",image:"/assets/images/blog/2020-10-15-apache-hudi-meets-apache-flink.png",tags:["blog","apache flink","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2020/10/15/apache-hudi-meets-apache-flink",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-10-15-apache-hudi-meets-apache-flink.md",source:"@site/blog/2020-10-15-apache-hudi-meets-apache-flink.md",title:"Apache Hudi meets Apache Flink",description:"Apache Hudi (Hudi for short) is a data lake framework created at Uber. Hudi joined the Apache incubator for incubation in January 2019, and was promoted to the top Apache project in May 2020. It is one of the most popular data lake frameworks.",date:"2020-10-15T00:00:00.000Z",formattedDate:"October 15, 2020",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache flink",permalink:"/cn/blog/tags/apache-flink"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:9.705,truncated:!0,authors:[{name:"wangxianghu"}],prevItem:{title:"Origins of Data Lake at Grofers",permalink:"/cn/blog/2020/10/19/Origins-of-Data-Lake-at-Grofers"},nextItem:{title:"How nClouds Helps Accelerate Data Delivery with Apache Hudi on Amazon EMR",permalink:"/cn/blog/2020/10/06/cdc-solution-using-hudi-by-nclouds"}},l={authorsImageUrls:[void 0]},d=[{value:"1. Why decouple",id:"1-why-decouple",children:[],level:2},{value:"2. Challenges",id:"2-challenges",children:[],level:2},{value:"3. Decoupling Spark",id:"3-decoupling-spark",children:[{value:"Decoupling principle",id:"decoupling-principle",children:[],level:3}],level:2},{value:"4. Flink integration design",id:"4-flink-integration-design",children:[{value:"4.1 Index design based on Flink State",id:"41-index-design-based-on-flink-state",children:[],level:3}],level:2},{value:"5. Implementation examples",id:"5-implementation-examples",children:[{value:"1) HoodieTable",id:"1-hoodietable",children:[],level:3},{value:"2) HoodieEngineContext",id:"2-hoodieenginecontext",children:[],level:3}],level:2},{value:"6. Current progress and follow-up plan",id:"6-current-progress-and-follow-up-plan",children:[{value:"6.1 Working time axis",id:"61-working-time-axis",children:[],level:3},{value:"6.2 Follow-up plan",id:"62-follow-up-plan",children:[{value:"1) Promote the integration of Hudi and Flink",id:"1-promote-the-integration-of-hudi-and-flink",children:[],level:4},{value:"2) Performance optimization",id:"2-performance-optimization",children:[],level:4},{value:"3) flink-connector-hudi like third-party package development",id:"3-flink-connector-hudi-like-third-party-package-development",children:[],level:4}],level:3}],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...o}=e;return(0,n.yg)(p,(0,i.A)({},c,o,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"Apache Hudi (Hudi for short) is a data lake framework created at Uber. Hudi joined the Apache incubator for incubation in January 2019, and was promoted to the top Apache project in May 2020. It is one of the most popular data lake frameworks."),(0,n.yg)("h2",{id:"1-why-decouple"},"1. Why decouple"),(0,n.yg)("p",null,"Hudi has been using Spark as its data processing engine since its birth. If users want to use Hudi as their data lake framework, they must introduce Spark into their platform technology stack.\nA few years ago, using Spark as a big data processing engine can be said to be very common or even natural. Since Spark can either perform batch processing or use micro-batch to simulate streaming, one engine solves both streaming and batch problems.\nHowever, in recent years, with the development of big data technology, Flink, which is also a big data processing engine, has gradually entered people's vision and has occupied a certain market in the field of computing engines.\nIn the big data technology community, forums and other territories, the voice of whether Hudi supports Flink has gradually appeared and has become more frequent. Therefore, it is a valuable thing to make Hudi support the Flink engine, and the first step of integrating the Flink engine is that Hudi and Spark are decoupled."),(0,n.yg)("p",null,"In addition, looking at the mature, active, and viable frameworks in the big data, all frameworks are elegant in design and can be integrated with other frameworks and leverage each other's expertise.\nTherefore, decoupling Hudi from Spark and turning it into an engine-independent data lake framework will undoubtedly create more possibilities for the integration of Hudi and other components, allowing Hudi to better integrate into the big data ecosystem."),(0,n.yg)("h2",{id:"2-challenges"},"2. Challenges"),(0,n.yg)("p",null,"Hudi's internal use of Spark API is as common as our usual development and use of List. Since the data source reads the data, and finally writes the data to the table, Spark RDD is used as the main data structure everywhere, and even ordinary tools are implemented using the Spark API.\nIt can be said that Hudi is a universal data lake framework implemented by Spark. Hudi also leverages deep Spark functionality like custom partitioning, in-memory caching to implement indexing and file sizing using workload heuristics.\nFor some of these, Flink offers better out-of-box support (e.g using Flink\u2019s state store for indexing) and can in fact, make Hudi approach real-time latencies more and more. "),(0,n.yg)("p",null,"In addition, the primary engine integrated after this decoupling is Flink. Flink and Spark differ greatly in core abstraction. Spark believes that data is bounded, and its core abstraction is a limited set of data.\nFlink believes that the essence of data is a stream, and its core abstract DataStream contains various operations on data. Hudi has a streaming first design (record level updates, record level streams), that arguably fit the Flink model more naturally.\nAt the same time, there are multiple RDDs operating at the same time in Hudi, and the processing result of one RDD is combined with another RDD.\nThis difference in abstraction and the reuse of intermediate results during implementation make it difficult for Hudi to use a unified API to operate both RDD and DataStream in terms of decoupling abstraction."),(0,n.yg)("h2",{id:"3-decoupling-spark"},"3. Decoupling Spark"),(0,n.yg)("p",null,"In theory, Hudi uses Spark as its computing engine to use Spark's distributed computing power and RDD's rich operator capabilities. Apart from distributed computing power, Hudi uses RDD more as a data structure, and RDD is essentially a bounded data set.\nTherefore, it is theoretically feasible to replace RDD with List (of course, it may sacrifice performance/scale). In order to ensure the performance and stability of the Hudi Spark version as much as possible. We can keep setting the bounded data set as the basic operation unit.\nHudi's main operation API remains unchanged, and RDD is extracted as a generic type. The Spark engine implementation still uses RDD, and other engines use List or other bounded  data set according to the actual situation."),(0,n.yg)("h3",{id:"decoupling-principle"},"Decoupling principle"),(0,n.yg)("p",null,"1) Unified generics. The input records ",(0,n.yg)("inlineCode",{parentName:"p"},"JavaRDD<HoodieRecord>"),", key of input records ",(0,n.yg)("inlineCode",{parentName:"p"},"JavaRDD<HoodieKey>"),", and result of write operations ",(0,n.yg)("inlineCode",{parentName:"p"},"JavaRDD<WriteStatus>")," used by the Spark API use generic ",(0,n.yg)("inlineCode",{parentName:"p"},"I,K,O")," instead;"),(0,n.yg)("p",null,"2) De-sparkization. All APIs of the abstraction layer must have nothing to do with Spark. Involving specific operations that are difficult to implement in the abstract layer, rewrite them as abstract methods and introduce Spark subclasses."),(0,n.yg)("p",null,"For example: Hudi uses the ",(0,n.yg)("inlineCode",{parentName:"p"},"JavaSparkContext#map()")," method in many places. To de-spark, you need to hide the ",(0,n.yg)("inlineCode",{parentName:"p"},"JavaSparkContext"),". For this problem, we introduced the ",(0,n.yg)("inlineCode",{parentName:"p"},"HoodieEngineContext#map()")," method, which will block the specific implementation details of ",(0,n.yg)("inlineCode",{parentName:"p"},"map"),", so as to achieve de-sparkization in abstraction."),(0,n.yg)("p",null,"3) Minimize changes to the abstraction layer to ensure the original function and performance of Hudi;"),(0,n.yg)("p",null,"4) Replace the ",(0,n.yg)("inlineCode",{parentName:"p"},"JavaSparkContext")," with the ",(0,n.yg)("inlineCode",{parentName:"p"},"HoodieEngineContext")," abstract class to provide the running environment context."),(0,n.yg)("p",null,"In addition, some of the core algorithms in Hudi, like ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/pull/1756"},"rollback"),", has been redone without the need for computing a workload profile ahead of time, which used to rely on Spark caching. "),(0,n.yg)("h2",{id:"4-flink-integration-design"},"4. Flink integration design"),(0,n.yg)("p",null,"Hudi's write operation is batch processing in nature, and the continuous mode of ",(0,n.yg)("inlineCode",{parentName:"p"},"DeltaStreamer")," is realized by looping batch processing. In order to use a unified API, when Hudi integrates Flink, we choose to collect a batch of data before processing, and finally submit it in a unified manner (here we use List to collect data in Flink).\nIn Hudi terminology, we will stream data for a given commit, but only publish the commits every so often, making it practical to scale storage on cloud storage and also tunable."),(0,n.yg)("p",null,"The easiest way to think of batch operation is to use a time window. However, when using a window, when there is no data flowing in a window, there will be no output data, and it is difficult for the Flink sink to judge whether all the data from a given batch has been processed.\nTherefore, we use Flink's checkpoint mechanism to collect batches. The data between every two barriers is a batch. When there is no data in a subtask, the mock result data is made up.\nIn this way, on the sink side, when each subtask has result data issued, it can be considered that a batch of data has been processed and the commit can be executed."),(0,n.yg)("p",null,"The DAG is as follows:"),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"dualism",src:t(94097).A})),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Source:")," receives Kafka data and converts it into ",(0,n.yg)("inlineCode",{parentName:"li"},"List<HoodieRecord>"),";"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"InstantGeneratorOperator:")," generates a globally unique instant. When the previous instant is not completed or the current batch has no data, no new instant is created;"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"KeyBy partitionPath:")," partitions according to ",(0,n.yg)("inlineCode",{parentName:"li"},"partitionPath")," to avoid multiple subtasks from writing the same partition;"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"WriteProcessOperator:")," performs a write operation. When there is no data in the current partition, it sends empty result data to the downstream to make up the number;"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"CommitSink:")," receives the calculation results of the upstream task. When receiving the parallelism results, it is considered that all the upstream subtasks are completed and the commit is executed.")),(0,n.yg)("p",null,"Note:\n",(0,n.yg)("inlineCode",{parentName:"p"},"InstantGeneratorOperator")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"WriteProcessOperator")," are both custom Flink operators. ",(0,n.yg)("inlineCode",{parentName:"p"},"InstantGeneratorOperator")," will block checking the state of the previous instant to ensure that there is only one instant in the global (or requested) state.\n",(0,n.yg)("inlineCode",{parentName:"p"},"WriteProcessOperator")," is the actual execution Where a write operation is performed, the write operation is triggered at checkpoint."),(0,n.yg)("h3",{id:"41-index-design-based-on-flink-state"},"4.1 Index design based on Flink State"),(0,n.yg)("p",null,"Stateful computing is one of the highlights of the Flink engine. Compared with using external storage, using Flink's built-in ",(0,n.yg)("inlineCode",{parentName:"p"},"State")," can significantly improve the performance of Flink applications.\nTherefore, it would be a good choice to implement a Hudi index based on Flink's State."),(0,n.yg)("p",null,"The core of the Hudi index is to maintain the mapping of the Hudi key ",(0,n.yg)("inlineCode",{parentName:"p"},"HoodieKey")," and the location of the Hudi data ",(0,n.yg)("inlineCode",{parentName:"p"},"HoodieRecordLocation"),".\nTherefore, based on the current design, we can simply maintain a ",(0,n.yg)("inlineCode",{parentName:"p"},"MapState<HoodieKey, HoodieRecordLocation>")," in Flink UDF to map the ",(0,n.yg)("inlineCode",{parentName:"p"},"HoodieKey")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"HoodieRecordLocation"),", and leave the fault tolerance and persistence of State to the Flink framework."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"dualism",src:t(63930).A})),(0,n.yg)("h2",{id:"5-implementation-examples"},"5. Implementation examples"),(0,n.yg)("h3",{id:"1-hoodietable"},"1) HoodieTable"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"/**\n  * Abstract implementation of a HoodieTable.\n  *\n  * @param <T> Sub type of HoodieRecordPayload\n  * @param <I> Type of inputs\n  * @param <K> Type of keys\n  * @param <O> Type of outputs\n  */\npublic abstract class HoodieTable<T extends HoodieRecordPayload, I, K, O> implements Serializable {\n\n   protected final HoodieWriteConfig config;\n   protected final HoodieTableMetaClient metaClient;\n   protected final HoodieIndex<T, I, K, O> index;\n\n   public abstract HoodieWriteMetadata<O> upsert(HoodieEngineContext context, String instantTime,\n       I records);\n\n   public abstract HoodieWriteMetadata<O> insert(HoodieEngineContext context, String instantTime,\n       I records);\n\n   public abstract HoodieWriteMetadata<O> bulkInsert(HoodieEngineContext context, String instantTime,\n       I records, Option<BulkInsertPartitioner<I>> bulkInsertPartitioner);\n\n   ...\n}\n")),(0,n.yg)("p",null,(0,n.yg)("inlineCode",{parentName:"p"},"HoodieTable")," is one of the core abstractions of Hudi, which defines operations such as ",(0,n.yg)("inlineCode",{parentName:"p"},"insert"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"upsert"),", and ",(0,n.yg)("inlineCode",{parentName:"p"},"bulkInsert")," supported by the table.\nTake ",(0,n.yg)("inlineCode",{parentName:"p"},"upsert")," as an example, the input data is changed from the original ",(0,n.yg)("inlineCode",{parentName:"p"},"JavaRDD<HoodieRecord> inputRdds")," to ",(0,n.yg)("inlineCode",{parentName:"p"},"I records"),", and the runtime ",(0,n.yg)("inlineCode",{parentName:"p"},"JavaSparkContext jsc")," is changed to ",(0,n.yg)("inlineCode",{parentName:"p"},"HoodieEngineContext context"),"."),(0,n.yg)("p",null,"From the class annotations, we can see that ",(0,n.yg)("inlineCode",{parentName:"p"},"T, I, K, O")," represents the load data type, input data type, primary key type and output data type of Hudi operation respectively.\nThese generics will run through the entire abstraction layer."),(0,n.yg)("h3",{id:"2-hoodieenginecontext"},"2) HoodieEngineContext"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"/**\n * Base class contains the context information needed by the engine at runtime. It will be extended by different\n * engine implementation if needed.\n */\npublic abstract class HoodieEngineContext {\n\n  public abstract <I, O> List<O> map(List<I> data, SerializableFunction<I, O> func, int parallelism);\n\n  public abstract <I, O> List<O> flatMap(List<I> data, SerializableFunction<I, Stream<O>> func, int parallelism);\n\n  public abstract <I> void foreach(List<I> data, SerializableConsumer<I> consumer, int parallelism);\n\n  ......\n}\n")),(0,n.yg)("p",null,(0,n.yg)("inlineCode",{parentName:"p"},"HoodieEngineContext")," plays the role of ",(0,n.yg)("inlineCode",{parentName:"p"},"JavaSparkContext"),", it not only provides all the information that ",(0,n.yg)("inlineCode",{parentName:"p"},"JavaSparkContext")," can provide,\nbut also encapsulates many methods such as ",(0,n.yg)("inlineCode",{parentName:"p"},"map"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"flatMap"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"foreach"),", and hides The specific implementation of ",(0,n.yg)("inlineCode",{parentName:"p"},"JavaSparkContext#map()"),",",(0,n.yg)("inlineCode",{parentName:"p"},"JavaSparkContext#flatMap()"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"JavaSparkContext#foreach()")," and other methods."),(0,n.yg)("p",null,"Take the ",(0,n.yg)("inlineCode",{parentName:"p"},"map")," method as an example. In the Spark implementation class ",(0,n.yg)("inlineCode",{parentName:"p"},"HoodieSparkEngineContext"),", the ",(0,n.yg)("inlineCode",{parentName:"p"},"map")," method is as follows:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"  @Override\n  public <I, O> List<O> map(List<I> data, SerializableFunction<I, O> func, int parallelism) {\n    return javaSparkContext.parallelize(data, parallelism).map(func::apply).collect();\n  }\n")),(0,n.yg)("p",null,"In the engine that operates List, the implementation can be as follows (different methods need to pay attention to thread-safety issues, use ",(0,n.yg)("inlineCode",{parentName:"p"},"parallel()")," with caution):"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"  @Override\n  public <I, O> List<O> map(List<I> data, SerializableFunction<I, O> func, int parallelism) {\n    return data.stream().parallel().map(func::apply).collect(Collectors.toList());\n  }\n")),(0,n.yg)("p",null,"Note:\nThe exception thrown in the map function can be solved by wrapping ",(0,n.yg)("inlineCode",{parentName:"p"},"SerializableFunction<I, O> func"),"."),(0,n.yg)("p",null,"Here is a brief introduction to ",(0,n.yg)("inlineCode",{parentName:"p"},"SerializableFunction"),":"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"@FunctionalInterface\npublic interface SerializableFunction<I, O> extends Serializable {\n  O apply(I v1) throws Exception;\n}\n")),(0,n.yg)("p",null,"This method is actually a variant of ",(0,n.yg)("inlineCode",{parentName:"p"},"java.util.function.Function"),". The difference from ",(0,n.yg)("inlineCode",{parentName:"p"},"java.util.function.Function")," is that ",(0,n.yg)("inlineCode",{parentName:"p"},"SerializableFunction")," can be serialized and can throw exceptions.\nThis function is introduced because the input parameters that the ",(0,n.yg)("inlineCode",{parentName:"p"},"JavaSparkContext#map()")," function can receive must be serializable.\nAt the same time, there are many exceptions that need to be thrown in the logic of Hudi, and the code for ",(0,n.yg)("inlineCode",{parentName:"p"},"try-catch")," in the Lambda expression will be omitted It is bloated and not very elegant."),(0,n.yg)("h2",{id:"6-current-progress-and-follow-up-plan"},"6. Current progress and follow-up plan"),(0,n.yg)("h3",{id:"61-working-time-axis"},"6.1 Working time axis"),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"dualism",src:t(81507).A})),(0,n.yg)("p",null,(0,n.yg)("a",{parentName:"p",href:"https://www.t3go.cn/"},"T3go"),"\n",(0,n.yg)("a",{parentName:"p",href:"https://cn.aliyun.com/"},"Aliyun"),"\n",(0,n.yg)("a",{parentName:"p",href:"https://www.sf-express.com/cn/sc/"},"SF-express")),(0,n.yg)("h3",{id:"62-follow-up-plan"},"6.2 Follow-up plan"),(0,n.yg)("h4",{id:"1-promote-the-integration-of-hudi-and-flink"},"1) Promote the integration of Hudi and Flink"),(0,n.yg)("p",null,"Push the integration of Flink and Hudi to the community as soon as possible. In the initial stage, this feature may only support Kafka data sources."),(0,n.yg)("h4",{id:"2-performance-optimization"},"2) Performance optimization"),(0,n.yg)("p",null,"In order to ensure the stability and performance of the Hudi-Spark version, the decoupling did not take too much into consideration the possible performance problems of the Flink version."),(0,n.yg)("h4",{id:"3-flink-connector-hudi-like-third-party-package-development"},"3) flink-connector-hudi like third-party package development"),(0,n.yg)("p",null,"Make the binding of Hudi-Flink into a third-party package. Users can this third-party package to read/write from/to Hudi with Flink."))}g.isMDXComponent=!0},29692:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Origins of Data Lake at Grofers",authors:[{name:"Akshay Agarwal"}],category:"blog",image:"/assets/images/blog/2020-10-19-Origins-of-Data-Lake-at-Grofers.gif",tags:["use-case","datalake","change data capture","cdc","grofers"]},s=void 0,l={permalink:"/cn/blog/2020/10/19/Origins-of-Data-Lake-at-Grofers",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-10-19-Origins-of-Data-Lake-at-Grofers.mdx",source:"@site/blog/2020-10-19-Origins-of-Data-Lake-at-Grofers.mdx",title:"Origins of Data Lake at Grofers",description:"Redirecting... please wait!!",date:"2020-10-19T00:00:00.000Z",formattedDate:"October 19, 2020",tags:[{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"datalake",permalink:"/cn/blog/tags/datalake"},{label:"change data capture",permalink:"/cn/blog/tags/change-data-capture"},{label:"cdc",permalink:"/cn/blog/tags/cdc"},{label:"grofers",permalink:"/cn/blog/tags/grofers"}],readingTime:.045,truncated:!1,authors:[{name:"Akshay Agarwal"}],prevItem:{title:"Apply record level changes from relational databases to Amazon S3 data lake using Apache Hudi on Amazon EMR and AWS Database Migration Service",permalink:"/cn/blog/2020/10/19/hudi-meets-aws-emr-and-aws-dms"},nextItem:{title:"Apache Hudi meets Apache Flink",permalink:"/cn/blog/2020/10/15/apache-hudi-meets-apache-flink"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://lambda.grofers.com/origins-of-data-lake-at-grofers-6c011f94b86c",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},1356:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Apply record level changes from relational databases to Amazon S3 data lake using Apache Hudi on Amazon EMR and AWS Database Migration Service",excerpt:"AWS blog showing how to build a CDC pipeline that captures data from an Amazon RDS for MySQL database using AWS DMS and applies those changes to an Amazon S3 dataset using Apache Hudi on Amazon EMR.",author:"aws",category:"blog",image:"/assets/images/blog/2020-10-19-hudi-meets-aws-emr-and-aws-dms.jpeg",tags:["blog","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2020/10/19/hudi-meets-aws-emr-and-aws-dms",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-10-19-hudi-meets-aws-emr-and-aws-dms.md",source:"@site/blog/2020-10-19-hudi-meets-aws-emr-and-aws-dms.md",title:"Apply record level changes from relational databases to Amazon S3 data lake using Apache Hudi on Amazon EMR and AWS Database Migration Service",description:"This blog published by AWS shows how to build a CDC pipeline that captures data from an Amazon Relational Database Service (Amazon RDS) for MySQL database using AWS Database Migration Service (AWS DMS) and applies those changes to a dataset in Amazon S3 using Apache Hudi on Amazon EMR.",date:"2020-10-19T00:00:00.000Z",formattedDate:"October 19, 2020",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:.245,truncated:!1,authors:[{name:"aws"}],prevItem:{title:"Data Lake Change Capture using Apache Hudi & Amazon AMS/EMR",permalink:"/cn/blog/2020/10/21/Data-Lake-Change-Capture-using-Apache-Hudi-and-Amazon-AMS-EMR"},nextItem:{title:"Origins of Data Lake at Grofers",permalink:"/cn/blog/2020/10/19/Origins-of-Data-Lake-at-Grofers"}},l={authorsImageUrls:[void 0]},d=[],c={toc:d},p="wrapper";function g(e){let{components:a,...t}=e;return(0,n.yg)(p,(0,i.A)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"This ",(0,n.yg)("a",{parentName:"p",href:"https://aws.amazon.com/blogs/big-data/apply-record-level-changes-from-relational-databases-to-amazon-s3-data-lake-using-apache-hudi-on-amazon-emr-and-aws-database-migration-service/"},"blog")," published by AWS shows how to build a CDC pipeline that captures data from an Amazon Relational Database Service (Amazon RDS) for MySQL database using AWS Database Migration Service (AWS DMS) and applies those changes to a dataset in Amazon S3 using Apache Hudi on Amazon EMR."))}g.isMDXComponent=!0},95978:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Architecting Data Lakes for the Modern Enterprise at Data Summit Connect Fall 2020",authors:[{name:"Stephanie Simone"}],category:"blog",image:"/assets/images/blog/data-summit-connect.jpeg",tags:["blog","dbta"]},s=void 0,l={permalink:"/cn/blog/2020/10/21/Architecting-Data-Lakes-for-the-Modern-Enterprise-at-Data-Summit-Connect-Fall-2020",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-10-21-Architecting-Data-Lakes-for-the-Modern-Enterprise-at-Data-Summit-Connect-Fall-2020.mdx",source:"@site/blog/2020-10-21-Architecting-Data-Lakes-for-the-Modern-Enterprise-at-Data-Summit-Connect-Fall-2020.mdx",title:"Architecting Data Lakes for the Modern Enterprise at Data Summit Connect Fall 2020",description:"Redirecting... please wait!!",date:"2020-10-21T00:00:00.000Z",formattedDate:"October 21, 2020",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"dbta",permalink:"/cn/blog/tags/dbta"}],readingTime:.045,truncated:!1,authors:[{name:"Stephanie Simone"}],prevItem:{title:"Employing the right indexes for fast updates, deletes in Apache Hudi",permalink:"/cn/blog/2020/11/11/hudi-indexing-mechanisms"},nextItem:{title:"Data Lake Change Capture using Apache Hudi & Amazon AMS/EMR",permalink:"/cn/blog/2020/10/21/Data-Lake-Change-Capture-using-Apache-Hudi-and-Amazon-AMS-EMR"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.dbta.com/Editorial/News-Flashes/Architecting-Data-Lakes-for-the-Modern-Enterprise-at-Data-Summit-Connect-Fall-2020-143512.aspx",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},94747:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Data Lake Change Capture using Apache Hudi & Amazon AMS/EMR",authors:[{name:"Manoj Kukreja"}],category:"blog",image:"/assets/images/blog/2020-10-21-Data-Lake-Change-Capture-using-Apache-Hudi-and-Amazon-AMS-EMR.jpeg",tags:["how-to","change data capture","cdc","towardsdatascience"]},s=void 0,l={permalink:"/cn/blog/2020/10/21/Data-Lake-Change-Capture-using-Apache-Hudi-and-Amazon-AMS-EMR",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-10-21-Data-Lake-Change-Capture-using-Apache-Hudi-and-Amazon-AMS-EMR.mdx",source:"@site/blog/2020-10-21-Data-Lake-Change-Capture-using-Apache-Hudi-and-Amazon-AMS-EMR.mdx",title:"Data Lake Change Capture using Apache Hudi & Amazon AMS/EMR",description:"Redirecting... please wait!!",date:"2020-10-21T00:00:00.000Z",formattedDate:"October 21, 2020",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"change data capture",permalink:"/cn/blog/tags/change-data-capture"},{label:"cdc",permalink:"/cn/blog/tags/cdc"},{label:"towardsdatascience",permalink:"/cn/blog/tags/towardsdatascience"}],readingTime:.045,truncated:!1,authors:[{name:"Manoj Kukreja"}],prevItem:{title:"Architecting Data Lakes for the Modern Enterprise at Data Summit Connect Fall 2020",permalink:"/cn/blog/2020/10/21/Architecting-Data-Lakes-for-the-Modern-Enterprise-at-Data-Summit-Connect-Fall-2020"},nextItem:{title:"Apply record level changes from relational databases to Amazon S3 data lake using Apache Hudi on Amazon EMR and AWS Database Migration Service",permalink:"/cn/blog/2020/10/19/hudi-meets-aws-emr-and-aws-dms"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://towardsdatascience.com/data-lake-change-data-capture-cdc-using-apache-hudi-on-amazon-emr-part-2-process-65e4662d7b4b",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},66030:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Employing the right indexes for fast updates, deletes in Apache Hudi",excerpt:"Detailing different indexing mechanisms in Hudi and when to use each of them",author:"vinoth",category:"blog",image:"/assets/images/blog/hudi-indexes/with-and-without-index.png",tags:["how-to","indexing","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2020/11/11/hudi-indexing-mechanisms",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-11-11-hudi-indexing-mechanisms.md",source:"@site/blog/2020-11-11-hudi-indexing-mechanisms.md",title:"Employing the right indexes for fast updates, deletes in Apache Hudi",description:"Apache Hudi employs an index to locate the file group, that an update/delete belongs to. For Copy-On-Write tables, this enables",date:"2020-11-11T00:00:00.000Z",formattedDate:"November 11, 2020",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"indexing",permalink:"/cn/blog/tags/indexing"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:7.585,truncated:!0,authors:[{name:"vinoth"}],prevItem:{title:"Can Big Data Solutions Be Affordable?",permalink:"/cn/blog/2020/11/29/Can-Big-Data-Solutions-Be-Affordable"},nextItem:{title:"Architecting Data Lakes for the Modern Enterprise at Data Summit Connect Fall 2020",permalink:"/cn/blog/2020/10/21/Architecting-Data-Lakes-for-the-Modern-Enterprise-at-Data-Summit-Connect-Fall-2020"}},l={authorsImageUrls:[void 0]},d=[{value:"Index Types in Hudi",id:"index-types-in-hudi",children:[],level:2},{value:"Workload: Late arriving updates to fact tables",id:"workload-late-arriving-updates-to-fact-tables",children:[],level:2},{value:"Workload: De-Duplication in event tables",id:"workload-de-duplication-in-event-tables",children:[],level:2},{value:"Workload: Random updates/deletes to a dimension table",id:"workload-random-updatesdeletes-to-a-dimension-table",children:[],level:2},{value:"Summary",id:"summary",children:[],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...o}=e;return(0,n.yg)(p,(0,i.A)({},c,o,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"Apache Hudi employs an index to locate the file group, that an update/delete belongs to. For Copy-On-Write tables, this enables\nfast upsert/delete operations, by avoiding the need to join against the entire dataset to determine which files to rewrite.\nFor Merge-On-Read tables, this design allows Hudi to bound the amount of records any given base file needs to be merged against.\nSpecifically, a given base file needs to merged only against updates for records that are part of that base file. In contrast,\ndesigns without an indexing component (e.g: ",(0,n.yg)("a",{parentName:"p",href:"https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions"},"Apache Hive ACID"),"),\ncould end up having to merge all the base files against all incoming updates/delete records."),(0,n.yg)("p",null,"At a high level, an index maps a record key + an optional partition path to a file group ID on storage (explained\nmore in detail ",(0,n.yg)("a",{parentName:"p",href:"/docs/concepts"},"here"),") and during write operations, we lookup this mapping to route an incoming update/delete\nto a log file attached to the base file (MOR) or to the latest base file that now needs to be merged against (COW). The index also enables\nHudi to enforce unique constraints based on the record keys."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Fact table",src:t(22542).A}),"\n",(0,n.yg)("em",{parentName:"p"},"Figure: Comparison of merge cost for updates (yellow blocks) against base files (white blocks)")),(0,n.yg)("p",null,"Given that Hudi already supports few different indexing techniques and is also continuously improving/adding more to its toolkit, the rest of the blog\nattempts to explain different categories of workloads, from our experience and suggests what index types to use for each. We will also interlace\ncommentary on existing limitations, upcoming work and optimizations/tradeoffs along the way. "),(0,n.yg)("h2",{id:"index-types-in-hudi"},"Index Types in Hudi"),(0,n.yg)("p",null,"Currently, Hudi supports the following indexing options. "),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Bloom Index (default):")," Employs bloom filters built out of the record keys, optionally also pruning candidate files using record key ranges."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Simple Index:")," Performs a lean join of the incoming update/delete records against keys extracted from the table on storage."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"HBase Index:")," Manages the index mapping in an external Apache HBase table.")),(0,n.yg)("p",null,"Writers can pick one of these options using ",(0,n.yg)("inlineCode",{parentName:"p"},"hoodie.index.type")," config option. Additionally, a custom index implementation can also be employed\nusing ",(0,n.yg)("inlineCode",{parentName:"p"},"hoodie.index.class")," and supplying a subclass of ",(0,n.yg)("inlineCode",{parentName:"p"},"SparkHoodieIndex")," (for Apache Spark writers) "),(0,n.yg)("p",null,"Another key aspect worth understanding is the difference between global and non-global indexes. Both bloom and simple index have\nglobal options - ",(0,n.yg)("inlineCode",{parentName:"p"},"hoodie.index.type=GLOBAL_BLOOM")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"hoodie.index.type=GLOBAL_SIMPLE")," - respectively. HBase index is by nature a global index."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Global index:"),"  Global indexes enforce uniqueness of keys across all partitions of a table i.e guarantees that exactly\none record exists in the table for a given record key. Global indexes offer stronger guarantees, but the update/delete cost grows\nwith size of the table ",(0,n.yg)("inlineCode",{parentName:"p"},"O(size of table)"),", which might still be acceptable for smaller tables.")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Non Global index:")," On the other hand, the default index implementations enforce this constraint only within a specific partition.\nAs one might imagine, non global indexes depends on the writer to provide the same consistent partition path for a given record key during update/delete,\nbut can deliver much better performance since the index lookup operation becomes ",(0,n.yg)("inlineCode",{parentName:"p"},"O(number of records updated/deleted)")," and\nscales well with write volume."))),(0,n.yg)("p",null,"Since data comes in at different volumes, velocity and has different access patterns, different indices could be used for different workloads.\nNext, let\u2019s walk through some typical workloads and see how to leverage the right Hudi index for such use-cases."),(0,n.yg)("h2",{id:"workload-late-arriving-updates-to-fact-tables"},"Workload: Late arriving updates to fact tables"),(0,n.yg)("p",null,"Many companies store large volumes of transactional data in NoSQL data stores. For eg, trip tables in case of ride-sharing, buying and selling of shares,\norders in an e-commerce site. These tables are usually ever growing with random updates on most recent data with long tail updates going to older data, either\ndue to transactions settling at a later date/data corrections. In other words, most updates go into the latest partitions with few updates going to older ones."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Fact table",src:t(82242).A}),"\n",(0,n.yg)("em",{parentName:"p"},"Figure: Typical update pattern for Fact tables")),(0,n.yg)("p",null,"For such workloads, the ",(0,n.yg)("inlineCode",{parentName:"p"},"BLOOM")," index performs well, since index look-up will prune a lot of data files based on a well-sized bloom filter.\nAdditionally, if the keys can be constructed such that they have a certain ordering, the number of files to be compared is further reduced by range pruning.\nHudi constructs an interval tree with all the file key ranges and efficiently filters out the files that don't match any key ranges in the updates/deleted records."),(0,n.yg)("p",null,"In order to efficiently compare incoming record keys against bloom filters i.e with minimal number of bloom filter reads and uniform distribution of work across\nthe executors, Hudi leverages caching of input records and employs a custom partitioner that can iron out data skews using statistics. At times, if the bloom filter\nfalse positive ratio is high, it could increase the amount of data shuffled to perform the lookup. Hudi supports dynamic bloom filters\n(enabled using ",(0,n.yg)("inlineCode",{parentName:"p"},"hoodie.bloom.index.filter.type=DYNAMIC_V0"),"), which adjusts its size based on the number of records stored in a given file to deliver the\nconfigured false positive ratio. "),(0,n.yg)("p",null,"In the near future, we plan to introduce a much speedier version of the BLOOM index that tracks bloom filters/ranges in an internal Hudi metadata table, indexed for fast\npoint lookups. This would avoid any current limitations around reading bloom filters/ranges from the base files themselves, to perform the lookup. (see\n",(0,n.yg)("a",{parentName:"p",href:"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+15%3A+HUDI+File+Listing+and+Query+Planning+Improvements?src=contextnavpagetreemode"},"RFC-15")," for the general design)"),(0,n.yg)("h2",{id:"workload-de-duplication-in-event-tables"},"Workload: De-Duplication in event tables"),(0,n.yg)("p",null,'Event Streaming is everywhere. Events coming from Apache Kafka or similar message bus are typically 10-100x the size of fact tables and often treat "time" (event\'s arrival time/processing\ntime) as a first class citizen. For eg, IoT event stream, click stream data, ad impressions etc. Inserts and updates only span the last few partitions as these are mostly append only data.\nGiven duplicate events can be introduced anywhere in the end-end pipeline, de-duplication before storing on the data lake is a common requirement. '),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Event table",src:t(28126).A}),"\n",(0,n.yg)("em",{parentName:"p"},"Figure showing the spread of updates for Event table.")),(0,n.yg)("p",null,"In general, this is a very challenging problem to solve at lower cost. Although, we could even employ a key value store to perform this de-duplication ala HBASE index, the index storage\ncosts would grow linear with number of events and thus can be prohibitively expensive. In fact, ",(0,n.yg)("inlineCode",{parentName:"p"},"BLOOM")," index with range pruning is the optimal solution here. One can leverage the fact\nthat time is often a first class citizen and construct a key such as ",(0,n.yg)("inlineCode",{parentName:"p"},"event_ts + event_id")," such that the inserted records have monotonically increasing keys. This yields great returns\nby pruning large amounts of files even within the latest table partitions. "),(0,n.yg)("h2",{id:"workload-random-updatesdeletes-to-a-dimension-table"},"Workload: Random updates/deletes to a dimension table"),(0,n.yg)("p",null,"These types of tables usually contain high dimensional data and hold reference data e.g user profile, merchant information. These are high fidelity tables where the updates are often small but also spread\nacross a lot of partitions and data files ranging across the dataset from old to new. Often times, these tables are also un-partitioned, since there is also not a good way to partition these tables."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Dimensions table",src:t(26874).A}),"\n",(0,n.yg)("em",{parentName:"p"},"Figure showing the spread of updates for Dimensions table.")),(0,n.yg)("p",null,"As discussed before, the ",(0,n.yg)("inlineCode",{parentName:"p"},"BLOOM")," index may not yield benefits if a good number of files cannot be pruned out by comparing ranges/filters. In such a random write workload, updates end up touching\nmost files within in the table and thus bloom filters will typically indicate a true positive for all files based on some incoming update. Consequently, we would end up comparing ranges/filter, only\nto finally check the incoming updates against all files. The ",(0,n.yg)("inlineCode",{parentName:"p"},"SIMPLE")," Index will be a better fit as it does not do any upfront pruning based, but directly joins with interested fields from every data file.\n",(0,n.yg)("inlineCode",{parentName:"p"},"HBASE")," index can be employed, if the operational overhead is acceptable and would provide much better lookup times for these tables. "),(0,n.yg)("p",null,"When using a global index, users should also consider setting ",(0,n.yg)("inlineCode",{parentName:"p"},"hoodie.bloom.index.update.partition.path=true")," or ",(0,n.yg)("inlineCode",{parentName:"p"},"hoodie.simple.index.update.partition.path=true")," to deal with cases where the\npartition path value could change due to an update e.g users table partitioned by home city; user relocates to a different city. These tables are also excellent candidates for the Merge-On-Read table type."),(0,n.yg)("p",null,"Going forward, we plan to build ",(0,n.yg)("a",{parentName:"p",href:"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+08+%3A+Record+level+indexing+mechanisms+for+Hudi+datasets?src=contextnavpagetreemode"},"record level indexing"),"\nright within Hudi, which will improve the index look-up time and will also avoid additional overhead of maintaining an external system like hbase. "),(0,n.yg)("h2",{id:"summary"},"Summary"),(0,n.yg)("p",null,"Without the indexing capabilities in Hudi, it would not been possible to make upserts/deletes happen at ",(0,n.yg)("a",{parentName:"p",href:"https://eng.uber.com/apache-hudi-graduation/"},"very large scales"),".\nHopefully this post gave you good enough context on the indexing mechanisms today and how different tradeoffs play out. "),(0,n.yg)("p",null,"Some interesting work underway in this area:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Apache Flink based writing with a RocksDB state store backed indexing mechanism, unlocking true streaming upserts on data lakes. "),(0,n.yg)("li",{parentName:"ul"},"A brand new MetadataIndex, which reimagines the bloom index today on top of the metadata table in Hudi."),(0,n.yg)("li",{parentName:"ul"},"Record level index implementation, as a secondary index using another Hudi table.")),(0,n.yg)("p",null,"Going forward, this will remain an area of active investment for the project. we are always looking for contributors who can drive these roadmap items forward.\nPlease ",(0,n.yg)("a",{parentName:"p",href:"/contribute/get-involved"},"engage")," with our community if you want to get involved."))}g.isMDXComponent=!0},16935:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Can Big Data Solutions Be Affordable?",category:"blog",image:"/assets/images/blog/2020-11-29-Can-Big-Data-Solutions-Be-Affordable.jpg",tags:["blog","big-data","near real-time analytics","analyticsinsight"]},s=void 0,l={permalink:"/cn/blog/2020/11/29/Can-Big-Data-Solutions-Be-Affordable",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-11-29-Can-Big-Data-Solutions-Be-Affordable.mdx",source:"@site/blog/2020-11-29-Can-Big-Data-Solutions-Be-Affordable.mdx",title:"Can Big Data Solutions Be Affordable?",description:"Redirecting... please wait!!",date:"2020-11-29T00:00:00.000Z",formattedDate:"November 29, 2020",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"big-data",permalink:"/cn/blog/tags/big-data"},{label:"near real-time analytics",permalink:"/cn/blog/tags/near-real-time-analytics"},{label:"analyticsinsight",permalink:"/cn/blog/tags/analyticsinsight"}],readingTime:.045,truncated:!1,authors:[],prevItem:{title:"Building High-Performance Data Lake Using Apache Hudi and Alluxio at T3Go",permalink:"/cn/blog/2020/12/01/high-perf-data-lake-with-hudi-and-alluxio-t3go"},nextItem:{title:"Employing the right indexes for fast updates, deletes in Apache Hudi",permalink:"/cn/blog/2020/11/11/hudi-indexing-mechanisms"}},d={authorsImageUrls:[]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.analyticsinsight.net/can-big-data-solutions-be-affordable/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},33616:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Building High-Performance Data Lake Using Apache Hudi and Alluxio at T3Go",excerpt:"How T3Go\u2019s high-performance data lake using Apache Hudi and Alluxio shortened the time for data ingestion into the lake by up to a factor of 2. Data analysts using Presto, Hudi, and Alluxio in conjunction to query data on the lake saw queries speed up by 10 times faster.",author:"t3go",category:"blog",image:"/assets/images/blog/2020-12-01-t3go-architecture.png",tags:["use-case","near real-time analytics","incremental processing","caching","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2020/12/01/high-perf-data-lake-with-hudi-and-alluxio-t3go",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-12-01-high-perf-data-lake-with-hudi-and-alluxio-t3go.md",source:"@site/blog/2020-12-01-high-perf-data-lake-with-hudi-and-alluxio-t3go.md",title:"Building High-Performance Data Lake Using Apache Hudi and Alluxio at T3Go",description:"Building High-Performance Data Lake Using Apache Hudi and Alluxio at T3Go",date:"2020-12-01T00:00:00.000Z",formattedDate:"December 1, 2020",tags:[{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"near real-time analytics",permalink:"/cn/blog/tags/near-real-time-analytics"},{label:"incremental processing",permalink:"/cn/blog/tags/incremental-processing"},{label:"caching",permalink:"/cn/blog/tags/caching"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:7.975,truncated:!0,authors:[{name:"t3go"}],prevItem:{title:"Optimize Data lake layout using Clustering in Apache Hudi",permalink:"/cn/blog/2021/01/27/hudi-clustering-intro"},nextItem:{title:"Can Big Data Solutions Be Affordable?",permalink:"/cn/blog/2020/11/29/Can-Big-Data-Solutions-Be-Affordable"}},l={authorsImageUrls:[void 0]},d=[{value:"Building High-Performance Data Lake Using Apache Hudi and Alluxio at T3Go",id:"building-high-performance-data-lake-using-apache-hudi-and-alluxio-at-t3go",children:[],level:2},{value:"I. T3Go data lake Overview",id:"i-t3go-data-lake-overview",children:[],level:2},{value:"II. Efficient Near Real-time Analytics Using Hudi",id:"ii-efficient-near-real-time-analytics-using-hudi",children:[{value:"Enable Near real time data ingestion and analysis",id:"enable-near-real-time-data-ingestion-and-analysis",children:[],level:3},{value:"Enable Incremental processing pipeline",id:"enable-incremental-processing-pipeline",children:[],level:3},{value:"Accessing Data using Hudi as a unified format",id:"accessing-data-using-hudi-as-a-unified-format",children:[],level:3}],level:2},{value:"III. Efficient Data Caching Using Alluxio",id:"iii-efficient-data-caching-using-alluxio",children:[{value:"Data lake ingestion",id:"data-lake-ingestion",children:[],level:3},{value:"Data analysis on the lake",id:"data-analysis-on-the-lake",children:[],level:3},{value:"Concurrent accesses across multiple storage systems",id:"concurrent-accesses-across-multiple-storage-systems",children:[],level:3},{value:"Microbenchmark",id:"microbenchmark",children:[],level:3}],level:2},{value:"IV. Next Step",id:"iv-next-step",children:[],level:2},{value:"V. Conclusion",id:"v-conclusion",children:[],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...o}=e;return(0,n.yg)(p,(0,i.A)({},c,o,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("h2",{id:"building-high-performance-data-lake-using-apache-hudi-and-alluxio-at-t3go"},"Building High-Performance Data Lake Using Apache Hudi and Alluxio at T3Go"),(0,n.yg)("p",null,(0,n.yg)("a",{parentName:"p",href:"https://www.t3go.cn/"},"T3Go"),"  is China\u2019s first platform for smart travel based on the Internet of Vehicles. In this article, Trevor Zhang and Vino Yang from T3Go describe the evolution of their data lake architecture, built on cloud-native or open-source technologies including Alibaba OSS, Apache Hudi, and Alluxio. Today, their data lake stores petabytes of data, supporting hundreds of pipelines and tens of thousands of tasks daily. It is essential for business units at T3Go including Data Warehouse, Internet of Vehicles, Order Dispatching, Machine Learning, and self-service query analysis."),(0,n.yg)("p",null,"In this blog, you will see how we slashed data ingestion time by half using Hudi and Alluxio. Furthermore, data analysts using Presto, Hudi, and Alluxio saw the queries speed up by 10 times. We built our data lake based on data orchestration for multiple stages of our data pipeline, including ingestion and analytics."),(0,n.yg)("h2",{id:"i-t3go-data-lake-overview"},"I. T3Go data lake Overview"),(0,n.yg)("p",null,"Prior to the data lake, different business units within T3Go managed their own data processing solutions, utilizing different storage systems, ETL tools, and data processing frameworks. Data for each became siloed from every other unit, significantly increasing cost and complexity. Due to the rapid business expansion of T3Go, this inefficiency became our engineering bottleneck."),(0,n.yg)("p",null,"We moved to a unified data lake solution based on Alibaba OSS, an object store similar to AWS S3, to provide a centralized location to store structured and unstructured data, following the design principles of  ",(0,n.yg)("em",{parentName:"p"},"Multi-cluster Shared-data Architecture"),"; all the applications access OSS storage as the source of truth, as opposed to different data silos. This architecture allows us to store the data as-is, without having to first structure the data, and run different types of analytics to guide better decisions, building dashboards and visualizations from big data processing, real-time analytics, and machine learning."),(0,n.yg)("h2",{id:"ii-efficient-near-real-time-analytics-using-hudi"},"II. Efficient Near Real-time Analytics Using Hudi"),(0,n.yg)("p",null,"Our business in smart travel drives the need to process and analyze data in a near real-time manner. With a traditional data warehouse, we faced the following challenges:  "),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"High overhead when updating due to long-tail latency"),(0,n.yg)("li",{parentName:"ol"},"High cost of order analysis due to the long window of a business session"),(0,n.yg)("li",{parentName:"ol"},"Reduced query accuracy due to late or ad-hoc updates"),(0,n.yg)("li",{parentName:"ol"},"Unreliability in data ingestion pipeline"),(0,n.yg)("li",{parentName:"ol"},"Data lost in the distributed data pipeline that cannot be reconciled"),(0,n.yg)("li",{parentName:"ol"},"High latency to access data storage")),(0,n.yg)("p",null,"As a result, we adopted Apache Hudi on top of OSS to address these issues. The following diagram outlines the architecture:"),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"architecture",src:t(87838).A})),(0,n.yg)("h3",{id:"enable-near-real-time-data-ingestion-and-analysis"},"Enable Near real time data ingestion and analysis"),(0,n.yg)("p",null,"With Hudi, our data lake supports multiple data sources including Kafka, MySQL binlog, GIS, and other business logs in near real time. As a result, more than 60% of the company\u2019s data is stored in the data lake and this proportion continues to increase."),(0,n.yg)("p",null,"We are also able to speed up the data ingestion time down to a few minutes by introducing Apache Hudi into the data pipeline. Combined with big data interactive query and analysis framework such as Presto and SparkSQL, real-time data analysis and insights are achieved."),(0,n.yg)("h3",{id:"enable-incremental-processing-pipeline"},"Enable Incremental processing pipeline"),(0,n.yg)("p",null,"With the help of Hudi, it is possible to provide incremental changes to the downstream derived table when the upstream table updates frequently. Even with a large number of interdependent tables, we can quickly run partial data updates. This also effectively avoids updating the full partitions of cold tables in the traditional Hive data warehouse."),(0,n.yg)("h3",{id:"accessing-data-using-hudi-as-a-unified-format"},"Accessing Data using Hudi as a unified format"),(0,n.yg)("p",null,"Traditional data warehouses often deploy Hadoop to store data and provide batch analysis. Kafka is used separately to distribute Hadoop data to other data processing frameworks, resulting in duplicated data. Hudi helps effectively solve this problem; we always use Spark pipelines to insert new updates into the Hudi tables, then incrementally read the update of Hudi tables. In other words, Hudi tables are used as the unified storage format to access data."),(0,n.yg)("h2",{id:"iii-efficient-data-caching-using-alluxio"},"III. Efficient Data Caching Using Alluxio"),(0,n.yg)("p",null,"In the early version of our data lake without Alluxio, data received from Kafka in real time is processed by Spark and then written to OSS data lake using Hudi DeltaStreamer tasks. With this architecture, Spark often suffered high network latency when writing to OSS directly. Since all data is in OSS storage, OLAP queries on Hudi data may also be slow due to lack of data locality."),(0,n.yg)("p",null,"To address the latency issue, we deployed Alluxio as a data orchestration layer, co-located with computing engines such as Spark and Presto, and used Alluxio to accelerate read and write on the data lake as shown in the following diagram:"),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"architecture-alluxio",src:t(88919).A})),(0,n.yg)("p",null,"Data in formats such as Hudi, Parquet, ORC, and JSON are stored mostly on OSS, consisting of 95% of the data. Computing engines such as Flink, Spark, Kylin, and Presto are deployed in isolated clusters respectively. When each engine accesses OSS, Alluxio acts as a virtual distributed storage system to accelerate data, being co-located with each of the computing clusters."),(0,n.yg)("p",null,"Specifically, here are a few applications leveraging Alluxio in the T3Go data lake."),(0,n.yg)("h3",{id:"data-lake-ingestion"},"Data lake ingestion"),(0,n.yg)("p",null,"We mount the corresponding OSS path to the Alluxio file system and set Hudi\u2019s  ",(0,n.yg)("em",{parentName:"p"},"\u201c",(0,n.yg)("strong",{parentName:"em"},"target-base-path"),"\u201d"),"  parameter value to use the alluxio:// scheme in place of oss:// scheme. Spark pipelines with Hudi continuously ingest data to Alluxio. After data is written to Alluxio, it is asynchronously persisted from the Alluxio cache to the remote OSS every minute. These modifications allow Spark to write to a local Alluxio node instead of writing to remote OSS, significantly reducing the time for the data to be available in data lake after ingestion."),(0,n.yg)("h3",{id:"data-analysis-on-the-lake"},"Data analysis on the lake"),(0,n.yg)("p",null,"We use Presto as an ad-hoc query engine to analyze the Hudi tables in the lake, co-locating Alluxio workers on each Presto worker node. When Presto and Alluxio services are co-located and running, Alluxio caches the input data locally in the Presto worker which greatly benefits Presto for subsequent retrievals. On a cache hit, Presto can read from the local Alluxio worker storage at memory speed without any additional data transfer over the network."),(0,n.yg)("h3",{id:"concurrent-accesses-across-multiple-storage-systems"},"Concurrent accesses across multiple storage systems"),(0,n.yg)("p",null,"In order to ensure the accuracy of training samples, our machine learning team often synchronizes desensitized data in production to an offline machine learning environment. During synchronization, the data flows across multiple file systems, from production OSS to an offline HDFS followed by another offline Machine Learning HDFS."),(0,n.yg)("p",null,"This data migration process is not only inefficient but also error-prune for modelers because multiple different storages with varying configurations are involved. Alluxio helps in this specific scenario by mounting the destination storage systems under the same filesystem to be accessed by their corresponding logical paths in Alluxio namespace. By decoupling the physical storage, this allows applications with different APIs to access and transfer data seamlessly. This data access layout also improves performance."),(0,n.yg)("h3",{id:"microbenchmark"},"Microbenchmark"),(0,n.yg)("p",null,"Overall, we observed the following improvements with Alluxio:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"It supports a hierarchical and transparent caching mechanism"),(0,n.yg)("li",{parentName:"ol"},"It supports cache promote omode mode when reading"),(0,n.yg)("li",{parentName:"ol"},"It supports asynchronous writing mode"),(0,n.yg)("li",{parentName:"ol"},"It supports LRU recycling strategy"),(0,n.yg)("li",{parentName:"ol"},"It has pin and TTL features")),(0,n.yg)("p",null,"After comparison and verification, we choose to use Spark SQL as the query engine. Our performance testing queries the Hudi table, comparing Alluxio + OSS together against OSS directly as well as HDFS."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"microbench",src:t(43806).A})),(0,n.yg)("p",null,"In the stress test shown above, after the data volume is greater than a certain magnitude (2400W), the query speed using Alluxio+OSS surpasses the HDFS query speed of the hybrid deployment. After the data volume is greater than 1E, the query speed starts to double. After reaching 6E data, it is up to 12 times higher than querying native OSS and 8 times higher than querying native HDFS. The improvement depends on the machine configuration."),(0,n.yg)("p",null,"Based on our performance benchmarking, we found that the performance can be improved by over 10 times with the help of Alluxio. Furthermore, the larger the data scale, the more prominent the performance improvement."),(0,n.yg)("h2",{id:"iv-next-step"},"IV. Next Step"),(0,n.yg)("p",null,"As T3Go\u2019s data lake ecosystem expands, we will continue facing the critical scenario of compute and storage segregation. With T3Go\u2019s growing data processing needs, our team plans to deploy Alluxio on a larger scale to accelerate our data lake storage."),(0,n.yg)("p",null,"In addition to the deployment of Alluxio on the data lake computing engine, which currently is mainly SparkSQL, we plan to add a layer of Alluxio to the OLAP cluster using Apache Kylin and an ad_hoc cluster using Presto. The goal is to have Alluxio cover all computing scenarios, with Alluxio interconnected between each scene to improve the read and write efficiency of the data lake and the surrounding lake ecology."),(0,n.yg)("h2",{id:"v-conclusion"},"V. Conclusion"),(0,n.yg)("p",null,"As mentioned earlier, Hudi and Alluxio covers all scenarios of Hudi\u2019s near real-time ingestion, near real-time analysis, incremental processing, and data distribution on DFS, among many others, and plays the role of a powerful accelerator on data ingestion and data analysis on the lake. With Hudi and Alluxio together,  ",(0,n.yg)("strong",{parentName:"p"},"our R&D engineers shortened the time for data ingestion into the lake by up to a factor of 2. Data analysts using Presto, Hudi, and Alluxio in conjunction to query data on the lake saw their queries speed up by 10 times faster.")," Furthermore, the larger the data scale, the more prominent the performance improvement. Alluxio is an important part of T3Go\u2019s plan to become a leading enterprise data lake in China. We look forward to seeing further integration with Alluxio in T3Go\u2019s data lake ecosystem."))}g.isMDXComponent=!0},91107:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Optimize Data lake layout using Clustering in Apache Hudi",excerpt:"Introduce clustering feature to change data layout",author:"satish.kotha",category:"blog",image:"/assets/images/blog/2021-01-27-hudi-clustering-intro.png",tags:["design","clustering","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2021/01/27/hudi-clustering-intro",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-01-27-hudi-clustering-intro.md",source:"@site/blog/2021-01-27-hudi-clustering-intro.md",title:"Optimize Data lake layout using Clustering in Apache Hudi",description:"Background",date:"2021-01-27T00:00:00.000Z",formattedDate:"January 27, 2021",tags:[{label:"design",permalink:"/cn/blog/tags/design"},{label:"clustering",permalink:"/cn/blog/tags/clustering"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:5.705,truncated:!0,authors:[{name:"satish.kotha"}],prevItem:{title:"Apache Hudi Key Generators",permalink:"/cn/blog/2021/02/13/hudi-key-generators"},nextItem:{title:"Building High-Performance Data Lake Using Apache Hudi and Alluxio at T3Go",permalink:"/cn/blog/2020/12/01/high-perf-data-lake-with-hudi-and-alluxio-t3go"}},l={authorsImageUrls:[void 0]},d=[{value:"Background",id:"background",children:[],level:2},{value:"Clustering Architecture",id:"clustering-architecture",children:[{value:"Overall, there are 2 parts to clustering",id:"overall-there-are-2-parts-to-clustering",children:[],level:4},{value:"Scheduling clustering",id:"scheduling-clustering",children:[],level:4},{value:"Running clustering",id:"running-clustering",children:[],level:4},{value:"Setting up clustering",id:"setting-up-clustering",children:[],level:4}],level:2},{value:"Table Query Performance",id:"table-query-performance",children:[{value:"Before Clustering",id:"before-clustering",children:[],level:3},{value:"After Clustering",id:"after-clustering",children:[],level:3}],level:2},{value:"Summary",id:"summary",children:[],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...o}=e;return(0,n.yg)(p,(0,i.A)({},c,o,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("h2",{id:"background"},"Background"),(0,n.yg)("p",null,"Apache Hudi brings stream processing to big data, providing fresh data while being an order of magnitude efficient over traditional batch processing. In a data lake/warehouse, one of the key trade-offs is between ingestion speed and query performance. Data ingestion typically prefers small files to improve parallelism and make data available to queries as soon as possible. However, query performance degrades poorly with a lot of small files. Also, during ingestion, data is typically co-located based on arrival time. However, the query engines perform better when the data frequently queried is co-located together. In most architectures each of these systems tend to add optimizations independently to improve performance which hits limitations due to un-optimized data layouts. This blog introduces a new kind of table service called clustering ",(0,n.yg)("a",{parentName:"p",href:"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+19+Clustering+data+for+freshness+and+query+performance"},"[RFC-19]")," to reorganize data for improved query performance without compromising on ingestion speed."),(0,n.yg)("h2",{id:"clustering-architecture"},"Clustering Architecture"),(0,n.yg)("p",null,"At a high level, Hudi provides different operations such as insert/upsert/bulk_insert through it\u2019s write client API to be able to write data to a Hudi table. To be able to choose a trade-off between file size and ingestion speed, Hudi provides a knob ",(0,n.yg)("inlineCode",{parentName:"p"},"hoodie.parquet.small.file.limit")," to be able to configure the smallest allowable file size. Users are able to configure the small file ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/configurations#compactionSmallFileSize"},"soft limit")," to ",(0,n.yg)("inlineCode",{parentName:"p"},"0")," to force new data to go into a new set of filegroups or set it to a higher value to ensure new data gets \u201cpadded\u201d to existing files until it meets that limit that adds to ingestion latencies."),(0,n.yg)("p",null,"To be able to support an architecture that allows for fast ingestion without compromising query performance, we have introduced a \u2018clustering\u2019 service to rewrite the data to optimize Hudi data lake file layout."),(0,n.yg)("p",null,"Clustering table service can run asynchronously or synchronously adding a new action type called \u201cREPLACE\u201d, that will mark the clustering action in the Hudi metadata timeline."),(0,n.yg)("h4",{id:"overall-there-are-2-parts-to-clustering"},"Overall, there are 2 parts to clustering"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Scheduling clustering: Create a clustering plan using a pluggable clustering strategy."),(0,n.yg)("li",{parentName:"ol"},"Execute clustering: Process the plan using an execution strategy to create new files and replace old files.")),(0,n.yg)("h4",{id:"scheduling-clustering"},"Scheduling clustering"),(0,n.yg)("p",null,"Following steps are followed to schedule clustering."),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Identify files that are eligible for clustering: Depending on the clustering strategy chosen, the scheduling logic will identify the files eligible for clustering."),(0,n.yg)("li",{parentName:"ol"},"Group files that are eligible for clustering based on specific criteria. Each group is expected to have data size in multiples of \u2018targetFileSize\u2019. Grouping is done as part of \u2018strategy\u2019 defined in the plan. Additionally, there is an option to put a cap on group size to improve parallelism and avoid shuffling large amounts of data."),(0,n.yg)("li",{parentName:"ol"},"Finally, the clustering plan is saved to the timeline in an avro ",(0,n.yg)("a",{parentName:"li",href:"https://github.com/apache/hudi/blob/master/hudi-common/src/main/avro/HoodieClusteringPlan.avsc"},"metadata format"),".")),(0,n.yg)("h4",{id:"running-clustering"},"Running clustering"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Read the clustering plan and get the \u2018clusteringGroups\u2019 that mark the file groups that need to be clustered."),(0,n.yg)("li",{parentName:"ol"},"For each group, we instantiate appropriate strategy class with strategyParams (example: sortColumns) and apply that strategy to rewrite the data."),(0,n.yg)("li",{parentName:"ol"},"Create a \u201cREPLACE\u201d commit and update the metadata in ",(0,n.yg)("a",{parentName:"li",href:"https://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceCommitMetadata.java"},"HoodieReplaceCommitMetadata"),".")),(0,n.yg)("p",null,"Clustering Service builds on Hudi\u2019s MVCC based design to allow for writers to continue to insert new data while clustering action runs in the background to reformat data layout, ensuring snapshot isolation between concurrent readers and writers."),(0,n.yg)("p",null,"NOTE: Clustering can only be scheduled for tables / partitions not receiving any concurrent updates. In the future, concurrent updates use-case will be supported as well."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Clustering example",src:t(51204).A}),"\n",(0,n.yg)("em",{parentName:"p"},"Figure: Illustrating query performance improvements by clustering")),(0,n.yg)("h4",{id:"setting-up-clustering"},"Setting up clustering"),(0,n.yg)("p",null,"Inline clustering can be setup easily using spark dataframe options. See sample below"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-scala"},'import org.apache.hudi.QuickstartUtils._\nimport scala.collection.JavaConversions._\nimport org.apache.spark.sql.SaveMode._\nimport org.apache.hudi.DataSourceReadOptions._\nimport org.apache.hudi.DataSourceWriteOptions._\nimport org.apache.hudi.config.HoodieWriteConfig._\n\n\nval df =  //generate data frame\ndf.write.format("org.apache.hudi").\n        options(getQuickstartWriteConfigs).\n        option(PRECOMBINE_FIELD_OPT_KEY, "ts").\n        option(RECORDKEY_FIELD_OPT_KEY, "uuid").\n        option(PARTITIONPATH_FIELD_OPT_KEY, "partitionpath").\n        option(TABLE_NAME, "tableName").\n        option("hoodie.parquet.small.file.limit", "0").\n        option("hoodie.clustering.inline", "true").\n        option("hoodie.clustering.inline.max.commits", "4").\n        option("hoodie.clustering.plan.strategy.target.file.max.bytes", "1073741824").\n        option("hoodie.clustering.plan.strategy.small.file.limit", "629145600").\n        option("hoodie.clustering.plan.strategy.sort.columns", "column1,column2"). //optional, if sorting is needed as part of rewriting data\n        mode(Append).\n        save("dfs://location");\n')),(0,n.yg)("p",null,"For more advanced usecases, async clustering pipeline can also be setup. See an example ",(0,n.yg)("a",{parentName:"p",href:"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+19+Clustering+data+for+freshness+and+query+performance#RFC19Clusteringdataforfreshnessandqueryperformance-SetupforAsyncclusteringJob"},"here"),"."),(0,n.yg)("h2",{id:"table-query-performance"},"Table Query Performance"),(0,n.yg)("p",null,"We created a dataset from one partition of a known production style table with ~20M records and on-disk size of ~200GB. The dataset has rows for multiple \u201csessions\u201d. Users always query this data using a predicate on session. Data for a single session is spread across multiple data files because ingestion groups data based on arrival time. The below experiment shows that by clustering on session, we are able to improve the data locality and reduce query execution time by more than 50%."),(0,n.yg)("p",null,"Query: "),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-scala"},'spark.sql("select  *  from table where session_id=123")\n')),(0,n.yg)("h3",{id:"before-clustering"},"Before Clustering"),(0,n.yg)("p",null,"Query took 2.2 minutes to complete. Note that the number of output rows in the \u201cscan parquet\u201d part of the query plan includes all 20M rows in the table."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Query Plan Before Clustering",src:t(81232).A}),"\n",(0,n.yg)("em",{parentName:"p"},"Figure: Spark SQL query details before clustering")),(0,n.yg)("h3",{id:"after-clustering"},"After Clustering"),(0,n.yg)("p",null,"The query plan is similar to above. But, because of improved data locality and predicate push down, spark is able to prune a lot of rows. After clustering, the same query only outputs 110K rows (out of 20M rows) while scanning parquet files. This cuts query time to less than a minute from 2.2 minutes."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Query Plan Before Clustering",src:t(41355).A}),"\n",(0,n.yg)("em",{parentName:"p"},"Figure: Spark SQL query details after clustering")),(0,n.yg)("p",null,"The table below summarizes query performance improvements from experiments run using Spark3"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Table State"),(0,n.yg)("th",{parentName:"tr",align:null},"Query runtime"),(0,n.yg)("th",{parentName:"tr",align:null},"Num Records Processed"),(0,n.yg)("th",{parentName:"tr",align:null},"Num files on disk"),(0,n.yg)("th",{parentName:"tr",align:null},"Size of each file"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("strong",{parentName:"td"},"Unclustered")),(0,n.yg)("td",{parentName:"tr",align:null},"130,673 ms"),(0,n.yg)("td",{parentName:"tr",align:null},"~20M"),(0,n.yg)("td",{parentName:"tr",align:null},"13642"),(0,n.yg)("td",{parentName:"tr",align:null},"~150 MB")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("strong",{parentName:"td"},"Clustered")),(0,n.yg)("td",{parentName:"tr",align:null},"55,963 ms"),(0,n.yg)("td",{parentName:"tr",align:null},"~110K"),(0,n.yg)("td",{parentName:"tr",align:null},"294"),(0,n.yg)("td",{parentName:"tr",align:null},"~600 MB")))),(0,n.yg)("p",null,"Query runtime is reduced by 60% after clustering. Similar results were observed on other sample datasets. See example query plans and more details at the ",(0,n.yg)("a",{parentName:"p",href:"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+19+Clustering+data+for+freshness+and+query+performance#RFC19Clusteringdataforfreshnessandqueryperformance-PerformanceEvaluation"},"RFC-19 performance evaluation"),"."),(0,n.yg)("p",null,"We expect dramatic speedup for large tables, where the query runtime is almost entirely dominated by actual I/O and not query planning, unlike the example above."),(0,n.yg)("h2",{id:"summary"},"Summary"),(0,n.yg)("p",null,"Using clustering, we can improve query performance by"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Leveraging concepts such as ",(0,n.yg)("a",{parentName:"li",href:"https://en.wikipedia.org/wiki/Z-order_curve"},"space filling curves")," to adapt data lake layout and reduce the amount of data read during queries."),(0,n.yg)("li",{parentName:"ol"},"Stitch small files into larger ones and reduce the total number of files that need to be scanned by the query engine.")),(0,n.yg)("p",null,"Clustering also enables stream processing over big data. Ingestion can write small files to satisfy latency requirements of stream processing. Clustering can be used in the background to stitch these small files into larger files and reduce file count."),(0,n.yg)("p",null,"Besides this, the clustering framework also provides the flexibility to asynchronously rewrite data based on specific requirements. We foresee many other use-cases adopting clustering framework with custom pluggable strategies to satisfy on-demand data lake management activities. Some such notable use-cases that are actively being solved using clustering:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Rewrite data and encrypt data at rest."),(0,n.yg)("li",{parentName:"ol"},"Prune unused columns from tables and reduce storage footprint.")))}g.isMDXComponent=!0},18058:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Apache Hudi Key Generators",excerpt:"Different key generators available with Apache Hudi",author:"shivnarayan",category:"blog",tags:["blog","key generators","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2021/02/13/hudi-key-generators",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-02-13-hudi-key-generators.md",source:"@site/blog/2021-02-13-hudi-key-generators.md",title:"Apache Hudi Key Generators",description:"Every record in Hudi is uniquely identified by a primary key, which is a pair of record key and partition path where",date:"2021-02-13T00:00:00.000Z",formattedDate:"February 13, 2021",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"key generators",permalink:"/cn/blog/tags/key-generators"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:5.855,truncated:!0,authors:[{name:"shivnarayan"}],prevItem:{title:"Time travel operations in Hopsworks Feature Store",permalink:"/cn/blog/2021/02/24/Time-travel-operations-in-Hopsworks-Feature-Store"},nextItem:{title:"Optimize Data lake layout using Clustering in Apache Hudi",permalink:"/cn/blog/2021/01/27/hudi-clustering-intro"}},l={authorsImageUrls:[void 0]},d=[{value:"Key Generators",id:"key-generators",children:[{value:"SimpleKeyGenerator",id:"simplekeygenerator",children:[],level:3},{value:"ComplexKeyGenerator",id:"complexkeygenerator",children:[],level:3},{value:"GlobalDeleteKeyGenerator",id:"globaldeletekeygenerator",children:[],level:3},{value:"TimestampBasedKeyGenerator",id:"timestampbasedkeygenerator",children:[{value:"Timestamp is GMT",id:"timestamp-is-gmt",children:[],level:4},{value:"Timestamp is DATE_STRING",id:"timestamp-is-date_string",children:[],level:4},{value:"Scalar examples",id:"scalar-examples",children:[],level:4},{value:"ISO8601WithMsZ with Single Input format",id:"iso8601withmsz-with-single-input-format",children:[],level:4},{value:"ISO8601WithMsZ with Multiple Input formats",id:"iso8601withmsz-with-multiple-input-formats",children:[],level:4},{value:"ISO8601NoMs with offset using multiple input formats",id:"iso8601noms-with-offset-using-multiple-input-formats",children:[],level:4},{value:"Input as short date string and expect date in date format",id:"input-as-short-date-string-and-expect-date-in-date-format",children:[],level:4}],level:3},{value:"CustomKeyGenerator",id:"customkeygenerator",children:[],level:3},{value:"NonpartitionedKeyGenerator",id:"nonpartitionedkeygenerator",children:[],level:3}],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...t}=e;return(0,n.yg)(p,(0,i.A)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"Every record in Hudi is uniquely identified by a primary key, which is a pair of record key and partition path where\nthe record belongs to. Using primary keys, Hudi can impose a) partition level uniqueness integrity constraint\nb) enable fast updates and deletes on records. One should choose the partitioning scheme wisely as it could be a\ndetermining factor for your ingestion and query latency."),(0,n.yg)("p",null,"In general, Hudi supports both partitioned and global indexes. For a dataset with partitioned index(which is most\ncommonly used), each record is uniquely identified by a pair of record key and partition path. But for a dataset with\nglobal index, each record is uniquely identified by just the record key. There won't be any duplicate record keys across\npartitions."),(0,n.yg)("h2",{id:"key-generators"},"Key Generators"),(0,n.yg)("p",null,"Hudi provides several key generators out of the box that users can use based on their need, while having a pluggable\nimplementation for users to implement and use their own KeyGenerator. This blog goes over all different types of key\ngenerators that are readily available to use."),(0,n.yg)("p",null,(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/master/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/KeyGenerator.java"},"Here"),"\nis the interface for KeyGenerator in Hudi for your reference."),(0,n.yg)("p",null,"Before diving into different types of key generators, let\u2019s go over some of the common configs required to be set for\nkey generators."),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Config"),(0,n.yg)("th",{parentName:"tr",align:"center"},"Meaning/purpose"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.datasource.write.recordkey.field")),(0,n.yg)("td",{parentName:"tr",align:"center"},"Refers to record key field. This is a mandatory field.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.datasource.write.partitionpath.field")),(0,n.yg)("td",{parentName:"tr",align:"center"},"Refers to partition path field. This is a mandatory field.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.datasource.write.keygenerator.class")),(0,n.yg)("td",{parentName:"tr",align:"center"},"Refers to Key generator class(including full path). Could refer to any of the available ones or user defined one. This is a mandatory field.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.datasource.write.partitionpath.urlencode")),(0,n.yg)("td",{parentName:"tr",align:"center"},"When set to true, partition path will be url encoded. Default value is false.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.datasource.write.hive_style_partitioning")),(0,n.yg)("td",{parentName:"tr",align:"center"},"When set to true, uses hive style partitioning. Partition field name will be prefixed to the value. Format: \u201c<partition_path_field_name>=<partition_path_value>\u201d. Default value is false.")))),(0,n.yg)("p",null,"NOTE:\nPlease use ",(0,n.yg)("inlineCode",{parentName:"p"},"hoodie.datasource.write.keygenerator.class")," instead of ",(0,n.yg)("inlineCode",{parentName:"p"},"hoodie.datasource.write.keygenerator.type"),". The second config was introduced more recently.\nand will internally instantiate the correct KeyGenerator class based on the type name. The second one is intended for ease of use and is being actively worked on.\nWe still recommend using the first config until it is marked as deprecated."),(0,n.yg)("p",null,"There are few more configs involved if you are looking for TimestampBasedKeyGenerator. Will cover those in the respective section."),(0,n.yg)("p",null,"Lets go over different key generators available to be used with Hudi."),(0,n.yg)("h3",{id:"simplekeygenerator"},(0,n.yg)("a",{parentName:"h3",href:"https://github.com/apache/hudi/blob/master/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/SimpleKeyGenerator.java"},"SimpleKeyGenerator")),(0,n.yg)("p",null,"Record key refers to one field(column in dataframe) by name and partition path refers to one field (single column in dataframe)\nby name. This is one of the most commonly used one. Values are interpreted as is from dataframe and converted to string."),(0,n.yg)("h3",{id:"complexkeygenerator"},(0,n.yg)("a",{parentName:"h3",href:"https://github.com/apache/hudi/blob/master/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/ComplexKeyGenerator.java"},"ComplexKeyGenerator")),(0,n.yg)("p",null,"Both record key and partition paths comprise one or more than one field by name(combination of multiple fields). Fields\nare expected to be comma separated in the config value. For example ",(0,n.yg)("inlineCode",{parentName:"p"},'"Hoodie.datasource.write.recordkey.field" : \u201ccol1,col4\u201d')),(0,n.yg)("h3",{id:"globaldeletekeygenerator"},(0,n.yg)("a",{parentName:"h3",href:"https://github.com/apache/hudi/blob/master/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/GlobalDeleteKeyGenerator.java"},"GlobalDeleteKeyGenerator")),(0,n.yg)("p",null,"Global index deletes do not require partition value. So this key generator avoids using partition value for generating HoodieKey."),(0,n.yg)("h3",{id:"timestampbasedkeygenerator"},(0,n.yg)("a",{parentName:"h3",href:"https://github.com/apache/hudi/blob/master/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/TimestampBasedKeyGenerator.java"},"TimestampBasedKeyGenerator")),(0,n.yg)("p",null,"This key generator relies on timestamps for the partition field. The field values are interpreted as timestamps\nand not just converted to string while generating partition path value for records.  Record key is same as before where it is chosen by\nfield name.  Users are expected to set few more configs to use this KeyGenerator."),(0,n.yg)("p",null,"Configs to be set:"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Config"),(0,n.yg)("th",{parentName:"tr",align:null},"Meaning/purpose"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.timestamp.type")),(0,n.yg)("td",{parentName:"tr",align:null},"One of the timestamp types supported(UNIX_TIMESTAMP, DATE_STRING, MIXED, EPOCHMILLISECONDS, SCALAR)")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.output.dateformat")),(0,n.yg)("td",{parentName:"tr",align:null},"Output date format")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.timezone")),(0,n.yg)("td",{parentName:"tr",align:null},"Timezone of the data format")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"oodie.deltastreamer.keygen.timebased.input.dateformat")),(0,n.yg)("td",{parentName:"tr",align:null},"Input date format")))),(0,n.yg)("p",null,"Let's go over some example values for TimestampBasedKeyGenerator."),(0,n.yg)("h4",{id:"timestamp-is-gmt"},"Timestamp is GMT"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Config field"),(0,n.yg)("th",{parentName:"tr",align:null},"Value"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.timestamp.type")),(0,n.yg)("td",{parentName:"tr",align:null},'"EPOCHMILLISECONDS"')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.output.dateformat")),(0,n.yg)("td",{parentName:"tr",align:null},'"yyyy-MM-dd hh"')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.timezone")),(0,n.yg)("td",{parentName:"tr",align:null},'"GMT+8:00"')))),(0,n.yg)("p",null,"Input Field value: \u201c1578283932000L\u201d ",(0,n.yg)("br",null),"\nPartition path generated from key generator: \u201c2020-01-06 12\u201d"),(0,n.yg)("p",null,"If input field value is null for some rows. ",(0,n.yg)("br",null),"\nPartition path generated from key generator: \u201c1970-01-01 08\u201d"),(0,n.yg)("h4",{id:"timestamp-is-date_string"},"Timestamp is DATE_STRING"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Config field"),(0,n.yg)("th",{parentName:"tr",align:null},"Value"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.timestamp.type")),(0,n.yg)("td",{parentName:"tr",align:null},'"DATE_STRING"')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.output.dateformat")),(0,n.yg)("td",{parentName:"tr",align:null},'"yyyy-MM-dd hh"')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.timezone")),(0,n.yg)("td",{parentName:"tr",align:null},'"GMT+8:00"')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.input.dateformat")),(0,n.yg)("td",{parentName:"tr",align:null},'"yyyy-MM-dd hh:mm:ss"')))),(0,n.yg)("p",null,"Input field value: \u201c2020-01-06 12:12:12\u201d ",(0,n.yg)("br",null),"\nPartition path generated from key generator: \u201c2020-01-06 12\u201d"),(0,n.yg)("p",null,"If input field value is null for some rows. ",(0,n.yg)("br",null),"\nPartition path generated from key generator: \u201c1970-01-01 12:00:00\u201d"),(0,n.yg)("br",null),(0,n.yg)("h4",{id:"scalar-examples"},"Scalar examples"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Config field"),(0,n.yg)("th",{parentName:"tr",align:null},"Value"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.timestamp.type")),(0,n.yg)("td",{parentName:"tr",align:null},'"SCALAR"')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.output.dateformat")),(0,n.yg)("td",{parentName:"tr",align:null},'"yyyy-MM-dd hh"')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.timezone")),(0,n.yg)("td",{parentName:"tr",align:null},'"GMT"')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.timestamp.scalar.time.unit")),(0,n.yg)("td",{parentName:"tr",align:null},'"days"')))),(0,n.yg)("p",null,"Input field value: \u201c20000L\u201d ",(0,n.yg)("br",null),"\nPartition path generated from key generator: \u201c2024-10-04 12\u201d"),(0,n.yg)("p",null,"If input field value is null. ",(0,n.yg)("br",null),"\nPartition path generated from key generator: \u201c1970-01-02 12\u201d"),(0,n.yg)("h4",{id:"iso8601withmsz-with-single-input-format"},"ISO8601WithMsZ with Single Input format"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Config field"),(0,n.yg)("th",{parentName:"tr",align:null},"Value"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.timestamp.type")),(0,n.yg)("td",{parentName:"tr",align:null},'"DATE_STRING"')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.input.dateformat")),(0,n.yg)("td",{parentName:"tr",align:null},"\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.input.dateformat.list.delimiter.regex")),(0,n.yg)("td",{parentName:"tr",align:null},'""')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.input.timezone")),(0,n.yg)("td",{parentName:"tr",align:null},'""')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.output.dateformat")),(0,n.yg)("td",{parentName:"tr",align:null},'"yyyyMMddHH"')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.output.timezone")),(0,n.yg)("td",{parentName:"tr",align:null},'"GMT"')))),(0,n.yg)("p",null,'Input field value: "2020-04-01T13:01:33.428Z" ',(0,n.yg)("br",null),'\nPartition path generated from key generator: "2020040113"'),(0,n.yg)("h4",{id:"iso8601withmsz-with-multiple-input-formats"},"ISO8601WithMsZ with Multiple Input formats"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Config field"),(0,n.yg)("th",{parentName:"tr",align:null},"Value"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.timestamp.type")),(0,n.yg)("td",{parentName:"tr",align:null},'"DATE_STRING"')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.input.dateformat")),(0,n.yg)("td",{parentName:"tr",align:null},"\"yyyy-MM-dd'T'HH:mm:ssZ,yyyy-MM-dd'T'HH:mm:ss.SSSZ\"")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.input.dateformat.list.delimiter.regex")),(0,n.yg)("td",{parentName:"tr",align:null},'""')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.input.timezone")),(0,n.yg)("td",{parentName:"tr",align:null},'""')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.output.dateformat")),(0,n.yg)("td",{parentName:"tr",align:null},'"yyyyMMddHH"')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.output.timezone")),(0,n.yg)("td",{parentName:"tr",align:null},'"UTC"')))),(0,n.yg)("p",null,'Input field value: "2020-04-01T13:01:33.428Z" ',(0,n.yg)("br",null),'\nPartition path generated from key generator: "2020040113"'),(0,n.yg)("h4",{id:"iso8601noms-with-offset-using-multiple-input-formats"},"ISO8601NoMs with offset using multiple input formats"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Config field"),(0,n.yg)("th",{parentName:"tr",align:null},"Value"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.timestamp.type")),(0,n.yg)("td",{parentName:"tr",align:null},'"DATE_STRING"')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.input.dateformat")),(0,n.yg)("td",{parentName:"tr",align:null},"\"yyyy-MM-dd'T'HH:mm:ssZ,yyyy-MM-dd'T'HH:mm:ss.SSSZ\"")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.input.dateformat.list.delimiter.regex")),(0,n.yg)("td",{parentName:"tr",align:null},'""')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.input.timezone")),(0,n.yg)("td",{parentName:"tr",align:null},'""')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.output.dateformat")),(0,n.yg)("td",{parentName:"tr",align:null},'"yyyyMMddHH"')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.output.timezone")),(0,n.yg)("td",{parentName:"tr",align:null},'"UTC"')))),(0,n.yg)("p",null,'Input field value: "2020-04-01T13:01:33-',(0,n.yg)("strong",{parentName:"p"},"05:00"),'" ',(0,n.yg)("br",null),'\nPartition path generated from key generator: "2020040118"'),(0,n.yg)("h4",{id:"input-as-short-date-string-and-expect-date-in-date-format"},"Input as short date string and expect date in date format"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Config field"),(0,n.yg)("th",{parentName:"tr",align:null},"Value"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.timestamp.type")),(0,n.yg)("td",{parentName:"tr",align:null},'"DATE_STRING"')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.input.dateformat")),(0,n.yg)("td",{parentName:"tr",align:null},"\"yyyy-MM-dd'T'HH:mm:ssZ,yyyy-MM-dd'T'HH:mm:ss.SSSZ,yyyyMMdd\"")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.input.dateformat.list.delimiter.regex")),(0,n.yg)("td",{parentName:"tr",align:null},'""')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.input.timezone")),(0,n.yg)("td",{parentName:"tr",align:null},'"UTC"')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.output.dateformat")),(0,n.yg)("td",{parentName:"tr",align:null},'"MM/dd/yyyy"')),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.deltastreamer.keygen.timebased.output.timezone")),(0,n.yg)("td",{parentName:"tr",align:null},'"UTC"')))),(0,n.yg)("p",null,'Input field value: "220200401" ',(0,n.yg)("br",null),'\nPartition path generated from key generator: "04/01/2020"'),(0,n.yg)("h3",{id:"customkeygenerator"},(0,n.yg)("a",{parentName:"h3",href:"https://github.com/apache/hudi/blob/master/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/CustomKeyGenerator.java"},"CustomKeyGenerator")),(0,n.yg)("p",null,"This is a generic implementation of KeyGenerator where users are able to leverage the benefits of SimpleKeyGenerator,\nComplexKeyGenerator and TimestampBasedKeyGenerator all at the same time. One can configure record key and partition\npaths as a single field or a combination of fields. This keyGenerator is particularly useful if you want to define\ncomplex partition paths involving regular fields and timestamp based fields. It expects value for prop ",(0,n.yg)("inlineCode",{parentName:"p"},'"hoodie.datasource.write.partitionpath.field"'),'\nin a specific format. The format should be "field1:PartitionKeyType1,field2:PartitionKeyType2..."'),(0,n.yg)("p",null,"The complete partition path is created as\n",(0,n.yg)("inlineCode",{parentName:"p"},"<value for field1 basis PartitionKeyType1>/<value for field2 basis PartitionKeyType2> "),"\nand so on. Each partition key type could either be SIMPLE or TIMESTAMP."),(0,n.yg)("p",null,"Example config value: ",(0,n.yg)("inlineCode",{parentName:"p"},"\u201cfield_3:simple,field_5:timestamp\u201d")),(0,n.yg)("p",null,"RecordKey config value is either single field incase of SimpleKeyGenerator or a comma separate field names if referring to ComplexKeyGenerator.\nEg: \u201ccol1\u201d or \u201ccol3,col4\u201d."),(0,n.yg)("h3",{id:"nonpartitionedkeygenerator"},(0,n.yg)("a",{parentName:"h3",href:"https://github.com/apache/hudi/blob/master/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/NonpartitionedKeyGenerator.java"},"NonpartitionedKeyGenerator")),(0,n.yg)("p",null,"If your hudi dataset is not partitioned, you could use this \u201cNonpartitionedKeyGenerator\u201d which will return an empty\npartition for all records. In other words, all records go to the same partition (which is empty \u201c\u201d) "),(0,n.yg)("p",null,"Hope this blog gave you a good understanding of different types of Key Generators available in Apache Hudi. Thanks for your continued support for Hudi's community."))}g.isMDXComponent=!0},42340:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Time travel operations in Hopsworks Feature Store",category:"blog",image:"/assets/images/blog/2021-02-24-featurestore_incremental_pull.png",tags:["use-case","incremental processing","feature store","time travel query","hopsworks"]},s=void 0,l={permalink:"/cn/blog/2021/02/24/Time-travel-operations-in-Hopsworks-Feature-Store",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-02-24-Time-travel-operations-in-Hopsworks-Feature-Store.mdx",source:"@site/blog/2021-02-24-Time-travel-operations-in-Hopsworks-Feature-Store.mdx",title:"Time travel operations in Hopsworks Feature Store",description:"Redirecting... please wait!!",date:"2021-02-24T00:00:00.000Z",formattedDate:"February 24, 2021",tags:[{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"incremental processing",permalink:"/cn/blog/tags/incremental-processing"},{label:"feature store",permalink:"/cn/blog/tags/feature-store"},{label:"time travel query",permalink:"/cn/blog/tags/time-travel-query"},{label:"hopsworks",permalink:"/cn/blog/tags/hopsworks"}],readingTime:.045,truncated:!1,authors:[],prevItem:{title:"Data Lakehouse: Building the Next Generation of Data Lakes using Apache Hudi",permalink:"/cn/blog/2021/03/01/Data-Lakehouse-Building-the-Next-Generation-of-Data-Lakes-using-Apache-Hudi"},nextItem:{title:"Apache Hudi Key Generators",permalink:"/cn/blog/2021/02/13/hudi-key-generators"}},d={authorsImageUrls:[]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://examples.hopsworks.ai/master/featurestore/hsfs/time_travel/time_travel_scala/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},50836:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Data Lakehouse: Building the Next Generation of Data Lakes using Apache Hudi",authors:[{name:"Ryan D'Souza"},{name:"Brandon Stanley"}],category:"blog",image:"/assets/images/blog/2021-03-01-Data-Lakehouse-Building-the-Next-Generation-of-Data-Lakes-using-Apache-Hudi.png",tags:["blog","data-lakehouse","medium"]},s=void 0,l={permalink:"/cn/blog/2021/03/01/Data-Lakehouse-Building-the-Next-Generation-of-Data-Lakes-using-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-03-01-Data-Lakehouse-Building-the-Next-Generation-of-Data-Lakes-using-Apache-Hudi.mdx",source:"@site/blog/2021-03-01-Data-Lakehouse-Building-the-Next-Generation-of-Data-Lakes-using-Apache-Hudi.mdx",title:"Data Lakehouse: Building the Next Generation of Data Lakes using Apache Hudi",description:"Redirecting... please wait!!",date:"2021-03-01T00:00:00.000Z",formattedDate:"March 1, 2021",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"data-lakehouse",permalink:"/cn/blog/tags/data-lakehouse"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Ryan D'Souza"},{name:"Brandon Stanley"}],prevItem:{title:"Streaming Responsibly - How Apache Hudi maintains optimum sized files",permalink:"/cn/blog/2021/03/01/hudi-file-sizing"},nextItem:{title:"Time travel operations in Hopsworks Feature Store",permalink:"/cn/blog/2021/02/24/Time-travel-operations-in-Hopsworks-Feature-Store"}},d={authorsImageUrls:[void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/slalom-build/data-lakehouse-building-the-next-generation-of-data-lakes-using-apache-hudi-41550f62f5f",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},77713:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Streaming Responsibly - How Apache Hudi maintains optimum sized files",excerpt:"Maintaining well-sized files can improve query performance significantly",author:"shivnarayan",category:"blog",image:"/assets/images/blog/2021-03-01-hudi-file-sizing.png",tags:["design","file sizing","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2021/03/01/hudi-file-sizing",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-03-01-hudi-file-sizing.md",source:"@site/blog/2021-03-01-hudi-file-sizing.md",title:"Streaming Responsibly - How Apache Hudi maintains optimum sized files",description:"Apache Hudi is a data lake platform technology that provides several functionalities needed to build and manage data lakes.",date:"2021-03-01T00:00:00.000Z",formattedDate:"March 1, 2021",tags:[{label:"design",permalink:"/cn/blog/tags/design"},{label:"file sizing",permalink:"/cn/blog/tags/file-sizing"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:4.33,truncated:!0,authors:[{name:"shivnarayan"}],prevItem:{title:"Build a data lake using amazon kinesis data stream for amazon dynamodb and apache hudi",permalink:"/cn/blog/2021/03/04/Build-a-data-lake-using-amazon-kinesis-data-stream-for-amazon-dynamodb-and-apache-hudi"},nextItem:{title:"Data Lakehouse: Building the Next Generation of Data Lakes using Apache Hudi",permalink:"/cn/blog/2021/03/01/Data-Lakehouse-Building-the-Next-Generation-of-Data-Lakes-using-Apache-Hudi"}},l={authorsImageUrls:[void 0]},d=[{value:"During Write vs After Write",id:"during-write-vs-after-write",children:[{value:"Configs",id:"configs",children:[],level:3},{value:"Example",id:"example",children:[],level:3}],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...o}=e;return(0,n.yg)(p,(0,i.A)({},c,o,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"Apache Hudi is a data lake platform technology that provides several functionalities needed to build and manage data lakes.\nOne such key feature that hudi provides is self-managing file sizing so that users don\u2019t need to worry about\nmanual table maintenance. Having a lot of small files will make it harder to achieve good query performance, due to query engines\nhaving to open/read/close files way too many times, to plan and execute queries. But for streaming data lake use-cases,\ninherently ingests are going to end up having smaller volume of writes, which might result in lot of small files if no special handling is done."),(0,n.yg)("h2",{id:"during-write-vs-after-write"},"During Write vs After Write"),(0,n.yg)("p",null,"Common approaches to writing very small files and then later stitching them together solve for system scalability issues posed\nby small files but might violate query SLA's by exposing small files to them. In fact, you can easily do so on a Hudi table,\nby running a clustering operation, as detailed in a ",(0,n.yg)("a",{parentName:"p",href:"/blog/2021/01/27/hudi-clustering-intro"},"previous blog"),". "),(0,n.yg)("p",null,"In this blog, we discuss file sizing optimizations in Hudi, during the initial write time, so we don't have to effectively\nre-write all data again, just for file sizing. If you want to have both (a) self managed file sizing and\n(b) Avoid exposing small files to queries, automatic file sizing feature saves the day."),(0,n.yg)("p",null,"Hudi has the ability to maintain a configured target file size, when performing inserts/upsert operations.\n(Note: bulk_insert operation does not provide this functionality and is designed as a simpler replacement for\nnormal ",(0,n.yg)("inlineCode",{parentName:"p"},"spark.write.parquet"),")."),(0,n.yg)("h3",{id:"configs"},"Configs"),(0,n.yg)("p",null,"For illustration purposes, we are going to consider only COPY_ON_WRITE table."),(0,n.yg)("p",null,"Configs of interest before we dive into the algorithm:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"/docs/configurations#limitFileSize"},"Max file size"),": Max size for a given data file. Hudi will try to maintain file sizes to this configured value ",(0,n.yg)("br",null)),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"/docs/configurations#compactionSmallFileSize"},"Soft file limit"),": Max file size below which a given data file is considered to a small file ",(0,n.yg)("br",null)),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"/docs/configurations#insertSplitSize"},"Insert split size"),": Number of inserts grouped for a single partition. This value should match\nthe number of records in a single file (you can determine based on max file size and per record size)")),(0,n.yg)("p",null,"For instance, if your first config value is 120MB and 2nd config value is set to 100MB, any file whose size is < 100MB\nwould be considered a small file."),(0,n.yg)("p",null,"If you wish to turn off this feature, set the config value for soft file limit to 0."),(0,n.yg)("h3",{id:"example"},"Example"),(0,n.yg)("p",null,"Let\u2019s say this is the layout of data files for a given partition."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Initial layout",src:t(65304).A}),"\n",(0,n.yg)("em",{parentName:"p"},"Figure: Initial data file sizes for a given partition of interest")),(0,n.yg)("p",null,"Let\u2019s assume the configured values for max file size and small file size limit are 120MB and 100MB. File_1\u2019s current\nsize is 40MB, File_2\u2019s size is 80MB, File_3\u2019s size is 90MB, File_4\u2019s size is 130MB and File_5\u2019s size is 105MB. Let\u2019s see\nwhat happens when a new write happens. "),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Step 1:")," Assigning updates to files. In this step, We look up the index to find the tagged location and records are\nassigned to respective files. Note that we assume updates are only going to increase the file size and that would simply result\nin a much bigger file. When updates lower the file size (by say, nulling out lot of fields), then a subsequent write will deem\nit a small file."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Step 2:"),"  Determine small files for each partition path. The soft file limit config value will be leveraged here\nto determine eligible small files. In our example, given the config value is set to 100MB, the small files are File_1(40MB)\nand File_2(80MB) and file_3\u2019s (90MB)."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Step 3:")," Once small files are determined, incoming inserts are assigned to them so that they reach their max capacity of\n120MB. File_1 will be ingested with 80MB worth of inserts, file_2 will be ingested with 40MB worth of inserts and\nFile_3 will be ingested with 30MB worth of inserts."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Bin packing small files",src:t(55069).A}),"\n",(0,n.yg)("em",{parentName:"p"},"Figure: Incoming records are bin packed to existing small files")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Step 4:")," Once all small files are bin packed to its max capacity and if there are pending inserts unassigned, new file\ngroups/data files are created and inserts are assigned to them. Number of records per new data file is determined from insert split\nsize config. Assuming the insert split size is configured to 120k records, if there are 300k remaining records, 3 new\nfiles will be created in which 2 of them (File_6 and File_7) will be filled with 120k records and the last one (File_8)\nwill be filled with 60k records (assuming each record is 1000 bytes). In future ingestions, 3rd new file will be\nconsidered as a small file to be packed with more data."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Assigning to new files",src:t(17581).A}),"\n",(0,n.yg)("em",{parentName:"p"},"Figure: Remaining records are assigned to new files")),(0,n.yg)("p",null,"Hudi leverages mechanisms such as custom partitioning for optimized record distribution to different files, executing\nthe algorithm above. After this round of ingestion is complete, all files except File_8 are nicely sized to the optimum size.\nThis process is followed during every ingestion to ensure there are no small files in your Hudi tables. "),(0,n.yg)("p",null,"Hopefully the blog gave you an overview into how hudi manages small files and assists in boosting your query performance."))}g.isMDXComponent=!0},76030:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Build a data lake using amazon kinesis data stream for amazon dynamodb and apache hudi",authors:[{name:"Dhiraj Thakur"},{name:"Dylan Qu"},{name:"Saurabh Shrivastava"}],category:"blog",image:"/assets/images/blog/2021-03-04-build-data-lake-using-amazon-kinesis-for-amazon-dynamodb-and-apache-hudi.jpeg",tags:["how-to","streaming ingestion","amazon"]},s=void 0,l={permalink:"/cn/blog/2021/03/04/Build-a-data-lake-using-amazon-kinesis-data-stream-for-amazon-dynamodb-and-apache-hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-03-04-Build-a-data-lake-using-amazon-kinesis-data-stream-for-amazon-dynamodb-and-apache-hudi.mdx",source:"@site/blog/2021-03-04-Build-a-data-lake-using-amazon-kinesis-data-stream-for-amazon-dynamodb-and-apache-hudi.mdx",title:"Build a data lake using amazon kinesis data stream for amazon dynamodb and apache hudi",description:"Redirecting... please wait!!",date:"2021-03-04T00:00:00.000Z",formattedDate:"March 4, 2021",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"streaming ingestion",permalink:"/cn/blog/tags/streaming-ingestion"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[{name:"Dhiraj Thakur"},{name:"Dylan Qu"},{name:"Saurabh Shrivastava"}],prevItem:{title:"New features from Apache hudi in Amazon EMR",permalink:"/cn/blog/2021/03/11/New-features-from-Apache-hudi-in-Amazon-EMR"},nextItem:{title:"Streaming Responsibly - How Apache Hudi maintains optimum sized files",permalink:"/cn/blog/2021/03/01/hudi-file-sizing"}},d={authorsImageUrls:[void 0,void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/build-a-data-lake-using-amazon-kinesis-data-streams-for-amazon-dynamodb-and-apache-hudi/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},66447:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"New features from Apache hudi in Amazon EMR",authors:[{name:"Udit Mehrotra"}],category:"blog",image:"/assets/images/blog/aws.jpg",tags:["blog","amazon"]},s=void 0,l={permalink:"/cn/blog/2021/03/11/New-features-from-Apache-hudi-in-Amazon-EMR",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-03-11-New-features-from-Apache-hudi-in-Amazon-EMR.mdx",source:"@site/blog/2021-03-11-New-features-from-Apache-hudi-in-Amazon-EMR.mdx",title:"New features from Apache hudi in Amazon EMR",description:"Redirecting... please wait!!",date:"2021-03-11T00:00:00.000Z",formattedDate:"March 11, 2021",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[{name:"Udit Mehrotra"}],prevItem:{title:"Build Slowly Changing Dimensions Type 2 (SCD2) with Apache Spark and Apache Hudi on Amazon EMR",permalink:"/cn/blog/2021/04/12/Build-Slowly-Changing-Dimensions-Type-2-SCD2-with-Apache-Spark-and-Apache-Hudi-on-Amazon-EMR"},nextItem:{title:"Build a data lake using amazon kinesis data stream for amazon dynamodb and apache hudi",permalink:"/cn/blog/2021/03/04/Build-a-data-lake-using-amazon-kinesis-data-stream-for-amazon-dynamodb-and-apache-hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/new-features-from-apache-hudi-available-in-amazon-emr/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},17921:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Build Slowly Changing Dimensions Type 2 (SCD2) with Apache Spark and Apache Hudi on Amazon EMR",authors:[{name:"David Greenshtein"}],category:"blog",image:"/assets/images/blog/aws.jpg",tags:["how-to","scd2","amazon"]},s=void 0,l={permalink:"/cn/blog/2021/04/12/Build-Slowly-Changing-Dimensions-Type-2-SCD2-with-Apache-Spark-and-Apache-Hudi-on-Amazon-EMR",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-04-12-Build-Slowly-Changing-Dimensions-Type-2-SCD2-with-Apache-Spark-and-Apache-Hudi-on-Amazon-EMR.mdx",source:"@site/blog/2021-04-12-Build-Slowly-Changing-Dimensions-Type-2-SCD2-with-Apache-Spark-and-Apache-Hudi-on-Amazon-EMR.mdx",title:"Build Slowly Changing Dimensions Type 2 (SCD2) with Apache Spark and Apache Hudi on Amazon EMR",description:"Redirecting... please wait!!",date:"2021-04-12T00:00:00.000Z",formattedDate:"April 12, 2021",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"scd2",permalink:"/cn/blog/tags/scd-2"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[{name:"David Greenshtein"}],prevItem:{title:"Experts primer on Apache Hudi",permalink:"/cn/blog/2021/05/12/Experts-primer-on-Apache-Hudi"},nextItem:{title:"New features from Apache hudi in Amazon EMR",permalink:"/cn/blog/2021/03/11/New-features-from-Apache-hudi-in-Amazon-EMR"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/build-slowly-changing-dimensions-type-2-scd2-with-apache-spark-and-apache-hudi-on-amazon-emr/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},7074:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Experts primer on Apache Hudi",authors:[{name:"Stephanie Simone"}],category:"blog",image:"/assets/images/blog/data-summit-connect.jpeg",tags:["blog","dbta"]},s=void 0,l={permalink:"/cn/blog/2021/05/12/Experts-primer-on-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-05-12-Experts-primer-on-Apache-Hudi.mdx",source:"@site/blog/2021-05-12-Experts-primer-on-Apache-Hudi.mdx",title:"Experts primer on Apache Hudi",description:"Redirecting... please wait!!",date:"2021-05-12T00:00:00.000Z",formattedDate:"May 12, 2021",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"dbta",permalink:"/cn/blog/tags/dbta"}],readingTime:.045,truncated:!1,authors:[{name:"Stephanie Simone"}],prevItem:{title:"Apache Hudi: How Uber gets data a ride to its destination",permalink:"/cn/blog/2021/06/04/Apache-Hudi-How-Uber-gets-data-a-ride-to-its-destination"},nextItem:{title:"Build Slowly Changing Dimensions Type 2 (SCD2) with Apache Spark and Apache Hudi on Amazon EMR",permalink:"/cn/blog/2021/04/12/Build-Slowly-Changing-Dimensions-Type-2-SCD2-with-Apache-Spark-and-Apache-Hudi-on-Amazon-EMR"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.dbta.com/Editorial/News-Flashes/Experts-Present-a-Primer-on-Apache-Hudi-at-Data-Summit-Connect-2021-146834.aspx",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},71488:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Apache Hudi: How Uber gets data a ride to its destination",authors:[{name:"Joe McKendrick"}],category:"blog",tags:["blog","rtinsights"]},s=void 0,l={permalink:"/cn/blog/2021/06/04/Apache-Hudi-How-Uber-gets-data-a-ride-to-its-destination",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-06-04-Apache-Hudi-How-Uber-gets-data-a-ride-to-its-destination.mdx",source:"@site/blog/2021-06-04-Apache-Hudi-How-Uber-gets-data-a-ride-to-its-destination.mdx",title:"Apache Hudi: How Uber gets data a ride to its destination",description:"Redirecting... please wait!!",date:"2021-06-04T00:00:00.000Z",formattedDate:"June 4, 2021",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"rtinsights",permalink:"/cn/blog/tags/rtinsights"}],readingTime:.045,truncated:!1,authors:[{name:"Joe McKendrick"}],prevItem:{title:"Employing correct configurations for Hudi's cleaner table service",permalink:"/cn/blog/2021/06/10/employing-right-configurations-for-hudi-cleaner"},nextItem:{title:"Experts primer on Apache Hudi",permalink:"/cn/blog/2021/05/12/Experts-primer-on-Apache-Hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.rtinsights.com/apache-hudi-how-uber-gets-data-a-ride-to-its-destination/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},19135:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Employing correct configurations for Hudi's cleaner table service",excerpt:"Ensuring isolation between Hudi writers and readers using `HoodieCleaner.java`",author:"pratyakshsharma",category:"blog",image:"/assets/images/blog/hoodie-cleaner/Initial_timeline.png",tags:["how-to","cleaner","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2021/06/10/employing-right-configurations-for-hudi-cleaner",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-06-10-employing-right-configurations-for-hudi-cleaner.md",source:"@site/blog/2021-06-10-employing-right-configurations-for-hudi-cleaner.md",title:"Employing correct configurations for Hudi's cleaner table service",description:"Apache Hudi provides snapshot isolation between writers and readers. This is made possible by Hudi\u2019s MVCC concurrency model. In this blog, we will explain how to employ the right configurations to manage multiple file versions. Furthermore, we will discuss mechanisms available to users on how to maintain just the required number of old file versions so that long running readers do not fail.",date:"2021-06-10T00:00:00.000Z",formattedDate:"June 10, 2021",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"cleaner",permalink:"/cn/blog/tags/cleaner"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:6.55,truncated:!0,authors:[{name:"pratyakshsharma"}],prevItem:{title:"Part1: Query apache hudi dataset in an amazon S3 data lake with amazon athena : Read optimized queries",permalink:"/cn/blog/2021/07/16/Query-apache-hudi-dataset-in-an-amazon-S3-data-lake-with-amazon-athena-Read-optimized-queries"},nextItem:{title:"Apache Hudi: How Uber gets data a ride to its destination",permalink:"/cn/blog/2021/06/04/Apache-Hudi-How-Uber-gets-data-a-ride-to-its-destination"}},l={authorsImageUrls:[void 0]},d=[{value:"Reclaiming space and keeping your data lake storage costs in check",id:"reclaiming-space-and-keeping-your-data-lake-storage-costs-in-check",children:[],level:3},{value:"Problem Statement",id:"problem-statement",children:[],level:3},{value:"Deeper dive into Hudi Cleaner",id:"deeper-dive-into-hudi-cleaner",children:[],level:3},{value:"Cleaning Policies",id:"cleaning-policies",children:[],level:3},{value:"Examples",id:"examples",children:[],level:3},{value:"Configurations",id:"configurations",children:[],level:3},{value:"Run command",id:"run-command",children:[],level:3},{value:"Future Scope",id:"future-scope",children:[],level:3}],c={toc:d},p="wrapper";function g(e){let{components:a,...o}=e;return(0,n.yg)(p,(0,i.A)({},c,o,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"Apache Hudi provides snapshot isolation between writers and readers. This is made possible by Hudi\u2019s MVCC concurrency model. In this blog, we will explain how to employ the right configurations to manage multiple file versions. Furthermore, we will discuss mechanisms available to users on how to maintain just the required number of old file versions so that long running readers do not fail. "),(0,n.yg)("h3",{id:"reclaiming-space-and-keeping-your-data-lake-storage-costs-in-check"},"Reclaiming space and keeping your data lake storage costs in check"),(0,n.yg)("p",null,"Hudi provides different table management services to be able to manage your tables on the data lake. One of these services is called the ",(0,n.yg)("strong",{parentName:"p"},"Cleaner"),". As you write more data to your table, for every batch of updates received, Hudi can either generate a new version of the data file with updates applied to records (COPY_ON_WRITE) or write these delta updates to a log file, avoiding rewriting newer version of an existing file (MERGE_ON_READ). In such situations, depending on the frequency of your updates, the number of file versions of log files can grow indefinitely. If your use-cases do not require keeping an infinite history of these versions, it is imperative to have a process that reclaims older versions of the data. This is Hudi\u2019s cleaner service."),(0,n.yg)("h3",{id:"problem-statement"},"Problem Statement"),(0,n.yg)("p",null,"In a data lake architecture, it is a very common scenario to have readers and writers concurrently accessing the same table. As the Hudi cleaner service periodically reclaims older file versions, scenarios arise where a long running query might be accessing a file version that is deemed to be reclaimed by the cleaner. Here, we need to employ the correct configs to ensure readers (aka queries) don\u2019t fail."),(0,n.yg)("h3",{id:"deeper-dive-into-hudi-cleaner"},"Deeper dive into Hudi Cleaner"),(0,n.yg)("p",null,"To deal with the mentioned scenario, lets understand the  different cleaning policies that Hudi offers and the corresponding properties that need to be configured. Options are available to schedule cleaning asynchronously or synchronously. Before going into more details, we would like to explain a few underlying concepts:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Hudi base file"),": Columnar file which consists of final data after compaction. A base file\u2019s name follows the following naming convention: ",(0,n.yg)("inlineCode",{parentName:"li"},"<fileId>_<writeToken>_<instantTime>.parquet"),". In subsequent writes of this file, file id remains the same and commit time gets updated to show the latest version. This also implies any particular version of a record, given its partition path, can be uniquely located using the file id and instant time. "),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"File slice"),": A file slice consists of the base file and any log files consisting of the delta, in case of MERGE_ON_READ table type."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Hudi File Group"),": Any file group in Hudi is uniquely identified by the partition path and the  file id that the files in this group have as part of their name. A file group consists of all the file slices in a particular partition path. Also any partition path can have multiple file groups.")),(0,n.yg)("h3",{id:"cleaning-policies"},"Cleaning Policies"),(0,n.yg)("p",null,"Hudi cleaner currently supports below cleaning policies:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"KEEP_LATEST_COMMITS"),": This is the default policy. This is a temporal cleaning policy that ensures the effect of having lookback into all the changes that happened in the last X commits. Suppose a writer is ingesting data  into a Hudi dataset every 30 minutes and the longest running query can take 5 hours to finish, then the user should retain atleast the last 10 commits. With such a configuration, we ensure that the oldest version of a file is kept on disk for at least 5 hours, thereby preventing the longest running query from failing at any point in time. Incremental cleaning is also possible using this policy."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"KEEP_LATEST_FILE_VERSIONS"),": This policy has the effect of keeping N number of file versions irrespective of time. This policy is useful when it is known how many MAX versions of the file does one want to keep at any given time. To achieve the same behaviour as before of preventing long running queries from failing, one should do their calculations based on data patterns. Alternatively, this policy is also useful if a user just wants to maintain 1 latest version of the file.")),(0,n.yg)("h3",{id:"examples"},"Examples"),(0,n.yg)("p",null,"Suppose a user is ingesting data into a hudi dataset of type COPY_ON_WRITE every 30 minutes as shown below:"),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Initial timeline",src:t(26738).A}),"\n",(0,n.yg)("em",{parentName:"p"},"Figure1: Incoming records getting ingested into a hudi dataset every 30 minutes")),(0,n.yg)("p",null,"The figure shows a particular partition on DFS where commits and corresponding file versions are color coded. 4 different file groups are created in this partition as depicted by fileGroup1, fileGroup2, fileGroup3 and fileGroup4. File group corresponding to fileGroup2 has records ingested from all the 5 commits, while the group corresponding to fileGroup4 has records from the latest 2 commits only."),(0,n.yg)("p",null,"Suppose the user uses the below configs for cleaning:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"hoodie.cleaner.policy=KEEP_LATEST_COMMITS\nhoodie.cleaner.commits.retained=2\n")),(0,n.yg)("p",null,"Cleaner selects the versions of files to be cleaned by taking care of the following:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Latest version of a file should not be cleaned."),(0,n.yg)("li",{parentName:"ul"},"The commit times of the last 2 (configured) + 1 commits are determined. In Figure1, ",(0,n.yg)("inlineCode",{parentName:"li"},"commit 10:30")," and ",(0,n.yg)("inlineCode",{parentName:"li"},"commit 10:00")," correspond to the latest 2 commits in the timeline. One extra commit is included because the time window for retaining commits is essentially equal to the longest query run time. So if the longest query takes 1 hour to finish, and ingestion happens every 30 minutes, you need to retain last 2 commits since 2*30 = 60 (1 hour). At this point of time, the longest query can still be using files written in 3rd commit in reverse order. Essentially this means if a query started executing after ",(0,n.yg)("inlineCode",{parentName:"li"},"commit 9:30"),", it will still be running when clean action is triggered right after ",(0,n.yg)("inlineCode",{parentName:"li"},"commit 10:30")," as in Figure2. "),(0,n.yg)("li",{parentName:"ul"},"Now for any file group, only those file slices are scheduled for cleaning which are not savepointed (another Hudi table service) and whose commit time is less than the 3rd commit (",(0,n.yg)("inlineCode",{parentName:"li"},"commit 9:30")," in figure below) in reverse order.")),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Retain latest commits",src:t(67594).A}),"\n",(0,n.yg)("em",{parentName:"p"},"Figure2: Files corresponding to latest 3 commits are retained")),(0,n.yg)("p",null,"Now, suppose the user uses the below configs for cleaning:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"hoodie.cleaner.policy=KEEP_LATEST_FILE_VERSIONS\nhoodie.cleaner.fileversions.retained=1\n")),(0,n.yg)("p",null,"Cleaner does the following:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"For any file group, latest version (including any for pending compaction) of file slices are kept and the rest are scheduled for cleaning. Clearly as shown in Figure3, if clean action is triggered right after ",(0,n.yg)("inlineCode",{parentName:"li"},"commit 10:30"),", the cleaner will simply leave the latest version in every file group and delete the rest.")),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Retain latest versions",src:t(57173).A}),"\n",(0,n.yg)("em",{parentName:"p"},"Figure3: Latest file version in every file group is retained")),(0,n.yg)("h3",{id:"configurations"},"Configurations"),(0,n.yg)("p",null,"You can find the details about all the possible configurations along with the default values ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/configurations#compaction-configs"},"here"),"."),(0,n.yg)("h3",{id:"run-command"},"Run command"),(0,n.yg)("p",null,"Hudi's cleaner table service can be run as a separate process or along with your data ingestion. As mentioned earlier, it basically cleans up any stale/old files lying around. In case you want to run it along with ingesting data, configs are available which enable you to run it ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/configurations#withAsyncClean"},"synchronously or asynchronously"),". You can use the below command for running the cleaner independently:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"[hoodie]$ spark-submit --class org.apache.hudi.utilities.HoodieCleaner \\\n  --props s3:///temp/hudi-ingestion-config/kafka-source.properties \\\n  --target-base-path s3:///temp/hudi \\\n  --spark-master yarn-cluster\n")),(0,n.yg)("p",null,"In case you wish to run the cleaner service asynchronously with writing, please configure the below:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-java"},"hoodie.clean.automatic=true\nhoodie.clean.async=true\n")),(0,n.yg)("p",null,"Further you can use ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/deployment#cli"},"Hudi CLI")," for managing your Hudi dataset. CLI provides the below commands for cleaner service:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"cleans show")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"clean showpartitions")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"cleans run"))),(0,n.yg)("p",null,"You can find more details and the relevant code for these commands in ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/master/hudi-cli/src/main/java/org/apache/hudi/cli/commands/CleansCommand.java"},(0,n.yg)("inlineCode",{parentName:"a"},"org.apache.hudi.cli.commands.CleansCommand")," class"),". "),(0,n.yg)("h3",{id:"future-scope"},"Future Scope"),(0,n.yg)("p",null,"Work is currently going on for introducing a new cleaning policy based on time elapsed. This will help in achieving a consistent retention throughout regardless of how frequently ingestion happens. You may track the progress ",(0,n.yg)("a",{parentName:"p",href:"https://issues.apache.org/jira/browse/HUDI-349"},"here"),"."),(0,n.yg)("p",null,"We hope this blog gives you an idea about how to configure the Hudi cleaner and the supported cleaning policies. Please visit the ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/blog"},"blog section")," for a deeper understanding of various Hudi concepts. Cheers!"))}g.isMDXComponent=!0},24639:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Amazon Athena expands Apache Hudi support",category:"blog",image:"/assets/images/blog/aws.jpg",tags:["blog","amazon"]},s=void 0,l={permalink:"/cn/blog/2021/07/16/Amazon-Athena-expands-Apache-Hudi-support",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-07-16-Amazon-Athena-expands-Apache-Hudi-support.mdx",source:"@site/blog/2021-07-16-Amazon-Athena-expands-Apache-Hudi-support.mdx",title:"Amazon Athena expands Apache Hudi support",description:"Redirecting... please wait!!",date:"2021-07-16T00:00:00.000Z",formattedDate:"July 16, 2021",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[],prevItem:{title:"Apache Hudi - The Data Lake Platform",permalink:"/cn/blog/2021/07/21/streaming-data-lake-platform"},nextItem:{title:"Part1: Query apache hudi dataset in an amazon S3 data lake with amazon athena : Read optimized queries",permalink:"/cn/blog/2021/07/16/Query-apache-hudi-dataset-in-an-amazon-S3-data-lake-with-amazon-athena-Read-optimized-queries"}},d={authorsImageUrls:[]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/about-aws/whats-new/2021/07/amazon-athena-expands-apache-hudi-support/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},9001:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Part1: Query apache hudi dataset in an amazon S3 data lake with amazon athena : Read optimized queries",authors:[{name:"Dhiraj Thakur"},{name:"Sameer Goel"},{name:"Imtiaz Sayed"}],category:"blog",image:"/assets/images/blog/2021-07-16-query-hudi-using-athena-ro-queries.png",tags:["how-to","read optimized query","amazon"]},s=void 0,l={permalink:"/cn/blog/2021/07/16/Query-apache-hudi-dataset-in-an-amazon-S3-data-lake-with-amazon-athena-Read-optimized-queries",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-07-16-Query-apache-hudi-dataset-in-an-amazon-S3-data-lake-with-amazon-athena-Read-optimized-queries.mdx",source:"@site/blog/2021-07-16-Query-apache-hudi-dataset-in-an-amazon-S3-data-lake-with-amazon-athena-Read-optimized-queries.mdx",title:"Part1: Query apache hudi dataset in an amazon S3 data lake with amazon athena : Read optimized queries",description:"Redirecting... please wait!!",date:"2021-07-16T00:00:00.000Z",formattedDate:"July 16, 2021",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"read optimized query",permalink:"/cn/blog/tags/read-optimized-query"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[{name:"Dhiraj Thakur"},{name:"Sameer Goel"},{name:"Imtiaz Sayed"}],prevItem:{title:"Amazon Athena expands Apache Hudi support",permalink:"/cn/blog/2021/07/16/Amazon-Athena-expands-Apache-Hudi-support"},nextItem:{title:"Employing correct configurations for Hudi's cleaner table service",permalink:"/cn/blog/2021/06/10/employing-right-configurations-for-hudi-cleaner"}},d={authorsImageUrls:[void 0,void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/part-1-query-an-apache-hudi-dataset-in-an-amazon-s3-data-lake-with-amazon-athena-part-1-read-optimized-queries/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},30562:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Apache Hudi - The Data Lake Platform",excerpt:"It's been called many things. But, we have always been building a data lake platform",author:"vinoth",category:"blog",image:"/assets/images/blog/hudi_streaming.png",tags:["datalake platform","blog","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2021/07/21/streaming-data-lake-platform",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-07-21-streaming-data-lake-platform.md",source:"@site/blog/2021-07-21-streaming-data-lake-platform.md",title:"Apache Hudi - The Data Lake Platform",description:"As early as 2016, we set out a bold, new vision reimagining batch data processing through a new \u201cincremental\u201d data processing stack - alongside the existing batch and streaming stacks.",date:"2021-07-21T00:00:00.000Z",formattedDate:"July 21, 2021",tags:[{label:"datalake platform",permalink:"/cn/blog/tags/datalake-platform"},{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:28.99,truncated:!0,authors:[{name:"vinoth"}],prevItem:{title:"Baixin bank\u2019s real-time data lake evolution scheme based on Apache Hudi",permalink:"/cn/blog/2021/07/26/Baixin-banksreal-time-data-lake-evolution-scheme-based-on-Apache-Hudi"},nextItem:{title:"Amazon Athena expands Apache Hudi support",permalink:"/cn/blog/2021/07/16/Amazon-Athena-expands-Apache-Hudi-support"}},l={authorsImageUrls:[void 0]},d=[{value:"Data Lake Platform",id:"data-lake-platform",children:[{value:"Is Hudi a \u201cformat\u201d?",id:"is-hudi-a-format",children:[],level:3},{value:"Is Hudi a transactional layer?",id:"is-hudi-a-transactional-layer",children:[],level:3}],level:2},{value:"Hudi Stack",id:"hudi-stack",children:[],level:2},{value:"Lake Storage",id:"lake-storage",children:[],level:2},{value:"File Format",id:"file-format",children:[],level:2},{value:"Table Format",id:"table-format",children:[],level:2},{value:"Indexes",id:"indexes",children:[],level:2},{value:"Concurrency Control",id:"concurrency-control",children:[],level:2},{value:"Writers",id:"writers",children:[],level:2},{value:"Readers",id:"readers",children:[],level:2},{value:"Table Services",id:"table-services",children:[],level:2},{value:"Data Services",id:"data-services",children:[],level:2},{value:"Timeline Metaserver",id:"timeline-metaserver",children:[],level:2},{value:"Lake Cache",id:"lake-cache",children:[],level:2},{value:"Onwards",id:"onwards",children:[],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...o}=e;return(0,n.yg)(p,(0,i.A)({},c,o,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"As early as 2016, we set out a ",(0,n.yg)("a",{parentName:"p",href:"https://www.oreilly.com/content/ubers-case-for-incremental-processing-on-hadoop/"},"bold, new vision")," reimagining batch data processing through a new \u201c",(0,n.yg)("strong",{parentName:"p"},"incremental"),"\u201d data processing stack - alongside the existing batch and streaming stacks.\nWhile a stream processing pipeline does row-oriented processing, delivering a few seconds of processing latency, an incremental pipeline would apply the same principles to ",(0,n.yg)("em",{parentName:"p"},"columnar")," data in the data lake,\ndelivering orders of magnitude improvements in processing efficiency within few minutes, on extremely scalable batch storage/compute infrastructure. This new stack would be able to effortlessly support regular batch processing for bulk reprocessing/backfilling as well.\nHudi was built as the manifestation of this vision, rooted in real, hard problems faced at ",(0,n.yg)("a",{parentName:"p",href:"https://eng.uber.com/uber-big-data-platform/"},"Uber")," and later took a life of its own in the open source community. Together, we have been able to\nusher in fully incremental data ingestion and moderately complex ETLs on data lakes already."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"the different components that make up the stream and batch processing stack today, showing how an incremental stack blends the best of both the worlds.",src:t(90531).A})),(0,n.yg)("p",null,"Today, this grand vision of being able to express almost any batch pipeline incrementally is more attainable than it ever was. Stream processing is ",(0,n.yg)("a",{parentName:"p",href:"https://flink.apache.org/blog/"},"maturing rapidly")," and gaining ",(0,n.yg)("a",{parentName:"p",href:"https://www.confluent.io/blog/every-company-is-becoming-software/"},"tremendous momentum"),",\nwith ",(0,n.yg)("a",{parentName:"p",href:"https://flink.apache.org/2021/03/11/batch-execution-mode.html"},"generalization")," of stream processing APIs to work over a batch execution model. Hudi completes the missing pieces of the puzzle by providing streaming optimized lake storage,\nmuch like how Kafka/Pulsar enable efficient storage for event streaming. ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/powered_by"},"Many organizations")," have already reaped real benefits of adopting a streaming model for their data lakes, in terms of fresh data, simplified architecture and great cost reductions."),(0,n.yg)("p",null,"But first, we needed to tackle the basics - transactions and mutability - on the data lake. In many ways, Apache Hudi pioneered the transactional data lake movement as we know it today. Specifically, during a time when more special-purpose systems were being born, Hudi introduced a server-less, transaction layer, which worked over the general-purpose Hadoop FileSystem abstraction on Cloud Stores/HDFS. This model helped Hudi to scale writers/readers to 1000s of cores on day one, compared to warehouses which offer a richer set of transactional guarantees but are often bottlenecked by the 10s of servers that need to handle them. We also experience a lot of joy to see similar systems (Delta Lake for e.g) later adopt the same server-less transaction layer model that we originally shared way back in ",(0,n.yg)("a",{parentName:"p",href:"https://eng.uber.com/hoodie/"},"early '17"),". We consciously introduced two table types Copy On Write (with simpler operability) and Merge On Read (for greater flexibility) and now these terms are used in ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/iceberg/pull/1862"},"projects")," outside Hudi, to refer to similar ideas being borrowed from Hudi. Through open sourcing and ",(0,n.yg)("a",{parentName:"p",href:"https://blogs.apache.org/foundation/entry/the-apache-software-foundation-announces64"},"graduating")," from the Apache Incubator, we have made some great progress elevating these ideas ",(0,n.yg)("a",{parentName:"p",href:"http://hudi.apache.org/docs/powered_by.html"},"across the industry"),", as well as bringing them to life with a cohesive software stack. Given the exciting developments in the past year or so that have propelled data lakes further mainstream, we thought some perspective can help users see Hudi with the right lens, appreciate what it stands for, and be a part of where it\u2019s headed. At this time, we also wanted to shine some light on all the great work done by ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/graphs/contributors"},"180+ contributors")," on the project, working with more than 2000 unique users over slack/github/jira, contributing all the different capabilities Hudi has gained over the past years, from its humble beginnings."),(0,n.yg)("p",null,"This is going to be a rather long post, but we will do our best to make it worth your time. Let\u2019s roll."),(0,n.yg)("h2",{id:"data-lake-platform"},"Data Lake Platform"),(0,n.yg)("p",null,"We have noticed that, Hudi is sometimes positioned as a \u201c",(0,n.yg)("a",{parentName:"p",href:"https://cloud.google.com/blog/products/data-analytics/getting-started-with-new-table-formats-on-dataproc"},"table format"),"\u201d or \u201ctransactional layer\u201d. While this is not incorrect, this does not do full justice to all that Hudi has to offer. "),(0,n.yg)("h3",{id:"is-hudi-a-format"},"Is Hudi a \u201cformat\u201d?"),(0,n.yg)("p",null,"Hudi was not designed as a general purpose table format, tracking files/folders for batch processing. Rather, the functionality provided by a table format is merely one layer in the Hudi software stack. Hudi was designed to play well with the Hive format (if you will), given how popular and widespread it is. Over time, to solve scaling challenges or bring in additional functionality, we have invested in our own native table format with an eye for incremental processing vision. for e.g, we need to support shorter transactions that commit every few seconds. We believe these requirements would fully subsume challenges solved by general purpose table formats over time. But, we are also open to plugging in or syncing to other open table formats, so their users can also benefit from the rest of the Hudi stack. Unlike the file formats, a table format is merely a representation of table metadata and it\u2019s actually quite possible to translate from Hudi to other formats/vice versa if users are willing to accept the trade-offs."),(0,n.yg)("h3",{id:"is-hudi-a-transactional-layer"},"Is Hudi a transactional layer?"),(0,n.yg)("p",null,"Of course, Hudi had to provide transactions for implementing deletes/updates, but Hudi\u2019s transactional layer is designed around an ",(0,n.yg)("a",{parentName:"p",href:"https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying"},"event log")," that is also well-integrated with an entire set of built-in table/data services. For e.g compaction is aware of clustering actions already scheduled and optimizes by skipping over the files being clustered - while the user is blissfully unaware of all this. Hudi also provides out-of-box tools for ingesting, ETLing data, and much more. We have always been thinking of Hudi as solving a database problem around stream processing - areas that are actually ",(0,n.yg)("a",{parentName:"p",href:"https://www.infoq.com/presentations/streaming-databases/"},"very related to each other"),". In fact, Stream processing is enabled by logs (capture/emit event streams, rewind/reprocess) and databases (state stores, updatable sinks). With Hudi, the idea was that if we build a database supporting efficient updates and extracting data streams while remaining optimized for large batch queries, incremental pipelines can be built using Hudi tables as state store & update-able sinks. "),(0,n.yg)("p",null,"Thus, the best way to describe Apache Hudi is as a ",(0,n.yg)("strong",{parentName:"p"},"Streaming Data Lake Platform")," built around a ",(0,n.yg)("em",{parentName:"p"},"database kernel"),". The words carry significant meaning."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"/assets/images/blog/datalake-platform/Screen_Shot_2021-07-20_at_5.35.47_PM.png",src:t(47013).A})),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Streaming"),": At its core, by optimizing for fast upserts & change streams, Hudi provides the primitives to data lake workloads that are comparable to what ",(0,n.yg)("a",{parentName:"p",href:"https://kafka.apache.org/"},"Apache Kafka")," does for event-streaming (namely, incremental produce/consume of events and a state-store for interactive querying)."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Data Lake"),": Nonetheless, Hudi provides an optimized, self-managing data plane for large scale data processing on the lake (adhoc queries, ML pipelines, batch pipelines), powering arguably the ",(0,n.yg)("a",{parentName:"p",href:"https://eng.uber.com/apache-hudi-graduation/"},"largest transactional lake")," in the world. While Hudi can be used to build a ",(0,n.yg)("a",{parentName:"p",href:"https://databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html"},"lakehouse"),", given its transactional capabilities, Hudi goes beyond and unlocks an end-to-end streaming architecture. In contrast, the word \u201cstreaming\u201d appears just 3 times in the lakehouse ",(0,n.yg)("a",{parentName:"p",href:"http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf"},"paper"),", and one of them is talking about Hudi."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Platform"),": Oftentimes in open source, there is great tech, but there is just too many of them - all differing ever so slightly in their opinionated ways, ultimately making the integration task onerous on the end user. Lake users deserve the same great usability that cloud warehouses provide, with the additional freedom and transparency of a true open source community. Hudi\u2019s data and table services, tightly integrated with the Hudi \u201ckernel\u201d, gives us the ability to deliver cross layer optimizations with reliability and ease of use."),(0,n.yg)("h2",{id:"hudi-stack"},"Hudi Stack"),(0,n.yg)("p",null,"The following stack captures layers of software components that make up Hudi, with each layer depending on and drawing strength from the layer below. Typically, data lake users write data out once using an open file format like Apache ",(0,n.yg)("a",{parentName:"p",href:"http://parquet.apache.org/"},"Parquet"),"/",(0,n.yg)("a",{parentName:"p",href:"https://orc.apache.org/"},"ORC")," stored on top of extremely scalable cloud storage or distributed file systems. Hudi provides a self-managing data plane to ingest, transform and manage this data, in a way that unlocks incremental data processing on them."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Figure showing the Hudi stack",src:t(62919).A})),(0,n.yg)("p",null,"Furthermore, Hudi either already provides or plans to add components that make this data universally accessible to all the different query engines out there. The features annotated with ",(0,n.yg)("inlineCode",{parentName:"p"},"*")," represent work in progress and dotted boxes represent planned future work, to complete our vision for the project.\nWhile we have strawman designs outlined for the newer components in the blog, we welcome with open arms fresh perspectives from the community.\nRest of the blog will delve into each layer in our stack - explaining what it does, how it's designed for incremental processing and how it will evolve in the future."),(0,n.yg)("h2",{id:"lake-storage"},"Lake Storage"),(0,n.yg)("p",null,"Hudi interacts with lake storage using the ",(0,n.yg)("a",{parentName:"p",href:"https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/FileSystem.html"},"Hadoop FileSystem API"),", which makes it compatible with all of its implementations ranging from HDFS to Cloud Stores to even in-memory filesystems like ",(0,n.yg)("a",{parentName:"p",href:"https://www.alluxio.io/blog/building-high-performance-data-lake-using-apache-hudi-and-alluxio-at-t3go/"},"Alluxio"),"/Ignite. Hudi internally implements its own ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/9d2a65a6a6ff9add81411147f1cddd03f7c08e6c/hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java"},"wrapper filesystem")," on top to provide additional storage optimizations (e.g: file sizing), performance optimizations (e.g: buffering), and metrics. Uniquely, Hudi takes full advantage of append support, for storage schemes that support it, like HDFS. This helps Hudi deliver streaming writes without causing an explosion in file counts/table metadata. Unfortunately, most cloud/object storages do not offer append capability today (except maybe ",(0,n.yg)("a",{parentName:"p",href:"https://azure.microsoft.com/en-us/updates/append-blob-support-in-azure-data-lake-storage-is-now-generally-available/"},"Azure"),"). In the future, we plan to leverage the lower-level APIs of major cloud object stores, to provide similar controls over file counts at streaming ingest latencies."),(0,n.yg)("h2",{id:"file-format"},"File Format"),(0,n.yg)("p",null,"Hudi is designed around the notion of base file and delta log files that store updates/deltas to a given base file (called a file slice). Their formats are pluggable, with Parquet (columnar access) and HFile (indexed access) being the supported base file formats today. The delta logs encode data in ",(0,n.yg)("a",{parentName:"p",href:"http://avro.apache.org/"},"Avro")," (row oriented) format for speedier logging (just like Kafka topics for e.g). Going forward, we plan to ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/pull/3228"},"inline any base file format")," into log blocks in the coming releases, providing columnar access to delta logs depending on block sizes. Future plans also include Orc base/log file formats, unstructured data formats (free form json, images), and even tiered storage layers in event-streaming systems/OLAP engines/warehouses, work with their native file formats."),(0,n.yg)("p",null,"Zooming one level up, Hudi's unique file layout scheme encodes all changes to a given base file, as a sequence of blocks (data blocks, delete blocks, rollback blocks) that are merged in order to derive newer base files. In essence, this makes up a self contained redo log that the lets us implement interesting features on top. For e.g, most of today's data privacy enforcement happens by masking data read off the lake storage on-the-fly, invoking hashing/encryption algorithms over and over on the same set of records and incurring significant compute overhead/cost. Users would be able to keep multiple pre-masked/encrypted copies of the same key in the logs and hand out the correct one based on a policy, avoiding all the overhead."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Hudi base and delta logs",src:t(22885).A})),(0,n.yg)("h2",{id:"table-format"},"Table Format"),(0,n.yg)("p",null,"The term \u201ctable format\u201d is new and still means many things to many people. Drawing an analogy to file formats, a table format simply consists of : the file layout of the table, table\u2019s schema and metadata tracking changes to the table. Hudi is not a table format, it implements one internally. Hudi uses Avro schemas to store, manage and evolve a table\u2019s schema. Currently, Hudi enforces schema-on-write, which although stricter than schema-on-read, is adopted ",(0,n.yg)("a",{parentName:"p",href:"https://docs.confluent.io/platform/current/schema-registry/avro.html"},"widely")," in the stream processing world to ensure pipelines don't break from non backwards compatible changes."),(0,n.yg)("p",null,"Hudi consciously lays out files within a table/partition into groups and maintains a mapping between an incoming record\u2019s key to an existing file group. All updates are recorded into delta log files specific to a given file group and this design ensures low merge overhead compared to approaches like Hive ACID, which have to merge all delta records against all base files to satisfy queries. For e.g, with uuid keys (used very widely) all base files are very likely to overlap with all delta logs, rendering any range based pruning useless. Much like state stores, Hudi\u2019s design anticipates fast key based upserts/deletes and only requires merging delta logs within each file group. This design choice also lets Hudi provide more capabilities for writing/querying as we will explain below. "),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Shows the Hudi table format components",src:t(69365).A})),(0,n.yg)("p",null,"The ",(0,n.yg)("em",{parentName:"p"},"timeline")," is the source-of-truth event log for all Hudi\u2019s table metadata, stored under the ",(0,n.yg)("inlineCode",{parentName:"p"},".hoodie")," folder, that provides an ordered log of all actions performed on the table. Events are retained on the timeline up to a configured interval of time/activity. Each file group is also designed as it\u2019s own self-contained log, which means that even if an action that affected a file group is archived from the timeline, the right state of the records in each file group can be reconstructed by simply locally applying the delta logs to the base file. This design bounds the metadata size, proportional to how often the table is being written to/operated on, independent of how large the entire table is. This is a critical design element needs for supporting frequent writes/commits to tables."),(0,n.yg)("p",null,"Lastly, new events on the timeline are then consumed and reflected onto an internal metadata table, implemented as another merge-on-read table offering low write amplification. Hudi is able to absorb quick/rapid changes to table\u2019s metadata, unlike table formats designed for slow-moving data. Additionally, the metadata table uses the ",(0,n.yg)("a",{parentName:"p",href:"https://hbase.apache.org/2.0/devapidocs/org/apache/hadoop/hbase/io/hfile/HFile.html"},"HFile")," base file format, which provides indexed lookups of keys avoiding the need for reading the entire metadata table to satisfy metadata reads. It currently stores all the physical file paths that are part of the table, to avoid expensive cloud file listings."),(0,n.yg)("p",null,"A key challenge faced by all the table formats out there today, is the need for expiring snapshots/controlling retention for time travel queries such that it does not interfere with query planning/performance. In the future, we plan to build an indexed timeline in Hudi, which can span the entire history of the table, supporting a time travel look back window of several months/years."),(0,n.yg)("h2",{id:"indexes"},"Indexes"),(0,n.yg)("p",null,"Indexes help databases plan better queries, that reduce the overall amount of I/O and deliver faster response times. Table metadata about file listings and column statistics are often enough for lake query engines to generate optimized, engine specific query plans quickly. This is however not sufficient for Hudi to realize fast upserts. Hudi already supports different key based indexing schemes to quickly map incoming record keys into the file group they reside in. For this purpose, Hudi exposes a pluggable indexing layer to the writer implementations, with built-in support for range pruning (when keys are ordered and largely arrive in order) using interval trees and bloom filters (e.g: for uuid based keys where ordering is of very little help). Hudi also implements a HBase backed external index which is much more performant although more expensive to operate. Hudi also consciously exploits the partitioning scheme of the table to implement global and non-global indexing schemes. Users can choose to enforce key constraints only within a partition, in return for ",(0,n.yg)("em",{parentName:"p"},(0,n.yg)("inlineCode",{parentName:"em"},"O(num_affected_partitions)"))," upsert performance as opposed to ",(0,n.yg)("em",{parentName:"p"},(0,n.yg)("inlineCode",{parentName:"em"},"O(total_partitions)"))," in the global indexing scenarios. We refer you to this ",(0,n.yg)("a",{parentName:"p",href:"http://hudi.apache.org/blog/2020/11/11/hudi-indexing-mechanisms"},"blog"),", that goes over indexing in detail. Ultimately, Hudi's writer path ensures the index is always kept in sync with the timeline and data, which is cumbersome and error prone to implement on top of a table format by hand."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"/assets/images/blog/datalake-platform/hudi-design-diagrams_-_Page_5.png",src:t(87458).A})),(0,n.yg)("p",null,"In the future, we intend to add additional forms of indexing as new partitions on the metadata table. Let\u2019s discuss the role\xa0 each one has to play briefly. Query engines typically rely on partitioning to cut down the number of files read for a given query. In database terms, a Hive partition is nothing but a coarse range index, that maps a set of columns to a list of files. Table formats born in the cloud like Iceberg/Delta Lake, have built-in tracking of column ranges per file in a single flat file (json/avro), that helps avoid planning costs for large/poorly sized tables. This need has been largely reduced for Hudi tables thus far, given Hudi automatically enforces file sizes which help bound time taken to read out stats from parquet footers for e.g. However, with the advent of features like clustering, there is a need for writing smaller files first and then reclustering in a query optimized way. We plan to add indexed column ranges, that can scale to lots of small files and support faster mutations . See ",(0,n.yg)("a",{parentName:"p",href:"https://cwiki.apache.org/confluence/display/HUDI/RFC-27+Data+skipping+index+to+improve+query+performance"},"RFC-27")," to track the design process and get involved."),(0,n.yg)("p",null,"While Hudi already supports external indexes for random write workloads, we would like to support ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/pull/2487"},"point-lookup-ish queries")," right on top of lake storage, which helps avoid the overhead of an additional database for many classes of data applications. We also anticipate that uuid/key based joins will be sped up a lot, by leveraging record level indexing schemes, we build out for fast upsert performance. We also plan to move our tracking of bloom filters out of the file footers and into its ",(0,n.yg)("a",{parentName:"p",href:"https://issues.apache.org/jira/browse/HUDI-1295"},"own partition")," on the metadata table. Ultimately, we look to exposing all of this to the queries as well in the coming releases."),(0,n.yg)("h2",{id:"concurrency-control"},"Concurrency Control"),(0,n.yg)("p",null,"Concurrency control defines how different writers/readers coordinate access to the table. Hudi ensures atomic writes, by way of publishing commits atomically to the timeline, stamped with an instant time that denotes the time at which the action is deemed to have occurred. Unlike general purpose file version control, Hudi draws clear distinction between writer processes (that issue user\u2019s upserts/deletes), table services (that write data/metadata to optimize/perform bookkeeping) and readers (that execute queries and read data). Hudi provides snapshot isolation between all three types of processes, meaning they all operate on a consistent snapshot of the table. Hudi provides ",(0,n.yg)("a",{parentName:"p",href:"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+22+%3A+Snapshot+Isolation+using+Optimistic+Concurrency+Control+for+multi-writers"},"optimistic concurrency control")," (OCC) between writers, while providing lock-free, non-blocking MVCC\xa0 based concurrency control between writers and table-services and between different table services."),(0,n.yg)("p",null,"Projects that solely rely on OCC deal with competing operations, by either implementing a lock or relying on atomic renames. Such approaches are optimistic that real contention never happens and resort to failing one of the writer operations if conflicts occur, which can cause significant resource wastage or operational overhead. Imagine a scenario of two writer processes : an ingest writer job producing new data every 30 minutes and a deletion writer job that is enforcing GDPR taking 2 hours to issue deletes. If there were to overlap on the same files (very likely to happen in real situations with random deletes), the deletion job is almost guaranteed to starve and fail to commit each time, wasting tons of cluster resources. Hudi takes a very different approach that we believe is more apt for lake transactions, which are typically long-running. For e.g async compaction that can keep deleting records in the background without blocking the ingest job. This is implemented via a file level, log based concurrency control protocol which orders actions based on their start instant times on the timeline."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Figure showing competing transactions leading to starvation with just OCC",src:t(64516).A})),(0,n.yg)("p",null,"We are hard at work, improving our OCC based implementation around early detection of conflicts for concurrent writers and terminate early without burning up CPU resources. We are also working on ",(0,n.yg)("a",{parentName:"p",href:"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+22+%3A+Snapshot+Isolation+using+Optimistic+Concurrency+Control+for+multi-writers#RFC22:SnapshotIsolationusingOptimisticConcurrencyControlformultiwriters-FutureWork(LockFree-ishConcurrencyControl)"},"adding fully log based"),", non-blocking concurrency control between writers, where writers proceed to write deltas and conflicts are resolved later in some deterministic timeline order - again much like how stream processing programs are written. This is possible only due to Hudi\u2019s unique design that sequences actions into an ordered event log and the transaction handling code is aware of the relationship/interdependence of actions to each other."),(0,n.yg)("h2",{id:"writers"},"Writers"),(0,n.yg)("p",null,"Hudi tables can be used as sinks for Spark/Flink pipelines and the Hudi writing path provides several enhanced capabilities over file writing done by vanilla parquet/avro sinks. Hudi classifies write operations carefully into incremental (",(0,n.yg)("inlineCode",{parentName:"p"},"insert"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"upsert"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"delete"),") and batch/bulk operations (",(0,n.yg)("inlineCode",{parentName:"p"},"insert_overwrite"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"insert_overwrite_table"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"delete_partition"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"bulk_insert"),") and provides relevant functionality for each operation in a performant and cohesive way. Both upsert and delete operations automatically handle merging of records with the same key in the input stream (say, a CDC stream obtained from upstream table) and then lookup the index, finally invoke a bin packing algorithm to pack data into files, while ",(0,n.yg)("a",{parentName:"p",href:"http://hudi.apache.org/blog/2021/03/01/hudi-file-sizing"},"respecting a pre-configured target file size"),". An insert operation on the other hand, is intelligent enough to avoid the precombining and index lookup, while retaining the benefits of the rest of the pipeline. Similarly, bulk_insert operation provides several sort modes for controlling initial file sizes and file counts, when importing data from an external table to Hudi. The other batch write operations provide MVCC based implementations of typical overwrite semantics used in batch data pipelines, while retaining all the transactional and incremental processing capabilities, making it seamless to switch between incremental pipelines for regular runs and batch pipelines for backfilling/dropping older partitions. The write pipeline also contains lower layers optimizations around handling large merges by spilling to ",(0,n.yg)("a",{parentName:"p",href:"https://rocksdb.org/"},"rocksDB")," or an external spillable map, multi-threaded/concurrent I/O to improve write performance."),(0,n.yg)("p",null,"Keys are first class citizens inside Hudi and the pre-combining/index lookups done before upsert/deletes ensure a key is unique across partitions or within partitions, as desired. In contrast with other approaches where this is left to data engineer to co-ordinate using ",(0,n.yg)("inlineCode",{parentName:"p"},"MERGE INTO")," statements, this approach ensures quality data especially for critical use-cases. Hudi also ships with several ",(0,n.yg)("a",{parentName:"p",href:"http://hudi.apache.org/blog/2021/02/13/hudi-key-generators/"},"built-in key generators")," that can parse all common date/timestamps, handle malformed data with an extensible framework for defining custom key generators. Keys are also materialized with the records using the ",(0,n.yg)("inlineCode",{parentName:"p"},"_hoodie_record_key")," meta column, which makes it possible to change the key fields and perform repairs on older data with incorrect keys for e.g. Finally, Hudi provides a ",(0,n.yg)("inlineCode",{parentName:"p"},"HoodieRecordPayload")," interface is very similar to processor APIs in Flink or Kafka Streams, and allows for expressing arbitrary merge conditions, between the base and delta log records. This allows users to express partial merges (e.g log only updated columns to the delta log for efficiency) and avoid reading all the base records before every merge. Routinely, we find users leverage such custom merge logic during replaying/backfilling older data onto a table, while ensuring newer updates are not overwritten causing the table's snapshot to go back in time. This is achieved by simply using the  ",(0,n.yg)("inlineCode",{parentName:"p"},"HoodieDefaultPayload")," where latest value for a given key is picked based a configured precombine field value in the data."),(0,n.yg)("p",null,"Hudi writers add metadata to each record, that codify the commit time and a sequence number for each record within that commit (comparable to a Kafka offset), which make it possible to derive record level change streams. Hudi also provides users the ability to specify event time fields in incoming data streams and track them in the timeline.Mapping these to stream processing concepts, Hudi contains both ",(0,n.yg)("a",{parentName:"p",href:"https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/"},"arrival and event time")," for records for each commit, that can help us build good ",(0,n.yg)("a",{parentName:"p",href:"https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/datastream/event-time/generating_watermarks/"},"watermarks")," that inform complex incremental processing pipelines. In the near future, we are looking to add new metadata columns, that encode the source operation (insert, update, delete) for each record, before we embark on this grand goal of full end-end incremental ETL pipelines. All said, we realized many users may simply want to use Hudi as an efficient write layer that supports transactions, fast updates/deletes. We are looking into adding support for ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/pull/3306"},"virtual keys")," and making the ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/pull/3247"},"meta columns optional"),", to lower storage overhead, while still making rest of Hudi's capabilities (metadata table, table services, ..) available."),(0,n.yg)("h2",{id:"readers"},"Readers"),(0,n.yg)("p",null,"Hudi provides snapshot isolation between writers and readers and allows for any table snapshot to be queries consistently from all major lake query engines (Spark, Hive, Flink, Presto, Trino, Impala) and even cloud warehouses like Redshift. In fact, we would love to bring Hudi tables as external tables with BigQuery/Snowflake as well, once they also embrace the lake table formats more natively. Our design philosophy around query performance has been to make Hudi as lightweight as possible whenever only base columnar files are read (CoW  snapshot, MOR read-optimized queries),  employing the engine specific vectorized readers in Presto, Trino, Spark for e.g to be employed. This model is far more scalable than maintaining our own readers and users to benefit from engine specific optimizations. For e.g ",(0,n.yg)("a",{parentName:"p",href:"https://prestodb.io/blog/2021/02/04/raptorx"},"Presto"),", ",(0,n.yg)("a",{parentName:"p",href:"https://trino.io/docs/current/connector/hive-caching.html"},"Trino")," all have their own data/metadata caches. Whenever, Hudi has to merge base and log files for a query, Hudi takes control and employs several mechanisms (spillable maps, lazy reading) to improve merge performance, while also providing a read-optimized query on the data that trades off data freshness for query performance. In the near future, we are investing deeply into improving MoR snapshot query performance in many ways such as inlining parquet data, special handling of overwrite payloads/merges. "),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Log merging done for incremental queries",src:t(72690).A})),(0,n.yg)("p",null,"True to its design goals, Hudi provides some very powerful incremental querying capabilities that tied together the meta fields added during writing and the file group based storage layout. While table formats that merely track files, are only able to provide information about files that changed during each snapshot or commits, Hudi generates the exact set of records that changed given a point in the timeline, due to tracking of record level event and arrival times. Further more, this design allows large commits to be consumed in smaller chunks by an incremental query, fully decoupling the writing and incremental querying of data. Time travel is merely implemented as an incremental query that starts and stops at an older portion of the timeline. Since Hudi ensures that a key is atomically mapped to a single file group at any point in time, it makes it possible to support full CDC capabilities on Hudi tables, such as providing all possible values for a given record since time ",(0,n.yg)("inlineCode",{parentName:"p"},"t"),", CDC streams with both before and after images. All of these functionalities can be built local to each file group, given each file group is a self-contained log. Much of our future work in this area will be around bringing such a powerful set of ",(0,n.yg)("a",{parentName:"p",href:"https://debezium.io/"},"debezium")," like capabilities to life in the coming months. "),(0,n.yg)("h2",{id:"table-services"},"Table Services"),(0,n.yg)("p",null,"What defines and sustains a project\u2019s value over years are its fundamental design principles and the subtle trade offs. Databases often consist of several internal components, working in tandem to deliver efficiency, performance and great operability to its users. True to intent to act as state store for incremental data pipelines, we designed Hudi with built-in table services and self-managing runtime that can orchestrate/trigger these services to optimize everything internally. In fact, if we compare rocksDB (a very popular stream processing state-store) and Hudi\u2019s components, the similarities become obvious."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"/assets/images/blog/datalake-platform/hudi-design-diagrams_-_Page_4.png",src:t(8587).A})),(0,n.yg)("p",null,"There are several built-in table services, all with the goal of ensuring performant table storage layout and metadata management, which are automatically invoked either synchronously after each write operation, or asynchronously as a separate background job. Furthermore, Spark (and Flink) streaming writers can run in continuous mode, and invoke table services asynchronously sharing the underlying executors intelligently with writers. Archival service ensures that the timeline holds sufficient history for inter service co-ordination (e.g compactions wait for other compactions to complete on the same file group), incremental queries. Once events expire from the timeline, the archival service cleans up any side-effects from lake storage (e.g. rolling back of failing concurrent transactions). Hudi's transaction management implementation allows all of these services to be idempotent and thus resilient to failure via just simple retries.  ",(0,n.yg)("a",{parentName:"p",href:"http://hudi.apache.org/blog/2021/06/10/employing-right-configurations-for-hudi-cleaner"},"Cleaner")," service works off the timeline incrementally (eating our own incremental design dog food), removing file slices that are past the configured retention period for incremental queries, while also allowing sufficient time for long running batch jobs (e.g Hive ETLs) to finish running. Compaction service comes with built-in strategies (date partitioning based, I/O bounded), that merges a base file with a set of delta log files to produce new base file, all while allowing writes to happen concurrently to the file group. This is only possible due to Hudi's grouping of files into groups and support for flexible log merging, and unlocks non-blocking execution of deletes while concurrent updates are being issues to the same set of records. ",(0,n.yg)("a",{parentName:"p",href:"http://hudi.apache.org/blog/2021/01/27/hudi-clustering-intro/"},"Clustering")," service functions similar to what users find in BigQuery or Snowflake, where users can group records that are often queried together by sort keys or control file sizes by coalescing smaller base files into larger ones. Clustering is fully aware of other actions on the timeline such as cleaning, compaction, and it helps Hudi implement intelligent optimizations like avoiding compaction on file groups that are already being clustered, to save on I/O. Hudi also performs rollback of partial writes and cleans up any uncommitted data from lake storage, by use of marker files that track any files created as a part of write operations. Finally, the bootstrap service performs one time zero copy migration of plain parquet tables to Hudi, while allowing both pipelines to operate in parallel, for data validation purposes. Cleaner service is once again aware of these bootstrapped base files and can optionally clean them up, to ensure use-cases like GDPR compliance are met."),(0,n.yg)("p",null,"We are always looking for ways to improve and enhance our table services in meaningful ways. In the coming releases, we are working towards a much more ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/pull/3233"},"scalable model")," of cleaning up partial writes, by consolidating marker file creation using our timeline metaserver, which avoids expensive full table scans to seek out and remove uncommitted files. We also have ",(0,n.yg)("a",{parentName:"p",href:"https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=181307144"},"various proposals")," to add more clustering schemes, unlock clustering with concurrent updates using fully log based concurrency control. "),(0,n.yg)("h2",{id:"data-services"},"Data Services"),(0,n.yg)("p",null,"As noted at the start, we wanted to make Hudi immediately usable for common end-end use-cases and thus invested deeply into a set of data services, that provide functionality that is data/workload specific, sitting on top of the table services, writers/readers directly. Foremost in that list, is the Hudi DeltaStreamer utility, which has been an extremely popular choice for painlessly building a data lake out of  Kafka streams and files landing in different formats on top of lake storage. Over time, we have also built out sources that cover all major systems like a JDBC source for RDBMS/other warehouses, Hive source and even incrementally pulling data from other Hudi tables. The utility supports automatic checkpoint management tracking source checkpoints as a part of target Hudi table metadata, with support for backfills/one-off runs. DeltaStreamer also integrates with major schema registries such as Confluent's and also provides checkpoint translation from other popular mechanisms like Kafka connect. It also supports de-duplication of data, multi-level configuration management system, built in transformers that take arbitrary SQL or coerce ",(0,n.yg)("a",{parentName:"p",href:"http://hudi.apache.org/blog/2020/10/19/hudi-meets-aws-emr-and-aws-dms/"},"CDC log changes")," into writable forms, that combined with other aforementioned features can be used for deploying production grade incremental pipelines. Finally, just like the Spark/Flink streaming writers, DeltaStreamer is able to run in a continuous mode, with automatic management of table services. Hudi also provides several other tools for snapshotting and incrementally exporting Hudi tables, also importing/",(0,n.yg)("a",{parentName:"p",href:"http://hudi.apache.org/blog/2020/03/22/exporting-hudi-datasets/"},"exporting"),"/bootstrapping new tables into Hudi. Hudi also provides commit notifications into Http endpoints or Kafka topics, about table commit activity, which can be used for analytics or building data sensors in workflow managers like Airflow to trigger pipelines."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"/assets/images/blog/datalake-platform/hudi-design-diagrams_-_Page_8.png",src:t(40855).A})),(0,n.yg)("p",null,"Going forward, we would love contributions to enhance our ",(0,n.yg)("a",{parentName:"p",href:"http://hudi.apache.org/blog/2020/08/22/ingest-multiple-tables-using-hudi/"},"multi delta streamer utility"),", which can ingest entire Kafka clusters in a single large Spark application, to be on par and hardened. To further our progress towards end-end complex incremental pipelines, we plan to work towards enhancing the delta streamer utility and its SQL transformers to be triggered by multiple source streams (as opposed to just the one today) and unlock materialized views at scale. We would like to bring an array of useful transformers that perform masking or data monitoring, and extend support for egress of data off Hudi tables into other external sinks as well. Finally, we would love to merge the FlinkStreamer and the DeltaStreamer utilities into one cohesive utility, that can be used across engines. We are constantly improving existing sources (e.g support for parallelized listings of DFS sources) and adding new ones (e.g S3 event based DFS source)"),(0,n.yg)("h2",{id:"timeline-metaserver"},"Timeline Metaserver"),(0,n.yg)("p",null,"Storing and serving table metadata right on the lake storage is scalable, but can be much less performant compared to RPCs against a scalable meta server. Most cloud warehouses internally are built on a metadata layer that leverages an external database (e.g ",(0,n.yg)("a",{parentName:"p",href:"https://www.snowflake.com/blog/how-foundationdb-powers-snowflake-metadata-forward/"},"Snowflake uses foundationDB"),"). Hudi also provides a metadata server, called the \u201cTimeline server\u201d, which offers an alternative backing store for Hudi\u2019s table metadata. Currently, the timeline server runs embedded in the Hudi writer processes, serving file listings out of a local rocksDB store/",(0,n.yg)("a",{parentName:"p",href:"https://javalin.io/"},"Javalin")," REST API during the write process, without needing to repeatedly list the cloud storage. Given we have hardened this as the default option since our 0.6.0 release, we are considering standalone timeline server installations, with support for horizontal scaling, database/table mappings, security and all the features necessary to turn it into a highly performant next generation lake metastore."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"/assets/images/blog/datalake-platform/hudi-design-diagrams_-_Page_6.png",src:t(27193).A})),(0,n.yg)("h2",{id:"lake-cache"},"Lake Cache"),(0,n.yg)("p",null,"There is a fundamental tradeoff today in data lakes between faster writing and great query performance. Faster writing typically involves writing smaller files (and later clustering them) or logging deltas (and later merging on read). While this provides good performance already, the pursuit of great query performance often warrants opening fewer number of files/objects on lake storage and may be pre-materializing the merges between base and delta logs. After all, most databases employ a ",(0,n.yg)("a",{parentName:"p",href:"https://dev.mysql.com/doc/refman/8.0/en/innodb-buffer-pool.html"},"buffer pool")," or ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/facebook/rocksdb/wiki/Block-Cache"},"block cache"),", to amortize the cost of accessing storage. Hudi already contains several design elements that are conducive for building a caching tier (write-through or even just populated by an incremental query), that will be multi-tenant and can cache pre-merged images of the latest file slices, consistent with the timeline. Hudi timeline can be used to simply communicate caching policies, just like how we perform inter table service co-ordination. Historically, caching has been done closer to the query engines or via intermediate in-memory file systems. By placing a caching tier closer and more tightly integrated with a transactional lake storage like Hudi, all query engines would be able to share and amortize the cost of the cache, while supporting updates/deletes as well. We look forward to building a buffer pool for the lake that works across all major engines, with the contributions from the rest of the community. "),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"/assets/images/blog/datalake-platform/hudi-design-diagrams_-_Page_7.png",src:t(14915).A})),(0,n.yg)("h2",{id:"onwards"},"Onwards"),(0,n.yg)("p",null,"We hope that this blog painted a complete picture of Apache Hudi, staying true to its founding principles. Interested users and readers can expect blogs delving into each layer of the stack and an overhaul of our docs along these lines in the coming weeks/months. We view the current efforts around table formats as merely removing decade-old bottlenecks in data lake storage/query planes, problems which have been already solved very well in cloud warehouses like Big Query/Snowflake. We would like to underscore that our vision here is much greater, much more technically challenging. We as an industry are just wrapping our heads around many of these deep, open-ended problems, that need to be solved to marry stream processing and data lakes, with scale and simplicity. We hope to continue to put community first and build/solve these hard problems together. If these challenges excite you and you would like to build for that exciting future, please come join our ",(0,n.yg)("a",{parentName:"p",href:"http://hudi.apache.org/contribute/get-involved"},"community"),"."))}g.isMDXComponent=!0},43988:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Baixin bank\u2019s real-time data lake evolution scheme based on Apache Hudi",category:"blog",image:"/assets/images/blog/2021-07-26-baixin-bank-real-time-data-lake.png",tags:["use-case","real-time datalake","incremental processing","developpaper"]},s=void 0,l={permalink:"/cn/blog/2021/07/26/Baixin-banksreal-time-data-lake-evolution-scheme-based-on-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-07-26-Baixin-banksreal-time-data-lake-evolution-scheme-based-on-Apache-Hudi.mdx",source:"@site/blog/2021-07-26-Baixin-banksreal-time-data-lake-evolution-scheme-based-on-Apache-Hudi.mdx",title:"Baixin bank\u2019s real-time data lake evolution scheme based on Apache Hudi",description:"Redirecting... please wait!!",date:"2021-07-26T00:00:00.000Z",formattedDate:"July 26, 2021",tags:[{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"real-time datalake",permalink:"/cn/blog/tags/real-time-datalake"},{label:"incremental processing",permalink:"/cn/blog/tags/incremental-processing"},{label:"developpaper",permalink:"/cn/blog/tags/developpaper"}],readingTime:.045,truncated:!1,authors:[],prevItem:{title:"MLOps Wars: Versioned Feature Data with a Lakehouse",permalink:"/cn/blog/2021/08/03/MLOps-Wars-Versioned-Feature-Data-with-a-Lakehouse"},nextItem:{title:"Apache Hudi - The Data Lake Platform",permalink:"/cn/blog/2021/07/21/streaming-data-lake-platform"}},d={authorsImageUrls:[]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://developpaper.com/baixin-banks-real-time-data-lake-evolution-scheme-based-on-apache-hudi/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},12550:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"MLOps Wars: Versioned Feature Data with a Lakehouse",authors:[{name:"David Bzhalava"},{name:"Jim Dowling"}],category:"blog",image:"/assets/images/blog/2021-08-03-mlops-wars.png",tags:["use-case","mlops","feature store","incremental processing","time travel query","logicalclocks"]},s=void 0,l={permalink:"/cn/blog/2021/08/03/MLOps-Wars-Versioned-Feature-Data-with-a-Lakehouse",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-08-03-MLOps-Wars-Versioned-Feature-Data-with-a-Lakehouse.mdx",source:"@site/blog/2021-08-03-MLOps-Wars-Versioned-Feature-Data-with-a-Lakehouse.mdx",title:"MLOps Wars: Versioned Feature Data with a Lakehouse",description:"Redirecting... please wait!!",date:"2021-08-03T00:00:00.000Z",formattedDate:"August 3, 2021",tags:[{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"mlops",permalink:"/cn/blog/tags/mlops"},{label:"feature store",permalink:"/cn/blog/tags/feature-store"},{label:"incremental processing",permalink:"/cn/blog/tags/incremental-processing"},{label:"time travel query",permalink:"/cn/blog/tags/time-travel-query"},{label:"logicalclocks",permalink:"/cn/blog/tags/logicalclocks"}],readingTime:.045,truncated:!1,authors:[{name:"David Bzhalava"},{name:"Jim Dowling"}],prevItem:{title:"Cost-Efficient Open Source Big Data Platform at Uber",permalink:"/cn/blog/2021/08/11/Cost-Efficient-Open-Source-Big-Data-Platform-at-Uber"},nextItem:{title:"Baixin bank\u2019s real-time data lake evolution scheme based on Apache Hudi",permalink:"/cn/blog/2021/07/26/Baixin-banksreal-time-data-lake-evolution-scheme-based-on-Apache-Hudi"}},d={authorsImageUrls:[void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.logicalclocks.com/blog/mlops-wars-versioned-feature-data-with-a-lakehouse",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},39998:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Cost-Efficient Open Source Big Data Platform at Uber",authors:[{name:"Zheng Shao"},{name:"Mohammad Islam"}],category:"blog",image:"/assets/images/blog/2021-08-11-cost-efficient-open-source-big-data-platform-at-uber.png",tags:["cost efficiency","optimization","bigdata","data platform","incremental processing","uber"]},s=void 0,l={permalink:"/cn/blog/2021/08/11/Cost-Efficient-Open-Source-Big-Data-Platform-at-Uber",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-08-11-Cost-Efficient-Open-Source-Big-Data-Platform-at-Uber.mdx",source:"@site/blog/2021-08-11-Cost-Efficient-Open-Source-Big-Data-Platform-at-Uber.mdx",title:"Cost-Efficient Open Source Big Data Platform at Uber",description:"Redirecting... please wait!!",date:"2021-08-11T00:00:00.000Z",formattedDate:"August 11, 2021",tags:[{label:"cost efficiency",permalink:"/cn/blog/tags/cost-efficiency"},{label:"optimization",permalink:"/cn/blog/tags/optimization"},{label:"bigdata",permalink:"/cn/blog/tags/bigdata"},{label:"data platform",permalink:"/cn/blog/tags/data-platform"},{label:"incremental processing",permalink:"/cn/blog/tags/incremental-processing"},{label:"uber",permalink:"/cn/blog/tags/uber"}],readingTime:.045,truncated:!1,authors:[{name:"Zheng Shao"},{name:"Mohammad Islam"}],prevItem:{title:"Schema evolution with DeltaStreamer using KafkaSource",permalink:"/cn/blog/2021/08/16/kafka-custom-deserializer"},nextItem:{title:"MLOps Wars: Versioned Feature Data with a Lakehouse",permalink:"/cn/blog/2021/08/03/MLOps-Wars-Versioned-Feature-Data-with-a-Lakehouse"}},d={authorsImageUrls:[void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://eng.uber.com/cost-efficient-big-data-platform/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},91254:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Schema evolution with DeltaStreamer using KafkaSource",excerpt:"Evolve schema used in Kafkasource of DeltaStreamer to keep data up to date with business",author:"sbernauer",category:"blog",image:"/assets/images/blog/hudi_schemaevolution.png",tags:["design","deltastreamer","schema","apache hudi","apache kafka"]},r=void 0,s={permalink:"/cn/blog/2021/08/16/kafka-custom-deserializer",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-08-16-kafka-custom-deserializer.md",source:"@site/blog/2021-08-16-kafka-custom-deserializer.md",title:"Schema evolution with DeltaStreamer using KafkaSource",description:"The schema used for data exchange between services can change rapidly with new business requirements.",date:"2021-08-16T00:00:00.000Z",formattedDate:"August 16, 2021",tags:[{label:"design",permalink:"/cn/blog/tags/design"},{label:"deltastreamer",permalink:"/cn/blog/tags/deltastreamer"},{label:"schema",permalink:"/cn/blog/tags/schema"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"apache kafka",permalink:"/cn/blog/tags/apache-kafka"}],readingTime:3.155,truncated:!0,authors:[{name:"sbernauer"}],prevItem:{title:"Adding support for Virtual Keys in Hudi",permalink:"/cn/blog/2021/08/18/virtual-keys"},nextItem:{title:"Cost-Efficient Open Source Big Data Platform at Uber",permalink:"/cn/blog/2021/08/11/Cost-Efficient-Open-Source-Big-Data-Platform-at-Uber"}},l={authorsImageUrls:[void 0]},d=[{value:"What do we want to achieve?",id:"what-do-we-want-to-achieve",children:[],level:2},{value:"What is the problem?",id:"what-is-the-problem",children:[],level:2},{value:"Solution",id:"solution",children:[],level:2},{value:"Configurations",id:"configurations",children:[],level:2},{value:"Conclusion",id:"conclusion",children:[],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...o}=e;return(0,n.yg)(p,(0,i.A)({},c,o,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"The schema used for data exchange between services can change rapidly with new business requirements.\nApache Hudi is often used in combination with kafka as a event stream where all events are transmitted according to a record schema.\nIn our case a Confluent schema registry is used to maintain the schema and as schema evolves, newer versions are updated in the schema registry."),(0,n.yg)("h2",{id:"what-do-we-want-to-achieve"},"What do we want to achieve?"),(0,n.yg)("p",null,"We have multiple instances of DeltaStreamer running, consuming many topics with different schemas ingesting to multiple Hudi tables. Deltastreamer is a utility in Hudi to assist in ingesting data from multiple sources like DFS, kafka, etc into Hudi. If interested, you can read more about DeltaStreamer tool ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/writing_data#deltastreamer"},"here"),"\nIdeally every topic should be able to evolve the schema to match new business requirements. Producers start producing data with a new schema version and the DeltaStreamer picks up the new schema and ingests the data with the new schema. For this to work, we run our DeltaStreamer instances with the latest schema version available from the Schema Registry to ensure that we always use the freshest schema with all attributes.\nA prerequisites is that all the mentioned Schema evolutions must be ",(0,n.yg)("inlineCode",{parentName:"p"},"BACKWARD_TRANSITIVE")," compatible (see ",(0,n.yg)("a",{parentName:"p",href:"https://docs.confluent.io/platform/current/schema-registry/avro.html"},"Schema Evolution and Compatibility of Avro Schema changes"),". This ensures that every record in the kafka topic can always be read using the latest schema."),(0,n.yg)("h2",{id:"what-is-the-problem"},"What is the problem?"),(0,n.yg)("p",null,"The normal operation looks like this. Multiple (or a single) producers write records to the kafka topic.\nIn regular flow of events, all records are in the same schema v1 and is in sync with schema registry.\n",(0,n.yg)("img",{alt:"Normal operation",src:t(51200).A}),(0,n.yg)("br",null),"\nThings get complicated when a producer switches to a new Writer-Schema v2 (in this case ",(0,n.yg)("inlineCode",{parentName:"p"},"Producer A"),"). ",(0,n.yg)("inlineCode",{parentName:"p"},"Producer B")," remains on Schema v1. E.g. an attribute ",(0,n.yg)("inlineCode",{parentName:"p"},"myattribute")," was added to the schema, resulting in schema version v2.\nDeltastreamer is capable of handling such schema evolution, if all incoming records were evolved and serialized with evolved schema. But the complication is that, some records are serialized with schema version v1 and some are serialized with schema version v2."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Schema evolution",src:t(25128).A}),(0,n.yg)("br",null),"\nThe default deserializer used by Hudi ",(0,n.yg)("inlineCode",{parentName:"p"},"io.confluent.kafka.serializers.KafkaAvroDeserializer")," uses the schema that the record was serialized with for deserialization. This causes Hudi to get records with multiple different schema from the kafka client. E.g. Event #13 has the new attribute ",(0,n.yg)("inlineCode",{parentName:"p"},"myattribute"),", Event #14 does not have the new attribute ",(0,n.yg)("inlineCode",{parentName:"p"},"myattribute"),". This makes things complicated and error-prone for Hudi."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Confluent Deserializer",src:t(2267).A}),(0,n.yg)("br",null)),(0,n.yg)("h2",{id:"solution"},"Solution"),(0,n.yg)("p",null,"Hudi added a new custom Deserializer ",(0,n.yg)("inlineCode",{parentName:"p"},"KafkaAvroSchemaDeserializer")," to solve this problem of different producers producing records in different schema versions, but to use the latest schema from schema registry to deserialize all the records.",(0,n.yg)("br",null),"\nAs first step the Deserializer gets the latest schema from the Hudi SchemaProvider. The SchemaProvider can get the schema for example from a Confluent Schema-Registry or a file.\nThe Deserializer then reads the records from the topic using the schema the record was written with. As next step it will convert all the records to the latest schema from the SchemaProvider, in our case the latest schema. As a result, the kafka client will return all records with a unified schema i.e. the latest schema as per schema registry. Hudi does not need to handle different schemas inside a single batch."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"KafkaAvroSchemaDeserializer",src:t(4653).A}),(0,n.yg)("br",null)),(0,n.yg)("h2",{id:"configurations"},"Configurations"),(0,n.yg)("p",null,"As of upcoming release 0.9.0, normal Confluent Deserializer is used by default. One has to explicitly set KafkaAvroSchemaDeserializer as below,\nin order to ensure smooth schema evolution with different producers producing records in different versions."),(0,n.yg)("p",null,(0,n.yg)("inlineCode",{parentName:"p"},"hoodie.deltastreamer.source.kafka.value.deserializer.class=org.apache.hudi.utilities.deser.KafkaAvroSchemaDeserializer")),(0,n.yg)("h2",{id:"conclusion"},"Conclusion"),(0,n.yg)("p",null,"Hope this blog helps in ingesting data from kafka into Hudi using Deltastreamer tool catering to different schema evolution\nneeds. Hudi has a very active development community and we look forward for more contributions.\nPlease check out ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/contribute/get-involved"},"this")," link to start contributing."))}g.isMDXComponent=!0},56944:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Improving Marker Mechanism in Apache Hudi",excerpt:"We introduce a new marker mechanism leveraging the timeline server to address performance bottlenecks due to rate-limiting on cloud storage like AWS S3.",author:"yihua",category:"blog",image:"/assets/images/blog/marker-mechanism/timeline-server-based-marker-mechanism.png",tags:["design","timeline-server","markers","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2021/08/18/improving-marker-mechanism",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-08-18-improving-marker-mechanism.md",source:"@site/blog/2021-08-18-improving-marker-mechanism.md",title:"Improving Marker Mechanism in Apache Hudi",description:"Hudi supports fully automatic cleanup of uncommitted data on storage during its write operations. Write operations in an Apache Hudi table use markers to efficiently track the data files written to storage. In this blog, we dive into the design of the existing direct marker file mechanism and explain its performance problems on cloud storage like AWS S3 for",date:"2021-08-18T00:00:00.000Z",formattedDate:"August 18, 2021",tags:[{label:"design",permalink:"/cn/blog/tags/design"},{label:"timeline-server",permalink:"/cn/blog/tags/timeline-server"},{label:"markers",permalink:"/cn/blog/tags/markers"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:8.605,truncated:!0,authors:[{name:"yihua"}],prevItem:{title:"Reliable ingestion from AWS S3 using Hudi",permalink:"/cn/blog/2021/08/23/s3-events-source"},nextItem:{title:"Adding support for Virtual Keys in Hudi",permalink:"/cn/blog/2021/08/18/virtual-keys"}},l={authorsImageUrls:[void 0]},d=[{value:"Need for Markers during Write Operations",id:"need-for-markers-during-write-operations",children:[],level:2},{value:"Existing Direct Marker Mechanism and its limitations",id:"existing-direct-marker-mechanism-and-its-limitations",children:[],level:2},{value:"Timeline-server-based marker mechanism improving write performance",id:"timeline-server-based-marker-mechanism-improving-write-performance",children:[],level:2},{value:"Marker-related write options",id:"marker-related-write-options",children:[],level:2},{value:"Performance",id:"performance",children:[],level:2},{value:"Conclusion",id:"conclusion",children:[],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...o}=e;return(0,n.yg)(p,(0,i.A)({},c,o,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"Hudi supports fully automatic cleanup of uncommitted data on storage during its write operations. Write operations in an Apache Hudi table use markers to efficiently track the data files written to storage. In this blog, we dive into the design of the existing direct marker file mechanism and explain its performance problems on cloud storage like AWS S3 for\nvery large writes. We demonstrate how we improve write performance with introduction of timeline-server-based markers."),(0,n.yg)("h2",{id:"need-for-markers-during-write-operations"},"Need for Markers during Write Operations"),(0,n.yg)("p",null,"A ",(0,n.yg)("strong",{parentName:"p"},"marker")," in Hudi, such as a marker file with a unique filename, is a label to indicate that a corresponding data file exists in storage, which then Hudi\nuses to automatically clean up uncommitted data during failure and rollback scenarios. Each marker entry is composed of three parts, the data file name,\nthe marker extension (",(0,n.yg)("inlineCode",{parentName:"p"},".marker"),"), and the I/O operation created the file (",(0,n.yg)("inlineCode",{parentName:"p"},"CREATE")," - inserts, ",(0,n.yg)("inlineCode",{parentName:"p"},"MERGE")," - updates/deletes, or ",(0,n.yg)("inlineCode",{parentName:"p"},"APPEND")," - either). For example, the marker ",(0,n.yg)("inlineCode",{parentName:"p"},"91245ce3-bb82-4f9f-969e-343364159174-0_140-579-0_20210820173605.parquet.marker.CREATE")," indicates\nthat the corresponding data file is ",(0,n.yg)("inlineCode",{parentName:"p"},"91245ce3-bb82-4f9f-969e-343364159174-0_140-579-0_20210820173605.parquet")," and the I/O type is ",(0,n.yg)("inlineCode",{parentName:"p"},"CREATE"),". Hudi creates a marker before creating the corresponding data file in the file system and deletes all markers pertaining to a commit when it succeeds."),(0,n.yg)("p",null,"The markers are useful for efficiently carrying out different operations by the write client.  Markers serve as a way to track data files of interest rather than scanning the whole Hudi table by listing all files in the table.  Two important operations use markers which come in handy to find uncommitted data files of interest efficiently:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Removing duplicate/partial data files"),": in Spark, the Hudi write client delegates the data file writing to multiple executors.  One executor can fail the task, leaving partial data files written, and Spark retries the task in this case until it succeeds. When speculative execution is enabled, there can also be multiple successful attempts at writing out the same data into different files, only one of which is finally handed to the Spark driver process for committing. The markers help efficiently identify the partial data files written, which contain duplicate data compared to the data files written by the successful trial later, and these duplicate data files are cleaned up when the commit is finalized.  If there are no such marker to track the per-commit data files, we have to list all files in the file system, correlate that with the files seen in timeline and then delete the ones that belong to partial write failures.  As you could imagine, this would be very costly in a very large installation of a datalake."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Rolling back failed commits"),": the write operation can fail in the middle, leaving some data files written in storage.  In this case, the marker entries stay in storage as the commit is failed.  In the next write operation, the write client rolls back the failed commit before proceeding with the new write. The rollback is done with the help of markers to identify the data files written as part of the failed commit.")),(0,n.yg)("p",null,"Next, we dive into the existing marker mechanism, explain its performance problem, and demonstrate the new timeline-server-based marker mechanism to address the problem."),(0,n.yg)("h2",{id:"existing-direct-marker-mechanism-and-its-limitations"},"Existing Direct Marker Mechanism and its limitations"),(0,n.yg)("p",null,"The ",(0,n.yg)("strong",{parentName:"p"},"existing marker mechanism")," simply creates a new marker file corresponding to each data file, with the marker filename as described above.  The marker file does not have any content, i.e., empty.  Each marker file is written to storage in the same directory hierarchy, i.e., commit instant and partition path, under a temporary folder ",(0,n.yg)("inlineCode",{parentName:"p"},".hoodie/.temp")," under the base path of the Hudi table.  For example, the figure below shows one example of the marker files created and the corresponding data files when writing data to the Hudi table.  When getting or deleting all the marker file paths, the mechanism first lists all the paths under the temporary folder, ",(0,n.yg)("inlineCode",{parentName:"p"},".hoodie/.temp/<commit_instant>"),", and then does the operation."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"An example of marker and data files in direct marker file mechanism",src:t(64052).A})),(0,n.yg)("p",null,"While it's much efficient over scanning the entire table for uncommitted data files, as the number of data files to write increases, so does the number of marker files to create.  For large writes which need to write significant number of data files, e.g., 10K or more, this can create performance bottlenecks for cloud storage such as AWS S3.  In AWS S3, each file create and delete call triggers an HTTP request and there is ",(0,n.yg)("a",{parentName:"p",href:"https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html"},"rate-limiting")," on how many requests can be processed per second per prefix in a bucket.  When the number of data files to write concurrently and the number of marker files is huge, the marker file operations could take up non-trivial time during the write operation, sometimes on the order of a few minutes or more.  Users may barely notice this on a storage like HDFS, where the file system metadata is efficiently cached in memory."),(0,n.yg)("h2",{id:"timeline-server-based-marker-mechanism-improving-write-performance"},"Timeline-server-based marker mechanism improving write performance"),(0,n.yg)("p",null,"To address the performance bottleneck due to rate-limiting of AWS S3 explained above, we introduce a ",(0,n.yg)("strong",{parentName:"p"},"new marker mechanism leveraging the timeline server"),", which optimizes the marker-related latency for storage with non-trivial file I/O latency.  The ",(0,n.yg)("strong",{parentName:"p"},"timeline server")," in Hudi serves as a centralized place for providing the file system and timeline views. As shown below, the new timeline-server-based marker mechanism delegates the marker creation and other marker-related operations from individual executors to the timeline server for centralized processing.  The timeline server batches the marker creation requests and writes the markers to a bounded set of files in the file system at regular intervals.  In such a way, the number of actual file operations and latency related to markers can be significantly reduced even with a huge number of data files, thus improving the performance of the writes."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Timeline-server-based marker mechanism",src:t(77597).A})),(0,n.yg)("p",null,"To improve the efficiency of processing marker creation requests, we design the batched handling of marker requests at the timeline server. Each marker creation request is handled asynchronously in the Javalin timeline server and queued before processing. For every batch interval, e.g., 20ms, the timeline server pulls the pending marker creation requests from the queue and writes all markers to the next file in a round robin fashion.  Inside the timeline server, such batch processing is multi-threaded, designed and implemented to guarantee consistency and correctness.  Both the batch interval and the batch concurrency can be configured through the write options."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Batched processing of marker creation requests",src:t(46585).A})),(0,n.yg)("p",null,"Note that the worker thread always checks whether the marker has already been created by comparing the marker name from the request with the memory copy of all markers maintained at the timeline server. The underlying files storing the markers are only read upon the first marker request (lazy loading).  The responses of requests are only sent back once the new markers are flushed to the files, so that in the case of the timeline server failure, the timeline server can recover the already created markers. These ensure consistency between storage and the in-memory copy, and improve the performance of processing marker requests."),(0,n.yg)("h2",{id:"marker-related-write-options"},"Marker-related write options"),(0,n.yg)("p",null,"We introduce the following new marker-related write options in ",(0,n.yg)("inlineCode",{parentName:"p"},"0.9.0")," release, to configure the marker mechanism.  Note that the timeline-server-based marker mechanism is not yet supported for HDFS in ",(0,n.yg)("inlineCode",{parentName:"p"},"0.9.0")," release, and we plan to support the timeline-server-based marker mechanism for HDFS in the future."),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Property Name"),(0,n.yg)("th",{parentName:"tr",align:null},"Default"),(0,n.yg)("th",{parentName:"tr",align:"center"},"Meaning"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.write.markers.type")),(0,n.yg)("td",{parentName:"tr",align:null},"direct"),(0,n.yg)("td",{parentName:"tr",align:"center"},"Marker type to use.  Two modes are supported: (1) ",(0,n.yg)("inlineCode",{parentName:"td"},"direct"),": individual marker file corresponding to each data file is directly created by the executor; (2) ",(0,n.yg)("inlineCode",{parentName:"td"},"timeline_server_based"),": marker operations are all handled at the timeline service which serves as a proxy.  New marker entries are batch processed and stored in a limited number of underlying files for efficiency.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.markers.timeline_server_based.batch.num_threads")),(0,n.yg)("td",{parentName:"tr",align:null},"20"),(0,n.yg)("td",{parentName:"tr",align:"center"},"Number of threads to use for batch processing marker creation requests at the timeline server.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.markers.timeline_server_based.batch.interval_ms")),(0,n.yg)("td",{parentName:"tr",align:null},"50"),(0,n.yg)("td",{parentName:"tr",align:"center"},"The batch interval in milliseconds for marker creation batch processing.")))),(0,n.yg)("h2",{id:"performance"},"Performance"),(0,n.yg)("p",null,"We evaluate the write performance over both direct and timeline-server-based marker mechanisms by bulk-inserting a large dataset using Amazon EMR with Spark and S3. The input data is around 100GB.  We configure the write operation to generate a large number of data files concurrently by setting the max parquet file size to be 1MB and parallelism to be 240.  Note that it is unlikely to set max parquet file size to 1MB in production and such a setup is only to evaluate the performance regarding the marker mechanisms. As we noted before, while the latency of direct marker mechanism is acceptable for incremental writes with smaller number of data files written, it increases dramatically for large bulk inserts/writes which produce much more data files."),(0,n.yg)("p",null,"As shown below, direct marker mechanism works really well, when a part of the table is written, e.g., 1K out of 165K data files.  However, the time of direct marker operations is non-trivial when we need to write significant number of data files. Compared to the direct marker mechanism, the timeline-server-based marker mechanism generates much fewer files storing markers because of the batch processing, leading to much less time on marker-related I/O operations, thus achieving 31% lower write completion time compared to the direct marker file mechanism."),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Marker Type"),(0,n.yg)("th",{parentName:"tr",align:null},"Total Files"),(0,n.yg)("th",{parentName:"tr",align:"center"},"Num data files written"),(0,n.yg)("th",{parentName:"tr",align:"center"},"Files created for markers"),(0,n.yg)("th",{parentName:"tr",align:"center"},"Marker deletion time"),(0,n.yg)("th",{parentName:"tr",align:"center"},"Bulk Insert Time (including marker deletion)"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Direct"),(0,n.yg)("td",{parentName:"tr",align:null},"165k"),(0,n.yg)("td",{parentName:"tr",align:"center"},"1k"),(0,n.yg)("td",{parentName:"tr",align:"center"},"1k"),(0,n.yg)("td",{parentName:"tr",align:"center"},"5.4secs"),(0,n.yg)("td",{parentName:"tr",align:"center"},"-")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Direct"),(0,n.yg)("td",{parentName:"tr",align:null},"165k"),(0,n.yg)("td",{parentName:"tr",align:"center"},"165k"),(0,n.yg)("td",{parentName:"tr",align:"center"},"165k"),(0,n.yg)("td",{parentName:"tr",align:"center"},"15min"),(0,n.yg)("td",{parentName:"tr",align:"center"},"55min")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Timeline-server-based"),(0,n.yg)("td",{parentName:"tr",align:null},"165k"),(0,n.yg)("td",{parentName:"tr",align:"center"},"165k"),(0,n.yg)("td",{parentName:"tr",align:"center"},"20"),(0,n.yg)("td",{parentName:"tr",align:"center"},"~3s"),(0,n.yg)("td",{parentName:"tr",align:"center"},"38min")))),(0,n.yg)("h2",{id:"conclusion"},"Conclusion"),(0,n.yg)("p",null,"We identify that for large writes which need to write significant number of data files, the existing direct marker file mechanism can incur performance bottlenecks due to the rate-limiting of file create and delete calls on cloud storage like AWS S3.  To address this issue, we introduce a new marker mechanism leveraging the timeline server, which delegates the marker creation and other marker-related operations from individual executors to the timeline server and uses batch processing to improve performance.  Performance evaluations on Amazon EMR with Spark and S3 show that the marker-related I/O latency and overall write time are reduced."))}g.isMDXComponent=!0},44870:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Adding support for Virtual Keys in Hudi",excerpt:"Supporting Virtual keys in Hudi for reducing storage overhead",author:"shivnarayan",category:"blog",tags:["design","metadata","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2021/08/18/virtual-keys",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-08-18-virtual-keys.md",source:"@site/blog/2021-08-18-virtual-keys.md",title:"Adding support for Virtual Keys in Hudi",description:"Apache Hudi helps you build and manage data lakes with different table types, config knobs to cater to everyone's need.",date:"2021-08-18T00:00:00.000Z",formattedDate:"August 18, 2021",tags:[{label:"design",permalink:"/cn/blog/tags/design"},{label:"metadata",permalink:"/cn/blog/tags/metadata"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:4.95,truncated:!0,authors:[{name:"shivnarayan"}],prevItem:{title:"Improving Marker Mechanism in Apache Hudi",permalink:"/cn/blog/2021/08/18/improving-marker-mechanism"},nextItem:{title:"Schema evolution with DeltaStreamer using KafkaSource",permalink:"/cn/blog/2021/08/16/kafka-custom-deserializer"}},l={authorsImageUrls:[void 0]},d=[{value:"Virtual Key support",id:"virtual-key-support",children:[{value:"Configurations",id:"configurations",children:[{value:"Supported Key Generators with CopyOnWrite(COW) table:",id:"supported-key-generators-with-copyonwritecow-table",children:[],level:4},{value:"Supported Key Generators with MergeOnRead(MOR) table:",id:"supported-key-generators-with-mergeonreadmor-table",children:[],level:4},{value:"Supported Index types:",id:"supported-index-types",children:[],level:4}],level:3},{value:"Supported Operations",id:"supported-operations",children:[],level:3},{value:"Sample Output",id:"sample-output",children:[],level:3},{value:"Incremental Queries",id:"incremental-queries",children:[],level:3},{value:"Conclusion",id:"conclusion",children:[],level:3}],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...t}=e;return(0,n.yg)(p,(0,i.A)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"Apache Hudi helps you build and manage data lakes with different table types, config knobs to cater to everyone's need.\nHudi adds per record metadata fields like ",(0,n.yg)("inlineCode",{parentName:"p"},"_hoodie_record_key"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"_hoodie_partition path"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"_hoodie_commit_time")," which serves multiple purposes.\nThey assist in avoiding re-computing the record key, partition path during merges, compaction and other table operations\nand also assists in supporting ",(0,n.yg)("a",{parentName:"p",href:"/blog/2021/07/21/streaming-data-lake-platform#readers"},"record-level")," incremental queries (in comparison to other table formats, that merely track files).\nIn addition, it ensures data quality by ensuring unique key constraints are enforced even if the key field changes for a given table, during its lifetime.\nBut one of the repeated asks from the community is to leverage existing fields and not to add additional meta fields, for simple use-cases where such benefits are not desired or key changes are very rare.  "),(0,n.yg)("h2",{id:"virtual-key-support"},"Virtual Key support"),(0,n.yg)("p",null,"Hudi now supports virtual keys, where Hudi meta fields can be computed on demand from the data fields. Currently, the meta fields are\ncomputed once and stored as per record metadata and re-used across various operations. If one does not need incremental query support,\nthey can start leveraging Hudi's Virtual key support and still go about using Hudi to build and manage their data lake to reduce the storage\noverhead due to per record metadata. "),(0,n.yg)("h3",{id:"configurations"},"Configurations"),(0,n.yg)("p",null,"Virtual keys can be enabled for a given table using the below config. When set to ",(0,n.yg)("inlineCode",{parentName:"p"},"hoodie.populate.meta.fields=false"),",\nHudi will use virtual keys for the corresponding table. Default value for this config is ",(0,n.yg)("inlineCode",{parentName:"p"},"true"),", which means, all  meta fields will be added by default."),(0,n.yg)("p",null,"Once virtual keys are enabled, it can't be disabled for a given hudi table, because already stored records may not have\nthe meta fields populated. But if you have an existing table from an older version of hudi, virtual keys can be enabled.\nAnother constraint w.r.t virtual key support is that, Key generator properties for a given table cannot be changed through\nthe course of the lifecycle of a given hudi table. In this model, the user also shares responsibility of ensuring uniqueness\nof key within a table. For instance, if you configure record key to point to ",(0,n.yg)("inlineCode",{parentName:"p"},"field_5")," for few batches of write and later switch to ",(0,n.yg)("inlineCode",{parentName:"p"},"field_10"),",\nHudi cannot guarantee uniqueness of key, since older writes could have had duplicates for ",(0,n.yg)("inlineCode",{parentName:"p"},"field_10"),". "),(0,n.yg)("p",null,"With virtual keys, keys will have to be re-computed everytime when in need (merges, compaction, MOR snapshot read). Hence we\nsupport virtual keys for all built-in key generators on Copy-On-Write tables. Supporting all key generators on Merge-On-Read table\nwould entail reading all fields out of base and delta logs, sacrificing core columnar query performance, which will be prohibitively expensive\nfor users. Thus, we support only simple key generators (the default key generator, where both record key and partition path refer\nto an existing field ) for now."),(0,n.yg)("h4",{id:"supported-key-generators-with-copyonwritecow-table"},"Supported Key Generators with CopyOnWrite(COW) table:"),(0,n.yg)("p",null,"SimpleKeyGenerator, ComplexKeyGenerator, CustomKeyGenerator, TimestampBasedKeyGenerator and NonpartitionedKeyGenerator. "),(0,n.yg)("h4",{id:"supported-key-generators-with-mergeonreadmor-table"},"Supported Key Generators with MergeOnRead(MOR) table:"),(0,n.yg)("p",null,"SimpleKeyGenerator"),(0,n.yg)("h4",{id:"supported-index-types"},"Supported Index types:"),(0,n.yg)("p",null,'Only "SIMPLE" and "GLOBAL_SIMPLE" index types are supported in the first cut. We plan to add support for other index\n(BLOOM, etc) in future releases. '),(0,n.yg)("h3",{id:"supported-operations"},"Supported Operations"),(0,n.yg)("p",null,"All existing features are supported for a hudi table with virtual keys, except the incremental\nqueries. Which means, cleaning, archiving, metadata table, clustering, etc can be enabled for a hudi table with\nvirtual keys enabled. So, you are able to merely use Hudi as a transactional table format with all the awesome\ntable service runtimes and platform services, if you wish to do so, without incurring any overheads associated with\nsupport for incremental data processing."),(0,n.yg)("h3",{id:"sample-output"},"Sample Output"),(0,n.yg)("p",null,"As called out earlier, one has to set ",(0,n.yg)("inlineCode",{parentName:"p"},"hoodie.populate.meta.fields=false")," to enable virtual keys. Let's see the\ndifference between records of a hudi table with and without virtual keys."),(0,n.yg)("p",null,"Here are some sample records for a regular hudi table (virtual keys disabled)"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"+--------------------+--------------------------------------+--------------------------------------+---------+---------+-------------------+\n|_hoodie_commit_time |           _hoodie_record_key         |        _hoodie_partition_path        |  rider  | driver  |        fare       |\n+--------------------+--------------------------------------+--------------------------------------+---------+---------+-------------------+\n|   20210825154123   | eb7819f1-6f04-429d-8371-df77620b9527 | americas/united_states/san_francisco |rider-284|driver-284|98.3428192817987  |\n|   20210825154123   | 37ea44f1-fda7-4ec4-84de-f43f5b5a4d84 | americas/united_states/san_francisco |rider-213|driver-213|19.179139106643607|\n|   20210825154123   | aa601d6b-7cc5-4b82-9687-675d0081616e | americas/united_states/san_francisco |rider-213|driver-213|93.56018115236618 |\n|   20210825154123   | 494bc080-881c-48be-8f8a-8f1739781816 | americas/united_states/san_francisco |rider-284|driver-284|90.9053809533154  |\n|   20210825154123   | 09573277-e1c1-4cdd-9b45-57176f184d4d | americas/united_states/san_francisco |rider-284|driver-284|49.527694252432056|\n|   20210825154123   | c9b055ed-cd28-4397-9704-93da8b2e601f | americas/brazil/sao_paulo            |rider-213|driver-213|43.4923811219014  |\n|   20210825154123   | e707355a-b8c0-432d-a80f-723b93dc13a8 | americas/brazil/sao_paulo            |rider-284|driver-284|63.72504913279929 |\n|   20210825154123   | d3c39c9e-d128-497a-bf3e-368882f45c28 | americas/brazil/sao_paulo            |rider-284|driver-284|91.99515909032544 |\n|   20210825154123   | 159441b0-545b-460a-b671-7cc2d509f47b | asia/india/chennai                   |rider-284|driver-284|9.384124531808036 |\n|   20210825154123   | 16031faf-ad8d-4968-90ff-16cead211d3c | asia/india/chennai                   |rider-284|driver-284|90.25710109008239 |\n+--------------------+--------------------------------------+--------------------------------------+---------+----------+------------------+\n")),(0,n.yg)("p",null,"And here are some sample records for a hudi table with virtual keys enabled."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"+--------------------+------------------------+-------------------------+---------+---------+-------------------+\n|_hoodie_commit_time |    _hoodie_record_key  |  _hoodie_partition_path |  rider  | driver  |        fare       |\n+--------------------+------------------------+-------------------------+---------+---------+-------------------+\n|        null        |            null        |          null           |rider-284|driver-284|98.3428192817987  |\n|        null        |            null        |          null           |rider-213|driver-213|19.179139106643607|\n|        null        |            null        |          null           |rider-213|driver-213|93.56018115236618 |\n|        null        |            null        |          null           |rider-284|driver-284|90.9053809533154  |\n|        null        |            null        |          null           |rider-284|driver-284|49.527694252432056|\n|        null        |            null        |          null           |rider-213|driver-213|43.4923811219014  |\n|        null        |            null        |          null           |rider-284|driver-284|63.72504913279929 |\n|        null        |            null        |          null           |rider-284|driver-284|91.99515909032544 |\n|        null        |            null        |          null           |rider-284|driver-284|9.384124531808036 |\n|        null        |            null        |          null           |rider-284|driver-284|90.25710109008239 |\n+--------------------+------------------------+-------------------------+---------+----------+------------------+\n")),(0,n.yg)("div",{className:"admonition admonition-note alert alert--secondary"},(0,n.yg)("div",{parentName:"div",className:"admonition-heading"},(0,n.yg)("h5",{parentName:"div"},(0,n.yg)("span",{parentName:"h5",className:"admonition-icon"},(0,n.yg)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,n.yg)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,n.yg)("div",{parentName:"div",className:"admonition-content"},(0,n.yg)("p",{parentName:"div"},"As you could see, all meta fields are null in storage, but all users fields remain intact similar to a regular table."))),(0,n.yg)("h3",{id:"incremental-queries"},"Incremental Queries"),(0,n.yg)("p",null,"Since hudi does not maintain any metadata (like commit time at a record level) for a table with virtual keys enabled,",(0,n.yg)("br",{parentName:"p"}),"\n","incremental queries are not supported. An exception will be thrown as below when an incremental query is triggered for such\na table."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},'scala> val tripsIncrementalDF = spark.read.format("hudi").\n     |   option(QUERY_TYPE_OPT_KEY, QUERY_TYPE_INCREMENTAL_OPT_VAL).\n     |   option(BEGIN_INSTANTTIME_OPT_KEY, "20210827180901").load(basePath)\norg.apache.hudi.exception.HoodieException: Incremental queries are not supported when meta fields are disabled\n  at org.apache.hudi.IncrementalRelation.<init>(IncrementalRelation.scala:69)\n  at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:120)\n  at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:67)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:344)\n  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)\n  at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)\n  at scala.Option.getOrElse(Option.scala:189)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:232)\n  ... 61 elided\n')),(0,n.yg)("h3",{id:"conclusion"},"Conclusion"),(0,n.yg)("p",null,"Hope this blog was useful for you to learn yet another feature in Apache Hudi. If you are interested in\nHudi and looking to contribute, do check out ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/contribute/get-involved"},"here"),"."))}g.isMDXComponent=!0},54355:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Asynchronous Clustering using Hudi",excerpt:"How to setup Hudi for asynchronous clustering",author:"codope",category:"blog",image:"/assets/images/blog/clustering/example_perf_improvement.png",tags:["design","clustering","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2021/08/23/async-clustering",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-08-23-async-clustering.md",source:"@site/blog/2021-08-23-async-clustering.md",title:"Asynchronous Clustering using Hudi",description:"In one of the previous blog posts, we introduced a new",date:"2021-08-23T00:00:00.000Z",formattedDate:"August 23, 2021",tags:[{label:"design",permalink:"/cn/blog/tags/design"},{label:"clustering",permalink:"/cn/blog/tags/clustering"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:6.055,truncated:!0,authors:[{name:"codope"}],prevItem:{title:"Building an ExaByte-level Data Lake Using Apache Hudi at ByteDance",permalink:"/cn/blog/2021/09/01/building-eb-level-data-lake-using-hudi-at-bytedance"},nextItem:{title:"Reliable ingestion from AWS S3 using Hudi",permalink:"/cn/blog/2021/08/23/s3-events-source"}},l={authorsImageUrls:[void 0]},d=[{value:"Introduction",id:"introduction",children:[],level:2},{value:"Clustering Strategies",id:"clustering-strategies",children:[{value:"Plan Strategy",id:"plan-strategy",children:[],level:3},{value:"Execution Strategy",id:"execution-strategy",children:[],level:3},{value:"Update Strategy",id:"update-strategy",children:[],level:3}],level:2},{value:"Asynchronous Clustering",id:"asynchronous-clustering",children:[{value:"HoodieClusteringJob",id:"hoodieclusteringjob",children:[],level:3},{value:"HoodieDeltaStreamer",id:"hoodiedeltastreamer",children:[],level:3},{value:"Spark Structured Streaming",id:"spark-structured-streaming",children:[],level:3}],level:2},{value:"Conclusion and Future Work",id:"conclusion-and-future-work",children:[],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...t}=e;return(0,n.yg)(p,(0,i.A)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"In one of the ",(0,n.yg)("a",{parentName:"p",href:"/blog/2021/01/27/hudi-clustering-intro"},"previous blog")," posts, we introduced a new\nkind of table service called clustering to reorganize data for improved query performance without compromising on\ningestion speed. We learnt how to setup inline clustering. In this post, we will discuss what has changed since then and\nsee how asynchronous clustering can be setup using HoodieClusteringJob as well as DeltaStreamer utility."),(0,n.yg)("h2",{id:"introduction"},"Introduction"),(0,n.yg)("p",null,"On a high level, clustering creates a plan based on a configurable strategy, groups eligible files based on specific\ncriteria and then executes the plan. Hudi supports ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/concurrency_control#enabling-multi-writing"},"multi-writers")," which provides\nsnapshot isolation between multiple table services, thus allowing writers to continue with ingestion while clustering\nruns in the background. For a more detailed overview of the clustering architecture please check out the previous blog\npost."),(0,n.yg)("h2",{id:"clustering-strategies"},"Clustering Strategies"),(0,n.yg)("p",null,"As mentioned before, clustering plan as well as execution depends on configurable strategy. These strategies can be\nbroadly classified into three types: clustering plan strategy, execution strategy and update strategy."),(0,n.yg)("h3",{id:"plan-strategy"},"Plan Strategy"),(0,n.yg)("p",null,"This strategy comes into play while creating clustering plan. It helps to decide what file groups should be clustered.\nLet's look at different plan strategies that are available with Hudi. Note that these strategies are easily pluggable\nusing this ",(0,n.yg)("a",{parentName:"p",href:"/docs/next/configurations#hoodieclusteringplanstrategyclass"},"config"),"."),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("inlineCode",{parentName:"li"},"SparkSizeBasedClusteringPlanStrategy"),": It selects file slices based on\nthe ",(0,n.yg)("a",{parentName:"li",href:"/docs/next/configurations/#hoodieclusteringplanstrategysmallfilelimit"},"small file limit"),"\nof base files and creates clustering groups upto max file size allowed per group. The max size can be specified using\nthis ",(0,n.yg)("a",{parentName:"li",href:"/docs/next/configurations/#hoodieclusteringplanstrategymaxbytespergroup"},"config"),". This\nstrategy is useful for stitching together medium-sized files into larger ones to reduce lot of files spread across\ncold partitions."),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("inlineCode",{parentName:"li"},"SparkRecentDaysClusteringPlanStrategy"),": It looks back previous 'N' days partitions and creates a plan that will\ncluster the 'small' file slices within those partitions. This is the default strategy. It could be useful when the\nworkload is predictable and data is partitioned by time."),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("inlineCode",{parentName:"li"},"SparkSelectedPartitionsClusteringPlanStrategy"),": In case you want to cluster only specific partitions within a range,\nno matter how old or new are those partitions, then this strategy could be useful. To use this strategy, one needs\nto set below two configs additionally (both begin and end partitions are inclusive):")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"hoodie.clustering.plan.strategy.cluster.begin.partition\nhoodie.clustering.plan.strategy.cluster.end.partition\n")),(0,n.yg)("div",{className:"admonition admonition-note alert alert--secondary"},(0,n.yg)("div",{parentName:"div",className:"admonition-heading"},(0,n.yg)("h5",{parentName:"div"},(0,n.yg)("span",{parentName:"h5",className:"admonition-icon"},(0,n.yg)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,n.yg)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,n.yg)("div",{parentName:"div",className:"admonition-content"},(0,n.yg)("p",{parentName:"div"},"All the strategies are partition-aware and the latter two are still bound by the size limits of the first strategy."))),(0,n.yg)("h3",{id:"execution-strategy"},"Execution Strategy"),(0,n.yg)("p",null,"After building the clustering groups in the planning phase, Hudi applies execution strategy, for each group, primarily\nbased on sort columns and size. The strategy can be specified using this ",(0,n.yg)("a",{parentName:"p",href:"/docs/next/configurations/#hoodieclusteringexecutionstrategyclass"},"config"),"."),(0,n.yg)("p",null,(0,n.yg)("inlineCode",{parentName:"p"},"SparkSortAndSizeExecutionStrategy")," is the default strategy. Users can specify the columns to sort the data by, when\nclustering using\nthis ",(0,n.yg)("a",{parentName:"p",href:"/docs/next/configurations/#hoodieclusteringplanstrategysortcolumns"},"config"),". Apart from\nthat, we can also set ",(0,n.yg)("a",{parentName:"p",href:"/docs/next/configurations/#hoodieparquetmaxfilesize"},"max file size"),"\nfor the parquet files produced due to clustering. The strategy uses bulk insert to write data into new files, in which\ncase, Hudi implicitly uses a partitioner that does sorting based on specified columns. In this way, the strategy changes\nthe data layout in a way that not only improves query performance but also balance rewrite overhead automatically."),(0,n.yg)("p",null,"Now this strategy can be executed either as a single spark job or multiple jobs depending on number of clustering groups\ncreated in the planning phase. By default, Hudi will submit multiple spark jobs and union the results. In case you want\nto force Hudi to use single spark job, set the execution strategy\nclass ",(0,n.yg)("a",{parentName:"p",href:"/docs/next/configurations/#hoodieclusteringexecutionstrategyclass"},"config"),"\nto ",(0,n.yg)("inlineCode",{parentName:"p"},"SingleSparkJobExecutionStrategy"),"."),(0,n.yg)("h3",{id:"update-strategy"},"Update Strategy"),(0,n.yg)("p",null,"Currently, clustering can only be scheduled for tables/partitions not receiving any concurrent updates. By default,\nthe ",(0,n.yg)("a",{parentName:"p",href:"/docs/next/configurations/#hoodieclusteringupdatesstrategy"},"config for update strategy")," is\nset to ",(0,n.yg)("strong",{parentName:"p"},(0,n.yg)("em",{parentName:"strong"},"SparkRejectUpdateStrategy")),". If some file group has updates during clustering then it will reject updates and\nthrow an exception. However, in some use-cases updates are very sparse and do not touch most file groups. The default\nstrategy to simply reject updates does not seem fair. In such use-cases, users can set the config to ",(0,n.yg)("strong",{parentName:"p"},(0,n.yg)("em",{parentName:"strong"},"SparkAllowUpdateStrategy")),"."),(0,n.yg)("p",null,"We discussed the critical strategy configurations. All other configurations related to clustering are\nlisted ",(0,n.yg)("a",{parentName:"p",href:"/docs/next/configurations/#Clustering-Configs"},"here"),". Out of this list, a few\nconfigurations that will be very useful are:"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Config key"),(0,n.yg)("th",{parentName:"tr",align:null},"Remarks"),(0,n.yg)("th",{parentName:"tr",align:null},"Default"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.clustering.async.enabled")),(0,n.yg)("td",{parentName:"tr",align:null},"Enable running of clustering service, asynchronously as writes happen on the table."),(0,n.yg)("td",{parentName:"tr",align:null},"False")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.clustering.async.max.commits")),(0,n.yg)("td",{parentName:"tr",align:null},"Control frequency of async clustering by specifying after how many commits clustering should be triggered."),(0,n.yg)("td",{parentName:"tr",align:null},"4")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"hoodie.clustering.preserve.commit.metadata")),(0,n.yg)("td",{parentName:"tr",align:null},"When rewriting data, preserves existing _hoodie_commit_time. This means users can run incremental queries on clustered data without any side-effects."),(0,n.yg)("td",{parentName:"tr",align:null},"False")))),(0,n.yg)("h2",{id:"asynchronous-clustering"},"Asynchronous Clustering"),(0,n.yg)("p",null,"Previously, we have seen how users\ncan ",(0,n.yg)("a",{parentName:"p",href:"/blog/2021/01/27/hudi-clustering-intro#setting-up-clustering"},"setup inline clustering"),".\nAdditionally, users can\nleverage ",(0,n.yg)("a",{parentName:"p",href:"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+19+Clustering+data+for+freshness+and+query+performance#RFC19Clusteringdataforfreshnessandqueryperformance-SetupforAsyncclusteringJob"},"HoodieClusteringJob"),"\nto setup 2-step asynchronous clustering."),(0,n.yg)("h3",{id:"hoodieclusteringjob"},"HoodieClusteringJob"),(0,n.yg)("p",null,"With the release of Hudi version 0.9.0, we can schedule as well as execute clustering in the same step. We just need to\nspecify the ",(0,n.yg)("inlineCode",{parentName:"p"},"\u2014mode")," or ",(0,n.yg)("inlineCode",{parentName:"p"},"-m")," option. There are three modes:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("inlineCode",{parentName:"li"},"schedule"),": Make a clustering plan. This gives an instant which can be passed in execute mode."),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("inlineCode",{parentName:"li"},"execute"),": Execute a clustering plan at given instant which means --instant-time is required here."),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("inlineCode",{parentName:"li"},"scheduleAndExecute"),": Make a clustering plan first and execute that plan immediately.")),(0,n.yg)("p",null,"Note that to run this job while the original writer is still running, please enable multi-writing:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"hoodie.write.concurrency.mode=optimistic_concurrency_control\nhoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.ZookeeperBasedLockProvider\n")),(0,n.yg)("p",null,"A sample spark-submit command to setup HoodieClusteringJob is as below:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-bash"},"spark-submit \\\n--class org.apache.hudi.utilities.HoodieClusteringJob \\\n/path/to/hudi-utilities-bundle/target/hudi-utilities-bundle_2.12-0.9.0-SNAPSHOT.jar \\\n--props /path/to/config/clusteringjob.properties \\\n--mode scheduleAndExecute \\\n--base-path /path/to/hudi_table/basePath \\\n--table-name hudi_table_schedule_clustering \\\n--spark-memory 1g\n")),(0,n.yg)("p",null,"A sample ",(0,n.yg)("inlineCode",{parentName:"p"},"clusteringjob.properties")," file:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"hoodie.clustering.async.enabled=true\nhoodie.clustering.async.max.commits=4\nhoodie.clustering.plan.strategy.target.file.max.bytes=1073741824\nhoodie.clustering.plan.strategy.small.file.limit=629145600\nhoodie.clustering.execution.strategy.class=org.apache.hudi.client.clustering.run.strategy.SparkSortAndSizeExecutionStrategy\nhoodie.clustering.plan.strategy.sort.columns=column1,column2\n")),(0,n.yg)("h3",{id:"hoodiedeltastreamer"},"HoodieDeltaStreamer"),(0,n.yg)("p",null,"This brings us to our users' favorite utility in Hudi. Now, we can trigger asynchronous clustering with DeltaStreamer.\nJust set the ",(0,n.yg)("inlineCode",{parentName:"p"},"hoodie.clustering.async.enabled")," config to true and specify other clustering config in properties file\nwhose location can be pased as ",(0,n.yg)("inlineCode",{parentName:"p"},"\u2014props")," when starting the deltastreamer (just like in the case of HoodieClusteringJob)."),(0,n.yg)("p",null,"A sample spark-submit command to setup HoodieDeltaStreamer is as below:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-bash"},"spark-submit \\\n--class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer \\\n/path/to/hudi-utilities-bundle/target/hudi-utilities-bundle_2.12-0.9.0-SNAPSHOT.jar \\\n--props /path/to/config/clustering_kafka.properties \\\n--schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \\\n--source-class org.apache.hudi.utilities.sources.AvroKafkaSource \\\n--source-ordering-field impresssiontime \\\n--table-type COPY_ON_WRITE \\\n--target-base-path /path/to/hudi_table/basePath \\\n--target-table impressions_cow_cluster \\\n--op INSERT \\\n--hoodie-conf hoodie.clustering.async.enabled=true \\\n--continuous\n")),(0,n.yg)("h3",{id:"spark-structured-streaming"},"Spark Structured Streaming"),(0,n.yg)("p",null,"We can also enable asynchronous clustering with Spark structured streaming sink as shown below. "),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-scala"},'val commonOpts = Map(\n   "hoodie.insert.shuffle.parallelism" -> "4",\n   "hoodie.upsert.shuffle.parallelism" -> "4",\n   DataSourceWriteOptions.RECORDKEY_FIELD.key -> "_row_key",\n   DataSourceWriteOptions.PARTITIONPATH_FIELD.key -> "partition",\n   DataSourceWriteOptions.PRECOMBINE_FIELD.key -> "timestamp",\n   HoodieWriteConfig.TBL_NAME.key -> "hoodie_test"\n)\n\ndef getAsyncClusteringOpts(isAsyncClustering: String, \n                           clusteringNumCommit: String, \n                           executionStrategy: String):Map[String, String] = {\n   commonOpts + (DataSourceWriteOptions.ASYNC_CLUSTERING_ENABLE.key -> isAsyncClustering,\n           HoodieClusteringConfig.ASYNC_CLUSTERING_MAX_COMMITS.key -> clusteringNumCommit,\n           HoodieClusteringConfig.EXECUTION_STRATEGY_CLASS_NAME.key -> executionStrategy\n   )\n}\n\ndef initStreamingWriteFuture(hudiOptions: Map[String, String]): Future[Unit] = {\n   val streamingInput = // define the source of streaming\n   Future {\n      println("streaming starting")\n      streamingInput\n              .writeStream\n              .format("org.apache.hudi")\n              .options(hudiOptions)\n              .option("checkpointLocation", basePath + "/checkpoint")\n              .mode(Append)\n              .start()\n              .awaitTermination(10000)\n      println("streaming ends")\n   }\n}\n\ndef structuredStreamingWithClustering(): Unit = {\n   val df = //generate data frame\n   val hudiOptions = getClusteringOpts("true", "1", "org.apache.hudi.client.clustering.run.strategy.SparkSortAndSizeExecutionStrategy")\n   val f1 = initStreamingWriteFuture(hudiOptions)\n   Await.result(f1, Duration.Inf)\n}\n')),(0,n.yg)("h2",{id:"conclusion-and-future-work"},"Conclusion and Future Work"),(0,n.yg)("p",null,"In this post, we discussed different clustering strategies and how to setup asynchronous clustering. The story is not\nover yet and future work entails:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Support clustering with updates."),(0,n.yg)("li",{parentName:"ul"},"CLI tools to support clustering.")),(0,n.yg)("p",null,"Please follow this ",(0,n.yg)("a",{parentName:"p",href:"https://issues.apache.org/jira/browse/HUDI-1042"},"JIRA")," to learn more about active development on\nthis issue. We look forward to contributions from the community. Hope you enjoyed this post. Put your Hudi on and keep\nstreaming!"))}g.isMDXComponent=!0},9160:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Reliable ingestion from AWS S3 using Hudi",excerpt:"From listing to log-based approach, a reliable way of ingesting data from AWS S3 into Hudi.",author:"codope",category:"blog",image:"/assets/images/blog/s3_events_source_design.png",tags:["design","deltastreamer","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2021/08/23/s3-events-source",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-08-23-s3-events-source.md",source:"@site/blog/2021-08-23-s3-events-source.md",title:"Reliable ingestion from AWS S3 using Hudi",description:"In this post we will talk about a new deltastreamer source which reliably and efficiently processes new data files as they arrive in AWS S3.",date:"2021-08-23T00:00:00.000Z",formattedDate:"August 23, 2021",tags:[{label:"design",permalink:"/cn/blog/tags/design"},{label:"deltastreamer",permalink:"/cn/blog/tags/deltastreamer"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:5.53,truncated:!0,authors:[{name:"codope"}],prevItem:{title:"Asynchronous Clustering using Hudi",permalink:"/cn/blog/2021/08/23/async-clustering"},nextItem:{title:"Improving Marker Mechanism in Apache Hudi",permalink:"/cn/blog/2021/08/18/improving-marker-mechanism"}},l={authorsImageUrls:[void 0]},d=[{value:"Design",id:"design",children:[],level:2},{value:"Advantages",id:"advantages",children:[],level:2},{value:"Configuration and Setup",id:"configuration-and-setup",children:[],level:2},{value:"Conclusion and Future Work",id:"conclusion-and-future-work",children:[],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...o}=e;return(0,n.yg)(p,(0,i.A)({},c,o,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"In this post we will talk about a new deltastreamer source which reliably and efficiently processes new data files as they arrive in AWS S3.\nAs of today, to ingest data from S3 into Hudi, users leverage DFS source whose ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/178767948e906f673d6d4a357c65c11bc574f619/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DFSPathSelector.java"},"path selector")," would identify the source files modified since the last checkpoint based on max modification time.\nThe problem with this approach is that modification time precision is upto seconds in S3. It maybe possible that there were many files (beyond what the configurable source limit allows) modifed in that second and some files might be skipped.\nFor more details, please refer to ",(0,n.yg)("a",{parentName:"p",href:"https://issues.apache.org/jira/browse/HUDI-1723"},"HUDI-1723"),".\nWhile the workaround is to ignore the source limit and keep reading, the problem motivated us to redesign so that users can reliably ingest from S3."),(0,n.yg)("h2",{id:"design"},"Design"),(0,n.yg)("p",null,"For use-cases where seconds granularity does not suffice, we have a new source in deltastreamer using log-based approach.\nThe new ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/178767948e906f673d6d4a357c65c11bc574f619/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/S3EventsSource.java"},"S3 events source")," relies on change notification and incremental processing to ingest from S3.\nThe architecture is as shown in the figure below."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Different components in the design",src:t(87711).A})),(0,n.yg)("p",null,"In this approach, users need to ",(0,n.yg)("a",{parentName:"p",href:"https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html"},"enable S3 event notifications"),".\nThere will be two types of deltastreamers as detailed below. "),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("a",{parentName:"li",href:"https://github.com/apache/hudi/blob/178767948e906f673d6d4a357c65c11bc574f619/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/S3EventsSource.java"},"S3EventsSource"),": Create Hudi S3 metadata table.\nThis source leverages AWS ",(0,n.yg)("a",{parentName:"li",href:"https://aws.amazon.com/sns"},"SNS")," and ",(0,n.yg)("a",{parentName:"li",href:"https://aws.amazon.com/sqs/"},"SQS")," services that subscribe to file events from the source bucket.",(0,n.yg)("pre",{parentName:"li"},(0,n.yg)("code",{parentName:"pre"},"- Events from SQS will be written to this table, which serves as a changelog for the subsequent incremental puller.\n- When the events are committed to the S3 metadata table they will be deleted from SQS.\n"))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("a",{parentName:"li",href:"https://github.com/apache/hudi/blob/178767948e906f673d6d4a357c65c11bc574f619/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/S3EventsHoodieIncrSource.java"},"S3EventsHoodieIncrSource")," and uses the metadata table written by S3EventsSource.",(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Read the S3 metadata table and get the objects that were added or modified. These objects contain the S3 path for the source files that were added or modified."),(0,n.yg)("li",{parentName:"ul"},"Write to Hudi table with source data corresponding to the source files in the S3 bucket.")))),(0,n.yg)("h2",{id:"advantages"},"Advantages"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Decoupling"),": Every step in the pipeline is decoupled. The two sources can be started independent of each other. We imagine most users run a single deltastreamer to get all changes for a given bucket and can fan-out multiple tables off that."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Performance and Scale"),": The previous approach used to list all files, sort by modification time and then filter based on checkpoint. While it did prune partition paths, the directory listing could still become a bottleneck. By relying on change notification and native cloud APIs, the new approach avoids directory listing and scales with the number of files being ingested."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Reliability"),": Since there is no longer any dependency on the max modification time and the fact that S3 events are being recorded in the metadata table, users can rest assured that all the events will be processed eventually."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Fault Tolerance"),": There are two levels of fault toerance in this design. Firstly, if some of the messages are not committed to the S3 metadata table, then those messages will remain in the queue so that they can be reprocessed in the next round. Secondly, if the incremental puller fails, then users can query the S3 metadata table for the last commit point and resume the incremental puller from that point onwards (kinda like how Kafka consumers can reset offset)."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Asynchronous backfills"),': With the log-based approach, it becomes much easier to trigger backfills. See the "Conclusion and Future Work" section for more details.')),(0,n.yg)("h2",{id:"configuration-and-setup"},"Configuration and Setup"),(0,n.yg)("p",null,"Users only need to specify the SQS queue url and region name to start the S3EventsSource (metadata source)."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"hoodie.deltastreamer.s3.source.queue.url=https://sqs.us-west-2.amazonaws.com/queue/url\nhoodie.deltastreamer.s3.source.queue.region=us-west-2\n")),(0,n.yg)("p",null,"There are a few other configurations for the metadata source which can be tuned to suit specific requirements:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("em",{parentName:"li"},(0,n.yg)("inlineCode",{parentName:"em"},"hoodie.deltastreamer.s3.source.queue.long.poll.wait")),": Value can range in ","[0, 20]"," seconds. If set to 0 then metadata source will consume messages from SQS using short polling. It is recommended to use long polling because it will reduce false empty responses and reduce the cost of using SQS. By default, this value is set to 20 seconds."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("em",{parentName:"li"},(0,n.yg)("inlineCode",{parentName:"em"},"hoodie.deltastreamer.s3.source.queue.visibility.timeout")),": Value can range in ","[0, 43200]"," seconds (i.e. max 12 hours). SQS does not automatically delete the messages once consumed. It is the responsibility of metadata source to delete the message after committing. SQS will move the consumed message to in-flight state during which it becomes invisible for the configured timeout period. By default, this value is set to 30 seconds."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("em",{parentName:"li"},(0,n.yg)("inlineCode",{parentName:"em"},"hoodie.deltastreamer.s3.source.queue.max.messages.per.batch")),": Maximum number of messages in a batch of one round of metadata source run. By default, this value is set to 5.")),(0,n.yg)("p",null,"To setup the pipeline, first ",(0,n.yg)("a",{parentName:"p",href:"https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html"},"enable S3 event notifications"),".\nDownload the ",(0,n.yg)("a",{parentName:"p",href:"https://mvnrepository.com/artifact/com.amazonaws/aws-java-sdk-sqs"},"aws-java-sdk-sqs")," jar.\nThen start the S3EventsSource and  S3EventsHoodieIncrSource using the HoodieDeltaStreamer utility as shown in sample commands below."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-bash"},'# To start S3EventsSource\nspark-submit \\\n--jars "/home/hadoop/hudi-utilities-bundle_2.11-0.9.0.jar,/usr/lib/spark/external/lib/spark-avro.jar,/home/hadoop/aws-java-sdk-sqs-1.12.22.jar" \\\n--master yarn --deploy-mode client \\\n--class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer /home/hadoop/hudi-packages/hudi-utilities-bundle_2.11-0.9.0-SNAPSHOT.jar \\\n--table-type COPY_ON_WRITE --source-ordering-field eventTime \\\n--target-base-path s3://bucket_name/path/for/s3_meta_table \\\n--target-table s3_meta_table  --continuous \\\n--min-sync-interval-seconds 10 \\\n--hoodie-conf hoodie.datasource.write.recordkey.field="s3.object.key,eventName" \\\n--hoodie-conf hoodie.datasource.write.keygenerator.class=org.apache.hudi.keygen.ComplexKeyGenerator \\\n--hoodie-conf hoodie.datasource.write.partitionpath.field=s3.bucket.name --enable-hive-sync \\\n--hoodie-conf hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor \\\n--hoodie-conf hoodie.datasource.write.hive_style_partitioning=true \\\n--hoodie-conf hoodie.datasource.hive_sync.database=default \\\n--hoodie-conf hoodie.datasource.hive_sync.table=s3_meta_table \\\n--hoodie-conf hoodie.datasource.hive_sync.partition_fields=bucket \\\n--source-class org.apache.hudi.utilities.sources.S3EventsSource \\\n--hoodie-conf hoodie.deltastreamer.s3.source.queue.url=https://sqs.us-west-2.amazonaws.com/queue/url\n--hoodie-conf hoodie.deltastreamer.s3.source.queue.region=us-west-2\n\n# To start S3EventsHoodieIncrSource use following command along with ordering field, record key(s) and \n# partition field(s) from the source s3 data.\nspark-submit \\\n--jars "/home/hadoop/hudi-utilities-bundle_2.11-0.9.0.jar,/usr/lib/spark/external/lib/spark-avro.jar,/home/hadoop/aws-java-sdk-sqs-1.12.22.jar" \\\n--master yarn --deploy-mode client \\\n--class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer /home/hadoop/hudi-packages/hudi-utilities-bundle_2.11-0.9.0-SNAPSHOT.jar \\\n--table-type COPY_ON_WRITE \\\n--source-ordering-field <ordering key from source data> --target-base-path s3://bucket_name/path/for/s3_hudi_table \\\n--target-table s3_hudi_table  --continuous --min-sync-interval-seconds 10 \\\n--hoodie-conf hoodie.datasource.write.recordkey.field="<record key from source data>" \\\n--hoodie-conf hoodie.datasource.write.keygenerator.class=org.apache.hudi.keygen.SimpleKeyGenerator \\\n--hoodie-conf hoodie.datasource.write.partitionpath.field=<partition key from source data> --enable-hive-sync \\\n--hoodie-conf hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor \\\n--hoodie-conf hoodie.datasource.write.hive_style_partitioning=true \\\n--hoodie-conf hoodie.datasource.hive_sync.database=default \\\n--hoodie-conf hoodie.datasource.hive_sync.table=s3_hudi_v6 \\\n--hoodie-conf hoodie.datasource.hive_sync.partition_fields=bucket \\\n--source-class org.apache.hudi.utilities.sources.S3EventsHoodieIncrSource \\\n--hoodie-conf hoodie.deltastreamer.source.hoodieincr.path=s3://bucket_name/path/for/s3_meta_table \\\n--hoodie-conf hoodie.deltastreamer.source.hoodieincr.read_latest_on_missing_ckpt=true\n')),(0,n.yg)("h2",{id:"conclusion-and-future-work"},"Conclusion and Future Work"),(0,n.yg)("p",null,"This post introduced a log-based approach to ingest data from S3 into Hudi tables reliably and efficiently. We are actively improving this along the following directions."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"One stream of work is to add support for other cloud-based object storage like Google Cloud Storage, Azure Blob Storage, etc. with this revamped design."),(0,n.yg)("li",{parentName:"ul"},"Another stream of work is to add resource manager that allows users to setup notifications and delete resources when no longer needed."),(0,n.yg)("li",{parentName:"ul"},"Another interesting piece of work is to support ",(0,n.yg)("strong",{parentName:"li"},"asynchronous backfills"),". Notification systems are evntually consistent and typically do not guarantee perfect delivery of all files right away. The log-based approach provides enough flexibility to trigger automatic backfills at a configurable interval e.g. once a day or once a week.")),(0,n.yg)("p",null,"Please follow this ",(0,n.yg)("a",{parentName:"p",href:"https://issues.apache.org/jira/browse/HUDI-1896"},"JIRA")," to learn more about active development on this issue.\nWe look forward to contributions from the community. Hope you enjoyed this post. "),(0,n.yg)("p",null,"Put your Hudi on and keep streaming!"))}g.isMDXComponent=!0},46013:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Building an ExaByte-level Data Lake Using Apache Hudi at ByteDance",excerpt:"Ziyue Guan from Bytedance shares the production experience of building an ExaByte-level data lake using Apache Hudi and how it is used in the recommendation system at Bytedance.",author:"Ziyue Guan, translated to English by yihua",category:"blog",image:"/assets/images/blog/bytedance_hudi.png",tags:["use-case","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2021/09/01/building-eb-level-data-lake-using-hudi-at-bytedance",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-09-01-building-eb-level-data-lake-using-hudi-at-bytedance.md",source:"@site/blog/2021-09-01-building-eb-level-data-lake-using-hudi-at-bytedance.md",title:"Building an ExaByte-level Data Lake Using Apache Hudi at ByteDance",description:"Ziyue Guan from Bytedance shares the experience of building an ExaByte(EB)-level data lake using Apache Hudi at Bytedance.",date:"2021-09-01T00:00:00.000Z",formattedDate:"September 1, 2021",tags:[{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:8.61,truncated:!0,authors:[{name:"Ziyue Guan, translated to English by yihua"}],prevItem:{title:"Data Platform 2.0 - Part I",permalink:"/cn/blog/2021/10/05/Data-Platform-2.0-Part-I"},nextItem:{title:"Asynchronous Clustering using Hudi",permalink:"/cn/blog/2021/08/23/async-clustering"}},l={authorsImageUrls:[void 0]},d=[{value:"Scenario Requirements",id:"scenario-requirements",children:[],level:2},{value:"Design Decisions",id:"design-decisions",children:[],level:2},{value:"Functionality Support",id:"functionality-support",children:[],level:2},{value:"Performance Tuning",id:"performance-tuning",children:[],level:2},{value:"Future Work",id:"future-work",children:[],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...o}=e;return(0,n.yg)(p,(0,i.A)({},c,o,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"Ziyue Guan from Bytedance shares the experience of building an ExaByte(EB)-level data lake using Apache Hudi at Bytedance."),(0,n.yg)("p",null,"This blog is a translated version of ",(0,n.yg)("a",{parentName:"p",href:"https://mp.weixin.qq.com/s/oZz_2HzPCWgzxwZO0nuDUQ"},"the same blog originally in Chinese/\u4e2d\u6587"),".  Here are the ",(0,n.yg)("a",{target:"_blank",href:t(41963).A},"original slides in Chinese/\u4e2d\u6587")," and ",(0,n.yg)("a",{target:"_blank",href:t(66558).A},"the translated slides in English"),"."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"slide1 title",src:t(17519).A})),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"slide2 agenda",src:t(82260).A})),(0,n.yg)("p",null,"Next, I will explain how we use Hudi in Bytedance\u2019s recommendation system in five parts: scenario requirements, design decisions, functionality support, performance tuning, and future work."),(0,n.yg)("h2",{id:"scenario-requirements"},"Scenario Requirements"),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"slide3 scenario requirements",src:t(3389).A}),"\n",(0,n.yg)("img",{alt:"slide4 scenario diagram",src:t(7042).A}),"\n",(0,n.yg)("img",{alt:"slide5 scenario details",src:t(28171).A})),(0,n.yg)("p",null,"In the recommendation system, we use the data lake in the following two scenarios:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"We use BigTable as the data storage for the near real-time processing in the entire system. There is an internally developed component TBase, which provides the semantics of BigTable and the abstraction of some requirements in the search advertisement recommendation scenarios, and shields the differences in underlying storage. For a better understanding, it can be directly regarded as an HBase. In this process, in order to serve offline data analysis and mining needs, the data needs to be exported to offline storage. In the past, users either use MR/Spark to directly access the storage, or obtain data by scanning the database, which do not meet the data access characteristics in the OLAP scenario. Therefore, we build BigTable's CDC based on the data lake to improve data timeliness, reduce the access pressure of the near real-time system, and provide efficient OLAP access and user-friendly SQL consumption methods.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"In addition, we also use data lakes in the scenarios of feature engineering and model training. We obtain two types of real-time data streams from internal and external sources. One is the instances returned from the system, which includes the features obtained when the recommendation system is serving. The other is the feedback from event tracking at vantage points and a variety of complex external data sources. This type of data is used as labels and forms a complete machine learning data sample with the previously mentioned features. For this scenario, we need to implement a merging operation based on the primary key to merge the instance and label together.  The time window range may be as long as tens of days, with the volume at the order of hundreds of billions of rows. The system needs to support efficient column selection and predicate pushdown. At the same time, it also needs to support concurrent updates and other related capabilities."))),(0,n.yg)("p",null,"These two scenarios pose the following challenges:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"The data is very irregular.")," Compared with Binlog, WAL cannot obtain all the information of a row, and the data size changes significantly.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"The throughput is relatively large."),"  The throughput of a single table exceeds ",(0,n.yg)("strong",{parentName:"p"},"100 GB/s"),", and the single table needs ",(0,n.yg)("strong",{parentName:"p"},"PB-level")," storage.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"The data schema is complex.")," The data is highly dimensional and sparse.  The number of table columns ranges from 1000 to 10000+. And there are a lot of complex data types."))),(0,n.yg)("h2",{id:"design-decisions"},"Design Decisions"),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"slide6 design decisions",src:t(74672).A}),"\n",(0,n.yg)("img",{alt:"slide7 design details",src:t(71577).A})),(0,n.yg)("p",null,"When making the decision on the engine, we examine three of the most popular data lake engines, ",(0,n.yg)("strong",{parentName:"p"},"Hudi"),", ",(0,n.yg)("strong",{parentName:"p"},"Iceberg"),", and ",(0,n.yg)("strong",{parentName:"p"},"DeltaLake"),". These three have their own advantages and disadvantages in our scenarios. Finally, ",(0,n.yg)("strong",{parentName:"p"},"Hudi")," is selected as the storage engine based on Hudi's openness to the upstream and downstream ecosystems, support for the global index, and customized development interfaces for certain storage logic."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"For real-time writing, MOR with better timeliness is selected.")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"We examine the index type.  First of all, because WAL can't get the partition of the data each time, it must use a global index. Among several global index implementations, in order to achieve high-performance writing, HBase is the only choice. The other two implementations have major performance gaps from HBase.")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"Regarding the computing engine and API, Hudi's support for Flink was not perfect at the time, so we choose Spark which has more mature support. In order to flexibly implement some customized functionality and logic, and because the DataFrame API has more semantic restrictions, we choose the lower-level RDD API."))),(0,n.yg)("h2",{id:"functionality-support"},"Functionality Support"),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"slide8 functionality support",src:t(82190).A})),(0,n.yg)("p",null,"Functionality support includes MVCC and Schema registration systems that store semantics."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"slide9 mvcc",src:t(25207).A})),(0,n.yg)("p",null,"First of all, in order to support WAL write, we implement the payload for MVCC, and based on Avro, we customized a set of data structure implementation with timestamp. This logic is hidden from users through view access. In addition, we also implement the HBase append semantics, which realizes the appending to the List type instead of overwriting."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"slide10 schema",src:t(47295).A})),(0,n.yg)("p",null,"Because Hudi obtains the schema from write data, it is not convenient for working with other systems.  We also need some extensions based on the schema, so we build a metadata center to provide metadata-related operations."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"First of all, we realized atomic changes and multi-site high availability based on the semantics provided by internal storage. Users can atomically trigger schema changes through the interface and get the results immediately.")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"Achieves versioning of the Schema by adding the version number. After having the version number, we can easily use the schema instead of passing JSON object back and forth. With multiple versions, schema evolution can also be flexibly achieved.")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"We also support additional information encoding at the column level to help the business achieve special extended functionality in some scenarios. We replace column names with IDs to save the cost in the storage process.")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"When the Spark job with Hudi is running, it builds a local cache at the JVM level and syncs the data with the metadata center through the pull method, to achieve rapid access to the schema and singleton instance of the in-process schema."))),(0,n.yg)("h2",{id:"performance-tuning"},"Performance Tuning"),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"slide11 performance tuning",src:t(50358).A})),(0,n.yg)("p",null,"In our scenario, the performance challenges are huge. ",(0,n.yg)("strong",{parentName:"p"},"The maximum data volume of a single table reaches 400PB+, the daily volume increase is PB level, and the total data volume reaches EB level.")," Therefore, we have done some work to improve performance based on the performance and data characteristics."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"slide12 serialization",src:t(64333).A})),(0,n.yg)("p",null,"Serialization includes the following optimizations:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"Schema: the cost of data serialization using Avro is very expensive which consumes a lot of compute resources. To address this problem, we first use the singleton schema instance in JVM to avoid CPU-consuming comparison operations during the serialization process.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"By optimizing the payload logic, the number of times of running serialization is reduced.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"With the help of a third-party Avro serialization implementation, the serialization process is compiled into bytecode to improve the speed of SerDe and reduce memory usage. The serialization process has been modified to ensure that our complex schema can also be compiled properly."))),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"slide13 compaction",src:t(10436).A})),(0,n.yg)("p",null,"The optimization of the compaction process is as follows."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"In addition to the default Inline/Async compaction options, Hudi also supports flexible deployment of compaction. The characteristics of the compaction job are quite different from the ingestion job. In the same Spark application, it not only is impossible to set targeted settings but also has the problem of insufficient resource flexibility. We first build an independently deployed script so that the compaction job can be triggered and run independently. A low-cost mixed queue is used for resource scheduling for the compaction plan. In addition, we have also developed a compaction strategy based on rules and heuristics. The user's requirement is usually to guarantee a day-level or hour-level SLA, and targeted compression of data in certain partitions, so targeted compression capabilities are provided.")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"In order to shorten the time of critical compaction, we usually do compaction in advance to avoid all work being completed in a single compaction job. However, if a FileGroup compacted has a new update, it has to be compacted again. In order to optimize the overall efficiency, we made a heuristic scheduling of when a FileGroup should be compacted based on business logic to reduce additional compaction costs.  The actual benefits of this feature are still being evaluated.")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"Finally, we made some process optimizations for the compaction, such as not using WriteStatus's Cache and so on."))),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"slide14 hdfs sla",src:t(74299).A})),(0,n.yg)("p",null,"As storage designed for throughput, HDFS has serious real-time write glitches when the cluster usage level is relatively high. Through communication and cooperation with the HDFS team, some improvements have been done."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"First, we replace the original data HSync operation with HFlush to avoid disk I/O write amplification caused by distributed updates.")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"We make aggressive pipeline switching settings based on the scenario tuning, and the HDFS team has developed a flexible API that can control the pipeline to achieve flexible configurations in this scenario.")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"Finally, the timeliness of real-time writing is ensured through independent I/O isolation of log files."))),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"slide15 process optimization",src:t(59474).A})),(0,n.yg)("p",null,"There are also some small performance improvements, process modifications, and bug fixes. If you are interested, feel free to discuss that with me."),(0,n.yg)("h2",{id:"future-work"},"Future Work"),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"slide16 future work",src:t(90441).A}),"\n",(0,n.yg)("img",{alt:"slide17 future work details",src:t(95968).A})),(0,n.yg)("p",null,"In the future, we will continue to iterate in the following aspects."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Productization issues"),": The current way of using APIs and tuning parameters are highly demanding for the users, especially for the tuning, operation, and maintenance, which requires a deep understanding of Hudi principles to complete.  This hinders the promotion of that to users.")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Support issues for ecosystems"),": In our scenario, the technology stack is mainly on Flink, and the use of Flink will be explored in the future. In addition, the applications and environments used in upstream and downstream are complex, which requires cross-language and universal interface implementation. The current binding with Spark is cumbersome.")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Cost and performance issues"),": a common topic, since our scenario is relatively broad, the benefits from optimization are highly considerable.")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Storage semantics"),": We use Hudi as storage rather than a table format. Therefore, in the future, we plan to expand scenarios using Hudi, and need richer storage semantics.  We'll do more work in this area."))),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"slide19 hiring",src:t(44574).A})),(0,n.yg)("p",null,"Finally, an advertisement, our recommendation architecture team is responsible for the recommendation architecture design and development for products such as Douyin, Toutiao, and Xigua Video. The challenges are big and the growth is fast. Now we are hiring people and the working locations include: Beijing/Shanghai/Hangzhou/Singapore/Mountain View.  If you are  interested, you are welcomed to add WeChat ",(0,n.yg)("inlineCode",{parentName:"p"},"qinglingcannotfly")," or send your resume to the email: ",(0,n.yg)("inlineCode",{parentName:"p"},"guanziyue.gzy@bytedance.com"),"."))}g.isMDXComponent=!0},35612:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Data Platform 2.0 - Part I",authors:[{name:"Jitendra Shah"}],category:"blog",image:"/assets/images/blog/2021-10-05-data-platform-2-0-part-1.png",tags:["use-case","halodoc","datalake","datalake platform"]},s=void 0,l={permalink:"/cn/blog/2021/10/05/Data-Platform-2.0-Part-I",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-10-05-Data-Platform-2.0-Part-I.mdx",source:"@site/blog/2021-10-05-Data-Platform-2.0-Part-I.mdx",title:"Data Platform 2.0 - Part I",description:"Redirecting... please wait!!",date:"2021-10-05T00:00:00.000Z",formattedDate:"October 5, 2021",tags:[{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"halodoc",permalink:"/cn/blog/tags/halodoc"},{label:"datalake",permalink:"/cn/blog/tags/datalake"},{label:"datalake platform",permalink:"/cn/blog/tags/datalake-platform"}],readingTime:.045,truncated:!1,authors:[{name:"Jitendra Shah"}],prevItem:{title:"How Amazon Transportation Service enabled near-real-time event analytics at petabyte scale using AWS Glue with Apache Hudi",permalink:"/cn/blog/2021/10/14/How-Amazon-Transportation-Service-enabled-near-real-time-event-analytics-at-petabyte-scale-using-AWS-Glue-with-Apache-Hudi"},nextItem:{title:"Building an ExaByte-level Data Lake Using Apache Hudi at ByteDance",permalink:"/cn/blog/2021/09/01/building-eb-level-data-lake-using-hudi-at-bytedance"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://blogs.halodoc.io/data-platform-2-0-part-1/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},84129:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"How Amazon Transportation Service enabled near-real-time event analytics at petabyte scale using AWS Glue with Apache Hudi",authors:[{name:"Madhavan Sriram"},{name:"Diego Menin"},{name:"Gabriele Cacciola"},{name:"Kunal Gautam"}],category:"blog",image:"/assets/images/blog/2021-10-14-near-real-time-analytics-at-amazon-transportation-service.png",tags:["use-case","near real-time analytics","analytics at scale","amazon"]},s=void 0,l={permalink:"/cn/blog/2021/10/14/How-Amazon-Transportation-Service-enabled-near-real-time-event-analytics-at-petabyte-scale-using-AWS-Glue-with-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-10-14-How-Amazon-Transportation-Service-enabled-near-real-time-event-analytics-at-petabyte-scale-using-AWS-Glue-with-Apache-Hudi.mdx",source:"@site/blog/2021-10-14-How-Amazon-Transportation-Service-enabled-near-real-time-event-analytics-at-petabyte-scale-using-AWS-Glue-with-Apache-Hudi.mdx",title:"How Amazon Transportation Service enabled near-real-time event analytics at petabyte scale using AWS Glue with Apache Hudi",description:"Redirecting... please wait!!",date:"2021-10-14T00:00:00.000Z",formattedDate:"October 14, 2021",tags:[{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"near real-time analytics",permalink:"/cn/blog/tags/near-real-time-analytics"},{label:"analytics at scale",permalink:"/cn/blog/tags/analytics-at-scale"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[{name:"Madhavan Sriram"},{name:"Diego Menin"},{name:"Gabriele Cacciola"},{name:"Kunal Gautam"}],prevItem:{title:"Practice of Apache Hudi in building real-time data lake at station B",permalink:"/cn/blog/2021/10/21/Practice-of-Apache-Hudi-in-building-real-time-data-lake-at-station-B"},nextItem:{title:"Data Platform 2.0 - Part I",permalink:"/cn/blog/2021/10/05/Data-Platform-2.0-Part-I"}},d={authorsImageUrls:[void 0,void 0,void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/how-amazon-transportation-service-enabled-near-real-time-event-analytics-at-petabyte-scale-using-aws-glue-with-apache-hudi/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},90023:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Practice of Apache Hudi in building real-time data lake at station B",authors:[{name:"Yu Zhaojing"}],category:"blog",image:"/assets/images/blog/2021-10-21-station-b-real-time-data-lake-using-hudi.png",tags:["use-case","real-time datalake","developpaper"]},s=void 0,l={permalink:"/cn/blog/2021/10/21/Practice-of-Apache-Hudi-in-building-real-time-data-lake-at-station-B",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-10-21-Practice-of-Apache-Hudi-in-building-real-time-data-lake-at-station-B.mdx",source:"@site/blog/2021-10-21-Practice-of-Apache-Hudi-in-building-real-time-data-lake-at-station-B.mdx",title:"Practice of Apache Hudi in building real-time data lake at station B",description:"Redirecting... please wait!!",date:"2021-10-21T00:00:00.000Z",formattedDate:"October 21, 2021",tags:[{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"real-time datalake",permalink:"/cn/blog/tags/real-time-datalake"},{label:"developpaper",permalink:"/cn/blog/tags/developpaper"}],readingTime:.045,truncated:!1,authors:[{name:"Yu Zhaojing"}],prevItem:{title:"How GE Aviation built cloud-native data pipelines at enterprise scale using the AWS platform",permalink:"/cn/blog/2021/11/16/How-GE-Aviation-built-cloud-native-data-pipelines-at-enterprise-scale-using-the-AWS-platform"},nextItem:{title:"How Amazon Transportation Service enabled near-real-time event analytics at petabyte scale using AWS Glue with Apache Hudi",permalink:"/cn/blog/2021/10/14/How-Amazon-Transportation-Service-enabled-near-real-time-event-analytics-at-petabyte-scale-using-AWS-Glue-with-Apache-Hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://developpaper.com/practice-of-apache-hudi-in-building-real-time-data-lake-at-station-b/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},18577:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"How GE Aviation built cloud-native data pipelines at enterprise scale using the AWS platform",authors:[{name:"Alcuin Weidus"},{name:"Suresh Patnam"}],category:"blog",image:"/assets/images/blog/2021-11-16-ge-aviation-cloud-native-data-pipelines.png",tags:["use-case","analytics at scale","amazon"]},s=void 0,l={permalink:"/cn/blog/2021/11/16/How-GE-Aviation-built-cloud-native-data-pipelines-at-enterprise-scale-using-the-AWS-platform",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-11-16-How-GE-Aviation-built-cloud-native-data-pipelines-at-enterprise-scale-using-the-AWS-platform.mdx",source:"@site/blog/2021-11-16-How-GE-Aviation-built-cloud-native-data-pipelines-at-enterprise-scale-using-the-AWS-platform.mdx",title:"How GE Aviation built cloud-native data pipelines at enterprise scale using the AWS platform",description:"Redirecting... please wait!!",date:"2021-11-16T00:00:00.000Z",formattedDate:"November 16, 2021",tags:[{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"analytics at scale",permalink:"/cn/blog/tags/analytics-at-scale"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[{name:"Alcuin Weidus"},{name:"Suresh Patnam"}],prevItem:{title:"Apache Hudi Architecture Tools and Best Practices",permalink:"/cn/blog/2021/11/22/Apache-Hudi-Architecture-Tools-and-Best-Practices"},nextItem:{title:"Practice of Apache Hudi in building real-time data lake at station B",permalink:"/cn/blog/2021/10/21/Practice-of-Apache-Hudi-in-building-real-time-data-lake-at-station-B"}},d={authorsImageUrls:[void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/how-ge-aviation-built-cloud-native-data-pipelines-at-enterprise-scale-using-the-aws-platform/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},28696:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Apache Hudi Architecture Tools and Best Practices",authors:[{name:"Chandan Gaur"}],category:"blog",image:"/assets/images/blog/2021-11-22-hudi-architecture-tools-best-practices.png",tags:["blog","xenonstack"]},s=void 0,l={permalink:"/cn/blog/2021/11/22/Apache-Hudi-Architecture-Tools-and-Best-Practices",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-11-22-Apache-Hudi-Architecture-Tools-and-Best-Practices.mdx",source:"@site/blog/2021-11-22-Apache-Hudi-Architecture-Tools-and-Best-Practices.mdx",title:"Apache Hudi Architecture Tools and Best Practices",description:"Redirecting... please wait!!",date:"2021-11-22T00:00:00.000Z",formattedDate:"November 22, 2021",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"xenonstack",permalink:"/cn/blog/tags/xenonstack"}],readingTime:.045,truncated:!1,authors:[{name:"Chandan Gaur"}],prevItem:{title:"Lakehouse Concurrency Control: Are we too optimistic?",permalink:"/cn/blog/2021/12/16/lakehouse-concurrency-control-are-we-too-optimistic"},nextItem:{title:"How GE Aviation built cloud-native data pipelines at enterprise scale using the AWS platform",permalink:"/cn/blog/2021/11/16/How-GE-Aviation-built-cloud-native-data-pipelines-at-enterprise-scale-using-the-AWS-platform"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.xenonstack.com/insights/what-is-hudi",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},42243:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Lakehouse Concurrency Control: Are we too optimistic?",excerpt:"Vinoth Chandar, Creator of Apache Hudi, dives into concurrency control mechanisms",author:"vinoth",category:"blog",image:"/assets/images/blog/concurrency/MultiWriter.gif",tags:["blog","concurrency-control","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2021/12/16/lakehouse-concurrency-control-are-we-too-optimistic",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-12-16-lakehouse-concurrency-control-are-we-too-optimistic.md",source:"@site/blog/2021-12-16-lakehouse-concurrency-control-are-we-too-optimistic.md",title:"Lakehouse Concurrency Control: Are we too optimistic?",description:"Transactions on data lakes are now considered a key characteristic of a Lakehouse these days. But what has actually been accomplished so far? What are the current approaches? How do they fare in real-world scenarios? These questions are the focus of this blog.",date:"2021-12-16T00:00:00.000Z",formattedDate:"December 16, 2021",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"concurrency-control",permalink:"/cn/blog/tags/concurrency-control"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:7.535,truncated:!0,authors:[{name:"vinoth"}],prevItem:{title:"New features from Apache Hudi 0.7.0 and 0.8.0 available on Amazon EMR",permalink:"/cn/blog/2021/12/20/New-features-from-Apache-Hudi-0.7.0-and-0.8.0-available-on-Amazon-EMR"},nextItem:{title:"Apache Hudi Architecture Tools and Best Practices",permalink:"/cn/blog/2021/11/22/Apache-Hudi-Architecture-Tools-and-Best-Practices"}},l={authorsImageUrls:[void 0]},d=[{value:"Pitfalls in Lake Concurrency Control",id:"pitfalls-in-lake-concurrency-control",children:[],level:3},{value:"Model 1 : Single Writer, Inline Table Services",id:"model-1--single-writer-inline-table-services",children:[],level:3},{value:"Model 2 : Single Writer, Async Table Services",id:"model-2--single-writer-async-table-services",children:[],level:3},{value:"Model 3 : Multiple Writers",id:"model-3--multiple-writers",children:[],level:3}],c={toc:d},p="wrapper";function g(e){let{components:a,...o}=e;return(0,n.yg)(p,(0,i.A)({},c,o,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"Transactions on data lakes are now considered a key characteristic of a Lakehouse these days. But what has actually been accomplished so far? What are the current approaches? How do they fare in real-world scenarios? These questions are the focus of this blog. "),(0,n.yg)("p",null,"Having had the good fortune of working on diverse database projects - an RDBMS (",(0,n.yg)("a",{parentName:"p",href:"https://www.oracle.com/database/"},"Oracle"),"), a NoSQL key-value store (",(0,n.yg)("a",{parentName:"p",href:"https://www.slideshare.net/vinothchandar/voldemort-prototype-to-production-nectar-edits"},"Voldemort"),"), a streaming database (",(0,n.yg)("a",{parentName:"p",href:"https://www.confluent.io/blog/ksqldb-pull-queries-high-availability/"},"ksqlDB"),"), a closed-source real-time datastore and of course, Apache Hudi, I can safely say that the nature of workloads deeply influence the concurrency control mechanisms adopted in different databases. This blog will also describe how we rethought concurrency control for the data lake in Apache Hudi."),(0,n.yg)("p",null,"First, let's set the record straight. RDBMS databases offer the richest set of transactional capabilities and the widest array of concurrency control ",(0,n.yg)("a",{parentName:"p",href:"https://dev.mysql.com/doc/refman/5.7/en/innodb-locking-transaction-model.html"},"mechanisms"),". Different isolation levels, fine grained locking, deadlock detection/avoidance, and more are possible because they have to support row-level mutations and reads across many tables while enforcing ",(0,n.yg)("a",{parentName:"p",href:"https://dev.mysql.com/doc/refman/8.0/en/create-table-foreign-keys.html"},"key constraints")," and maintaining ",(0,n.yg)("a",{parentName:"p",href:"https://dev.mysql.com/doc/refman/8.0/en/create-table-secondary-indexes.html"},"indexes"),". NoSQL stores offer dramatically weaker guarantees like eventual-consistency and simple row level atomicity in exchange for greater scalability for simpler workloads. Drawing a similar parallel, traditional data warehouses offer more or less the full set of capabilities that you would find in an RDBMS, over columnar data, with locking and key constraints ",(0,n.yg)("a",{parentName:"p",href:"https://docs.teradata.com/r/a8IdS6iVHR77Z9RrIkmMGg/wFPZS4jwZgSG21GnOIpEsw"},"enforced")," whereas cloud data warehouses seem to have focused a lot more on separating the data and compute in architecture, while offering fewer isolation levels. As a surprising example, ",(0,n.yg)("a",{parentName:"p",href:"https://docs.snowflake.com/en/sql-reference/constraints-overview.html#supported-constraint-types"},"no enforcement")," of key constraints!"),(0,n.yg)("h3",{id:"pitfalls-in-lake-concurrency-control"},"Pitfalls in Lake Concurrency Control"),(0,n.yg)("p",null,"Historically, data lakes have been viewed as batch jobs reading/writing files on cloud storage and it's interesting to see how most new work extends this view and implements glorified file version control using some form of \"",(0,n.yg)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Optimistic_concurrency_control"},(0,n.yg)("strong",{parentName:"a"},"Optimistic concurrency control")),'" (OCC). With OCC jobs take a table level lock to check if they have impacted overlapping files and if a conflict exists, they abort their operations completely. Without naming names, the lock is sometimes even just a JVM level lock held on a single Apache Spark driver node. Once again, this may be okay for lightweight coordination of old school batch jobs that mostly append files to tables, but cannot be applied broadly to modern data lake workloads. Such approaches are built with immutable/append-only data models in mind, which are inadequate for incremental data processing or keyed updates/deletes. OCC is very optimistic that real contention never happens. Developer evangelism comparing OCC to the full fledged transactional capabilities of an RDBMS or a traditional data warehouse is rather misinformed. Quoting Wikipedia directly - "',(0,n.yg)("em",{parentName:"p"},"if contention for data resources is frequent, the cost of repeatedly restarting transactions hurts performance significantly, in which case other")," ",(0,n.yg)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Concurrency_control"},(0,n.yg)("em",{parentName:"a"},"concurrency control"))," ",(0,n.yg)("em",{parentName:"p"},"methods may be better suited."),' " When conflicts do occur, they can cause massive resource wastage since you have a batch job that fails after it ran for a few hours, during every attempt!'),(0,n.yg)("p",null,"Imagine a real-life scenario of two writer processes : an ingest writer job producing new data every 30 minutes and a deletion writer job that is enforcing GDPR, taking 2 hours to issue deletes. It's very likely for these to overlap files with random deletes, and the deletion job is almost guaranteed to starve and fail to commit each time. In database speak, mixing long running transactions with optimism leads to disappointment, since the longer the transactions the higher the probability they will overlap."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"concurrency",src:t(96101).A})),(0,n.yg)("p",null,"So, what's the alternative? Locking? Wikipedia also says - \"",(0,n.yg)("em",{parentName:"p"},'However, locking-based ("pessimistic") methods also can deliver poor performance because locking can drastically limit effective concurrency even when deadlocks are avoided.".')," Here is where Hudi takes a different approach, that we believe is more apt for modern lake transactions which are typically long-running and even continuous. Data lake workloads share more characteristics with high throughput stream processing jobs, than they do to standard reads/writes from a database and this is where we borrow from. In stream processing events are serialized into a single ordered log, avoiding any locks/concurrency bottlenecks and you can continuously process millions of events/sec. Hudi implements a file level, log based concurrency control protocol on the Hudi ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/timeline"},"timeline"),", which in-turn relies on bare minimum atomic puts to cloud storage. By building on an event log as the central piece for inter process coordination, Hudi is able to offer a few flexible deployment models that offer greater concurrency over pure OCC approaches that just track table snapshots."),(0,n.yg)("h3",{id:"model-1--single-writer-inline-table-services"},"Model 1 : Single Writer, Inline Table Services"),(0,n.yg)("p",null,"The simplest form of concurrency control is just no concurrency at all. A data lake table often has common services operating on it to ensure efficiency. Reclaiming storage space from older versions and logs, coalescing files (clustering in Hudi), merging deltas (compactions in Hudi), and more. Hudi can simply eliminate the need for concurrency control and maximizes throughput by supporting these table services out-of-box and running inline after every write to the table."),(0,n.yg)("p",null,"Execution plans are idempotent, persisted to the timeline and auto-recover from failures. For most simple use-cases, this means just writing is sufficient to get a well-managed table that needs no concurrency control."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"concurrency-single-writer",src:t(81007).A})),(0,n.yg)("h3",{id:"model-2--single-writer-async-table-services"},"Model 2 : Single Writer, Async Table Services"),(0,n.yg)("p",null,"Our delete/ingest example above is n't really that simple. While ingest/writer may just be updating the last N partitions on the table, delete may span across the entire table even. Mixing them in the same job, could slow down ingest latency by a lot. But, Hudi provides the option of running the table services in an async fashion, where most of the heavy lifting (e.g actually rewriting the columnar data by compaction service) is done asynchronously, eliminating any repeated wasteful retries whatsoever, while also optimizing the table using clustering techniques. Thus a single writer could consumes both regular updates and GDPR deletes and serialize them into a log. Given Hudi has record level indexing and avro log writes are much cheaper (as opposed to writing parquet, which can be 10x or more expensive), ingest latency can be sustained while enjoying great replayability. In fact, we were able to scale this model at ",(0,n.yg)("a",{parentName:"p",href:"https://eng.uber.com/uber-big-data-platform/"},"Uber"),", across 100s of petabytes, by sequencing all deletes & updates into the same source Apache Kafka topic. There's more to concurrency control than locking and Hudi accomplishes all this without needing any external locking."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"concurrency-async",src:t(6856).A})),(0,n.yg)("h3",{id:"model-3--multiple-writers"},"Model 3 : Multiple Writers"),(0,n.yg)("p",null,"But it's not always possible to serialize the deletes into the same write stream or sql based deletes are required. With multiple distributed processes, some form of locking is inevitable, but like real databases Hudi's concurrency model is intelligent enough to differentiate actual writing to the table, from table services that manage or optimize the table. Hudi offers similar optimistic concurrency control across multiple writers, but table services can still execute completely lock-free and async. This means the delete job can merely encode deletes and the ingest job can log updates, while the compaction service again applies the updates/deletes to base files. Even though the delete job and ingest job can contend and starve each other like like we mentioned above, their run-times are much lower and the wastage is drastically lower, since the compaction does the heavy-lifting of parquet/columnar data writing."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"concurrency-multi",src:t(56401).A})),(0,n.yg)("p",null,"All this said, there are still many ways we can improve upon this foundation."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"For starters, Hudi has already implemented a ",(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/blog/2021/08/18/improving-marker-mechanism/"},"marker mechanism")," that tracks all the files that are part of an active write transaction and a heartbeat mechanism that can track active writers to a table. This can be directly used by other active transactions/writers to detect what other writers are doing and ",(0,n.yg)("a",{parentName:"li",href:"https://issues.apache.org/jira/browse/HUDI-1575"},"abort early")," if conflicts are detected, yielding the cluster resources back to other jobs sooner."),(0,n.yg)("li",{parentName:"ul"},"While optimistic concurrency control is attractive when serializable snapshot isolation is desired, it's neither optimal nor the only method for dealing with concurrency between writers. We plan to implement a fully lock-free concurrency control using CRDTs and widely adopted stream processing concepts, over our log ",(0,n.yg)("a",{parentName:"li",href:"https://github.com/apache/hudi/blob/bc8bf043d5512f7afbb9d94882c4e43ee61d6f06/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordPayload.java#L38"},"merge API"),", that has already been ",(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/blog/2021/09/01/building-eb-level-data-lake-using-hudi-at-bytedance/#functionality-support"},"proven")," to sustain enormous continuous write volumes for the data lake."),(0,n.yg)("li",{parentName:"ul"},"Touching upon key constraints, Hudi is the only lake transactional layer that ensures unique ",(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/docs/key_generation"},"key")," constraints today, but limited to the record key of the table. We will be looking to expand this capability in a more general form to non-primary key fields, with the said newer concurrency models.")),(0,n.yg)("p",null,'Finally, for data lakes to transform successfully into lakehouses, we must learn from the failing of the "hadoop warehouse" vision, which shared similar goals with the new "lakehouse" vision. Designers did not pay closer attention to the missing technology gaps against warehouses and created unrealistic expectations from the actual software. As transactions and database functionality finally goes mainstream on data lakes, we must apply these lessons and remain candid about the current shortcomings. If you are building a lakehouse, I hope this post encourages you to closely consider various operational and efficiency aspects around concurrency control. Join our fast growing community by trying out ',(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/overview"},"Apache Hudi")," or join us in conversations on ",(0,n.yg)("a",{parentName:"p",href:"https://join.slack.com/t/apache-hudi/shared_invite/zt-2ggm1fub8-_yt4Reu9djwqqVRFC7X49g"},"Slack"),"."))}g.isMDXComponent=!0},36710:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"New features from Apache Hudi 0.7.0 and 0.8.0 available on Amazon EMR",authors:[{name:"Udit Mehrotra"},{name:"Gagan Brahmi"}],category:"blog",image:"/assets/images/blog/aws.jpg",tags:["blog","amazon"]},s=void 0,l={permalink:"/cn/blog/2021/12/20/New-features-from-Apache-Hudi-0.7.0-and-0.8.0-available-on-Amazon-EMR",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-12-20-New-features-from-Apache-Hudi-0.7.0-and-0.8.0-available-on-Amazon-EMR.mdx",source:"@site/blog/2021-12-20-New-features-from-Apache-Hudi-0.7.0-and-0.8.0-available-on-Amazon-EMR.mdx",title:"New features from Apache Hudi 0.7.0 and 0.8.0 available on Amazon EMR",description:"Redirecting... please wait!!",date:"2021-12-20T00:00:00.000Z",formattedDate:"December 20, 2021",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[{name:"Udit Mehrotra"},{name:"Gagan Brahmi"}],prevItem:{title:"Hudi Z-Order and Hilbert Space Filling Curves",permalink:"/cn/blog/2021/12/29/hudi-zorder-and-hilbert-space-filling-curves"},nextItem:{title:"Lakehouse Concurrency Control: Are we too optimistic?",permalink:"/cn/blog/2021/12/16/lakehouse-concurrency-control-are-we-too-optimistic"}},d={authorsImageUrls:[void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/new-features-from-apache-hudi-0-7-0-and-0-8-0-available-on-amazon-emr/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},34890:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Hudi Z-Order and Hilbert Space Filling Curves",excerpt:"Explore the benefits of new Apache Hudi Z-Order and Hilbert Curves",author:"Alexey Kudinkin and Tao Meng",category:"blog",image:"/assets/images/zordercurve.png",tags:["design","clustering","data skipping","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2021/12/29/hudi-zorder-and-hilbert-space-filling-curves",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-12-29-hudi-zorder-and-hilbert-space-filling-curves.md",source:"@site/blog/2021-12-29-hudi-zorder-and-hilbert-space-filling-curves.md",title:"Hudi Z-Order and Hilbert Space Filling Curves",description:"As of Hudi v0.10.0, we are excited to introduce support for an advanced Data Layout Optimization technique known in the database realm as Z-order and Hilbert space filling curves.",date:"2021-12-29T00:00:00.000Z",formattedDate:"December 29, 2021",tags:[{label:"design",permalink:"/cn/blog/tags/design"},{label:"clustering",permalink:"/cn/blog/tags/clustering"},{label:"data skipping",permalink:"/cn/blog/tags/data-skipping"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:8.5,truncated:!0,authors:[{name:"Alexey Kudinkin and Tao Meng"}],prevItem:{title:"The Art of Building Open Data Lakes with Apache Hudi, Kafka, Hive, and Debezium",permalink:"/cn/blog/2021/12/31/The-Art-of-Building-Open-Data-Lakes-with-Apache-Hudi-Kafka-Hive-and-Debezium"},nextItem:{title:"New features from Apache Hudi 0.7.0 and 0.8.0 available on Amazon EMR",permalink:"/cn/blog/2021/12/20/New-features-from-Apache-Hudi-0.7.0-and-0.8.0-available-on-Amazon-EMR"}},l={authorsImageUrls:[void 0]},d=[{value:"Background",id:"background",children:[],level:3},{value:"Setup",id:"setup",children:[],level:3},{value:"Testing",id:"testing",children:[],level:3},{value:"Results",id:"results",children:[],level:3},{value:"Epilogue",id:"epilogue",children:[],level:3}],c={toc:d},p="wrapper";function g(e){let{components:a,...t}=e;return(0,n.yg)(p,(0,i.A)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"As of Hudi v0.10.0, we are excited to introduce support for an advanced Data Layout Optimization technique known in the database realm as ",(0,n.yg)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Z-order_curve"},"Z-order")," and ",(0,n.yg)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Hilbert_curve"},"Hilbert")," space filling curves."),(0,n.yg)("h3",{id:"background"},"Background"),(0,n.yg)("p",null,"Amazon EMR team recently published a ",(0,n.yg)("a",{parentName:"p",href:"https://aws.amazon.com/blogs/big-data/new-features-from-apache-hudi-0-7-0-and-0-8-0-available-on-amazon-emr/"},"great article")," show-casing how ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/clustering"},"clustering")," your data can improve your ",(0,n.yg)("em",{parentName:"p"},"query performance"),"."),(0,n.yg)("p",null,"To better understand what's going on and how it's related to space-filling curves, let's zoom in to the setup in that article:"),(0,n.yg)("p",null,"In the article, 2 ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/overview"},"Apache Hudi")," tables are compared (both ingested from the well-known ",(0,n.yg)("a",{parentName:"p",href:"https://s3.amazonaws.com/amazon-reviews-pds/tsv/index.txt"},"Amazon Reviews")," dataset):"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"amazon_reviews")," table which is not clustered (ie the data has not been re-ordered by any particular key)"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"amazon_reviews_clustered")," table which is clustered. When data is clustered by Apache Hudi the data is ",(0,n.yg)("a",{parentName:"li",href:"https://en.wikipedia.org/wiki/Lexicographic_order"},(0,n.yg)("strong",{parentName:"a"},"lexicographically ordered"))," (hereon we will be referring to this kind of ordering as ",(0,n.yg)("strong",{parentName:"li"},(0,n.yg)("em",{parentName:"strong"},"linear ordering")),") by 2 columns: ",(0,n.yg)("inlineCode",{parentName:"li"},"star_rating"),", ",(0,n.yg)("inlineCode",{parentName:"li"},"total_votes")," (see screenshot below)")),(0,n.yg)("img",{src:"/assets/images/hudiconfigz.png",alt:"drawing",width:"800"}),(0,n.yg)("p",null,(0,n.yg)("em",{parentName:"p"},"Screenshot of the Hudi configuration (from Amazon EMR team article)")),(0,n.yg)("p",null,"To showcase the improvement in querying performance, the following queries are executed against both of these tables:"),(0,n.yg)("img",{src:"/assets/images/table1.png",alt:"drawing",width:"800"}),(0,n.yg)("img",{src:"/assets/images/table2.png",alt:"drawing",width:"800"}),(0,n.yg)("p",null,(0,n.yg)("em",{parentName:"p"},"Screenshots of the queries run against the previously setup tables (from Amazon EMR team article)")),(0,n.yg)("p",null,"The important consideration to point out here is that the queries were specifying ",(0,n.yg)("strong",{parentName:"p"},"both of the columns")," latter table is ordered by (both ",(0,n.yg)("inlineCode",{parentName:"p"},"star_rating")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"total_votes"),")."),(0,n.yg)("p",null,"And this is unfortunately a crucial limitation of the linear/lexicographic ordering, the value of the ordering diminishes very quickly as you add more columns. It's not hard to see why:"),(0,n.yg)("img",{src:"/assets/images/lexicographicorder.png",alt:"drawing",width:"250"}),(0,n.yg)("p",null,(0,n.yg)("em",{parentName:"p"},"Courtesy of Wikipedia,")," ",(0,n.yg)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Lexicographic_order"},(0,n.yg)("em",{parentName:"a"},"Lexicographic Order article"))),(0,n.yg)("p",null,"From this image you can see that with lexicographically ordered 3-tuples of integers, only the first column is able to feature crucial property of ",(0,n.yg)("strong",{parentName:"p"},"locality"),' for all of the records having the same value: for ex, all of the records wit values starting with "1", "2", "3" (in the first columns) are nicely clumped together. However if you try to find all the values that have "5" as the value in their third column you\'d see that those are now dispersed all over the place, not being localized at all.'),(0,n.yg)("p",null,"The crucial property that improves query performance is locality: it enables queries to substantially reduce the search space and the number of files that need to be scanned, parsed, etc."),(0,n.yg)("p",null,"But... does this mean that our queries are doomed to do a full-scan if we're filtering by anything other than the first (or more accurate would be to say prefix) of the list of columns the table is ordered by?"),(0,n.yg)("p",null,"Not exactly: luckily, locality is also a property that space-filling curves enable while enumerating multi-dimensional spaces (records in our table could be represented as points in N-dimensional space, where N is the number of columns in our table)"),(0,n.yg)("p",null,"How does it work?"),(0,n.yg)("p",null,"Let's take Z-curve as an example: Z-order curves fitting a 2-dimensional plane would look like the following:"),(0,n.yg)("img",{src:"/assets/images/zordercurve.png",alt:"drawing",width:"400"}),(0,n.yg)("p",null,(0,n.yg)("em",{parentName:"p"},"Courtesy of Wikipedia,")," ",(0,n.yg)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Z-order_curve"},(0,n.yg)("em",{parentName:"a"},"Z-order curve article"))),(0,n.yg)("p",null,'As you can see following its path, instead of simply ordering by one coordinate ("x") first, following with the other, it\'s actually ordering them as if the bits of those coordinates have been ',(0,n.yg)("em",{parentName:"p"},"interwoven")," into a single value:"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Coordinate"),(0,n.yg)("th",{parentName:"tr",align:null},"X (binary)"),(0,n.yg)("th",{parentName:"tr",align:null},"Y (binary)"),(0,n.yg)("th",{parentName:"tr",align:null},"Z-values (ordered)"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"(0, 0)"),(0,n.yg)("td",{parentName:"tr",align:null},"000"),(0,n.yg)("td",{parentName:"tr",align:null},"000"),(0,n.yg)("td",{parentName:"tr",align:null},"000000")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"(1, 0)"),(0,n.yg)("td",{parentName:"tr",align:null},"001"),(0,n.yg)("td",{parentName:"tr",align:null},"000"),(0,n.yg)("td",{parentName:"tr",align:null},"000001")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"(0, 1)"),(0,n.yg)("td",{parentName:"tr",align:null},"000"),(0,n.yg)("td",{parentName:"tr",align:null},"001"),(0,n.yg)("td",{parentName:"tr",align:null},"000010")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"(1, 1)"),(0,n.yg)("td",{parentName:"tr",align:null},"001"),(0,n.yg)("td",{parentName:"tr",align:null},"001"),(0,n.yg)("td",{parentName:"tr",align:null},"000011")))),(0,n.yg)("p",null,'This allows for that crucial property of locality (even though a slightly "stretched" one) to be carried over to all columns as compared to just the first one in case of linear ordering.'),(0,n.yg)("p",null,"In a similar fashion, Hilbert curves also allow you to map points in a N-dimensional space (rows in our table) onto 1-dimensional curve, essentially ",(0,n.yg)("em",{parentName:"p"},"ordering")," them, while still preserving the crucial property of locality. Read more details about Hilbert Curves ",(0,n.yg)("a",{parentName:"p",href:"https://drum.lib.umd.edu/handle/1903/804"},"here"),". Our personal experiments so far show that ordering data along a Hilbert curve leads to better clustering and performance outcomes."),(0,n.yg)("p",null,"Now, let's check it out in action!"),(0,n.yg)("h3",{id:"setup"},"Setup"),(0,n.yg)("p",null,"We will use the ",(0,n.yg)("a",{parentName:"p",href:"https://s3.amazonaws.com/amazon-reviews-pds/readme.html"},"Amazon Reviews")," dataset again, but this time we will use Hudi to Z-Order by ",(0,n.yg)("inlineCode",{parentName:"p"},"product_id"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"customer_id")," columns tuple instead of Clustering or ",(0,n.yg)("em",{parentName:"p"},"linear ordering"),"."),(0,n.yg)("p",null,"No special preparations are required for the dataset, you can simply download it from ",(0,n.yg)("a",{parentName:"p",href:"https://s3.amazonaws.com/amazon-reviews-pds/readme.html"},"S3")," in Parquet format and use it directly as an input for Spark ingesting it into Hudi table."),(0,n.yg)("p",null,"Launch Spark Shell"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"./bin/spark-shell --master 'local[4]' --driver-memory 8G --executor-memory 8G \\\n  --jars ../../packaging/hudi-spark-bundle/target/hudi-spark3-bundle_2.12-0.10.0.jar \\\n  --packages org.apache.spark:spark-avro_2.12:2.4.4 \\\n  --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer'\n")),(0,n.yg)("p",null,"Paste"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-scala"},'import org.apache.hadoop.fs.{FileStatus, Path}\nimport scala.collection.JavaConversions._\nimport org.apache.spark.sql.SaveMode._\nimport org.apache.hudi.{DataSourceReadOptions, DataSourceWriteOptions}\nimport org.apache.hudi.DataSourceWriteOptions._\nimport org.apache.hudi.common.fs.FSUtils\nimport org.apache.hudi.common.table.HoodieTableMetaClient\nimport org.apache.hudi.common.util.ClusteringUtils\nimport org.apache.hudi.config.HoodieClusteringConfig\nimport org.apache.hudi.config.HoodieWriteConfig._\nimport org.apache.spark.sql.DataFrame\n\nimport java.util.stream.Collectors\n\nval layoutOptStrategy = "z-order"; // OR "hilbert"\n\nval inputPath = s"file:///${System.getProperty("user.home")}/datasets/amazon_reviews_parquet"\nval tableName = s"amazon_reviews_${layoutOptStrategy}"\nval outputPath = s"file:///tmp/hudi/$tableName"\n\n\ndef safeTableName(s: String) = s.replace(\'-\', \'_\')\n\nval commonOpts =\n  Map(\n    "hoodie.compact.inline" -> "false",\n    "hoodie.bulk_insert.shuffle.parallelism" -> "10"\n  )\n\n\n////////////////////////////////////////////////////////////////\n// Writing to Hudi\n////////////////////////////////////////////////////////////////\n\nval df = spark.read.parquet(inputPath)\n\ndf.write.format("hudi")\n  .option(DataSourceWriteOptions.TABLE_TYPE.key(), COW_TABLE_TYPE_OPT_VAL)\n  .option("hoodie.table.name", tableName)\n  .option(PRECOMBINE_FIELD.key(), "review_id")\n  .option(RECORDKEY_FIELD.key(), "review_id")\n  .option(DataSourceWriteOptions.PARTITIONPATH_FIELD.key(), "product_category")\n  .option("hoodie.clustering.inline", "true")\n  .option("hoodie.clustering.inline.max.commits", "1")\n  // NOTE: Small file limit is intentionally kept _ABOVE_ target file-size max threshold for Clustering,\n  // to force re-clustering\n  .option("hoodie.clustering.plan.strategy.small.file.limit", String.valueOf(1024 * 1024 * 1024)) // 1Gb\n  .option("hoodie.clustering.plan.strategy.target.file.max.bytes", String.valueOf(128 * 1024 * 1024)) // 128Mb\n  // NOTE: We\'re increasing cap on number of file-groups produced as part of the Clustering run to be able to accommodate for the \n  // whole dataset (~33Gb)\n  .option("hoodie.clustering.plan.strategy.max.num.groups", String.valueOf(4096))\n  .option(HoodieClusteringConfig.LAYOUT_OPTIMIZE_ENABLE.key, "true")\n  .option(HoodieClusteringConfig.LAYOUT_OPTIMIZE_STRATEGY.key, layoutOptStrategy)\n  .option(HoodieClusteringConfig.PLAN_STRATEGY_SORT_COLUMNS.key, "product_id,customer_id")\n  .option(DataSourceWriteOptions.OPERATION.key(), DataSourceWriteOptions.BULK_INSERT_OPERATION_OPT_VAL)\n  .option(BULK_INSERT_SORT_MODE.key(), "NONE")\n  .options(commonOpts)\n  .mode(ErrorIfExists)\n  \n')),(0,n.yg)("h3",{id:"testing"},"Testing"),(0,n.yg)("p",null,"Please keep in mind, that each individual test is run in a separate spark-shell to avoid caching getting in the way of our measurements."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-scala"},'////////////////////////////////////////////////////////////////\n// Reading\n///////////////////////////////////////////////////////////////\n\n// Temp Table w/ Data Skipping DISABLED\nval readDf: DataFrame =\n  spark.read.option(DataSourceReadOptions.ENABLE_DATA_SKIPPING.key(), "false").format("hudi").load(outputPath)\n\nval rawSnapshotTableName = safeTableName(s"${tableName}_sql_snapshot")\n\nreadDf.createOrReplaceTempView(rawSnapshotTableName)\n\n\n// Temp Table w/ Data Skipping ENABLED\nval readDfSkip: DataFrame =\n  spark.read.option(DataSourceReadOptions.ENABLE_DATA_SKIPPING.key(), "true").format("hudi").load(outputPath)\n\nval dataSkippingSnapshotTableName = safeTableName(s"${tableName}_sql_snapshot_skipping")\n\nreadDfSkip.createOrReplaceTempView(dataSkippingSnapshotTableName)\n\n// Query 1: Total votes by product_category, for 6 months\ndef runQuery1(tableName: String) = {\n  // Query 1: Total votes by product_category, for 6 months\n  spark.sql(s"SELECT sum(total_votes), product_category FROM $tableName WHERE review_date > \'2013-12-15\' AND review_date < \'2014-06-01\' GROUP BY product_category").show()\n}\n\n// Query 2: Average star rating by product_id, for some product\ndef runQuery2(tableName: String) = {\n  spark.sql(s"SELECT avg(star_rating), product_id FROM $tableName WHERE product_id in (\'B0184XC75U\') GROUP BY product_id").show()\n}\n\n// Query 3: Count number of reviews by customer_id for some 5 customers\ndef runQuery3(tableName: String) = {\n  spark.sql(s"SELECT count(*) as num_reviews, customer_id FROM $tableName WHERE customer_id in (\'53096570\',\'10046284\',\'53096576\',\'10000196\',\'21700145\') GROUP BY customer_id").show()\n}\n\n//\n// Query 1: Is a "wide" query and hence it\'s expected to touch a lot of files\n//\nscala> runQuery1(rawSnapshotTableName)\n+----------------+--------------------+\n|sum(total_votes)|    product_category|\n+----------------+--------------------+\n|         1050944|                  PC|\n|          867794|             Kitchen|\n|         1167489|                Home|\n|          927531|            Wireless|\n|            6861|               Video|\n|           39602| Digital_Video_Games|\n|          954924|Digital_Video_Dow...|\n|           81876|             Luggage|\n|          320536|         Video_Games|\n|          817679|              Sports|\n|           11451|  Mobile_Electronics|\n|          228739|  Home_Entertainment|\n|         3769269|Digital_Ebook_Pur...|\n|          252273|                Baby|\n|          735042|             Apparel|\n|           49101|    Major_Appliances|\n|          484732|             Grocery|\n|          285682|               Tools|\n|          459980|         Electronics|\n|          454258|            Outdoors|\n+----------------+--------------------+\nonly showing top 20 rows\n\nscala> runQuery1(dataSkippingSnapshotTableName)\n+----------------+--------------------+\n|sum(total_votes)|    product_category|\n+----------------+--------------------+\n|         1050944|                  PC|\n|          867794|             Kitchen|\n|         1167489|                Home|\n|          927531|            Wireless|\n|            6861|               Video|\n|           39602| Digital_Video_Games|\n|          954924|Digital_Video_Dow...|\n|           81876|             Luggage|\n|          320536|         Video_Games|\n|          817679|              Sports|\n|           11451|  Mobile_Electronics|\n|          228739|  Home_Entertainment|\n|         3769269|Digital_Ebook_Pur...|\n|          252273|                Baby|\n|          735042|             Apparel|\n|           49101|    Major_Appliances|\n|          484732|             Grocery|\n|          285682|               Tools|\n|          459980|         Electronics|\n|          454258|            Outdoors|\n+----------------+--------------------+\nonly showing top 20 rows\n\n//\n// Query 2: Is a "pointwise" query and hence it\'s expected that data-skipping should substantially reduce number \n// of files scanned (as compared to Baseline)\n//\n// NOTE: That Linear Ordering (as compared to Space-curve based on) will have similar effect on performance reducing\n// total # of Parquet files scanned, since we\'re querying on the prefix of the ordering key\n//\nscala> runQuery2(rawSnapshotTableName)\n+----------------+----------+\n|avg(star_rating)|product_id|\n+----------------+----------+\n|             1.0|B0184XC75U|\n+----------------+----------+\n\n\nscala> runQuery2(dataSkippingSnapshotTableName)\n+----------------+----------+\n|avg(star_rating)|product_id|\n+----------------+----------+\n|             1.0|B0184XC75U|\n+----------------+----------+\n\n//\n// Query 3: Similar to Q2, is a "pointwise" query, but querying other part of the ordering-key (product_id, customer_id)\n// and hence it\'s expected that data-skipping should substantially reduce number of files scanned (as compared to Baseline, Linear Ordering).\n//\n// NOTE: That Linear Ordering (as compared to Space-curve based on) will _NOT_ have similar effect on performance reducing\n// total # of Parquet files scanned, since we\'re NOT querying on the prefix of the ordering key\n//\nscala> runQuery3(rawSnapshotTableName)\n+-----------+-----------+\n|num_reviews|customer_id|\n+-----------+-----------+\n|         50|   53096570|\n|          3|   53096576|\n|         25|   10046284|\n|          1|   10000196|\n|         14|   21700145|\n+-----------+-----------+\n\nscala> runQuery3(dataSkippingSnapshotTableName)\n+-----------+-----------+\n|num_reviews|customer_id|\n+-----------+-----------+\n|         50|   53096570|\n|          3|   53096576|\n|         25|   10046284|\n|          1|   10000196|\n|         14|   21700145|\n+-----------+-----------+\n')),(0,n.yg)("h3",{id:"results"},"Results"),(0,n.yg)("p",null,"We've summarized the measured performance metrics below:"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},(0,n.yg)("strong",{parentName:"th"},"Query")),(0,n.yg)("th",{parentName:"tr",align:null},(0,n.yg)("strong",{parentName:"th"},"Baseline (B)")," duration (files scanned / size)"),(0,n.yg)("th",{parentName:"tr",align:null},(0,n.yg)("strong",{parentName:"th"},"Linear Sorting (S)")),(0,n.yg)("th",{parentName:"tr",align:null},(0,n.yg)("strong",{parentName:"th"},"Z-order (Z)")," duration (scanned)"),(0,n.yg)("th",{parentName:"tr",align:null},(0,n.yg)("strong",{parentName:"th"},"Hilbert (H)")," duration (scanned)"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Q1"),(0,n.yg)("td",{parentName:"tr",align:null},"14s (543 / 31.4Gb)"),(0,n.yg)("td",{parentName:"tr",align:null},"15s (533 / 28.8Gb)"),(0,n.yg)("td",{parentName:"tr",align:null},"15s (543 / 31.4Gb)"),(0,n.yg)("td",{parentName:"tr",align:null},"14s (541 / 31.3Gb)")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Q2"),(0,n.yg)("td",{parentName:"tr",align:null},"21s (543 / 31.4Gb)"),(0,n.yg)("td",{parentName:"tr",align:null},"10s (533 / 28.8Gb)"),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("strong",{parentName:"td"},"8s")," ",(0,n.yg)("strong",{parentName:"td"},"(243 / 14.4Gb)")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("strong",{parentName:"td"},"7s")," ",(0,n.yg)("strong",{parentName:"td"},"(237 / 13.9Gb)"))),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Q3"),(0,n.yg)("td",{parentName:"tr",align:null},"17s (543 / 31.4Gb)"),(0,n.yg)("td",{parentName:"tr",align:null},"15s (533 / 28.8Gb)"),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("strong",{parentName:"td"},"6s")," ",(0,n.yg)("strong",{parentName:"td"},"(224 / 12.4Gb)")),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("strong",{parentName:"td"},"6s")," ",(0,n.yg)("strong",{parentName:"td"},"(219 / 11.9Gb)"))))),(0,n.yg)("p",null,"As you can see multi-column linear ordering is not very effective for the queries that do filtering by columns other than the first one (Q2, Q3)."),(0,n.yg)("p",null,"Which is a very clear contrast with space-filling curves (both Z-order and Hilbert) that allow to speed up query time by up to ",(0,n.yg)("strong",{parentName:"p"},"3x!")),(0,n.yg)("p",null,"It's worth noting that the performance gains are heavily dependent on your underlying data and queries. In benchmarks on our internal data we were able to achieve queries performance improvements of more than ",(0,n.yg)("strong",{parentName:"p"},"11x!")),(0,n.yg)("h3",{id:"epilogue"},"Epilogue"),(0,n.yg)("p",null,"Apache Hudi v0.10 brings new layout optimization capabilities Z-order and Hilbert to open source. Using these industry leading layout optimization techniques can bring substantial performance improvement and cost savings to your queries!"))}g.isMDXComponent=!0},11470:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"The Art of Building Open Data Lakes with Apache Hudi, Kafka, Hive, and Debezium",authors:[{name:"Gary Stafford"}],category:"blog",image:"/assets/images/blog/2021-12-31-open-source-data-lakes-on-aws.png",tags:["how-to","datalake","medium"]},s=void 0,l={permalink:"/cn/blog/2021/12/31/The-Art-of-Building-Open-Data-Lakes-with-Apache-Hudi-Kafka-Hive-and-Debezium",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-12-31-The-Art-of-Building-Open-Data-Lakes-with-Apache-Hudi-Kafka-Hive-and-Debezium.mdx",source:"@site/blog/2021-12-31-The-Art-of-Building-Open-Data-Lakes-with-Apache-Hudi-Kafka-Hive-and-Debezium.mdx",title:"The Art of Building Open Data Lakes with Apache Hudi, Kafka, Hive, and Debezium",description:"Redirecting... please wait!!",date:"2021-12-31T00:00:00.000Z",formattedDate:"December 31, 2021",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"datalake",permalink:"/cn/blog/tags/datalake"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Gary Stafford"}],prevItem:{title:"Apache Hudi - 2021 a Year in Review",permalink:"/cn/blog/2022/01/06/apache-hudi-2021-a-year-in-review"},nextItem:{title:"Hudi Z-Order and Hilbert Space Filling Curves",permalink:"/cn/blog/2021/12/29/hudi-zorder-and-hilbert-space-filling-curves"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://garystafford.medium.com/the-art-of-building-open-data-lakes-with-apache-hudi-kafka-hive-and-debezium-3d2f71c5981f",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},21015:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Apache Hudi - 2021 a Year in Review",excerpt:"A reflection on the growth and momentum of Apache Hudi in 2021",author:"vinoth",category:"blog",image:"/assets/images/Hudi_community.png",tags:["blog","community","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2022/01/06/apache-hudi-2021-a-year-in-review",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-01-06-apache-hudi-2021-a-year-in-review.md",source:"@site/blog/2022-01-06-apache-hudi-2021-a-year-in-review.md",title:"Apache Hudi - 2021 a Year in Review",description:"As the year came to end, I took some time to reflect on where we are and what we accomplished in 2021. I am humbled by how strong our community is and how regardless of it being another tough pandemic year, that people from around the globe leaned in together and made this the best year yet for Apache Hudi. In this blog I want to recap some of the 2021 highlights.",date:"2022-01-06T00:00:00.000Z",formattedDate:"January 6, 2022",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"community",permalink:"/cn/blog/tags/community"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:3.09,truncated:!0,authors:[{name:"vinoth"}],prevItem:{title:"Change Data Capture with Debezium and Apache Hudi",permalink:"/cn/blog/2022/01/14/change-data-capture-with-debezium-and-apache-hudi"},nextItem:{title:"The Art of Building Open Data Lakes with Apache Hudi, Kafka, Hive, and Debezium",permalink:"/cn/blog/2021/12/31/The-Art-of-Building-Open-Data-Lakes-with-Apache-Hudi-Kafka-Hive-and-Debezium"}},l={authorsImageUrls:[void 0]},d=[],c={toc:d},p="wrapper";function g(e){let{components:a,...t}=e;return(0,n.yg)(p,(0,i.A)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"As the year came to end, I took some time to reflect on where we are and what we accomplished in 2021. I am humbled by how strong our community is and how regardless of it being another tough pandemic year, that people from around the globe leaned in together and made this the best year yet for Apache Hudi. In this blog I want to recap some of the 2021 highlights."),(0,n.yg)("img",{src:"/assets/images/Hudi_community.png",alt:"drawing",width:"600"}),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},(0,n.yg)("em",{parentName:"strong"},"Community"))),(0,n.yg)("p",null,"I want to call out how amazing it is to see such a diverse group of people step up and contribute to this project. There were over 30,000 interactions with the ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/"},"project on github"),", up 2x from last year. Over the last year 300 people have contributed to the project, with over 3,000 PRs over 5 releases. We moved Apache Hudi from release 0.5.X all the way to our feature packed 0.10.0 release. Come and join us on our ",(0,n.yg)("a",{parentName:"p",href:"https://join.slack.com/t/apache-hudi/shared_invite/zt-2ggm1fub8-_yt4Reu9djwqqVRFC7X49g"},"active slack channel"),"! Over 850 community members engaged on our slack, up about 100% from the year before. I want to add a special shout out to our top slack participants who have helped answer so many questions and drive rich discussions on our channel. Sivabalan Narayanan, Nishith Agarwal, Bhavani Sudha Saktheeswaran, Vinay Patil, Rubens Soto, Dave Hagman, Raghav Tandon, Sagar Sumit, Joyan Sil, Jake D, Felix Jose, Nick Vintila, KimL, Andrew Sukhan, Danny Chan, Biswajit Mohapatra, and Pratyaksh Sharma! I know I am missing plenty of other important callouts, every PR that landed this year has helped shape Hudi into what it is today. Thank you!"),(0,n.yg)("img",{src:"/assets/images/powers/hudi-logo-page.png",alt:"drawing",width:"600"}),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},(0,n.yg)("em",{parentName:"strong"},"Impact"))),(0,n.yg)("p",null,"In 2021, I personally developed a deeper gratitude and understanding of the magnitude of the impact we are making in the industry. Throughout the year I met more and more people that told me about how Hudi transformed their business and I was impressed by the large variety of use cases and applications that Hudi was able to serve. Some from the community who publicly shared their story include: ",(0,n.yg)("a",{parentName:"p",href:"https://aws.amazon.com/blogs/big-data/how-amazon-transportation-service-enabled-near-real-time-event-analytics-at-petabyte-scale-using-aws-glue-with-apache-hudi/"},"Amazon"),", ",(0,n.yg)("a",{parentName:"p",href:"https://aws.amazon.com/blogs/big-data/how-ge-aviation-built-cloud-native-data-pipelines-at-enterprise-scale-using-the-aws-platform/"},"GE"),", ",(0,n.yg)("a",{parentName:"p",href:"https://s.apache.org/hudi-robinhood-talk"},"Robinhood"),", ",(0,n.yg)("a",{parentName:"p",href:"http://hudi.apache.org/blog/2021/09/01/building-eb-level-data-lake-using-hudi-at-bytedance"},"ByteDance"),", ",(0,n.yg)("a",{parentName:"p",href:"https://blogs.halodoc.io/data-platform-2-0-part-1/"},"Halodoc"),", ",(0,n.yg)("a",{parentName:"p",href:"https://developpaper.com/baixin-banks-real-time-data-lake-evolution-scheme-based-on-apache-hudi/"},"Baixin Bank"),", ",(0,n.yg)("a",{parentName:"p",href:"https://developpaper.com/practice-of-apache-hudi-in-building-real-time-data-lake-at-station-b/"},"BiliBili"),", and so many more that haven\u2019t even shared yet. One particular highlight from 2021 was attending ",(0,n.yg)("a",{parentName:"p",href:"https://youtu.be/lGm8qe4tBrg?t=2115"},"AWS Re:Invent")," and meeting an overwhelmingly large number of users who expressed joy with using Apache Hudi. This raises my sense of responsibility even more to be aware of just how many people depend on Apache Hudi."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},(0,n.yg)("em",{parentName:"strong"},"New Features"))),(0,n.yg)("p",null,"Apache Hudi has come a long way in 2021 from v0.5.X to 0.10.0. Throughout this year we have developed innovative and leading edge features that make it easier and easier to build streaming data lakes. Some of these features include ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/sql_ddl"},"Spark SQL DML Support"),", ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/clustering"},"Clustering"),", ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/blog/2021/12/29/hudi-zorder-and-hilbert-space-filling-curves"},"Z-Order/Hilbert curves"),", ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/metadata"},"Metadata Table file listing elimination"),", ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/markers"},"Timeline Server Markers"),", ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/precommit_validator"},"Precommit Validators"),", ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/writing_data#flink-sql-writer"},"Flink MOR write/read"),", ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/concurrency_control"},"Parallel Write support with OCC"),", ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/clustering"},"Clustering"),", ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/querying_data#spark-incr-query"},"Incremental Queries for MOR"),", ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/tree/master/hudi-kafka-connect"},"Kafka Connect Sink"),", Delta Streamer sources for ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/hoodie_streaming_ingestion/#s3-events"},"S3")," and ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/releases/release-0.10.0/#debezium-deltastreamer-sources"},"Debezium"),", ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/releases/release-0.10.0/#dbt-support"},"DBT Support")," all of which are were added in 2021. To top it all, we put together ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/blog/2021/07/21/streaming-data-lake-platform"},"a manifesto")," to realize our vision for streaming data lakes."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},(0,n.yg)("em",{parentName:"strong"},"The Road Ahead"))),(0,n.yg)("p",null,"2021 may have been our best year so far, but it still feels like we are just getting started when we look at our new year's resolutions for 2022. In the year ahead we have bold plans to realize the first cut of our entire vision and take Hudi 1.0, that includes full-featured multi-modal indexing for faster writes/queries, pathbreaking lock free concurrency, new server components for caching/metadata and finally Flink based incremental materialized views! \xa0",(0,n.yg)("em",{parentName:"p"},"You can find our")," ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/roadmap"},(0,n.yg)("em",{parentName:"a"},"detailed roadmap here")),(0,n.yg)("em",{parentName:"p"},".")),(0,n.yg)("p",null,"I look forward to continued collaboration with the growing Hudi community! Come join our ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/community/syncs"},(0,n.yg)("em",{parentName:"a"},"community events"))," ",(0,n.yg)("em",{parentName:"p"},"and discussions in our")," ",(0,n.yg)("a",{parentName:"p",href:"https://join.slack.com/t/apache-hudi/shared_invite/zt-2ggm1fub8-_yt4Reu9djwqqVRFC7X49g"},(0,n.yg)("em",{parentName:"a"},"slack channel")),(0,n.yg)("em",{parentName:"p"},"! Happy new year 2022!")))}g.isMDXComponent=!0},30661:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Change Data Capture with Debezium and Apache Hudi",excerpt:"A review of new Debezium source connector for Apache Hudi",author:"Rajesh Mahindra",category:"blog",image:"/assets/images/blog/debezium.png",tags:["design","deltastreamer","cdc","change data capture","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2022/01/14/change-data-capture-with-debezium-and-apache-hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-01-14-change-data-capture-with-debezium-and-apache-hudi.md",source:"@site/blog/2022-01-14-change-data-capture-with-debezium-and-apache-hudi.md",title:"Change Data Capture with Debezium and Apache Hudi",description:"As of Hudi v0.10.0, we are excited to announce the availability of Debezium sources for Deltastreamer that provide the ingestion of change capture data (CDC) from Postgres and Mysql databases to your data lake. For more details, please refer to the original RFC.",date:"2022-01-14T00:00:00.000Z",formattedDate:"January 14, 2022",tags:[{label:"design",permalink:"/cn/blog/tags/design"},{label:"deltastreamer",permalink:"/cn/blog/tags/deltastreamer"},{label:"cdc",permalink:"/cn/blog/tags/cdc"},{label:"change data capture",permalink:"/cn/blog/tags/change-data-capture"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:7.28,truncated:!0,authors:[{name:"Rajesh Mahindra"}],prevItem:{title:"Why and How I Integrated Airbyte and Apache Hudi",permalink:"/cn/blog/2022/01/18/Why-and-How-I-Integrated-Airbyte-and-Apache-Hudi"},nextItem:{title:"Apache Hudi - 2021 a Year in Review",permalink:"/cn/blog/2022/01/06/apache-hudi-2021-a-year-in-review"}},l={authorsImageUrls:[void 0]},d=[{value:"Background",id:"background",children:[],level:2},{value:"Design Overview",id:"design-overview",children:[],level:2},{value:"Apache Hudi Configurations",id:"apache-hudi-configurations",children:[{value:"Bootstrapping Existing tables",id:"bootstrapping-existing-tables",children:[],level:3},{value:"Example Implementation",id:"example-implementation",children:[],level:3},{value:"Database",id:"database",children:[],level:3},{value:"Debezium Connector",id:"debezium-connector",children:[],level:3},{value:"Hudi Deltastreamer",id:"hudi-deltastreamer",children:[],level:3}],level:2},{value:"Conclusion",id:"conclusion",children:[],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...t}=e;return(0,n.yg)(p,(0,i.A)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"As of Hudi v0.10.0, we are excited to announce the availability of ",(0,n.yg)("a",{parentName:"p",href:"https://debezium.io/"},"Debezium")," sources for ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/hoodie_streaming_ingestion"},"Deltastreamer")," that provide the ingestion of change capture data (CDC) from Postgres and Mysql databases to your data lake. For more details, please refer to the original ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/master/rfc/rfc-39/rfc-39.md"},"RFC"),"."),(0,n.yg)("h2",{id:"background"},"Background"),(0,n.yg)("img",{src:"/assets/images/blog/data-network.png",alt:"drawing",width:"600"}),(0,n.yg)("p",null,"When you want to perform analytics on data from transactional databases like Postgres or Mysql you typically need to bring this data into an OLAP system such as a data warehouse or a data lake through a process called ",(0,n.yg)("a",{parentName:"p",href:"https://debezium.io/documentation/faq/#what_is_change_data_capture"},"Change Data Capture")," (CDC). Debezium is a popular tool that makes CDC easy. It provides a way to capture row-level changes in your databases by ",(0,n.yg)("a",{parentName:"p",href:"https://debezium.io/blog/2018/07/19/advantages-of-log-based-change-data-capture/"},"reading changelogs"),". By doing so, Debezium avoids increased CPU load on your database and ensures you capture all changes including deletes."),(0,n.yg)("p",null,"Now that ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/overview/"},"Apache Hudi")," offers a Debezium source connector, CDC ingestion into a data lake is easier than ever with some ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/use_cases"},"unique differentiated capabilities"),". Hudi enables efficient update, merge, and delete transactions on a data lake. Hudi uniquely provides ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/table_types#merge-on-read-table"},"Merge-On-Read")," writers which unlock ",(0,n.yg)("a",{parentName:"p",href:"https://aws.amazon.com/blogs/big-data/how-amazon-transportation-service-enabled-near-real-time-event-analytics-at-petabyte-scale-using-aws-glue-with-apache-hudi/"},"significantly lower latency")," ingestion than typical data lake writers with Spark or Flink. Last but not least, Apache Hudi offers ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/querying_data#spark-incr-query"},"incremental queries")," so after capturing changes from your database, you can incrementally process these changes downstream throughout all of your subsequent ETL pipelines."),(0,n.yg)("h2",{id:"design-overview"},"Design Overview"),(0,n.yg)("img",{src:"/assets/images/blog/debezium.png",alt:"drawing",width:"600"}),(0,n.yg)("p",null,"The architecture for an end-to-end CDC ingestion flow with Apache Hudi is shown above. The first component is the Debezium deployment, which consists of a Kafka cluster, schema registry (Confluent or Apicurio), and the Debezium connector. The Debezium connector continuously polls the changelogs from the database and writes an AVRO message with the changes for each database row to a dedicated Kafka topic per table."),(0,n.yg)("p",null,"The second component is ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/hoodie_streaming_ingestion"},"Hudi Deltastreamer")," that reads and processes the incoming Debezium records from Kafka for each table and writes (updates) the corresponding rows in a Hudi table on your cloud storage."),(0,n.yg)("p",null,"To ingest the data from the database table into a Hudi table in near real-time, we implement two classes that can be plugged into the Deltastreamer. Firstly, we implemented a ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/83f8ed2ae3ba7fb20813cbb8768deae6244b020c/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/debezium/DebeziumSource.java"},"Debezium source"),". With Deltastreamer running in continuous mode, the source continuously reads and processes the Debezium change records in Avro format from the Kafka topic for a given table, and writes the updated record to the destination Hudi table. In addition to the columns from the database table, we also ingest some meta fields that are added by Debezium in the target Hudi table. The meta fields help us correctly merge updates and delete records. The records are read using the latest schema from the ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/hoodie_streaming_ingestion#schema-providers"},"Schema Registry"),"."),(0,n.yg)("p",null,"Secondly, we implement a custom ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/83f8ed2ae3ba7fb20813cbb8768deae6244b020c/hudi-common/src/main/java/org/apache/hudi/common/model/debezium/AbstractDebeziumAvroPayload.java"},"Debezium Payload")," that essentially governs how Hudi records are merged when the same row is updated or deleted. When a new Hudi record is received for an existing row, the payload picks the latest record using the higher value of the appropriate column (FILEID and POS fields in MySql and LSN fields in Postgres). In the case that the latter event is a delete record, the payload implementation ensures that the record is hard deleted from the storage. Delete records are identified using the op field, which has a value of ",(0,n.yg)("strong",{parentName:"p"},"d")," for deletes."),(0,n.yg)("h2",{id:"apache-hudi-configurations"},"Apache Hudi Configurations"),(0,n.yg)("p",null,"It is important to consider the following configurations of your Hudi deployments when using the Debezium source connector for CDC ingestion."),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Record Keys -")," The Hudi ",(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/docs/next/indexing"},"record key(s)")," for a table should be set as the Primary keys of the table in the upstream database. This ensures that updates are applied correctly as record key(s) uniquely identify a row in the Hudi table."),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Source Ordering Fields")," -\xa0 For de-duplication of changelog records the source ordering field should be set to the actual position of the change event as it happened on the database. For instance, we use the FILEID and POS fields in MySql and LSN fields in Postgres databases respectively to ensure records are processed in the correct order of occurrence in the original database."),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Partition Fields")," - Don\u2019t feel restricted to matching the partitioning of your Hudi tables with the same partition fields as the upstream database. You can set partition fields independently for the Hudi table as needed.")),(0,n.yg)("h3",{id:"bootstrapping-existing-tables"},"Bootstrapping Existing tables"),(0,n.yg)("p",null,"One important use case might be when CDC ingestion has to be done for existing database tables. There are two ways we can ingest existing database data prior to streaming the changes:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"By default on initialization, Debezium performs an initial consistent snapshot of the database (controlled by config snapshot.mode). After the initial snapshot, it continues streaming updates from the correct position to avoid loss of data."),(0,n.yg)("li",{parentName:"ol"},"While the first approach is simple, for large tables it may take a long time for Debezium to bootstrap the initial snapshot. Alternatively, we could run a Deltastreamer job to bootstrap the table directly from the database using the ",(0,n.yg)("a",{parentName:"li",href:"https://github.com/apache/hudi/blob/master/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/JdbcSource.java"},"JDBC source"),". This provides more flexibility to the users in defining and executing more optimized SQL queries required to bootstrap the database table. Once the bootstrap job finishes successfully, another Deltastreamer job is executed that processes the database changelogs from Debezium. Users will have to use ",(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/docs/hoodie_streaming_ingestion/#checkpointing"},"checkpointing")," in Deltastreamer to ensure the second job starts processing the changelogs from the correct position to avoid data loss.")),(0,n.yg)("h3",{id:"example-implementation"},"Example Implementation"),(0,n.yg)("p",null,"The following describes steps to implement an end-to-end CDC pipeline using an AWS RDS instance of Postgres, Kubernetes-based Debezium deployment, and Hudi Deltastreamer running on a spark cluster."),(0,n.yg)("h3",{id:"database"},"Database"),(0,n.yg)("p",null,"A few configuration changes are required for the RDS instance to enable logical replication."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-roomsql"},"SET rds.logical_replication to 1 (instead of 0)\n\npsql --host=<aws_rds_instance> --port=5432 --username=postgres --password -d <database_name>;\n\nCREATE PUBLICATION <publication_name> FOR TABLE schema1.table1, schema1.table2;\n\nALTER TABLE schema1.table1 REPLICA IDENTITY FULL;\n")),(0,n.yg)("h3",{id:"debezium-connector"},"Debezium Connector"),(0,n.yg)("p",null,(0,n.yg)("a",{parentName:"p",href:"https://strimzi.io/blog/2020/01/27/deploying-debezium-with-kafkaconnector-resource/"},"Strimzi")," is the recommended option to deploy and manage Kafka connectors on Kubernetes clusters. Alternatively, you have the option to use the Confluent managed ",(0,n.yg)("a",{parentName:"p",href:"https://docs.confluent.io/debezium-connect-postgres-source/current/overview.html"},"Debezium connector"),"."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"kubectl create namespace kafka\nkubectl create -f https://strimzi.io/install/latest?namespace=kafka -n kafka\nkubectl -n kafka apply -f kafka-connector.yaml\n")),(0,n.yg)("p",null,"An example for kafka-connector.yaml is shown below:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaConnect\nmetadata:\nname: debezium-kafka-connect\nannotations:\nstrimzi.io/use-connector-resources: "false"\nspec:\nimage: debezium-kafka-connect:latest\nreplicas: 1\nbootstrapServers: localhost:9092\nconfig:\nconfig.storage.replication.factor: 1\noffset.storage.replication.factor: 1\nstatus.storage.replication.factor: 1\n')),(0,n.yg)("p",null,"The docker image debezium-kafka-connect can be built using the following Dockerfile that includes the Postgres Debezium Connector."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yaml"},"FROM confluentinc/cp-kafka-connect:6.2.0 as cp\nRUN confluent-hub install --no-prompt confluentinc/kafka-connect-avro-converter:6.2.0\nFROM strimzi/kafka:0.18.0-kafka-2.5.0\nUSER root:root\nRUN yum -y update\nRUN yum -y install git\nRUN yum -y install wget\n\nRUN wget https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/1.6.1.Final/debezium-connector-postgres-1.6.1.Final-plugin.tar.gz\nRUN tar xzf debezium-connector-postgres-1.6.1.Final-plugin.tar.gz\n\nRUN mkdir -p /opt/kafka/plugins/debezium && mkdir -p /opt/kafka/plugins/avro/\nRUN mv debezium-connector-postgres /opt/kafka/plugins/debezium/\nCOPY --from=cp /usr/share/confluent-hub-components/confluentinc-kafka-connect-avro-converter/lib /opt/kafka/plugins/avro/\nUSER 1001\n")),(0,n.yg)("p",null,"Once the Strimzi operator and the Kafka connect are deployed, we can start the Debezium connector."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},'curl -X POST -H "Content-Type:application/json" -d @connect-source.json http://localhost:8083/connectors/\n')),(0,n.yg)("p",null,"The following is an example of a configuration to setup Debezium connector for generating the changelogs for two tables, table1, and table2."),(0,n.yg)("p",null,"Contents of connect-source.json:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-json"},'{\n  "name": "postgres-debezium-connector",\n  "config": {\n    "connector.class": "io.debezium.connector.postgresql.PostgresConnector",\n    "database.hostname": "localhost",\n    "database.port": "5432",\n    "database.user": "postgres",\n    "database.password": "postgres",\n    "database.dbname": "database",\n    "plugin.name": "pgoutput",\n    "database.server.name": "postgres",\n    "table.include.list": "schema1.table1,schema1.table2",\n    "publication.autocreate.mode": "filtered",\n    "tombstones.on.delete":"false",\n    "key.converter": "io.confluent.connect.avro.AvroConverter",\n    "key.converter.schema.registry.url": "<schema_registry_host>",\n    "value.converter": "io.confluent.connect.avro.AvroConverter",\n    "value.converter.schema.registry.url": "<schema_registry_host>",\n    "slot.name": "pgslot"\n  }\n}\n')),(0,n.yg)("h3",{id:"hudi-deltastreamer"},"Hudi Deltastreamer"),(0,n.yg)("p",null,"Next, we run the Hudi Deltastreamer using spark that will ingest the Debezium changelogs from kafka and write them as a Hudi table. One such instance of the command is shown below that works for Postgres database.\xa0 A few key configurations are as follows:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Set the source class to PostgresDebeziumSource."),(0,n.yg)("li",{parentName:"ol"},"Set the payload class to PostgresDebeziumAvroPayload."),(0,n.yg)("li",{parentName:"ol"},"Configure the schema registry URLs for Debezium Source and Kafka Source."),(0,n.yg)("li",{parentName:"ol"},"Set the record key(s) as the primary key(s) of the database table."),(0,n.yg)("li",{parentName:"ol"},"Set the source ordering field (dedup) to _event_lsn")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-scala"},'spark-submit \\\\\n  --jars "/home/hadoop/hudi-utilities-bundle_2.12-0.10.0.jar,/usr/lib/spark/external/lib/spark-avro.jar" \\\\\n  --master yarn --deploy-mode client \\\\\n  --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer /home/hadoop/hudi-packages/hudi-utilities-bundle_2.12-0.10.0-SNAPSHOT.jar \\\\\n  --table-type COPY_ON_WRITE --op UPSERT \\\\\n  --target-base-path s3://bucket_name/path/for/hudi_table1 \\\\\n  --target-table hudi_table1\xa0 --continuous \\\\\n  --min-sync-interval-seconds 60 \\\\\n  --source-class org.apache.hudi.utilities.sources.debezium.PostgresDebeziumSource \\\\\n  --source-ordering-field _event_lsn \\\\\n  --payload-class org.apache.hudi.common.model.debezium.PostgresDebeziumAvroPayload \\\\\n  --hoodie-conf schema.registry.url=https://localhost:8081 \\\\\n  --hoodie-conf hoodie.deltastreamer.schemaprovider.registry.url=https://localhost:8081/subjects/postgres.schema1.table1-value/versions/latest \\\\\n  --hoodie-conf hoodie.deltastreamer.source.kafka.value.deserializer.class=io.confluent.kafka.serializers.KafkaAvroDeserializer \\\\\n  --hoodie-conf hoodie.deltastreamer.source.kafka.topic=postgres.schema1.table1 \\\\\n  --hoodie-conf auto.offset.reset=earliest \\\\\n  --hoodie-conf hoodie.datasource.write.recordkey.field=\u201ddatabase_primary_key\u201d \\\\\n  --hoodie-conf hoodie.datasource.write.partitionpath.field=partition_key \\\\\n  --enable-hive-sync \\\\\n  --hoodie-conf hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor \\\\\n  --hoodie-conf hoodie.datasource.write.hive_style_partitioning=true \\\\\n  --hoodie-conf hoodie.datasource.hive_sync.database=default \\\\\n  --hoodie-conf hoodie.datasource.hive_sync.table=hudi_table1 \\\\\n  --hoodie-conf hoodie.datasource.hive_sync.partition_fields=partition_key\n')),(0,n.yg)("h2",{id:"conclusion"},"Conclusion"),(0,n.yg)("p",null,"This post introduced the Debezium Source for Hudi Deltastreamer to ingest Debezium changelogs into Hudi tables. Database data can now be ingested into data lakes to provide a cost-effective way to store and analyze database data."),(0,n.yg)("p",null,"Please follow this ",(0,n.yg)("a",{parentName:"p",href:"https://issues.apache.org/jira/browse/HUDI-1290"},"JIRA")," to learn more about active development on this new feature. I look forward to more contributions and feedback from the community. Come join our ",(0,n.yg)("a",{parentName:"p",href:"https://join.slack.com/t/apache-hudi/shared_invite/zt-2ggm1fub8-_yt4Reu9djwqqVRFC7X49g"},"Hudi Slack")," channel or attend one of our ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/community/syncs"},"community events")," to learn more."))}g.isMDXComponent=!0},18920:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Why and How I Integrated Airbyte and Apache Hudi",authors:[{name:"Harsha Teja Kanna"}],category:"blog",image:"/assets/images/blog/2022-01-18-airbyte-hudi-integration.png",tags:["how-to","deltastreamer","selectfrom"]},s=void 0,l={permalink:"/cn/blog/2022/01/18/Why-and-How-I-Integrated-Airbyte-and-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-01-18-Why-and-How-I-Integrated-Airbyte-and-Apache-Hudi.mdx",source:"@site/blog/2022-01-18-Why-and-How-I-Integrated-Airbyte-and-Apache-Hudi.mdx",title:"Why and How I Integrated Airbyte and Apache Hudi",description:"Redirecting... please wait!!",date:"2022-01-18T00:00:00.000Z",formattedDate:"January 18, 2022",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"deltastreamer",permalink:"/cn/blog/tags/deltastreamer"},{label:"selectfrom",permalink:"/cn/blog/tags/selectfrom"}],readingTime:.045,truncated:!1,authors:[{name:"Harsha Teja Kanna"}],prevItem:{title:"Hudi powering data lake efforts at Walmart and Disney+ Hotstar",permalink:"/cn/blog/2022/01/20/Hudi-powering-data-lake-efforts-at-Walmart-and-Disney-Hotstar"},nextItem:{title:"Change Data Capture with Debezium and Apache Hudi",permalink:"/cn/blog/2022/01/14/change-data-capture-with-debezium-and-apache-hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://selectfrom.dev/why-and-how-i-integrated-airbyte-and-apache-hudi-c18aff3af21a",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},10082:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Hudi powering data lake efforts at Walmart and Disney+ Hotstar",authors:[{name:"Sean Michael Kerner"}],category:"blog",image:"/assets/images/blog/2022-01-20-hudi-powering-datalake-efforts.png",tags:["use-case","techtarget"]},s=void 0,l={permalink:"/cn/blog/2022/01/20/Hudi-powering-data-lake-efforts-at-Walmart-and-Disney-Hotstar",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-01-20-Hudi-powering-data-lake-efforts-at-Walmart-and-Disney-Hotstar.mdx",source:"@site/blog/2022-01-20-Hudi-powering-data-lake-efforts-at-Walmart-and-Disney-Hotstar.mdx",title:"Hudi powering data lake efforts at Walmart and Disney+ Hotstar",description:"Redirecting... please wait!!",date:"2022-01-20T00:00:00.000Z",formattedDate:"January 20, 2022",tags:[{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"techtarget",permalink:"/cn/blog/tags/techtarget"}],readingTime:.045,truncated:!1,authors:[{name:"Sean Michael Kerner"}],prevItem:{title:"Cost Efficiency @ Scale in Big Data File Format",permalink:"/cn/blog/2022/01/25/Cost-Efficiency-Scale-in-Big-Data-File-Format"},nextItem:{title:"Why and How I Integrated Airbyte and Apache Hudi",permalink:"/cn/blog/2022/01/18/Why-and-How-I-Integrated-Airbyte-and-Apache-Hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.techtarget.com/searchdatamanagement/feature/Hudi-powering-data-lake-efforts-at-Walmart-and-Disney-Hotstar",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},22951:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Cost Efficiency @ Scale in Big Data File Format",authors:[{name:"Xinli Shang"},{name:"Kai Jiang"},{name:"Zheng Shao"},{name:"Mohammad Islam"}],category:"blog",image:"/assets/images/blog/2022-01-25-cost-efficiency-at-scale-in-big-data-file-format.png",tags:["blog","cost efficiency","compression","analytics at scale","uber"]},s=void 0,l={permalink:"/cn/blog/2022/01/25/Cost-Efficiency-Scale-in-Big-Data-File-Format",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-01-25-Cost-Efficiency-Scale-in-Big-Data-File-Format.mdx",source:"@site/blog/2022-01-25-Cost-Efficiency-Scale-in-Big-Data-File-Format.mdx",title:"Cost Efficiency @ Scale in Big Data File Format",description:"Redirecting... please wait!!",date:"2022-01-25T00:00:00.000Z",formattedDate:"January 25, 2022",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"cost efficiency",permalink:"/cn/blog/tags/cost-efficiency"},{label:"compression",permalink:"/cn/blog/tags/compression"},{label:"analytics at scale",permalink:"/cn/blog/tags/analytics-at-scale"},{label:"uber",permalink:"/cn/blog/tags/uber"}],readingTime:.045,truncated:!1,authors:[{name:"Xinli Shang"},{name:"Kai Jiang"},{name:"Zheng Shao"},{name:"Mohammad Islam"}],prevItem:{title:"Onehouse Commitment to Openness",permalink:"/cn/blog/2022/02/02/Onehouse-Commitment-to-Openness"},nextItem:{title:"Hudi powering data lake efforts at Walmart and Disney+ Hotstar",permalink:"/cn/blog/2022/01/20/Hudi-powering-data-lake-efforts-at-Walmart-and-Disney-Hotstar"}},d={authorsImageUrls:[void 0,void 0,void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://eng.uber.com/cost-efficiency-big-data/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},80318:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Onehouse Commitment to Openness",authors:[{name:"Vinoth Chandar"}],category:"blog",image:"/assets/images/blog/2022-02-02-onehouse-commitment-to-openness.jpeg",tags:["blog","community","onehouse"]},s=void 0,l={permalink:"/cn/blog/2022/02/02/Onehouse-Commitment-to-Openness",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-02-02-Onehouse-Commitment-to-Openness.mdx",source:"@site/blog/2022-02-02-Onehouse-Commitment-to-Openness.mdx",title:"Onehouse Commitment to Openness",description:"Redirecting... please wait!!",date:"2022-02-02T00:00:00.000Z",formattedDate:"February 2, 2022",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"community",permalink:"/cn/blog/tags/community"},{label:"onehouse",permalink:"/cn/blog/tags/onehouse"}],readingTime:.045,truncated:!1,authors:[{name:"Vinoth Chandar"}],prevItem:{title:"Onehouse brings a fully-managed lakehouse to Apache Hudi",permalink:"/cn/blog/2022/02/03/Onehouse-brings-a-fully-managed-lakehouse-to-Apache-Hudi"},nextItem:{title:"Cost Efficiency @ Scale in Big Data File Format",permalink:"/cn/blog/2022/01/25/Cost-Efficiency-Scale-in-Big-Data-File-Format"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.onehouse.ai/blog/onehouse-commitment-to-openness",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},41836:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Onehouse brings a fully-managed lakehouse to Apache Hudi",authors:[{name:"Paul Sawers"}],category:"blog",image:"/assets/images/blog/2022-02-03-onehouse_billboard.png",tags:["blog","lakehouse","venturebeat"]},s=void 0,l={permalink:"/cn/blog/2022/02/03/Onehouse-brings-a-fully-managed-lakehouse-to-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-02-03-Onehouse-brings-a-fully-managed-lakehouse-to-Apache-Hudi.mdx",source:"@site/blog/2022-02-03-Onehouse-brings-a-fully-managed-lakehouse-to-Apache-Hudi.mdx",title:"Onehouse brings a fully-managed lakehouse to Apache Hudi",description:"Redirecting... please wait!!",date:"2022-02-03T00:00:00.000Z",formattedDate:"February 3, 2022",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"lakehouse",permalink:"/cn/blog/tags/lakehouse"},{label:"venturebeat",permalink:"/cn/blog/tags/venturebeat"}],readingTime:.045,truncated:!1,authors:[{name:"Paul Sawers"}],prevItem:{title:"ACID transformations on Distributed file system",permalink:"/cn/blog/2022/02/09/ACID-transformations-on-Distributed-file-system"},nextItem:{title:"Onehouse Commitment to Openness",permalink:"/cn/blog/2022/02/02/Onehouse-Commitment-to-Openness"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://venturebeat.com/2022/02/03/onehouse-brings-a-fully-managed-lakehouse-to-apache-hudi/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},79014:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"ACID transformations on Distributed file system",authors:[{name:"Rajasekhar"}],category:"blog",image:"/assets/images/blog/2022-02-09-acid-transformations-on-distributed-files-systems.png",tags:["blog","walmartglobaltech"]},s=void 0,l={permalink:"/cn/blog/2022/02/09/ACID-transformations-on-Distributed-file-system",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-02-09-ACID-transformations-on-Distributed-file-system.mdx",source:"@site/blog/2022-02-09-ACID-transformations-on-Distributed-file-system.mdx",title:"ACID transformations on Distributed file system",description:"Redirecting... please wait!!",date:"2022-02-09T00:00:00.000Z",formattedDate:"February 9, 2022",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"walmartglobaltech",permalink:"/cn/blog/tags/walmartglobaltech"}],readingTime:.045,truncated:!1,authors:[{name:"Rajasekhar"}],prevItem:{title:"Open Source Data Lake Table Formats: Evaluating Current Interest and Rate of Adoption",permalink:"/cn/blog/2022/02/12/Open-Source-Data-Lake-Table-Formats-Evaluating-Current-Interest-and-Rate-of-Adoption"},nextItem:{title:"Onehouse brings a fully-managed lakehouse to Apache Hudi",permalink:"/cn/blog/2022/02/03/Onehouse-brings-a-fully-managed-lakehouse-to-Apache-Hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/walmartglobaltech/acid-transformations-on-distributed-file-system-fdec5301c1b1",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},89906:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Open Source Data Lake Table Formats: Evaluating Current Interest and Rate of Adoption",authors:[{name:"Gary Stafford"}],category:"blog",image:"/assets/images/blog/2022-02-12-open-source-data-lake-formats.png",tags:["blog","datalake","comparison","community","medium"]},s=void 0,l={permalink:"/cn/blog/2022/02/12/Open-Source-Data-Lake-Table-Formats-Evaluating-Current-Interest-and-Rate-of-Adoption",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-02-12-Open-Source-Data-Lake-Table-Formats-Evaluating-Current-Interest-and-Rate-of-Adoption.mdx",source:"@site/blog/2022-02-12-Open-Source-Data-Lake-Table-Formats-Evaluating-Current-Interest-and-Rate-of-Adoption.mdx",title:"Open Source Data Lake Table Formats: Evaluating Current Interest and Rate of Adoption",description:"Redirecting... please wait!!",date:"2022-02-12T00:00:00.000Z",formattedDate:"February 12, 2022",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"datalake",permalink:"/cn/blog/tags/datalake"},{label:"comparison",permalink:"/cn/blog/tags/comparison"},{label:"community",permalink:"/cn/blog/tags/community"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Gary Stafford"}],prevItem:{title:"Fresher Data Lake on AWS S3",permalink:"/cn/blog/2022/02/17/Fresher-Data-Lake-on-AWS-S3"},nextItem:{title:"ACID transformations on Distributed file system",permalink:"/cn/blog/2022/02/09/ACID-transformations-on-Distributed-file-system"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://garystafford.medium.com/data-lake-table-formats-interest-and-adoption-rate-40817b87be9e",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},8155:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Fresher Data Lake on AWS S3",authors:[{name:"Balaji Varadarajan"}],category:"blog",image:"/assets/images/blog/2022-02-17-fresher-data-lake-on-aws-s3.png",tags:["use-case","incremental processing","robinhood"]},s=void 0,l={permalink:"/cn/blog/2022/02/17/Fresher-Data-Lake-on-AWS-S3",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-02-17-Fresher-Data-Lake-on-AWS-S3.mdx",source:"@site/blog/2022-02-17-Fresher-Data-Lake-on-AWS-S3.mdx",title:"Fresher Data Lake on AWS S3",description:"Redirecting... please wait!!",date:"2022-02-17T00:00:00.000Z",formattedDate:"February 17, 2022",tags:[{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"incremental processing",permalink:"/cn/blog/tags/incremental-processing"},{label:"robinhood",permalink:"/cn/blog/tags/robinhood"}],readingTime:.045,truncated:!1,authors:[{name:"Balaji Varadarajan"}],prevItem:{title:"Understanding its core concepts from hudi persistence files",permalink:"/cn/blog/2022/02/20/Understanding-its-core-concepts-from-hudi-persistence-files"},nextItem:{title:"Open Source Data Lake Table Formats: Evaluating Current Interest and Rate of Adoption",permalink:"/cn/blog/2022/02/12/Open-Source-Data-Lake-Table-Formats-Evaluating-Current-Interest-and-Rate-of-Adoption"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://robinhood.engineering/author-balaji-varadarajan-e3f496815ebf",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},89449:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Understanding its core concepts from hudi persistence files",authors:[{name:"QbertsBrother"}],category:"blog",image:"/assets/images/blog/2022-02-20-understanding-core-concepts-from-hudi-persistence-files.png",tags:["blog","storage spec","programmer"]},s=void 0,l={permalink:"/cn/blog/2022/02/20/Understanding-its-core-concepts-from-hudi-persistence-files",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-02-20-Understanding-its-core-concepts-from-hudi-persistence-files.mdx",source:"@site/blog/2022-02-20-Understanding-its-core-concepts-from-hudi-persistence-files.mdx",title:"Understanding its core concepts from hudi persistence files",description:"Redirecting... please wait!!",date:"2022-02-20T00:00:00.000Z",formattedDate:"February 20, 2022",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"storage spec",permalink:"/cn/blog/tags/storage-spec"},{label:"programmer",permalink:"/cn/blog/tags/programmer"}],readingTime:.045,truncated:!1,authors:[{name:"QbertsBrother"}],prevItem:{title:"Create a low-latency source-to-data lake pipeline using Amazon MSK Connect, Apache Flink, and Apache Hudi",permalink:"/cn/blog/2022/03/01/Create-a-low-latency-source-to-data-lake-pipeline-using-Amazon-MSK-Connect-Apache-Flink-and-Apache-Hudi"},nextItem:{title:"Fresher Data Lake on AWS S3",permalink:"/cn/blog/2022/02/17/Fresher-Data-Lake-on-AWS-S3"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://programmer.ink/think/understanding-its-core-concepts-from-hudi-persistence-files.html",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},6511:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Create a low-latency source-to-data lake pipeline using Amazon MSK Connect, Apache Flink, and Apache Hudi",authors:[{name:"Ali Alemi"}],category:"blog",image:"/assets/images/blog/2022-03-01-low-latency-pipeline-using-msk-flink-hudi.png",tags:["how-to","streaming ingestion","apache flink","apache kafka","amazon"]},s=void 0,l={permalink:"/cn/blog/2022/03/01/Create-a-low-latency-source-to-data-lake-pipeline-using-Amazon-MSK-Connect-Apache-Flink-and-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-03-01-Create-a-low-latency-source-to-data-lake-pipeline-using-Amazon-MSK-Connect-Apache-Flink-and-Apache-Hudi.mdx",source:"@site/blog/2022-03-01-Create-a-low-latency-source-to-data-lake-pipeline-using-Amazon-MSK-Connect-Apache-Flink-and-Apache-Hudi.mdx",title:"Create a low-latency source-to-data lake pipeline using Amazon MSK Connect, Apache Flink, and Apache Hudi",description:"Redirecting... please wait!!",date:"2022-03-01T00:00:00.000Z",formattedDate:"March 1, 2022",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"streaming ingestion",permalink:"/cn/blog/tags/streaming-ingestion"},{label:"apache flink",permalink:"/cn/blog/tags/apache-flink"},{label:"apache kafka",permalink:"/cn/blog/tags/apache-kafka"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[{name:"Ali Alemi"}],prevItem:{title:"Build a serverless pipeline to analyze streaming data using AWS Glue, Apache Hudi, and Amazon S3",permalink:"/cn/blog/2022/03/09/Build-a-serverless-pipeline-to-analyze-streaming-data-using-AWS-Glue-Apache-Hudi-and-Amazon-S3"},nextItem:{title:"Understanding its core concepts from hudi persistence files",permalink:"/cn/blog/2022/02/20/Understanding-its-core-concepts-from-hudi-persistence-files"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/create-a-low-latency-source-to-data-lake-pipeline-using-amazon-msk-connect-apache-flink-and-apache-hudi/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},28718:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Build a serverless pipeline to analyze streaming data using AWS Glue, Apache Hudi, and Amazon S3",authors:[{name:"Nikhil Khokhar"},{name:"Dipta Bhattacharya"}],category:"blog",image:"/assets/images/blog/2022-03-09-serverless-pipeline-using-glue-hudi-s3.png",tags:["how-to","streaming ingestion","amazon"]},s=void 0,l={permalink:"/cn/blog/2022/03/09/Build-a-serverless-pipeline-to-analyze-streaming-data-using-AWS-Glue-Apache-Hudi-and-Amazon-S3",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-03-09-Build-a-serverless-pipeline-to-analyze-streaming-data-using-AWS-Glue-Apache-Hudi-and-Amazon-S3.mdx",source:"@site/blog/2022-03-09-Build-a-serverless-pipeline-to-analyze-streaming-data-using-AWS-Glue-Apache-Hudi-and-Amazon-S3.mdx",title:"Build a serverless pipeline to analyze streaming data using AWS Glue, Apache Hudi, and Amazon S3",description:"Redirecting... please wait!!",date:"2022-03-09T00:00:00.000Z",formattedDate:"March 9, 2022",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"streaming ingestion",permalink:"/cn/blog/tags/streaming-ingestion"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[{name:"Nikhil Khokhar"},{name:"Dipta Bhattacharya"}],prevItem:{title:"Zendesk - Insights for CTOs: Part 3 \u2013 Growing your business with modern data capabilities",permalink:"/cn/blog/2022/03/24/Zendesk-Insights-for-CTOs-Part-3-Growing-your-business-with-modern-data-capabilities"},nextItem:{title:"Create a low-latency source-to-data lake pipeline using Amazon MSK Connect, Apache Flink, and Apache Hudi",permalink:"/cn/blog/2022/03/01/Create-a-low-latency-source-to-data-lake-pipeline-using-Amazon-MSK-Connect-Apache-Flink-and-Apache-Hudi"}},d={authorsImageUrls:[void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/build-a-serverless-pipeline-to-analyze-streaming-data-using-aws-glue-apache-hudi-and-amazon-s3/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},98357:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Zendesk - Insights for CTOs: Part 3 \u2013 Growing your business with modern data capabilities",authors:[{name:"Syed Jaffry"},{name:"Johnathan Hwang"}],category:"blog",image:"/assets/images/blog/2022-03-24-insights-for-ctos-part-3.png",tags:["blog","modern data architecture","near real-time analytics","gdpr deletion","streaming ingestion","amazon"]},s=void 0,l={permalink:"/cn/blog/2022/03/24/Zendesk-Insights-for-CTOs-Part-3-Growing-your-business-with-modern-data-capabilities",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-03-24-Zendesk-Insights-for-CTOs-Part-3-Growing-your-business-with-modern-data-capabilities.mdx",source:"@site/blog/2022-03-24-Zendesk-Insights-for-CTOs-Part-3-Growing-your-business-with-modern-data-capabilities.mdx",title:"Zendesk - Insights for CTOs: Part 3 \u2013 Growing your business with modern data capabilities",description:"Redirecting... please wait!!",date:"2022-03-24T00:00:00.000Z",formattedDate:"March 24, 2022",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"modern data architecture",permalink:"/cn/blog/tags/modern-data-architecture"},{label:"near real-time analytics",permalink:"/cn/blog/tags/near-real-time-analytics"},{label:"gdpr deletion",permalink:"/cn/blog/tags/gdpr-deletion"},{label:"streaming ingestion",permalink:"/cn/blog/tags/streaming-ingestion"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[{name:"Syed Jaffry"},{name:"Johnathan Hwang"}],prevItem:{title:"New features from Apache Hudi 0.9.0 on Amazon EMR",permalink:"/cn/blog/2022/04/04/New-features-from-Apache-Hudi-0.9.0-on-Amazon-EMR"},nextItem:{title:"Build a serverless pipeline to analyze streaming data using AWS Glue, Apache Hudi, and Amazon S3",permalink:"/cn/blog/2022/03/09/Build-a-serverless-pipeline-to-analyze-streaming-data-using-AWS-Glue-Apache-Hudi-and-Amazon-S3"}},d={authorsImageUrls:[void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/architecture/insights-for-ctos-part-3-growing-your-business-with-modern-data-capabilities/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},48644:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Key Learnings on Using Apache HUDI in building Lakehouse Architecture @ Halodoc",authors:[{name:"Jitendra Shah"}],category:"blog",image:"/assets/images/blog/2022-04-04-halodoc-lakehouse-architecture.png",tags:["use-case","lakehouse","incremental processing","halodoc"]},s=void 0,l={permalink:"/cn/blog/2022/04/04/Key-Learnings-on-Using-Apache-HUDI-in-building-Lakehouse-Architecture-at-Halodoc",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-04-04-Key-Learnings-on-Using-Apache-HUDI-in-building-Lakehouse-Architecture-at-Halodoc.mdx",source:"@site/blog/2022-04-04-Key-Learnings-on-Using-Apache-HUDI-in-building-Lakehouse-Architecture-at-Halodoc.mdx",title:"Key Learnings on Using Apache HUDI in building Lakehouse Architecture @ Halodoc",description:"Redirecting... please wait!!",date:"2022-04-04T00:00:00.000Z",formattedDate:"April 4, 2022",tags:[{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"lakehouse",permalink:"/cn/blog/tags/lakehouse"},{label:"incremental processing",permalink:"/cn/blog/tags/incremental-processing"},{label:"halodoc",permalink:"/cn/blog/tags/halodoc"}],readingTime:.045,truncated:!1,authors:[{name:"Jitendra Shah"}],prevItem:{title:"Corrections in data lakehouse table format comparisons",permalink:"/cn/blog/2022/04/19/Corrections-in-data-lakehouse-table-format-comparisons"},nextItem:{title:"New features from Apache Hudi 0.9.0 on Amazon EMR",permalink:"/cn/blog/2022/04/04/New-features-from-Apache-Hudi-0.9.0-on-Amazon-EMR"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://blogs.halodoc.io/key-learnings-on-using-apache-hudi-in-building-lakehouse-architecture-halodoc/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},25075:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"New features from Apache Hudi 0.9.0 on Amazon EMR",authors:[{name:"Kunal Gautam"},{name:"Gabriele Cacciola"},{name:"Udit Mehrotra"}],category:"blog",image:"/assets/images/blog/aws.jpg",tags:["blog","amazon"]},s=void 0,l={permalink:"/cn/blog/2022/04/04/New-features-from-Apache-Hudi-0.9.0-on-Amazon-EMR",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-04-04-New-features-from-Apache-Hudi-0.9.0-on-Amazon-EMR.mdx",source:"@site/blog/2022-04-04-New-features-from-Apache-Hudi-0.9.0-on-Amazon-EMR.mdx",title:"New features from Apache Hudi 0.9.0 on Amazon EMR",description:"Redirecting... please wait!!",date:"2022-04-04T00:00:00.000Z",formattedDate:"April 4, 2022",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[{name:"Kunal Gautam"},{name:"Gabriele Cacciola"},{name:"Udit Mehrotra"}],prevItem:{title:"Key Learnings on Using Apache HUDI in building Lakehouse Architecture @ Halodoc",permalink:"/cn/blog/2022/04/04/Key-Learnings-on-Using-Apache-HUDI-in-building-Lakehouse-Architecture-at-Halodoc"},nextItem:{title:"Zendesk - Insights for CTOs: Part 3 \u2013 Growing your business with modern data capabilities",permalink:"/cn/blog/2022/03/24/Zendesk-Insights-for-CTOs-Part-3-Growing-your-business-with-modern-data-capabilities"}},d={authorsImageUrls:[void 0,void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/new-features-from-apache-hudi-0-9-0-on-amazon-emr/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},21592:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Corrections in data lakehouse table format comparisons",authors:[{name:"Vinoth Chandar"}],category:"blog",image:"/assets/images/blog/2022-04-19-corrections-in-data-lakehouse-table-format-comparisons.png",tags:["blog","lakehouse","bytearray"]},s=void 0,l={permalink:"/cn/blog/2022/04/19/Corrections-in-data-lakehouse-table-format-comparisons",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-04-19-Corrections-in-data-lakehouse-table-format-comparisons.mdx",source:"@site/blog/2022-04-19-Corrections-in-data-lakehouse-table-format-comparisons.mdx",title:"Corrections in data lakehouse table format comparisons",description:"Redirecting... please wait!!",date:"2022-04-19T00:00:00.000Z",formattedDate:"April 19, 2022",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"lakehouse",permalink:"/cn/blog/tags/lakehouse"},{label:"bytearray",permalink:"/cn/blog/tags/bytearray"}],readingTime:.045,truncated:!1,authors:[{name:"Vinoth Chandar"}],prevItem:{title:"Multi-Modal Index for the Lakehouse in Apache Hudi",permalink:"/cn/blog/2022/05/17/Introducing-Multi-Modal-Index-for-the-Lakehouse-in-Apache-Hudi"},nextItem:{title:"Key Learnings on Using Apache HUDI in building Lakehouse Architecture @ Halodoc",permalink:"/cn/blog/2022/04/04/Key-Learnings-on-Using-Apache-HUDI-in-building-Lakehouse-Architecture-at-Halodoc"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://bytearray.io/corrections-in-data-lakehouse-table-format-comparisons-b72eb63ece32",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},93386:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Multi-Modal Index for the Lakehouse in Apache Hudi",authors:[{name:"Sivabalan Narayanan"},{name:"Ethan Guo"}],category:"blog",image:"/assets/images/blog/2022-05-17-multimodal-index.gif",tags:["design","multi modal indexing","lakehouse","onehouse"]},s=void 0,l={permalink:"/cn/blog/2022/05/17/Introducing-Multi-Modal-Index-for-the-Lakehouse-in-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-05-17-Introducing-Multi-Modal-Index-for-the-Lakehouse-in-Apache-Hudi.mdx",source:"@site/blog/2022-05-17-Introducing-Multi-Modal-Index-for-the-Lakehouse-in-Apache-Hudi.mdx",title:"Multi-Modal Index for the Lakehouse in Apache Hudi",description:"Redirecting... please wait!!",date:"2022-05-17T00:00:00.000Z",formattedDate:"May 17, 2022",tags:[{label:"design",permalink:"/cn/blog/tags/design"},{label:"multi modal indexing",permalink:"/cn/blog/tags/multi-modal-indexing"},{label:"lakehouse",permalink:"/cn/blog/tags/lakehouse"},{label:"onehouse",permalink:"/cn/blog/tags/onehouse"}],readingTime:.045,truncated:!1,authors:[{name:"Sivabalan Narayanan"},{name:"Ethan Guo"}],prevItem:{title:"The story of building a data lake that can be deleted on a record-by-record basis using Apache Hudi",permalink:"/cn/blog/2022/05/25/Record-by-record-deletable-data-lake-using-Apache-Hudi"},nextItem:{title:"Corrections in data lakehouse table format comparisons",permalink:"/cn/blog/2022/04/19/Corrections-in-data-lakehouse-table-format-comparisons"}},d={authorsImageUrls:[void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.onehouse.ai/blog/introducing-multi-modal-index-for-the-lakehouse-in-apache-hudi",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},37092:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"The story of building a data lake that can be deleted on a record-by-record basis using Apache Hudi",authors:[{name:"Shota Ejima"}],category:"blog",image:"/assets/images/blog/2022-05-25-data-lake-at-yahoo-advertising-at-yahoo-japan.png",tags:["use-case","gdpr deletion","yahoo"]},s=void 0,l={permalink:"/cn/blog/2022/05/25/Record-by-record-deletable-data-lake-using-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-05-25-Record-by-record-deletable-data-lake-using-Apache-Hudi.mdx",source:"@site/blog/2022-05-25-Record-by-record-deletable-data-lake-using-Apache-Hudi.mdx",title:"The story of building a data lake that can be deleted on a record-by-record basis using Apache Hudi",description:"Redirecting... please wait!!",date:"2022-05-25T00:00:00.000Z",formattedDate:"May 25, 2022",tags:[{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"gdpr deletion",permalink:"/cn/blog/tags/gdpr-deletion"},{label:"yahoo",permalink:"/cn/blog/tags/yahoo"}],readingTime:.045,truncated:!1,authors:[{name:"Shota Ejima"}],prevItem:{title:"Asynchronous Indexing using Hudi",permalink:"/cn/blog/2022/06/04/Asynchronous-Indexing-Using-Hudi"},nextItem:{title:"Multi-Modal Index for the Lakehouse in Apache Hudi",permalink:"/cn/blog/2022/05/17/Introducing-Multi-Modal-Index-for-the-Lakehouse-in-Apache-Hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://techblog.yahoo.co.jp/entry/2022052530303179/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},86229:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Asynchronous Indexing using Hudi",authors:[{name:"Sagar Sumit"}],category:"blog",image:"/assets/images/blog/2022-06-04-async-index.png",tags:["design","multi modal indexing","onehouse","async indexing"]},s=void 0,l={permalink:"/cn/blog/2022/06/04/Asynchronous-Indexing-Using-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-06-04-Asynchronous-Indexing-Using-Hudi.mdx",source:"@site/blog/2022-06-04-Asynchronous-Indexing-Using-Hudi.mdx",title:"Asynchronous Indexing using Hudi",description:"Redirecting... please wait!!",date:"2022-06-04T00:00:00.000Z",formattedDate:"June 4, 2022",tags:[{label:"design",permalink:"/cn/blog/tags/design"},{label:"multi modal indexing",permalink:"/cn/blog/tags/multi-modal-indexing"},{label:"onehouse",permalink:"/cn/blog/tags/onehouse"},{label:"async indexing",permalink:"/cn/blog/tags/async-indexing"}],readingTime:.045,truncated:!1,authors:[{name:"Sagar Sumit"}],prevItem:{title:"Hudi\u2019s Column Stats Index and Data Skipping feature help speed up queries by an orders of magnitude!",permalink:"/cn/blog/2022/06/09/Singificant-queries-speedup-from-Hudi-Column-Stats-Index-and-Data-Skipping-features"},nextItem:{title:"The story of building a data lake that can be deleted on a record-by-record basis using Apache Hudi",permalink:"/cn/blog/2022/05/25/Record-by-record-deletable-data-lake-using-Apache-Hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.onehouse.ai/blog/asynchronous-indexing-using-hudi",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},22229:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Hudi\u2019s Column Stats Index and Data Skipping feature help speed up queries by an orders of magnitude!",authors:[{name:"Alexey Kudinkin"}],category:"blog",image:"/assets/images/blog/2022-06-09-col-stats-and-data-skipping.png",tags:["design","indexing","data skipping","onehouse"]},s=void 0,l={permalink:"/cn/blog/2022/06/09/Singificant-queries-speedup-from-Hudi-Column-Stats-Index-and-Data-Skipping-features",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-06-09-Singificant-queries-speedup-from-Hudi-Column-Stats-Index-and-Data-Skipping-features.mdx",source:"@site/blog/2022-06-09-Singificant-queries-speedup-from-Hudi-Column-Stats-Index-and-Data-Skipping-features.mdx",title:"Hudi\u2019s Column Stats Index and Data Skipping feature help speed up queries by an orders of magnitude!",description:"Redirecting... please wait!!",date:"2022-06-09T00:00:00.000Z",formattedDate:"June 9, 2022",tags:[{label:"design",permalink:"/cn/blog/tags/design"},{label:"indexing",permalink:"/cn/blog/tags/indexing"},{label:"data skipping",permalink:"/cn/blog/tags/data-skipping"},{label:"onehouse",permalink:"/cn/blog/tags/onehouse"}],readingTime:.045,truncated:!1,authors:[{name:"Alexey Kudinkin"}],prevItem:{title:"Apache Hudi vs Delta Lake - Transparent TPC-DS Lakehouse Performance Benchmarks",permalink:"/cn/blog/2022/06/29/Apache-Hudi-vs-Delta-Lake-transparent-tpc-ds-lakehouse-performance-benchmarks"},nextItem:{title:"Asynchronous Indexing using Hudi",permalink:"/cn/blog/2022/06/04/Asynchronous-Indexing-Using-Hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.onehouse.ai/blog/hudis-column-stats-index-and-data-skipping-feature-help-speed-up-queries-by-an-orders-of-magnitude",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},61574:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Apache Hudi vs Delta Lake - Transparent TPC-DS Lakehouse Performance Benchmarks",authors:[{name:"Alexey Kudinkin"}],category:"blog",image:"/assets/images/blog/2022-06-29-apache_hudi_vs_delta_lake_tpc_ds_benchmarks.png",tags:["performance","datalake","comparison","onehouse"]},s=void 0,l={permalink:"/cn/blog/2022/06/29/Apache-Hudi-vs-Delta-Lake-transparent-tpc-ds-lakehouse-performance-benchmarks",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-06-29-Apache-Hudi-vs-Delta-Lake-transparent-tpc-ds-lakehouse-performance-benchmarks.mdx",source:"@site/blog/2022-06-29-Apache-Hudi-vs-Delta-Lake-transparent-tpc-ds-lakehouse-performance-benchmarks.mdx",title:"Apache Hudi vs Delta Lake - Transparent TPC-DS Lakehouse Performance Benchmarks",description:"Redirecting... please wait!!",date:"2022-06-29T00:00:00.000Z",formattedDate:"June 29, 2022",tags:[{label:"performance",permalink:"/cn/blog/tags/performance"},{label:"datalake",permalink:"/cn/blog/tags/datalake"},{label:"comparison",permalink:"/cn/blog/tags/comparison"},{label:"onehouse",permalink:"/cn/blog/tags/onehouse"}],readingTime:.045,truncated:!1,authors:[{name:"Alexey Kudinkin"}],prevItem:{title:"Build Open Lakehouse using Apache Hudi & dbt",permalink:"/cn/blog/2022/07/11/build-open-lakehouse-using-apache-hudi-and-dbt"},nextItem:{title:"Hudi\u2019s Column Stats Index and Data Skipping feature help speed up queries by an orders of magnitude!",permalink:"/cn/blog/2022/06/09/Singificant-queries-speedup-from-Hudi-Column-Stats-Index-and-Data-Skipping-features"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.onehouse.ai/blog/apache-hudi-vs-delta-lake-transparent-tpc-ds-lakehouse-performance-benchmarks",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},96814:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Build Open Lakehouse using Apache Hudi & dbt",excerpt:"How to style blog focused projects on teaching how to build an open Lakehouse using Apache Hudi & dbt",author:"Vinoth Govindarajan",category:"blog",image:"/assets/images/blog/hudi_dbt_lakehouse.png",tags:["how-to","deltastreamer","incremental processing","apache hudi"]},r=void 0,s={permalink:"/cn/blog/2022/07/11/build-open-lakehouse-using-apache-hudi-and-dbt",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-07-11-build-open-lakehouse-using-apache-hudi-and-dbt.md",source:"@site/blog/2022-07-11-build-open-lakehouse-using-apache-hudi-and-dbt.md",title:"Build Open Lakehouse using Apache Hudi & dbt",description:"The focus of this blog is to show you how to build an open lakehouse leveraging incremental data processing and performing field-level updates. We are excited to announce that you can now use Apache Hudi + dbt for building open data lakehouses.",date:"2022-07-11T00:00:00.000Z",formattedDate:"July 11, 2022",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"deltastreamer",permalink:"/cn/blog/tags/deltastreamer"},{label:"incremental processing",permalink:"/cn/blog/tags/incremental-processing"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:6.64,truncated:!1,authors:[{name:"Vinoth Govindarajan"}],prevItem:{title:"How NerdWallet uses AWS and Apache Hudi to build a serverless, real-time analytics platform",permalink:"/cn/blog/2022/08/09/How-NerdWallet-uses-AWS-and-Apache-Hudi-to-build-a-serverless-real-time-analytics-platform"},nextItem:{title:"Apache Hudi vs Delta Lake - Transparent TPC-DS Lakehouse Performance Benchmarks",permalink:"/cn/blog/2022/06/29/Apache-Hudi-vs-Delta-Lake-transparent-tpc-ds-lakehouse-performance-benchmarks"}},l={authorsImageUrls:[void 0]},d=[{value:"What is Apache Hudi?",id:"what-is-apache-hudi",children:[],level:2},{value:"What is dbt?",id:"what-is-dbt",children:[],level:2},{value:"What is a Lakehouse?",id:"what-is-a-lakehouse",children:[],level:2},{value:"How to build an open lakehouse?",id:"how-to-build-an-open-lakehouse",children:[],level:2},{value:"Step 1: How to extract &amp; load the raw data datasets?",id:"step-1-how-to-extract--load-the-raw-data-datasets",children:[],level:2},{value:"Step 2: How to configure hudi with the dbt project?",id:"step-2-how-to-configure-hudi-with-the-dbt-project",children:[],level:2},{value:"Step 3: How to read the raw data incrementally?",id:"step-3-how-to-read-the-raw-data-incrementally",children:[{value:"How to apply filters on an incremental run?",id:"how-to-apply-filters-on-an-incremental-run",children:[],level:3},{value:"How to define the uniqueness constraint?",id:"how-to-define-the-uniqueness-constraint",children:[],level:3}],level:2},{value:"Step 4: How to use the upsert feature while writing datasets?",id:"step-4-how-to-use-the-upsert-feature-while-writing-datasets",children:[{value:"How to perform field-level updates?",id:"how-to-perform-field-level-updates",children:[],level:3},{value:"How to configure additional hoodie custom configs?",id:"how-to-configure-additional-hoodie-custom-configs",children:[],level:3}],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...o}=e;return(0,n.yg)(p,(0,i.A)({},c,o,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"The focus of this blog is to show you how to build an open lakehouse leveraging incremental data processing and performing field-level updates. We are excited to announce that you can now use Apache Hudi + dbt for building open data lakehouses."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"/assets/images/blog/hudi_dbt_lakehouse.png",src:t(35707).A})),(0,n.yg)("p",null,"Let's first clarify a few terminologies used in this blog before we dive into the details."),(0,n.yg)("h2",{id:"what-is-apache-hudi"},"What is Apache Hudi?"),(0,n.yg)("p",null,"Apache Hudi brings ACID transactions, record-level updates/deletes, and change streams to data lakehouses."),(0,n.yg)("p",null,"Apache Hudi is an open-source data management framework used to simplify incremental data processing and data pipeline development. This framework more efficiently manages business requirements like data lifecycle and improves data quality."),(0,n.yg)("h2",{id:"what-is-dbt"},"What is dbt?"),(0,n.yg)("p",null,"dbt (data build tool) is a data transformation tool that enables data analysts and engineers to transform, test, and document data in the cloud data warehouses."),(0,n.yg)("p",null,"dbt enables analytics engineers to transform data in their warehouses by simply writing select statements. dbt handles turning these select statements into tables and views."),(0,n.yg)("p",null,"dbt does the T in ELT (Extract, Load, Transform) processes \u2013 it doesn\u2019t extract or load data, but it\u2019s extremely good at transforming data that\u2019s already loaded into your warehouse."),(0,n.yg)("h2",{id:"what-is-a-lakehouse"},"What is a Lakehouse?"),(0,n.yg)("p",null,"A lakehouse is a new, open architecture that combines the best elements of data lakes and data warehouses. Lakehouses are enabled by a new system design: implementing transaction management and data management features similar to those in a data warehouse directly on top of low-cost cloud storage in open formats. They are what you would get if you had to redesign data warehouses in the modern world, now that cheap and highly reliable storage (in the form of object stores) are available."),(0,n.yg)("p",null,"In other words, while data lakes historically have been viewed as a bunch of files added to cloud storage folders, lakehouse tables support transactions, updates, deletes, and in the case of Apache Hudi, even database-like functionality like indexing or change capture."),(0,n.yg)("h2",{id:"how-to-build-an-open-lakehouse"},"How to build an open lakehouse?"),(0,n.yg)("p",null,"Now, we know what is a lakehouse, so let's build one, In order to build an open lakehouse, you need a few components:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Open table format which supports ACID transactions",(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Apache Hudi (integrated with dbt)"),(0,n.yg)("li",{parentName:"ul"},"Delta Lake (proprietary features locked to Databricks runtime)"),(0,n.yg)("li",{parentName:"ul"},"Apache Iceberg (currently not integrated with dbt)"))),(0,n.yg)("li",{parentName:"ul"},"Data transformation tool",(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Open source dbt is the de-facto popular choice for transformation layer"))),(0,n.yg)("li",{parentName:"ul"},"Distributed data processing engine",(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Apache Spark is the de-facto popular choice for compute engine"))),(0,n.yg)("li",{parentName:"ul"},"Cloud Storage",(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"You can choose any of the cost-effective cloud stores or HDFS"))),(0,n.yg)("li",{parentName:"ul"},"Bring your favorite query engine")),(0,n.yg)("p",null,"To build the lakehouse you need a way to extract and load the data into Hudi table format and then transform in-place using dbt."),(0,n.yg)("p",null,"DBT supports Hudi out of the box with the ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/dbt-labs/dbt-spark"},"dbt-spark")," adapter package. When creating modeled datasets using dbt you can choose Hudi as the format for your tables."),(0,n.yg)("p",null,"You can follow the instructions on this ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/master/hudi-examples/hudi-examples-dbt/README.md"},"page")," to learn how to install and configure dbt+hudi."),(0,n.yg)("h2",{id:"step-1-how-to-extract--load-the-raw-data-datasets"},"Step 1: How to extract & load the raw data datasets?"),(0,n.yg)("p",null,"This is the first step in building your data lake and there are many choices here to load the data into our open lakehouse. I\u2019m going to go with one of the Hudi\u2019s native tools called Delta Streamer since all the ingestion features are pre-built and battle-tested in production at scale."),(0,n.yg)("p",null,"Hudi\u2019s ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/hoodie_streaming_ingestion"},"DeltaStreamer")," does the EL in ELT (Extract, Load, Transform) processes \u2013 it\u2019s extremely good at extracting, loading, and optionally ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/hoodie_streaming_ingestion#transformers"},"transforming data")," that\u2019s already loaded into your lakehouse."),(0,n.yg)("h2",{id:"step-2-how-to-configure-hudi-with-the-dbt-project"},"Step 2: How to configure hudi with the dbt project?"),(0,n.yg)("p",null,"To use the Hudi with your dbt project,  all you need to do is choose the file format as Hudi. The file format config can either be specified in specific models, or for all the models in your dbt_project.yml file:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-yml",metastring:'title="dbt_project.yml"',title:'"dbt_project.yml"'},"models:\n   +file_format: hudi\n")),(0,n.yg)("p",null,"or:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-sql",metastring:'title="model/my_model.sql"',title:'"model/my_model.sql"'},"{{ config(\n   materialized = 'incremental',\n   incremental_strategy = 'merge',\n   file_format = 'hudi',\n   unique_key = 'id',\n   \u2026\n) }}\n")),(0,n.yg)("p",null,"After choosing hudi as the file_format you can create materialized datasets using dbt, which offers additional benefits that are unique to the Hudi table format such as field-level upserts/deletes."),(0,n.yg)("h2",{id:"step-3-how-to-read-the-raw-data-incrementally"},"Step 3: How to read the raw data incrementally?"),(0,n.yg)("p",null,"Before we learn how to build incremental materialization, let\u2019s quickly learn, What are materializations in dbt? Materializations are strategies for persisting dbt models in a lakehouse. There are four types of materializations built into dbt. They are:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"table"),(0,n.yg)("li",{parentName:"ul"},"view"),(0,n.yg)("li",{parentName:"ul"},"incremental"),(0,n.yg)("li",{parentName:"ul"},"ephemeral")),(0,n.yg)("p",null,"Among all the materialization types, only incremental models allow dbt to insert or update records into a table since the last time that dbt was run, which unlocks the powers of Hudi, we will dive into the details."),(0,n.yg)("p",null,"To use incremental models, you need to perform these two activities:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Tell dbt how to filter the rows on the incremental executions"),(0,n.yg)("li",{parentName:"ol"},"Define the uniqueness constraint of the model (required when using >= Hudi 0.10.1 version)")),(0,n.yg)("h3",{id:"how-to-apply-filters-on-an-incremental-run"},"How to apply filters on an incremental run?"),(0,n.yg)("p",null,"dbt provides you a macro ",(0,n.yg)("inlineCode",{parentName:"p"},"is_incremental()")," which is very useful to define the filters exclusively for incremental materializations."),(0,n.yg)("p",null,'Often, you\'ll want to filter for "new" rows, as in, rows that have been created since the last time dbt ran this model. The best way to find the timestamp of the most recent run of this model is by checking the most recent timestamp in your target table. dbt makes it easy to query your target table by using the "',(0,n.yg)("a",{parentName:"p",href:"https://docs.getdbt.com/reference/dbt-jinja-functions/this"},"{{ this }}"),'" variable.'),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-sql",metastring:'title="models/my_model.sql"',title:'"models/my_model.sql"'},"{{\n   config(\n       materialized='incremental',\n       file_format='hudi',\n   )\n}}\n\nselect\n   *\nfrom raw_app_data.events\n{% if is_incremental() %}\n   -- this filter will only be applied on an incremental run\n   where event_time > (select max(event_time) from {{ this }})\n{% endif %}\n")),(0,n.yg)("h3",{id:"how-to-define-the-uniqueness-constraint"},"How to define the uniqueness constraint?"),(0,n.yg)("p",null,"A unique_key is the primary key of the dataset, which determines whether a record has new values and should be updated/deleted, or inserted."),(0,n.yg)("p",null,"You can define the unique_key in the configuration block at the top of your model. This unique_key will act as the primaryKey (hoodie.datasource.write.recordkey.field) on the hudi table."),(0,n.yg)("h2",{id:"step-4-how-to-use-the-upsert-feature-while-writing-datasets"},"Step 4: How to use the upsert feature while writing datasets?"),(0,n.yg)("p",null,"dbt offers multiple load strategies when loading the transformed datasets, such as:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"append (default)"),(0,n.yg)("li",{parentName:"ul"},"insert_overwrite (optional)"),(0,n.yg)("li",{parentName:"ul"},"merge (optional, Only available for Hudi and Delta formats)")),(0,n.yg)("p",null,"By default dbt uses the append strategy, which may cause duplicate rows when you execute dbt run command multiple times on the same payload."),(0,n.yg)("p",null,"When you choose the insert_overwrite strategy, dbt will overwrite the entire partition or full table load for every dbt run, which causes unnecessary overheads and is very expensive."),(0,n.yg)("p",null,"In addition to all the existing strategies to load the data, with hudi you can use the exclusive merge strategy when using incremental materialization. Using the merge strategy you can perform field-level updates/deletes on your data lakehouse which is performant and cost-efficient. As a result, you will get access to fresher data and accelerated insights."),(0,n.yg)("h3",{id:"how-to-perform-field-level-updates"},"How to perform field-level updates?"),(0,n.yg)("p",null,"If you are using the merge strategy and have specified a unique_key, by default, dbt will entirely overwrite matched rows with new values."),(0,n.yg)("p",null,"Since Apache Spark adapter supports the merge strategy, you may optionally pass a list of column names to a ",(0,n.yg)("inlineCode",{parentName:"p"},"merge_update_columns")," config. In that case, dbt will update only the columns specified by the config, and keep the previous values of other columns."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-sql",metastring:'title="models/my_model.sql"',title:'"models/my_model.sql"'},"{{ config(\n   materialized = 'incremental',\n   incremental_strategy = 'merge',\n   file_format = 'hudi',\n   unique_key = 'id',\n   merge_update_columns = ['msg', 'updated_ts'],\n) }}\n")),(0,n.yg)("h3",{id:"how-to-configure-additional-hoodie-custom-configs"},"How to configure additional hoodie custom configs?"),(0,n.yg)("p",null,"When you want to specify additional hudi configs, you can do that with the options config:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-sql",metastring:'title="models/my_model.sql"',title:'"models/my_model.sql"'},"{{ config(\n   materialized='incremental',\n   file_format='hudi',\n   incremental_strategy='merge',\n   options={\n       'type': 'mor',\n       'primaryKey': 'id',\n       'precombineKey': 'ts',\n   },\n   unique_key='id',\n   partition_by='datestr',\n   pre_hook=[\"set spark.sql.datetime.java8API.enabled=false;\"],\n  )\n}}\n")),(0,n.yg)("p",null,"Hope you understood the benefits of using Apache Hudi with dbt to build your next open lakehouse, good luck!"))}g.isMDXComponent=!0},12029:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"How NerdWallet uses AWS and Apache Hudi to build a serverless, real-time analytics platform",authors:[{name:"Kevin Chun"},{name:"Dylan Qu"}],category:"blog",image:"/assets/images/blog/2022-08-09-How-NerdWallet-uses-AWS-and-Apache-Hudi-to-build-a-serverless-real-time-analytics-platform.png",tags:["use-case","near real-time analytics","incremental processing","amazon"]},s=void 0,l={permalink:"/cn/blog/2022/08/09/How-NerdWallet-uses-AWS-and-Apache-Hudi-to-build-a-serverless-real-time-analytics-platform",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-08-09-How-NerdWallet-uses-AWS-and-Apache-Hudi-to-build-a-serverless-real-time-analytics-platform.mdx",source:"@site/blog/2022-08-09-How-NerdWallet-uses-AWS-and-Apache-Hudi-to-build-a-serverless-real-time-analytics-platform.mdx",title:"How NerdWallet uses AWS and Apache Hudi to build a serverless, real-time analytics platform",description:"Redirecting... please wait!!",date:"2022-08-09T00:00:00.000Z",formattedDate:"August 9, 2022",tags:[{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"near real-time analytics",permalink:"/cn/blog/tags/near-real-time-analytics"},{label:"incremental processing",permalink:"/cn/blog/tags/incremental-processing"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[{name:"Kevin Chun"},{name:"Dylan Qu"}],prevItem:{title:"Use Flink Hudi to Build a Streaming Data Lake Platform",permalink:"/cn/blog/2022/08/12/Use-Flink-Hudi-to-Build-a-Streaming-Data-Lake-Platform"},nextItem:{title:"Build Open Lakehouse using Apache Hudi & dbt",permalink:"/cn/blog/2022/07/11/build-open-lakehouse-using-apache-hudi-and-dbt"}},d={authorsImageUrls:[void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/how-nerdwallet-uses-aws-and-apache-hudi-to-build-a-serverless-real-time-analytics-platform/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},1135:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Use Flink Hudi to Build a Streaming Data Lake Platform",authors:[{name:"Chen Yuzhao"},{name:"Liu Dalong"}],category:"blog",image:"/assets/images/blog/2022-08-12-Use-Flink-Hudi-to-Build-a-Streaming-Data-Lake-Platform.png",tags:["blog","apache flink","alibabacloud","streaming ingestion"]},s=void 0,l={permalink:"/cn/blog/2022/08/12/Use-Flink-Hudi-to-Build-a-Streaming-Data-Lake-Platform",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-08-12-Use-Flink-Hudi-to-Build-a-Streaming-Data-Lake-Platform.mdx",source:"@site/blog/2022-08-12-Use-Flink-Hudi-to-Build-a-Streaming-Data-Lake-Platform.mdx",title:"Use Flink Hudi to Build a Streaming Data Lake Platform",description:"Redirecting... please wait!!",date:"2022-08-12T00:00:00.000Z",formattedDate:"August 12, 2022",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache flink",permalink:"/cn/blog/tags/apache-flink"},{label:"alibabacloud",permalink:"/cn/blog/tags/alibabacloud"},{label:"streaming ingestion",permalink:"/cn/blog/tags/streaming-ingestion"}],readingTime:.045,truncated:!1,authors:[{name:"Chen Yuzhao"},{name:"Liu Dalong"}],prevItem:{title:"Implementation of SCD-2 (Slowly Changing Dimension) with Apache Hudi & Spark",permalink:"/cn/blog/2022/08/24/Implementation-of-SCD-2-with-Apache-Hudi-and-Spark"},nextItem:{title:"How NerdWallet uses AWS and Apache Hudi to build a serverless, real-time analytics platform",permalink:"/cn/blog/2022/08/09/How-NerdWallet-uses-AWS-and-Apache-Hudi-to-build-a-serverless-real-time-analytics-platform"}},d={authorsImageUrls:[void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.alibabacloud.com/blog/use-flink-hudi-to-build-a-streaming-data-lake-platform_599240",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},8215:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Implementation of SCD-2 (Slowly Changing Dimension) with Apache Hudi & Spark",authors:[{name:"Jayasheel Kalgal"},{name:"Esha Dhing"},{name:"Prashant Mishra"}],category:"blog",image:"/assets/images/blog/2022-08-24_implementation_of_scd_2_with_hudi_and_spark.jpeg",tags:["use-case","scd2","walmartglobaltech"]},s=void 0,l={permalink:"/cn/blog/2022/08/24/Implementation-of-SCD-2-with-Apache-Hudi-and-Spark",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-08-24-Implementation-of-SCD-2-with-Apache-Hudi-and-Spark.mdx",source:"@site/blog/2022-08-24-Implementation-of-SCD-2-with-Apache-Hudi-and-Spark.mdx",title:"Implementation of SCD-2 (Slowly Changing Dimension) with Apache Hudi & Spark",description:"Redirecting... please wait!!",date:"2022-08-24T00:00:00.000Z",formattedDate:"August 24, 2022",tags:[{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"scd2",permalink:"/cn/blog/tags/scd-2"},{label:"walmartglobaltech",permalink:"/cn/blog/tags/walmartglobaltech"}],readingTime:.045,truncated:!1,authors:[{name:"Jayasheel Kalgal"},{name:"Esha Dhing"},{name:"Prashant Mishra"}],prevItem:{title:"Data Lake / Lakehouse Guide: Powered by Data Lake Table Formats (Delta Lake, Iceberg, Hudi)",permalink:"/cn/blog/2022/08/25/Data-Lake-Lakehouse-Guide-Powered-by-Data-Lake-Table-Formats-Delta-Lake-Iceberg-Hudi"},nextItem:{title:"Use Flink Hudi to Build a Streaming Data Lake Platform",permalink:"/cn/blog/2022/08/12/Use-Flink-Hudi-to-Build-a-Streaming-Data-Lake-Platform"}},d={authorsImageUrls:[void 0,void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/walmartglobaltech/implementation-of-scd-2-slowly-changing-dimension-with-apache-hudi-465e0eb94a5",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},83264:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Data Lake / Lakehouse Guide: Powered by Data Lake Table Formats (Delta Lake, Iceberg, Hudi)",authors:[{name:"Simon Sp\xe4ti"}],category:"blog",image:"/assets/images/blog/2022-08-25-Data-Lake-Lakehouse-Guide-Powered-by-Data-Lake-Table-Formats-Delta-Lake-Iceberg-Hudi.png",tags:["blog","datalake","lakehouse","comparison","airbyte"]},s=void 0,l={permalink:"/cn/blog/2022/08/25/Data-Lake-Lakehouse-Guide-Powered-by-Data-Lake-Table-Formats-Delta-Lake-Iceberg-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-08-25-Data-Lake-Lakehouse-Guide-Powered-by-Data-Lake-Table-Formats-Delta-Lake-Iceberg-Hudi.mdx",source:"@site/blog/2022-08-25-Data-Lake-Lakehouse-Guide-Powered-by-Data-Lake-Table-Formats-Delta-Lake-Iceberg-Hudi.mdx",title:"Data Lake / Lakehouse Guide: Powered by Data Lake Table Formats (Delta Lake, Iceberg, Hudi)",description:"Redirecting... please wait!!",date:"2022-08-25T00:00:00.000Z",formattedDate:"August 25, 2022",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"datalake",permalink:"/cn/blog/tags/datalake"},{label:"lakehouse",permalink:"/cn/blog/tags/lakehouse"},{label:"comparison",permalink:"/cn/blog/tags/comparison"},{label:"airbyte",permalink:"/cn/blog/tags/airbyte"}],readingTime:.045,truncated:!1,authors:[{name:"Simon Sp\xe4ti"}],prevItem:{title:"Building Streaming Data Lakes with Hudi and MinIO",permalink:"/cn/blog/2022/09/20/Building-Streaming-Data-Lakes-with-Hudi-and-MinIO"},nextItem:{title:"Implementation of SCD-2 (Slowly Changing Dimension) with Apache Hudi & Spark",permalink:"/cn/blog/2022/08/24/Implementation-of-SCD-2-with-Apache-Hudi-and-Spark"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},63441:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Building Streaming Data Lakes with Hudi and MinIO",authors:[{name:"Matt Sarrel"}],category:"blog",image:"/assets/images/blog/2022-09-20_streaming_data_lakes_with_hudi_and_minio.png",tags:["how-to","datalake","datalake platform","streaming ingestion","minio"]},s=void 0,l={permalink:"/cn/blog/2022/09/20/Building-Streaming-Data-Lakes-with-Hudi-and-MinIO",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-09-20-Building-Streaming-Data-Lakes-with-Hudi-and-MinIO.mdx",source:"@site/blog/2022-09-20-Building-Streaming-Data-Lakes-with-Hudi-and-MinIO.mdx",title:"Building Streaming Data Lakes with Hudi and MinIO",description:"Redirecting... please wait!!",date:"2022-09-20T00:00:00.000Z",formattedDate:"September 20, 2022",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"datalake",permalink:"/cn/blog/tags/datalake"},{label:"datalake platform",permalink:"/cn/blog/tags/datalake-platform"},{label:"streaming ingestion",permalink:"/cn/blog/tags/streaming-ingestion"},{label:"minio",permalink:"/cn/blog/tags/minio"}],readingTime:.045,truncated:!1,authors:[{name:"Matt Sarrel"}],prevItem:{title:"Data processing with Spark: time traveling",permalink:"/cn/blog/2022/09/28/Data-processing-with-Spark-time-traveling"},nextItem:{title:"Data Lake / Lakehouse Guide: Powered by Data Lake Table Formats (Delta Lake, Iceberg, Hudi)",permalink:"/cn/blog/2022/08/25/Data-Lake-Lakehouse-Guide-Powered-by-Data-Lake-Table-Formats-Delta-Lake-Iceberg-Hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://blog.min.io/streaming-data-lakes-hudi-minio/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},90465:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Data processing with Spark: time traveling",authors:[{name:"Petrica Leuca"}],category:"blog",image:"/assets/images/blog/2022-09-28_Data_processing_with_Spark_time_traveling.png",tags:["how-to","time travel query","devgenius"]},s=void 0,l={permalink:"/cn/blog/2022/09/28/Data-processing-with-Spark-time-traveling",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-09-28-Data-processing-with-Spark-time-traveling.mdx",source:"@site/blog/2022-09-28-Data-processing-with-Spark-time-traveling.mdx",title:"Data processing with Spark: time traveling",description:"Redirecting... please wait!!",date:"2022-09-28T00:00:00.000Z",formattedDate:"September 28, 2022",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"time travel query",permalink:"/cn/blog/tags/time-travel-query"},{label:"devgenius",permalink:"/cn/blog/tags/devgenius"}],readingTime:.045,truncated:!1,authors:[{name:"Petrica Leuca"}],prevItem:{title:"Ingest streaming data to Apache Hudi tables using AWS Glue and Apache Hudi DeltaStreamer",permalink:"/cn/blog/2022/10/06/Ingest-streaming-data-to-Apache-Hudi-using-AWS-Glue-and-DeltaStreamer"},nextItem:{title:"Building Streaming Data Lakes with Hudi and MinIO",permalink:"/cn/blog/2022/09/20/Building-Streaming-Data-Lakes-with-Hudi-and-MinIO"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://blog.devgenius.io/data-processing-with-spark-time-traveling-55905f765694",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},22896:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Ingest streaming data to Apache Hudi tables using AWS Glue and Apache Hudi DeltaStreamer",authors:[{name:"Vishal Pathak"},{name:"Anand Prakash"},{name:"Noritaka Sekiyama"}],category:"blog",image:"/assets/images/blog/2022-10-06_Ingest_streaming_data_to_Apache_Hudi_tables_using_AWS_Glue_and_DeltaStreamer.png",tags:["how-to","streaming ingestion","deltastreamer","amazon"]},s=void 0,l={permalink:"/cn/blog/2022/10/06/Ingest-streaming-data-to-Apache-Hudi-using-AWS-Glue-and-DeltaStreamer",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-10-06-Ingest-streaming-data-to-Apache-Hudi-using-AWS-Glue-and-DeltaStreamer.mdx",source:"@site/blog/2022-10-06-Ingest-streaming-data-to-Apache-Hudi-using-AWS-Glue-and-DeltaStreamer.mdx",title:"Ingest streaming data to Apache Hudi tables using AWS Glue and Apache Hudi DeltaStreamer",description:"Redirecting... please wait!!",date:"2022-10-06T00:00:00.000Z",formattedDate:"October 6, 2022",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"streaming ingestion",permalink:"/cn/blog/tags/streaming-ingestion"},{label:"deltastreamer",permalink:"/cn/blog/tags/deltastreamer"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[{name:"Vishal Pathak"},{name:"Anand Prakash"},{name:"Noritaka Sekiyama"}],prevItem:{title:"What, Why and How : Apache Hudi\u2019s Bloom Index",permalink:"/cn/blog/2022/10/08/what-why-and-how-apache-hudis-bloom-index"},nextItem:{title:"Data processing with Spark: time traveling",permalink:"/cn/blog/2022/09/28/Data-processing-with-Spark-time-traveling"}},d={authorsImageUrls:[void 0,void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/ingest-streaming-data-to-apache-hudi-tables-using-aws-glue-and-apache-hudi-deltastreamer/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},74731:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"What, Why and How : Apache Hudi\u2019s Bloom Index",authors:[{name:"Sivabalan Narayanan"}],category:"blog",image:"/assets/images/blog/2022-10-08-what-why-and-how-apache-hudis-bloom-index.png",tags:["how-to","design","bloom","indexing","medium"]},s=void 0,l={permalink:"/cn/blog/2022/10/08/what-why-and-how-apache-hudis-bloom-index",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-10-08-what-why-and-how-apache-hudis-bloom-index.mdx",source:"@site/blog/2022-10-08-what-why-and-how-apache-hudis-bloom-index.mdx",title:"What, Why and How : Apache Hudi\u2019s Bloom Index",description:"Redirecting... please wait!!",date:"2022-10-08T00:00:00.000Z",formattedDate:"October 8, 2022",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"design",permalink:"/cn/blog/tags/design"},{label:"bloom",permalink:"/cn/blog/tags/bloom"},{label:"indexing",permalink:"/cn/blog/tags/indexing"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Sivabalan Narayanan"}],prevItem:{title:"Get started with Apache Hudi using AWS Glue by implementing key design concepts \u2013 Part 1",permalink:"/cn/blog/2022/10/17/Get-started-with-Apache-Hudi-using-AWS"},nextItem:{title:"Ingest streaming data to Apache Hudi tables using AWS Glue and Apache Hudi DeltaStreamer",permalink:"/cn/blog/2022/10/06/Ingest-streaming-data-to-Apache-Hudi-using-AWS-Glue-and-DeltaStreamer"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@simpsons/what-why-and-how-apache-hudis-bloom-index-8646747520c1",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},12161:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Get started with Apache Hudi using AWS Glue by implementing key design concepts \u2013 Part 1",authors:[{name:"Amit Maindola"},{name:"Srinivas Kandi"},{name:"Mitesh Patel"}],category:"blog",image:"/assets/images/blog/2022-10-17-Get_started_with_apache_hudi_using_glue.jpeg",tags:["how-to","bulk-insert","amazon"]},s=void 0,l={permalink:"/cn/blog/2022/10/17/Get-started-with-Apache-Hudi-using-AWS",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-10-17-Get-started-with-Apache-Hudi-using-AWS.mdx",source:"@site/blog/2022-10-17-Get-started-with-Apache-Hudi-using-AWS.mdx",title:"Get started with Apache Hudi using AWS Glue by implementing key design concepts \u2013 Part 1",description:"Redirecting... please wait!!",date:"2022-10-17T00:00:00.000Z",formattedDate:"October 17, 2022",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"bulk-insert",permalink:"/cn/blog/tags/bulk-insert"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[{name:"Amit Maindola"},{name:"Srinivas Kandi"},{name:"Mitesh Patel"}],prevItem:{title:"How Hudl built a cost-optimized AWS Glue pipeline with Apache Hudi datasets",permalink:"/cn/blog/2022/11/10/How-Hudl-built-a-cost-optimized-AWS-Glue-pipeline-with-Apache-Hudi-datasets"},nextItem:{title:"What, Why and How : Apache Hudi\u2019s Bloom Index",permalink:"/cn/blog/2022/10/08/what-why-and-how-apache-hudis-bloom-index"}},d={authorsImageUrls:[void 0,void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/part-1-get-started-with-apache-hudi-using-aws-glue-by-implementing-key-design-concepts/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},54424:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"How Hudl built a cost-optimized AWS Glue pipeline with Apache Hudi datasets",authors:[{name:"Indira Balakrishnan"},{name:"Ramzi Yassine"},{name:"Swagat Kulkarni"}],category:"blog",image:"/assets/images/blog/2022-11-10_How_to_build_a_cost_optimized_glue_pipeline_with_apache_hudi.png",tags:["use-case","cost efficiency","incremental processing","near real-time analytics","amazon"]},s=void 0,l={permalink:"/cn/blog/2022/11/10/How-Hudl-built-a-cost-optimized-AWS-Glue-pipeline-with-Apache-Hudi-datasets",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-11-10-How-Hudl-built-a-cost-optimized-AWS-Glue-pipeline-with-Apache-Hudi-datasets.mdx",source:"@site/blog/2022-11-10-How-Hudl-built-a-cost-optimized-AWS-Glue-pipeline-with-Apache-Hudi-datasets.mdx",title:"How Hudl built a cost-optimized AWS Glue pipeline with Apache Hudi datasets",description:"Redirecting... please wait!!",date:"2022-11-10T00:00:00.000Z",formattedDate:"November 10, 2022",tags:[{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"cost efficiency",permalink:"/cn/blog/tags/cost-efficiency"},{label:"incremental processing",permalink:"/cn/blog/tags/incremental-processing"},{label:"near real-time analytics",permalink:"/cn/blog/tags/near-real-time-analytics"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[{name:"Indira Balakrishnan"},{name:"Ramzi Yassine"},{name:"Swagat Kulkarni"}],prevItem:{title:"Build your Apache Hudi data lake on AWS using Amazon EMR \u2013 Part 1",permalink:"/cn/blog/2022/11/22/Build-your-Apache-Hudi-data-lake-on-AWS-using-Amazon-EMR-Part-1"},nextItem:{title:"Get started with Apache Hudi using AWS Glue by implementing key design concepts \u2013 Part 1",permalink:"/cn/blog/2022/10/17/Get-started-with-Apache-Hudi-using-AWS"}},d={authorsImageUrls:[void 0,void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/how-hudl-built-a-cost-optimized-aws-glue-pipeline-with-apache-hudi-datasets/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},77010:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Build your Apache Hudi data lake on AWS using Amazon EMR \u2013 Part 1",authors:[{name:"Suthan Phillips"},{name:"Dylan Qu"}],category:"blog",image:"/assets/images/blog/2022-11-22-aws_hudi_best_practices_part1.png",tags:["how-to","best practices","amazon"]},s=void 0,l={permalink:"/cn/blog/2022/11/22/Build-your-Apache-Hudi-data-lake-on-AWS-using-Amazon-EMR-Part-1",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-11-22-Build-your-Apache-Hudi-data-lake-on-AWS-using-Amazon-EMR-Part-1.mdx",source:"@site/blog/2022-11-22-Build-your-Apache-Hudi-data-lake-on-AWS-using-Amazon-EMR-Part-1.mdx",title:"Build your Apache Hudi data lake on AWS using Amazon EMR \u2013 Part 1",description:"Redirecting... please wait!!",date:"2022-11-22T00:00:00.000Z",formattedDate:"November 22, 2022",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"best practices",permalink:"/cn/blog/tags/best-practices"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[{name:"Suthan Phillips"},{name:"Dylan Qu"}],prevItem:{title:"Run Apache Hudi at scale on AWS",permalink:"/cn/blog/2022/12/01/Run-apache-hudi-at-scale-on-aws"},nextItem:{title:"How Hudl built a cost-optimized AWS Glue pipeline with Apache Hudi datasets",permalink:"/cn/blog/2022/11/10/How-Hudl-built-a-cost-optimized-AWS-Glue-pipeline-with-Apache-Hudi-datasets"}},d={authorsImageUrls:[void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/part-1-build-your-apache-hudi-data-lake-on-aws-using-amazon-emr//",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},80012:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Run Apache Hudi at scale on AWS",authors:[{name:"Imtiaz Sayed,"},{name:"Shana Schipers"},{name:"Dylan Qu"},{name:"Carlos Rodrigues"},{name:"Arun A K"},{name:"Francisco Morillo"}],category:"technical guide",image:"/assets/images/blog/run-hudi-at-scale-on-aws.png",tags:["aws","guide","apache hudi"]},s=void 0,l={permalink:"/cn/blog/2022/12/01/Run-apache-hudi-at-scale-on-aws",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-12-01-Run-apache-hudi-at-scale-on-aws.mdx",source:"@site/blog/2022-12-01-Run-apache-hudi-at-scale-on-aws.mdx",title:"Run Apache Hudi at scale on AWS",description:"Redirecting... please wait!!",date:"2022-12-01T00:00:00.000Z",formattedDate:"December 1, 2022",tags:[{label:"aws",permalink:"/cn/blog/tags/aws"},{label:"guide",permalink:"/cn/blog/tags/guide"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:.045,truncated:!1,authors:[{name:"Imtiaz Sayed,"},{name:"Shana Schipers"},{name:"Dylan Qu"},{name:"Carlos Rodrigues"},{name:"Arun A K"},{name:"Francisco Morillo"}],prevItem:{title:"Build Your First Hudi Lakehouse with AWS S3 and AWS Glue",permalink:"/cn/blog/2022/12/19/Build-Your-First-Hudi-Lakehouse-with-AWS-Glue-and-AWS-S3"},nextItem:{title:"Build your Apache Hudi data lake on AWS using Amazon EMR \u2013 Part 1",permalink:"/cn/blog/2022/11/22/Build-your-Apache-Hudi-data-lake-on-AWS-using-Amazon-EMR-Part-1"}},d={authorsImageUrls:[void 0,void 0,void 0,void 0,void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://pages.awscloud.com/GLOBAL-devadopt-DL-Apache-Hudi-Technical-Guide-2023-learn.html?sc_channel=sm&sc_campaign=DB_Blog&sc_publisher=LINKEDIN&sc_geo=GLOBAL&sc_outcome=awareness&trk=DB_Blog&linkId=205888417/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},62099:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Build Your First Hudi Lakehouse with AWS S3 and AWS Glue",excerpt:"Follow this tutorial on building your first hudi lakehouse with AWS S3 & AWS Glue",author:"Nadine Farah",category:"blog",image:"/assets/images/blog/DataCouncil.jpg",tags:["how-to","use-case","apache hudi","aws s3","aws glue"]},r=void 0,s={permalink:"/cn/blog/2022/12/19/Build-Your-First-Hudi-Lakehouse-with-AWS-Glue-and-AWS-S3",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-12-19-Build-Your-First-Hudi-Lakehouse-with-AWS-Glue-and-AWS-S3.md",source:"@site/blog/2022-12-19-Build-Your-First-Hudi-Lakehouse-with-AWS-Glue-and-AWS-S3.md",title:"Build Your First Hudi Lakehouse with AWS S3 and AWS Glue",description:"/assets/images/blog/DataCouncil.jpg",date:"2022-12-19T00:00:00.000Z",formattedDate:"December 19, 2022",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"aws s3",permalink:"/cn/blog/tags/aws-s-3"},{label:"aws glue",permalink:"/cn/blog/tags/aws-glue"}],readingTime:1.26,truncated:!1,authors:[{name:"Nadine Farah"}],prevItem:{title:"Apache Hudi 2022 - A year in Review",permalink:"/cn/blog/2022/12/29/Apache-Hudi-2022-A-Year-In-Review"},nextItem:{title:"Run Apache Hudi at scale on AWS",permalink:"/cn/blog/2022/12/01/Run-apache-hudi-at-scale-on-aws"}},l={authorsImageUrls:[void 0]},d=[{value:"Getting Started",id:"getting-started",children:[],level:2},{value:"Questions",id:"questions",children:[],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...o}=e;return(0,n.yg)(p,(0,i.A)({},c,o,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"/assets/images/blog/DataCouncil.jpg",src:t(82717).A})),(0,n.yg)("h1",{id:"build-your-first-hudi-lakehouse-with-aws-s3-and-aws-glue"},"Build Your First Hudi Lakehouse with AWS S3 and AWS Glue"),(0,n.yg)("p",null,"Soumil Shah is a Hudi community champion building ",(0,n.yg)("a",{parentName:"p",href:"https://www.youtube.com/@SoumilShah/playlists"},"YouTube content")," so developers can easily get started incorporating a lakehouse into their data infrastructure. In this ",(0,n.yg)("a",{parentName:"p",href:"https://www.youtube.com/watch?v=5zF4jc_3rFs&list=PLL2hlSFBmWwwbMpcyMjYuRn8cN99gFSY6"},"video"),", Soumil shows you how to get started with AWS Glue, AWS S3, Hudi and Athena."),(0,n.yg)("p",null,"In this tutorial, you\u2019ll learn how to:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Create and configure AWS Glue"),(0,n.yg)("li",{parentName:"ul"},"Create a Hudi Table"),(0,n.yg)("li",{parentName:"ul"},"Create a Spark Data Frame"),(0,n.yg)("li",{parentName:"ul"},"Add data to the Hudi Table "),(0,n.yg)("li",{parentName:"ul"},"Query data via Athena")),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"/assets/images/blog/build-your-first-hudi-lakehouse-12-19-diagram.jpg",src:t(41209).A})),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Step 1"),": Users in this architecture purchase things from online retailers and generate an order transaction that is kept in DynamoDB."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Step 2"),": The raw data layer stores the order transaction data that is fed into the data lake. To accomplish this, enable Kinesis Data Streams for DynamoDB, and we will stream real-time transactions from DynamoDB into kinesis data streams, process the streaming data with lambda, and insert the data into the next kinesis stream, where a glue streaming job will process and insert the data into Apache Hudi Transaction data lake."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Step 3"),": Users can build dashboards and derive insights using QuickSight."),(0,n.yg)("h2",{id:"getting-started"},"Getting Started"),(0,n.yg)("p",null,"To get started on building this data app, follow the YouTube video on\n",(0,n.yg)("a",{parentName:"p",href:"https://www.youtube.com/watch?v=5zF4jc_3rFs&list=PLL2hlSFBmWwwbMpcyMjYuRn8cN99gFSY6&"},"Build Datalakes on S3 and Glue with Apache HUDI"),"."),(0,n.yg)("p",null,"Follow the the ",(0,n.yg)("a",{parentName:"p",href:"https://drive.google.com/file/d/1W-E_SupsoI8VZWGtq5d7doxdWdNDPEoj/view"},"step-by-step instructions"),". "),(0,n.yg)("p",null,"Apply the ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/soumilshah1995/dynamodb-hudi-stream-project"},"code source"),"."),(0,n.yg)("h2",{id:"questions"},"Questions"),(0,n.yg)("p",null,"If you run into blockers doing this tutorial, please reach out on the Apache Hudi community and tag ",(0,n.yg)("strong",{parentName:"p"},"soumilshah1995")," to help debug."))}g.isMDXComponent=!0},68611:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Apache Hudi 2022 - A year in Review",excerpt:"2022 was the best year for Apache Hudi yet! Huge thank you to everyone who contributed!",author:"Sivabalan Narayanan",category:"blog",image:"/assets/images/blog/Apache-Hudi-2022-Review.png",tags:["apache hudi","community"]},r=void 0,s={permalink:"/cn/blog/2022/12/29/Apache-Hudi-2022-A-Year-In-Review",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-12-29-Apache-Hudi-2022-A-Year-In-Review.md",source:"@site/blog/2022-12-29-Apache-Hudi-2022-A-Year-In-Review.md",title:"Apache Hudi 2022 - A year in Review",description:"Apache Hudi Momentum",date:"2022-12-29T00:00:00.000Z",formattedDate:"December 29, 2022",tags:[{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"community",permalink:"/cn/blog/tags/community"}],readingTime:7.36,truncated:!1,authors:[{name:"Sivabalan Narayanan"}],prevItem:{title:"Apache Hudi vs Delta Lake vs Apache Iceberg - Lakehouse Feature Comparison",permalink:"/cn/blog/2023/01/11/Apache-Hudi-vs-Delta-Lake-vs-Apache-Iceberg-Lakehouse-Feature-Comparison"},nextItem:{title:"Build Your First Hudi Lakehouse with AWS S3 and AWS Glue",permalink:"/cn/blog/2022/12/19/Build-Your-First-Hudi-Lakehouse-with-AWS-Glue-and-AWS-S3"}},l={authorsImageUrls:[void 0]},d=[{value:"Apache Hudi Momentum",id:"apache-hudi-momentum",children:[],level:2},{value:"Key Releases in 2022",id:"key-releases-in-2022",children:[],level:2},{value:"Community Events",id:"community-events",children:[],level:2},{value:"Community Content",id:"community-content",children:[],level:2},{value:"What to look for in 2023",id:"what-to-look-for-in-2023",children:[],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...t}=e;return(0,n.yg)(p,(0,i.A)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("img",{src:"/assets/images/blog/Apache-Hudi-2022-Review.png",alt:"drawing",style:{width:"80%",display:"block",marginLeft:"auto",marginRight:"auto"}}),(0,n.yg)("h2",{id:"apache-hudi-momentum"},"Apache Hudi Momentum"),(0,n.yg)("p",null,"As we wrap up 2022 I want to take the opportunity to reflect on and highlight the incredible progress of the Apache Hudi\nproject and most importantly, the community. First and foremost, I want to thank all of the contributors who have made\n2022 the best year for the project ever. There were ",(0,n.yg)("a",{parentName:"p",href:"https://ossinsight.io/analyze/apache/hudi#pull-requests"},"over 2,200 PRs"),"\ncreated (+38% YoY) and over 600+ users engaged on ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/"},"Github"),". The Apache Hudi community\n",(0,n.yg)("a",{parentName:"p",href:"https://join.slack.com/t/apache-hudi/shared_invite/zt-2ggm1fub8-_yt4Reu9djwqqVRFC7X49g"},"slack channel")," has grown to more\nthan 2,600 users (+100% YoY growth) averaging nearly 200 messages per month! The most impressive stat is that with this\nvolume growth, the median response time to questions is ~3h. ",(0,n.yg)("a",{parentName:"p",href:"https://join.slack.com/t/apache-hudi/shared_invite/zt-2ggm1fub8-_yt4Reu9djwqqVRFC7X49g"},"Come join the community"),"\nwhere people are sharing and helping each other!"),(0,n.yg)("img",{src:"/assets/images/blog/Apache-Hudi-Pull-Request-History.png",alt:"drawing",style:{width:"80%",display:"block",marginLeft:"auto",marginRight:"auto"}}),(0,n.yg)("h2",{id:"key-releases-in-2022"},"Key Releases in 2022"),(0,n.yg)("p",null,"2022 has been a year jam packed with exciting new features for Apache Hudi across 0.11.0 and 0.12.0 releases. In addition to new features, vendor/ecosystem partnerships and relationships have been strengthened across many in the community. ",(0,n.yg)("a",{parentName:"p",href:"https://www.onehouse.ai/blog/apache-hudi-native-aws-integrations"},"AWS continues to double down")," on Apache Hudi, upgrading versions in ",(0,n.yg)("a",{parentName:"p",href:"https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi.html"},"EMR"),", ",(0,n.yg)("a",{parentName:"p",href:"https://docs.aws.amazon.com/athena/latest/ug/querying-hudi.html"},"Athena"),", ",(0,n.yg)("a",{parentName:"p",href:"https://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-external-tables.html"},"Redshift"),", and announcing a new ",(0,n.yg)("a",{parentName:"p",href:"https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-hudi.html"},"native connector inside Glue"),". ",(0,n.yg)("a",{parentName:"p",href:"https://prestodb.io/docs/current/connector/hudi.html"},"Presto")," and ",(0,n.yg)("a",{parentName:"p",href:"https://trino.io/docs/current/connector/hudi.html"},"Trino")," merged native Hudi connectors for interactive analytics. ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/blog/2022/07/11/build-open-lakehouse-using-apache-hudi-and-dbt/"},"DBT"),", ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/tree/master/hudi-kafka-connect"},"Confluent"),", ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/syncing_datahub"},"Datahub"),", and several others have added support for Hudi tables. While Google has supported Hudi for a while in ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/gcp_bigquery/"},"BigQuery")," and ",(0,n.yg)("a",{parentName:"p",href:"https://cloud.google.com/blog/products/data-analytics/getting-started-with-new-table-formats-on-dataproc"},"Dataproc"),", it also announced plans to add Hudi in ",(0,n.yg)("a",{parentName:"p",href:"https://cloud.google.com/blog/products/data-analytics/building-most-open-data-cloud-all-data-all-source-any-platform"},"BigLake"),". The first tutorial for ",(0,n.yg)("a",{parentName:"p",href:"https://www.onehouse.ai/blog/apache-hudi-on-microsoft-azure"},"Hudi on Azure Synapse Analytics")," was published."),(0,n.yg)("p",null,"While there are too many features added in 2022 to list them all, take a look at some of the exciting highlights:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/blog/2022/05/17/Introducing-Multi-Modal-Index-for-the-Lakehouse-in-Apache-Hudi"},"Multi-Modal Index")," is a first-of-its-kind high-performance indexing subsystem for the Lakehouse. It improves metadata lookup performance by up to 100x and reduces overall query latency by up to 30x. Two new indices were added to the metadata table - Bloom filter index that enables faster upsert performance and",(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/blog/2022/06/09/Singificant-queries-speedup-from-Hudi-Column-Stats-Index-and-Data-Skipping-features"},"  column stats index along with Data skipping"),"  helps speed up queries dramatically."),(0,n.yg)("li",{parentName:"ul"},"Hudi added support for ",(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/releases/release-0.11.0/#async-indexer"},"asynchronous indexing")," to assist building such indices without blocking ingestion so that regular writers don't need to scale up resources for such one off spikes."),(0,n.yg)("li",{parentName:"ul"},"A new type of index called Bucket Index was introduced this year. This could be game changing for deterministic workloads with partitioned datasets. It is very light-weight and allows the distribution of records to buckets using a hash function."),(0,n.yg)("li",{parentName:"ul"},"Filesystem based Lock Provider - This implementation avoids the need of external systems and leverages the abilities of underlying filesystem to support lock provider needed for optimistic concurrency control in case of multiple writers. Please check the ",(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/docs/configurations#Locks-Configurations"},"lock configuration")," for details."),(0,n.yg)("li",{parentName:"ul"},"Deltastreamer Graceful Completion - Users can now configure a post-write completion strategy with deltastreamer continuous mode for graceful shutdown."),(0,n.yg)("li",{parentName:"ul"},"Schema on read is supported as an experimental feature since 0.11.0, allowing users to leverage Spark SQL DDL\xa0 support for ",(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/docs/schema_evolution"},"evolving data schema")," needs(drop, rename etc).\xa0 Added support for a lot of ",(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/docs/procedures/"},"CALL commands")," to invoke an array of actions on Hudi tables."),(0,n.yg)("li",{parentName:"ul"},"It is now feasible to ",(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/docs/encryption/"},"encrypt")," your data that you store with Apache Hudi."),(0,n.yg)("li",{parentName:"ul"},"Pulsar Write Commit Callback - On new events to the Hudi table, users can get notified via Pulsar."),(0,n.yg)("li",{parentName:"ul"},"Flink Enhancements: We added metadata table support, async clustering, data skipping, and bucket index for write paths. We also extended flink support to versions 1.13.x, 1.14.x and",(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/releases/release-0.12.0/#bundle-updates"},"  1.15.x"),"."),(0,n.yg)("li",{parentName:"ul"},"Presto Hudi integration: In addition to the hive connector we have had for a long time, we added ",(0,n.yg)("a",{parentName:"li",href:"https://prestodb.io/docs/current/connector/hudi.html"},"native Presto Hudi connector"),". This enables users to get access to advanced features of Hudi faster. Users can now leverage metadata table to reduce file listing cost. We also added support for accessing clustered datasets this year."),(0,n.yg)("li",{parentName:"ul"},"Trino Hudi integration: We also added ",(0,n.yg)("a",{parentName:"li",href:"https://trino.io/docs/current/connector/hudi.html"},"native Trino Hudi connector")," to assist in querying Hudi tables via Trino Engine. Users can now leverage metadata table to make their queries performant."),(0,n.yg)("li",{parentName:"ul"},"Performance enhancements: Many performance optimizations were landed by the community throughout the year to keep Hudi on par with competition or better. Check out this ",(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/blog/2022/06/29/Apache-Hudi-vs-Delta-Lake-transparent-tpc-ds-lakehouse-performance-benchmarks"},"TPC-DS benchmark")," comparing Hudi vs Delta Lake."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/releases/release-0.12.3#long-term-support"},"Long Term Support"),": We start to maintain 0.12 as the Long Term Support releases for users to migrate to and stay for a longer duration. In lieu of that, we have made 0.12.1\xa0 and 0.12.2 releases to assist users with stable release that comes packed with a lot of stability and bug fixes.")),(0,n.yg)("h2",{id:"community-events"},"Community Events"),(0,n.yg)("p",null,"Apache Hudi is a global community and thankfully we live in a world today that empowers virtual collaboration and productivity. In addition to connecting virtually this year we have seen the Apache Hudi community gather at many events in person. Re:Invent, Data+AI Summit, Flink Forward, Alluxio Day, Data Council, PrestoCon, Confluent Current, DBT Coalesce, Cinco de Trino, Data Platform Summit, and many more."),(0,n.yg)("img",{src:"/assets/images/blog/Apache-Hudi-Conferences.png",alt:"drawing",style:{width:"80%",display:"block",marginLeft:"auto",marginRight:"auto"}}),(0,n.yg)("p",null,"You don\u2019t have to travel far to meet and collaborate with the Hudi community. We hold monthly virtual meetups, weekly office hours, and there are plenty of friendly faces on Hudi Slack who like to talk shop. Join us via Zoom for the next Hudi meetup!"),(0,n.yg)("h2",{id:"community-content"},"Community Content"),(0,n.yg)("p",null,"A wide diversity of organizations around the globe use Apache Hudi as the foundation of their production data platforms. Over 800+ organizations have engaged with Hudi (up 60% YoY) Here are a few highlights of content written by the community sharing their experiences, designs, and best practices:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("a",{parentName:"li",href:"https://aws.amazon.com/blogs/big-data/part-1-build-your-apache-hudi-data-lake-on-aws-using-amazon-emr/"},"Build your Hudi data lake on AWS")," - Suthan Phillips and Dylan Qu from AWS"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("a",{parentName:"li",href:"https://www.youtube.com/playlist?list=PLL2hlSFBmWwwbMpcyMjYuRn8cN99gFSY6"},"Soumil Shah Hudi Youtube Playlist")," - Soumil Shah from JobTarget"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("a",{parentName:"li",href:"https://medium.com/walmartglobaltech/implementation-of-scd-2-slowly-changing-dimension-with-apache-hudi-465e0eb94a5"},"SCD-2 with Apache Hudi")," - Jayasheel Kalgal from Walmart"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("a",{parentName:"li",href:"https://www.onehouse.ai/blog/apache-hudi-vs-delta-lake-vs-apache-iceberg-lakehouse-feature-comparison"},"Hudi vs Delta vs Iceberg comparisons")," - Kyle Weller from Onehouse"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("a",{parentName:"li",href:"https://aws.amazon.com/blogs/big-data/how-nerdwallet-uses-aws-and-apache-hudi-to-build-a-serverless-real-time-analytics-platform/"},"Serverless, real-time analytics platform")," - Kevin Chun from NerdWallet"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/blog/2022/07/11/build-open-lakehouse-using-apache-hudi-and-dbt/"},"DBT and Hudi to Build Open Lakehouse")," - Vinoth Govindarajan from Apple"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("a",{parentName:"li",href:"https://www.onehouse.ai/blog/apache-hudi-vs-delta-lake-transparent-tpc-ds-lakehouse-performance-benchmarks"},"TPC-DS Benchmarks Hudi vs Delta Lake")," - Alexey Kudinkin from Onehouse"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("a",{parentName:"li",href:"https://blogs.halodoc.io/key-learnings-on-using-apache-hudi-in-building-lakehouse-architecture-halodoc/"},"Key Learnings Using Hudi building a Lakehouse")," - Jitendra Shah from Halodoc"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("a",{parentName:"li",href:"https://aws.amazon.com/blogs/architecture/insights-for-ctos-part-3-growing-your-business-with-modern-data-capabilities/"},"Growing your business with modern data capabilities")," - Jonathan Hwang from Zendesk"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("a",{parentName:"li",href:"https://aws.amazon.com/blogs/big-data/create-a-low-latency-source-to-data-lake-pipeline-using-amazon-msk-connect-apache-flink-and-apache-hudi/"},"Low-latency data lake using MSK, Flink, and Hudi")," - Ali Alemi from AWS"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("a",{parentName:"li",href:"https://robinhood.engineering/author-balaji-varadarajan-e3f496815ebf"},"Fresher data lakes on AWS S3")," - Balaji Varadarajan from Robinhood"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("a",{parentName:"li",href:"https://www.youtube.com/watch?v=ZamXiT9aqs8"},"Experiences with Hudi from Uber meetup")," - Sam Guleff from Walmart and Vinay Patil from Disney+ Hotstar")),(0,n.yg)("h2",{id:"what-to-look-for-in-2023"},"What to look for in 2023"),(0,n.yg)("p",null,"Thanks to the strength of the community, Apache Hudi has a bright future for 2023. Check out ",(0,n.yg)("a",{parentName:"p",href:"https://youtu.be/9LPSdd-AS8E?t=2090"},"this recording")," from our Re:Invent meetup where Vinoth Chandar talks about exciting new features to expect in 2023."),(0,n.yg)("p",null,"0.13.0 will be the next major release, with a package of exciting new features. Here are a few highlights:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://cwiki.apache.org/confluence/display/HUDI/RFC-08++Record+level+indexing+mechanisms+for+Hudi+datasets"},"Record-key-based index")," to speed up the lookup of records for UUID-based updates and deletes, well tested with 10+ TB index data for hundreds of billions of records at Uber;"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://github.com/apache/hudi/blob/master/rfc/rfc-42/rfc-42.md"},"Consistent Hashing Index")," with dynamically-sized buckets to achieve fast upsert performance with no data skew among file groups compared to existing ",(0,n.yg)("a",{parentName:"li",href:"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+29%3A+Hash+Index"},"Bucket Index"),";"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://github.com/apache/hudi/blob/master/rfc/rfc-51/rfc-51.md"},"New CDC format")," with Debezium-like database change logs to provide before and after image and operation field for streaming changes from Hudi tables, friendly to engines like Flink;"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://github.com/apache/hudi/blob/master/rfc/rfc-46/rfc-46.md"},"New Record Merge API")," to support engine-specific record representation for more efficient writes;"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://github.com/apache/hudi/blob/master/rfc/rfc-56/rfc-56.md"},"Early detection of conflicts")," among concurrent writers to give back compute resources proactively.")),(0,n.yg)("p",null,"The long-term vision of Apache Hudi is to make streaming data lake the mainstream, achieving sub-minute commit SLAs with stellar query performance and incremental ETLs.\xa0 We plan to harden the indexing subsystem with ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/pull/7080"},"Table APIs")," for easy integration with query engines and access to Hudi metadata and indexes, ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/pull/7235"},"Indexing Functions")," and ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/master/rfc/rfc-60/rfc-60.md"},"a Federated Storage Layer")," to eliminate the notion of partitions and reduce I/O, and new ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/pull/5370"},"secondary indexes"),".\xa0 To realize fast queries, we will provide an option of a standalone ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/pull/4718"},"MetaServer")," serving Hudi metadata to plan queries in milliseconds and a ",(0,n.yg)("a",{parentName:"p",href:"https://docs.google.com/presentation/d/1QBgLw11TM2Qf1KUESofGrQDb63EuggNCpPaxc82Kldo/edit#slide=id.gf7e0551254_0_5"},"Hudi-aware lake cache")," that speeds up the read performance of MOR tables along with fast writes for updates.\xa0 Incremental and streaming SQL will be enhanced in Spark and Flink.\xa0 For Hudi on Flink, we plan to make the multi-modal indexing production-ready, bring read and write compatibility between Flink and Spark engines, and harden the streaming capabilities, including CDC, streaming ETL semantics, pre-aggregation models and materialized views."),(0,n.yg)("p",null,"Check out ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/roadmap"},"Hudi Roadmap")," for more to come in 2023!"),(0,n.yg)("p",null,"If you haven't tried Apache Hudi yet, 2023 is your year! Here are a few useful links to help you get started:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/docs/overview"},"Apache Hudi Docs")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("a",{parentName:"li",href:"https://join.slack.com/t/apache-hudi/shared_invite/zt-2ggm1fub8-_yt4Reu9djwqqVRFC7X49g"},"Hudi Slack Channel")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/community/office_hours"},"Hudi Weekly Office Hours")," and ",(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/community/syncs#monthly-community-call"},"Monthly Meetup")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/contribute/how-to-contribute"},"Contributor Guide"))),(0,n.yg)("p",null,"If you enjoyed Hudi in 2022 don't forget to give it a little star on ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/"},"Github")," \u2b50"))}g.isMDXComponent=!0},67072:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Apache Hudi vs Delta Lake vs Apache Iceberg - Lakehouse Feature Comparison",authors:[{name:"Kyle Weller"}],category:"blog",image:"/assets/images/blog/2022-08-18-apache_hudi_vs_delta_lake_vs_apache_iceberg_feature_comparison.png",tags:["lakehouse","datalake","comparison","onehouse"]},s=void 0,l={permalink:"/cn/blog/2023/01/11/Apache-Hudi-vs-Delta-Lake-vs-Apache-Iceberg-Lakehouse-Feature-Comparison",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-01-11-Apache-Hudi-vs-Delta-Lake-vs-Apache-Iceberg-Lakehouse-Feature-Comparison.mdx",source:"@site/blog/2023-01-11-Apache-Hudi-vs-Delta-Lake-vs-Apache-Iceberg-Lakehouse-Feature-Comparison.mdx",title:"Apache Hudi vs Delta Lake vs Apache Iceberg - Lakehouse Feature Comparison",description:"Redirecting... please wait!!",date:"2023-01-11T00:00:00.000Z",formattedDate:"January 11, 2023",tags:[{label:"lakehouse",permalink:"/cn/blog/tags/lakehouse"},{label:"datalake",permalink:"/cn/blog/tags/datalake"},{label:"comparison",permalink:"/cn/blog/tags/comparison"},{label:"onehouse",permalink:"/cn/blog/tags/onehouse"}],readingTime:.045,truncated:!1,authors:[{name:"Kyle Weller"}],prevItem:{title:"Introducing native support for Apache Hudi, Delta Lake, and Apache Iceberg on AWS Glue for Apache Spark, Part 1: Getting Started",permalink:"/cn/blog/2023/01/27/Introducing-native-support-for-Apache-Hudi-Delta-Lake-Apache-Iceberg-on-AWS-Glue-for-Apache-Spark"},nextItem:{title:"Apache Hudi 2022 - A year in Review",permalink:"/cn/blog/2022/12/29/Apache-Hudi-2022-A-Year-In-Review"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.onehouse.ai/blog/apache-hudi-vs-delta-lake-vs-apache-iceberg-lakehouse-feature-comparison",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},11472:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Introducing native support for Apache Hudi, Delta Lake, and Apache Iceberg on AWS Glue for Apache Spark, Part 1: Getting Started",authors:[{name:"Akira Ajisaka, Noritaka Sekiyama and Savio Dsouza"}],category:"blog",image:"/assets/images/blog/0127-introducing-native-support-hudi-aws-glue.png",tags:["blog","amazon"]},s=void 0,l={permalink:"/cn/blog/2023/01/27/Introducing-native-support-for-Apache-Hudi-Delta-Lake-Apache-Iceberg-on-AWS-Glue-for-Apache-Spark",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-01-27-Introducing-native-support-for-Apache-Hudi-Delta-Lake-Apache-Iceberg-on-AWS-Glue-for-Apache-Spark.mdx",source:"@site/blog/2023-01-27-Introducing-native-support-for-Apache-Hudi-Delta-Lake-Apache-Iceberg-on-AWS-Glue-for-Apache-Spark.mdx",title:"Introducing native support for Apache Hudi, Delta Lake, and Apache Iceberg on AWS Glue for Apache Spark, Part 1: Getting Started",description:"Redirecting... please wait!!",date:"2023-01-27T00:00:00.000Z",formattedDate:"January 27, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[{name:"Akira Ajisaka, Noritaka Sekiyama and Savio Dsouza"}],prevItem:{title:"Automate schema evolution at scale with Apache Hudi in AWS Glue | Amazon Web Services",permalink:"/cn/blog/2023/02/07/automate-schema-evolution-at-scale-with-apache-hudi-in-aws-glue"},nextItem:{title:"Apache Hudi vs Delta Lake vs Apache Iceberg - Lakehouse Feature Comparison",permalink:"/cn/blog/2023/01/11/Apache-Hudi-vs-Delta-Lake-vs-Apache-Iceberg-Lakehouse-Feature-Comparison"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/part-1-getting-started-introducing-native-support-for-apache-hudi-delta-lake-and-apache-iceberg-on-aws-glue-for-apache-spark/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},44662:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Automate schema evolution at scale with Apache Hudi in AWS Glue | Amazon Web Services",authors:[{name:"Subhro Bose"},{name:"Eva Fang"},{name:"Ketan Karalkar"}],category:"blog",image:"/assets/images/blog/automate-schema-evolution-at-scale-with-apache-hudi-in-aws-glue.png",tags:["how-to","schema evolution","amazon"]},s=void 0,l={permalink:"/cn/blog/2023/02/07/automate-schema-evolution-at-scale-with-apache-hudi-in-aws-glue",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-02-07-automate-schema-evolution-at-scale-with-apache-hudi-in-aws-glue.mdx",source:"@site/blog/2023-02-07-automate-schema-evolution-at-scale-with-apache-hudi-in-aws-glue.mdx",title:"Automate schema evolution at scale with Apache Hudi in AWS Glue | Amazon Web Services",description:"Redirecting... please wait!!",date:"2023-02-07T00:00:00.000Z",formattedDate:"February 7, 2023",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"schema evolution",permalink:"/cn/blog/tags/schema-evolution"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[{name:"Subhro Bose"},{name:"Eva Fang"},{name:"Ketan Karalkar"}],prevItem:{title:"Table service deployment models in Apache Hudi",permalink:"/cn/blog/2023/02/12/table-service-deployment-models-in-apache-hudi"},nextItem:{title:"Introducing native support for Apache Hudi, Delta Lake, and Apache Iceberg on AWS Glue for Apache Spark, Part 1: Getting Started",permalink:"/cn/blog/2023/01/27/Introducing-native-support-for-Apache-Hudi-Delta-Lake-Apache-Iceberg-on-AWS-Glue-for-Apache-Spark"}},d={authorsImageUrls:[void 0,void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/automate-schema-evolution-at-scale-with-apache-hudi-in-aws-glue/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},65321:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Table service deployment models in Apache Hudi",authors:[{name:"Sivabalan Narayanan"}],category:"blog",tags:["how-to","table services","deployment","medium"]},s=void 0,l={permalink:"/cn/blog/2023/02/12/table-service-deployment-models-in-apache-hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-02-12-table-service-deployment-models-in-apache-hudi.mdx",source:"@site/blog/2023-02-12-table-service-deployment-models-in-apache-hudi.mdx",title:"Table service deployment models in Apache Hudi",description:"Redirecting... please wait!!",date:"2023-02-12T00:00:00.000Z",formattedDate:"February 12, 2023",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"table services",permalink:"/cn/blog/tags/table-services"},{label:"deployment",permalink:"/cn/blog/tags/deployment"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Sivabalan Narayanan"}],prevItem:{title:"Bulk Insert Sort Modes with Apache Hudi",permalink:"/cn/blog/2023/02/19/bulk-insert-sort-modes-with-apache-hudi"},nextItem:{title:"Automate schema evolution at scale with Apache Hudi in AWS Glue | Amazon Web Services",permalink:"/cn/blog/2023/02/07/automate-schema-evolution-at-scale-with-apache-hudi-in-aws-glue"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@simpsons/table-service-deployment-models-in-apache-hudi-9cfa5a44addf",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},14926:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Bulk Insert Sort Modes with Apache Hudi",authors:[{name:"Sivabalan Narayanan"}],category:"blog",tags:["blog","bulk-insert","medium"]},s=void 0,l={permalink:"/cn/blog/2023/02/19/bulk-insert-sort-modes-with-apache-hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-02-19-bulk-insert-sort-modes-with-apache-hudi.mdx",source:"@site/blog/2023-02-19-bulk-insert-sort-modes-with-apache-hudi.mdx",title:"Bulk Insert Sort Modes with Apache Hudi",description:"Redirecting... please wait!!",date:"2023-02-19T00:00:00.000Z",formattedDate:"February 19, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"bulk-insert",permalink:"/cn/blog/tags/bulk-insert"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Sivabalan Narayanan"}],prevItem:{title:"Getting Started: Manage your Hudi tables with the admin Hudi-CLI tool",permalink:"/cn/blog/2023/02/22/Getting-Started-Manage-your-Hudi-tables-with-the-admin-Hudi-CLI-tool"},nextItem:{title:"Table service deployment models in Apache Hudi",permalink:"/cn/blog/2023/02/12/table-service-deployment-models-in-apache-hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@simpsons/bulk-insert-sort-modes-with-apache-hudi-c781e77841bc",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},90932:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Getting Started: Manage your Hudi tables with the admin Hudi-CLI tool",authors:[{name:"Sivabalan Narayanan"}],category:"blog",image:"/assets/images/blog/2023-02-22-Getting-Started-Manage-your-Hudi-tables-with-the-admin-Hudi-CLI-tool.png",tags:["how-to","hudi cli","onehouse"]},s=void 0,l={permalink:"/cn/blog/2023/02/22/Getting-Started-Manage-your-Hudi-tables-with-the-admin-Hudi-CLI-tool",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-02-22-Getting-Started-Manage-your-Hudi-tables-with-the-admin-Hudi-CLI-tool.mdx",source:"@site/blog/2023-02-22-Getting-Started-Manage-your-Hudi-tables-with-the-admin-Hudi-CLI-tool.mdx",title:"Getting Started: Manage your Hudi tables with the admin Hudi-CLI tool",description:"Redirecting... please wait!!",date:"2023-02-22T00:00:00.000Z",formattedDate:"February 22, 2023",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"hudi cli",permalink:"/cn/blog/tags/hudi-cli"},{label:"onehouse",permalink:"/cn/blog/tags/onehouse"}],readingTime:.045,truncated:!1,authors:[{name:"Sivabalan Narayanan"}],prevItem:{title:"Setting Uber\u2019s Transactional Data Lake in Motion with Incremental ETL Using Apache Hudi",permalink:"/cn/blog/2023/03/16/Setting-Uber-Transactional-Data-Lake-in-Motion-with-Incremental-ETL-Using-Apache-Hudi"},nextItem:{title:"Bulk Insert Sort Modes with Apache Hudi",permalink:"/cn/blog/2023/02/19/bulk-insert-sort-modes-with-apache-hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.onehouse.ai/blog/getting-started-manage-your-hudi-tables-with-the-admin-hudi-cli-tool",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},97143:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Setting Uber\u2019s Transactional Data Lake in Motion with Incremental ETL Using Apache Hudi",authors:[{name:"Vinoth Govindarajan"},{name:"Saketh Chintapalli"},{name:"Yogesh Saswade"},{name:"Aayush Bareja"}],category:"blog",image:"/assets/images/blog/hudi-lakehouse-architecture-uber.png",tags:["incremental processing","datalake","apache hudi","medallion architecture","uber"]},s=void 0,l={permalink:"/cn/blog/2023/03/16/Setting-Uber-Transactional-Data-Lake-in-Motion-with-Incremental-ETL-Using-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-03-16-Setting-Uber-Transactional-Data-Lake-in-Motion-with-Incremental-ETL-Using-Apache-Hudi.mdx",source:"@site/blog/2023-03-16-Setting-Uber-Transactional-Data-Lake-in-Motion-with-Incremental-ETL-Using-Apache-Hudi.mdx",title:"Setting Uber\u2019s Transactional Data Lake in Motion with Incremental ETL Using Apache Hudi",description:"Redirecting... please wait!!",date:"2023-03-16T00:00:00.000Z",formattedDate:"March 16, 2023",tags:[{label:"incremental processing",permalink:"/cn/blog/tags/incremental-processing"},{label:"datalake",permalink:"/cn/blog/tags/datalake"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"medallion architecture",permalink:"/cn/blog/tags/medallion-architecture"},{label:"uber",permalink:"/cn/blog/tags/uber"}],readingTime:.045,truncated:!1,authors:[{name:"Vinoth Govindarajan"},{name:"Saketh Chintapalli"},{name:"Yogesh Saswade"},{name:"Aayush Bareja"}],prevItem:{title:"Introduction to Apache Hudi",permalink:"/cn/blog/2023/03/17/introduction-to-apache-hudi"},nextItem:{title:"Getting Started: Manage your Hudi tables with the admin Hudi-CLI tool",permalink:"/cn/blog/2023/02/22/Getting-Started-Manage-your-Hudi-tables-with-the-admin-Hudi-CLI-tool"}},d={authorsImageUrls:[void 0,void 0,void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.uber.com/blog/ubers-lakehouse-architecture/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},16488:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Introduction to Apache Hudi",authors:[{name:"Itamar Syn-Hershko"}],category:"blog",image:"/assets/images/blog/2023-03-17-introduction-to-apache-hudi.png",tags:["how-to","guide","introduction"]},s=void 0,l={permalink:"/cn/blog/2023/03/17/introduction-to-apache-hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-03-17-introduction-to-apache-hudi.mdx",source:"@site/blog/2023-03-17-introduction-to-apache-hudi.mdx",title:"Introduction to Apache Hudi",description:"Redirecting... please wait!!",date:"2023-03-17T00:00:00.000Z",formattedDate:"March 17, 2023",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"guide",permalink:"/cn/blog/tags/guide"},{label:"introduction",permalink:"/cn/blog/tags/introduction"}],readingTime:.045,truncated:!1,authors:[{name:"Itamar Syn-Hershko"}],prevItem:{title:"Introducing native support for Apache Hudi, Delta Lake, and Apache Iceberg on AWS Glue for Apache Spark, Part 2: AWS Glue Studio Visual Editor",permalink:"/cn/blog/2023/03/20/Introducing-native-support-for-Apache Hudi-Delta-Lake-and-Apache-Iceberg-on-AWS-Glue-for-Apache-Spark-Part-2-AWS-Glue-Studio-Visual-Editor"},nextItem:{title:"Setting Uber\u2019s Transactional Data Lake in Motion with Incremental ETL Using Apache Hudi",permalink:"/cn/blog/2023/03/16/Setting-Uber-Transactional-Data-Lake-in-Motion-with-Incremental-ETL-Using-Apache-Hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://bigdataboutique.com/blog/introduction-to-apache-hudi-c83367",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},77947:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Introducing native support for Apache Hudi, Delta Lake, and Apache Iceberg on AWS Glue for Apache Spark, Part 2: AWS Glue Studio Visual Editor",authors:[{name:"Noritaka Sekiyama"},{name:"Scott Long"},{name:"Sean Ma"}],category:"blog",image:"/assets/images/blog/native-support-hudi-for-glue-studio.png",tags:["aws glue","glue studio","blog","amazon"]},s=void 0,l={permalink:"/cn/blog/2023/03/20/Introducing-native-support-for-Apache Hudi-Delta-Lake-and-Apache-Iceberg-on-AWS-Glue-for-Apache-Spark-Part-2-AWS-Glue-Studio-Visual-Editor",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-03-20-Introducing-native-support-for-Apache Hudi-Delta-Lake-and-Apache-Iceberg-on-AWS-Glue-for-Apache-Spark-Part-2-AWS-Glue-Studio-Visual-Editor.mdx",source:"@site/blog/2023-03-20-Introducing-native-support-for-Apache Hudi-Delta-Lake-and-Apache-Iceberg-on-AWS-Glue-for-Apache-Spark-Part-2-AWS-Glue-Studio-Visual-Editor.mdx",title:"Introducing native support for Apache Hudi, Delta Lake, and Apache Iceberg on AWS Glue for Apache Spark, Part 2: AWS Glue Studio Visual Editor",description:"Redirecting... please wait!!",date:"2023-03-20T00:00:00.000Z",formattedDate:"March 20, 2023",tags:[{label:"aws glue",permalink:"/cn/blog/tags/aws-glue"},{label:"glue studio",permalink:"/cn/blog/tags/glue-studio"},{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[{name:"Noritaka Sekiyama"},{name:"Scott Long"},{name:"Sean Ma"}],prevItem:{title:"Spark ETL Chapter 8 with Lakehouse | Apache HUDI",permalink:"/cn/blog/2023/03/23/Spark-ETL-Chapter-8-with-Lakehouse-Apache-HUDI"},nextItem:{title:"Introduction to Apache Hudi",permalink:"/cn/blog/2023/03/17/introduction-to-apache-hudi"}},d={authorsImageUrls:[void 0,void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/part-2-glue-studio-visual-editor-introducing-native-support-for-apache-hudi-delta-lake-and-apache-iceberg-on-aws-glue-for-apache-spark/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},9528:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Spark ETL Chapter 8 with Lakehouse | Apache HUDI",authors:[{name:"Kalpan Shah"}],category:"blog",image:"/assets/images/blog/2023-03-23-Spark-ETL-Chapter-8-with-Lakehouse-Apache-HUDI.png",tags:["how-to","guide","medium"]},s=void 0,l={permalink:"/cn/blog/2023/03/23/Spark-ETL-Chapter-8-with-Lakehouse-Apache-HUDI",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-03-23-Spark-ETL-Chapter-8-with-Lakehouse-Apache-HUDI.mdx",source:"@site/blog/2023-03-23-Spark-ETL-Chapter-8-with-Lakehouse-Apache-HUDI.mdx",title:"Spark ETL Chapter 8 with Lakehouse | Apache HUDI",description:"Redirecting... please wait!!",date:"2023-03-23T00:00:00.000Z",formattedDate:"March 23, 2023",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"guide",permalink:"/cn/blog/tags/guide"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Kalpan Shah"}],prevItem:{title:"Global vs Non-global index in Apache Hudi",permalink:"/cn/blog/2023/04/02/global-vs-non-global-index-in-apache-hudi"},nextItem:{title:"Introducing native support for Apache Hudi, Delta Lake, and Apache Iceberg on AWS Glue for Apache Spark, Part 2: AWS Glue Studio Visual Editor",permalink:"/cn/blog/2023/03/20/Introducing-native-support-for-Apache Hudi-Delta-Lake-and-Apache-Iceberg-on-AWS-Glue-for-Apache-Spark-Part-2-AWS-Glue-Studio-Visual-Editor"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/plumbersofdatascience/spark-etl-chapter-8-with-lakehouse-apache-hudi-d4794b8a79e6",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},12741:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Global vs Non-global index in Apache Hudi",authors:[{name:"Sivabalan Narayanan"}],category:"blog",tags:["how-to","indexing","medium"]},s=void 0,l={permalink:"/cn/blog/2023/04/02/global-vs-non-global-index-in-apache-hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-04-02-global-vs-non-global-index-in-apache-hudi.mdx",source:"@site/blog/2023-04-02-global-vs-non-global-index-in-apache-hudi.mdx",title:"Global vs Non-global index in Apache Hudi",description:"Redirecting... please wait!!",date:"2023-04-02T00:00:00.000Z",formattedDate:"April 2, 2023",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"indexing",permalink:"/cn/blog/tags/indexing"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Sivabalan Narayanan"}],prevItem:{title:"Speed up your write latencies using Bucket Index in Apache Hudi",permalink:"/cn/blog/2023/04/07/Speed-up-your-write-latencies-using-Bucket-Index-in-Apache-Hudi"},nextItem:{title:"Spark ETL Chapter 8 with Lakehouse | Apache HUDI",permalink:"/cn/blog/2023/03/23/Spark-ETL-Chapter-8-with-Lakehouse-Apache-HUDI"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@simpsons/global-vs-non-global-index-in-apache-hudi-ac880b031cbc",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},22239:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Speed up your write latencies using Bucket Index in Apache Hudi",authors:[{name:"Sivabalan Narayanan"}],category:"blog",image:"/assets/images/blog/2023-04-07-Speed-up-your-write-latencies-using-Bucket-Index-in-Apache-Hudi.png",tags:["how-to","indexing","medium"]},s=void 0,l={permalink:"/cn/blog/2023/04/07/Speed-up-your-write-latencies-using-Bucket-Index-in-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-04-07-Speed-up-your-write-latencies-using-Bucket-Index-in-Apache-Hudi.mdx",source:"@site/blog/2023-04-07-Speed-up-your-write-latencies-using-Bucket-Index-in-Apache-Hudi.mdx",title:"Speed up your write latencies using Bucket Index in Apache Hudi",description:"Redirecting... please wait!!",date:"2023-04-07T00:00:00.000Z",formattedDate:"April 7, 2023",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"indexing",permalink:"/cn/blog/tags/indexing"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Sivabalan Narayanan"}],prevItem:{title:"Getting Started: Incrementally process data with Apache Hudi",permalink:"/cn/blog/2023/04/18/getting-started-incrementally-process-data-with-apache-hudi"},nextItem:{title:"Global vs Non-global index in Apache Hudi",permalink:"/cn/blog/2023/04/02/global-vs-non-global-index-in-apache-hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@simpsons/speed-up-your-write-latencies-using-bucket-index-in-apache-hudi-2f7c297493dc",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},57100:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Getting Started: Incrementally process data with Apache Hudi",authors:[{name:"Raymond Xu"}],category:"blog",image:"/assets/images/blog/2023-04-18-getting-started-incrementally-process-data-with-apache-hudi.png",tags:["how-to","incremental processing","onehouse"]},s=void 0,l={permalink:"/cn/blog/2023/04/18/getting-started-incrementally-process-data-with-apache-hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-04-18-getting-started-incrementally-process-data-with-apache-hudi.mdx",source:"@site/blog/2023-04-18-getting-started-incrementally-process-data-with-apache-hudi.mdx",title:"Getting Started: Incrementally process data with Apache Hudi",description:"Redirecting... please wait!!",date:"2023-04-18T00:00:00.000Z",formattedDate:"April 18, 2023",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"incremental processing",permalink:"/cn/blog/tags/incremental-processing"},{label:"onehouse",permalink:"/cn/blog/tags/onehouse"}],readingTime:.045,truncated:!1,authors:[{name:"Raymond Xu"}],prevItem:{title:"Delta, Hudi, and Iceberg: The Data Lakehouse Trifecta",permalink:"/cn/blog/2023/04/26/the-lakehouse-trifecta"},nextItem:{title:"Speed up your write latencies using Bucket Index in Apache Hudi",permalink:"/cn/blog/2023/04/07/Speed-up-your-write-latencies-using-Bucket-Index-in-Apache-Hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.onehouse.ai/blog/getting-started-incrementally-process-data-with-apache-hudi",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},36547:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Delta, Hudi, and Iceberg: The Data Lakehouse Trifecta",authors:[{name:"Andrey Gusarov"}],category:"blog",image:"/assets/images/blog/0426-lakehouse-trifecta.png",tags:["lakehouse","delta lake","apache hudi","apache iceberg","comparison","dzone"]},s=void 0,l={permalink:"/cn/blog/2023/04/26/the-lakehouse-trifecta",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-04-26-the-lakehouse-trifecta.mdx",source:"@site/blog/2023-04-26-the-lakehouse-trifecta.mdx",title:"Delta, Hudi, and Iceberg: The Data Lakehouse Trifecta",description:"Redirecting... please wait!!",date:"2023-04-26T00:00:00.000Z",formattedDate:"April 26, 2023",tags:[{label:"lakehouse",permalink:"/cn/blog/tags/lakehouse"},{label:"delta lake",permalink:"/cn/blog/tags/delta-lake"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"apache iceberg",permalink:"/cn/blog/tags/apache-iceberg"},{label:"comparison",permalink:"/cn/blog/tags/comparison"},{label:"dzone",permalink:"/cn/blog/tags/dzone"}],readingTime:.045,truncated:!1,authors:[{name:"Andrey Gusarov"}],prevItem:{title:"Can you concurrently write data to Apache Hudi w/o any lock provider?",permalink:"/cn/blog/2023/04/29/can-you-concurrently-write-data-to-apache-hudi-w-o-any-lock-provider"},nextItem:{title:"Getting Started: Incrementally process data with Apache Hudi",permalink:"/cn/blog/2023/04/18/getting-started-incrementally-process-data-with-apache-hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://dzone.com/articles/delta-hudi-and-iceberg-the-data-lakehouse-trifecta",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},76121:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Can you concurrently write data to Apache Hudi w/o any lock provider?",authors:[{name:"Sivabalan Narayanan"}],category:"blog",image:"/assets/images/blog/2023-04-29-can-you-concurrently-write-data-to-apache-hudi-w-o-any-lock-provider.gif",tags:["how-to","concurrency","medium"]},s=void 0,l={permalink:"/cn/blog/2023/04/29/can-you-concurrently-write-data-to-apache-hudi-w-o-any-lock-provider",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-04-29-can-you-concurrently-write-data-to-apache-hudi-w-o-any-lock-provider.mdx",source:"@site/blog/2023-04-29-can-you-concurrently-write-data-to-apache-hudi-w-o-any-lock-provider.mdx",title:"Can you concurrently write data to Apache Hudi w/o any lock provider?",description:"Redirecting... please wait!!",date:"2023-04-29T00:00:00.000Z",formattedDate:"April 29, 2023",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"concurrency",permalink:"/cn/blog/tags/concurrency"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Sivabalan Narayanan"}],prevItem:{title:"An Introduction to the Hudi and Flink Integration",permalink:"/cn/blog/2023/05/02/intro-to-hudi-and-flink"},nextItem:{title:"Delta, Hudi, and Iceberg: The Data Lakehouse Trifecta",permalink:"/cn/blog/2023/04/26/the-lakehouse-trifecta"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@simpsons/can-you-concurrently-write-data-to-apache-hudi-w-o-any-lock-provider-51ea55bf2dd6",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},9374:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"An Introduction to the Hudi and Flink Integration",authors:[{name:"Danny Chan"}],category:"blog",image:"/assets/images/blog/2023-05-02-intro-to-hudi-and-flink.png",tags:["blog","apache hudi","apache flink","onehouse"]},s=void 0,l={permalink:"/cn/blog/2023/05/02/intro-to-hudi-and-flink",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-05-02-intro-to-hudi-and-flink.mdx",source:"@site/blog/2023-05-02-intro-to-hudi-and-flink.mdx",title:"An Introduction to the Hudi and Flink Integration",description:"Redirecting... please wait!!",date:"2023-05-02T00:00:00.000Z",formattedDate:"May 2, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"apache flink",permalink:"/cn/blog/tags/apache-flink"},{label:"onehouse",permalink:"/cn/blog/tags/onehouse"}],readingTime:.045,truncated:!1,authors:[{name:"Danny Chan"}],prevItem:{title:"Lakehouse at Fortune 1 Scale",permalink:"/cn/blog/2023/05/03/lakehouse-at-fortune-1-scale"},nextItem:{title:"Can you concurrently write data to Apache Hudi w/o any lock provider?",permalink:"/cn/blog/2023/04/29/can-you-concurrently-write-data-to-apache-hudi-w-o-any-lock-provider"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.onehouse.ai/blog/intro-to-hudi-and-flink",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},10283:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Lakehouse at Fortune 1 Scale",authors:[{name:"Samuel Guleff"}],category:"blog",image:"/assets/images/blog/2023-05-03-lakehouse-at-fortune-1-scale.jpeg",tags:["use-case","comparison","performance","walmartglobaltech"]},s=void 0,l={permalink:"/cn/blog/2023/05/03/lakehouse-at-fortune-1-scale",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-05-03-lakehouse-at-fortune-1-scale.mdx",source:"@site/blog/2023-05-03-lakehouse-at-fortune-1-scale.mdx",title:"Lakehouse at Fortune 1 Scale",description:"Redirecting... please wait!!",date:"2023-05-03T00:00:00.000Z",formattedDate:"May 3, 2023",tags:[{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"comparison",permalink:"/cn/blog/tags/comparison"},{label:"performance",permalink:"/cn/blog/tags/performance"},{label:"walmartglobaltech",permalink:"/cn/blog/tags/walmartglobaltech"}],readingTime:.045,truncated:!1,authors:[{name:"Samuel Guleff"}],prevItem:{title:"Amazon Athena now supports Apache Hudi 0.12.2",permalink:"/cn/blog/2023/05/09/amazon-athena-apache-hudi"},nextItem:{title:"An Introduction to the Hudi and Flink Integration",permalink:"/cn/blog/2023/05/02/intro-to-hudi-and-flink"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/walmartglobaltech/lakehouse-at-fortune-1-scale-480bcb10391b",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},64193:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Amazon Athena now supports Apache Hudi 0.12.2",category:"blog",image:"/assets/images/blog/aws.jpg",tags:["blog","amazon"]},s=void 0,l={permalink:"/cn/blog/2023/05/09/amazon-athena-apache-hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-05-09-amazon-athena-apache-hudi.mdx",source:"@site/blog/2023-05-09-amazon-athena-apache-hudi.mdx",title:"Amazon Athena now supports Apache Hudi 0.12.2",description:"Redirecting... please wait!!",date:"2023-05-09T00:00:00.000Z",formattedDate:"May 9, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[],prevItem:{title:"Top 3 Things You Can Do to Get Fast Upsert Performance in Apache Hudi",permalink:"/cn/blog/2023/05/10/top-3-things-you-can-do-to-get-fast-upsert-performance-in-apache-hudi"},nextItem:{title:"Lakehouse at Fortune 1 Scale",permalink:"/cn/blog/2023/05/03/lakehouse-at-fortune-1-scale"}},d={authorsImageUrls:[]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/about-aws/whats-new/2023/05/amazon-athena-apache-hudi/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},19688:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Top 3 Things You Can Do to Get Fast Upsert Performance in Apache Hudi",authors:[{name:"Nadine Farah"}],category:"blog",image:"/assets/images/blog/2023-05-10-top-3-things-you-can-do-to-get-fast-upsert-performance-in-apache-hudi.png",tags:["how-to","performance","onehouse"]},s=void 0,l={permalink:"/cn/blog/2023/05/10/top-3-things-you-can-do-to-get-fast-upsert-performance-in-apache-hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-05-10-top-3-things-you-can-do-to-get-fast-upsert-performance-in-apache-hudi.mdx",source:"@site/blog/2023-05-10-top-3-things-you-can-do-to-get-fast-upsert-performance-in-apache-hudi.mdx",title:"Top 3 Things You Can Do to Get Fast Upsert Performance in Apache Hudi",description:"Redirecting... please wait!!",date:"2023-05-10T00:00:00.000Z",formattedDate:"May 10, 2023",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"performance",permalink:"/cn/blog/tags/performance"},{label:"onehouse",permalink:"/cn/blog/tags/onehouse"}],readingTime:.045,truncated:!1,authors:[{name:"Nadine Farah"}],prevItem:{title:"Ingesting data to Apache Hudi using Spark sql",permalink:"/cn/blog/2023/05/12/ingesting-data-to-apache-hudi-using-spark-sql"},nextItem:{title:"Amazon Athena now supports Apache Hudi 0.12.2",permalink:"/cn/blog/2023/05/09/amazon-athena-apache-hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.onehouse.ai/blog/top-3-things-you-can-do-to-get-fast-upsert-performance-in-apache-hudi",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},31552:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Ingesting data to Apache Hudi using Spark sql",authors:[{name:"Sivabalan Narayanan"}],category:"blog",tags:["how-to","spark-sql","medium"]},s=void 0,l={permalink:"/cn/blog/2023/05/12/ingesting-data-to-apache-hudi-using-spark-sql",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-05-12-ingesting-data-to-apache-hudi-using-spark-sql.mdx",source:"@site/blog/2023-05-12-ingesting-data-to-apache-hudi-using-spark-sql.mdx",title:"Ingesting data to Apache Hudi using Spark sql",description:"Redirecting... please wait!!",date:"2023-05-12T00:00:00.000Z",formattedDate:"May 12, 2023",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"spark-sql",permalink:"/cn/blog/tags/spark-sql"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Sivabalan Narayanan"}],prevItem:{title:"How Zoom implemented streaming log ingestion and efficient GDPR deletes using Apache Hudi on Amazon EMR",permalink:"/cn/blog/2023/05/16/how-zoom-implemented-streaming-log-ingestion-and-efficient-gdpr-deletes-using-apache-hudi-on-amazon-emr"},nextItem:{title:"Top 3 Things You Can Do to Get Fast Upsert Performance in Apache Hudi",permalink:"/cn/blog/2023/05/10/top-3-things-you-can-do-to-get-fast-upsert-performance-in-apache-hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@simpsons/ingesting-data-to-apache-hudi-using-spark-sql-36d9815423b3",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},27151:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"How Zoom implemented streaming log ingestion and efficient GDPR deletes using Apache Hudi on Amazon EMR",authors:[{name:"Sekar Srinivasan"},{name:"Amit Kumar Agrawal"},{name:"Chandra Dhandapani"},{name:"Viral Shah"}],category:"blog",image:"/assets/images/blog/2023-05-16-how-zoom-implemented-streaming-log-ingestion-and-efficient-gdpr-deletes-using-apache-hudi-on-amazon-emr.png",tags:["use-case","streaming ingestion","gdpr deletion","deletes","amazon"]},s=void 0,l={permalink:"/cn/blog/2023/05/16/how-zoom-implemented-streaming-log-ingestion-and-efficient-gdpr-deletes-using-apache-hudi-on-amazon-emr",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-05-16-how-zoom-implemented-streaming-log-ingestion-and-efficient-gdpr-deletes-using-apache-hudi-on-amazon-emr.mdx",source:"@site/blog/2023-05-16-how-zoom-implemented-streaming-log-ingestion-and-efficient-gdpr-deletes-using-apache-hudi-on-amazon-emr.mdx",title:"How Zoom implemented streaming log ingestion and efficient GDPR deletes using Apache Hudi on Amazon EMR",description:"Redirecting... please wait!!",date:"2023-05-16T00:00:00.000Z",formattedDate:"May 16, 2023",tags:[{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"streaming ingestion",permalink:"/cn/blog/tags/streaming-ingestion"},{label:"gdpr deletion",permalink:"/cn/blog/tags/gdpr-deletion"},{label:"deletes",permalink:"/cn/blog/tags/deletes"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[{name:"Sekar Srinivasan"},{name:"Amit Kumar Agrawal"},{name:"Chandra Dhandapani"},{name:"Viral Shah"}],prevItem:{title:"Hudi Metafields demystified",permalink:"/cn/blog/2023/05/19/hudi-metafields-demystified"},nextItem:{title:"Ingesting data to Apache Hudi using Spark sql",permalink:"/cn/blog/2023/05/12/ingesting-data-to-apache-hudi-using-spark-sql"}},d={authorsImageUrls:[void 0,void 0,void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/how-zoom-implemented-streaming-log-ingestion-and-efficient-gdpr-deletes-using-apache-hudi-on-amazon-emr/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},36891:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Hudi Metafields demystified",authors:[{name:"Bhavani Sudha Saktheeswaran"}],category:"blog",image:"/assets/images/blog/2023-05-19-Hudi-Metafields-demystified.png",tags:["design","metadata","metafields","onehouse"]},s=void 0,l={permalink:"/cn/blog/2023/05/19/hudi-metafields-demystified",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-05-19-hudi-metafields-demystified.mdx",source:"@site/blog/2023-05-19-hudi-metafields-demystified.mdx",title:"Hudi Metafields demystified",description:"Redirecting... please wait!!",date:"2023-05-19T00:00:00.000Z",formattedDate:"May 19, 2023",tags:[{label:"design",permalink:"/cn/blog/tags/design"},{label:"metadata",permalink:"/cn/blog/tags/metadata"},{label:"metafields",permalink:"/cn/blog/tags/metafields"},{label:"onehouse",permalink:"/cn/blog/tags/onehouse"}],readingTime:.045,truncated:!1,authors:[{name:"Bhavani Sudha Saktheeswaran"}],prevItem:{title:"Different Query types with Apache Hudi",permalink:"/cn/blog/2023/05/29/different-query-types-with-apache-hudi"},nextItem:{title:"How Zoom implemented streaming log ingestion and efficient GDPR deletes using Apache Hudi on Amazon EMR",permalink:"/cn/blog/2023/05/16/how-zoom-implemented-streaming-log-ingestion-and-efficient-gdpr-deletes-using-apache-hudi-on-amazon-emr"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.onehouse.ai/blog/hudi-metafields-demystified",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},90362:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Different Query types with Apache Hudi",authors:[{name:"Sivabalan Narayanan"}],category:"blog",tags:["blog","snapshot query","real-time query","time travel query","timestamp as of query","read optimized query","incremental query","medium"]},s=void 0,l={permalink:"/cn/blog/2023/05/29/different-query-types-with-apache-hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-05-29-different-query-types-with-apache-hudi.mdx",source:"@site/blog/2023-05-29-different-query-types-with-apache-hudi.mdx",title:"Different Query types with Apache Hudi",description:"Redirecting... please wait!!",date:"2023-05-29T00:00:00.000Z",formattedDate:"May 29, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"snapshot query",permalink:"/cn/blog/tags/snapshot-query"},{label:"real-time query",permalink:"/cn/blog/tags/real-time-query"},{label:"time travel query",permalink:"/cn/blog/tags/time-travel-query"},{label:"timestamp as of query",permalink:"/cn/blog/tags/timestamp-as-of-query"},{label:"read optimized query",permalink:"/cn/blog/tags/read-optimized-query"},{label:"incremental query",permalink:"/cn/blog/tags/incremental-query"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Sivabalan Narayanan"}],prevItem:{title:"Text-Based Search: From Elastic Search to Vector Search",permalink:"/cn/blog/2023/06/03/text-based-search-from-elastic-search-to-vector-search"},nextItem:{title:"Hudi Metafields demystified",permalink:"/cn/blog/2023/05/19/hudi-metafields-demystified"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@simpsons/different-query-types-with-apache-hudi-e14c2064cfd6",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},49999:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Text-Based Search: From Elastic Search to Vector Search",authors:[{name:"Kaushik Muniandi"}],category:"blog",image:"/assets/images/blog/2023-06-03-text-based-search-from-elastic-search-to-vector-search.png",tags:["blog","vector search","indexing","bloom","medium"]},s=void 0,l={permalink:"/cn/blog/2023/06/03/text-based-search-from-elastic-search-to-vector-search",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-06-03-text-based-search-from-elastic-search-to-vector-search.mdx",source:"@site/blog/2023-06-03-text-based-search-from-elastic-search-to-vector-search.mdx",title:"Text-Based Search: From Elastic Search to Vector Search",description:"Redirecting... please wait!!",date:"2023-06-03T00:00:00.000Z",formattedDate:"June 3, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"vector search",permalink:"/cn/blog/tags/vector-search"},{label:"indexing",permalink:"/cn/blog/tags/indexing"},{label:"bloom",permalink:"/cn/blog/tags/bloom"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Kaushik Muniandi"}],prevItem:{title:"Cleaner and Archival in Apache Hudi",permalink:"/cn/blog/2023/06/11/cleaner-and-archival-in-apache-hudi"},nextItem:{title:"Different Query types with Apache Hudi",permalink:"/cn/blog/2023/05/29/different-query-types-with-apache-hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@m.kaushik90/text-based-search-from-elastic-search-to-vector-search-15d686258bf2",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},77448:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Cleaner and Archival in Apache Hudi",authors:[{name:"Sivabalan Narayanan"}],category:"blog",image:"/assets/images/blog/2023-06-11-cleaner-and-archival-in-apache-hudi.jpg",tags:["blog","cleaner","timeline","active timeline","archival timeline","medium"]},s=void 0,l={permalink:"/cn/blog/2023/06/11/cleaner-and-archival-in-apache-hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-06-11-cleaner-and-archival-in-apache-hudi.mdx",source:"@site/blog/2023-06-11-cleaner-and-archival-in-apache-hudi.mdx",title:"Cleaner and Archival in Apache Hudi",description:"Redirecting... please wait!!",date:"2023-06-11T00:00:00.000Z",formattedDate:"June 11, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"cleaner",permalink:"/cn/blog/tags/cleaner"},{label:"timeline",permalink:"/cn/blog/tags/timeline"},{label:"active timeline",permalink:"/cn/blog/tags/active-timeline"},{label:"archival timeline",permalink:"/cn/blog/tags/archival-timeline"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Sivabalan Narayanan"}],prevItem:{title:"Exploring New Frontiers: How Apache Flink, Apache Hudi and Presto Power New Insights at Scale",permalink:"/cn/blog/2023/06/16/Exploring-New-Frontiers-How-Apache-Flink-Apache-Hudi-and-Presto-Power-New-Insights-at-Scale"},nextItem:{title:"Text-Based Search: From Elastic Search to Vector Search",permalink:"/cn/blog/2023/06/03/text-based-search-from-elastic-search-to-vector-search"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@simpsons/cleaner-and-archival-in-apache-hudi-9e15b08b2933",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},63939:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Exploring New Frontiers: How Apache Flink, Apache Hudi and Presto Power New Insights at Scale",authors:[{name:"Nadine Farah"}],category:"blog",image:"/assets/images/blog/2023-06-16-Exploring-New-Frontiers-How-Apache-Flink-Apache-Hudi-and-Presto-Power-New-Insights-at-Scale.png",tags:["blog","prestocon","flink","presto","streaming","incremental etl"]},s=void 0,l={permalink:"/cn/blog/2023/06/16/Exploring-New-Frontiers-How-Apache-Flink-Apache-Hudi-and-Presto-Power-New-Insights-at-Scale",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-06-16-Exploring-New-Frontiers-How-Apache-Flink-Apache-Hudi-and-Presto-Power-New-Insights-at-Scale.mdx",source:"@site/blog/2023-06-16-Exploring-New-Frontiers-How-Apache-Flink-Apache-Hudi-and-Presto-Power-New-Insights-at-Scale.mdx",title:"Exploring New Frontiers: How Apache Flink, Apache Hudi and Presto Power New Insights at Scale",description:"Redirecting... please wait!!",date:"2023-06-16T00:00:00.000Z",formattedDate:"June 16, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"prestocon",permalink:"/cn/blog/tags/prestocon"},{label:"flink",permalink:"/cn/blog/tags/flink"},{label:"presto",permalink:"/cn/blog/tags/presto"},{label:"streaming",permalink:"/cn/blog/tags/streaming"},{label:"incremental etl",permalink:"/cn/blog/tags/incremental-etl"}],readingTime:.045,truncated:!1,authors:[{name:"Nadine Farah"}],prevItem:{title:"Timeline Server in Apache Hudi",permalink:"/cn/blog/2023/06/20/timeline-server-in-apache-hudi"},nextItem:{title:"Cleaner and Archival in Apache Hudi",permalink:"/cn/blog/2023/06/11/cleaner-and-archival-in-apache-hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.onehouse.ai/blog/exploring-new-frontiers-how-apache-flink-apache-hudi-and-presto-power-new-insights-at-scale",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},36969:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"How to query data in Apache Hudi using StarRocks",authors:[{name:"Albert Wong"}],category:"blog",image:"/assets/images/blog/2023-06-20-How-to-query-data-in-Apache-Hudi-using-StarRocks.png",tags:["blog","starrocks","queries","medium"]},s=void 0,l={permalink:"/cn/blog/2023/06/20/How-to-query-data-in-Apache-Hudi-using-StarRocks",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-06-20-How-to-query-data-in-Apache-Hudi-using-StarRocks.mdx",source:"@site/blog/2023-06-20-How-to-query-data-in-Apache-Hudi-using-StarRocks.mdx",title:"How to query data in Apache Hudi using StarRocks",description:"Redirecting... please wait!!",date:"2023-06-20T00:00:00.000Z",formattedDate:"June 20, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"starrocks",permalink:"/cn/blog/tags/starrocks"},{label:"queries",permalink:"/cn/blog/tags/queries"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Albert Wong"}],prevItem:{title:"Multi-writer support with Apache Hudi",permalink:"/cn/blog/2023/06/24/multi-writer-support-in-apache-hudi"},nextItem:{title:"Timeline Server in Apache Hudi",permalink:"/cn/blog/2023/06/20/timeline-server-in-apache-hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@atwong/how-to-query-data-in-apache-hudi-using-starrocks-bf0336eaa817",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},84730:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Timeline Server in Apache Hudi",authors:[{name:"Sivabalan Narayanan"}],category:"blog",image:"/assets/images/blog/2023-06-20-timeline-server-in-apache-hudi.png",tags:["blog","timeline Server","FileSystemView","medium"]},s=void 0,l={permalink:"/cn/blog/2023/06/20/timeline-server-in-apache-hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-06-20-timeline-server-in-apache-hudi.mdx",source:"@site/blog/2023-06-20-timeline-server-in-apache-hudi.mdx",title:"Timeline Server in Apache Hudi",description:"Redirecting... please wait!!",date:"2023-06-20T00:00:00.000Z",formattedDate:"June 20, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"timeline Server",permalink:"/cn/blog/tags/timeline-server"},{label:"FileSystemView",permalink:"/cn/blog/tags/file-system-view"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Sivabalan Narayanan"}],prevItem:{title:"How to query data in Apache Hudi using StarRocks",permalink:"/cn/blog/2023/06/20/How-to-query-data-in-Apache-Hudi-using-StarRocks"},nextItem:{title:"Exploring New Frontiers: How Apache Flink, Apache Hudi and Presto Power New Insights at Scale",permalink:"/cn/blog/2023/06/16/Exploring-New-Frontiers-How-Apache-Flink-Apache-Hudi-and-Presto-Power-New-Insights-at-Scale"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@simpsons/timeline-server-in-apache-hudi-b5be25f85e47",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},55702:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Multi-writer support with Apache Hudi",authors:[{name:"Sivabalan Narayanan"}],category:"blog",image:"/assets/images/blog/2023-06-24-multi-writer-support-in-apache-hudi.png",tags:["blog","concurrency control","lock provider","multi writer","medium"]},s=void 0,l={permalink:"/cn/blog/2023/06/24/multi-writer-support-in-apache-hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-06-24-multi-writer-support-in-apache-hudi.mdx",source:"@site/blog/2023-06-24-multi-writer-support-in-apache-hudi.mdx",title:"Multi-writer support with Apache Hudi",description:"Redirecting... please wait!!",date:"2023-06-24T00:00:00.000Z",formattedDate:"June 24, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"concurrency control",permalink:"/cn/blog/tags/concurrency-control"},{label:"lock provider",permalink:"/cn/blog/tags/lock-provider"},{label:"multi writer",permalink:"/cn/blog/tags/multi-writer"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Sivabalan Narayanan"}],prevItem:{title:"Unlimited Big Data Exchange: A Wonderful Review of Apache DolphinScheduler & Hudi Hangzhou Meetup",permalink:"/cn/blog/2023/06/26/Unlimited-Big-Data-Exchange-A-Wonderful-Review-of-Apache-DolphinScheduler-and-Hudi-Hangzhou-Meetup"},nextItem:{title:"How to query data in Apache Hudi using StarRocks",permalink:"/cn/blog/2023/06/20/How-to-query-data-in-Apache-Hudi-using-StarRocks"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@simpsons/multi-writer-support-with-apache-hudi-e1b75dca29e6",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},26910:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Unlimited Big Data Exchange: A Wonderful Review of Apache DolphinScheduler & Hudi Hangzhou Meetup",authors:[{name:"Apache DolphinScheduler"}],category:"blog",image:"/assets/images/blog/2023-06-26-Unlimited-Big-Data-Exchange-A-Wonderful-Review-of-Apache-DolphinScheduler-and-Hudi-Hangzhou-Meetup.jpeg",tags:["blog","Apache DolphinScheduler","meetup","medium"]},s=void 0,l={permalink:"/cn/blog/2023/06/26/Unlimited-Big-Data-Exchange-A-Wonderful-Review-of-Apache-DolphinScheduler-and-Hudi-Hangzhou-Meetup",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-06-26-Unlimited-Big-Data-Exchange-A-Wonderful-Review-of-Apache-DolphinScheduler-and-Hudi-Hangzhou-Meetup.mdx",source:"@site/blog/2023-06-26-Unlimited-Big-Data-Exchange-A-Wonderful-Review-of-Apache-DolphinScheduler-and-Hudi-Hangzhou-Meetup.mdx",title:"Unlimited Big Data Exchange: A Wonderful Review of Apache DolphinScheduler & Hudi Hangzhou Meetup",description:"Redirecting... please wait!!",date:"2023-06-26T00:00:00.000Z",formattedDate:"June 26, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"Apache DolphinScheduler",permalink:"/cn/blog/tags/apache-dolphin-scheduler"},{label:"meetup",permalink:"/cn/blog/tags/meetup"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Apache DolphinScheduler"}],prevItem:{title:"What about Apache Hudi, Apache Iceberg, and Delta Lake?",permalink:"/cn/blog/2023/06/30/What-about-Apache-Hudi-Apache-Iceberg-and-Delta-Lake"},nextItem:{title:"Multi-writer support with Apache Hudi",permalink:"/cn/blog/2023/06/24/multi-writer-support-in-apache-hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@ApacheDolphinScheduler/unlimited-big-data-exchange-a-wonderful-review-of-apache-dolphinscheduler-hudi-hangzhou-meetup-4e4e7dae0f55",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},1310:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"What about Apache Hudi, Apache Iceberg, and Delta Lake?",authors:[{name:"Martin Jurado Pedroza"}],category:"blog",image:"/assets/images/blog/2023-06-30-What-about-Apache-Hudi-Apache-Iceberg-and-Delta-Lake.png",tags:["blog","vector search","comparison","apache hudi","delta lake","iceberg","medium"]},s=void 0,l={permalink:"/cn/blog/2023/06/30/What-about-Apache-Hudi-Apache-Iceberg-and-Delta-Lake",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-06-30-What-about-Apache-Hudi-Apache-Iceberg-and-Delta-Lake.mdx",source:"@site/blog/2023-06-30-What-about-Apache-Hudi-Apache-Iceberg-and-Delta-Lake.mdx",title:"What about Apache Hudi, Apache Iceberg, and Delta Lake?",description:"Redirecting... please wait!!",date:"2023-06-30T00:00:00.000Z",formattedDate:"June 30, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"vector search",permalink:"/cn/blog/tags/vector-search"},{label:"comparison",permalink:"/cn/blog/tags/comparison"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"delta lake",permalink:"/cn/blog/tags/delta-lake"},{label:"iceberg",permalink:"/cn/blog/tags/iceberg"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Martin Jurado Pedroza"}],prevItem:{title:"Monitoring Table Size stats",permalink:"/cn/blog/2023/07/01/monitoring-table-size-stats"},nextItem:{title:"Unlimited Big Data Exchange: A Wonderful Review of Apache DolphinScheduler & Hudi Hangzhou Meetup",permalink:"/cn/blog/2023/06/26/Unlimited-Big-Data-Exchange-A-Wonderful-Review-of-Apache-DolphinScheduler-and-Hudi-Hangzhou-Meetup"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@martin.jurado.p/what-about-apache-hudi-apache-iceberg-and-delta-lake-3cae0eecd148",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},47364:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Monitoring Table Size stats",authors:[{name:"Sivabalan Narayanan"}],category:"blog",image:"/assets/images/blog/2023-07-01-monitoring-table-size-stats.png",tags:["blog","table size stats","medium"]},s=void 0,l={permalink:"/cn/blog/2023/07/01/monitoring-table-size-stats",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-07-01-monitoring-table-size-stats.mdx",source:"@site/blog/2023-07-01-monitoring-table-size-stats.mdx",title:"Monitoring Table Size stats",description:"Redirecting... please wait!!",date:"2023-07-01T00:00:00.000Z",formattedDate:"July 1, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"table size stats",permalink:"/cn/blog/tags/table-size-stats"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Sivabalan Narayanan"}],prevItem:{title:"Hudi Best Practices: Handling Failed Inserts/Upserts with Error Tables",permalink:"/cn/blog/2023/07/02/Hudi-Best-Practices-Handling-Failed-Inserts-Upserts-with-Error-Tables"},nextItem:{title:"What about Apache Hudi, Apache Iceberg, and Delta Lake?",permalink:"/cn/blog/2023/06/30/What-about-Apache-Hudi-Apache-Iceberg-and-Delta-Lake"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@simpsons/monitoring-table-stats-22684eb70ee1",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},84458:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Hudi Best Practices: Handling Failed Inserts/Upserts with Error Tables",authors:[{name:"Soumil Shah"}],category:"blog",image:"/assets/images/blog/2023-07-02-Hudi-Best-Practices-Handling-Failed-Inserts-Upserts-with-Error-Tables.png",tags:["blog","linkedin","apache hudi","inserts","upserts"]},s=void 0,l={permalink:"/cn/blog/2023/07/02/Hudi-Best-Practices-Handling-Failed-Inserts-Upserts-with-Error-Tables",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-07-02-Hudi-Best-Practices-Handling-Failed-Inserts-Upserts-with-Error-Tables.mdx",source:"@site/blog/2023-07-02-Hudi-Best-Practices-Handling-Failed-Inserts-Upserts-with-Error-Tables.mdx",title:"Hudi Best Practices: Handling Failed Inserts/Upserts with Error Tables",description:"Redirecting... please wait!!",date:"2023-07-02T00:00:00.000Z",formattedDate:"July 2, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"linkedin",permalink:"/cn/blog/tags/linkedin"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"inserts",permalink:"/cn/blog/tags/inserts"},{label:"upserts",permalink:"/cn/blog/tags/upserts"}],readingTime:.045,truncated:!1,authors:[{name:"Soumil Shah"}],prevItem:{title:"Skip rocks and files: Turbocharge Trino queries with Hudi\u2019s multi-modal indexing subsystem",permalink:"/cn/blog/2023/07/07/Skip-rocks-and-files-Turbocharge-Trino-queries-with-Hudi-multi-modal-indexing-subsystem"},nextItem:{title:"Monitoring Table Size stats",permalink:"/cn/blog/2023/07/01/monitoring-table-size-stats"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.linkedin.com/pulse/hudi-best-practices-handling-failed-insertsupserts-error-soumil-shah/?utm_source=share&utm_medium=member_ios&utm_campaign=share_via",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},60881:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Skip rocks and files: Turbocharge Trino queries with Hudi\u2019s multi-modal indexing subsystem",authors:[{name:"Nadine Farah"},{name:"Sagar Sumit"},{name:"Cole Bowden"}],category:"blog",image:"/assets/images/blog/2023-07-07-Skip-rocks-and-files-Turbocharge-Trino-queries-with-Hudi-multi-modal-indexing-subsystem.png",tags:["blog","conference","trino","apache hudi","multi modal indexing","queries"]},s=void 0,l={permalink:"/cn/blog/2023/07/07/Skip-rocks-and-files-Turbocharge-Trino-queries-with-Hudi-multi-modal-indexing-subsystem",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-07-07-Skip-rocks-and-files-Turbocharge-Trino-queries-with-Hudi-multi-modal-indexing-subsystem.mdx",source:"@site/blog/2023-07-07-Skip-rocks-and-files-Turbocharge-Trino-queries-with-Hudi-multi-modal-indexing-subsystem.mdx",title:"Skip rocks and files: Turbocharge Trino queries with Hudi\u2019s multi-modal indexing subsystem",description:"Redirecting... please wait!!",date:"2023-07-07T00:00:00.000Z",formattedDate:"July 7, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"conference",permalink:"/cn/blog/tags/conference"},{label:"trino",permalink:"/cn/blog/tags/trino"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"multi modal indexing",permalink:"/cn/blog/tags/multi-modal-indexing"},{label:"queries",permalink:"/cn/blog/tags/queries"}],readingTime:.045,truncated:!1,authors:[{name:"Nadine Farah"},{name:"Sagar Sumit"},{name:"Cole Bowden"}],prevItem:{title:"Quickly start using Apache Hudi on AWS EMR",permalink:"/cn/blog/2023/07/08/Quickly-start-using-Apache-Hudi-on-AWS-EMR"},nextItem:{title:"Hudi Best Practices: Handling Failed Inserts/Upserts with Error Tables",permalink:"/cn/blog/2023/07/02/Hudi-Best-Practices-Handling-Failed-Inserts-Upserts-with-Error-Tables"}},d={authorsImageUrls:[void 0,void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://trino.io/blog/2023/07/07/trino-fest-2023-onehouse-recap.html",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},82317:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Quickly start using Apache Hudi on AWS EMR",authors:[{name:"Ritik Kaushik"}],category:"blog",tags:["blog","aws emr","cow","medium"]},s=void 0,l={permalink:"/cn/blog/2023/07/08/Quickly-start-using-Apache-Hudi-on-AWS-EMR",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-07-08-Quickly-start-using-Apache-Hudi-on-AWS-EMR.mdx",source:"@site/blog/2023-07-08-Quickly-start-using-Apache-Hudi-on-AWS-EMR.mdx",title:"Quickly start using Apache Hudi on AWS EMR",description:"Redirecting... please wait!!",date:"2023-07-08T00:00:00.000Z",formattedDate:"July 8, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"aws emr",permalink:"/cn/blog/tags/aws-emr"},{label:"cow",permalink:"/cn/blog/tags/cow"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Ritik Kaushik"}],prevItem:{title:"Hoodie Timeline: Foundational pillar for ACID transactions",permalink:"/cn/blog/2023/07/09/Hoodie-Timeline-Foundational-pillar-for-ACID-transactions"},nextItem:{title:"Skip rocks and files: Turbocharge Trino queries with Hudi\u2019s multi-modal indexing subsystem",permalink:"/cn/blog/2023/07/07/Skip-rocks-and-files-Turbocharge-Trino-queries-with-Hudi-multi-modal-indexing-subsystem"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@ritik20023/quickly-start-using-apache-hudi-on-aws-emr-de432c01e488",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},2507:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Hoodie Timeline: Foundational pillar for ACID transactions",authors:[{name:"Sivabalan Narayanan"}],category:"blog",image:"/assets/images/hudi_timeline.png",tags:["blog","ACID","transactions","commits","timeline","medium"]},s=void 0,l={permalink:"/cn/blog/2023/07/09/Hoodie-Timeline-Foundational-pillar-for-ACID-transactions",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-07-09-Hoodie-Timeline-Foundational-pillar-for-ACID-transactions.mdx",source:"@site/blog/2023-07-09-Hoodie-Timeline-Foundational-pillar-for-ACID-transactions.mdx",title:"Hoodie Timeline: Foundational pillar for ACID transactions",description:"Redirecting... please wait!!",date:"2023-07-09T00:00:00.000Z",formattedDate:"July 9, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"ACID",permalink:"/cn/blog/tags/acid"},{label:"transactions",permalink:"/cn/blog/tags/transactions"},{label:"commits",permalink:"/cn/blog/tags/commits"},{label:"timeline",permalink:"/cn/blog/tags/timeline"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Sivabalan Narayanan"}],prevItem:{title:"Backfilling Apache Hudi Tables in Production: Techniques & Approaches Using AWS Glue by Job Target LLC",permalink:"/cn/blog/2023/07/20/Backfilling-Apache-Hudi-Tables-in-Production-Techniques-and-Approaches-Using-AWS-Glue-by-Job-Target-LLC"},nextItem:{title:"Quickly start using Apache Hudi on AWS EMR",permalink:"/cn/blog/2023/07/08/Quickly-start-using-Apache-Hudi-on-AWS-EMR"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@simpsons/hoodie-timeline-foundational-pillar-for-acid-transactions-be871399cbae",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},14390:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Backfilling Apache Hudi Tables in Production: Techniques & Approaches Using AWS Glue by Job Target LLC",authors:[{name:"Soumil Shah"}],category:"blog",image:"/assets/images/blog/2023-07-20-Backfilling-Apache-Hudi-Tables-in-Production-Techniques-and-Approaches-Using-AWS-Glue-by-Job-Target-LLC.png",tags:["blog","backfilling","hudi","aws glue","code sample"]},s=void 0,l={permalink:"/cn/blog/2023/07/20/Backfilling-Apache-Hudi-Tables-in-Production-Techniques-and-Approaches-Using-AWS-Glue-by-Job-Target-LLC",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-07-20-Backfilling-Apache-Hudi-Tables-in-Production-Techniques-and-Approaches-Using-AWS-Glue-by-Job-Target-LLC.mdx",source:"@site/blog/2023-07-20-Backfilling-Apache-Hudi-Tables-in-Production-Techniques-and-Approaches-Using-AWS-Glue-by-Job-Target-LLC.mdx",title:"Backfilling Apache Hudi Tables in Production: Techniques & Approaches Using AWS Glue by Job Target LLC",description:"Redirecting... please wait!!",date:"2023-07-20T00:00:00.000Z",formattedDate:"July 20, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"backfilling",permalink:"/cn/blog/tags/backfilling"},{label:"hudi",permalink:"/cn/blog/tags/hudi"},{label:"aws glue",permalink:"/cn/blog/tags/aws-glue"},{label:"code sample",permalink:"/cn/blog/tags/code-sample"}],readingTime:.045,truncated:!1,authors:[{name:"Soumil Shah"}],prevItem:{title:"AWS Glue Crawlers now supports Apache Hudi Tables",permalink:"/cn/blog/2023/07/21/AWS-Glue-Crawlers-now-supports-Apache-Hudi-Tables"},nextItem:{title:"Hoodie Timeline: Foundational pillar for ACID transactions",permalink:"/cn/blog/2023/07/09/Hoodie-Timeline-Foundational-pillar-for-ACID-transactions"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.linkedin.com/pulse/backfilling-apache-hudi-tables-production-techniques-approaches-shah",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},57924:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"AWS Glue Crawlers now supports Apache Hudi Tables",authors:[{name:"AWS Team"}],category:"blog",image:"/assets/images/blog/2023-07-21-AWS-Glue-Crawlers-now-supports-Apache-Hudi-Tables.png",tags:["blog","aws glue","hudi","glue crawler"]},s=void 0,l={permalink:"/cn/blog/2023/07/21/AWS-Glue-Crawlers-now-supports-Apache-Hudi-Tables",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-07-21-AWS-Glue-Crawlers-now-supports-Apache-Hudi-Tables.mdx",source:"@site/blog/2023-07-21-AWS-Glue-Crawlers-now-supports-Apache-Hudi-Tables.mdx",title:"AWS Glue Crawlers now supports Apache Hudi Tables",description:"Redirecting... please wait!!",date:"2023-07-21T00:00:00.000Z",formattedDate:"July 21, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"aws glue",permalink:"/cn/blog/tags/aws-glue"},{label:"hudi",permalink:"/cn/blog/tags/hudi"},{label:"glue crawler",permalink:"/cn/blog/tags/glue-crawler"}],readingTime:.045,truncated:!1,authors:[{name:"AWS Team"}],prevItem:{title:"Apache Hudi: Revolutionizing Big Data Management for Real-Time Analytics",permalink:"/cn/blog/2023/07/27/Apache-Hudi-Revolutionizing-Big-Data-Management-for-Real-Time-Analytics"},nextItem:{title:"Backfilling Apache Hudi Tables in Production: Techniques & Approaches Using AWS Glue by Job Target LLC",permalink:"/cn/blog/2023/07/20/Backfilling-Apache-Hudi-Tables-in-Production-Techniques-and-Approaches-Using-AWS-Glue-by-Job-Target-LLC"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/about-aws/whats-new/2023/07/aws-glue-crawlers-apache-hudi-tables/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},71533:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Apache Hudi: Revolutionizing Big Data Management for Real-Time Analytics",authors:[{name:"Dev Jain"}],category:"blog",image:"/assets/images/blog/2023-07-27-Apache-Hudi-Revolutionizing-Big-Data-Management-for-Real-Time-Analytics.png",tags:["blog","medium","hudi"]},s=void 0,l={permalink:"/cn/blog/2023/07/27/Apache-Hudi-Revolutionizing-Big-Data-Management-for-Real-Time-Analytics",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-07-27-Apache-Hudi-Revolutionizing-Big-Data-Management-for-Real-Time-Analytics.mdx",source:"@site/blog/2023-07-27-Apache-Hudi-Revolutionizing-Big-Data-Management-for-Real-Time-Analytics.mdx",title:"Apache Hudi: Revolutionizing Big Data Management for Real-Time Analytics",description:"Redirecting... please wait!!",date:"2023-07-27T00:00:00.000Z",formattedDate:"July 27, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"medium",permalink:"/cn/blog/tags/medium"},{label:"hudi",permalink:"/cn/blog/tags/hudi"}],readingTime:.045,truncated:!1,authors:[{name:"Dev Jain"}],prevItem:{title:"Data lake Table formats: Apache Iceberg vs Apache Hudi vs Delta lake",permalink:"/cn/blog/2023/08/03/Data-lake-Table-formats-Apache-Iceberg-vs-Apache-Hudi-vs-Delta-lake"},nextItem:{title:"AWS Glue Crawlers now supports Apache Hudi Tables",permalink:"/cn/blog/2023/07/21/AWS-Glue-Crawlers-now-supports-Apache-Hudi-Tables"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@devjain1299/apache-hudi-revolutionizing-big-data-management-for-real-time-analytics-5130808e067a",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},39520:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Apache Hudi on AWS Glue: A Step-by-Step Guide",authors:[{name:"Dev Jain"}],category:"blog",image:"/assets/images/blog/2023-08-03-Apache-Hudi-on-AWS-Glue-A-Step-by-Step-Guide.png",tags:["how-to","aws-glue","apache-hudi","medium"]},s=void 0,l={permalink:"/cn/blog/2023/08/03/Apache-Hudi-on-AWS-Glue-A-Step-by-Step-Guide",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-08-03-Apache-Hudi-on-AWS-Glue-A-Step-by-Step-Guide.mdx",source:"@site/blog/2023-08-03-Apache-Hudi-on-AWS-Glue-A-Step-by-Step-Guide.mdx",title:"Apache Hudi on AWS Glue: A Step-by-Step Guide",description:"Redirecting... please wait!!",date:"2023-08-03T00:00:00.000Z",formattedDate:"August 3, 2023",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"aws-glue",permalink:"/cn/blog/tags/aws-glue"},{label:"apache-hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Dev Jain"}],prevItem:{title:"Data Lakehouse Architecture for Big Data with Apache Hudi",permalink:"/cn/blog/2023/08/05/Data-Lakehouse-Architecture-for-Big-Data-with-Apache-Hudi"},nextItem:{title:"Create an Apache Hudi-based-near-real-time transactional data lake using AWS DMS, Amazon Kinesis, AWS Glue streaming ETL, and data visualization using Amazon QuickSight",permalink:"/cn/blog/2023/08/03/Create-an-Apache-Hudi-based-near-real-time-transactional-data lake-using-AWS-DMS-Amazon-Kinesis-AWS-Glue-streaming-ETL-and-data-visualization-using-Amazon-QuickSight"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@devjain1299/apache-hudi-on-aws-glue-a-step-by-step-guide-503c34a9aa95",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},72440:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Create an Apache Hudi-based-near-real-time transactional data lake using AWS DMS, Amazon Kinesis, AWS Glue streaming ETL, and data visualization using Amazon QuickSight",authors:[{name:"Raj Ramasubbu"},{name:"Sundeep Kumar"},{name:"Rahul Sonawane"}],category:"blog",image:"/assets/images/blog/2023-08-03-near-realtime-trans-datalake-aws-dms-kinesis.png",tags:["how-to","cdc","change data capture","upserts","amazon"]},s=void 0,l={permalink:"/cn/blog/2023/08/03/Create-an-Apache-Hudi-based-near-real-time-transactional-data lake-using-AWS-DMS-Amazon-Kinesis-AWS-Glue-streaming-ETL-and-data-visualization-using-Amazon-QuickSight",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-08-03-Create-an-Apache-Hudi-based-near-real-time-transactional-data lake-using-AWS-DMS-Amazon-Kinesis-AWS-Glue-streaming-ETL-and-data-visualization-using-Amazon-QuickSight.mdx",source:"@site/blog/2023-08-03-Create-an-Apache-Hudi-based-near-real-time-transactional-data lake-using-AWS-DMS-Amazon-Kinesis-AWS-Glue-streaming-ETL-and-data-visualization-using-Amazon-QuickSight.mdx",title:"Create an Apache Hudi-based-near-real-time transactional data lake using AWS DMS, Amazon Kinesis, AWS Glue streaming ETL, and data visualization using Amazon QuickSight",description:"Redirecting... please wait!!",date:"2023-08-03T00:00:00.000Z",formattedDate:"August 3, 2023",tags:[{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"cdc",permalink:"/cn/blog/tags/cdc"},{label:"change data capture",permalink:"/cn/blog/tags/change-data-capture"},{label:"upserts",permalink:"/cn/blog/tags/upserts"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[{name:"Raj Ramasubbu"},{name:"Sundeep Kumar"},{name:"Rahul Sonawane"}],prevItem:{title:"Apache Hudi on AWS Glue: A Step-by-Step Guide",permalink:"/cn/blog/2023/08/03/Apache-Hudi-on-AWS-Glue-A-Step-by-Step-Guide"},nextItem:{title:"Data lake Table formats: Apache Iceberg vs Apache Hudi vs Delta lake",permalink:"/cn/blog/2023/08/03/Data-lake-Table-formats-Apache-Iceberg-vs-Apache-Hudi-vs-Delta-lake"}},d={authorsImageUrls:[void 0,void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/create-an-apache-hudi-based-near-real-time-transactional-data-lake-using-aws-dms-amazon-kinesis-aws-glue-streaming-etl-and-data-visualization-using-amazon-quicksight/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},4578:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Data lake Table formats: Apache Iceberg vs Apache Hudi vs Delta lake",authors:[{name:"Shashwat Pandey"}],category:"blog",image:"/assets/images/blog/2023-08-03-Data-lake-Table-formats-Apache-Iceberg-vs-Apache-Hudi-vs-Delta-lake.png",tags:["blog","hudi","iceberg","delta lake","medium"]},s=void 0,l={permalink:"/cn/blog/2023/08/03/Data-lake-Table-formats-Apache-Iceberg-vs-Apache-Hudi-vs-Delta-lake",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-08-03-Data-lake-Table-formats-Apache-Iceberg-vs-Apache-Hudi-vs-Delta-lake.mdx",source:"@site/blog/2023-08-03-Data-lake-Table-formats-Apache-Iceberg-vs-Apache-Hudi-vs-Delta-lake.mdx",title:"Data lake Table formats: Apache Iceberg vs Apache Hudi vs Delta lake",description:"Redirecting... please wait!!",date:"2023-08-03T00:00:00.000Z",formattedDate:"August 3, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"hudi",permalink:"/cn/blog/tags/hudi"},{label:"iceberg",permalink:"/cn/blog/tags/iceberg"},{label:"delta lake",permalink:"/cn/blog/tags/delta-lake"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Shashwat Pandey"}],prevItem:{title:"Create an Apache Hudi-based-near-real-time transactional data lake using AWS DMS, Amazon Kinesis, AWS Glue streaming ETL, and data visualization using Amazon QuickSight",permalink:"/cn/blog/2023/08/03/Create-an-Apache-Hudi-based-near-real-time-transactional-data lake-using-AWS-DMS-Amazon-Kinesis-AWS-Glue-streaming-ETL-and-data-visualization-using-Amazon-QuickSight"},nextItem:{title:"Apache Hudi: Revolutionizing Big Data Management for Real-Time Analytics",permalink:"/cn/blog/2023/07/27/Apache-Hudi-Revolutionizing-Big-Data-Management-for-Real-Time-Analytics"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://shashwat-pandey.medium.com/data-lake-table-formats-apache-iceberg-vs-apache-hudi-vs-delta-lake-10b67a1d587",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},73927:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Data Lakehouse Architecture for Big Data with Apache Hudi",authors:[{name:"Tauno Treier"}],category:"blog",image:"/assets/images/blog/2023-08-05-Data-Lakehouse-Architecture-for-Big-Data-with-Apache-Hudi.png",tags:["blog","apache hudi","data lakehouse","big data","google scholar"]},s=void 0,l={permalink:"/cn/blog/2023/08/05/Data-Lakehouse-Architecture-for-Big-Data-with-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-08-05-Data-Lakehouse-Architecture-for-Big-Data-with-Apache-Hudi.mdx",source:"@site/blog/2023-08-05-Data-Lakehouse-Architecture-for-Big-Data-with-Apache-Hudi.mdx",title:"Data Lakehouse Architecture for Big Data with Apache Hudi",description:"Redirecting... please wait!!",date:"2023-08-05T00:00:00.000Z",formattedDate:"August 5, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"data lakehouse",permalink:"/cn/blog/tags/data-lakehouse"},{label:"big data",permalink:"/cn/blog/tags/big-data"},{label:"google scholar",permalink:"/cn/blog/tags/google-scholar"}],readingTime:.045,truncated:!1,authors:[{name:"Tauno Treier"}],prevItem:{title:"Lakehouse Trifecta \u2014 Delta Lake, Apache Iceberg & Apache Hudi",permalink:"/cn/blog/2023/08/09/Lakehouse-Trifecta-Delta-Lake-Apache-Iceberg-and-Apache-Hudi"},nextItem:{title:"Apache Hudi on AWS Glue: A Step-by-Step Guide",permalink:"/cn/blog/2023/08/03/Apache-Hudi-on-AWS-Glue-A-Step-by-Step-Guide"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://scholar.googleusercontent.com/scholar?q=cache:Cwi32O7nQCUJ:scholar.google.com/+apache+hudi&hl=en&as_sdt=0,7",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},38940:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Lakehouse Trifecta \u2014 Delta Lake, Apache Iceberg & Apache Hudi",authors:[{name:"Sandip Roy"}],category:"blog",image:"/assets/images/blog/2023-08-09-Lakehouse-Trifecta-Delta-Lake-Apache-Iceberg-and-Apache-Hudi.png",tags:["blog","hudi","delta lake","iceberg","medium"]},s=void 0,l={permalink:"/cn/blog/2023/08/09/Lakehouse-Trifecta-Delta-Lake-Apache-Iceberg-and-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-08-09-Lakehouse-Trifecta-Delta-Lake-Apache-Iceberg-and-Apache-Hudi.mdx",source:"@site/blog/2023-08-09-Lakehouse-Trifecta-Delta-Lake-Apache-Iceberg-and-Apache-Hudi.mdx",title:"Lakehouse Trifecta \u2014 Delta Lake, Apache Iceberg & Apache Hudi",description:"Redirecting... please wait!!",date:"2023-08-09T00:00:00.000Z",formattedDate:"August 9, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"hudi",permalink:"/cn/blog/tags/hudi"},{label:"delta lake",permalink:"/cn/blog/tags/delta-lake"},{label:"iceberg",permalink:"/cn/blog/tags/iceberg"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Sandip Roy"}],prevItem:{title:"Exploring various storage types in Apache Hudi",permalink:"/cn/blog/2023/08/22/Exploring-various-storage-types-in-Apache-Hudi"},nextItem:{title:"Data Lakehouse Architecture for Big Data with Apache Hudi",permalink:"/cn/blog/2023/08/05/Data-Lakehouse-Architecture-for-Big-Data-with-Apache-Hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://roysandip.medium.com/lakehouse-trifecta-delta-lake-apache-iceberg-apache-hudi-747e99c467b",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},75859:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Exploring various storage types in Apache Hudi",excerpt:"Hudi Storage Format Overview",author:"Arun Kumar Nagaraj",category:"blog",image:"/assets/images/blog/2023-08-22-Exploring-various-storage-types-in-Apache-Hudi.png",tags:["blog","apache hudi","storage types","medium"]},s=void 0,l={permalink:"/cn/blog/2023/08/22/Exploring-various-storage-types-in-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-08-22-Exploring-various-storage-types-in-Apache-Hudi.mdx",source:"@site/blog/2023-08-22-Exploring-various-storage-types-in-Apache-Hudi.mdx",title:"Exploring various storage types in Apache Hudi",description:"Redirecting... please wait!!",date:"2023-08-22T00:00:00.000Z",formattedDate:"August 22, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"storage types",permalink:"/cn/blog/tags/storage-types"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Arun Kumar Nagaraj"}],prevItem:{title:"Delta, Hudi, Iceberg \u2014 Which is most popular?",permalink:"/cn/blog/2023/08/25/Delta-Hudi-Iceberg-Which-is-most-popular"},nextItem:{title:"Lakehouse Trifecta \u2014 Delta Lake, Apache Iceberg & Apache Hudi",permalink:"/cn/blog/2023/08/09/Lakehouse-Trifecta-Delta-Lake-Apache-Iceberg-and-Apache-Hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@aruun1995/exploring-various-storage-types-in-apache-hudi-a3f0ab394a95",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},46218:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Delta, Hudi, Iceberg \u2014 Which is most popular?",excerpt:"Popular Lakehoue Project",author:"Kyle Weller",category:"blog",image:"/assets/images/blog/2023-08-25-Delta-Hudi-Iceberg-Which-is-most-popular.png",tags:["blog","apache hudi","delta lake","iceberg","medium"]},s=void 0,l={permalink:"/cn/blog/2023/08/25/Delta-Hudi-Iceberg-Which-is-most-popular",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-08-25-Delta-Hudi-Iceberg-Which-is-most-popular.mdx",source:"@site/blog/2023-08-25-Delta-Hudi-Iceberg-Which-is-most-popular.mdx",title:"Delta, Hudi, Iceberg \u2014 Which is most popular?",description:"Redirecting... please wait!!",date:"2023-08-25T00:00:00.000Z",formattedDate:"August 25, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"delta lake",permalink:"/cn/blog/tags/delta-lake"},{label:"iceberg",permalink:"/cn/blog/tags/iceberg"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Kyle Weller"}],prevItem:{title:"Delta, Hudi, Iceberg \u2014 A Benchmark Compilation",permalink:"/cn/blog/2023/08/28/Delta-Hudi-Iceberg-A-Benchmark-Compilation"},nextItem:{title:"Exploring various storage types in Apache Hudi",permalink:"/cn/blog/2023/08/22/Exploring-various-storage-types-in-Apache-Hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@kywe665/delta-hudi-iceberg-which-is-most-popular-29ca56767199",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},86793:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Apache Hudi: From Zero To One (1/10)",excerpt:"Apache Hudi: From Zero To One (1/10)",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2023-08-28-Apache-Hudi-From-Zero-To-One.png",tags:["blog","apache hudi","cow","mor","datumagic","storage"]},s=void 0,l={permalink:"/cn/blog/2023/08/28/Apache-Hudi-From-Zero-To-One",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-08-28-Apache-Hudi-From-Zero-To-One.mdx",source:"@site/blog/2023-08-28-Apache-Hudi-From-Zero-To-One.mdx",title:"Apache Hudi: From Zero To One (1/10)",description:"Redirecting... please wait!!",date:"2023-08-28T00:00:00.000Z",formattedDate:"August 28, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"cow",permalink:"/cn/blog/tags/cow"},{label:"mor",permalink:"/cn/blog/tags/mor"},{label:"datumagic",permalink:"/cn/blog/tags/datumagic"},{label:"storage",permalink:"/cn/blog/tags/storage"}],readingTime:.045,truncated:!1,authors:[{name:"Shiyan Xu"}],prevItem:{title:"Incremental Queries with Apache Hudi and Apache Flink",permalink:"/cn/blog/2023/08/31/Incremental-Queries-with-Apache-Hudi-and-Apache-Flink"},nextItem:{title:"Delta, Hudi, Iceberg \u2014 A Benchmark Compilation",permalink:"/cn/blog/2023/08/28/Delta-Hudi-Iceberg-A-Benchmark-Compilation"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://blog.datumagic.com/p/apache-hudi-from-zero-to-one-110",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},31776:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Delta, Hudi, Iceberg \u2014 A Benchmark Compilation",excerpt:"Benchmark Compilation",author:"Kyle Weller",category:"blog",image:"/assets/images/blog/2023-08-28-Delta-Hudi-Iceberg-A-Benchmark-Compilation.png",tags:["performance","apache hudi","delta lake","iceberg","medium"]},s=void 0,l={permalink:"/cn/blog/2023/08/28/Delta-Hudi-Iceberg-A-Benchmark-Compilation",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-08-28-Delta-Hudi-Iceberg-A-Benchmark-Compilation.mdx",source:"@site/blog/2023-08-28-Delta-Hudi-Iceberg-A-Benchmark-Compilation.mdx",title:"Delta, Hudi, Iceberg \u2014 A Benchmark Compilation",description:"Redirecting... please wait!!",date:"2023-08-28T00:00:00.000Z",formattedDate:"August 28, 2023",tags:[{label:"performance",permalink:"/cn/blog/tags/performance"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"delta lake",permalink:"/cn/blog/tags/delta-lake"},{label:"iceberg",permalink:"/cn/blog/tags/iceberg"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Kyle Weller"}],prevItem:{title:"Apache Hudi: From Zero To One (1/10)",permalink:"/cn/blog/2023/08/28/Apache-Hudi-From-Zero-To-One"},nextItem:{title:"Delta, Hudi, Iceberg \u2014 Which is most popular?",permalink:"/cn/blog/2023/08/25/Delta-Hudi-Iceberg-Which-is-most-popular"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@kywe665/delta-hudi-iceberg-a-benchmark-compilation-a5630c69cffc",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},61105:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Incremental Queries with Apache Hudi and Apache Flink",excerpt:"Incremental Queries with Apache Hudi and Apache Flink",author:"nello",category:"blog",image:"/assets/images/blog/2023-08-31-Incremental-Queries-with-Apache-Hudi-and-Apache-Flink.png",tags:["incremental query","blog","apache flink","apache hudi","medium"]},s=void 0,l={permalink:"/cn/blog/2023/08/31/Incremental-Queries-with-Apache-Hudi-and-Apache-Flink",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-08-31-Incremental-Queries-with-Apache-Hudi-and-Apache-Flink.mdx",source:"@site/blog/2023-08-31-Incremental-Queries-with-Apache-Hudi-and-Apache-Flink.mdx",title:"Incremental Queries with Apache Hudi and Apache Flink",description:"Redirecting... please wait!!",date:"2023-08-31T00:00:00.000Z",formattedDate:"August 31, 2023",tags:[{label:"incremental query",permalink:"/cn/blog/tags/incremental-query"},{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache flink",permalink:"/cn/blog/tags/apache-flink"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"nello"}],prevItem:{title:"Lakehouse or Warehouse? Part 1 of 2",permalink:"/cn/blog/2023/09/06/Lakehouse-or-Warehouse-Part-1-of-2"},nextItem:{title:"Apache Hudi: From Zero To One (1/10)",permalink:"/cn/blog/2023/08/28/Apache-Hudi-From-Zero-To-One"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@acmilanellosw/incremental-queries-with-apache-hudi-and-apache-flink-5a90d088327",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},83342:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Apache Hudi: From Zero To One (2/10)",excerpt:"Dive into read operation flow and query types",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2023-09-06-Apache-Hudi-From-Zero-To-One-blog-2.png",tags:["blog","apache hudi","queries","reads","datumagic","apache spark","time travel query","incremental query","snapshot query","read optimized query"]},s=void 0,l={permalink:"/cn/blog/2023/09/06/Apache-Hudi-From-Zero-To-One-blog-2",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-09-06-Apache-Hudi-From-Zero-To-One-blog-2.mdx",source:"@site/blog/2023-09-06-Apache-Hudi-From-Zero-To-One-blog-2.mdx",title:"Apache Hudi: From Zero To One (2/10)",description:"Redirecting... please wait!!",date:"2023-09-06T00:00:00.000Z",formattedDate:"September 6, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"queries",permalink:"/cn/blog/tags/queries"},{label:"reads",permalink:"/cn/blog/tags/reads"},{label:"datumagic",permalink:"/cn/blog/tags/datumagic"},{label:"apache spark",permalink:"/cn/blog/tags/apache-spark"},{label:"time travel query",permalink:"/cn/blog/tags/time-travel-query"},{label:"incremental query",permalink:"/cn/blog/tags/incremental-query"},{label:"snapshot query",permalink:"/cn/blog/tags/snapshot-query"},{label:"read optimized query",permalink:"/cn/blog/tags/read-optimized-query"}],readingTime:.045,truncated:!1,authors:[{name:"Shiyan Xu"}],prevItem:{title:"Demystifying Copy-on-Write in Apache Hudi: Understanding Read and Write Operations",permalink:"/cn/blog/2023/09/10/Demystifying-Copy-on-Write-in-Apache-Hudi-Understanding-Read-and-Write-Operations"},nextItem:{title:"Lakehouse or Warehouse? Part 1 of 2",permalink:"/cn/blog/2023/09/06/Lakehouse-or-Warehouse-Part-1-of-2"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://blog.datumagic.com/p/apache-hudi-from-zero-to-one-210",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},30282:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Lakehouse or Warehouse? Part 1 of 2",excerpt:"Lakehouse or Warehouse? Part 1 of 2",author:"Floyd Smith",category:"blog",image:"/assets/images/blog/2023-09-06-Lakehouse-or-Warehouse-Part-1-of-2.png",tags:["blog","onehouse","data lakehouse","data warehouse","apache hudi"]},s=void 0,l={permalink:"/cn/blog/2023/09/06/Lakehouse-or-Warehouse-Part-1-of-2",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-09-06-Lakehouse-or-Warehouse-Part-1-of-2.mdx",source:"@site/blog/2023-09-06-Lakehouse-or-Warehouse-Part-1-of-2.mdx",title:"Lakehouse or Warehouse? Part 1 of 2",description:"Redirecting... please wait!!",date:"2023-09-06T00:00:00.000Z",formattedDate:"September 6, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"onehouse",permalink:"/cn/blog/tags/onehouse"},{label:"data lakehouse",permalink:"/cn/blog/tags/data-lakehouse"},{label:"data warehouse",permalink:"/cn/blog/tags/data-warehouse"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:.045,truncated:!1,authors:[{name:"Floyd Smith"}],prevItem:{title:"Apache Hudi: From Zero To One (2/10)",permalink:"/cn/blog/2023/09/06/Apache-Hudi-From-Zero-To-One-blog-2"},nextItem:{title:"Incremental Queries with Apache Hudi and Apache Flink",permalink:"/cn/blog/2023/08/31/Incremental-Queries-with-Apache-Hudi-and-Apache-Flink"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.onehouse.ai/blog/lakehouse-or-warehouse-part-1-of-2",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},31974:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Demystifying Copy-on-Write in Apache Hudi: Understanding Read and Write Operations",excerpt:"COW Overview",author:"Eswaramoorthy P",category:"blog",image:"/assets/images/blog/2023-09-10-Demystifying-Copy-on-Write-in-Apache-Hudi-Understanding-Read-and-Write-Operations.png",tags:["reads","medium","blog","apache hudi","writes","cow"]},s=void 0,l={permalink:"/cn/blog/2023/09/10/Demystifying-Copy-on-Write-in-Apache-Hudi-Understanding-Read-and-Write-Operations",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-09-10-Demystifying-Copy-on-Write-in-Apache-Hudi-Understanding-Read-and-Write-Operations.mdx",source:"@site/blog/2023-09-10-Demystifying-Copy-on-Write-in-Apache-Hudi-Understanding-Read-and-Write-Operations.mdx",title:"Demystifying Copy-on-Write in Apache Hudi: Understanding Read and Write Operations",description:"Redirecting... please wait!!",date:"2023-09-10T00:00:00.000Z",formattedDate:"September 10, 2023",tags:[{label:"reads",permalink:"/cn/blog/tags/reads"},{label:"medium",permalink:"/cn/blog/tags/medium"},{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"writes",permalink:"/cn/blog/tags/writes"},{label:"cow",permalink:"/cn/blog/tags/cow"}],readingTime:.045,truncated:!1,authors:[{name:"Eswaramoorthy P"}],prevItem:{title:"Lakehouse or Warehouse? Part 2 of 2",permalink:"/cn/blog/2023/09/12/Lakehouse-or-Warehouse-Part-2-of-2"},nextItem:{title:"Apache Hudi: From Zero To One (2/10)",permalink:"/cn/blog/2023/09/06/Apache-Hudi-From-Zero-To-One-blog-2"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/walmartglobaltech/demystifying-copy-on-write-in-apache-hudi-understanding-read-and-write-operations-3aa274017884",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},38494:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Lakehouse or Warehouse? Part 2 of 2",excerpt:"Lakehouse or Warehouse? Part 2 of 2",author:"Floyd Smith",category:"blog",image:"/assets/images/blog/2023-09-12-Lakehouse-or-Warehouse-Part-2-of-2.png",tags:["data warehouse","data lakehouse","apache hudi","onehouse","blog"]},s=void 0,l={permalink:"/cn/blog/2023/09/12/Lakehouse-or-Warehouse-Part-2-of-2",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-09-12-Lakehouse-or-Warehouse-Part-2-of-2.mdx",source:"@site/blog/2023-09-12-Lakehouse-or-Warehouse-Part-2-of-2.mdx",title:"Lakehouse or Warehouse? Part 2 of 2",description:"Redirecting... please wait!!",date:"2023-09-12T00:00:00.000Z",formattedDate:"September 12, 2023",tags:[{label:"data warehouse",permalink:"/cn/blog/tags/data-warehouse"},{label:"data lakehouse",permalink:"/cn/blog/tags/data-lakehouse"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"onehouse",permalink:"/cn/blog/tags/onehouse"},{label:"blog",permalink:"/cn/blog/tags/blog"}],readingTime:.045,truncated:!1,authors:[{name:"Floyd Smith"}],prevItem:{title:"Simplify operational data processing in data lakes using AWS Glue and Apache Hudi",permalink:"/cn/blog/2023/09/13/Simplify-operational-data-processing-in-data-lakes-using-AWS-Glue-and-Apache-Hudi"},nextItem:{title:"Demystifying Copy-on-Write in Apache Hudi: Understanding Read and Write Operations",permalink:"/cn/blog/2023/09/10/Demystifying-Copy-on-Write-in-Apache-Hudi-Understanding-Read-and-Write-Operations"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.onehouse.ai/blog/lakehouse-or-warehouse-part-2-of-2",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},7746:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Simplify operational data processing in data lakes using AWS Glue and Apache Hudi",excerpt:"Use AWS Glue and Apache Hudi for data processing",authors:[{name:"Srinivas Kandi"},{name:"Ravi Itha"}],category:"blog",image:"/assets/images/blog/2023-09-13-Simplify-operational-data-processing-in-data-lakes-using-AWS-Glue-and-Apache-Hudi.png",tags:["aws glue","amazon","how-to","data processing","apache hudi"]},s=void 0,l={permalink:"/cn/blog/2023/09/13/Simplify-operational-data-processing-in-data-lakes-using-AWS-Glue-and-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-09-13-Simplify-operational-data-processing-in-data-lakes-using-AWS-Glue-and-Apache-Hudi.mdx",source:"@site/blog/2023-09-13-Simplify-operational-data-processing-in-data-lakes-using-AWS-Glue-and-Apache-Hudi.mdx",title:"Simplify operational data processing in data lakes using AWS Glue and Apache Hudi",description:"Redirecting... please wait!!",date:"2023-09-13T00:00:00.000Z",formattedDate:"September 13, 2023",tags:[{label:"aws glue",permalink:"/cn/blog/tags/aws-glue"},{label:"amazon",permalink:"/cn/blog/tags/amazon"},{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"data processing",permalink:"/cn/blog/tags/data-processing"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:.045,truncated:!1,authors:[{name:"Srinivas Kandi"},{name:"Ravi Itha"}],prevItem:{title:"Apache Hudi: From Zero To One (3/10)",permalink:"/cn/blog/2023/09/15/Apache-Hudi-From-Zero-To-One-blog-3"},nextItem:{title:"Lakehouse or Warehouse? Part 2 of 2",permalink:"/cn/blog/2023/09/12/Lakehouse-or-Warehouse-Part-2-of-2"}},d={authorsImageUrls:[void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/simplify-operational-data-processing-in-data-lakes-using-aws-glue-and-apache-hudi/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},22811:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Apache Hudi: From Zero To One (3/10)",excerpt:"Understand write flows and operations",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2023-09-15-Apache-Hudi-From-Zero-To-One-blog-3.png",tags:["blog","apache hudi","queries","writes","datumagic","upserts","bulk insert","deletes","delete partition","inserts"]},s=void 0,l={permalink:"/cn/blog/2023/09/15/Apache-Hudi-From-Zero-To-One-blog-3",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-09-15-Apache-Hudi-From-Zero-To-One-blog-3.mdx",source:"@site/blog/2023-09-15-Apache-Hudi-From-Zero-To-One-blog-3.mdx",title:"Apache Hudi: From Zero To One (3/10)",description:"Redirecting... please wait!!",date:"2023-09-15T00:00:00.000Z",formattedDate:"September 15, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"queries",permalink:"/cn/blog/tags/queries"},{label:"writes",permalink:"/cn/blog/tags/writes"},{label:"datumagic",permalink:"/cn/blog/tags/datumagic"},{label:"upserts",permalink:"/cn/blog/tags/upserts"},{label:"bulk insert",permalink:"/cn/blog/tags/bulk-insert"},{label:"deletes",permalink:"/cn/blog/tags/deletes"},{label:"delete partition",permalink:"/cn/blog/tags/delete-partition"},{label:"inserts",permalink:"/cn/blog/tags/inserts"}],readingTime:.045,truncated:!1,authors:[{name:"Shiyan Xu"}],prevItem:{title:"A Beginner\u2019s Guide to Apache Hudi with PySpark \u2014 Part 1 of 2",permalink:"/cn/blog/2023/09/19/A-Beginners-Guide-to-Apache-Hudi-with-PySpark-Part-1-of-2"},nextItem:{title:"Simplify operational data processing in data lakes using AWS Glue and Apache Hudi",permalink:"/cn/blog/2023/09/13/Simplify-operational-data-processing-in-data-lakes-using-AWS-Glue-and-Apache-Hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://blog.datumagic.com/p/apache-hudi-from-zero-to-one-310",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},34565:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"A Beginner\u2019s Guide to Apache Hudi with PySpark \u2014 Part 1 of 2",author:"Sagar Lakshmipathy",category:"blog",image:"/assets/images/blog/2023-09-19-A-Beginners-Guide-to-Apache-Hudi-with-PySpark-Part-1-of-2.png",tags:["pyspark","apache hudi","how-to","medium"]},s=void 0,l={permalink:"/cn/blog/2023/09/19/A-Beginners-Guide-to-Apache-Hudi-with-PySpark-Part-1-of-2",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-09-19-A-Beginners-Guide-to-Apache-Hudi-with-PySpark-Part-1-of-2.mdx",source:"@site/blog/2023-09-19-A-Beginners-Guide-to-Apache-Hudi-with-PySpark-Part-1-of-2.mdx",title:"A Beginner\u2019s Guide to Apache Hudi with PySpark \u2014 Part 1 of 2",description:"Redirecting... please wait!!",date:"2023-09-19T00:00:00.000Z",formattedDate:"September 19, 2023",tags:[{label:"pyspark",permalink:"/cn/blog/tags/pyspark"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Sagar Lakshmipathy"}],prevItem:{title:"Exploring the Architecture of Apache Iceberg, Delta Lake, and Apache Hudi",permalink:"/cn/blog/2023/09/22/Exploring-the-Architecture-of-Apache-Iceberg-Delta-Lake-and-Apache-Hudi"},nextItem:{title:"Apache Hudi: From Zero To One (3/10)",permalink:"/cn/blog/2023/09/15/Apache-Hudi-From-Zero-To-One-blog-3"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@sagarlakshmipathy/a-beginners-guide-to-apache-hudi-with-pyspark-part-1-of-2-8a4e78f6ad2e",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},12114:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Exploring the Architecture of Apache Iceberg, Delta Lake, and Apache Hudi",excerpt:"Exploring the Architecture of Apache Iceberg, Delta Lake, and Apache Hudi",author:"Alex Merced",category:"blog",image:"/assets/images/blog/2023-09-22-Exploring-the-Architecture-of-Apache-Iceberg-Delta-Lake-and-Apache-Hudi.png",tags:["apache hudi","apache iceberg","blog","delta lake","dremio","architecture"]},s=void 0,l={permalink:"/cn/blog/2023/09/22/Exploring-the-Architecture-of-Apache-Iceberg-Delta-Lake-and-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-09-22-Exploring-the-Architecture-of-Apache-Iceberg-Delta-Lake-and-Apache-Hudi.mdx",source:"@site/blog/2023-09-22-Exploring-the-Architecture-of-Apache-Iceberg-Delta-Lake-and-Apache-Hudi.mdx",title:"Exploring the Architecture of Apache Iceberg, Delta Lake, and Apache Hudi",description:"Redirecting... please wait!!",date:"2023-09-22T00:00:00.000Z",formattedDate:"September 22, 2023",tags:[{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"apache iceberg",permalink:"/cn/blog/tags/apache-iceberg"},{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"delta lake",permalink:"/cn/blog/tags/delta-lake"},{label:"dremio",permalink:"/cn/blog/tags/dremio"},{label:"architecture",permalink:"/cn/blog/tags/architecture"}],readingTime:.045,truncated:!1,authors:[{name:"Alex Merced"}],prevItem:{title:"Apache Hudi: From Zero To One (4/10)",permalink:"/cn/blog/2023/09/27/Apache-Hudi-From-Zero-To-One-blog-4"},nextItem:{title:"A Beginner\u2019s Guide to Apache Hudi with PySpark \u2014 Part 1 of 2",permalink:"/cn/blog/2023/09/19/A-Beginners-Guide-to-Apache-Hudi-with-PySpark-Part-1-of-2"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.dremio.com/blog/exploring-the-architecture-of-apache-iceberg-delta-lake-and-apache-hudi/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},34811:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Apache Hudi: From Zero To One (4/10)",excerpt:"All about writer indexes",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2023-09-27-Apache-Hudi-From-Zero-To-One-blog-4.png",tags:["blog","apache hudi","indexing","bloom index","record index","datumagic","hbase index","bucket index"]},s=void 0,l={permalink:"/cn/blog/2023/09/27/Apache-Hudi-From-Zero-To-One-blog-4",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-09-27-Apache-Hudi-From-Zero-To-One-blog-4.mdx",source:"@site/blog/2023-09-27-Apache-Hudi-From-Zero-To-One-blog-4.mdx",title:"Apache Hudi: From Zero To One (4/10)",description:"Redirecting... please wait!!",date:"2023-09-27T00:00:00.000Z",formattedDate:"September 27, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"indexing",permalink:"/cn/blog/tags/indexing"},{label:"bloom index",permalink:"/cn/blog/tags/bloom-index"},{label:"record index",permalink:"/cn/blog/tags/record-index"},{label:"datumagic",permalink:"/cn/blog/tags/datumagic"},{label:"hbase index",permalink:"/cn/blog/tags/hbase-index"},{label:"bucket index",permalink:"/cn/blog/tags/bucket-index"}],readingTime:.045,truncated:!1,authors:[{name:"Shiyan Xu"}],prevItem:{title:"Apache Hudi: Copy on Write(CoW) Table",permalink:"/cn/blog/2023/10/06/Apache-Hudi-Copy-on-Write-CoW-Table"},nextItem:{title:"Exploring the Architecture of Apache Iceberg, Delta Lake, and Apache Hudi",permalink:"/cn/blog/2023/09/22/Exploring-the-Architecture-of-Apache-Iceberg-Delta-Lake-and-Apache-Hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://blog.datumagic.com/p/apache-hudi-from-zero-to-one-410",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},18512:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Apache Hudi: Copy on Write(CoW) Table",excerpt:"Apache Hudi: Copy on Write(CoW) Table",authors:[{name:"Ankur Ranjan"}],category:"blog",image:"/assets/images/blog/2023-10-06-Apache-Hudi-Copy-on-Write-CoW-Table.png",tags:["medium","blog","cow","deep dive","apache hudi"]},s=void 0,l={permalink:"/cn/blog/2023/10/06/Apache-Hudi-Copy-on-Write-CoW-Table",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-10-06-Apache-Hudi-Copy-on-Write-CoW-Table.mdx",source:"@site/blog/2023-10-06-Apache-Hudi-Copy-on-Write-CoW-Table.mdx",title:"Apache Hudi: Copy on Write(CoW) Table",description:"Redirecting... please wait!!",date:"2023-10-06T00:00:00.000Z",formattedDate:"October 6, 2023",tags:[{label:"medium",permalink:"/cn/blog/tags/medium"},{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"cow",permalink:"/cn/blog/tags/cow"},{label:"deep dive",permalink:"/cn/blog/tags/deep-dive"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:.045,truncated:!1,authors:[{name:"Ankur Ranjan"}],prevItem:{title:"StarRocks query performance with Apache Hudi and Onehouse",permalink:"/cn/blog/2023/10/11/starrocks-query-performance-with-apache-hudi-and-onehouse"},nextItem:{title:"Apache Hudi: From Zero To One (4/10)",permalink:"/cn/blog/2023/09/27/Apache-Hudi-From-Zero-To-One-blog-4"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@ranjanankur/apache-hudi-copy-on-write-cow-table-77fb2b849733",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},10762:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"StarRocks query performance with Apache Hudi and Onehouse",excerpt:"StarRocks Query Performance with Apache Hudi",authors:[{name:"Albert Wong"}],category:"blog",image:"/assets/images/blog/2023-10-11-starrocks-query-performance-with-apache-hudi-and-onehouse.png",tags:["starrocks","medium","blog","query performance","apache hudi"]},s=void 0,l={permalink:"/cn/blog/2023/10/11/starrocks-query-performance-with-apache-hudi-and-onehouse",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-10-11-starrocks-query-performance-with-apache-hudi-and-onehouse.mdx",source:"@site/blog/2023-10-11-starrocks-query-performance-with-apache-hudi-and-onehouse.mdx",title:"StarRocks query performance with Apache Hudi and Onehouse",description:"Redirecting... please wait!!",date:"2023-10-11T00:00:00.000Z",formattedDate:"October 11, 2023",tags:[{label:"starrocks",permalink:"/cn/blog/tags/starrocks"},{label:"medium",permalink:"/cn/blog/tags/medium"},{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"query performance",permalink:"/cn/blog/tags/query-performance"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:.045,truncated:!1,authors:[{name:"Albert Wong"}],prevItem:{title:"Get started with Apache Hudi using AWS Glue by implementing key design concepts \u2013 Part 1",permalink:"/cn/blog/2023/10/17/Get-started-with-Apache-Hudi-using-AWS-Glue-by-implementing-key-design-concepts-Part-1"},nextItem:{title:"Apache Hudi: Copy on Write(CoW) Table",permalink:"/cn/blog/2023/10/06/Apache-Hudi-Copy-on-Write-CoW-Table"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@atwong/starrocks-query-performance-with-apache-hudi-and-onehouse-04b859fff86c",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},62981:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Get started with Apache Hudi using AWS Glue by implementing key design concepts \u2013 Part 1",excerpt:"Get started with Apache Hudi using AWS Glue by implementing key design concepts \u2013 Part 1",authors:[{name:"Srinivas Kandi"},{name:"Ravi Itha"}],category:"blog",image:"/assets/images/blog/2023-10-17-Get-started-with-Apache-Hudi-using-AWS-Glue-by-implementing-key-design-concepts-Part-1.png",tags:["aws glue","apache hudi","how-to","amazon","design","aws glue","upserts","bulk insert","indexing"]},s=void 0,l={permalink:"/cn/blog/2023/10/17/Get-started-with-Apache-Hudi-using-AWS-Glue-by-implementing-key-design-concepts-Part-1",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-10-17-Get-started-with-Apache-Hudi-using-AWS-Glue-by-implementing-key-design-concepts-Part-1.mdx",source:"@site/blog/2023-10-17-Get-started-with-Apache-Hudi-using-AWS-Glue-by-implementing-key-design-concepts-Part-1.mdx",title:"Get started with Apache Hudi using AWS Glue by implementing key design concepts \u2013 Part 1",description:"Redirecting... please wait!!",date:"2023-10-17T00:00:00.000Z",formattedDate:"October 17, 2023",tags:[{label:"aws glue",permalink:"/cn/blog/tags/aws-glue"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"amazon",permalink:"/cn/blog/tags/amazon"},{label:"design",permalink:"/cn/blog/tags/design"},{label:"upserts",permalink:"/cn/blog/tags/upserts"},{label:"bulk insert",permalink:"/cn/blog/tags/bulk-insert"},{label:"indexing",permalink:"/cn/blog/tags/indexing"}],readingTime:.045,truncated:!1,authors:[{name:"Srinivas Kandi"},{name:"Ravi Itha"}],prevItem:{title:"Apache Hudi: From Zero To One (5/10)",permalink:"/cn/blog/2023/10/18/Apache-Hudi-From-Zero-To-One-blog-5"},nextItem:{title:"StarRocks query performance with Apache Hudi and Onehouse",permalink:"/cn/blog/2023/10/11/starrocks-query-performance-with-apache-hudi-and-onehouse"}},d={authorsImageUrls:[void 0,void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/part-1-get-started-with-apache-hudi-using-aws-glue-by-implementing-key-design-concepts/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},42166:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Apache Hudi: From Zero To One (5/10)",excerpt:"Introduce table services: compaction, cleaning, and indexing",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2023-10-18-Apache-Hudi-From-Zero-To-One-blog-5.png",tags:["blog","apache hudi","table services","compaction","cleaning","datumagic","indexing"]},s=void 0,l={permalink:"/cn/blog/2023/10/18/Apache-Hudi-From-Zero-To-One-blog-5",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-10-18-Apache-Hudi-From-Zero-To-One-blog-5.mdx",source:"@site/blog/2023-10-18-Apache-Hudi-From-Zero-To-One-blog-5.mdx",title:"Apache Hudi: From Zero To One (5/10)",description:"Redirecting... please wait!!",date:"2023-10-18T00:00:00.000Z",formattedDate:"October 18, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"table services",permalink:"/cn/blog/tags/table-services"},{label:"compaction",permalink:"/cn/blog/tags/compaction"},{label:"cleaning",permalink:"/cn/blog/tags/cleaning"},{label:"datumagic",permalink:"/cn/blog/tags/datumagic"},{label:"indexing",permalink:"/cn/blog/tags/indexing"}],readingTime:.045,truncated:!1,authors:[{name:"Shiyan Xu"}],prevItem:{title:"Load data incrementally from transactional data lakes to data warehouses",permalink:"/cn/blog/2023/10/19/load-data-incrementally-from-transactional-data-lakes-to-data-warehouses"},nextItem:{title:"Get started with Apache Hudi using AWS Glue by implementing key design concepts \u2013 Part 1",permalink:"/cn/blog/2023/10/17/Get-started-with-Apache-Hudi-using-AWS-Glue-by-implementing-key-design-concepts-Part-1"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://blog.datumagic.com/p/apache-hudi-from-zero-to-one-510",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},10116:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Load data incrementally from transactional data lakes to data warehouses",excerpt:"Load data incrementally from Apache Hudi table to Amazon Redshift using a Hudi incremental query",author:"Noritaka Sekiyama",category:"blog",image:"/assets/images/blog/2023-10-19-load-data-incrementally-from-transactional-data-lakes-to-data-warehouses.png",tags:["incremental updates","amazon","how to","querying","aws","amazon redshift","apache hudi"]},s=void 0,l={permalink:"/cn/blog/2023/10/19/load-data-incrementally-from-transactional-data-lakes-to-data-warehouses",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-10-19-load-data-incrementally-from-transactional-data-lakes-to-data-warehouses.mdx",source:"@site/blog/2023-10-19-load-data-incrementally-from-transactional-data-lakes-to-data-warehouses.mdx",title:"Load data incrementally from transactional data lakes to data warehouses",description:"Redirecting... please wait!!",date:"2023-10-19T00:00:00.000Z",formattedDate:"October 19, 2023",tags:[{label:"incremental updates",permalink:"/cn/blog/tags/incremental-updates"},{label:"amazon",permalink:"/cn/blog/tags/amazon"},{label:"how to",permalink:"/cn/blog/tags/how-to"},{label:"querying",permalink:"/cn/blog/tags/querying"},{label:"aws",permalink:"/cn/blog/tags/aws"},{label:"amazon redshift",permalink:"/cn/blog/tags/amazon-redshift"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:.045,truncated:!1,authors:[{name:"Noritaka Sekiyama"}],prevItem:{title:"It's Time for the Universal Data Lakehouse",permalink:"/cn/blog/2023/10/20/Its-Time-for-the-Universal-Data-Lakehouse"},nextItem:{title:"Apache Hudi: From Zero To One (5/10)",permalink:"/cn/blog/2023/10/18/Apache-Hudi-From-Zero-To-One-blog-5"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/load-data-incrementally-from-transactional-data-lakes-to-data-warehouses/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},47237:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"It's Time for the Universal Data Lakehouse",excerpt:"Universal Lakehouse",author:"Vinoth Chandar",category:"blog",image:"/assets/images/blog/2023-10-20-Its-Time-for-the-Universal-Data-Lakehouse.png",tags:["data lakehouse","onehouse","blog","apache hudi","interoperability"]},s=void 0,l={permalink:"/cn/blog/2023/10/20/Its-Time-for-the-Universal-Data-Lakehouse",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-10-20-Its-Time-for-the-Universal-Data-Lakehouse.mdx",source:"@site/blog/2023-10-20-Its-Time-for-the-Universal-Data-Lakehouse.mdx",title:"It's Time for the Universal Data Lakehouse",description:"Redirecting... please wait!! s",date:"2023-10-20T00:00:00.000Z",formattedDate:"October 20, 2023",tags:[{label:"data lakehouse",permalink:"/cn/blog/tags/data-lakehouse"},{label:"onehouse",permalink:"/cn/blog/tags/onehouse"},{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"interoperability",permalink:"/cn/blog/tags/interoperability"}],readingTime:.045,truncated:!1,authors:[{name:"Vinoth Chandar"}],prevItem:{title:"Tipico Facilitates Faster Data Access with a Modern Data Strategy on AWS",permalink:"/cn/blog/2023/10/22/Tipico-Facilitates-Faster-Data-Access-with-a-Modern-Data-Strategy-on-AWS"},nextItem:{title:"Load data incrementally from transactional data lakes to data warehouses",permalink:"/cn/blog/2023/10/19/load-data-incrementally-from-transactional-data-lakes-to-data-warehouses"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.onehouse.ai/blog/its-time-for-the-universal-data-lakehouse",mdxType:"Redirect"},"Redirecting... please wait!! "),"s")}u.isMDXComponent=!0},46040:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Tipico Facilitates Faster Data Access with a Modern Data Strategy on AWS",category:"blog",image:"/assets/images/blog/2023-10-22-Tipico-Facilitates-Faster-Data-Access-with-a-Modern-Data-Strategy-on-AWS.png",tags:["case study","amazon","apache hudi"]},s=void 0,l={permalink:"/cn/blog/2023/10/22/Tipico-Facilitates-Faster-Data-Access-with-a-Modern-Data-Strategy-on-AWS",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-10-22-Tipico-Facilitates-Faster-Data-Access-with-a-Modern-Data-Strategy-on-AWS.mdx",source:"@site/blog/2023-10-22-Tipico-Facilitates-Faster-Data-Access-with-a-Modern-Data-Strategy-on-AWS.mdx",title:"Tipico Facilitates Faster Data Access with a Modern Data Strategy on AWS",description:"Redirecting... please wait!! s",date:"2023-10-22T00:00:00.000Z",formattedDate:"October 22, 2023",tags:[{label:"case study",permalink:"/cn/blog/tags/case-study"},{label:"amazon",permalink:"/cn/blog/tags/amazon"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"}],readingTime:.045,truncated:!1,authors:[],prevItem:{title:"UPSERT Performance Evaluation of Hudi 0.14 and Spark 3.4.1: Record Level Index vs. Global Bloom & Global Simple Indexes",permalink:"/cn/blog/2023/10/29/UPSERT-Performance-Evaluation-of-Hudi-0-14-and-Spark-3-4-1-Record-Level-Index-Global-Bloom-Global-Simple-Indexes"},nextItem:{title:"It's Time for the Universal Data Lakehouse",permalink:"/cn/blog/2023/10/20/Its-Time-for-the-Universal-Data-Lakehouse"}},d={authorsImageUrls:[]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/solutions/case-studies/tipico-case-study/",mdxType:"Redirect"},"Redirecting... please wait!! "),"s")}u.isMDXComponent=!0},91989:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"UPSERT Performance Evaluation of Hudi 0.14 and Spark 3.4.1: Record Level Index vs. Global Bloom & Global Simple Indexes",excerpt:"Record Level Index Performance",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2023-10-29-UPSERT-Performance-Evaluation-of-Hudi-0-14-and-Spark-3-4-1-Record-Level-Index-Global-Bloom-Global-Simple-Indexes.png",tags:["linkedin","apache hudi","querying","indexing","performance"]},s=void 0,l={permalink:"/cn/blog/2023/10/29/UPSERT-Performance-Evaluation-of-Hudi-0-14-and-Spark-3-4-1-Record-Level-Index-Global-Bloom-Global-Simple-Indexes",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-10-29-UPSERT-Performance-Evaluation-of-Hudi-0-14-and-Spark-3-4-1-Record-Level-Index-Global-Bloom-Global-Simple-Indexes.mdx",source:"@site/blog/2023-10-29-UPSERT-Performance-Evaluation-of-Hudi-0-14-and-Spark-3-4-1-Record-Level-Index-Global-Bloom-Global-Simple-Indexes.mdx",title:"UPSERT Performance Evaluation of Hudi 0.14 and Spark 3.4.1: Record Level Index vs. Global Bloom & Global Simple Indexes",description:"Redirecting... please wait!!",date:"2023-10-29T00:00:00.000Z",formattedDate:"October 29, 2023",tags:[{label:"linkedin",permalink:"/cn/blog/tags/linkedin"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"querying",permalink:"/cn/blog/tags/querying"},{label:"indexing",permalink:"/cn/blog/tags/indexing"},{label:"performance",permalink:"/cn/blog/tags/performance"}],readingTime:.045,truncated:!1,authors:[{name:"Soumil Shah"}],prevItem:{title:"Record Level Index: Hudi's blazing fast indexing for large-scale datasets",permalink:"/cn/blog/2023/11/01/record-level-index"},nextItem:{title:"Tipico Facilitates Faster Data Access with a Modern Data Strategy on AWS",permalink:"/cn/blog/2023/10/22/Tipico-Facilitates-Faster-Data-Access-with-a-Modern-Data-Strategy-on-AWS"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.linkedin.com/pulse/upsert-performance-evaluation-hudi-014-spark-341-record-soumil-shah-oupre/?utm_source=share&utm_medium=member_ios&utm_campaign=share_via",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},92329:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Record Level Index: Hudi's blazing fast indexing for large-scale datasets",excerpt:"Announcing the Record Level Index in Apache Hudi",author:"Shiyan Xu and Sivabalan Narayanan",category:"blog",image:"/assets/images/blog/record-level-index/03.RLI_bulkinsert.png",tags:["design","indexing","metadata","apache hudi","blog"]},r=void 0,s={permalink:"/cn/blog/2023/11/01/record-level-index",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-11-01-record-level-index.md",source:"@site/blog/2023-11-01-record-level-index.md",title:"Record Level Index: Hudi's blazing fast indexing for large-scale datasets",description:"Introduction",date:"2023-11-01T00:00:00.000Z",formattedDate:"November 1, 2023",tags:[{label:"design",permalink:"/cn/blog/tags/design"},{label:"indexing",permalink:"/cn/blog/tags/indexing"},{label:"metadata",permalink:"/cn/blog/tags/metadata"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"blog",permalink:"/cn/blog/tags/blog"}],readingTime:11.355,truncated:!1,authors:[{name:"Shiyan Xu and Sivabalan Narayanan"}],prevItem:{title:"Apache Hudi: From Zero To One (6/10)",permalink:"/cn/blog/2023/11/13/Apache-Hudi-From-Zero-To-One-blog-6"},nextItem:{title:"UPSERT Performance Evaluation of Hudi 0.14 and Spark 3.4.1: Record Level Index vs. Global Bloom & Global Simple Indexes",permalink:"/cn/blog/2023/10/29/UPSERT-Performance-Evaluation-of-Hudi-0-14-and-Spark-3-4-1-Record-Level-Index-Global-Bloom-Global-Simple-Indexes"}},l={authorsImageUrls:[void 0]},d=[{value:"Introduction",id:"introduction",children:[],level:2},{value:"Metadata table",id:"metadata-table",children:[],level:2},{value:"Record Level Index",id:"record-level-index",children:[{value:"Initialization",id:"initialization",children:[],level:3},{value:"Updating RLI upon data table writes",id:"updating-rli-upon-data-table-writes",children:[],level:3},{value:"Writer Indexing",id:"writer-indexing",children:[],level:3},{value:"Read Flow",id:"read-flow",children:[],level:3},{value:"Storage",id:"storage",children:[],level:3},{value:"Performance",id:"performance",children:[{value:"Write latency",id:"write-latency",children:[],level:4},{value:"Index look-up latency",id:"index-look-up-latency",children:[],level:4},{value:"Data shuffling",id:"data-shuffling",children:[],level:4},{value:"Query latency",id:"query-latency",children:[],level:4}],level:3},{value:"When to Use",id:"when-to-use",children:[],level:3}],level:2},{value:"Future Work",id:"future-work",children:[],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...t}=e;return(0,n.yg)(p,(0,i.A)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("h2",{id:"introduction"},"Introduction"),(0,n.yg)("p",null,"Index is a critical component that facilitates quick updates and deletes for Hudi writers, and it plays a pivotal\nrole in boosting query executions as well. Hudi provides several index types, including the Bloom and Simple indexes with global\nvariations, the HBase Index that leverages a HBase server, the hash-based Bucket index, and the multi-modal index\nrealized through the metadata table. The choice of an index depends on factors such as table sizes, partition data distributions,\nor traffic patterns, where a specific index may be more suitable for simpler operation or better performance",(0,n.yg)("sup",{parentName:"p",id:"fnref-1"},(0,n.yg)("a",{parentName:"sup",href:"#fn-1",className:"footnote-ref"},"1")),".\nUsers often face trade-offs when selecting index types for different tables, since there hasn't been\na generally performant index capable of facilitating both writes and reads with minimal operational overhead."),(0,n.yg)("p",null,"Starting from ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/releases/release-0.14.0"},"Hudi 0.14.0"),", we are thrilled to announce a\ngeneral purpose index for Apache Hudi - the Record Level Index (RLI). This innovation not only dramatically boosts\nwrite efficiency but also improves read efficiency for relevant queries. Integrated seamlessly within the table storage layer,\nRLI can easily work without any additional operational efforts."),(0,n.yg)("p",null,"In the subsequent sections of this blog, we will give a brief introduction to Hudi's metadata table, a pre-requisite for discussing RLI.\nFollowing that, we will delve into the design and workflows of RLI, and then show performance analysis and index type comparisons. The blog\nwill conclude with insights into future work for RLI."),(0,n.yg)("h2",{id:"metadata-table"},"Metadata table"),(0,n.yg)("p",null,"A ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/metadata"},"Hudi metadata table")," is a Merge-on-Read (MoR) table within the ",(0,n.yg)("inlineCode",{parentName:"p"},".hoodie/metadata/")," directory. It contains various\nmetadata pertaining to records, seamlessly integrated into both the writer and reader paths to improve indexing efficiency.\nThe metadata is segregated into four partitions: ",(0,n.yg)("inlineCode",{parentName:"p"},"files"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"column stats"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"bloom filters"),", and ",(0,n.yg)("inlineCode",{parentName:"p"},"record level index"),"."),(0,n.yg)("img",{src:"/assets/images/blog/record-level-index/01.metadatatable_layout.png",alt:"Hudi metadata table layout",width:"800",align:"middle"}),(0,n.yg)("p",null,"The metadata table is updated synchronously with each commit action on the Timeline, in other words, the commits to the\nmetadata table are part of the transactions to the Hudi data table. With four partitions containing different types of\nmetadata, this layout serves the purpose of a multi-modal index:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"files")," partition keeps track of the Hudi data table\u2019s partitions, and data files of each partition"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"column stats")," partition records statistics about each column of the data table"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"bloom filter")," partition stores serialized bloom filters for base files"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"record level index")," partition contains mappings of individual record key and the corresponding file group id")),(0,n.yg)("p",null,"Users can activate the metadata table by setting ",(0,n.yg)("inlineCode",{parentName:"p"},"hoodie.metadata.enable=true"),". Once activated, the ",(0,n.yg)("inlineCode",{parentName:"p"},"files")," partition\nwill always be enabled. Other partitions can be enabled and configured individually to harness additional indexing\ncapabilities."),(0,n.yg)("h2",{id:"record-level-index"},"Record Level Index"),(0,n.yg)("p",null,"Starting from release 0.14.0, the Record Level Index (RLI) can be activated by setting ",(0,n.yg)("inlineCode",{parentName:"p"},"hoodie.metadata.record.index.enable=true"),"\nand ",(0,n.yg)("inlineCode",{parentName:"p"},"hoodie.index.type=RECORD_INDEX"),'. The core concept behind RLI is the ability to determine the location of records, thus\nreducing the number of files that need to be scanned to extract the desired data. This process is usually referred to as "index look-up".\nHudi employs a primary-key model, requiring each record to be associated with a key\nto satisfy the uniqueness constraint. Consequently, we can establish one-to-one mappings between record keys and file groups,\nprecisely the data we intend to store within the ',(0,n.yg)("inlineCode",{parentName:"p"},"record level index")," partition."),(0,n.yg)("p",null,"Performance is paramount when it comes to indexes. The metadata table, which includes the RLI partition, chooses ",(0,n.yg)("a",{parentName:"p",href:"https://hbase.apache.org/book.html#_hfile_format_2"},"HFile"),(0,n.yg)("sup",{parentName:"p",id:"fnref-2"},(0,n.yg)("a",{parentName:"sup",href:"#fn-2",className:"footnote-ref"},"2")),",\nHBase\u2019s file format that utilizes B+ tree-like structures for fast look-up, as the file format. Real-world benchmarking\nhas shown that an HFile containing 1 million RLI mappings can look up a batch of 100k records in just 600 ms.\nWe will cover the performance topic in a later section with detailed analysis."),(0,n.yg)("h3",{id:"initialization"},"Initialization"),(0,n.yg)("p",null,"Initializing the RLI partition for an existing Hudi table can be a laborious and time-consuming task, contingent on the number\nof records. Just like with a typical database, building indexes takes time, but the investment ultimately pays off by speeding up\nnumerous queries in the future."),(0,n.yg)("img",{src:"/assets/images/blog/record-level-index/02.RLI_init_flow.png",alt:"RLI init flow",width:"800",align:"middle"}),(0,n.yg)("p",null,"The diagram above shows the high-level steps of RLI initialization. Since these jobs are all parallelizable, users can\nscale the cluster and configure relevant parallelism settings (e.g., ",(0,n.yg)("inlineCode",{parentName:"p"},"hoodie.metadata.max.init.parallelism"),") accordingly\nto meet their time requirement."),(0,n.yg)("p",null,'Focusing on the final step, "Bulk insert to RLI partition," the metadata table writer employs a hash function to\npartition the RLI records, ensuring that the number of resulting file groups aligns with the number of partitions.\nThis guarantees consistent record key look-ups.'),(0,n.yg)("img",{src:"/assets/images/blog/record-level-index/03.RLI_bulkinsert.png",alt:"RLI bulkinsert",width:"800",align:"middle"}),(0,n.yg)("p",null,"It\u2019s important to note that the current implementation fixes the number of file groups in the RLI partition once it\u2019s initialized.\nTherefore, users should lean towards over-provisioning the file groups and adjust these configurations accordingly."),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"hoodie.metadata.record.index.max.filegroup.count\nhoodie.metadata.record.index.min.filegroup.count\nhoodie.metadata.record.index.max.filegroup.size\nhoodie.metadata.record.index.growth.factor\n")),(0,n.yg)("p",null,"In future development iterations, RLI should be able to overcome this limitation by dynamically rebalancing file groups to\naccommodate the ever-increasing number of records."),(0,n.yg)("h3",{id:"updating-rli-upon-data-table-writes"},"Updating RLI upon data table writes"),(0,n.yg)("p",null,"During regular writes, the RLI partition will be updated as part of the transactions. Metadata records will be generated\nusing the incoming record keys with their corresponding location info. Given that the RLI partition contains the exact\nmappings of record keys and locations, upserts to the data table will result in upsertion of the corresponding keys to the\nRLI partition, The hash function employed will guarantee that identical keys are routed to the same file group."),(0,n.yg)("h3",{id:"writer-indexing"},"Writer Indexing"),(0,n.yg)("p",null,"Being part of the write flow, RLI follows the high-level indexing flow, similar to any other global index: for a given\nset of records, it tags each record with location information if the index finds them present in any existing file group.\nThe key distinction lies in the source of truth for the existence test\u2014the RLI partition. The diagram below illustrates\nthe tagging flow with detailed steps."),(0,n.yg)("img",{src:"/assets/images/blog/record-level-index/04.RLI_tagging.png",alt:"RLI tagging",width:"800",align:"middle"}),(0,n.yg)("p",null,"The tagged records will be passed to Hudi write handles and will undergo write operations to their respective file groups.\nThe indexing process is a critical step in applying updates to the table, as its efficiency directly influences the write\nlatency. In a later section, we will demonstrate the Record Level Index performance using benchmarking results."),(0,n.yg)("h3",{id:"read-flow"},"Read Flow"),(0,n.yg)("p",null,"The Record Level Index is also integrated on the query side",(0,n.yg)("sup",{parentName:"p",id:"fnref-3"},(0,n.yg)("a",{parentName:"sup",href:"#fn-3",className:"footnote-ref"},"3")),". In queries that involve equality check (e.g., EqualTo or IN)\nagainst the record key column, Hudi\u2019s file index implementation optimizes the file pruning process. This optimization is\nachieved by leveraging RLI to precisely locate the file groups that need to be read for completing the queries."),(0,n.yg)("h3",{id:"storage"},"Storage"),(0,n.yg)("p",null,"Storage efficiency is another vital aspect of the design. Each RLI mapping entry must include some necessary information\nto precisely locate files, such as record key, partition path, file group id, etc. To optimize the storage, RLI adopts\nsome compression techniques such as encoding file group id (in the form of UUID) into 2 Longs to represent the high and\nlow bits. Using Gzip compression and a 4MB block size, an individual RLI record averages only 48 bytes in size. To\nillustrate this more practically, let\u2019s assume we have a table of 100TB data with about 1 billion records (average record size = 100Kb).\nThe storage space required by the RLI partition will be approximately 48 Gb, which is less than 0.05% of the total data size.\nSince RLI contains the same number of entries as the data table, storage optimization is crucial to make RLI practical,\nespecially for tables of petabyte size and beyond."),(0,n.yg)("p",null,"RLI exploits the low cost of storage to enable the rapid look-up process similar to the HBase index, while avoiding the\noperational overhead of running an extra server. In the next section, we will review some benchmarking results to demonstrate\nits performance advantages."),(0,n.yg)("h3",{id:"performance"},"Performance"),(0,n.yg)("p",null,"We conducted a comprehensive benchmarking analysis of the Record Level Index evaluating aspects such write latency,\nindex look-up latency, and data shuffling in comparison to existing indexing mechanisms in Hudi. In addition to the\nbenchmarks for write operations, we will also showcase the reduction in query latencies for point look-ups. Hudi 0.14.0\nand Spark 3.2.1 were used throughout the experiments."),(0,n.yg)("p",null,"In comparison to the Global Simple Index (GSI) in Hudi, Record Level Index (RLI) is crafted for significant performance\nadvantages stemming from a greatly reduced scan space and minimized data shuffling. GSI conducts join operations between\nincoming records and existing data across all partitions of the data table, resulting in substantial data shuffling and\ncomputational overhead to pinpoint the records. On the other hand, RLI efficiently extracts location info through a\nhash function, leading to a considerably smaller amount of data shuffling by only loading the file groups of interest\nfrom the metadata table."),(0,n.yg)("h4",{id:"write-latency"},"Write latency"),(0,n.yg)("p",null,"In the first set of experiments, we established two pipelines: one configured using GSI, and the other configured with RLI.\nEach pipeline was executed on an EMR cluster of 10 m5.4xlarge core instances, and was set to ingest batches of 200Mb data\ninto a 1TB dataset of 2 billion records. The RLI partition was configured with 1000 file groups. For N batches of ingestion,\n",(0,n.yg)("strong",{parentName:"p"},"the average write latency using RLI showed a remarkable 72% improvement over GSI"),"."),(0,n.yg)("img",{src:"/assets/images/blog/record-level-index/write-latency.png",alt:"metadata-rli",width:"600",align:"middle"}),(0,n.yg)("p",null,"Note: Between Global Simple Index and Global Bloom Index in Hudi, the former yielded better results due to the randomness\nof record keys. Therefore, we omitted the presentation of the Global Bloom Index in the chart."),(0,n.yg)("h4",{id:"index-look-up-latency"},"Index look-up latency"),(0,n.yg)("p",null,"We also isolated the index look-up step using HoodieReadClient to accurately gauge indexing efficiency. Through\nexperiments involving the look-up of 400,000 records (0.02%) in a 1TB dataset of 2 billion records, ",(0,n.yg)("strong",{parentName:"p"},"RLI showcased a\n72% improvement over GSI, consistent with the end-to-end write latency results"),"."),(0,n.yg)("img",{src:"/assets/images/blog/record-level-index/index-latency.png",alt:"index-latency",width:"600",align:"middle"}),(0,n.yg)("h4",{id:"data-shuffling"},"Data shuffling"),(0,n.yg)("p",null,"In the index look-up experiments, we observed that around 85Gb of data was shuffled for GSI, whereas only 700Mb was shuffled\nfor RLI. ",(0,n.yg)("strong",{parentName:"p"},"This reflects an impressive 92% reduction in data shuffling when using RLI compared to GSI"),"."),(0,n.yg)("h4",{id:"query-latency"},"Query latency"),(0,n.yg)("p",null,"The Record Level Index will greatly boost Spark queries with \u201cEqualTo\u201d and \u201cIN\u201d predicates on record key columns.\nWe created a 400GB Hudi table comprising 20,000 file groups. When we executed a query predicated on a single record key,\nwe observed a significant improvement in query time. ",(0,n.yg)("strong",{parentName:"p"},"With RLI enabled, the query time decreased from 977 seconds to just\n12 seconds, representing an impressive 98% reduction in latency"),(0,n.yg)("sup",{parentName:"p",id:"fnref-4"},(0,n.yg)("a",{parentName:"sup",href:"#fn-4",className:"footnote-ref"},"4")),"."),(0,n.yg)("h3",{id:"when-to-use"},"When to Use"),(0,n.yg)("p",null,"RLI demonstrates outstanding performance in general, elevating update and delete efficiency to a new level and\nfast-tracking reads when executing key-matching queries. Enabling RLI is also as simple as setting some configuration flags.\nBelow, we have summarized a comparison table highlighting these important characteristics of RLI in contrast to other common Hudi index types."),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null}),(0,n.yg)("th",{parentName:"tr",align:null},"Record Level Index"),(0,n.yg)("th",{parentName:"tr",align:null},"Global Simple Index"),(0,n.yg)("th",{parentName:"tr",align:null},"Global Bloom Index"),(0,n.yg)("th",{parentName:"tr",align:null},"HBase Index"),(0,n.yg)("th",{parentName:"tr",align:null},"Bucket Index"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Performant look-up in general"),(0,n.yg)("td",{parentName:"tr",align:null},"Yes"),(0,n.yg)("td",{parentName:"tr",align:null},"No"),(0,n.yg)("td",{parentName:"tr",align:null},"No"),(0,n.yg)("td",{parentName:"tr",align:null},"Yes, with possible throttling issues"),(0,n.yg)("td",{parentName:"tr",align:null},"Yes")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Boost both writes and reads"),(0,n.yg)("td",{parentName:"tr",align:null},"Yes"),(0,n.yg)("td",{parentName:"tr",align:null},"No, write-only"),(0,n.yg)("td",{parentName:"tr",align:null},"No, write-only"),(0,n.yg)("td",{parentName:"tr",align:null},"No, write-only"),(0,n.yg)("td",{parentName:"tr",align:null},"No, write-only")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Easy to enable"),(0,n.yg)("td",{parentName:"tr",align:null},"Yes"),(0,n.yg)("td",{parentName:"tr",align:null},"Yes"),(0,n.yg)("td",{parentName:"tr",align:null},"Yes"),(0,n.yg)("td",{parentName:"tr",align:null},"No, require HBase server"),(0,n.yg)("td",{parentName:"tr",align:null},"Yes")))),(0,n.yg)("p",null,"Many real-world applications will significantly benefit from using RLI. A common example is fulfilling the GDPR requirements.\nTypically, when users make requests, a set of IDs will be provided to identify the to-be-deleted records,\nwhich will either be updated (columns being nullified) or permanently removed.\nBy enabling RLI, offline jobs performing such changes will become notably more efficient, resulting in cost savings.\nOn the read side, analysts or engineers collecting historical events through certain tracing IDs will also\nexperience blazing fast responses from the key-matching queries."),(0,n.yg)("p",null,"While RLI holds the above-mentioned advantages over all other index types, it is important to consider certain\naspects when using it. Similar to any other global index, RLI requires record-key uniqueness across all partitions in a table.\nAs RLI keeps track of all record keys and locations, the initialization process may take time for large tables.\nIn scenarios with extremely skewed large workloads, RLI might not achieve the desired performance due to limitations in the current design."),(0,n.yg)("h2",{id:"future-work"},"Future Work"),(0,n.yg)("p",null,'In this initial version of the Record Level Index, certain limitations are acknowledged. As mentioned in the\n"Initialization" section, the number of file groups must be predetermined during the creation of the RLI partition.\nHudi does use some heuristics and a growth factor for an existing table, but for a new table, it is recommended to\nset appropriate file group configs for RLI. As the data volume increases, the RLI partition requires re-bootstrapping\nwhen additional file groups are needed for scaling out. To address the need for rebalancing, a consistent hashing\ntechnique could be employed.'),(0,n.yg)("p",null,"Another valuable enhancement would involve supporting the indexing of secondary columns alongside the record key\nfields, thus catering to a broader range of queries. On the reader side, there is a plan to integrate more query\nengines, such as Presto and Trino, with the Record Level Index to fully leverage the performance benefits offered\nby Hudi metadata tables."),(0,n.yg)("hr",null),(0,n.yg)("p",null,(0,n.yg)("sup",{parentName:"p",id:"fnref-1"},(0,n.yg)("a",{parentName:"sup",href:"#fn-1",className:"footnote-ref"},"1"))," ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/blog/2020/11/11/hudi-indexing-mechanisms/"},"This blog")," well-explained some best practices regarding index selection and configuration."),(0,n.yg)("p",null,(0,n.yg)("sup",{parentName:"p",id:"fnref-2"},(0,n.yg)("a",{parentName:"sup",href:"#fn-2",className:"footnote-ref"},"2"))," Other formats like Parquet can also be supported in the future."),(0,n.yg)("p",null,(0,n.yg)("sup",{parentName:"p",id:"fnref-3"},(0,n.yg)("a",{parentName:"sup",href:"#fn-3",className:"footnote-ref"},"3"))," As of now, query engine integration is only available for Spark, with plans to support additional engines in the future."),(0,n.yg)("p",null,(0,n.yg)("sup",{parentName:"p",id:"fnref-4"},(0,n.yg)("a",{parentName:"sup",href:"#fn-4",className:"footnote-ref"},"4"))," The query improvement is specific to record-key-matching queries and does not reflect a general reduction in latency by enabling RLI. In the case of the single record-key query, 99.995% of file groups (19999 out of 20000) were pruned during query execution."))}g.isMDXComponent=!0},42361:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Apache Hudi: From Zero To One (6/10)",excerpt:"Demystify clustering and space-filling curves",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2023-11-13-Apache-Hudi-From-Zero-To-One-blog-6.png",tags:["blog","apache hudi","table services","clustering","space filling curves","datumagic"]},s=void 0,l={permalink:"/cn/blog/2023/11/13/Apache-Hudi-From-Zero-To-One-blog-6",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-11-13-Apache-Hudi-From-Zero-To-One-blog-6.mdx",source:"@site/blog/2023-11-13-Apache-Hudi-From-Zero-To-One-blog-6.mdx",title:"Apache Hudi: From Zero To One (6/10)",description:"Redirecting... please wait!!",date:"2023-11-13T00:00:00.000Z",formattedDate:"November 13, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"table services",permalink:"/cn/blog/tags/table-services"},{label:"clustering",permalink:"/cn/blog/tags/clustering"},{label:"space filling curves",permalink:"/cn/blog/tags/space-filling-curves"},{label:"datumagic",permalink:"/cn/blog/tags/datumagic"}],readingTime:.045,truncated:!1,authors:[{name:"Shiyan Xu"}],prevItem:{title:"Hudi Streamer (Delta Streamer) Hands-On Guide: Local Ingestion from Parquet Source",permalink:"/cn/blog/2023/11/19/Hudi-Streamer-DeltaStreamer-Hands-On-Guide-Local-Ingestion-from-Parquet-Source"},nextItem:{title:"Record Level Index: Hudi's blazing fast indexing for large-scale datasets",permalink:"/cn/blog/2023/11/01/record-level-index"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://blog.datumagic.com/p/apache-hudi-from-zero-to-one-610",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},96470:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Hudi Streamer (Delta Streamer) Hands-On Guide: Local Ingestion from Parquet Source",excerpt:"Hudi Streamer (Delta Streamer) Hands-On Guide: Local Ingestion from Parquet Source",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2023-11-19-Hudi-Streamer-DeltaStreamer-Hands-On-Guide-Local-Ingestion-from-Parquet-Source.png",tags:["apache hudi","hudi streamer","how-to","apache parquet","linkedin"]},s=void 0,l={permalink:"/cn/blog/2023/11/19/Hudi-Streamer-DeltaStreamer-Hands-On-Guide-Local-Ingestion-from-Parquet-Source",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-11-19-Hudi-Streamer-DeltaStreamer-Hands-On-Guide-Local-Ingestion-from-Parquet-Source.mdx",source:"@site/blog/2023-11-19-Hudi-Streamer-DeltaStreamer-Hands-On-Guide-Local-Ingestion-from-Parquet-Source.mdx",title:"Hudi Streamer (Delta Streamer) Hands-On Guide: Local Ingestion from Parquet Source",description:"Redirecting... please wait!!",date:"2023-11-19T00:00:00.000Z",formattedDate:"November 19, 2023",tags:[{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"hudi streamer",permalink:"/cn/blog/tags/hudi-streamer"},{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"apache parquet",permalink:"/cn/blog/tags/apache-parquet"},{label:"linkedin",permalink:"/cn/blog/tags/linkedin"}],readingTime:.045,truncated:!1,authors:[{name:"Soumil Shah"}],prevItem:{title:"Introducing Apache Hudi support with AWS Glue crawlers",permalink:"/cn/blog/2023/11/22/Introducing-Apache-Hudi-support-with-AWS-Glue-crawlers"},nextItem:{title:"Apache Hudi: From Zero To One (6/10)",permalink:"/cn/blog/2023/11/13/Apache-Hudi-From-Zero-To-One-blog-6"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.linkedin.com/pulse/hudi-streamer-delta-hands-on-guide-local-ingestion-from-soumil-shah-jssse/?utm_source=share&utm_medium=member_ios&utm_campaign=share_via",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},50472:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Introducing Apache Hudi support with AWS Glue crawlers",excerpt:"Introducing Apache Hudi support with AWS Glue crawlers",author:"Noritaka Sekiyama, Kyle Duong, Sandeep Adwankar",category:"blog",image:"/assets/images/blog/2023-11-22-Introducing-Apache-Hudi-support-with-AWS-Glue-crawlers.png",tags:["apache hudi","how-to","aws glue crawlers"]},s=void 0,l={permalink:"/cn/blog/2023/11/22/Introducing-Apache-Hudi-support-with-AWS-Glue-crawlers",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-11-22-Introducing-Apache-Hudi-support-with-AWS-Glue-crawlers.mdx",source:"@site/blog/2023-11-22-Introducing-Apache-Hudi-support-with-AWS-Glue-crawlers.mdx",title:"Introducing Apache Hudi support with AWS Glue crawlers",description:"Redirecting... please wait!!",date:"2023-11-22T00:00:00.000Z",formattedDate:"November 22, 2023",tags:[{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"aws glue crawlers",permalink:"/cn/blog/tags/aws-glue-crawlers"}],readingTime:.045,truncated:!1,authors:[{name:"Noritaka Sekiyama, Kyle Duong, Sandeep Adwankar"}],prevItem:{title:"Real-Time Data Processing with Postgres, Debezium, Kafka, Schema Registry, and Delta Streamer Guide for Begineers",permalink:"/cn/blog/2023/11/26/Real-Time-Data-Processing-with-Postgres-Debezium-Kafka-Schema-Registry-and-DeltaStreamer-Guide-for-Begineers"},nextItem:{title:"Hudi Streamer (Delta Streamer) Hands-On Guide: Local Ingestion from Parquet Source",permalink:"/cn/blog/2023/11/19/Hudi-Streamer-DeltaStreamer-Hands-On-Guide-Local-Ingestion-from-Parquet-Source"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/introducing-apache-hudi-support-with-aws-glue-crawlers/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},30090:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Real-Time Data Processing with Postgres, Debezium, Kafka, Schema Registry, and Delta Streamer Guide for Begineers",excerpt:"Real-Time Data Processing with Postgres, Debezium, Kafka, Schema Registry, and Delta Streamer Guide for Begineers",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2023-11-26-Real-Time-Data-Processing-with-Postgres-Debezium-Kafka-Schema-Registry-and-DeltaStreamer-Guide-for-Begineers.png",tags:["apache hudi","postgres","how-to","debezium","apache kafka","deltastreamer","linkedin"]},s=void 0,l={permalink:"/cn/blog/2023/11/26/Real-Time-Data-Processing-with-Postgres-Debezium-Kafka-Schema-Registry-and-DeltaStreamer-Guide-for-Begineers",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-11-26-Real-Time-Data-Processing-with-Postgres-Debezium-Kafka-Schema-Registry-and-DeltaStreamer-Guide-for-Begineers.mdx",source:"@site/blog/2023-11-26-Real-Time-Data-Processing-with-Postgres-Debezium-Kafka-Schema-Registry-and-DeltaStreamer-Guide-for-Begineers.mdx",title:"Real-Time Data Processing with Postgres, Debezium, Kafka, Schema Registry, and Delta Streamer Guide for Begineers",description:"Redirecting... please wait!!",date:"2023-11-26T00:00:00.000Z",formattedDate:"November 26, 2023",tags:[{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"postgres",permalink:"/cn/blog/tags/postgres"},{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"debezium",permalink:"/cn/blog/tags/debezium"},{label:"apache kafka",permalink:"/cn/blog/tags/apache-kafka"},{label:"deltastreamer",permalink:"/cn/blog/tags/deltastreamer"},{label:"linkedin",permalink:"/cn/blog/tags/linkedin"}],readingTime:.045,truncated:!1,authors:[{name:"Soumil Shah"}],prevItem:{title:"Apache Hudi (Part 1): History, Getting Started",permalink:"/cn/blog/2023/11/28/Apache-Hudi-Part-1-History-Getting-Started"},nextItem:{title:"Introducing Apache Hudi support with AWS Glue crawlers",permalink:"/cn/blog/2023/11/22/Introducing-Apache-Hudi-support-with-AWS-Glue-crawlers"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.linkedin.com/pulse/real-time-data-processing-postgres-debezium-kafka-schema-soumil-shah-li19e/?utm_source=share&utm_medium=member_ios&utm_campaign=share_via",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},9158:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Apache Hudi (Part 1): History, Getting Started",excerpt:"Apache Hudi (Part 1): History, Getting Started",author:"Dipankar Mazumdar",category:"blog",image:"/assets/images/blog/2023-11-28-Apache-Hudi-Part-1-History-Getting-Started.png",tags:["apache hudi","blog","getting started","medium"]},s=void 0,l={permalink:"/cn/blog/2023/11/28/Apache-Hudi-Part-1-History-Getting-Started",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-11-28-Apache-Hudi-Part-1-History-Getting-Started.mdx",source:"@site/blog/2023-11-28-Apache-Hudi-Part-1-History-Getting-Started.mdx",title:"Apache Hudi (Part 1): History, Getting Started",description:"Redirecting... please wait!!",date:"2023-11-28T00:00:00.000Z",formattedDate:"November 28, 2023",tags:[{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"getting started",permalink:"/cn/blog/tags/getting-started"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Dipankar Mazumdar"}],prevItem:{title:"Mastering Data Lakes: A Deep Dive into MINIO, Hudi, and Delta Streamer",permalink:"/cn/blog/2023/11/30/Mastering-Data-Lakes-A-Deep-Dive-into-MINIO-Hudi-and-Delta-Streamer"},nextItem:{title:"Real-Time Data Processing with Postgres, Debezium, Kafka, Schema Registry, and Delta Streamer Guide for Begineers",permalink:"/cn/blog/2023/11/26/Real-Time-Data-Processing-with-Postgres-Debezium-Kafka-Schema-Registry-and-DeltaStreamer-Guide-for-Begineers"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://dipankar-tnt.medium.com/apache-hudi-part-1-history-getting-started-95030b003759",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},80584:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Mastering Data Lakes: A Deep Dive into MINIO, Hudi, and Delta Streamer",excerpt:"A Deep Dive into MINIO, Hudi, and Delta Streamer",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2023-11-30-Mastering-Data-Lakes-A-Deep-Dive-into-MINIO-Hudi-and-Delta-Streamer.png",tags:["apache hudi","mino","how-to","deltastreamer","linkedin"]},s=void 0,l={permalink:"/cn/blog/2023/11/30/Mastering-Data-Lakes-A-Deep-Dive-into-MINIO-Hudi-and-Delta-Streamer",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-11-30-Mastering-Data-Lakes-A-Deep-Dive-into-MINIO-Hudi-and-Delta-Streamer.mdx",source:"@site/blog/2023-11-30-Mastering-Data-Lakes-A-Deep-Dive-into-MINIO-Hudi-and-Delta-Streamer.mdx",title:"Mastering Data Lakes: A Deep Dive into MINIO, Hudi, and Delta Streamer",description:"Redirecting... please wait!!",date:"2023-11-30T00:00:00.000Z",formattedDate:"November 30, 2023",tags:[{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"mino",permalink:"/cn/blog/tags/mino"},{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"deltastreamer",permalink:"/cn/blog/tags/deltastreamer"},{label:"linkedin",permalink:"/cn/blog/tags/linkedin"}],readingTime:.045,truncated:!1,authors:[{name:"Soumil Shah"}],prevItem:{title:"Getting started with Apache Hudi",permalink:"/cn/blog/2023/12/01/Getting-started-with-Apache-Hudi"},nextItem:{title:"Apache Hudi (Part 1): History, Getting Started",permalink:"/cn/blog/2023/11/28/Apache-Hudi-Part-1-History-Getting-Started"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.linkedin.com/pulse/mastering-data-lakes-deep-dive-minio-hudi-delta-streamer-soumil-shah-wxzsf/?utm_source=share&utm_medium=member_ios&utm_campaign=share_via",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},69216:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Getting started with Apache Hudi",excerpt:"Getting started with Apache Hudi",author:"DataCouch",category:"blog",image:"/assets/images/blog/2023-12-01-Getting-started-with-Apache-Hudi.png",tags:["apache hudi","apache spark","how-to","getting started","medium"]},s=void 0,l={permalink:"/cn/blog/2023/12/01/Getting-started-with-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-12-01-Getting-started-with-Apache-Hudi.mdx",source:"@site/blog/2023-12-01-Getting-started-with-Apache-Hudi.mdx",title:"Getting started with Apache Hudi",description:"Redirecting... please wait!!",date:"2023-12-01T00:00:00.000Z",formattedDate:"December 1, 2023",tags:[{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"apache spark",permalink:"/cn/blog/tags/apache-spark"},{label:"how-to",permalink:"/cn/blog/tags/how-to"},{label:"getting started",permalink:"/cn/blog/tags/getting-started"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"DataCouch"}],prevItem:{title:"Apache Hudi: From Zero To One (7/10)",permalink:"/cn/blog/2023/12/06/Apache-Hudi-From-Zero-To-One-blog-7"},nextItem:{title:"Mastering Data Lakes: A Deep Dive into MINIO, Hudi, and Delta Streamer",permalink:"/cn/blog/2023/11/30/Mastering-Data-Lakes-A-Deep-Dive-into-MINIO-Hudi-and-Delta-Streamer"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://datacouch.medium.com/getting-started-with-apache-hudi-711b89c107aa",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},20040:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Apache Hudi: From Zero To One (7/10)",excerpt:"Concurrently run writers and table services",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2023-12-06-Apache-Hudi-From-Zero-To-One-blog-7.png",tags:["blog","apache hudi","concurrency","datumagic","lock provider"]},s=void 0,l={permalink:"/cn/blog/2023/12/06/Apache-Hudi-From-Zero-To-One-blog-7",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-12-06-Apache-Hudi-From-Zero-To-One-blog-7.mdx",source:"@site/blog/2023-12-06-Apache-Hudi-From-Zero-To-One-blog-7.mdx",title:"Apache Hudi: From Zero To One (7/10)",description:"Redirecting... please wait!!",date:"2023-12-06T00:00:00.000Z",formattedDate:"December 6, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"concurrency",permalink:"/cn/blog/tags/concurrency"},{label:"datumagic",permalink:"/cn/blog/tags/datumagic"},{label:"lock provider",permalink:"/cn/blog/tags/lock-provider"}],readingTime:.045,truncated:!1,authors:[{name:"Shiyan Xu"}],prevItem:{title:"Getting started with Apache Hudi",permalink:"/cn/blog/2023/12/09/Getting-started-with-Apache-Hudi"},nextItem:{title:"Getting started with Apache Hudi",permalink:"/cn/blog/2023/12/01/Getting-started-with-Apache-Hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://blog.datumagic.com/p/apache-hudi-from-zero-to-one-710",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},40104:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Getting started with Apache Hudi",excerpt:"Getting started with Apache Hudi",author:"DataCouch",category:"blog",image:"/assets/images/blog/2023-12-09-Getting-started-with-Apache-Hudi.png",tags:["blog","apache hudi","medium","beginner"]},s=void 0,l={permalink:"/cn/blog/2023/12/09/Getting-started-with-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-12-09-Getting-started-with-Apache-Hudi.mdx",source:"@site/blog/2023-12-09-Getting-started-with-Apache-Hudi.mdx",title:"Getting started with Apache Hudi",description:"Redirecting... please wait!!",date:"2023-12-09T00:00:00.000Z",formattedDate:"December 9, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"medium",permalink:"/cn/blog/tags/medium"},{label:"beginner",permalink:"/cn/blog/tags/beginner"}],readingTime:.045,truncated:!1,authors:[{name:"DataCouch"}],prevItem:{title:"What is Apache Hudi",permalink:"/cn/blog/2023/12/13/what-is-apache-hudi"},nextItem:{title:"Apache Hudi: From Zero To One (7/10)",permalink:"/cn/blog/2023/12/06/Apache-Hudi-From-Zero-To-One-blog-7"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://datacouch.medium.com/getting-started-with-apache-hudi-711b89c107aa",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},38399:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"What is Apache Hudi",excerpt:"What is Apache Hudi",author:"Karim Faiz",category:"blog",image:"/assets/images/blog/2023-12-13-what-is-apache-hudi.png",tags:["blog","apache hudi","medium","beginner","apache spark"]},s=void 0,l={permalink:"/cn/blog/2023/12/13/what-is-apache-hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-12-13-what-is-apache-hudi.mdx",source:"@site/blog/2023-12-13-what-is-apache-hudi.mdx",title:"What is Apache Hudi",description:"Redirecting... please wait!!",date:"2023-12-13T00:00:00.000Z",formattedDate:"December 13, 2023",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"medium",permalink:"/cn/blog/tags/medium"},{label:"beginner",permalink:"/cn/blog/tags/beginner"},{label:"apache spark",permalink:"/cn/blog/tags/apache-spark"}],readingTime:.045,truncated:!1,authors:[{name:"Karim Faiz"}],prevItem:{title:"Apache Hudi 2023: A Year In Review",permalink:"/cn/blog/2023/12/28/apache-hudi-2023-a-year-in-review"},nextItem:{title:"Getting started with Apache Hudi",permalink:"/cn/blog/2023/12/09/Getting-started-with-Apache-Hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@karim.faiz/what-is-apache-hudi-e9363083830e",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},23298:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Apache Hudi 2023: A Year In Review",excerpt:"Reflect on and celebrate the myriad of exciting developments and accomplishments that have defined the year 2023 for the Hudi community.",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2023-12-28-a-year-in-review-2023/00.cover.png",tags:["apache hudi","community"]},r=void 0,s={permalink:"/cn/blog/2023/12/28/apache-hudi-2023-a-year-in-review",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-12-28-apache-hudi-2023-a-year-in-review.md",source:"@site/blog/2023-12-28-apache-hudi-2023-a-year-in-review.md",title:"Apache Hudi 2023: A Year In Review",description:"In the warm glow of the holiday season, I am delighted to convey a message of deep appreciation on behalf of the",date:"2023-12-28T00:00:00.000Z",formattedDate:"December 28, 2023",tags:[{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"community",permalink:"/cn/blog/tags/community"}],readingTime:7.065,truncated:!1,authors:[{name:"Shiyan Xu"}],prevItem:{title:"From Data lake to Microservices: Unleashing the Power of Apache Hudi's Record Level Index with FastAPI and Spark Connect",permalink:"/cn/blog/2024/01/01/From-Data-lake-to-Microservices-Unleashing-the-Power-of-Apache-Hudi-Record-Level-Index-with-FastAPI-and-Spark-Connect"},nextItem:{title:"What is Apache Hudi",permalink:"/cn/blog/2023/12/13/what-is-apache-hudi"}},l={authorsImageUrls:[void 0]},d=[{value:"Development Highlights",id:"development-highlights",children:[{value:"Indexing has elevated to a whole new level",id:"indexing-has-elevated-to-a-whole-new-level",children:[],level:3},{value:"Write throughput achieves remarkable advancement",id:"write-throughput-achieves-remarkable-advancement",children:[],level:3},{value:"Programming APIs have a brand-new look",id:"programming-apis-have-a-brand-new-look",children:[],level:3},{value:"Usability receives significant attention",id:"usability-receives-significant-attention",children:[],level:3},{value:"Platform capabilities are substantially enhanced",id:"platform-capabilities-are-substantially-enhanced",children:[],level:3},{value:"Ecosystem integrations undergo notable expansions",id:"ecosystem-integrations-undergo-notable-expansions",children:[],level:3},{value:"Interoperability is the key",id:"interoperability-is-the-key",children:[],level:3},{value:"Stay tuned for 2024",id:"stay-tuned-for-2024",children:[],level:3}],level:2},{value:"Content Spotlight",id:"content-spotlight",children:[],level:2},{value:"Engage with the Community",id:"engage-with-the-community",children:[],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...t}=e;return(0,n.yg)(p,(0,i.A)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("img",{src:"/assets/images/blog/2023-12-28-a-year-in-review-2023/00.cover.png",alt:"drawing",style:{width:"80%",display:"block",marginLeft:"auto",marginRight:"auto",marginTop:"18pt",marginBottom:"18pt"}}),(0,n.yg)("p",null,"In the warm glow of the holiday season, I am delighted to convey a message of deep appreciation on behalf of the\nHudi Project Management Committee (PMC) to all the contributors and users in the community who made 2023 an\nextraordinary year for Hudi. "),(0,n.yg)("p",null,"In 2023, the Hudi community continued strong engagement and activities, evident in the\n",(0,n.yg)("a",{parentName:"p",href:"https://ossinsight.io/analyze/apache/hudi#pull-requests"},"1,832 pull requests created"),",\nwith a significant 1,363 of these being merged. We proudly welcomed 2 new PMC members and 3 new Committers.\nOur community ",(0,n.yg)("a",{parentName:"p",href:"https://apache-hudi.slack.com/join/shared_invite/zt-20r833rxh-627NWYDUyR8jRtMa2mZ~gg#/"},"Slack channel"),"\nwitnessed a remarkable 44% increase in users, with numbers exceeding 3,800.\nOur presence on social media platforms has grown impressively, with our ",(0,n.yg)("a",{parentName:"p",href:"https://x.com/apachehudi"},"X (Twitter) account"),"\ngarnering 2,274 followers, and our newly established ",(0,n.yg)("a",{parentName:"p",href:"https://www.linkedin.com/company/apache-hudi/"},"LinkedIn page"),"\nrapidly gaining 2,245 followers in just three months. Let\u2019s take a moment to reflect on and celebrate the myriad of\nexciting developments and accomplishments that have defined the year 2023 for the Hudi community."),(0,n.yg)("img",{src:"/assets/images/blog/2023-12-28-a-year-in-review-2023/01.PR_histogram.png",alt:"drawing",style:{width:"80%",display:"block",marginLeft:"auto",marginRight:"auto",marginTop:"18pt",marginBottom:"18pt"}}),(0,n.yg)("h2",{id:"development-highlights"},"Development Highlights"),(0,n.yg)("p",null,"The year 2023 has been exceptionally productive for Hudi, marked by significant advancements and innovations.\nThere have been three major releases: ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/releases/release-0.13.0"},"0.13.0"),",\n",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/releases/release-0.14.0"},"0.14.0"),", and the trailblazing\n",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/releases/release-1.0.0-beta1"},"1.0.0-beta1")," that have collectively reshaped the\ndatabase experience for Hudi data lakehouses. Here are some brief summaries highlighting key features introduced:"),(0,n.yg)("h3",{id:"indexing-has-elevated-to-a-whole-new-level"},"Indexing has elevated to a whole new level"),(0,n.yg)("p",null,"Hudi's new ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/releases/release-0.14.0#record-level-index"},"Record Level Index"),"\nis a game-changing feature that boosts write performance for large tables. It achieves this by efficiently\nstoring per-record locations, enabling rapid retrieval during index look-ups. Benchmarks indicate a 72%\nimprovement in write latency compared to the Global Simple Index, alongside notable reductions in query latency\nfor equality-matching queries. The new ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/releases/release-0.14.0#consistent-hashing-index-support"},"Consistent Hash Index"),"\ndynamically scales the buckets for hash-based indexing schemes. By addressing data skew issues inherent in bucket\nindex, it can achieve blazing fast look-up similar to the Record Level Index during the write process.\n",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/releases/release-1.0.0-beta1#functional-index"},"Functional Index"),"\nenables the creation and deletion of indexes on specific columns, providing users with additional means to\nspeed up queries and adjust partitioning."),(0,n.yg)("h3",{id:"write-throughput-achieves-remarkable-advancement"},"Write throughput achieves remarkable advancement"),(0,n.yg)("p",null,"A common reason why developers choose Apache Hudi is for its ",(0,n.yg)("a",{parentName:"p",href:"https://medium.com/@kywe665/delta-hudi-iceberg-a-benchmark-compilation-a5630c69cffc"},"industry leading write throughput and performance"),".\nThe community has continued innovations on write performance including\n",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/releases/release-0.13.0#early-conflict-detection-for-multi-writer"},"Early-conflict detection for OCC"),"\nwhich proactively validates concurrent writes before they are written to disk, avoiding significant resource wastage\nand enhancing throughput. Up-leveling this, the\n",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/releases/release-1.0.0-beta1#concurrency-control"},"Non-Blocking Concurrency Control"),"\nintroduced in 1.0 further optimizes multi-writer throughput by allowing conflicts to be resolved later in query\nor via compaction. Responding to popular community requests,\n",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/releases/release-0.13.0#support-for-partial-payload-update"},"partial update capability"),"\nwas implemented to allow updates to be applied only to changed fields, particularly benefiting the dimension\ntables that are usually super wide."),(0,n.yg)("h3",{id:"programming-apis-have-a-brand-new-look"},"Programming APIs have a brand-new look"),(0,n.yg)("p",null,(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/releases/release-0.13.0#optimizing-record-payload-handling"},"HoodieRecordMerger"),"\nis a new abstraction that unifies the merging semantics and makes use of the engine-native representation for\nrecords in the process. Benchmark shows a ballpark of 10-20% boost for upsert performance.\n",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/releases/release-1.0.0-beta1#new-filegroup-reader"},"File Group Reader"),"\nis another API that standardizes File Group access, reducing MoR tables' read latencies by approximately 20%.\nEnabling position-based merging and page-skipping can further accelerate snapshot queries by 5.7 times."),(0,n.yg)("h3",{id:"usability-receives-significant-attention"},"Usability receives significant attention"),(0,n.yg)("p",null,(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/releases/release-0.14.0#table-valued-function-named-hudi_table_changes-designed-for-incremental-reading-through-spark-sql"},"Table-valued function ",(0,n.yg)("inlineCode",{parentName:"a"},"hudi_table_changes")),"\nsimplifies performing incremental queries via SQLs.\n",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/releases/release-0.14.0#support-for-hudi-tables-with-autogenerated-keys"},"Auto-generated keys"),"\nallows users to omit providing a record key field, especially useful for append-only tables. Among many other\nuser-friendly updates, two more notable ones are the addition of a\n",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/releases/release-0.13.0#hudi-cli-bundle"},(0,n.yg)("inlineCode",{parentName:"a"},"hudi-cli-bundle")," jar"),"\nand a revamped ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/basic_configurations"},"configuration page"),"."),(0,n.yg)("h3",{id:"platform-capabilities-are-substantially-enhanced"},"Platform capabilities are substantially enhanced"),(0,n.yg)("p",null,(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/releases/release-0.13.0#change-data-capture"},"Changed Data Capture"),"\nwas supported by logging additional information alongside writers. The changed data, including ",(0,n.yg)("inlineCode",{parentName:"p"},"before"),"\nand ",(0,n.yg)("inlineCode",{parentName:"p"},"after")," images, can be served through incremental queries, offering rich analytical insights.\n",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/releases/release-0.13.0#metaserver"},"Metaserver"),"\noffers centralized management services for operating numerous tables in lakehouse projects, signifying a major\nstep in Hudi's platform features.\n",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/releases/release-0.14.0#hoodiedeltastreamer-renamed-to-hoodiestreamer"},(0,n.yg)("inlineCode",{parentName:"a"},"HoodieStreamer")),"\n(formerly ",(0,n.yg)("inlineCode",{parentName:"p"},"HoodieDeltaStreamer"),") remains a highly popular tool for data ingestion:\n",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/releases/release-0.13.0#new-source-support-in-deltastreamer"},"new sources"),"\nsuch as Protobuf Kafka source, GCS incremental source, and Pulsar source were added, further expanding\nthe integration capabilities."),(0,n.yg)("h3",{id:"ecosystem-integrations-undergo-notable-expansions"},"Ecosystem integrations undergo notable expansions"),(0,n.yg)("p",null,"On AWS,\n",(0,n.yg)("a",{parentName:"p",href:"https://aws.amazon.com/about-aws/whats-new/2023/05/amazon-athena-apache-hudi/"},"Athena supported Hudi 0.12.2 and Hudi's metadata table"),",\nelevating query performance.\n",(0,n.yg)("a",{parentName:"p",href:"https://aws.amazon.com/blogs/big-data/introducing-apache-hudi-support-with-aws-glue-crawlers/"},"AWS Glue crawlers added Hudi support"),"\nwith Glue 4.0 working with Hudi 0.12.1, and AWS EMR extended the\n",(0,n.yg)("a",{parentName:"p",href:"https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-app-versions-6.x.html"},"support matrix"),"\nto cover Hudi 0.13 and 0.14. GCP improved\n",(0,n.yg)("a",{parentName:"p",href:"https://cloud.google.com/blog/products/data-analytics/bigquery-manifest-file-support-for-open-table-format-queries"},"Hudi integration in BigQuery"),"\nwith a new manifest file integration for improved performance.\n",(0,n.yg)("a",{parentName:"p",href:"https://docs.starburst.io/latest/connector/hudi.html"},"Starburst also added a Hudi connector"),".\nExecution engine support has also been extended to newer versions, including Spark 3.4 and 3.5,\nas well as Flink 1.16, 1.17, and 1.18."),(0,n.yg)("h3",{id:"interoperability-is-the-key"},"Interoperability is the key"),(0,n.yg)("p",null,"While Apache Hudi continues its strong growth momentum, some members of the community also decided it is time to\nstart building interoperability bridges across Lakehouse table formats with Delta Lake and Iceberg. The\n",(0,n.yg)("a",{parentName:"p",href:"https://www.onehouse.ai/blog/onetable-is-now-open-source"},"recent announcement about OneTable becoming open source"),"\nmarks a big leap forward for all developers looking to build a data lakehouse architecture. This development not\nonly emphasizes Hudi's commitment to openness but also enables a wider range of users to experience the\ntechnological advantages offered by Hudi."),(0,n.yg)("h3",{id:"stay-tuned-for-2024"},"Stay tuned for 2024"),(0,n.yg)("p",null,"The File Group Reader APIs are poised for widespread adoption, promising benefits for numerous query\nengines. We also anticipate broad adoption for Non-Blocking Concurrency Control. And there's more on\nthe horizon, including innovations like infinite timeline, secondary indexes, multi-table transactions,\nand the support for unstructured data. For the latest updates and detailed insights, I encourage you to\nvisit the ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/roadmap"},"roadmap page"),"."),(0,n.yg)("h2",{id:"content-spotlight"},"Content Spotlight"),(0,n.yg)("img",{src:"/assets/images/blog/2023-12-28-a-year-in-review-2023/02.contentspotlight.png",alt:"drawing",style:{width:"80%",display:"block",marginLeft:"auto",marginRight:"auto",marginTop:"18pt",marginBottom:"18pt"}}),(0,n.yg)("p",null,"Below is a curated list highlighting noteworthy pieces of content from the diverse Hudi community in 2023:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://aws.amazon.com/blogs/big-data/create-an-apache-hudi-based-near-real-time-transactional-data-lake-using-aws-dms-amazon-kinesis-aws-glue-streaming-etl-and-data-visualization-using-amazon-quicksight/"},"Create Hudi-based near-real-time transactional data lake")," - AWS"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://aws.amazon.com/blogs/big-data/automate-schema-evolution-at-scale-with-apache-hudi-in-aws-glue/"},"Automate schema evolution at scale with Apache Hudi")," - AWS"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://aws.amazon.com/blogs/big-data/how-zoom-implemented-streaming-log-ingestion-and-efficient-gdpr-deletes-using-apache-hudi-on-amazon-emr/"},"Zoom implemented streaming log ingestion and efficient GDPR deletes using Hudi")," - AWS"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://medium.com/walmartglobaltech/lakehouse-at-fortune-1-scale-480bcb10391b"},"Lakehouse at Fortune 1 scale")," - Walmart"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://www.uber.com/blog/ubers-lakehouse-architecture/"},"Setting Uber\u2019s Transactional Data Lake in Motion")," - Uber"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://youtu.be/dZbXC4mlNck"},"Notion\u2019s journey: transition from Snowflake to Hudi")," - Notion"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://opensourcedatasummit.com/robinhoods-data-lakehouse/"},"Scaling and governing Robinhood's data lakehouse")," - Robinhood"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://www.onehouse.ai/blog/apache-hudi-vs-delta-lake-vs-apache-iceberg-lakehouse-feature-comparison"},"Feature comparison: Hudi vs Delta vs Iceberg")," - Kyle Weller, Onehouse"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://opensourcedatasummit.com/apache-hudi-1-preview/"},"Apache Hudi 1.0 preview: A database experience on the data lake")," - Sagar Sumit & Bhavani Sudha Saktheeswaran, Hudi PMC"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://www.onehouse.ai/blog/hudi-metafields-demystified"},"Hudi Metafields demystified")," and ",(0,n.yg)("a",{parentName:"li",href:"https://www.onehouse.ai/blog/knowing-your-data-partitioning-vices-on-the-data-lakehouse"},"Knowing your data partitioning vices")," - Bhavani Sudha Saktheeswaran, Hudi PMC"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/blog/2023/11/01/record-level-index/"},"Record Level Index: blazing fast indexing for large-scale datasets")," - Shiyan Xu & Sivabalan Narayanan, Hudi PMC"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://blog.datumagic.com/p/apache-hudi-from-zero-to-one-110"},"Apache Hudi from zero to one: a 10-post blog series")," - Shiyan Xu, Hudi PMC"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://youtu.be/YgmOASLum7g"},"Hudi Workshop: Build a ride-share lakehouse platform on AWS")," - Soumil Shah, Jaganath Achari, Nadine Farah")),(0,n.yg)("p",null,"Additionally, the official Hudi website is a treasure trove of valuable learning materials. Begin your\njourney on ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/overview"},"the documentation page"),", and then explore a wealth of\n",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/talks"},"talks"),", ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/videos"},"videos"),",\nand ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/blog"},"blogs")," to deepen your understanding and knowledge of Hudi."),(0,n.yg)("h2",{id:"engage-with-the-community"},"Engage with the Community"),(0,n.yg)("p",null,"Throughout 2023, the Hudi community played an important role in the data industry altogether, gathering and\nfeaturing in many virtual syncs, live events, meet-ups, and conference presentations. We marked our presence\nat a variety of events, listed here in no particular order: Re:Invent, PrestoCon, Trino Fest, Current,\nthe Data & AI Summit, Flink Forward, the Open-source Data Summit, ApacheCon, AI.dev, QCon, OSA Con, DEWCon,\nand Data Council."),(0,n.yg)("img",{src:"/assets/images/blog/2023-12-28-a-year-in-review-2023/03.events.png",alt:"drawing",style:{width:"80%",display:"block",marginLeft:"auto",marginRight:"auto",marginTop:"18pt",marginBottom:"18pt"}}),(0,n.yg)("p",null,"As we reflect on an eventful 2023, the Hudi community continues to thrive and welcomes diverse forms\nof engagement. For those looking to connect, our\n",(0,n.yg)("a",{parentName:"p",href:"https://join.slack.com/t/apache-hudi/shared_invite/zt-2ggm1fub8-_yt4Reu9djwqqVRFC7X49g"},"Slack group"),"\nis an excellent place for general inquiries, being watched out by Hudi experts and an LLM-backed\nquestion bot. You can also participate in our\n",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/community/office_hours"},"weekly office hours"),"\nand ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/community/syncs"},"monthly community syncs"),"\nto stay updated and involved. To keep abreast of the latest developments, follow Hudi's\n",(0,n.yg)("a",{parentName:"p",href:"https://www.linkedin.com/company/apache-hudi/"},"LinkedIn page"),",\n",(0,n.yg)("a",{parentName:"p",href:"https://twitter.com/apachehudi"},"X (Twitter) account"),",\nand ",(0,n.yg)("a",{parentName:"p",href:"https://www.youtube.com/@apachehudi"},"YouTube channel"),"."),(0,n.yg)("p",null,"If you encounter any issues or have feature requests, we encourage you to file them through\n",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/issues"},"GitHub issues")," or\n",(0,n.yg)("a",{parentName:"p",href:"https://issues.apache.org/jira/projects/HUDI/summary"},"JIRA tickets"),".\nFor more in-depth discussions and contributions to the ongoing development of Hudi,\nsubscribing (by sending an empty email) to\n",(0,n.yg)("a",{parentName:"p",href:"mailto:dev-subscribe@hudi.apache.org"},"our dev mailing list")," is a great option."),(0,n.yg)("p",null,"And for those inspired to contribute directly to the project,\n",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/contribute/how-to-contribute"},"our contribution guide")," is your\nstarting point. Your involvement, whether it's by contributing code, sharing ideas, or simply giving\n",(0,n.yg)("a",{parentName:"p",href:"https://github.com/apache/hudi/"},"our GitHub repository")," a star, is greatly valued. Together,\nlet's continue to shape Hudi's future and drive innovation in the open-source community.\nHere's to an even more vibrant and successful 2024 ahead!"))}g.isMDXComponent=!0},58112:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"From Data lake to Microservices: Unleashing the Power of Apache Hudi's Record Level Index with FastAPI and Spark Connect",excerpt:"From Data lake to Microservices: Unleashing the Power of Apache Hudi's Record Level Index with FastAPI and Spark Connect",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2024-01-01-From-Data-lake-to-Microservices-Unleashing-the-Power-of-Apache-Hudi-Record-Level-Index-with-FastAPI-and-Spark-Connect.png",tags:["blog","apache hudi","linkedin","beginner","apache spark","record level index","pyspark","upserts","FastAPI"]},s=void 0,l={permalink:"/cn/blog/2024/01/01/From-Data-lake-to-Microservices-Unleashing-the-Power-of-Apache-Hudi-Record-Level-Index-with-FastAPI-and-Spark-Connect",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-01-01-From-Data-lake-to-Microservices-Unleashing-the-Power-of-Apache-Hudi-Record-Level-Index-with-FastAPI-and-Spark-Connect.mdx",source:"@site/blog/2024-01-01-From-Data-lake-to-Microservices-Unleashing-the-Power-of-Apache-Hudi-Record-Level-Index-with-FastAPI-and-Spark-Connect.mdx",title:"From Data lake to Microservices: Unleashing the Power of Apache Hudi's Record Level Index with FastAPI and Spark Connect",description:"Redirecting... please wait!!",date:"2024-01-01T00:00:00.000Z",formattedDate:"January 1, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"linkedin",permalink:"/cn/blog/tags/linkedin"},{label:"beginner",permalink:"/cn/blog/tags/beginner"},{label:"apache spark",permalink:"/cn/blog/tags/apache-spark"},{label:"record level index",permalink:"/cn/blog/tags/record-level-index"},{label:"pyspark",permalink:"/cn/blog/tags/pyspark"},{label:"upserts",permalink:"/cn/blog/tags/upserts"},{label:"FastAPI",permalink:"/cn/blog/tags/fast-api"}],readingTime:.045,truncated:!1,authors:[{name:"Soumil Shah"}],prevItem:{title:"Build a federated query solution with Apache Doris, Apache Flink, and Apache Hudi",permalink:"/cn/blog/2024/01/02/Build-a-federated-query-solution-with-Apache-Doris-Apache-Flink-and-Apache-Hudi"},nextItem:{title:"Apache Hudi 2023: A Year In Review",permalink:"/cn/blog/2023/12/28/apache-hudi-2023-a-year-in-review"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.linkedin.com/pulse/from-datalake-microservices-unleashing-power-apache-hudis-soumil-shah-ylkoe/?trk=article-ssr-frontend-pulse_more-articles_related-content-card",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},79725:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Build a federated query solution with Apache Doris, Apache Flink, and Apache Hudi",excerpt:"Build a federated query solution with Apache Doris, Apache Flink, and Apache Hudi",author:"Apache Doris",category:"blog",image:"/assets/images/blog/2024-01-02-Build-a-federated-query-solution-with-Apache-Doris-Apache-Flink-and-Apache-Hudi.png",tags:["blog","apache hudi","dev to","beginner","apache doris","apache flink"]},s=void 0,l={permalink:"/cn/blog/2024/01/02/Build-a-federated-query-solution-with-Apache-Doris-Apache-Flink-and-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-01-02-Build-a-federated-query-solution-with-Apache-Doris-Apache-Flink-and-Apache-Hudi.mdx",source:"@site/blog/2024-01-02-Build-a-federated-query-solution-with-Apache-Doris-Apache-Flink-and-Apache-Hudi.mdx",title:"Build a federated query solution with Apache Doris, Apache Flink, and Apache Hudi",description:"Redirecting... please wait!!",date:"2024-01-02T00:00:00.000Z",formattedDate:"January 2, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"dev to",permalink:"/cn/blog/tags/dev-to"},{label:"beginner",permalink:"/cn/blog/tags/beginner"},{label:"apache doris",permalink:"/cn/blog/tags/apache-doris"},{label:"apache flink",permalink:"/cn/blog/tags/apache-flink"}],readingTime:.045,truncated:!1,authors:[{name:"Apache Doris"}],prevItem:{title:"Small Talk about Apache Hudi",permalink:"/cn/blog/2024/01/05/Small-Talk-about-Apache-Hudi"},nextItem:{title:"From Data lake to Microservices: Unleashing the Power of Apache Hudi's Record Level Index with FastAPI and Spark Connect",permalink:"/cn/blog/2024/01/01/From-Data-lake-to-Microservices-Unleashing-the-Power-of-Apache-Hudi-Record-Level-Index-with-FastAPI-and-Spark-Connect"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://dev.to/apachedoris/build-a-federated-query-solution-with-apache-doris-apache-flink-and-apache-hudi-40io",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},17876:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Small Talk about Apache Hudi",excerpt:"Small Talk about Apache Hudi",author:"Ashok Kumar Kunkala",category:"blog",image:"/assets/images/blog/2024-01-05-Small-Talk-about-Apache-Hudi.png",tags:["blog","apache hudi","linkedin","beginner","inserts","upserts","cow","mor"]},s=void 0,l={permalink:"/cn/blog/2024/01/05/Small-Talk-about-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-01-05-Small-Talk-about-Apache-Hudi.mdx",source:"@site/blog/2024-01-05-Small-Talk-about-Apache-Hudi.mdx",title:"Small Talk about Apache Hudi",description:"Redirecting... please wait!!",date:"2024-01-05T00:00:00.000Z",formattedDate:"January 5, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"linkedin",permalink:"/cn/blog/tags/linkedin"},{label:"beginner",permalink:"/cn/blog/tags/beginner"},{label:"inserts",permalink:"/cn/blog/tags/inserts"},{label:"upserts",permalink:"/cn/blog/tags/upserts"},{label:"cow",permalink:"/cn/blog/tags/cow"},{label:"mor",permalink:"/cn/blog/tags/mor"}],readingTime:.045,truncated:!1,authors:[{name:"Ashok Kumar Kunkala"}],prevItem:{title:"Introduction to Apache Hudi",permalink:"/cn/blog/2024/01/09/introduction-to-apache-hudi"},nextItem:{title:"Build a federated query solution with Apache Doris, Apache Flink, and Apache Hudi",permalink:"/cn/blog/2024/01/02/Build-a-federated-query-solution-with-Apache-Doris-Apache-Flink-and-Apache-Hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.linkedin.com/pulse/small-talk-apache-hudi-ashok-kumar-kunkala-3ldge/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},75868:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Introduction to Apache Hudi",excerpt:"Introduction to Apache Hudi",author:"Andrew Savchyns",category:"blog",image:"/assets/images/blog/2024-01-09-introduction-to-apache-hudi.png",tags:["blog","apache hudi","medium","beginner","apache spark"]},s=void 0,l={permalink:"/cn/blog/2024/01/09/introduction-to-apache-hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-01-09-introduction-to-apache-hudi.mdx",source:"@site/blog/2024-01-09-introduction-to-apache-hudi.mdx",title:"Introduction to Apache Hudi",description:"Redirecting... please wait!!",date:"2024-01-09T00:00:00.000Z",formattedDate:"January 9, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"medium",permalink:"/cn/blog/tags/medium"},{label:"beginner",permalink:"/cn/blog/tags/beginner"},{label:"apache spark",permalink:"/cn/blog/tags/apache-spark"}],readingTime:.045,truncated:!1,authors:[{name:"Andrew Savchyns"}],prevItem:{title:"In-House Data Lake with CDC Processing, Hudi, Docker",permalink:"/cn/blog/2024/01/11/In-House-Data-Lake-with-CDC-Processing-Hudi-Docker"},nextItem:{title:"Small Talk about Apache Hudi",permalink:"/cn/blog/2024/01/05/Small-Talk-about-Apache-Hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/blue-orange-digital/introduction-to-apache-hudi-209521970112",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},20216:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"In-House Data Lake with CDC Processing, Hudi, Docker",excerpt:"In-House Data Lake with CDC Processing, Hudi, Docker",author:"Rahul",category:"blog",image:"/assets/images/blog/2024-01-11-In-House-Data-Lake-with-CDC-Processing-Hudi-Docker.png",tags:["blog","apache hudi","medium","intermediate","docker","cdc","apache kafka","debezium","apache spark","aws s3"]},s=void 0,l={permalink:"/cn/blog/2024/01/11/In-House-Data-Lake-with-CDC-Processing-Hudi-Docker",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-01-11-In-House-Data-Lake-with-CDC-Processing-Hudi-Docker.mdx",source:"@site/blog/2024-01-11-In-House-Data-Lake-with-CDC-Processing-Hudi-Docker.mdx",title:"In-House Data Lake with CDC Processing, Hudi, Docker",description:"Redirecting... please wait!!",date:"2024-01-11T00:00:00.000Z",formattedDate:"January 11, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"medium",permalink:"/cn/blog/tags/medium"},{label:"intermediate",permalink:"/cn/blog/tags/intermediate"},{label:"docker",permalink:"/cn/blog/tags/docker"},{label:"cdc",permalink:"/cn/blog/tags/cdc"},{label:"apache kafka",permalink:"/cn/blog/tags/apache-kafka"},{label:"debezium",permalink:"/cn/blog/tags/debezium"},{label:"apache spark",permalink:"/cn/blog/tags/apache-spark"},{label:"aws s3",permalink:"/cn/blog/tags/aws-s-3"}],readingTime:.045,truncated:!1,authors:[{name:"Rahul"}],prevItem:{title:"Enforce fine-grained access control on Open Table Formats via Amazon EMR integrated with AWS Lake Formation",permalink:"/cn/blog/2024/01/17/Enforce-fine-grained-access-control-on-Open-Table-Formats-via-Amazon-EMR-integrated-with-AWS-Lake-Formation"},nextItem:{title:"Introduction to Apache Hudi",permalink:"/cn/blog/2024/01/09/introduction-to-apache-hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@rhlkmr089/in-house-data-lake-with-cdc-processing-hudi-docker-878cee483ca0",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},9495:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Enforce fine-grained access control on Open Table Formats via Amazon EMR integrated with AWS Lake Formation",excerpt:"Enforce fine-grained access control on Open Table Formats via Amazon EMR integrated with AWS Lake Formation",author:"Raymond Lai, Aditya Shah, Bin Wang, and Melody Yang",category:"blog",image:"/assets/images/blog/2024-01-17-Enforce-fine-grained-access-control-on-Open-Table-Formats-via-Amazon-EMR-integrated-with-AWS-Lake-Formation.png",tags:["blog","apache hudi","aws","intermediate","amazon emr","aws lake formation","aws glue","aws s3","amazon sagemaker","aws cloud9","amazon athena","access control"]},s=void 0,l={permalink:"/cn/blog/2024/01/17/Enforce-fine-grained-access-control-on-Open-Table-Formats-via-Amazon-EMR-integrated-with-AWS-Lake-Formation",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-01-17-Enforce-fine-grained-access-control-on-Open-Table-Formats-via-Amazon-EMR-integrated-with-AWS-Lake-Formation.mdx",source:"@site/blog/2024-01-17-Enforce-fine-grained-access-control-on-Open-Table-Formats-via-Amazon-EMR-integrated-with-AWS-Lake-Formation.mdx",title:"Enforce fine-grained access control on Open Table Formats via Amazon EMR integrated with AWS Lake Formation",description:"Redirecting... please wait!!",date:"2024-01-17T00:00:00.000Z",formattedDate:"January 17, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"aws",permalink:"/cn/blog/tags/aws"},{label:"intermediate",permalink:"/cn/blog/tags/intermediate"},{label:"amazon emr",permalink:"/cn/blog/tags/amazon-emr"},{label:"aws lake formation",permalink:"/cn/blog/tags/aws-lake-formation"},{label:"aws glue",permalink:"/cn/blog/tags/aws-glue"},{label:"aws s3",permalink:"/cn/blog/tags/aws-s-3"},{label:"amazon sagemaker",permalink:"/cn/blog/tags/amazon-sagemaker"},{label:"aws cloud9",permalink:"/cn/blog/tags/aws-cloud-9"},{label:"amazon athena",permalink:"/cn/blog/tags/amazon-athena"},{label:"access control",permalink:"/cn/blog/tags/access-control"}],readingTime:.045,truncated:!1,authors:[{name:"Raymond Lai, Aditya Shah, Bin Wang, and Melody Yang"}],prevItem:{title:"Deleting Items from Apache Hudi using Delta Streamer in UPSERT Mode with Kafka Avro Messages",permalink:"/cn/blog/2024/01/18/Deleting-Items-from-Apache-Hudi-using-Delta-Streamer-in-UPSERT-Mode-with-Kafka-Avro-Messages"},nextItem:{title:"In-House Data Lake with CDC Processing, Hudi, Docker",permalink:"/cn/blog/2024/01/11/In-House-Data-Lake-with-CDC-Processing-Hudi-Docker"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/enforce-fine-grained-access-control-on-open-table-formats-via-amazon-emr-integrated-with-aws-lake-formation/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},24326:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Deleting Items from Apache Hudi using Delta Streamer in UPSERT Mode with Kafka Avro Messages",excerpt:"Deleting Items from Apache Hudi using Delta Streamer in UPSERT Mode with Kafka Avro Messages",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2024-01-18-Deleting-Items-from-Apache-Hudi-using-Delta-Streamer-in-UPSERT-Mode-with-Kafka-Avro-Messages.png",tags:["blog","apache hudi","linkedin","beginner","hudi streamer","deltastreamer","apache kafka","apache avro","upsert","delete"]},s=void 0,l={permalink:"/cn/blog/2024/01/18/Deleting-Items-from-Apache-Hudi-using-Delta-Streamer-in-UPSERT-Mode-with-Kafka-Avro-Messages",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-01-18-Deleting-Items-from-Apache-Hudi-using-Delta-Streamer-in-UPSERT-Mode-with-Kafka-Avro-Messages.mdx",source:"@site/blog/2024-01-18-Deleting-Items-from-Apache-Hudi-using-Delta-Streamer-in-UPSERT-Mode-with-Kafka-Avro-Messages.mdx",title:"Deleting Items from Apache Hudi using Delta Streamer in UPSERT Mode with Kafka Avro Messages",description:"Redirecting... please wait!!",date:"2024-01-18T00:00:00.000Z",formattedDate:"January 18, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"linkedin",permalink:"/cn/blog/tags/linkedin"},{label:"beginner",permalink:"/cn/blog/tags/beginner"},{label:"hudi streamer",permalink:"/cn/blog/tags/hudi-streamer"},{label:"deltastreamer",permalink:"/cn/blog/tags/deltastreamer"},{label:"apache kafka",permalink:"/cn/blog/tags/apache-kafka"},{label:"apache avro",permalink:"/cn/blog/tags/apache-avro"},{label:"upsert",permalink:"/cn/blog/tags/upsert"},{label:"delete",permalink:"/cn/blog/tags/delete"}],readingTime:.045,truncated:!1,authors:[{name:"Soumil Shah"}],prevItem:{title:"Learn How to Move Data From MongoDB to Apache Hudi Using PySpark",permalink:"/cn/blog/2024/01/20/Learn-How-to-Move-Data-From-MongoDB-to-Apache-Hudi-Using-PySpark"},nextItem:{title:"Enforce fine-grained access control on Open Table Formats via Amazon EMR integrated with AWS Lake Formation",permalink:"/cn/blog/2024/01/17/Enforce-fine-grained-access-control-on-Open-Table-Formats-via-Amazon-EMR-integrated-with-AWS-Lake-Formation"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.linkedin.com/pulse/deleting-items-from-apache-hudi-using-delta-streamer-upsert-shah-sxlce/?utm_source=share&utm_medium=member_ios&utm_campaign=share_via",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},10244:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Data Engineering: Bootstrapping Data lake with Apache Hudi",excerpt:"Data Engineering: Bootstrapping Data lake with Apache Hudi",author:"Krishna Prasad",category:"blog",image:"/assets/images/blog/2024-01-20-Data-Engineering-Bootstrapping-Data-lake-with-Apache-Hudi.png",tags:["blog","apache hudi","medium","beginner","ETL","aws glue","apache spark","aws s3"]},s=void 0,l={permalink:"/cn/blog/2024/01/20/Data-Engineering-Bootstrapping-Data-lake-with-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-01-20-Data-Engineering-Bootstrapping-Data-lake-with-Apache-Hudi.mdx",source:"@site/blog/2024-01-20-Data-Engineering-Bootstrapping-Data-lake-with-Apache-Hudi.mdx",title:"Data Engineering: Bootstrapping Data lake with Apache Hudi",description:"Redirecting... please wait!!",date:"2024-01-20T00:00:00.000Z",formattedDate:"January 20, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"medium",permalink:"/cn/blog/tags/medium"},{label:"beginner",permalink:"/cn/blog/tags/beginner"},{label:"ETL",permalink:"/cn/blog/tags/etl"},{label:"aws glue",permalink:"/cn/blog/tags/aws-glue"},{label:"apache spark",permalink:"/cn/blog/tags/apache-spark"},{label:"aws s3",permalink:"/cn/blog/tags/aws-s-3"}],readingTime:.045,truncated:!1,authors:[{name:"Krishna Prasad"}],prevItem:{title:"Use Amazon Athena with Spark SQL for your open-source transactional table formats",permalink:"/cn/blog/2024/01/24/Use-Amazon-Athena-with-Spark-SQL-for-your-open-source-transactional-table-formats"},nextItem:{title:"Learn How to Move Data From MongoDB to Apache Hudi Using PySpark",permalink:"/cn/blog/2024/01/20/Learn-How-to-Move-Data-From-MongoDB-to-Apache-Hudi-Using-PySpark"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@krishnaiitd/data-engineering-bootstrapping-data-lake-with-apache-hudi-b3323dff65fa",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},36371:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Learn How to Move Data From MongoDB to Apache Hudi Using PySpark",excerpt:"Learn How to Move Data From MongoDB to Apache Hudi Using PySpark",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2024-01-20-Learn-How-to-Move-Data-From-MongoDB-to-Apache-Hudi-Using-PySpark.png",tags:["blog","apache hudi","linkedin","beginner","mongodb","apache spark","pyspark"]},s=void 0,l={permalink:"/cn/blog/2024/01/20/Learn-How-to-Move-Data-From-MongoDB-to-Apache-Hudi-Using-PySpark",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-01-20-Learn-How-to-Move-Data-From-MongoDB-to-Apache-Hudi-Using-PySpark.mdx",source:"@site/blog/2024-01-20-Learn-How-to-Move-Data-From-MongoDB-to-Apache-Hudi-Using-PySpark.mdx",title:"Learn How to Move Data From MongoDB to Apache Hudi Using PySpark",description:"Redirecting... please wait!!",date:"2024-01-20T00:00:00.000Z",formattedDate:"January 20, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"linkedin",permalink:"/cn/blog/tags/linkedin"},{label:"beginner",permalink:"/cn/blog/tags/beginner"},{label:"mongodb",permalink:"/cn/blog/tags/mongodb"},{label:"apache spark",permalink:"/cn/blog/tags/apache-spark"},{label:"pyspark",permalink:"/cn/blog/tags/pyspark"}],readingTime:.045,truncated:!1,authors:[{name:"Soumil Shah"}],prevItem:{title:"Data Engineering: Bootstrapping Data lake with Apache Hudi",permalink:"/cn/blog/2024/01/20/Data-Engineering-Bootstrapping-Data-lake-with-Apache-Hudi"},nextItem:{title:"Deleting Items from Apache Hudi using Delta Streamer in UPSERT Mode with Kafka Avro Messages",permalink:"/cn/blog/2024/01/18/Deleting-Items-from-Apache-Hudi-using-Delta-Streamer-in-UPSERT-Mode-with-Kafka-Avro-Messages"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.linkedin.com/pulse/learn-how-move-data-from-mongodb-apache-hudi-using-pyspark-shah-cq3pe/?utm_source=share&utm_medium=member_ios&utm_campaign=share_via",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},85538:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Use Amazon Athena with Spark SQL for your open-source transactional table formats",excerpt:"Use Amazon Athena with Spark SQL for your open-source transactional table formats",author:"Pathik Shah, Raj Devnath",category:"blog",image:"/assets/images/blog/2024-01-24-Use-Amazon-Athena-with-Spark-SQL-for-your-open-source-transactional-table-formats.png",tags:["blog","apache hudi","aws","beginner","aws glue","aws athena","time travel query","clustering","compaction","aws s3","apache iceberg","delta lake"]},s=void 0,l={permalink:"/cn/blog/2024/01/24/Use-Amazon-Athena-with-Spark-SQL-for-your-open-source-transactional-table-formats",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-01-24-Use-Amazon-Athena-with-Spark-SQL-for-your-open-source-transactional-table-formats.mdx",source:"@site/blog/2024-01-24-Use-Amazon-Athena-with-Spark-SQL-for-your-open-source-transactional-table-formats.mdx",title:"Use Amazon Athena with Spark SQL for your open-source transactional table formats",description:"Redirecting... please wait!!",date:"2024-01-24T00:00:00.000Z",formattedDate:"January 24, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"aws",permalink:"/cn/blog/tags/aws"},{label:"beginner",permalink:"/cn/blog/tags/beginner"},{label:"aws glue",permalink:"/cn/blog/tags/aws-glue"},{label:"aws athena",permalink:"/cn/blog/tags/aws-athena"},{label:"time travel query",permalink:"/cn/blog/tags/time-travel-query"},{label:"clustering",permalink:"/cn/blog/tags/clustering"},{label:"compaction",permalink:"/cn/blog/tags/compaction"},{label:"aws s3",permalink:"/cn/blog/tags/aws-s-3"},{label:"apache iceberg",permalink:"/cn/blog/tags/apache-iceberg"},{label:"delta lake",permalink:"/cn/blog/tags/delta-lake"}],readingTime:.045,truncated:!1,authors:[{name:"Pathik Shah, Raj Devnath"}],prevItem:{title:"Leverage Partition Paths of your data lake tables to Optimize Data Retrieval Costs on the cloud",permalink:"/cn/blog/2024/01/30/Leverage-Partition-Paths-of-your-data-lake-tables-to-Optimize-Data-Retrieval-Costs-on-the-cloud"},nextItem:{title:"Data Engineering: Bootstrapping Data lake with Apache Hudi",permalink:"/cn/blog/2024/01/20/Data-Engineering-Bootstrapping-Data-lake-with-Apache-Hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/use-amazon-athena-with-spark-sql-for-your-open-source-transactional-table-formats/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},37909:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Leverage Partition Paths of your data lake tables to Optimize Data Retrieval Costs on the cloud",excerpt:"Leverage Partition Paths of your data lake tables to Optimize Data Retrieval Costs on the cloud",author:"Krishna Prasad",category:"blog",image:"/assets/images/blog/2024-01-30-Leverage-Partition-Paths-of-your-data-lake-tables-to-Optimize-Data-Retrieval-Costs-on-the-cloud.png",tags:["blog","apache hudi","medium","intermediate","aws glue","cost","apache spark","partition"]},s=void 0,l={permalink:"/cn/blog/2024/01/30/Leverage-Partition-Paths-of-your-data-lake-tables-to-Optimize-Data-Retrieval-Costs-on-the-cloud",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-01-30-Leverage-Partition-Paths-of-your-data-lake-tables-to-Optimize-Data-Retrieval-Costs-on-the-cloud.mdx",source:"@site/blog/2024-01-30-Leverage-Partition-Paths-of-your-data-lake-tables-to-Optimize-Data-Retrieval-Costs-on-the-cloud.mdx",title:"Leverage Partition Paths of your data lake tables to Optimize Data Retrieval Costs on the cloud",description:"Redirecting... please wait!!",date:"2024-01-30T00:00:00.000Z",formattedDate:"January 30, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"medium",permalink:"/cn/blog/tags/medium"},{label:"intermediate",permalink:"/cn/blog/tags/intermediate"},{label:"aws glue",permalink:"/cn/blog/tags/aws-glue"},{label:"cost",permalink:"/cn/blog/tags/cost"},{label:"apache spark",permalink:"/cn/blog/tags/apache-spark"},{label:"partition",permalink:"/cn/blog/tags/partition"}],readingTime:.045,truncated:!1,authors:[{name:"Krishna Prasad"}],prevItem:{title:"Apache Hudi: Managing Partition on a petabyte-scale table",permalink:"/cn/blog/2024/02/04/Apache-Hudi-Managing-Partition-on-a-petabyte-scale-table"},nextItem:{title:"Use Amazon Athena with Spark SQL for your open-source transactional table formats",permalink:"/cn/blog/2024/01/24/Use-Amazon-Athena-with-Spark-SQL-for-your-open-source-transactional-table-formats"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@krishnaiitd/leverage-partition-paths-of-your-data-lake-tables-to-optimize-data-retrieval-costs-on-the-cloud-6f4cced24398",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},1740:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Apache Hudi: Managing Partition on a petabyte-scale table",excerpt:"Apache Hudi: Managing Partition on a petabyte-scale table",author:"Krishna Prasad",category:"blog",image:"/assets/images/blog/2024-02-04-Apache-Hudi-Managing-Partition-on-a-petabyte-scale-table.png",tags:["blog","apache hudi","medium","intermediate","partition","aws glue","apache spark","aws s3"]},s=void 0,l={permalink:"/cn/blog/2024/02/04/Apache-Hudi-Managing-Partition-on-a-petabyte-scale-table",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-02-04-Apache-Hudi-Managing-Partition-on-a-petabyte-scale-table.mdx",source:"@site/blog/2024-02-04-Apache-Hudi-Managing-Partition-on-a-petabyte-scale-table.mdx",title:"Apache Hudi: Managing Partition on a petabyte-scale table",description:"Redirecting... please wait!!",date:"2024-02-04T00:00:00.000Z",formattedDate:"February 4, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"medium",permalink:"/cn/blog/tags/medium"},{label:"intermediate",permalink:"/cn/blog/tags/intermediate"},{label:"partition",permalink:"/cn/blog/tags/partition"},{label:"aws glue",permalink:"/cn/blog/tags/aws-glue"},{label:"apache spark",permalink:"/cn/blog/tags/apache-spark"},{label:"aws s3",permalink:"/cn/blog/tags/aws-s-3"}],readingTime:.045,truncated:!1,authors:[{name:"Krishna Prasad"}],prevItem:{title:"Combine Transactional Integrity and Data Lake Operations with YugabyteDB and Apache Hudi",permalink:"/cn/blog/2024/02/06/Combine-Transactional-Integrity-and-Data-Lake-Operations-with-YugabyteDB-and-Apache-Hudi"},nextItem:{title:"Leverage Partition Paths of your data lake tables to Optimize Data Retrieval Costs on the cloud",permalink:"/cn/blog/2024/01/30/Leverage-Partition-Paths-of-your-data-lake-tables-to-Optimize-Data-Retrieval-Costs-on-the-cloud"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@krishnaiitd/partitioning-apache-hudi-data-lake-table-ffd0ac28aad4",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},55711:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Building an Open Source Data Lake House with Hudi, Postgres Hive Metastore, Minio, and StarRocks",excerpt:"Building an Open Source Data Lake House with Hudi, Postgres Hive Metastore, Minio, and StarRocks",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2024-02-06-Building-an-Open-Source-Data-Lake-House-with-Hudi-Postgres-Hive-Metastore-Minio-and-StarRocks.png",tags:["blog","apache hudi","linkedin","beginner","apache spark","apache hive","hive metastore","minio","starrocks","docker","python","postgres","postgresql"]},s=void 0,l={permalink:"/cn/blog/2024/02/06/Building-an-Open-Source-Data-Lake-House-with-Hudi-Postgres-Hive-Metastore-Minio-and-StarRocks",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-02-06-Building-an-Open-Source-Data-Lake-House-with-Hudi-Postgres-Hive-Metastore-Minio-and-StarRocks.mdx",source:"@site/blog/2024-02-06-Building-an-Open-Source-Data-Lake-House-with-Hudi-Postgres-Hive-Metastore-Minio-and-StarRocks.mdx",title:"Building an Open Source Data Lake House with Hudi, Postgres Hive Metastore, Minio, and StarRocks",description:"Redirecting... please wait!!",date:"2024-02-06T00:00:00.000Z",formattedDate:"February 6, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"linkedin",permalink:"/cn/blog/tags/linkedin"},{label:"beginner",permalink:"/cn/blog/tags/beginner"},{label:"apache spark",permalink:"/cn/blog/tags/apache-spark"},{label:"apache hive",permalink:"/cn/blog/tags/apache-hive"},{label:"hive metastore",permalink:"/cn/blog/tags/hive-metastore"},{label:"minio",permalink:"/cn/blog/tags/minio"},{label:"starrocks",permalink:"/cn/blog/tags/starrocks"},{label:"docker",permalink:"/cn/blog/tags/docker"},{label:"python",permalink:"/cn/blog/tags/python"},{label:"postgres",permalink:"/cn/blog/tags/postgres"},{label:"postgresql",permalink:"/cn/blog/tags/postgresql"}],readingTime:.045,truncated:!1,authors:[{name:"Soumil Shah"}],prevItem:{title:"How a POC became a production-ready Hudi data lakehouse through close team collaboration",permalink:"/cn/blog/2024/02/12/How-a-POC-became-a-production-ready-Hudi-data-lakehouse-through-close-team-collaboration"},nextItem:{title:"Combine Transactional Integrity and Data Lake Operations with YugabyteDB and Apache Hudi",permalink:"/cn/blog/2024/02/06/Combine-Transactional-Integrity-and-Data-Lake-Operations-with-YugabyteDB-and-Apache-Hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.linkedin.com/pulse/building-open-source-data-lake-house-hudi-postgres-hive-soumil-shah-wwyye/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},38859:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Combine Transactional Integrity and Data Lake Operations with YugabyteDB and Apache Hudi",author:"Balachandar Seetharaman",category:"blog",image:"/assets/images/blog/2024-02-06-Combine-Transactional-Integrity-and-Data-Lake-Operations-with-YugabyteDB-and-Apache-Hudi.png",tags:["blog","apache hudi","ACID","transactions","real-time datalake","cdc","etl","yugabyte"]},s=void 0,l={permalink:"/cn/blog/2024/02/06/Combine-Transactional-Integrity-and-Data-Lake-Operations-with-YugabyteDB-and-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-02-06-Combine-Transactional-Integrity-and-Data-Lake-Operations-with-YugabyteDB-and-Apache-Hudi.mdx",source:"@site/blog/2024-02-06-Combine-Transactional-Integrity-and-Data-Lake-Operations-with-YugabyteDB-and-Apache-Hudi.mdx",title:"Combine Transactional Integrity and Data Lake Operations with YugabyteDB and Apache Hudi",description:"Redirecting... please wait!!",date:"2024-02-06T00:00:00.000Z",formattedDate:"February 6, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"ACID",permalink:"/cn/blog/tags/acid"},{label:"transactions",permalink:"/cn/blog/tags/transactions"},{label:"real-time datalake",permalink:"/cn/blog/tags/real-time-datalake"},{label:"cdc",permalink:"/cn/blog/tags/cdc"},{label:"etl",permalink:"/cn/blog/tags/etl"},{label:"yugabyte",permalink:"/cn/blog/tags/yugabyte"}],readingTime:.045,truncated:!1,authors:[{name:"Balachandar Seetharaman"}],prevItem:{title:"Building an Open Source Data Lake House with Hudi, Postgres Hive Metastore, Minio, and StarRocks",permalink:"/cn/blog/2024/02/06/Building-an-Open-Source-Data-Lake-House-with-Hudi-Postgres-Hive-Metastore-Minio-and-StarRocks"},nextItem:{title:"Apache Hudi: Managing Partition on a petabyte-scale table",permalink:"/cn/blog/2024/02/04/Apache-Hudi-Managing-Partition-on-a-petabyte-scale-table"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.yugabyte.com/blog/apache-hudi-data-lakehouse-integration/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},55505:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"How a POC became a production-ready Hudi data lakehouse through close team collaboration",excerpt:"How a POC became a production-ready Hudi data lakehouse through close team collaboration",author:"Xiaoxiao Rey and Hussein Awala",category:"blog",image:"/assets/images/blog/2024-02-12-How-a-POC-became-a-production-ready-Hudi-data-lakehouse-through-close-team-collaboration.png",tags:["use-case","apache hudi","leboncoin-tech-blog","beginner","delete","gdpr deletion","upsert"]},s=void 0,l={permalink:"/cn/blog/2024/02/12/How-a-POC-became-a-production-ready-Hudi-data-lakehouse-through-close-team-collaboration",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-02-12-How-a-POC-became-a-production-ready-Hudi-data-lakehouse-through-close-team-collaboration.mdx",source:"@site/blog/2024-02-12-How-a-POC-became-a-production-ready-Hudi-data-lakehouse-through-close-team-collaboration.mdx",title:"How a POC became a production-ready Hudi data lakehouse through close team collaboration",description:"Redirecting... please wait!!",date:"2024-02-12T00:00:00.000Z",formattedDate:"February 12, 2024",tags:[{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"leboncoin-tech-blog",permalink:"/cn/blog/tags/leboncoin-tech-blog"},{label:"beginner",permalink:"/cn/blog/tags/beginner"},{label:"delete",permalink:"/cn/blog/tags/delete"},{label:"gdpr deletion",permalink:"/cn/blog/tags/gdpr-deletion"},{label:"upsert",permalink:"/cn/blog/tags/upsert"}],readingTime:.045,truncated:!1,authors:[{name:"Xiaoxiao Rey and Hussein Awala"}],prevItem:{title:"Enabling near real-time data analytics on the data lake",permalink:"/cn/blog/2024/02/23/Enabling-near-real-time-data-analytics-on-the-data-lake"},nextItem:{title:"Building an Open Source Data Lake House with Hudi, Postgres Hive Metastore, Minio, and StarRocks",permalink:"/cn/blog/2024/02/06/Building-an-Open-Source-Data-Lake-House-with-Hudi-Postgres-Hive-Metastore-Minio-and-StarRocks"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/leboncoin-tech-blog/how-a-poc-became-a-production-ready-hudi-data-lakehouse-through-close-team-collaboration-c7f33eb746a8",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},6052:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Enabling near real-time data analytics on the data lake",author:"Shi Kai Ng and Shuguang Xiang",category:"blog",image:"/assets/images/blog/2024-02-23-Enabling-near-real-time-data-analytics-on-the-data-lake.jpg",tags:["blog","apache hudi","near real-time analytics","mor","grab"]},s=void 0,l={permalink:"/cn/blog/2024/02/23/Enabling-near-real-time-data-analytics-on-the-data-lake",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-02-23-Enabling-near-real-time-data-analytics-on-the-data-lake.mdx",source:"@site/blog/2024-02-23-Enabling-near-real-time-data-analytics-on-the-data-lake.mdx",title:"Enabling near real-time data analytics on the data lake",description:"Redirecting... please wait!!",date:"2024-02-23T00:00:00.000Z",formattedDate:"February 23, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"near real-time analytics",permalink:"/cn/blog/tags/near-real-time-analytics"},{label:"mor",permalink:"/cn/blog/tags/mor"},{label:"grab",permalink:"/cn/blog/tags/grab"}],readingTime:.045,truncated:!1,authors:[{name:"Shi Kai Ng and Shuguang Xiang"}],prevItem:{title:"Empowering data-driven excellence: How the Bluestone Data Platform embraced data mesh for success",permalink:"/cn/blog/2024/02/27/empowering-data-driven-excellence-how-the-bluestone-data-platform-embraced-data-mesh-for-success"},nextItem:{title:"How a POC became a production-ready Hudi data lakehouse through close team collaboration",permalink:"/cn/blog/2024/02/12/How-a-POC-became-a-production-ready-Hudi-data-lakehouse-through-close-team-collaboration"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://engineering.grab.com/enabling-near-realtime-data-analytics",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},57339:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Building Data Lakes on AWS with Kafka Connect, Debezium, Apicurio Registry, and Apache Hudi",excerpt:"Building Data Lakes on AWS with Kafka Connect, Debezium, Apicurio Registry, and Apache Hudi",author:"Gary A. Stafford",category:"blog",image:"/assets/images/blog/2024-02-27-Building-Data-Lakes-on-AWS-with-Kafka-Connect-Debezium-Apicurio-Registry-and-Apache-Hudi.png",tags:["blog","apache hudi","itnext","beginner","apache kafka","kafka connect","debezium","apicurio registry","aws","apache spark","deltastreamer","hudi streamer","amazon rds","amazon mks","amazon eks","aws glue","amazon emr"]},s=void 0,l={permalink:"/cn/blog/2024/02/27/Building-Data-Lakes-on-AWS-with-Kafka-Connect-Debezium-Apicurio-Registry-and-Apache-Hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-02-27-Building-Data-Lakes-on-AWS-with-Kafka-Connect-Debezium-Apicurio-Registry-and-Apache-Hudi.mdx",source:"@site/blog/2024-02-27-Building-Data-Lakes-on-AWS-with-Kafka-Connect-Debezium-Apicurio-Registry-and-Apache-Hudi.mdx",title:"Building Data Lakes on AWS with Kafka Connect, Debezium, Apicurio Registry, and Apache Hudi",description:"Redirecting... please wait!!",date:"2024-02-27T00:00:00.000Z",formattedDate:"February 27, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"itnext",permalink:"/cn/blog/tags/itnext"},{label:"beginner",permalink:"/cn/blog/tags/beginner"},{label:"apache kafka",permalink:"/cn/blog/tags/apache-kafka"},{label:"kafka connect",permalink:"/cn/blog/tags/kafka-connect"},{label:"debezium",permalink:"/cn/blog/tags/debezium"},{label:"apicurio registry",permalink:"/cn/blog/tags/apicurio-registry"},{label:"aws",permalink:"/cn/blog/tags/aws"},{label:"apache spark",permalink:"/cn/blog/tags/apache-spark"},{label:"deltastreamer",permalink:"/cn/blog/tags/deltastreamer"},{label:"hudi streamer",permalink:"/cn/blog/tags/hudi-streamer"},{label:"amazon rds",permalink:"/cn/blog/tags/amazon-rds"},{label:"amazon mks",permalink:"/cn/blog/tags/amazon-mks"},{label:"amazon eks",permalink:"/cn/blog/tags/amazon-eks"},{label:"aws glue",permalink:"/cn/blog/tags/aws-glue"},{label:"amazon emr",permalink:"/cn/blog/tags/amazon-emr"}],readingTime:.045,truncated:!1,authors:[{name:"Gary A. Stafford"}],prevItem:{title:"Apache Hudi: From Zero To One (9/10)",permalink:"/cn/blog/2024/03/05/Apache-Hudi-From-Zero-To-One-blog-9"},nextItem:{title:"Empowering data-driven excellence: How the Bluestone Data Platform embraced data mesh for success",permalink:"/cn/blog/2024/02/27/empowering-data-driven-excellence-how-the-bluestone-data-platform-embraced-data-mesh-for-success"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/itnext/building-data-lakes-on-aws-with-kafka-connect-debezium-apicurio-registry-and-apache-hudi-b4da0268dce",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},48392:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Empowering data-driven excellence: How the Bluestone Data Platform embraced data mesh for success",author:"Toney Thomas, Ben Vengerovsky and Rada Stanic",category:"blog",image:"/assets/images/blog/2024-02-27-empowering-data-driven-excellence-how-the-bluestone-data-platform-embraced-data-mesh-for-success.png",tags:["blog","apache hudi","use-case","data mesh","amazon"]},s=void 0,l={permalink:"/cn/blog/2024/02/27/empowering-data-driven-excellence-how-the-bluestone-data-platform-embraced-data-mesh-for-success",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-02-27-empowering-data-driven-excellence-how-the-bluestone-data-platform-embraced-data-mesh-for-success.mdx",source:"@site/blog/2024-02-27-empowering-data-driven-excellence-how-the-bluestone-data-platform-embraced-data-mesh-for-success.mdx",title:"Empowering data-driven excellence: How the Bluestone Data Platform embraced data mesh for success",description:"Redirecting... please wait!!",date:"2024-02-27T00:00:00.000Z",formattedDate:"February 27, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"data mesh",permalink:"/cn/blog/tags/data-mesh"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[{name:"Toney Thomas, Ben Vengerovsky and Rada Stanic"}],prevItem:{title:"Building Data Lakes on AWS with Kafka Connect, Debezium, Apicurio Registry, and Apache Hudi",permalink:"/cn/blog/2024/02/27/Building-Data-Lakes-on-AWS-with-Kafka-Connect-Debezium-Apicurio-Registry-and-Apache-Hudi"},nextItem:{title:"Enabling near real-time data analytics on the data lake",permalink:"/cn/blog/2024/02/23/Enabling-near-real-time-data-analytics-on-the-data-lake"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/empowering-data-driven-excellence-how-the-bluestone-data-platform-embraced-data-mesh-for-success/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},90943:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Apache Hudi: From Zero To One (9/10)",excerpt:"HoodieStreamer - a Swiss Army knife for ingestion",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2024-03-05-Apache-Hudi-From-Zero-To-One-blog-9.png",tags:["blog","apache hudi","deltastreamer","hudi streamer","table service","datumagic"]},s=void 0,l={permalink:"/cn/blog/2024/03/05/Apache-Hudi-From-Zero-To-One-blog-9",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-03-05-Apache-Hudi-From-Zero-To-One-blog-9.mdx",source:"@site/blog/2024-03-05-Apache-Hudi-From-Zero-To-One-blog-9.mdx",title:"Apache Hudi: From Zero To One (9/10)",description:"Redirecting... please wait!!",date:"2024-03-05T00:00:00.000Z",formattedDate:"March 5, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"deltastreamer",permalink:"/cn/blog/tags/deltastreamer"},{label:"hudi streamer",permalink:"/cn/blog/tags/hudi-streamer"},{label:"table service",permalink:"/cn/blog/tags/table-service"},{label:"datumagic",permalink:"/cn/blog/tags/datumagic"}],readingTime:.045,truncated:!1,authors:[{name:"Shiyan Xu"}],prevItem:{title:"Navigating the Future: The Evolutionary Journey of Upstox\u2019s Data Platform",permalink:"/cn/blog/2024/03/10/navigating-the-future-the-evolutionary-journey-of-upstoxs-data-platform"},nextItem:{title:"Building Data Lakes on AWS with Kafka Connect, Debezium, Apicurio Registry, and Apache Hudi",permalink:"/cn/blog/2024/02/27/Building-Data-Lakes-on-AWS-with-Kafka-Connect-Debezium-Apicurio-Registry-and-Apache-Hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://blog.datumagic.com/p/apache-hudi-from-zero-to-one-910?r=2fl10k&utm_campaign=post&utm_medium=web",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},14113:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Navigating the Future: The Evolutionary Journey of Upstox\u2019s Data Platform",author:"Manish Gaurav",category:"blog",image:"/assets/images/blog/2024-03-10-navigating-the-future-the-evolutionary-journey-of-upstoxs-data-platform.png",tags:["use-case","apache hudi","upstox-engineering"]},s=void 0,l={permalink:"/cn/blog/2024/03/10/navigating-the-future-the-evolutionary-journey-of-upstoxs-data-platform",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-03-10-navigating-the-future-the-evolutionary-journey-of-upstoxs-data-platform.mdx",source:"@site/blog/2024-03-10-navigating-the-future-the-evolutionary-journey-of-upstoxs-data-platform.mdx",title:"Navigating the Future: The Evolutionary Journey of Upstox\u2019s Data Platform",description:"Redirecting... please wait!!",date:"2024-03-10T00:00:00.000Z",formattedDate:"March 10, 2024",tags:[{label:"use-case",permalink:"/cn/blog/tags/use-case"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"upstox-engineering",permalink:"/cn/blog/tags/upstox-engineering"}],readingTime:.045,truncated:!1,authors:[{name:"Manish Gaurav"}],prevItem:{title:"Modern Datalakes with Hudi, MinIO, and HMS",permalink:"/cn/blog/2024/03/14/Modern-Datalakes-with-Hudi--MinIO--and-HMS"},nextItem:{title:"Apache Hudi: From Zero To One (9/10)",permalink:"/cn/blog/2024/03/05/Apache-Hudi-From-Zero-To-One-blog-9"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/upstox-engineering/navigating-the-future-the-evolutionary-journey-of-upstoxs-data-platform-92dc10ff22ae",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},78948:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Modern Datalakes with Hudi, MinIO, and HMS",author:"Brenna Buuck",category:"blog",image:"/assets/images/blog/2024-03-14-Modern-Datalakes-with-Hudi--MinIO--and-HMS.jpg",tags:["blog","apache hudi","minio","hms","hive metastore","min"]},s=void 0,l={permalink:"/cn/blog/2024/03/14/Modern-Datalakes-with-Hudi--MinIO--and-HMS",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-03-14-Modern-Datalakes-with-Hudi--MinIO--and-HMS.mdx",source:"@site/blog/2024-03-14-Modern-Datalakes-with-Hudi--MinIO--and-HMS.mdx",title:"Modern Datalakes with Hudi, MinIO, and HMS",description:"Redirecting... please wait!!",date:"2024-03-14T00:00:00.000Z",formattedDate:"March 14, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"minio",permalink:"/cn/blog/tags/minio"},{label:"hms",permalink:"/cn/blog/tags/hms"},{label:"hive metastore",permalink:"/cn/blog/tags/hive-metastore"},{label:"min",permalink:"/cn/blog/tags/min"}],readingTime:.045,truncated:!1,authors:[{name:"Brenna Buuck"}],prevItem:{title:"Open Table Formats (part-1): Apache Hudi (Hadoop Upserts Deletes and Incrementals)",permalink:"/cn/blog/2024/03/16/Open-Table-Formats-part-1-Apache-Hudi-Hadoop-Upserts-Deletes-and-Incrementals"},nextItem:{title:"Navigating the Future: The Evolutionary Journey of Upstox\u2019s Data Platform",permalink:"/cn/blog/2024/03/10/navigating-the-future-the-evolutionary-journey-of-upstoxs-data-platform"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://blog.min.io/datalakes-with-hudi-and-hms/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},16304:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Open Table Formats (part-1): Apache Hudi (Hadoop Upserts Deletes and Incrementals)",author:"Vivek L Alex",category:"blog",image:"/assets/images/blog/2024-03-16-Open-Table-Formats-part-1-Apache-Hudi-Hadoop-Upserts-Deletes-and-Incrementals.jpg",tags:["blog","apache hudi","beginner","defogdata"]},s=void 0,l={permalink:"/cn/blog/2024/03/16/Open-Table-Formats-part-1-Apache-Hudi-Hadoop-Upserts-Deletes-and-Incrementals",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-03-16-Open-Table-Formats-part-1-Apache-Hudi-Hadoop-Upserts-Deletes-and-Incrementals.mdx",source:"@site/blog/2024-03-16-Open-Table-Formats-part-1-Apache-Hudi-Hadoop-Upserts-Deletes-and-Incrementals.mdx",title:"Open Table Formats (part-1): Apache Hudi (Hadoop Upserts Deletes and Incrementals)",description:"Redirecting... please wait!!",date:"2024-03-16T00:00:00.000Z",formattedDate:"March 16, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"beginner",permalink:"/cn/blog/tags/beginner"},{label:"defogdata",permalink:"/cn/blog/tags/defogdata"}],readingTime:.045,truncated:!1,authors:[{name:"Vivek L Alex"}],prevItem:{title:"Cost Optimization Strategies for scalable Data Lakehouse",permalink:"/cn/blog/2024/03/22/data-lake-cost-optimisation-strategies"},nextItem:{title:"Modern Datalakes with Hudi, MinIO, and HMS",permalink:"/cn/blog/2024/03/14/Modern-Datalakes-with-Hudi--MinIO--and-HMS"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://defogdata.substack.com/p/table-format-series-apache-hudi-hadoop?r=37wv2a&utm_campaign=post&utm_medium=web&triedRedirect=true",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},32606:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Cost Optimization Strategies for scalable Data Lakehouse",author:"Suresh Hasundi",category:"blog",image:"/assets/images/blog/2024-03-22-data-lake-cost-optimisation-strategies.png",tags:["blog","apache hudi","amazon s3","amazon emr","apcache spark","lakehouse","cost optimization","halodoc"]},s=void 0,l={permalink:"/cn/blog/2024/03/22/data-lake-cost-optimisation-strategies",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-03-22-data-lake-cost-optimisation-strategies.mdx",source:"@site/blog/2024-03-22-data-lake-cost-optimisation-strategies.mdx",title:"Cost Optimization Strategies for scalable Data Lakehouse",description:"Redirecting... please wait!!",date:"2024-03-22T00:00:00.000Z",formattedDate:"March 22, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"amazon s3",permalink:"/cn/blog/tags/amazon-s-3"},{label:"amazon emr",permalink:"/cn/blog/tags/amazon-emr"},{label:"apcache spark",permalink:"/cn/blog/tags/apcache-spark"},{label:"lakehouse",permalink:"/cn/blog/tags/lakehouse"},{label:"cost optimization",permalink:"/cn/blog/tags/cost-optimization"},{label:"halodoc",permalink:"/cn/blog/tags/halodoc"}],readingTime:.045,truncated:!1,authors:[{name:"Suresh Hasundi"}],prevItem:{title:"Options on Kafka sink to open table Formats: Apache Iceberg and Apache Hudi",permalink:"/cn/blog/2024/03/23/options-on-kafka-sink-to-open-table-formats-apache-iceberg-and-apache-hudi"},nextItem:{title:"Open Table Formats (part-1): Apache Hudi (Hadoop Upserts Deletes and Incrementals)",permalink:"/cn/blog/2024/03/16/Open-Table-Formats-part-1-Apache-Hudi-Hadoop-Upserts-Deletes-and-Incrementals"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://blogs.halodoc.io/data-lake-cost-optimisation-strategies/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},87593:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Options on Kafka sink to open table Formats: Apache Iceberg and Apache Hudi",author:"Albert Wong",category:"blog",image:"/assets/images/blog/2024-03-23-options-on-kafka-sink-to-open-table-formats-apache-iceberg-and-apache-hudi.png",tags:["blog","apache hudi","apache iceberg","apache Kafka","kafka connect","starrocks","devgenius"]},s=void 0,l={permalink:"/cn/blog/2024/03/23/options-on-kafka-sink-to-open-table-formats-apache-iceberg-and-apache-hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-03-23-options-on-kafka-sink-to-open-table-formats-apache-iceberg-and-apache-hudi.mdx",source:"@site/blog/2024-03-23-options-on-kafka-sink-to-open-table-formats-apache-iceberg-and-apache-hudi.mdx",title:"Options on Kafka sink to open table Formats: Apache Iceberg and Apache Hudi",description:"Redirecting... please wait!!",date:"2024-03-23T00:00:00.000Z",formattedDate:"March 23, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"apache iceberg",permalink:"/cn/blog/tags/apache-iceberg"},{label:"apache Kafka",permalink:"/cn/blog/tags/apache-kafka"},{label:"kafka connect",permalink:"/cn/blog/tags/kafka-connect"},{label:"starrocks",permalink:"/cn/blog/tags/starrocks"},{label:"devgenius",permalink:"/cn/blog/tags/devgenius"}],readingTime:.045,truncated:!1,authors:[{name:"Albert Wong"}],prevItem:{title:"Record Level Indexing in Apache Hudi Delivers 70% Faster Point Lookups",permalink:"/cn/blog/2024/03/30/record-level-indexing-apache-hudi-delivers-70-faster-point"},nextItem:{title:"Cost Optimization Strategies for scalable Data Lakehouse",permalink:"/cn/blog/2024/03/22/data-lake-cost-optimisation-strategies"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://blog.devgenius.io/options-on-kafka-sink-to-open-table-formats-apache-iceberg-and-apache-hudi-f6839ddad978",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},73919:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Record Level Indexing in Apache Hudi Delivers 70% Faster Point Lookups",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2024-03-30-record-level-indexing-apache-hudi-delivers-70-faster-point.png",tags:["blog","apache hudi","record level index","performance","linkedin"]},s=void 0,l={permalink:"/cn/blog/2024/03/30/record-level-indexing-apache-hudi-delivers-70-faster-point",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-03-30-record-level-indexing-apache-hudi-delivers-70-faster-point.mdx",source:"@site/blog/2024-03-30-record-level-indexing-apache-hudi-delivers-70-faster-point.mdx",title:"Record Level Indexing in Apache Hudi Delivers 70% Faster Point Lookups",description:"Redirecting... please wait!!",date:"2024-03-30T00:00:00.000Z",formattedDate:"March 30, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"record level index",permalink:"/cn/blog/tags/record-level-index"},{label:"performance",permalink:"/cn/blog/tags/performance"},{label:"linkedin",permalink:"/cn/blog/tags/linkedin"}],readingTime:.045,truncated:!1,authors:[{name:"Soumil Shah"}],prevItem:{title:"Hands-On Guide: Reading Data from Hudi Tables Incrementally, Joining with Delta Tables using HudiStreamer and SQL-Based Transformer",permalink:"/cn/blog/2024/04/03/hands-on-guide-reading-data-from-hudi-tables-joining-delta"},nextItem:{title:"Options on Kafka sink to open table Formats: Apache Iceberg and Apache Hudi",permalink:"/cn/blog/2024/03/23/options-on-kafka-sink-to-open-table-formats-apache-iceberg-and-apache-hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.linkedin.com/pulse/record-level-indexing-apache-hudi-delivers-70-faster-point-shah-hlite/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},40966:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Hands-On Guide: Reading Data from Hudi Tables Incrementally, Joining with Delta Tables using HudiStreamer and SQL-Based Transformer",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2024-04-03-hands-on-guide-reading-data-from-hudi-tables-joining-delta.png",tags:["blog","apache hudi","deltastreamer","hudi streamer","delta","sql transformer","linkedin"]},s=void 0,l={permalink:"/cn/blog/2024/04/03/hands-on-guide-reading-data-from-hudi-tables-joining-delta",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-04-03-hands-on-guide-reading-data-from-hudi-tables-joining-delta.mdx",source:"@site/blog/2024-04-03-hands-on-guide-reading-data-from-hudi-tables-joining-delta.mdx",title:"Hands-On Guide: Reading Data from Hudi Tables Incrementally, Joining with Delta Tables using HudiStreamer and SQL-Based Transformer",description:"Redirecting... please wait!!",date:"2024-04-03T00:00:00.000Z",formattedDate:"April 3, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"deltastreamer",permalink:"/cn/blog/tags/deltastreamer"},{label:"hudi streamer",permalink:"/cn/blog/tags/hudi-streamer"},{label:"delta",permalink:"/cn/blog/tags/delta"},{label:"sql transformer",permalink:"/cn/blog/tags/sql-transformer"},{label:"linkedin",permalink:"/cn/blog/tags/linkedin"}],readingTime:.045,truncated:!1,authors:[{name:"Soumil Shah"}],prevItem:{title:"Build Real Time Streaming Pipeline with Kinesis, Apache Flink and Apache Hudi with Hands-on",permalink:"/cn/blog/2024/04/21/build-real-time-streaming-pipeline-with-kinesis-apache-flink-and-apache-hudi"},nextItem:{title:"Record Level Indexing in Apache Hudi Delivers 70% Faster Point Lookups",permalink:"/cn/blog/2024/03/30/record-level-indexing-apache-hudi-delivers-70-faster-point"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.linkedin.com/pulse/hands-on-guide-reading-data-from-hudi-tables-joining-delta-shah-vqivf/?trk=public_post_main-feed-card_feed-article-content",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},40968:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Build Real Time Streaming Pipeline with Kinesis, Apache Flink and Apache Hudi with Hands-on",author:"Md Shahid Afridi P",category:"blog",image:"/assets/images/blog/2024-04-21-build-real-time-streaming-pipeline-with-kinesis-apache-flink-and-apache-hudi.png",tags:["blog","apache hudi","apache flink","amazon kinesis","amazon s3","streaming ingestion","real-time datalake","incremental processing","devgenius"]},s=void 0,l={permalink:"/cn/blog/2024/04/21/build-real-time-streaming-pipeline-with-kinesis-apache-flink-and-apache-hudi",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-04-21-build-real-time-streaming-pipeline-with-kinesis-apache-flink-and-apache-hudi.mdx",source:"@site/blog/2024-04-21-build-real-time-streaming-pipeline-with-kinesis-apache-flink-and-apache-hudi.mdx",title:"Build Real Time Streaming Pipeline with Kinesis, Apache Flink and Apache Hudi with Hands-on",description:"Redirecting... please wait!!",date:"2024-04-21T00:00:00.000Z",formattedDate:"April 21, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"apache flink",permalink:"/cn/blog/tags/apache-flink"},{label:"amazon kinesis",permalink:"/cn/blog/tags/amazon-kinesis"},{label:"amazon s3",permalink:"/cn/blog/tags/amazon-s-3"},{label:"streaming ingestion",permalink:"/cn/blog/tags/streaming-ingestion"},{label:"real-time datalake",permalink:"/cn/blog/tags/real-time-datalake"},{label:"incremental processing",permalink:"/cn/blog/tags/incremental-processing"},{label:"devgenius",permalink:"/cn/blog/tags/devgenius"}],readingTime:.045,truncated:!1,authors:[{name:"Md Shahid Afridi P"}],prevItem:{title:"Understanding Apache Hudi's Consistency Model Part 3",permalink:"/cn/blog/2024/04/24/understanding-apache-hudi-consistency-model-part-3"},nextItem:{title:"Hands-On Guide: Reading Data from Hudi Tables Incrementally, Joining with Delta Tables using HudiStreamer and SQL-Based Transformer",permalink:"/cn/blog/2024/04/03/hands-on-guide-reading-data-from-hudi-tables-joining-delta"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://blog.devgenius.io/build-real-time-streaming-pipeline-with-kinesis-apache-flink-and-apache-hudi-35d8501855b4",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},24779:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Understanding Apache Hudi's Consistency Model Part 1",author:"Jack Vanlightly",category:"blog",image:"/assets/images/blog/2024-04-24-understanding-apache-hudi-consistency-model-part-1.png",tags:["blog","apache hudi","table formats","ACID","consistency","cow","concurrency control","multi writer","tla+ specification","jack-vanlightly"]},s=void 0,l={permalink:"/cn/blog/2024/04/24/understanding-apache-hudi-consistency-model-part-1",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-04-24-understanding-apache-hudi-consistency-model-part-1.mdx",source:"@site/blog/2024-04-24-understanding-apache-hudi-consistency-model-part-1.mdx",title:"Understanding Apache Hudi's Consistency Model Part 1",description:"Redirecting... please wait!!",date:"2024-04-24T00:00:00.000Z",formattedDate:"April 24, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"table formats",permalink:"/cn/blog/tags/table-formats"},{label:"ACID",permalink:"/cn/blog/tags/acid"},{label:"consistency",permalink:"/cn/blog/tags/consistency"},{label:"cow",permalink:"/cn/blog/tags/cow"},{label:"concurrency control",permalink:"/cn/blog/tags/concurrency-control"},{label:"multi writer",permalink:"/cn/blog/tags/multi-writer"},{label:"tla+ specification",permalink:"/cn/blog/tags/tla-specification"},{label:"jack-vanlightly",permalink:"/cn/blog/tags/jack-vanlightly"}],readingTime:.045,truncated:!1,authors:[{name:"Jack Vanlightly"}],prevItem:{title:"Apache Hudi vs Apache Iceberg: A Comprehensive Comparison",permalink:"/cn/blog/2024/04/25/apache-hudi-vs-apache-iceberg-a-comprehensive-comparison"},nextItem:{title:"Understanding Apache Hudi's Consistency Model Part 2",permalink:"/cn/blog/2024/04/24/understanding-apache-hudi-consistency-model-part-2"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://jack-vanlightly.com/analyses/2024/4/24/understanding-apache-hudi-consistency-model-part-1",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},16804:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Understanding Apache Hudi's Consistency Model Part 2",author:"Jack Vanlightly",category:"blog",image:"/assets/images/blog/2024-04-24-understanding-apache-hudi-consistency-model-part-2.png",tags:["blog","consistency","concurrency control","multi writer","monotonic timestamp","timestamp collision","jack-vanlightly"]},s=void 0,l={permalink:"/cn/blog/2024/04/24/understanding-apache-hudi-consistency-model-part-2",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-04-24-understanding-apache-hudi-consistency-model-part-2.mdx",source:"@site/blog/2024-04-24-understanding-apache-hudi-consistency-model-part-2.mdx",title:"Understanding Apache Hudi's Consistency Model Part 2",description:"Redirecting... please wait!!",date:"2024-04-24T00:00:00.000Z",formattedDate:"April 24, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"consistency",permalink:"/cn/blog/tags/consistency"},{label:"concurrency control",permalink:"/cn/blog/tags/concurrency-control"},{label:"multi writer",permalink:"/cn/blog/tags/multi-writer"},{label:"monotonic timestamp",permalink:"/cn/blog/tags/monotonic-timestamp"},{label:"timestamp collision",permalink:"/cn/blog/tags/timestamp-collision"},{label:"jack-vanlightly",permalink:"/cn/blog/tags/jack-vanlightly"}],readingTime:.045,truncated:!1,authors:[{name:"Jack Vanlightly"}],prevItem:{title:"Understanding Apache Hudi's Consistency Model Part 1",permalink:"/cn/blog/2024/04/24/understanding-apache-hudi-consistency-model-part-1"},nextItem:{title:"Understanding Apache Hudi's Consistency Model Part 3",permalink:"/cn/blog/2024/04/24/understanding-apache-hudi-consistency-model-part-3"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://jack-vanlightly.com/analyses/2024/4/24/understanding-apache-hudi-consistency-model-part-2",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},63577:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Understanding Apache Hudi's Consistency Model Part 3",author:"Jack Vanlightly",category:"blog",image:"/assets/images/blog/2024-04-24-understanding-apache-hudi-consistency-model-part-3.png",tags:["blog","apache hudi","tla+ specification","consistency","concurrency control","multi writer","monotonic timestamp","jack-vanlightly"]},s=void 0,l={permalink:"/cn/blog/2024/04/24/understanding-apache-hudi-consistency-model-part-3",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-04-24-understanding-apache-hudi-consistency-model-part-3.mdx",source:"@site/blog/2024-04-24-understanding-apache-hudi-consistency-model-part-3.mdx",title:"Understanding Apache Hudi's Consistency Model Part 3",description:"Redirecting... please wait!!",date:"2024-04-24T00:00:00.000Z",formattedDate:"April 24, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"tla+ specification",permalink:"/cn/blog/tags/tla-specification"},{label:"consistency",permalink:"/cn/blog/tags/consistency"},{label:"concurrency control",permalink:"/cn/blog/tags/concurrency-control"},{label:"multi writer",permalink:"/cn/blog/tags/multi-writer"},{label:"monotonic timestamp",permalink:"/cn/blog/tags/monotonic-timestamp"},{label:"jack-vanlightly",permalink:"/cn/blog/tags/jack-vanlightly"}],readingTime:.045,truncated:!1,authors:[{name:"Jack Vanlightly"}],prevItem:{title:"Understanding Apache Hudi's Consistency Model Part 2",permalink:"/cn/blog/2024/04/24/understanding-apache-hudi-consistency-model-part-2"},nextItem:{title:"Build Real Time Streaming Pipeline with Kinesis, Apache Flink and Apache Hudi with Hands-on",permalink:"/cn/blog/2024/04/21/build-real-time-streaming-pipeline-with-kinesis-apache-flink-and-apache-hudi"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://jack-vanlightly.com/analyses/2024/4/25/understanding-apache-hudi-consistency-model-part-3",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},45598:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Apache Hudi vs Apache Iceberg: A Comprehensive Comparison",author:"RisingWave marketing team",category:"blog",image:"/assets/images/blog/2024-04-25-apache-hudi-vs-apache-iceberg-a-comprehensive-comparison.png",tags:["blog","apache hudi","apache iceberg","comparison","risingwave"]},s=void 0,l={permalink:"/cn/blog/2024/04/25/apache-hudi-vs-apache-iceberg-a-comprehensive-comparison",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-04-25-apache-hudi-vs-apache-iceberg-a-comprehensive-comparison.mdx",source:"@site/blog/2024-04-25-apache-hudi-vs-apache-iceberg-a-comprehensive-comparison.mdx",title:"Apache Hudi vs Apache Iceberg: A Comprehensive Comparison",description:"Redirecting... please wait!!",date:"2024-04-25T00:00:00.000Z",formattedDate:"April 25, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"apache iceberg",permalink:"/cn/blog/tags/apache-iceberg"},{label:"comparison",permalink:"/cn/blog/tags/comparison"},{label:"risingwave",permalink:"/cn/blog/tags/risingwave"}],readingTime:.045,truncated:!1,authors:[{name:"RisingWave marketing team"}],prevItem:{title:"How to Query Apache Hudi Tables with Python Using Daft: A Spark-Free Approach",permalink:"/cn/blog/2024/05/02/how-query-apache-hudi-tables-python-using-daft-spark-free"},nextItem:{title:"Understanding Apache Hudi's Consistency Model Part 1",permalink:"/cn/blog/2024/04/24/understanding-apache-hudi-consistency-model-part-1"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://risingwave.com/blog/apache-hudi-vs-apache-iceberg-a-comprehensive-comparison/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},33987:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"How to Query Apache Hudi Tables with Python Using Daft: A Spark-Free Approach",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2024-05-02-how-query-apache-hudi-tables-python-using-daft-spark-free.png",tags:["blog","apache hudi","python","daft","linkedin"]},s=void 0,l={permalink:"/cn/blog/2024/05/02/how-query-apache-hudi-tables-python-using-daft-spark-free",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-05-02-how-query-apache-hudi-tables-python-using-daft-spark-free.mdx",source:"@site/blog/2024-05-02-how-query-apache-hudi-tables-python-using-daft-spark-free.mdx",title:"How to Query Apache Hudi Tables with Python Using Daft: A Spark-Free Approach",description:"Redirecting... please wait!!",date:"2024-05-02T00:00:00.000Z",formattedDate:"May 2, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"python",permalink:"/cn/blog/tags/python"},{label:"daft",permalink:"/cn/blog/tags/daft"},{label:"linkedin",permalink:"/cn/blog/tags/linkedin"}],readingTime:.045,truncated:!1,authors:[{name:"Soumil Shah"}],prevItem:{title:"Learn how to read Hudi data with AWS Glue Ray using Daft (No Spark)",permalink:"/cn/blog/2024/05/07/learn-how-read-hudi-data-aws-glue-ray-using-daft-spark"},nextItem:{title:"Apache Hudi vs Apache Iceberg: A Comprehensive Comparison",permalink:"/cn/blog/2024/04/25/apache-hudi-vs-apache-iceberg-a-comprehensive-comparison"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.linkedin.com/pulse/how-query-apache-hudi-tables-python-using-daft-spark-free-soumil-shah-hpdwf/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},27468:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Learn how to read Hudi data with AWS Glue Ray using Daft (No Spark)",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2024-05-07-learn-how-read-hudi-data-aws-glue-ray-using-daft-spark.png",tags:["blog","apache hudi","aws glue","ray","daft","linkedin"]},s=void 0,l={permalink:"/cn/blog/2024/05/07/learn-how-read-hudi-data-aws-glue-ray-using-daft-spark",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-05-07-learn-how-read-hudi-data-aws-glue-ray-using-daft-spark.mdx",source:"@site/blog/2024-05-07-learn-how-read-hudi-data-aws-glue-ray-using-daft-spark.mdx",title:"Learn how to read Hudi data with AWS Glue Ray using Daft (No Spark)",description:"Redirecting... please wait!!",date:"2024-05-07T00:00:00.000Z",formattedDate:"May 7, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"aws glue",permalink:"/cn/blog/tags/aws-glue"},{label:"ray",permalink:"/cn/blog/tags/ray"},{label:"daft",permalink:"/cn/blog/tags/daft"},{label:"linkedin",permalink:"/cn/blog/tags/linkedin"}],readingTime:.045,truncated:!1,authors:[{name:"Soumil Shah"}],prevItem:{title:"Building Analytical Apps on the Lakehouse using Apache Hudi, Daft & Streamlit",permalink:"/cn/blog/2024/05/10/building-analytical-apps-on-the-lakehouse-using-apache-hudi-daft-streamlit"},nextItem:{title:"How to Query Apache Hudi Tables with Python Using Daft: A Spark-Free Approach",permalink:"/cn/blog/2024/05/02/how-query-apache-hudi-tables-python-using-daft-spark-free"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.linkedin.com/pulse/learn-how-read-hudi-data-aws-glue-ray-using-daft-spark-soumil-shah-kycbe/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},66217:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Building Analytical Apps on the Lakehouse using Apache Hudi, Daft & Streamlit",author:"Dipankar Mazumdar",category:"blog",image:"/assets/images/blog/2024-05-10-building-analytical-apps-on-the-lakehouse-using-apache-hudi-daft-streamlit.png",tags:["blog","apache hudi","python","daft","streamlit","lakehouse","apache-hudi-blogs"]},s=void 0,l={permalink:"/cn/blog/2024/05/10/building-analytical-apps-on-the-lakehouse-using-apache-hudi-daft-streamlit",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-05-10-building-analytical-apps-on-the-lakehouse-using-apache-hudi-daft-streamlit.mdx",source:"@site/blog/2024-05-10-building-analytical-apps-on-the-lakehouse-using-apache-hudi-daft-streamlit.mdx",title:"Building Analytical Apps on the Lakehouse using Apache Hudi, Daft & Streamlit",description:"Redirecting... please wait!!",date:"2024-05-10T00:00:00.000Z",formattedDate:"May 10, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"python",permalink:"/cn/blog/tags/python"},{label:"daft",permalink:"/cn/blog/tags/daft"},{label:"streamlit",permalink:"/cn/blog/tags/streamlit"},{label:"lakehouse",permalink:"/cn/blog/tags/lakehouse"},{label:"apache-hudi-blogs",permalink:"/cn/blog/tags/apache-hudi-blogs"}],readingTime:.045,truncated:!1,authors:[{name:"Dipankar Mazumdar"}],prevItem:{title:"Apache Hudi on AWS Glue",permalink:"/cn/blog/2024/05/19/apache-hudi-on-aws-glue"},nextItem:{title:"Learn how to read Hudi data with AWS Glue Ray using Daft (No Spark)",permalink:"/cn/blog/2024/05/07/learn-how-read-hudi-data-aws-glue-ray-using-daft-spark"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/apache-hudi-blogs/building-analytical-apps-on-the-lakehouse-using-apache-hudi-daft-streamlit-3224766fe58a",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},25734:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Apache Hudi on AWS Glue",author:"Sagar Lakshmipathy",category:"blog",image:"/assets/images/blog/2024-05-19-apache-hudi-on-aws-glue.png",tags:["blog","apache hudi","aws glue","dev to"]},s=void 0,l={permalink:"/cn/blog/2024/05/19/apache-hudi-on-aws-glue",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-05-19-apache-hudi-on-aws-glue.mdx",source:"@site/blog/2024-05-19-apache-hudi-on-aws-glue.mdx",title:"Apache Hudi on AWS Glue",description:"Redirecting... please wait!!",date:"2024-05-19T00:00:00.000Z",formattedDate:"May 19, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"aws glue",permalink:"/cn/blog/tags/aws-glue"},{label:"dev to",permalink:"/cn/blog/tags/dev-to"}],readingTime:.045,truncated:!1,authors:[{name:"Sagar Lakshmipathy"}],prevItem:{title:"Use AWS Data Exchange to seamlessly share Apache Hudi datasets",permalink:"/cn/blog/2024/05/22/use-aws-data-exchange-to-seamlessly-share-apache-hudi-datasets"},nextItem:{title:"Building Analytical Apps on the Lakehouse using Apache Hudi, Daft & Streamlit",permalink:"/cn/blog/2024/05/10/building-analytical-apps-on-the-lakehouse-using-apache-hudi-daft-streamlit"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://dev.to/sagarlakshmipathy/apache-hudi-on-aws-glue-450l",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},87602:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Use AWS Data Exchange to seamlessly share Apache Hudi datasets",author:"Saurabh Bhutyani, Ankith Ede, and Chandra Krishnan",category:"blog",image:"/assets/images/blog/2024-05-22-use-aws-data-exchange-to-seamlessly-share-apache-hudi-datasets.png",tags:["blog","apache hudi","aws data exchange","amazon emr","amazon s3","amazon athena","data sahring","amazon"]},s=void 0,l={permalink:"/cn/blog/2024/05/22/use-aws-data-exchange-to-seamlessly-share-apache-hudi-datasets",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-05-22-use-aws-data-exchange-to-seamlessly-share-apache-hudi-datasets.mdx",source:"@site/blog/2024-05-22-use-aws-data-exchange-to-seamlessly-share-apache-hudi-datasets.mdx",title:"Use AWS Data Exchange to seamlessly share Apache Hudi datasets",description:"Redirecting... please wait!!",date:"2024-05-22T00:00:00.000Z",formattedDate:"May 22, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"aws data exchange",permalink:"/cn/blog/tags/aws-data-exchange"},{label:"amazon emr",permalink:"/cn/blog/tags/amazon-emr"},{label:"amazon s3",permalink:"/cn/blog/tags/amazon-s-3"},{label:"amazon athena",permalink:"/cn/blog/tags/amazon-athena"},{label:"data sahring",permalink:"/cn/blog/tags/data-sahring"},{label:"amazon",permalink:"/cn/blog/tags/amazon"}],readingTime:.045,truncated:!1,authors:[{name:"Saurabh Bhutyani, Ankith Ede, and Chandra Krishnan"}],prevItem:{title:"Apache Hudi vs. Delta Lake: Choosing the Right Tool for Your Data Lake on AWS",permalink:"/cn/blog/2024/05/27/apache-hudi-vs-delta-lake-choosing-the-right-tool-for-your-data-lake-on-aws"},nextItem:{title:"Apache Hudi on AWS Glue",permalink:"/cn/blog/2024/05/19/apache-hudi-on-aws-glue"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://aws.amazon.com/blogs/big-data/use-aws-data-exchange-to-seamlessly-share-apache-hudi-datasets/",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},8432:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Apache Hudi vs. Delta Lake: Choosing the Right Tool for Your Data Lake on AWS",author:"Siladitya Ghosh",category:"blog",image:"/assets/images/blog/2024-05-27-apache-hudi-vs-delta-lake-choosing-the-right-tool-for-your-data-lake-on-aws.png",tags:["blog","apache hudi","delta lake","comparison","medium"]},s=void 0,l={permalink:"/cn/blog/2024/05/27/apache-hudi-vs-delta-lake-choosing-the-right-tool-for-your-data-lake-on-aws",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-05-27-apache-hudi-vs-delta-lake-choosing-the-right-tool-for-your-data-lake-on-aws.mdx",source:"@site/blog/2024-05-27-apache-hudi-vs-delta-lake-choosing-the-right-tool-for-your-data-lake-on-aws.mdx",title:"Apache Hudi vs. Delta Lake: Choosing the Right Tool for Your Data Lake on AWS",description:"Redirecting... please wait!!",date:"2024-05-27T00:00:00.000Z",formattedDate:"May 27, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"delta lake",permalink:"/cn/blog/tags/delta-lake"},{label:"comparison",permalink:"/cn/blog/tags/comparison"},{label:"medium",permalink:"/cn/blog/tags/medium"}],readingTime:.045,truncated:!1,authors:[{name:"Siladitya Ghosh"}],prevItem:{title:"Apache Hudi: A Deep Dive with Python Code Examples",permalink:"/cn/blog/2024/06/07/apache-hudi-a-deep-dive-with-python-code-examples"},nextItem:{title:"Use AWS Data Exchange to seamlessly share Apache Hudi datasets",permalink:"/cn/blog/2024/05/22/use-aws-data-exchange-to-seamlessly-share-apache-hudi-datasets"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://medium.com/@siladityaghosh/apache-hudi-vs-delta-lake-choosing-the-right-tool-for-your-data-lake-on-aws-8b97c66a5a12",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},65944:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"Apache Hudi: A Deep Dive with Python Code Examples",author:"Harsh Daiya",category:"blog",image:"/assets/images/blog/2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.png",tags:["blog","apache hudi","python","pyspark","harshdaiya"]},s=void 0,l={permalink:"/cn/blog/2024/06/07/apache-hudi-a-deep-dive-with-python-code-examples",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.mdx",source:"@site/blog/2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.mdx",title:"Apache Hudi: A Deep Dive with Python Code Examples",description:"Redirecting... please wait!!",date:"2024-06-07T00:00:00.000Z",formattedDate:"June 7, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"python",permalink:"/cn/blog/tags/python"},{label:"pyspark",permalink:"/cn/blog/tags/pyspark"},{label:"harshdaiya",permalink:"/cn/blog/tags/harshdaiya"}],readingTime:.045,truncated:!1,authors:[{name:"Harsh Daiya"}],prevItem:{title:"How to use Apache Hudi with Databricks",permalink:"/cn/blog/2024/06/18/how-to-use-apache-hudi-with-databricks"},nextItem:{title:"Apache Hudi vs. Delta Lake: Choosing the Right Tool for Your Data Lake on AWS",permalink:"/cn/blog/2024/05/27/apache-hudi-vs-delta-lake-choosing-the-right-tool-for-your-data-lake-on-aws"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://blog.harshdaiya.com/apache-hudi-a-deep-dive-with-python-code-examples",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},76535:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=t(58168),n=(t(96540),t(15680)),o=t(9230);const r={title:"How to use Apache Hudi with Databricks",author:"Sagar Lakshmipathy",category:"blog",image:"/assets/images/blog/2024-06-18-how-to-use-apache-hudi-with-databricks.jpeg",tags:["blog","apache hudi","databricks","onehouse"]},s=void 0,l={permalink:"/cn/blog/2024/06/18/how-to-use-apache-hudi-with-databricks",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-06-18-how-to-use-apache-hudi-with-databricks.mdx",source:"@site/blog/2024-06-18-how-to-use-apache-hudi-with-databricks.mdx",title:"How to use Apache Hudi with Databricks",description:"Redirecting... please wait!!",date:"2024-06-18T00:00:00.000Z",formattedDate:"June 18, 2024",tags:[{label:"blog",permalink:"/cn/blog/tags/blog"},{label:"apache hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"databricks",permalink:"/cn/blog/tags/databricks"},{label:"onehouse",permalink:"/cn/blog/tags/onehouse"}],readingTime:.045,truncated:!1,authors:[{name:"Sagar Lakshmipathy"}],prevItem:{title:"What is a Data Lakehouse & How does it Work?",permalink:"/cn/blog/2024/07/11/what-is-a-data-lakehouse"},nextItem:{title:"Apache Hudi: A Deep Dive with Python Code Examples",permalink:"/cn/blog/2024/06/07/apache-hudi-a-deep-dive-with-python-code-examples"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c},g="wrapper";function u(e){let{components:a,...t}=e;return(0,n.yg)(g,(0,i.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)(o.A,{url:"https://www.onehouse.ai/blog/how-to-use-apache-hudi-with-databricks",mdxType:"Redirect"},"Redirecting... please wait!! "))}u.isMDXComponent=!0},78555:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"What is a Data Lakehouse & How does it Work?",excerpt:"Explains the concept of the lakehouse architecture",author:"Dipankar Mazumdar",category:"blog",image:"/assets/images/blog/dlh_1200.png",tags:["data lakehouse","Apache Hudi","Apache Iceberg","Delta Lake","Open Architecture"]},r=void 0,s={permalink:"/cn/blog/2024/07/11/what-is-a-data-lakehouse",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-07-11-what-is-a-data-lakehouse.md",source:"@site/blog/2024-07-11-what-is-a-data-lakehouse.md",title:"What is a Data Lakehouse & How does it Work?",description:"A data lakehouse is a hybrid data architecture that combines the best attributes of data warehouses and data lakes to address their respective limitations. This innovative approach to data management brings the transactional capabilities of data warehouses to cloud-based data lakes, offering scalability at lower costs.",date:"2024-07-11T00:00:00.000Z",formattedDate:"July 11, 2024",tags:[{label:"data lakehouse",permalink:"/cn/blog/tags/data-lakehouse"},{label:"Apache Hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"Apache Iceberg",permalink:"/cn/blog/tags/apache-iceberg"},{label:"Delta Lake",permalink:"/cn/blog/tags/delta-lake"},{label:"Open Architecture",permalink:"/cn/blog/tags/open-architecture"}],readingTime:15.885,truncated:!1,authors:[{name:"Dipankar Mazumdar"}],prevItem:{title:"Understanding Data Lake Change Data Capture",permalink:"/cn/blog/2024/07/30/data-lake-cdc"},nextItem:{title:"How to use Apache Hudi with Databricks",permalink:"/cn/blog/2024/06/18/how-to-use-apache-hudi-with-databricks"}},l={authorsImageUrls:[void 0]},d=[{value:"The Evolution of Data Storage Solutions: How did we go from Warehouses to Lakes to Lakehouses?",id:"the-evolution-of-data-storage-solutions-how-did-we-go-from-warehouses-to-lakes-to-lakehouses",children:[],level:2},{value:"Introducing: Data Lakehouses",id:"introducing-data-lakehouses",children:[],level:2},{value:"Advantages of Data Lakehouses",id:"advantages-of-data-lakehouses",children:[],level:2},{value:"Implementing a Data Lakehouse",id:"implementing-a-data-lakehouse",children:[{value:"Data Ingestion",id:"data-ingestion",children:[],level:3},{value:"Metadata &amp; Transactional Layer",id:"metadata--transactional-layer",children:[],level:3},{value:"Processing Layer",id:"processing-layer",children:[],level:3},{value:"Catalog Layer",id:"catalog-layer",children:[],level:3}],level:2},{value:"Use Cases",id:"use-cases",children:[{value:"Unified Batch &amp; Streaming",id:"unified-batch--streaming",children:[],level:3},{value:"Diverse Analytical Workloads",id:"diverse-analytical-workloads",children:[],level:3},{value:"Cost-Effective Data Management",id:"cost-effective-data-management",children:[],level:3}],level:2},{value:"Real World Examples",id:"real-world-examples",children:[],level:2},{value:"Key Data Lakehouse Technologies",id:"key-data-lakehouse-technologies",children:[{value:"Open Source Solutions",id:"open-source-solutions",children:[{value:"Apache Hudi",id:"apache-hudi",children:[],level:4},{value:"Apache Iceberg",id:"apache-iceberg",children:[],level:4},{value:"Delta Lake",id:"delta-lake",children:[],level:4}],level:3},{value:"Vendor Lakehouse Platforms",id:"vendor-lakehouse-platforms",children:[{value:"Onehouse",id:"onehouse",children:[],level:4},{value:"Databricks",id:"databricks",children:[],level:4},{value:"Snowflake",id:"snowflake",children:[],level:4}],level:3}],level:2},{value:"The Future of Data Lakehouses",id:"the-future-of-data-lakehouses",children:[],level:2},{value:"Conclusion",id:"conclusion",children:[],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...o}=e;return(0,n.yg)(p,(0,i.A)({},c,o,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"A data lakehouse is a hybrid data architecture that combines the best attributes of data warehouses and data lakes to address their respective limitations. This innovative approach to data management brings the transactional capabilities of data warehouses to cloud-based data lakes, offering scalability at lower costs. "),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"/assets/images/blog/dlh_new.png",src:t(33539).A})),(0,n.yg)("p",{align:"center"},"Figure: Data Lakehouse Architecture"),(0,n.yg)("p",null,"The lakehouse architecture supports the management of various data types, such as structured, semi-structured, and unstructured, and caters to a wide range of use cases, including business intelligence, machine learning, and real-time streaming. This flexibility enables businesses to move away from the traditional two-tier architecture\u2014using warehouses for relational workloads and data lakes for machine learning and advanced analytics. As a result, organizations can reduce operational costs and streamline their data strategies by working on a single data store."),(0,n.yg)("h2",{id:"the-evolution-of-data-storage-solutions-how-did-we-go-from-warehouses-to-lakes-to-lakehouses"},"The Evolution of Data Storage Solutions: How did we go from Warehouses to Lakes to Lakehouses?"),(0,n.yg)("p",null,"Historically, organizations have been investing in building centralized and scalable data architectures to enable more data access and to support different types of analytical workloads. As demand for these workloads has grown, data architectures have evolved to address the complex needs of modern data processing and storage."),(0,n.yg)("p",null,"Data warehouses were among the first to serve as centralized repositories for structured workloads, allowing organizations to derive historical insights from disparate data sources. However, they also introduce challenges, including proprietary storage formats that can result in lock-in issues, and limited support for analytical workloads, particularly with unstructured data like machine learning."),(0,n.yg)("p",null,"Data lakes emerged as the next generation of analytics architectures, enabling organizations to scale storage and compute independently, thereby optimizing resources and enhancing cost efficiency. They support storing all types of data\u2014structured, semi-structured, and unstructured\u2014in low-cost storage systems using open file formats like ",(0,n.yg)("a",{parentName:"p",href:"https://parquet.apache.org"},"Apache Parquet")," and ",(0,n.yg)("a",{parentName:"p",href:"https://orc.apache.org"},"Apache ORC"),". Although data lakes offer flexibility with their schema-on-read approach, they lack transactional capabilities (ACID characteristics) and often face challenges related to data quality and governance."),(0,n.yg)("p",null,"The challenges presented by these two data management approaches led to the development of a new architecture called data lakehouse. A lakehouse brings the transactional capabilities of database management systems (DBMS) to scalable data lakes, enabling running various types of workloads on open storage formats."),(0,n.yg)("h2",{id:"introducing-data-lakehouses"},"Introducing: Data Lakehouses"),(0,n.yg)("p",null,"A data lakehouse combines the reliability and performance of data warehouses with the scalability and cost-effectiveness of data lakes. This combined approach enables features such as time-travel, indexing, schema evolution, and performance optimization capabilities on openly accessible formats."),(0,n.yg)("p",null,"Specifically, a lakehouse architecture is characterized by the following attributes."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Open Data Architecture"),": A lakehouse stores data in open storage formats. This allows various analytical workloads to be run by different engines (from multiple vendors) on the same data, preventing lock-in to proprietary formats.")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Support for Varied Data Types & Workloads"),": Data lakehouses accommodate a diverse range of data types\u2014including structured, semi-structured, and unstructured\u2014and are therefore equipped to handle various analytical workloads, such as business intelligence, machine learning, and real-time analytics. ")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Transactional support"),": Data lakehouses enhance reliability and consistency by providing ACID guarantees in transactions, such as INSERT or UPDATE, akin to those in an RDBMS-OLAP system. This ensures safe, concurrent reads and writes.")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Less data copies"),": A data lakehouse minimizes data duplication since the compute engine can directly access data from open storage formats.")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Schema management"),": Data lakehouses ensure that a specific schema is adhered to when writing new data into the storage. They also facilitate schema evolution over time without the need to rewrite the entire table, thus reducing storage and operational costs.")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Data Quality and Governance"),": Lakehouses ensure data integrity and incorporate robust governance and auditing mechanisms. These features uphold high data quality standards, facilitate regulatory compliance (such as GDPR), and enable secure data management practices."))),(0,n.yg)("p",null,"A data lakehouse architecture consists of six technical components that are modular, offering the flexibility to select and combine the best technologies based on specific requirements."),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Lake Storage"),": The storage is where files from various operational systems land after ingestion through ETL/ELT processes. Cloud object stores such as Amazon S3, Azure Blob, and Google Cloud Storage support any type of data and can scale to virtually unlimited volumes. Their cost-effectiveness is a significant advantage over traditional data warehouse storage costs.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"File Format: File Format"),": In a lakehouse architecture, file formats like Apache Parquet or ORC store the actual raw data on object storage. These open formats enable multiple engines to consume the data for various workloads. Being typically column-oriented, they offer significant advantages in data reading.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Table Format"),": A key component of a lakehouse architecture is the table format, which is open in nature and acts as a metadata layer above file formats like Apache Parquet. This layer abstracts the complexity of the physical data structure by defining a schema on top of immutable data files. It allows different engines to concurrently read and write on the same dataset, supporting ACID-based transactions. Table formats like ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org"},"Apache Hudi"),", ",(0,n.yg)("a",{parentName:"p",href:"https://iceberg.apache.org"},"Apache Iceberg"),", and ",(0,n.yg)("a",{parentName:"p",href:"https://delta.io"},"Delta Lake")," bring essential features such as schema evolution, partitioning, and time travel.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Storage Engine"),": The storage engine in a lakehouse orchestrates essential data management tasks including clustering, compaction, cleaning, and indexing to streamline data organization in cloud object storages for improved query performance. Both open source and proprietary lakehouse platforms are equipped with native storage engines that enhance these capabilities, optimizing the storage layout effectively.")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Catalog"),": Often referred to as a metastore, the catalog is a crucial component of the lakehouse architecture that facilitates efficient search and discovery by tracking all tables and their metadata. It records table names, schemas (column names and types), and references to each table's specific metadata (table format).")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Compute Engine"),": The compute engine in a lakehouse processes data and ensures efficient read and write performance. It interacts with data using read and write APIs provided by table formats. Compute engines are tailored to specific workloads, with options such as ",(0,n.yg)("a",{parentName:"p",href:"https://trino.io"},"Trino")," and ",(0,n.yg)("a",{parentName:"p",href:"https://prestodb.io"},"Presto")," for low-latency ad hoc SQL, ",(0,n.yg)("a",{parentName:"p",href:"https://flink.apache.org"},"Apache Flink")," for streaming, and ",(0,n.yg)("a",{parentName:"p",href:"https://spark.apache.org"},"Apache Spark")," for machine learning tasks."))),(0,n.yg)("h2",{id:"advantages-of-data-lakehouses"},"Advantages of Data Lakehouses"),(0,n.yg)("p",null,"A lakehouse architecture, characterized by its open data storage formats and cost-effective options, offers numerous advantages. Here are some key benefits:"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Attributes"),(0,n.yg)("th",{parentName:"tr",align:null},"Description"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Open Data Foundation"),(0,n.yg)("td",{parentName:"tr",align:null},"Data in a lakehouse is stored in open file formats like Apache Parquet and table formats such as Apache Hudi, Iceberg, or Delta Lake. This allows various engines to concurrently work on the same data, enhancing accessibility and compatibility.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Unified Data Platform"),(0,n.yg)("td",{parentName:"tr",align:null},"Lakehouses combine the functionalities of data warehouses and lakes into a single platform, supporting both types of workloads efficiently. This integration simplifies data management and accelerates analytics processes.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Centralized Data Repository for Diverse Data Types"),(0,n.yg)("td",{parentName:"tr",align:null},"A lakehouse architecture can store and manage structured, semi-structured, and unstructured data, serving different types of analytical workloads.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Cost Efficiency"),(0,n.yg)("td",{parentName:"tr",align:null},"Using low-cost cloud storage options and reducing the need for managing multiple systems significantly lowers overall engineering and ETL costs.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Performance and Scalability"),(0,n.yg)("td",{parentName:"tr",align:null},"Lakehouses allow independent scaling of storage and compute resources, which can be adjusted based on demand, ensuring high concurrency and cost-effective scalability.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Enhanced Query Performance"),(0,n.yg)("td",{parentName:"tr",align:null},"Lakehouse\u2019s storage engine component optimizes data layout in formats like Parquet and ORC to offer high performance comparable to traditional data warehouses, on large datasets.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Data Governance and Management"),(0,n.yg)("td",{parentName:"tr",align:null},"Lakehouses centralize data storage and management, streamlining the deployment of governance policies and security measures. This consolidation makes it easier to monitor, control, and secure data.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Improved Data Quality and Consistency"),(0,n.yg)("td",{parentName:"tr",align:null},"Lakehouses enforce strict schema adherence and provide transactional consistency, which minimizes write job failures and ensures data reliability.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Support for various Compute Engines"),(0,n.yg)("td",{parentName:"tr",align:null},"A lakehouse architecture supports SQL-based engines, ML tools, and streaming engines, making it versatile for handling diverse analytical demands on a single data store.")))),(0,n.yg)("h2",{id:"implementing-a-data-lakehouse"},"Implementing a Data Lakehouse"),(0,n.yg)("p",null,"The modular and open design of data lakehouse architecture allows for selection of best-of-breed engines and tools according to specific requirements. Therefore, the implementation of a lakehouse can vary based on the use case. This section outlines common considerations for implementing a lakehouse architecture. Given the variability in complexity (workloads, security, etc.) and tool stack, large-scale implementations may require tailored approaches."),(0,n.yg)("h3",{id:"data-ingestion"},"Data Ingestion"),(0,n.yg)("p",null,"The first phase in a lakehouse architecture involves extracting and loading data into a cloud-based low cost data lake such as ",(0,n.yg)("a",{parentName:"p",href:"https://aws.amazon.com/s3/"},"Amazon S3"),', where it lands in its raw format (Parquet files). This approach utilizes the "schema-on-read" method, which means there\'s no need to process data immediately upon arrival. Once the data is in place, transformation logic can be applied to shift towards a "schema-on-write" setup, which organizes the data for specific analytical workloads such as ad hoc SQL queries or machine learning.'),(0,n.yg)("h3",{id:"metadata--transactional-layer"},"Metadata & Transactional Layer"),(0,n.yg)("p",null,"To enable transactional capabilities, Apache Hudi, Apache Iceberg or Delta Lake can be chosen as the table format. They provide a robust metadata layer with a table-like schema atop the physical data files in the object store. Together with the storage engine, they bring in data optimization strategies to maintain fast and efficient query performance. The metadata layer also facilitates capabilities such as time-travel querying, version rollbacks, and schema evolution akin to a traditional data warehouse."),(0,n.yg)("h3",{id:"processing-layer"},"Processing Layer"),(0,n.yg)("p",null,"The compute engine is a crucial component in a lakehouse architecture that processes the data files managed by the table format. Depending on the specific workload, SQL-based distributed query engines like Presto or Trino can be used for ad-hoc interactive analytics, or Apache Spark for distributed ETL tasks. Lakehouse table formats provide several optimizations such as indexes, and statistics, along with data layout optimizations including clustering, compaction, and Z-ordering. These enable the compute engines to achieve performance comparable to traditional data warehouses."),(0,n.yg)("h3",{id:"catalog-layer"},"Catalog Layer"),(0,n.yg)("p",null,"The catalog layer in a lakehouse architecture is responsible for tracking all tables and maintaining essential metadata. It ensures that data is easily accessible to query engines, supporting efficient data management, accessibility, and governance. Options for catalog implementation include Unity Catalog, AWS Glue, Hive Metastore, and file system-based ones. This layer plays a key role in upholding data quality and governance standards by establishing policies for data validation, security measures, and compliance protocols."),(0,n.yg)("h2",{id:"use-cases"},"Use Cases"),(0,n.yg)("p",null,"A Lakehouse architecture is used for a multitude of use cases. Here are some prominent examples."),(0,n.yg)("h3",{id:"unified-batch--streaming"},"Unified Batch & Streaming"),(0,n.yg)("p",null,"Traditional analytics architectures often separate real-time and batch storage, using specialized data stores for real-time insights and data lakes for delayed batch processing. Lakehouse platforms bridge this divide by introducing streaming capabilities to data lakes, allowing data ingestion within minutes and the creation of faster incremental pipelines. This integration reduces data freshness issues and eliminates the need for significant upfront infrastructure investments, making it a scalable and cost-effective solution for complex analytics."),(0,n.yg)("h3",{id:"diverse-analytical-workloads"},"Diverse Analytical Workloads"),(0,n.yg)("p",null,"Lakehouse architecture supports various data types\u2014structured, semi-structured, and unstructured\u2014enabling users to run both BI and ML workloads on the same dataset without the need for costly data duplication or movement. This unified approach allows data scientists and analysts to easily access and manipulate data for training ML models, deploying AI algorithms, and conducting in-depth BI analysis. By eliminating the need to create and maintain separate BI extracts and cubes, lakehouses reduce both storage and compute costs while maintaining a simple, self-service model for end-users. As a result, organizations can streamline their data operations and enhance analytical flexibility, making it easier to derive insights across different domains."),(0,n.yg)("h3",{id:"cost-effective-data-management"},"Cost-Effective Data Management"),(0,n.yg)("p",null,"Lakehouses leverage the low-cost storage of cloud-based data lakes while providing sophisticated data management and querying capabilities similar to data warehouses. This dual advantage makes it an economical choice for startups and enterprises alike that need to manage costs without compromising on analytics capabilities. Additionally, the open, unified architecture of a lakehouse eliminates non-monetary costs, such as running and maintaining ETL pipelines and creating multiple data copies, further streamlining operations."),(0,n.yg)("h2",{id:"real-world-examples"},"Real World Examples"),(0,n.yg)("p",null,"ByteDance, Notion and Halodoc are some of the examples of how lakehouse architecture is being adopted in the industry. ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/blog/2021/09/01/building-eb-level-data-lake-using-hudi-at-bytedance/"},"ByteDance")," has built an exabyte-level data lakehouse using Apache Hudi to enhance their recommendation systems. The implementation of Hudi's Merge-on-read (MOR) tables, indexing, and Multi-Version Concurrency Control (MVCC) features allow ByteDance to provide real-time machine learning capabilities, providing instant and relevant recommendations."),(0,n.yg)("p",null,(0,n.yg)("a",{parentName:"p",href:"https://www.notion.so/blog/building-and-scaling-notions-data-lake"},"Notion")," scaled its data infrastructure by building an in-house lakehouse to handle rapid data growth and meet product demands, especially for Notion AI. The architecture uses S3 for storage, Kafka and Debezium for data ingestion, and Apache Hudi for efficient data management. This setup resulted in significant cost savings, faster data ingestion, and enhanced capabilities for analytics and product development."),(0,n.yg)("p",null,"Similarly, ",(0,n.yg)("a",{parentName:"p",href:"https://blogs.halodoc.io/lake-house-architecture-halodoc-data-platform-2-0/"},"Halodoc's")," adoption of a lakehouse architecture allows them to enhance healthcare services by enabling real-time processing and analytics. This architecture helps Halodoc tackle challenges associated with managing vast healthcare data volumes, thus improving patient care through faster, more accurate decision-making and supporting both batch and stream processing crucial for timely health interventions."),(0,n.yg)("h2",{id:"key-data-lakehouse-technologies"},"Key Data Lakehouse Technologies"),(0,n.yg)("h3",{id:"open-source-solutions"},"Open Source Solutions"),(0,n.yg)("h4",{id:"apache-hudi"},"Apache Hudi"),(0,n.yg)("p",null,"Apache Hudi is an open source ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/hudi_stack"},"transactional data lakehouse platform")," built around a database kernel. It provides table-level abstractions over open file formats like Apache Parquet and ORC thereby delivering core warehouse and database functionalities directly in the data lake and supporting transactional capabilities such as updates and deletes. "),(0,n.yg)("p",null,"Hudi also incorporates critical table services tightly integrated with its database kernel. These services can be run automatically, managing aspects like table bookkeeping, metadata, and storage layouts across both ingested and derived data. These capabilities, combined with specific platform services (ingestion, catalog sync tool, admin CLI, etc.) in Hudi, elevates its role from merely a table format to a comprehensive and robust data lakehouse platform. Apache Hudi has a broad support for various data sources and query engines, such as Apache Spark, Apache Flink, AWS Athena, Presto, Trino and StarRocks."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Hudi Stack",src:t(78985).A})),(0,n.yg)("p",{align:"center"},"Figure: Apache Hudi Architectural stack"),(0,n.yg)("p",null,"Below are some of the key features of Hudi\u2019s lakehouse platform."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Mutability Support"),": Hudi enables quick updates and deletions through an efficient, pluggable ",(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/docs/indexing/"},"indexing")," mechanism supporting workloads such as streaming, out-of-order data, and data deduplication."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Incremental Processing"),": Hudi optimizes for efficiency by enabling ",(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/blog/2020/08/18/hudi-incremental-processing-on-data-lakes/"},"incremental processing")," of new data. This feature allows you to replace traditional batch processing pipelines with more dynamic, incremental streaming, enhancing data ingestion and reducing processing times for analytical workloads."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"ACID Transactions"),": Hudi brings ACID transactional guarantees to data lakes, offering consistent and atomic writes along with different ",(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/docs/concurrency_control"},"concurrency control")," techniques essential for managing longer-running transactions."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Time Travel"),": Hudi includes capabilities for ",(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/docs/sql_queries#time-travel-query"},"querying")," historical data, allowing users to roll back to previous versions of tables to debug or audit changes. "),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Comprehensive Table Management"),": Hudi brings automated table services that continuously orchestrate ",(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/docs/clustering"},"clustering"),", ",(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/docs/compaction"},"compaction"),", ",(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/docs/hoodie_cleaner"},"cleaning"),", and indexing, ensuring high performance for analytical queries."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Query Performance Optimization"),": Hudi introduces a novel ",(0,n.yg)("a",{parentName:"li",href:"https://www.onehouse.ai/blog/introducing-multi-modal-index-for-the-lakehouse-in-apache-hudi"},"multi-modal indexing")," subsystem that speeds up write transactions and enhances query performance, especially in large or wide tables."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Schema Evolution and Enforcement"),": With Hudi, you can ",(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/docs/schema_evolution"},"adapt the schema")," of your tables as your data evolves, enhancing pipeline resilience by quickly identifying and preventing potential data integrity issues.")),(0,n.yg)("h4",{id:"apache-iceberg"},"Apache Iceberg"),(0,n.yg)("p",null,"Apache Iceberg is a table format designed for managing large-scale analytical datasets in cloud data lakes, facilitating a lakehouse architecture. Technically, Iceberg serves as a table format specification, providing APIs and libraries that enable compute engines to interact with tables according to this specification. It introduces features essential for data lake workloads, including schema evolution, hidden partitioning, ACID-compliant transactions, and time travel capabilities. These features ensure robust data management, akin to that found in traditional data warehouses."),(0,n.yg)("h4",{id:"delta-lake"},"Delta Lake"),(0,n.yg)("p",null,"Delta Lake is another open source table format that enables building a lakehouse architecture on top of cloud data lakes. By offering an ACID-compliant layer that operates over cloud object stores, Delta Lake addresses the typical performance and consistency issues associated with data lakes. It enables features like schema enforcement and evolution, time travel, efficient metadata handling and DML operations, which are crucial for handling large-scale workloads on data lakes effectively."),(0,n.yg)("h3",{id:"vendor-lakehouse-platforms"},"Vendor Lakehouse Platforms"),(0,n.yg)("h4",{id:"onehouse"},"Onehouse"),(0,n.yg)("p",null,(0,n.yg)("a",{parentName:"p",href:"https://www.onehouse.ai/product"},"Onehouse")," offers a universal data platform that streamlines data ingestion and transformation into a lakehouse architecture. It eliminates lakehouse table format friction by working seamlessly with Apache Hudi, Apache Iceberg and Delta Lake tables (thanks to ",(0,n.yg)("a",{parentName:"p",href:"https://xtable.apache.org"},"Apache XTable"),"). The platform supports continuous data ingestion from diverse sources, including events streams such as Kafka, databases and cloud storage, enabling real-time data updates while ensuring data integrity through automated table optimizations and rigorous data quality measures. Onehouse provides a fully managed ingestion pipeline with serverless autoscaling and cost-efficient infrastructure. With its flexible querying capabilities across multiple engines and formats, Onehouse empowers organizations to efficiently manage and utilize their data."),(0,n.yg)("h4",{id:"databricks"},"Databricks"),(0,n.yg)("p",null,"The ",(0,n.yg)("a",{parentName:"p",href:"https://www.databricks.com/product/data-intelligence-platform"},"Databricks")," Lakehouse Platform unifies data engineering, machine learning, and analytics on a single platform. It combines the reliability, governance, and performance of data warehouses with the scalability, flexibility, and low cost of data lakes. By offering Delta Lake as its foundational storage layer and ",(0,n.yg)("a",{parentName:"p",href:"https://docs.delta.io/latest/delta-uniform.html"},"UniFormat")," for interoperability between Apache Iceberg and Apache Hudi, the platform supports ACID transactions, scalable metadata handling, and unifies batch and streaming data processing. "),(0,n.yg)("h4",{id:"snowflake"},"Snowflake"),(0,n.yg)("p",null,"The ",(0,n.yg)("a",{parentName:"p",href:"https://www.snowflake.com/en/data-cloud/platform/"},"Snowflake")," Data Cloud provides a unified, fully managed platform for seamless data management and advanced analytics capabilities. It offers near-infinite scalability, robust security, and native support for diverse data types and SQL workloads. Snowflake currently supports Apache Iceberg as the open table format to facilitate a lakehouse architecture. This integration allows users to leverage Iceberg's rich table metadata and Parquet file storage within Snowflake's ecosystem, enabling seamless data handling, multi-table transactions, dynamic data masking, and row-level security, all while using customer-supplied cloud storage."),(0,n.yg)("h2",{id:"the-future-of-data-lakehouses"},"The Future of Data Lakehouses"),(0,n.yg)("p",null,"The future of data lakehouses is shaped by their truly open data architecture, which meets the ongoing need for flexible, scalable, and cost-effective data management solutions. They offer a unified platform capable of efficiently handling both streaming and batch workloads, supporting a wide array of analytical workloads including BI and ML. With the rapid advancement of artificial intelligence, including generative AI, there is an increasing demand for robust platforms that provide the foundation for building and deploying powerful models. Lakehouse architecture rises to this challenge, offering a solid base for the evolving demands of modern data analytics."),(0,n.yg)("h2",{id:"conclusion"},"Conclusion"),(0,n.yg)("p",null,"The data lakehouse architecture utilizes an open data foundation to blend the best features of data lakes and warehouses, establishing a versatile platform that effectively handles a range of analytical workloads. This architecture marries cost-effective data management with robust performance, offering a cohesive system for both batch and streaming data processes. By enabling organizations to work on a single data store, this approach not only simplifies management but also equips businesses to swiftly integrate new technologies and adapt to evolving market demands. Additionally, by supporting diverse data types and analytical workloads, the lakehouse framework eliminates the need for a two-tier architecture, which helps save costs and enhances the efficiency of data teams."))}g.isMDXComponent=!0},36637:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Understanding Data Lake Change Data Capture",excerpt:"Explains the concept of CDC in data lakes",author:"Sagar Lakshmipathy",category:"blog",image:"/assets/images/blog/data-lake-cdc/hudi-cdc.jpg",tags:["Data Lake","Apache Hudi","Change Data Capture","CDC"]},r=void 0,s={permalink:"/cn/blog/2024/07/30/data-lake-cdc",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-07-30-data-lake-cdc.md",source:"@site/blog/2024-07-30-data-lake-cdc.md",title:"Understanding Data Lake Change Data Capture",description:"Introduction",date:"2024-07-30T00:00:00.000Z",formattedDate:"July 30, 2024",tags:[{label:"Data Lake",permalink:"/cn/blog/tags/data-lake"},{label:"Apache Hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"Change Data Capture",permalink:"/cn/blog/tags/change-data-capture"},{label:"CDC",permalink:"/cn/blog/tags/cdc"}],readingTime:8.485,truncated:!1,authors:[{name:"Sagar Lakshmipathy"}],prevItem:{title:"Column File Formats: How Hudi Leverages Parquet and ORC ",permalink:"/cn/blog/2024/07/31/hudi-file-formats"},nextItem:{title:"What is a Data Lakehouse & How does it Work?",permalink:"/cn/blog/2024/07/11/what-is-a-data-lakehouse"}},l={authorsImageUrls:[void 0]},d=[{value:"Introduction",id:"introduction",children:[{value:"Data Lake",id:"data-lake",children:[],level:3},{value:"Change Data Capture",id:"change-data-capture",children:[],level:3}],level:2},{value:"CDC architecture pattern",id:"cdc-architecture-pattern",children:[{value:"Common CDC Components",id:"common-cdc-components",children:[{value:"Change Detection",id:"change-detection",children:[{value:"Timestamp-based / Query-based:",id:"timestamp-based--query-based",children:[],level:5},{value:"Pros:",id:"pros",children:[],level:5},{value:"Cons:",id:"cons",children:[],level:5},{value:"Trigger-based:",id:"trigger-based",children:[],level:5},{value:"Pros:",id:"pros-1",children:[],level:5},{value:"Cons:",id:"cons-1",children:[],level:5},{value:"Log-based:",id:"log-based",children:[],level:5},{value:"Pros:",id:"pros-2",children:[],level:5},{value:"Cons:",id:"cons-2",children:[],level:5}],level:4},{value:"Data Extraction",id:"data-extraction",children:[],level:4},{value:"Data Transformation",id:"data-transformation",children:[],level:4},{value:"Data Loading",id:"data-loading",children:[],level:4}],level:3}],level:2},{value:"Why Combine CDC with Data Lakes?",id:"why-combine-cdc-with-data-lakes",children:[{value:"Flexibility",id:"flexibility",children:[],level:3},{value:"Cost-effective",id:"cost-effective",children:[],level:3},{value:"Streamlined ETL Processes",id:"streamlined-etl-processes",children:[],level:3}],level:2},{value:"Designing a CDC Architecture",id:"designing-a-cdc-architecture",children:[{value:"Solution:",id:"solution",children:[{value:"Revised architecture:",id:"revised-architecture",children:[],level:4}],level:3}],level:2},{value:"Implementation Blogs/Guides",id:"implementation-blogsguides",children:[],level:2},{value:"Conclusion",id:"conclusion",children:[],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...o}=e;return(0,n.yg)(p,(0,i.A)({},c,o,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("h2",{id:"introduction"},"Introduction"),(0,n.yg)("p",null,"In data management, two concepts have garnered significant attention: data lakes and change data capture (CDC)."),(0,n.yg)("h3",{id:"data-lake"},"Data Lake"),(0,n.yg)("p",null,"Data lakes serve as vast repositories that store raw data in its native format until needed for analytics."),(0,n.yg)("h3",{id:"change-data-capture"},"Change Data Capture"),(0,n.yg)("p",null,"Change Data Capture (CDC) is a technique used to identify and capture data changes, ensuring that the data remains fresh and consistent across various systems."),(0,n.yg)("p",null,"Combining CDC with data lakes can significantly simplify data management by addressing several challenges commonly faced by ETL pipelines delivering data from transactional databases to analytical databases. These include maintaining data freshness, ensuring consistency, and improving efficiency in data handling. This article will explore the integration between data lakes and CDC, their benefits, implementation methods, key technologies and tools involved, best practices, and how to choose the right tools for your needs."),(0,n.yg)("h2",{id:"cdc-architecture-pattern"},"CDC architecture pattern"),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"CDC Architecture",src:t(83444).A})),(0,n.yg)("h3",{id:"common-cdc-components"},"Common CDC Components"),(0,n.yg)("h4",{id:"change-detection"},"Change Detection"),(0,n.yg)("h5",{id:"timestamp-based--query-based"},"Timestamp-based / Query-based:"),(0,n.yg)("p",null,"This method relies on table schemas to include a column to indicate when it was previously modified, i.e. LAST_UPDATED etc. Whenever the source system is updated, the LAST_UPDATED column should be designed to get updated with the current timestamp. This column can then be queried by consumer applications to get the records, and process the records that have been previously updated."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Timestamp-based CDC",src:t(46886).A})),(0,n.yg)("h5",{id:"pros"},"Pros:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Its simple to implement and use")),(0,n.yg)("h5",{id:"cons"},"Cons:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"If source applications did not have the timestamp columns, the database design needs to be changed to include it"),(0,n.yg)("li",{parentName:"ul"},"Only supports soft deletes and not DELETE operations in the source table. This is because, once a DELETE operation is performed on the source database the record is removed and the consumer applications cannot track it automatically without the help of a custom log table or an audit trail."),(0,n.yg)("li",{parentName:"ul"},"As there is no metadata to track, schema evolution scenarios require custom implementations to track the source database schema changes and update the target database schema appropriately. This is complex and hard to implement.")),(0,n.yg)("h5",{id:"trigger-based"},"Trigger-based:"),(0,n.yg)("p",null,"In a trigger-based CDC design, database triggers are used to detect changes in the data and are used to update target tables accordingly. This method involves having trigger functions automatically executed to capture and store any changes from the source table in the target table; these target tables are commonly referred to as ",(0,n.yg)("strong",{parentName:"p"},"shadow tables")," or ",(0,n.yg)("strong",{parentName:"p"},"change tables"),". For example, in this method, stored procedures are triggered when there are specific events in the source database, such as INSERTs, UPDATEs, DELETEs."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Trigger-based CDC",src:t(31479).A})),(0,n.yg)("h5",{id:"pros-1"},"Pros:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Simple to implement"),(0,n.yg)("li",{parentName:"ul"},"Triggers are supported natively by most database engines")),(0,n.yg)("h5",{id:"cons-1"},"Cons:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Maintenance overhead - requires maintaining separate trigger for each operation in each table"),(0,n.yg)("li",{parentName:"ul"},"Performance overhead - in a highly concurrent database, addition of these triggers may significantly impact performance"),(0,n.yg)("li",{parentName:"ul"},"Trigger-based CDC does not inherently provide mechanisms for informing downstream applications about schema changes, complicating consumer-side adaptations.")),(0,n.yg)("h5",{id:"log-based"},"Log-based:"),(0,n.yg)("p",null,"Databases maintain transaction logs, a file that records all transactions and database modifications made by each transaction. By reading this log, CDC tools can identify what data has been changed, when it changed and the type of change. Because this method reads changes directly from the database transaction log, ensuring low-latency and minimal impact on database performance. "),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Log-based CDC",src:t(76589).A})),(0,n.yg)("h5",{id:"pros-2"},"Pros:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Supports all kinds of database transactions i.e. INSERTs, UPDATEs, DELETEs"),(0,n.yg)("li",{parentName:"ul"},"Minimal performance impact on the source/operational databases"),(0,n.yg)("li",{parentName:"ul"},"No schema changes required in source databases"),(0,n.yg)("li",{parentName:"ul"},"With a table format support, i.e. Apache Hudi, schema evolution ",(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/docs/schema_evolution/"},"can be supported"))),(0,n.yg)("h5",{id:"cons-2"},"Cons:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"No standardization in publishing the transactional logs between databases - this results in complex design and development overhead to implement support for different database vendors")),(0,n.yg)("h4",{id:"data-extraction"},"Data Extraction"),(0,n.yg)("p",null,"Once changes are detected, the CDC system extracts the relevant data. This includes the type of operation (insert, update, delete), the affected rows, and the before-and-after state of the data if applicable."),(0,n.yg)("h4",{id:"data-transformation"},"Data Transformation"),(0,n.yg)("p",null,"Extracted data often needs to be transformed before it can be used. This might include converting data formats, applying business rules, or enriching the data with additional context."),(0,n.yg)("h4",{id:"data-loading"},"Data Loading"),(0,n.yg)("p",null,"The transformed data is then loaded into the target system. This could be another database, a data warehouse, a data lake, or a real-time analytics platform. The loading process ensures that the target system reflects the latest state of the source database."),(0,n.yg)("h2",{id:"why-combine-cdc-with-data-lakes"},"Why Combine CDC with Data Lakes?"),(0,n.yg)("h3",{id:"flexibility"},"Flexibility"),(0,n.yg)("p",null,"In general, data lakes offer more flexibility at a lower cost, because of its tendency to support storing any type of data i.e. unstructured, semi-structured and structured data while data warehouses typically only support structured and in some cases semi-structured. This flexibility allows users to maintain a single source of truth and access the same dataset from different query engines. For example, the dataset stored in S3, can be queried using Redshift Spectrum and Amazon Athena."),(0,n.yg)("h3",{id:"cost-effective"},"Cost-effective"),(0,n.yg)("p",null,"Data lakes, when compared to data warehouses, are generally cheaper in terms of storage costs as the volume grows in time. This allows users to implement a medallion architecture which involves storing a huge volume of data in three different levels i.e. bronze, silver and gold layer tables. Over time, data lake users typically implement tiered storage which further reduces storage cost by moving infrequently accessed data to colder storage systems. In a traditional data warehouse implementation, storage costs will be higher to maintain different levels of data and will continue growing as the source database grows."),(0,n.yg)("h3",{id:"streamlined-etl-processes"},"Streamlined ETL Processes"),(0,n.yg)("p",null,"CDC simplifies the Extract, Transform, Load (ETL) processes by continuously capturing and applying changes to the data lake. This streamlining reduces the complexity and resource intensity of traditional ETL operations, often involving bulk data transfers and significant processing overhead. By only dealing with data changes, CDC makes the process more efficient and reduces the load on source systems."),(0,n.yg)("p",null,"For organizations using multiple ingestion pipelines, for example a combination of CDC pipelines, ERP data ingestion, IOT sensor data, having a common storage layer may simplify data processing while giving you the opportunity to build unified tables combining data from different sources."),(0,n.yg)("h2",{id:"designing-a-cdc-architecture"},"Designing a CDC Architecture"),(0,n.yg)("p",null,"For organizations with specific needs or unique data environments, developing custom CDC solutions is a common practice, especially with open source tools/frameworks. These solutions offer flexibility and can be tailored to meet the exact requirements of the business. However, developing custom CDC solutions requires significant expertise and resources, making it a viable option for organizations with complex data needs. Examples include Debezium/Airbyte combined Apache Kafka."),(0,n.yg)("h3",{id:"solution"},"Solution:"),(0,n.yg)("p",null,"Apache Hudi is an open-source framework designed to streamline incremental data processing and data pipeline development. It efficiently handles business requirements such as data lifecycle management and enhances data quality.\nStarting with Hudi 0.13.0, ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/releases/release-0.13.0#change-data-capture"},"the CDC feature was introduced natively"),", allowing logging before and after images of the changed records, along with the associated write operation type."),(0,n.yg)("p",null,"This enables users to"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},'Perform record-level insert, update, and delete for privacy regulations and simplified pipelines \u2013 for privacy regulations like GDPR and CCPA, companies need to perform record-level updates and deletions to comply with individuals\' rights such as the "right to be forgotten" or consent changes. Without support for record-level updates/deletes this required custom solutions to track individual changes and rewrite large data sets for minor updates. With Apache Hudi, you can use familiar operations (insert, update, upsert, delete), and Hudi will track transactions and make granular changes in the data lake, simplifying your data pipelines.'),(0,n.yg)("li",{parentName:"ul"},"Simplified and efficient file management and near real-time data access \u2013 Streaming IoT and ingestion pipelines need to handle data insertion and update events without creating performance issues due to numerous small files. Hudi automatically tracks changes and merges files to maintain optimal sizes, eliminating the need for custom solutions to manage and rewrite small files."),(0,n.yg)("li",{parentName:"ul"},"Simplify CDC data pipeline development \u2013 meaning users can store data in the data lake using open storage formats, while integrations with Presto, Apache Hive, Apache Spark, and various data catalogs give you near real-time access to updated data using familiar tools.")),(0,n.yg)("h4",{id:"revised-architecture"},"Revised architecture:"),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"CDC Architecture with Apache Hudi",src:t(32435).A})),(0,n.yg)("p",null,"In this architecture, with the addition of the data processing layer, we have added two important components"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"A data catalog")," \u2013 acting as a metadata repository for all your data assets across various data sources. This component is updated by the writer i.e. Spark/Flink and is used by the readers i.e. Presto/Trino. Common examples include AWS Glue Catalog, Hive Metastore and Unity Catalog."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"A schema registry")," \u2013 acting centralized repository for managing and validating schemas. It decouples schemas from producers and consumers, which allows applications to serialize and deserialize messages. Schema registry is also important to ensure data quality. Common examples include, Confluent schema registry, Apicurio schema registry and Glue schema registry."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Apache Hudi")," \u2013 acting as a platform used in conjunction with Spark/Flink which refers to the schema registry and writes to the data lake and simultaneously catalogs the data to the data catalog.")),(0,n.yg)("p",null,"The tables written by Spark/Flink + Hudi can now be queried from popular query engines such as Presto, Trino, Amazon Redshift, and Spark SQL."),(0,n.yg)("h2",{id:"implementation-blogsguides"},"Implementation Blogs/Guides"),(0,n.yg)("p",null,"Over time, the Apache Hudi community has written great step-by-step blogs/guides to help implement Change Data Capture architectures. Some of these blogs can be referred to ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/blog/tags/cdc"},"here"),"."),(0,n.yg)("h2",{id:"conclusion"},"Conclusion"),(0,n.yg)("p",null,"Combining data lakes with Change Data Capture (CDC) techniques offers a powerful solution for addressing the challenges associated with maintaining data freshness, consistency, and efficiency in ETL pipelines."),(0,n.yg)("p",null,"Several methods exist for implementing CDC, including timestamp-based, trigger-based, and log-based approaches, each with its own advantages and drawbacks. Log-based CDC, in particular, stands out for its minimal performance impact on source databases and support for various transactions, though it requires handling different database vendors' transaction log formats."),(0,n.yg)("p",null,"Using tools like Apache Hudi can significantly enhance the CDC process by streamlining incremental data processing and data pipeline development. Hudi provides efficient storage management, supports record-level operations for privacy regulations, and offers near real-time access to data. It also simplifies the management of streaming data and ingestion pipelines by automatically tracking changes and optimizing file sizes, thereby reducing the need for custom solutions."))}g.isMDXComponent=!0},70435:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var i=t(58168),n=(t(96540),t(15680));const o={title:"Column File Formats: How Hudi Leverages Parquet and ORC ",excerpt:"Explains how Hudi uses Parquet and ORC",author:"Albert Wong",category:"blog",image:"/assets/images/blog/hudi-parquet-orc.jpg",tags:["Data Lake","Apache Hudi","Apache Parquet","Apache ORC"]},r=void 0,s={permalink:"/cn/blog/2024/07/31/hudi-file-formats",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-07-31-hudi-file-formats.md",source:"@site/blog/2024-07-31-hudi-file-formats.md",title:"Column File Formats: How Hudi Leverages Parquet and ORC ",description:"Introduction",date:"2024-07-31T00:00:00.000Z",formattedDate:"July 31, 2024",tags:[{label:"Data Lake",permalink:"/cn/blog/tags/data-lake"},{label:"Apache Hudi",permalink:"/cn/blog/tags/apache-hudi"},{label:"Apache Parquet",permalink:"/cn/blog/tags/apache-parquet"},{label:"Apache ORC",permalink:"/cn/blog/tags/apache-orc"}],readingTime:3.91,truncated:!1,authors:[{name:"Albert Wong"}],nextItem:{title:"Understanding Data Lake Change Data Capture",permalink:"/cn/blog/2024/07/30/data-lake-cdc"}},l={authorsImageUrls:[void 0]},d=[{value:"Introduction",id:"introduction",children:[],level:2},{value:"How does data storage work in Apache Hudi",id:"how-does-data-storage-work-in-apache-hudi",children:[],level:2},{value:"Parquet vs ORC for your Apache Hudi Base File",id:"parquet-vs-orc-for-your-apache-hudi-base-file",children:[{value:"Apache Parquet",id:"apache-parquet",children:[],level:3},{value:"Optimized Row Columnar (ORC)",id:"optimized-row-columnar-orc",children:[],level:3}],level:2},{value:"Choosing the Right Format:",id:"choosing-the-right-format",children:[],level:2},{value:"Conclusion",id:"conclusion",children:[],level:2}],c={toc:d},p="wrapper";function g(e){let{components:a,...t}=e;return(0,n.yg)(p,(0,i.A)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("h2",{id:"introduction"},"Introduction"),(0,n.yg)("p",null,"Apache Hudi emerges as a game-changer in the big data ecosystem by transforming data lakes into transactional hubs. Unlike traditional data lakes which struggle with updates and deletes, Hudi empowers users with functionalities like data ingestion, streaming updates (upserts), and even deletions. This allows for efficient incremental processing, keeping your data pipelines agile and data fresh for real-time analytics. Hudi seamlessly integrates with existing storage solutions and boasts compatibility with popular columnar file formats like ",(0,n.yg)("a",{parentName:"p",href:"https://parquet.apache.org/"},"Parquet")," and ",(0,n.yg)("a",{parentName:"p",href:"https://orc.apache.org/"},"ORC"),". Choosing the right file format is crucial for optimized performance and efficient data manipulation within Hudi, as it directly impacts processing speed and storage efficiency. This blog will delve deeper into these features, and explore the significance of file format selection."),(0,n.yg)("h2",{id:"how-does-data-storage-work-in-apache-hudi"},"How does data storage work in Apache Hudi"),(0,n.yg)("p",null,(0,n.yg)("img",{parentName:"p",src:"https://miro.medium.com/v2/resize:fit:600/format:webp/0*_NFdQLaRGiqDuK3V.png",alt:"Hudi COW MOR"})),(0,n.yg)("p",null,"Apache Hudi offers two table storage options: Copy-on-Write (COW) and Merge-on-Read (MOR)."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/docs/table_types#copy-on-write-table"},"COW tables"),":",(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Data is stored in base files, with Parquet and ORC being the supported formats."),(0,n.yg)("li",{parentName:"ul"},"Updates involve rewriting the entire base file with the modified data."))),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"https://hudi.apache.org/docs/table_types#merge-on-read-table"},"MOR tables"),":",(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Data resides in base files, again supporting Parquet and ORC formats."),(0,n.yg)("li",{parentName:"ul"},"Updates are stored in separate delta files (using Apache Avro format) and later merged with the base file by a periodic compaction process in the background.")))),(0,n.yg)("h2",{id:"parquet-vs-orc-for-your-apache-hudi-base-file"},"Parquet vs ORC for your Apache Hudi Base File"),(0,n.yg)("p",null,"Choosing the right file format for your Hudi environment depends on your specific needs. Here's a breakdown of Parquet, and ORC along with their strengths, weaknesses, and ideal use cases within Hudi:"),(0,n.yg)("h3",{id:"apache-parquet"},"Apache Parquet"),(0,n.yg)("p",null,(0,n.yg)("a",{parentName:"p",href:"https://parquet.apache.org/"},"Apache Parquet")," is a columnar storage file format. It\u2019s designed for efficiency and performance, and it\u2019s particularly well-suited for running complex queries on large datasets."),(0,n.yg)("p",null,"Pros of Parquet:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Columnar Storage: Unlike row-based files, Parquet is columnar-oriented. This means it stores data by columns, which allows for more efficient disk I/O and compression. It reduces the amount of data transferred from disk to memory, leading to faster query performance."),(0,n.yg)("li",{parentName:"ul"},"Compression: Parquet has good compression and encoding schemes. It reduces the disk storage space and improves performance, especially for columnar data retrieval, which is a common case in data analytics.")),(0,n.yg)("p",null,"Cons of Parquet:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Write-heavy Workloads: Since Parquet performs column-wise compression and encoding, the cost of writing data can be high for write-heavy workloads."),(0,n.yg)("li",{parentName:"ul"},"Small Data Sets: Parquet may not be the best choice for small datasets because the advantages of its columnar storage model aren\u2019t as pronounced.")),(0,n.yg)("p",null,"Use Cases for Parquet:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Parquet is an excellent choice when dealing with large, complex, and nested data structures, especially for read-heavy workloads. Its columnar storage approach makes it an excellent choice for data lakehouse solutions where aggregation queries are common.")),(0,n.yg)("h3",{id:"optimized-row-columnar-orc"},"Optimized Row Columnar (ORC)"),(0,n.yg)("p",null,(0,n.yg)("a",{parentName:"p",href:"https://orc.apache.org/"},"Apache ORC")," is another popular file format that is self-describing, and type-aware columnar file format."),(0,n.yg)("p",null,"Pros of ORC:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Compression: ORC provides impressive compression rates that minimize storage space. It also includes lightweight indexes stored within the file, helping to improve read performance."),(0,n.yg)("li",{parentName:"ul"},"Complex Types: ORC supports complex types, including structs, lists, maps, and union types.")),(0,n.yg)("p",null,"Cons of ORC:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Less Community Support: Compared to Parquet, ORC has less community support, meaning fewer resources, libraries, and tools for this file format."),(0,n.yg)("li",{parentName:"ul"},"Write Costs: Similar to Parquet, ORC may have high write costs due to its columnar nature.")),(0,n.yg)("p",null,"Use Cases for ORC:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"ORC is commonly used in cases where high-speed writing is necessary.")),(0,n.yg)("h2",{id:"choosing-the-right-format"},"Choosing the Right Format:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Prioritize query performance: If complex analytical queries are your primary use case, Parquet is the clear winner due to its superior columnar access."),(0,n.yg)("li",{parentName:"ul"},"Balance performance and cost: ORC offers a good balance between read/write performance and compression, making it suitable for general-purpose data storage in Hudi.")),(0,n.yg)("p",null,"Remember, the best format depends on your specific Hudi application. Consider your workload mix, and performance requirements to make an informed decision."),(0,n.yg)("h2",{id:"conclusion"},"Conclusion"),(0,n.yg)("p",null,"In conclusion, understanding file formats is crucial for optimizing your Hudi data management. Parquet for COW and MOR tables excels in analytical queries with its columnar storage and rich metadata. ORC for COW and MOR tables strikes a balance between read/write performance and compression for general-purpose storage. Avro comes into play for storing delta table data in MOR tables. By considering these strengths, you can make informed decisions on file formats to best suit your big data workloads within the Hudi framework.   "),(0,n.yg)("p",null,"Unleash the power of Apache Hudi for your big data challenges! Head over to ",(0,n.yg)("a",{parentName:"p",href:"https://hudi.apache.org/"},"https://hudi.apache.org/")," and dive into the quickstarts to get started. Want to learn more? Join our vibrant Hudi community! Attend the monthly Community Call or hop into the Apache Hudi Slack to ask questions and gain deeper insights."))}g.isMDXComponent=!0},41963:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/files/bytedance-hudi-slides-chinese-697ab94acf090a0dd627bec988d9cc74.pdf"},66558:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/files/bytedance-hudi-slides-english-878d79120cd12b838418e1815b12d8d6.pdf"},15679:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi-lake-overview-e39f80337517a0a1999d8eb5cd0ac965.png"},13683:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAfQAAAFWCAMAAACo3J9dAAAAV1BMVEUANmMANGEAH0EAHT4APGEASl0AsOtHcEwAMWAALlsAOWYAMGEAr+sAM2IAse0AreYAsu4AMF0Ar+oAAgQANWYAg7kARncAAwQAregAs+4ATn8AN2cAXJD327ieAAAAGXRSTlPR8j4kDwb3ANjlv2hRppIox4rhZvz+/Jl6EYnrqgAAG4VJREFUeNrsnYlyozgQhgVIMuaGYTIQ7/s/5wp8JgbULdQC2+ramslsVS4+/r7UktjB28cZ84/AQ/fmoXvz0L156N48dG8eujcP3dubQi+zjAvhn+pHQReFbIqs5J77J0HPmr6Xlef+UTE9q3pl0nP/JOjKv/f9lXtXcv+APyF77y7QB+7NqHf/jN8e+tm/36wZ9O7d/JtD55X8QX3Ue1f6B/3O0B/9+08/75/1+0IfirYJk577G0Mvq37Ghr6Nb9e9JfRD0c/b2K/z4N8Peib7RfN9mzeEPu/ff8R3z/2doB+00G99Wv/43wZ6J3uIjdx9Qv8m0EsY9HO/LvP9ureAfqjA1C/rMp7760PverjJS7/Oc39x6Jnskeb7dS8PnVc93sZ+nefxstBF0RvZ2K/zSF4TuoF/9/26l4deNr25+Xna14TOi36N+bnKV4QuOtn3Frj7AP9C0Nf5dz9f96LQK2mBuvTztK8E/dDZgD5y933al4GeNb01833aF4FeVr1V8/26F4DOC9lbtiGfXyrjVBAYzCPdDLqw6d8fEnrFfda3FJxzT31D6L+3N9nDPkNdFH1VRpHHviX0lU25WSumGzZCJRGyiJT5kzC2gy46ScG8msnixxyiScNw0LqnuhF0Ev8uZ537UC1IWfxR1CMv9c2glwT+ven4UrEgm/qPh74l9ANB0VaUy25F5n889S1jemkf+lxAP/BrAjFIPfRRfSPoWdFI+9WamBX69ZslXuobQRcDcvvOvZtDye+VQpN7qW8CXSHvCeq1ggMKBTlK3dfqbqFzglh+CegHvdBHqf8ZOjSerCvogmdEyFVAB3YEvNTdQueDY6cx2R1AQr9EdS91R9B52VWSinlRglt/XuruoBMiX6jQn4R+j+qeOjV0nlUNGfLZlvuU0H2t7gi6KCkKc33LfUrovlZ3A50rz06HfHYN/fy6TSzmnaO6p04IXagyjRJ5X2UL33xqLU/WvlanhS66pie1hQp9biuNj+qk0JV7JWY+33IfbLoX1PzzUZ0OOqeW+WKFPrtnTlZe6lTQRVn01FYtMRdzTd+/Y1T3g7H2oZdUY86gNfS51N0vthFCFyW5a19suR8Wt9H4xTYS6GTrabA19Olm3C+p+6huFTrPKgfMq8W9qouj9b4tZx0671wwX2q5H3ST9T6qW4ZeFvThXFehT3XdDaO6+DSHwHbLvC+WzxnS1Q5wqfM08tB1zCsXyJcrdK3QESM0IkxS7qFrFCZdMG80J09omwTgWj2qWR556EuyyNzovFkO6HqhX6WubcspoTP2WVJHQueZk3CuabkfYJtiYVJXQmcfJnUcdFWquWGuCegQod+lrhf6h0md7ZG5NqADd79DRmhGoTOWRB76tsw1FTpQ6KBa/SJ0JXXhoU+2PV0x1wV0Aa0a9R14fhb6Z0V1OHThKIfTVuiIU+r0+9VDdrUPkjrbH3NtQBfw9pAuqt+Eviqqcz6caxVG4dnUR8MxV28A3VF9DgjomDti5L/lqH4XuonUFeswTfM8T1h8tuPl75glSZ7X6eBnXhd66Yq5ruV+wJ0mvxzVRf0AHRPVhaJdJ0l8DII2aJWdBvs6fX0Nfwwfq/8XBMHXMWaK/c5kz8iZD4c4S4sBfX4ybiaqL0zLPQodXqvzOmHBFfPp/MG0nV8GhT9OBvJ7Wc2DQTc/aaCpuhKTDlT6w55x10YsSF3w+gd0qNQ5axdRz7wAR5am+5jrAEE3PdNZiTwr1fOGX/SwtG3tVjni3rr5EZpbjY6UusjxyM8WxHm6g83zEOiG0+3yfmYz9MxQqQ/o6IxyXurRT6ErqYewh5aaQh8EHyR1uHGMB0A3LNYer1MEfgVZ6S9wEFivc27LTUB/EjqDrqtHx9Ma25w7ALrRArr8cQMHMA8EBHSBvhNodjD2SehQqQuenNZRb0c/z/cL3SSJkyp94+jKGhDQTdKLmaj+LHSw1EV6Wm+j3HcK3SSgN8VvNw0K6oCAbtIjmpb679QdJfUwWA+9bb+U3KM9QjcJ6CplPwh8nV8BmHOTY+Qno/qU0MFSj9jJirVxsgV2HXR8V0YWU9mY/ss0kOuYzJrBUyM00ZTQgbW64Lkd5kP1vsFUJtOFULzMJ38HrUKXt62tEfrkYOy00KG1ukiDkzULFHaxI+gCexO6Krq4WRutgPzepqs+z1KfETq0LRfGp9PrYmcWO55jBjf3+lRrtq2tE/rECM2s0IFSj3Kb0Nv2mIcOsTObzr1aqLm69QF9xfLub6nPCh0odV63J7sW1+7aNWzRuSNd+9KGw1KurNBXCP1pBn5B6DCpizS2DP3UukvkmbXMfZn58heDVOgHvmZN/0etPl2jo0ZoouRk3eLcUZOO2epya/vmxcqAvu7+1kHqd+hLQoeN0ER5a5+6SugisS10XFum0IGbLwSWXcRD133NQM6j1BeFDovq3GbR9oDdidiZpZ67lvm8f4cF9LUXNT+eLReyZYNIPYxPJNSZg8g+Dx3Vc9czny0FJCig48vH+cFYjdBBUV3YLdoeB2zoJ/DZvNDtMp89IgYW0Ndf7nc/MXY5oo8deP2PZL9ou0f2UGwDHXVBbgHiNi3VpeN+bQr9HtW1QodJPT2eqCwgbsez9eWaBGp10nc0HeytRk7GLa2rh0wPHfDUw4QM+qnNSbN4troPIqsMym3iaxbAWcTSwv6as9QjvdAhUhc8b+mgt6RZPFvd8IT658kaEBrQhY2jCs8deIDQQVInKtru/TnuGDoiojcZ2BE9+/cG+MIIOxtsBqmHEKGDRmhSdqI0wtqNrYzoTVeCv9dTpQ1aQ7dQoz9G9T8g5hCpR0lLCb2N68gldLjQoUX2ZQGn+fXJUA9mafekzP/9+cfi2IrUVRUQkEr9FFOlc2yd0KsS881+fV1oQF+zvPZ7ZxuUOUDqlEXbtSlLQ30KOgfPyzQo5r+8dAXOBqxtk27+JsdjzGBS1zrXkJ2oqSehM+jQZyw75Iv4+DbBP9ma0Pv+77iDHCp13ck3tEH9nMSHjqCDhV5gq4rHm87h2YC9Myrl3+B4BGKPdVIXgjqoDwU7C51ABx/0gAvov4o2+CfbFPr3VwCWun6EJqQO6gN2Zr8TPwG9hMomw/80N37wgG7x4BP59/s7OIKpa6N6xNqLHgcLhqMnzh+2dgt2QQ+9g1ZrBi2jq6cGrqHbFvp/34PUFXVYLqeTusiPgUoSVPwfTpjJ87oe/lT/SIbjZ4KgtdWcE9TQofUauOCa8O+Y8t6u0K1KXYR1mqZhGEWcq/+UifFPHoZhOpxAlLCjBfLKw3Ni6MA0rulMXr9LDx2RDVgW+rfVqD5OX/060Ob8TzF0bxT8tM5vB9SYZ3OJXerMdGmjKI2+39iUQwR0XloW+lXqVhJ4/UuuyEdK80m8jrvduQpmOKxQZWY/xXCmAKa854VFoX+N0L/Gqg0odSvPWkleKf7Y7qVeZ0/9cUmWxV0dCeJzhX2hY6Ru8XBoHoV1Yl7XtzYn556gwxoh8EX0p9qgwZT3wr7QlTmX+jnUK0dfx+bU7W12Y0bKkoXxD1AWmPeltHcg7U3odmt1DHeV24U1a01XXziV0mG5clWaf0dUpdf1BEK/JPCxW6lfuUcpM2veBDWV0kHuVHZize+NiOiN/YiOlDrBlQ88NSri2qOtF5CZdGbWCB2X9ZEI/daWszRCg7fIDDtLSaCDvPuKiI5jTpG6u6/Vp3P5mgUmhZuwDx1WFTfOhG65GfcgdUwCTzKWysMcncm3gZ03kBl498INc+td90foX4gEPqR5paMUXbe3Qc2tQweNK8jMDXPrXXdDqZNNoKv6DSv2NrbxwzB0SJfV6wsd15Yj20YqVB7fbtCFZz9DOuQRvoPQsQk8VeYqwjxw349l6GzZVRpHK3RMrR5T7hiP6qPzsM5+rnsCxOWqXuukJBT6HhL4W6sGGdZXO3iGDumZeAuh7yGBv7p45Cg1iyxCB1XpjrpxxBF9Lwn8pXjLUf25dq2DZ8gq3XghfW9Cxy62CVLquMAepPagQx60oyLdgdBRCTzx/eoD9RbTjo3sQd+Pd3ch9IvU2ba1+q1ij1GrrNwSdNAGZTfe3YnQ9yT1gTqmYF+XwTNca2bVSvoelteMo3pCfLqbon505eDZYx4nAd7dBXROMhk3ncBvNEIzrXV4XF81UPEIXT+mIp2EdItC7xeFvtm03ExzDuPh18xEs8d+nP4Zdi5CuuhcMd9mMHahJRsgcjlhAzrgUTedA+ZEI7AvIHXBEcsvKzaus4eMmXLcHfOr25yM00FHJfDkUh96cw4G4RkqeS/KlxK61Ap9Vwn8mXpLX7bdoUOOfXaxwiZcpe6PHfh4+w78dfUFPlZhHG8YKnl3ENKd1ej4xbY4dyB1nkI3uH4dTaX+AF0vsMZBSOeFdBfR97Sufl9fD6ilzjCddwd5nGuhI6flQgeuLgJfH2Aq9Rt0yCVsDvI450LffrvLc7kODeut4Z5GhumIOMjjbJ4Z9/2NkfpOavVhUzP0qDLTqM4Q1bGk3+UgOudCv0X1eDdSh59KaLbEeo/p1R7yuMx5RMdKPXQBHXw/+zFaBZ3r/So9dL6F0JEHTjlZW4be+tbWK6HvYGpmG6GPUj/uKKoPx4oDHXws1kAH9GbIF9M3EvpN6vFeavXhLlcY9dYklbsncnJ7pW8l9N0l8GM7FriuztcoXeqTd/6mQt/VtNz1YdSwOZpjuAb65mX6Bs24PUsdeC+QyfFDCOjUSudul9emonq8J6kDh6INFnxv0LOtezObCn03m1h/5HIgqccpJXTiWSlu88w4PPSvXU3Lncu2GFaqC2Po+kdOvKPJ5WTca0R14G2ueP+OgN6QblK2OTDzbWQ7lDqDjcCTQn9noe9Q6geY1PH+fS9KF4XcMqLvM4E/gKI6fovTXmK62xHYV0ngYdf9oU8P/b+9a9FOWweCtiyBHyGNE500be//f+flDQbbGkm7Zm1ZhoRw0iRlGO1rdiWE6eaVMbpUtdyB6jlHs4sMm/7iGL0rjC0lWfUMUk0Vc2S6fr1F959CM0lhHRJJ+jYzyrDpbxKILlBCs90WFUPQ5sN0toyc+RFBdK/jGeuJMvA7KGjT8wP9FRLYuVC9sfRdDxIKLkIsuqjZcrf9HXHlPE9evoBuENDN0ol+duAzQQ68bumNugfTuUQUgoguMFY3Ctjfv/3+FB8RBZNGThDRr8JYQVRH9nfPSP0mgQbmUOjFE5320OXp9vfaa4yphzCSSQ0rJUaXWmyDJPB+8sgb0wHdO0d2Rv+IIrpAYSyyv/uNCL4x/UW9bNKILmli7Bl0JD/j9fa7NTt8vCQ7o3lOU52M6hMk4HUDta8aHtAZAnUjoY4ue+DUdgsMjfUTUmQ+FW36SRTCXHfvgVNUp5+O7+818REft/50ZHYgOeivV8bNwIFHjLqX++41c4a+ov4uzqILFMZqYDCFl/t+Ax0ZKfYjl+j//f37EqpPYdWBSN02IY4cBABxnY10Ciwl6h7C2CmKbUDbsvVR8tyBDgTMtJ6ckWnRxQljDeLJ+ego/AYC06ZnRLruEtNyiHxmF8R0JE1Ce/yivGSczDGSBpFH1kFMn/ywJiOX6MIkNEjNxSdQvz/Ow40BaUldMtFlqeWQpuUqDHTEkyM06loy0YVNjAXGyuWBoEPnsmk6om8EE93PqnOf2aac7rv9VkGgI+47XSZWv39KJrosqw7EbLkK8d6xs1Y/fyVh0f3q6uyxunv8jPVJyXVAR7hHtL8Lt+jSTmJ1TycIBR1qHSXa3+UTXZJVR2aBB4IOGXWaNkZxyjjZGngIdB0EukGMOk1+Zg5EF1RXR7IzgUzHjDqFKzcDi+7ZxMpLdSQ70waCjs2D+EmF6IKo7gTd2lDQMUsb3+gyC4su6sgH90S5YNC3P1DCJNaVE1xHlyqhcW/v4aBj+/tH5HvaCFXGxarlOJPv35YPdAiO2KrLfIguxqq7D2ANBx3c3yOp/j4fzKW0uwA2PTBOh/f3KKqbORFdilUHQrYmmOkaY2GMA09q0flB//YAnS9WV9U3U0buuL8jwdQmguoiu9fcYyRfLIxlZTq4v0fIpkRMgWXqbGOjummc3nsM00FMwhuc5pKMk6WWayvLVFqFlRQRFdaZZN2FqeWAKpv1SBM8ga4xJn4GiinmR3QJdXXtFr7nKgL0LZgXDxtAI14ZF+/AcwTrQItLXkRs72i+bBPU1zZHovs48D5W3SiFAgWcxhgogfaMo0PCttmU16Zw4E2Tla2CsHJLoEObHa5sBF/2AF9ulkRny8AfodzvDYXTJAC9ylkc6FuwC8G/8WGGrjunMPZ8ApfN2mYcd0Q4E9bA6B21+QfrcyU6lwN/DcTyum3UMO6ARM7uIkH/hTYceW7wM7XofMLY+xa1E+56IEx3y97bSNANSnXPwXLzJToT1btOuT3i3uPQG2QoQRMJ+hY+MMtrg5+tRWcbDv3A4P0XVb3r2ecB591Gg44fpOOxwcs4e03UFJo+W52X2SPuBulUjgYdp7rH6CE9I2VcrFVH97+iF037wHcDHN6TBw0PDMrFHuI29D84L8HMJBKa4RPSbV4ecD+/NYAx/1URDzrswMNmfXZ19EmEscPVM3vC3Wyx2bBeRzsMgK7fYIhAsz53ovOk5ZQjAC/rptCASfc7mS0bwgi3wBDq5n3umLMIY90H6O4DOYDogfPen6w6zMxPwJkz8ye6V6wOW3XkVB5khZ3sEOFsA4KKmUlgo2N1uNjWWEuBed4YCtCNR/7M6czNTQI7XayuKxKiB57WFJVAc1F91sm4ULUcxD1dk4CeEYHukaH5cDH97WMJRGeZGEtj1ANPYOxLxn6iu3saRGeZLadyCtC9nPcx0FFfzq2Wkz4c8pWxOnRUNmV7iwN0bAaNG/NZSmCnKrZpiv099Cjt/mCdwqAvxqKz1NWRaopzlYoQdOPel92H+cxYMDOBWg6pmxIfquwC3RlfA+KZJRGdwaojshi3804IupOlQPvqYlz3jgNPN1sOaFly2/TdoL4uAPTtr3HIgGLLsohOT3WtKJJy+UFvg/I9i8Js86YTIzqDVVc0STmb1yjuTtD1SNy2AebELo3o5FQ3mqrS9qSzCgZ9bIMHaqqLct29O9ugurpu8n//6HDP3Li7QR9G3R2hL6W8xqqWIwnaOjqr7KavCwXdvH1uQtVxi7Pot7Qc3fnqQCOyt30fxT1DvL3eysvmB5BJLUEww27VKYK2J7ofdZWFDge9V8kKKd4JiS4Hcz8NPCKfyf79o4f9pKPuM+8Q6H1mHRo/skyiU1t16NTkYMdu99wHjYH+LKiA5O6LUMYNa+DJRlO4T+CKovxxn9f+oD964Z/ISS6LdN051HKkQVt/30R779eBoD/U1jfvv5ImOnVajjZo68/T3um3UNBNp5P1A+plWl4y7oHqGVFdndOoX6WTTQDT7/Uvn29Q+WiJMXqYWs64gzbLCrmtG2MCQL+58BvsaLYlE/0yW45IGGuaintvN/6OXKe/DZv2viBlHL8wtsg4Ma/abvHNB/TzLAnwNL5FE504LTfcqE6xykf74gP6AfXNnw3WkG5+NksmumcGXjuNOlvQZjOlTQzoBwU7eNbq0olOnIFvSjYX7nkwoRfoR7uuQV9/4UQnjtV1xrO/5329L56gw6LL5ROdVgPPs7/bKqLg4r0SIDrxGEmOTGw3OmcHPQWi01p1hkxsVSsTXk/3J/pPAkSn7WzT5PKZx+icG/Q0iE5r1anlM9kg5jyg658/KRCdduBUQZp+t2NpQA7QzVIFM5wSGqMpjXo+ZM75QJ/9cEgeDbzjVdtRRuejnS4coKdDdFIHvrGEGZnYZgfvNe9xzyFqORKrXuQ0qNvSpcDNVqJLobqpqcz5dnLQFzAF9jUZeEMin3GYcx7QFyyBjY/V1XhPW06wtbdAS00m2nWfA+h0Gfh4+YzFOuKzlehTWnU1Xmljjc75QNepEZ1wOLSODNoe5I/TgZ5K1p3Dqkc2qlctOiA2Wy26HAc+quehxOf8Z9RE3yRHdDqrrpuI+YEKHyVHC3oidfQeB57Eqof3PNidojjDZbXovlSHOttGm1hNaNCGm3N60PV7kkT3teqGuFHdZn6Tv2lBT5XoZA68CZFH5j7mnBz0RC06ZVpOZSHR+faFoKdLdCoJTUCjuqc5pwY9GWUcozDWd7qYLb1O4SMHPcGsOwPV/YI2W4dgTgh6sq77vQMfPTHWayR0vvN14YhBN2kJZpio7tWojuglmEFPRwLLqZbzaGQsm0DM6UA/E31zu206N3z9+e/vHvSJb52Hr6Q6XGkLM+fEoL+9/by17a6+rN/Z3VX/xldW5XKWPd2Pt/OH0/Vtj9fhdlkVhQMPBm2A/HGK7V0XSn19tb+jV1lVe9Lwrbx7vz3cX4+306fq8un8+HydbtdvrioKqkNBm80DonMW0Peof319/f56hvHrcDtd7fXR0PX7Cg3TzfmO6H68viuq7hvlDvXrUzDT69YMZ2ItZe2cF3RTHKneu9Ttk7p+qfrvB6bPdaFDJMcy8M5MrD0UWIwQ0HVxgH1gFcdLHb7j8E3F6avzE3d39ZUd+HIsX/Cs6nyryvvb8bp9ePz08PD6RAdxfNrUWF3deThfvlORWBFm5A4bfOzSqs2yknVl549Zz1cUPz2LpLoZD9psviuMGNCPqO9Xsb+OH27XYZnLdbodnzBG31+H7/ranclSsl2Xn376fPtd198J/6hgxEepPn44X3h0zgP6HnZ4bfue2gcs7T0kPKsL/O13cf5OD6qPyWcs2M0wdbPD9UPn3YD+a1U/IUT4yeeNgf6r7g7is4bgG8nE2lqZrTzQ49aJ6ImsQbXcoHyGwJzLA/2Z6IteQ2q5ohrSSxia1zlbif46qg9YdV0PyB+pXudsJforqd7/QvRlYm2ttksEPTGiD2fgle2LzrdLBD05og9S/Wm6mC1bbZYIutGpEX2Q6o/TxWzWUGIuCfT0iD5I9eaxdk6KuSDQEyT6INU702eqHTHmgkBXWYqrXy13PxK6aokhFwR6kkQforppqfQSskFPk+hDVL8GbaUy28WCnijRB6huivI8a6DgeLGlgJ6i6z7iwB8rbZbBnAsC3SRL9CGr3tjA5sQZMV3VZbKg99bOVEkenUsDvUiX6P3FNlPs9tE518udrRb95VTv28W1Ksx20aAnTXTkfPUFgm6SJjp0vvriQE/ZdX8N1bMZEL2+XINr17nvhlbLs3Zj6/qHXf6+vtWY1EDXqrkt1fS0RJ3X/sFd/8RDm8TxmeuDa1NF93Z91Hd/vg38lMcf2v/7Tvf7P8zohz/79t9JDvRDb8ydNN50vro8Y44te2a7rkWVVte1gr6uFfR1raCvawV9XSvo61pBX9cK+rpW0FfQ17WCvq5Frv8BfId6dRgGlZkAAAAASUVORK5CYII="},96346:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/2020-05-28-datadog-metrics-demo-fff08d34cd7ef2473f16e9b48dd66793.png"},49720:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i="data:image/png;base64,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"},20976:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/2020-08-20-skeleton-dcb4339a37b8ac8ce291a4f119e326c9.png"},88919:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/2020-12-01-t3go-architecture-alluxio-b29648cdbfd10db14fde73b66ec499a5.png"},87838:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/2020-12-01-t3go-architecture-93f0c75faf35e99a62d7ab952a12ff74.png"},43806:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/2020-12-01-t3go-microbenchmark-f8bb8b80b32eaedbcc990260459b319b.png"},82717:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/DataCouncil-04dfc7a9001968f04689ba9fda4dbaab.jpg"},52237:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/batch_vs_incremental-ea2dd4a52745c47a6b2aa11b65c058c9.png"},41209:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/build-your-first-hudi-lakehouse-12-19-diagram-7bec1745b0437f71e86e4ab659bee730.jpg"},67527:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/change-capture-architecture-dc9c69c50296a6a38721ec93fee9ba71.png"},53261:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/change-logs-mysql-a76f7760403ba59c5d11ba48b12cd4d6.png"},41355:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/Query_Plan_After_Clustering-e669cd3887eaadb0f9d23cd4a773535e.png"},81232:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/Query_Plan_Before_Clustering-cbf67c7e66765c7b42ad88521739d39f.png"},51204:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/example_perf_improvement-acd223093d7c84fb6f0a896dcb571737.png"},96101:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/ConcurrencyControlConflicts-55bed17c500b3b29e4f8cdb42cf0f483.png"},56401:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/MultiWriter-fec6bf4269df78d4fa91e7a353144def.gif"},6856:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/SingleWriterAsync-3d7ddf7312381eab7fdb91a7f2746376.gif"},81007:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/SingleWriterInline-d18346421aa3f1d11a3247164389e1ce.gif"},83444:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/database-cdc-e9ee525e81a47e7744ae4f408c4e1d8f.png"},32435:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi-cdc-263ca6e0f40b6bff91517bd02c798e2d.jpg"},76589:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/log-based-cdc-92eff429e89653b892b63f1af3485ac6.png"},31479:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/trigger-based-cdc-51c20f90024a12e97cbc728cfc7c0ed4.png"},46886:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/ts-based-cdc-30ce5c2462ea39b02dbf9a93467a360a.png"},17519:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide1-4395683e8b063979208436c3ecdecfbd.png"},47295:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide10-698c9a0980df4f61806ae141521a12b0.png"},50358:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide11-e6a0a852c7c72494dae31e8a9f95ba74.png"},64333:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide12-fee0df68abb2b276bfc5c14f3733f5fc.png"},10436:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide13-998b520f6392c9d218febdcb0f87b59f.png"},74299:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide14-df2e6e49dd6da70d80a106fb39950575.png"},59474:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide15-4340ab9068964e510a89f2bb70ef4adb.png"},90441:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide16-358d556fb2c770f517d56ced6a880a66.png"},95968:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide17-ae7bfe343dd1d0cd170f2f5d00094dea.png"},44574:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide19-787cf360bbd5e0f4cdc06cbf6df16016.png"},82260:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide2-33dbf59c0d2ac48489e6e8e0c1918b44.png"},3389:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide3-717ff61fa0432142d84c418fc3a73200.png"},7042:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide4-4d2f3854977f63b75788213b21518b62.png"},28171:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide5-63a50a959a8ca1fd6371bafd1765bd0a.png"},74672:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide6-dfbac2ecb760185c7c401305c4796192.png"},71577:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide7-5941e55407477f2a843749121fd90709.png"},82190:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide8-1d407f163ced76b9b0a6a1c3a45ce6d6.png"},25207:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide9-92ae2d70a81caf694e03658351410de8.png"},64516:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/Hudi_design_diagram_-_Page_2_1-fee4ee8ef3b5ba97b06ffb9d47644a79.png"},47013:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/Screen_Shot_2021-07-20_at_5.35.47_PM-0a1e607d305e2293307a6fa9a980813d.png"},62919:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi-data-lake-platform_-_Copy_of_Page_1_3-2d54eeaee61f34b3146391bec58c11e5.png"},90531:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi-data-lake-platform_-_Page_2_4-a088663c0cb4f7e97b5b74da634975ef.png"},72690:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi-design-diagram_-incr-read-1c9bc7f09b69e8d9f1b2d439e3232a01.png"},69365:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi-design-diagrams-table-format-3ba591d07f846d8a739366efdf6071ce.png"},22885:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi-design-diagrams_-_Page_2_1-d998a263b380ed3357fcb2006ffb5bfe.png"},8587:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi-design-diagrams_-_Page_4-163995bcecff993234f40d17228ecd6b.png"},87458:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi-design-diagrams_-_Page_5-5ca4af1d2d91e19dc4e9e9e5138bb2b7.png"},27193:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi-design-diagrams_-_Page_6-3b292156302554ff2ad53e6f2847f56c.png"},14915:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi-design-diagrams_-_Page_7-03e378cf49a27e58e544a3eca59905f0.png"},40855:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi-design-diagrams_-_Page_8-5f886f1e198375aa996a989c03a707e9.png"},33539:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/dlh_new-ae34f6d692de93292db9eb4e19690670.png"},23608:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/dms-demo-files-2c926cf6a9fb12b5e9bc44a65df8e2b7.png"},73700:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/dms-task-cf605b4a3c85bea264a16a20a1645608.png"},26738:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/Initial_timeline-fd0812aa0c22d797d2192745d103bc41.png"},67594:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/Retain_latest_commits-e387b7c19e4ee4d9cbef7b0bb0466983.png"},57173:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/Retain_latest_versions-723f83313beb86b46c9cd1fcb8ea0b25.png"},17581:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/adding_new_files-13e5a1cf0c213c07a412b09a29be4e3d.png"},55069:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/bin_packing_existing_data_files-021f5b531f048bfd9cc1230f93c22a71.png"},65304:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/initial_layout-ba5e4c454e6d2328f74dfc5e9fa2966a.png"},26874:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/Dimension20tables-6bbe96fbe9102541487b1819532f6bd4.gif"},28126:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/Event20tables-8998b57588a66cb2f5d3e9233dfb6d0f.gif"},82242:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/Fact20tables-0255e82a96683124f7116060e9f76cbe.gif"},22542:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/with-and-without-index-81d481917e61e4cd1be2426c12994b8b.png"},94097:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image1-ad0429306e0ebe38c0eacbf4b2aed222.png"},63930:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image2-73e008bce9561bc07e7586c36a0cb745.png"},81507:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image3-570b43b2c0cec6865c17b50ddef9a3f4.png"},35707:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi_dbt_lakehouse-14b1cb2c180ecb95dc78e3b4c44a6301.png"},78985:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hstck_new-a0f2451aad8bf4e2003f1efb98c5e179.png"},56803:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image1-3639ab8c9d27f6e461866dc83e8346c0.png"},55176:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image2-971db03016b54c2da63fec8a2df4f412.png"},69393:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image3-8f35b19f5afc8d6b571f4479eb024189.png"},87706:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image4-e0b30b97adaffc9f1a9cf2eb8f0a9c52.jpg"},46695:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image5-58bbba66797e915ba7518ad7e61bcd56.png"},26636:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image6-3f3cbe07ee5f79cce9b6f18241058961.png"},95733:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image7-0a4cfd0fa02ba7efc07901ae75d1c188.png"},66578:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image8-953168c35d108f42143d2942a7197941.png"},4653:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/KafkaAvroSchemaDeserializer-7077d39b24f01312dbefecdc9cfb937a.png"},2267:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/confluent_deserializer-acede4110283a5d72af7029f3c4a98a6.png"},51200:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/normal_operation-5bf358ee14c1ee57978939d66f0ccc3e.png"},25128:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/schema_evolution-b6cbf3c7c40814a0d8fcbd9f9176ea72.png"},46585:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/batched-marker-creation-e8455c544f3b11ceed810b663df59f7f.png"},64052:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/direct-marker-file-mechanism-b97b82f80430598f1d6a9b96521bb1a0.png"},77597:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/timeline-server-based-marker-mechanism-11d616800a7a241382c8a4ed647515a6.png"},91030:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/read_optimized_view-f86557dfea584b97e869ec2d1aa9a46e.png"},94838:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/real_time_view-101973743bcb2dccb6e7cd4aa411aa73.png"},13755:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/s3-endpoint-configuration-1-6246a9d09772ac527a13f5b26a6fb38e.png"},35424:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/s3-endpoint-configuration-2-b275c182ed2fa52e4c7a33bffba394d5.png"},70239:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/s3-endpoint-list-8d89e05bd7f4d82958a6c11a0cc0c8ea.png"},98847:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/s3-migration-task-1-61e22d0e163cf67bb9a9dd0879222177.png"},85284:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/s3-migration-task-2-797ea4b89d2a3be41d476785040e2886.png"},87711:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/s3_events_source_design-897267ee0a269c5e796ebb92faa8c149.png"},33346:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i="data:image/png;base64,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"},15256:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/spark_read_optimized_view-3aeb662ab165a9702e1d73ee495107ec.png"},23940:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/spark_real_time_view-1b76ee7a1e9e884439da562f46d95f57.png"}}]);