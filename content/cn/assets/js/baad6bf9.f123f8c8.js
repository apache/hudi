"use strict";(globalThis.webpackChunkhudi=globalThis.webpackChunkhudi||[]).push([[38056],{28453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>s});var i=t(96540);const o={},r=i.createContext(o);function a(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),i.createElement(r.Provider,{value:n},e.children)}},32753:e=>{e.exports=JSON.parse('{"permalink":"/cn/blog/2024/12/06/non-blocking-concurrency-control","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-12-06-non-blocking-concurrency-control.md","source":"@site/blog/2024-12-06-non-blocking-concurrency-control.md","title":"Introducing Hudi\'s Non-blocking Concurrency Control for streaming, high-frequency writes","description":"Introduction","date":"2024-12-06T00:00:00.000Z","tags":[{"inline":true,"label":"design","permalink":"/cn/blog/tags/design"},{"inline":true,"label":"streaming ingestion","permalink":"/cn/blog/tags/streaming-ingestion"},{"inline":true,"label":"multi-writer","permalink":"/cn/blog/tags/multi-writer"},{"inline":true,"label":"concurrency-control","permalink":"/cn/blog/tags/concurrency-control"},{"inline":true,"label":"blog","permalink":"/cn/blog/tags/blog"}],"readingTime":6.89,"hasTruncateMarker":false,"authors":[{"name":"Danny Chan","key":null,"page":null}],"frontMatter":{"title":"Introducing Hudi\'s Non-blocking Concurrency Control for streaming, high-frequency writes","excerpt":"Announcing the Non-blocking Concurrency Control in Apache Hudi","author":"Danny Chan","category":"blog","image":"/assets/images/blog/non-blocking-concurrency-control/lsm_archive_timeline.png","tags":["design","streaming ingestion","multi-writer","concurrency-control","blog"]},"unlisted":false,"prevItem":{"title":"Announcing Apache Hudi 1.0 and the Next Generation of Data Lakehouses","permalink":"/cn/blog/2024/12/16/announcing-hudi-1-0-0"},"nextItem":{"title":"Use open table format libraries on AWS Glue 5.0 for Apache Spark","permalink":"/cn/blog/2024/12/04/use-open-table-format-libraries-on-aws-glue-5-0-for-apache-spark"}}')},79399:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>a,metadata:()=>i,toc:()=>c});var i=t(32753),o=t(74848),r=t(28453);const a={title:"Introducing Hudi's Non-blocking Concurrency Control for streaming, high-frequency writes",excerpt:"Announcing the Non-blocking Concurrency Control in Apache Hudi",author:"Danny Chan",category:"blog",image:"/assets/images/blog/non-blocking-concurrency-control/lsm_archive_timeline.png",tags:["design","streaming ingestion","multi-writer","concurrency-control","blog"]},s=void 0,l={authorsImageUrls:[void 0]},c=[{value:"Introduction",id:"introduction",level:2},{value:"Older Design",id:"older-design",level:2},{value:"NBCC Design",id:"nbcc-design",level:2},{value:"True Time API",id:"true-time-api",level:3},{value:"LSM timeline",id:"lsm-timeline",level:3},{value:"Flink SQL demo",id:"flink-sql-demo",level:2},{value:"Future Roadmap",id:"future-roadmap",level:2}];function h(e){const n={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",hr:"hr",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:"In streaming ingestion scenarios, there are plenty of use cases that require concurrent ingestion from multiple streaming sources.\nThe user can union all the upstream source inputs into one downstream table to collect the records for unified access across federated queries.\nAnother very common scenario is multiple stream sources joined together to supplement dimensions of the records to build a wide-dimension table where each source\nstream is taking records with partial table schema fields. Common and strong demand for multi-stream concurrent ingestion has always been there.\nThe Hudi community has collected so many feedbacks from users ever since the day Hudi supported streaming ingestion and processing."}),"\n",(0,o.jsxs)(n.p,{children:["Starting from ",(0,o.jsx)(n.a,{href:"https://hudi.apache.org/releases/release-1.0.0",children:"Hudi 1.0.0"}),", we are thrilled to announce a new general-purpose\nconcurrency model for Apache Hudi - the Non-blocking Concurrency Control (NBCC)- aimed at the stream processing or high-contention/frequent writing scenarios.\nIn contrast to ",(0,o.jsx)(n.a,{href:"/blog/2021/12/16/lakehouse-concurrency-control-are-we-too-optimistic/",children:"Optimistic Concurrency Control"}),", where writers abort the transaction\nif there is a hint of contention, this innovation allows multiple streaming writes to the same Hudi table without any overhead of conflict resolution, while\nkeeping the semantics of ",(0,o.jsx)(n.a,{href:"https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/",children:"event-time ordering"})," found in streaming systems, along with\nasynchronous table service such as compaction, archiving and cleaning."]}),"\n",(0,o.jsx)(n.p,{children:"NBCC works seamlessly without any new infrastructure or operational overhead. In the subsequent sections of this blog, we will give a brief introduction to Hudi's internals\nabout the data file layout and TrueTime semantics for time generation, a pre-requisite for discussing NBCC. Following that, we will delve into the design and workflows of NBCC,\nand then a simple SQL demo to show the NBCC related config options. The blog will conclude with insights into future work for NBCC."}),"\n",(0,o.jsx)(n.h2,{id:"older-design",children:"Older Design"}),"\n",(0,o.jsxs)(n.p,{children:["It's important to understand the Hudi ",(0,o.jsx)(n.a,{href:"/docs/next/storage_layouts",children:"storage layout"})," and it evolves/manages data versions. In older release before 1.0.0,\nHudi organizes the data files with units as ",(0,o.jsx)(n.code,{children:"FileGroup"}),". Each file group contains multiple ",(0,o.jsx)(n.code,{children:"FileSlice"}),"s. Every compaction on this file group generates a new file slice.\nEach file slice may comprise an optional base file(columnar file format like Apache Parquet or ORC) and multiple log files(row file format in Apache Avro or Parquet)."]}),"\n",(0,o.jsx)("img",{src:"/assets/images/blog/non-blocking-concurrency-control/legacy_file_layout.png",alt:"Legacy file layout",width:"800",align:"middle"}),"\n",(0,o.jsx)(n.p,{children:'The timestamp in the base file name is the instant time of the compaction that writes it, it is also called as "requested instant time" in Hudi\'s notion.\nThe timestamp in the log file name is the same timestamp as the current file slice base instant time. Data files with the same instant time belong to one file slice.\nIn effect, a file group represented a linear ordered sequence of base files (checkpoints) followed by logs files (deltas), followed by base files (checkpoints).'}),"\n",(0,o.jsx)(n.p,{children:"The instant time naming convention in log files becomes a hash limitation in concurrency mode. Each log file contains incremental changes from\nmultiple commits. Each writer needs to query the file layout to get the base instant time and figure out the full file name before flushing the records.\nA more severe problem is the base instant time can be variable with the async compaction pushing forward. In order to make the base instant time deterministic for the log writers, Hudi\nforces the schedule sequence between a write commit and compaction scheduling: a compaction can be scheduled only if there is no ongoing ingestion into the Hudi table. Without this, a log file\ncan be written with a wrong base instant time which could introduce data loss. This means a compaction scheduling could block all the writers in concurrency mode."}),"\n",(0,o.jsx)(n.h2,{id:"nbcc-design",children:"NBCC Design"}),"\n",(0,o.jsxs)(n.p,{children:["In order to resolve these pains, since 1.0.0, Hudi introduces a new storage layout based on both requested and completion times of actions, viewing them as an interval.\nEach commit in 1.x Hudi has two ",(0,o.jsx)(n.a,{href:"/docs/next/timeline",children:"important notions of time"}),": instant time(or requested time) and completion time.\nAll the generated timestamp are globally monotonically increasing. Instead of putting the base instant time in the log file name, Hudi now just uses the requested instant time\nof the write. During file slicing, Hudi queries the completion time for each log file with the instant time, and we have a new rule for file slicing:"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.em,{children:"A log file belongs to the file slice with the maximum base requested time smaller than(or equals with) it's completion time."}),"[^1]"]}),"\n",(0,o.jsx)("img",{src:"/assets/images/blog/non-blocking-concurrency-control/new_file_layout.png",alt:"New file layout",width:"800",align:"middle"}),"\n",(0,o.jsxs)(n.p,{children:["With the flexibility of the new file layout, the overhead of querying base instant time is eliminated for log writers and a compaction can be scheduled anywhere with any instant time.\nSee ",(0,o.jsx)(n.a,{href:"https://github.com/apache/hudi/blob/master/rfc/rfc-66/rfc-66.md",children:"RFC-66"})," for more."]}),"\n",(0,o.jsx)(n.h3,{id:"true-time-api",children:"True Time API"}),"\n",(0,o.jsxs)(n.p,{children:['In order to ensure the monotonicity of timestamp generation, Hudi introduces the "',(0,o.jsx)(n.a,{href:"/docs/next/timeline#timeline-components",children:"TrueTime API"}),'" since 1.x release.\nBasically there are two ways to make the time generation monotonically increasing, inline with TrueTime semantics:']}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"A global lock to guard the time generation with mutex, along with a wait for an estimated max allowed clock skew on distributed hosts;"}),"\n",(0,o.jsx)(n.li,{children:"Globally synchronized time generation service, e.g. Google Spanner Time Service, the service itself can ensure the monotonicity."}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:'Hudi now implements the "TrueTime" semantics with the first solution, a configurable max waiting time is supported.'}),"\n",(0,o.jsx)(n.h3,{id:"lsm-timeline",children:"LSM timeline"}),"\n",(0,o.jsxs)(n.p,{children:["The new file layout requires efficient queries from instant time to get the completion time. Hudi re-implements the archived timeline since 1.x, the\nnew archived timeline data files are organized as ",(0,o.jsx)(n.a,{href:"/docs/next/timeline#lsm-timeline-history",children:"an LSM tree"})," to support fast time range filtering queries with instant time data-skipping on it."]}),"\n",(0,o.jsx)("img",{src:"/assets/images/blog/non-blocking-concurrency-control/lsm_archive_timeline.png",alt:"LSM archive timeline",align:"middle"}),"\n",(0,o.jsx)(n.p,{children:"With the powerful new file layout, it is quite straight-forward to implement non-blocking concurrency control. The function is implemented with the simple bucket index on MOR table for Flink.\nThe bucket index ensures fixed record key to file group mappings for multiple workloads. The log writer writes the records into avro logs and the compaction table service would take care of\nthe conflict resolution. Because each log file name contains the instant time and each record contains the event time ordering field, Hudi reader can merge the records either\nwith natural order(processing time sequence) or event time order."}),"\n",(0,o.jsxs)(n.p,{children:["The concurrency mode should be configured as ",(0,o.jsx)(n.code,{children:"NON_BLOCKING_CONCURRENCY_CONTROL"}),", you can enable the table services on one job and disable it for the others."]}),"\n",(0,o.jsx)(n.h2,{id:"flink-sql-demo",children:"Flink SQL demo"}),"\n",(0,o.jsx)(n.p,{children:"Here is a demo to show 2 pipelines that ingest into the same downstream table, the two sink table views share the same table path."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-sql",children:"-- NB-CC demo\n\n-- The source table\nCREATE TABLE sourceT (\n  uuid varchar(20),\n  name varchar(10),\n  age int,\n  ts timestamp(3),\n  `partition` as 'par1'\n) WITH (\n  'connector' = 'datagen',\n  'rows-per-second' = '200'\n);\n\n-- table view for writer1\ncreate table t1(\n  uuid varchar(20),\n  name varchar(10),\n  age int,\n  ts timestamp(3),\n  `partition` varchar(20)\n)\nwith (\n  'connector' = 'hudi',\n  'path' = '/Users/chenyuzhao/workspace/hudi-demo/t1',\n  'table.type' = 'MERGE_ON_READ',\n  'index.type' = 'BUCKET',\n  'hoodie.write.concurrency.mode' = 'NON_BLOCKING_CONCURRENCY_CONTROL',\n  'write.tasks' = '2'\n);\n\ninsert into t1/*+options('metadata.enabled'='true')*/ select * from sourceT;\n\n-- table view for writer2\n-- compaction and cleaning are disabled because writer1 has taken care of it.\ncreate table t1_2(\n  uuid varchar(20),\n  name varchar(10),\n  age int,\n  ts timestamp(3),\n  `partition` varchar(20)\n)\nwith (\n  'connector' = 'hudi',\n  'path' = '/Users/chenyuzhao/workspace/hudi-demo/t1',\n  'table.type' = 'MERGE_ON_READ',\n  'index.type' = 'BUCKET',\n  'hoodie.write.concurrency.mode' = 'NON_BLOCKING_CONCURRENCY_CONTROL',\n  'write.tasks' = '2',\n  'compaction.schedule.enabled' = 'false',\n  'compaction.async.enabled' = 'false',\n  'clean.async.enabled' = 'false'\n);\n\n-- executes the ingestion workloads\ninsert into t1 select * from sourceT;\ninsert into t1_2 select * from sourceT;\n"})}),"\n",(0,o.jsx)(n.h2,{id:"future-roadmap",children:"Future Roadmap"}),"\n",(0,o.jsx)(n.p,{children:"While non-blocking concurrency control is a very powerful feature for streaming users, it is a general solution for multiple writer conflict resolution,\nhere are some plans that improve the Hudi core features:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"NBCC support for metadata table"}),"\n",(0,o.jsx)(n.li,{children:"NBCC for clustering"}),"\n",(0,o.jsx)(n.li,{children:"NBCC for other index type"}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.p,{children:["[^1] ",(0,o.jsx)(n.a,{href:"https://github.com/apache/hudi/blob/master/rfc/rfc-66/rfc-66.md",children:"RFC-66"})," well-explained the completion time based file slicing with a pseudocode."]})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}}}]);