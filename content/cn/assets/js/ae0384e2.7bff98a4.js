"use strict";(globalThis.webpackChunkhudi=globalThis.webpackChunkhudi||[]).push([[6791],{28453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>o});var t=r(96540);const i={},a=t.createContext(i);function s(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(a.Provider,{value:n},e.children)}},69240:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>d});var t=r(71611),i=r(74848),a=r(28453);const s={title:"Hudi Z-Order and Hilbert Space Filling Curves",excerpt:"Explore the benefits of new Apache Hudi Z-Order and Hilbert Curves",author:"Alexey Kudinkin and Tao Meng",category:"blog",image:"/assets/images/zordercurve.png",tags:["design","clustering","data skipping","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Background",id:"background",level:3},{value:"Setup",id:"setup",level:3},{value:"Testing",id:"testing",level:3},{value:"Results",id:"results",level:3},{value:"Epilogue",id:"epilogue",level:3}];function c(e){const n={a:"a",code:"code",em:"em",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:["As of Hudi v0.10.0, we are excited to introduce support for an advanced Data Layout Optimization technique known in the database realm as ",(0,i.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Z-order_curve",children:"Z-order"})," and ",(0,i.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Hilbert_curve",children:"Hilbert"})," space filling curves."]}),"\n",(0,i.jsx)(n.h3,{id:"background",children:"Background"}),"\n",(0,i.jsxs)(n.p,{children:["Amazon EMR team recently published a ",(0,i.jsx)(n.a,{href:"https://aws.amazon.com/blogs/big-data/new-features-from-apache-hudi-0-7-0-and-0-8-0-available-on-amazon-emr/",children:"great article"})," show-casing how ",(0,i.jsx)(n.a,{href:"https://hudi.apache.org/docs/clustering",children:"clustering"})," your data can improve your ",(0,i.jsx)(n.em,{children:"query performance"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"To better understand what's going on and how it's related to space-filling curves, let's zoom in to the setup in that article:"}),"\n",(0,i.jsxs)(n.p,{children:["In the article, 2 ",(0,i.jsx)(n.a,{href:"https://hudi.apache.org/docs/overview",children:"Apache Hudi"})," tables are compared (both ingested from the well-known ",(0,i.jsx)(n.a,{href:"https://s3.amazonaws.com/amazon-reviews-pds/tsv/index.txt",children:"Amazon Reviews"})," dataset):"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"amazon_reviews"})," table which is not clustered (ie the data has not been re-ordered by any particular key)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"amazon_reviews_clustered"})," table which is clustered. When data is clustered by Apache Hudi the data is ",(0,i.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Lexicographic_order",children:(0,i.jsx)(n.strong,{children:"lexicographically ordered"})})," (hereon we will be referring to this kind of ordering as ",(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.em,{children:"linear ordering"})}),") by 2 columns: ",(0,i.jsx)(n.code,{children:"star_rating"}),", ",(0,i.jsx)(n.code,{children:"total_votes"})," (see screenshot below)"]}),"\n"]}),"\n",(0,i.jsx)("img",{src:"/assets/images/hudiconfigz.png",alt:"drawing",width:"800"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"Screenshot of the Hudi configuration (from Amazon EMR team article)"})}),"\n",(0,i.jsx)(n.p,{children:"To showcase the improvement in querying performance, the following queries are executed against both of these tables:"}),"\n",(0,i.jsx)("img",{src:"/assets/images/table1.png",alt:"drawing",width:"800"}),"\n",(0,i.jsx)("img",{src:"/assets/images/table2.png",alt:"drawing",width:"800"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"Screenshots of the queries run against the previously setup tables (from Amazon EMR team article)"})}),"\n",(0,i.jsxs)(n.p,{children:["The important consideration to point out here is that the queries were specifying ",(0,i.jsx)(n.strong,{children:"both of the columns"})," latter table is ordered by (both ",(0,i.jsx)(n.code,{children:"star_rating"})," and ",(0,i.jsx)(n.code,{children:"total_votes"}),")."]}),"\n",(0,i.jsx)(n.p,{children:"And this is unfortunately a crucial limitation of the linear/lexicographic ordering, the value of the ordering diminishes very quickly as you add more columns. It's not hard to see why:"}),"\n",(0,i.jsx)("img",{src:"/assets/images/lexicographicorder.png",alt:"drawing",width:"250"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.em,{children:"Courtesy of Wikipedia,"})," ",(0,i.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Lexicographic_order",children:(0,i.jsx)(n.em,{children:"Lexicographic Order article"})})]}),"\n",(0,i.jsxs)(n.p,{children:["From this image you can see that with lexicographically ordered 3-tuples of integers, only the first column is able to feature crucial property of ",(0,i.jsx)(n.strong,{children:"locality"}),' for all of the records having the same value: for ex, all of the records wit values starting with "1", "2", "3" (in the first columns) are nicely clumped together. However if you try to find all the values that have "5" as the value in their third column you\'d see that those are now dispersed all over the place, not being localized at all.']}),"\n",(0,i.jsx)(n.p,{children:"The crucial property that improves query performance is locality: it enables queries to substantially reduce the search space and the number of files that need to be scanned, parsed, etc."}),"\n",(0,i.jsx)(n.p,{children:"But... does this mean that our queries are doomed to do a full-scan if we're filtering by anything other than the first (or more accurate would be to say prefix) of the list of columns the table is ordered by?"}),"\n",(0,i.jsx)(n.p,{children:"Not exactly: luckily, locality is also a property that space-filling curves enable while enumerating multi-dimensional spaces (records in our table could be represented as points in N-dimensional space, where N is the number of columns in our table)"}),"\n",(0,i.jsx)(n.p,{children:"How does it work?"}),"\n",(0,i.jsx)(n.p,{children:"Let's take Z-curve as an example: Z-order curves fitting a 2-dimensional plane would look like the following:"}),"\n",(0,i.jsx)("img",{src:"/assets/images/zordercurve.png",alt:"drawing",width:"400"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.em,{children:"Courtesy of Wikipedia,"})," ",(0,i.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Z-order_curve",children:(0,i.jsx)(n.em,{children:"Z-order curve article"})})]}),"\n",(0,i.jsxs)(n.p,{children:['As you can see following its path, instead of simply ordering by one coordinate ("x") first, following with the other, it\'s actually ordering them as if the bits of those coordinates have been ',(0,i.jsx)(n.em,{children:"interwoven"})," into a single value:"]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Coordinate"}),(0,i.jsx)(n.th,{children:"X (binary)"}),(0,i.jsx)(n.th,{children:"Y (binary)"}),(0,i.jsx)(n.th,{children:"Z-values (ordered)"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"(0, 0)"}),(0,i.jsx)(n.td,{children:"000"}),(0,i.jsx)(n.td,{children:"000"}),(0,i.jsx)(n.td,{children:"000000"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"(1, 0)"}),(0,i.jsx)(n.td,{children:"001"}),(0,i.jsx)(n.td,{children:"000"}),(0,i.jsx)(n.td,{children:"000001"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"(0, 1)"}),(0,i.jsx)(n.td,{children:"000"}),(0,i.jsx)(n.td,{children:"001"}),(0,i.jsx)(n.td,{children:"000010"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"(1, 1)"}),(0,i.jsx)(n.td,{children:"001"}),(0,i.jsx)(n.td,{children:"001"}),(0,i.jsx)(n.td,{children:"000011"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:'This allows for that crucial property of locality (even though a slightly "stretched" one) to be carried over to all columns as compared to just the first one in case of linear ordering.'}),"\n",(0,i.jsxs)(n.p,{children:["In a similar fashion, Hilbert curves also allow you to map points in a N-dimensional space (rows in our table) onto 1-dimensional curve, essentially ",(0,i.jsx)(n.em,{children:"ordering"})," them, while still preserving the crucial property of locality. Read more details about Hilbert Curves ",(0,i.jsx)(n.a,{href:"https://drum.lib.umd.edu/handle/1903/804",children:"here"}),". Our personal experiments so far show that ordering data along a Hilbert curve leads to better clustering and performance outcomes."]}),"\n",(0,i.jsx)(n.p,{children:"Now, let's check it out in action!"}),"\n",(0,i.jsx)(n.h3,{id:"setup",children:"Setup"}),"\n",(0,i.jsxs)(n.p,{children:["We will use the ",(0,i.jsx)(n.a,{href:"https://s3.amazonaws.com/amazon-reviews-pds/readme.html",children:"Amazon Reviews"})," dataset again, but this time we will use Hudi to Z-Order by ",(0,i.jsx)(n.code,{children:"product_id"}),", ",(0,i.jsx)(n.code,{children:"customer_id"})," columns tuple instead of Clustering or ",(0,i.jsx)(n.em,{children:"linear ordering"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["No special preparations are required for the dataset, you can simply download it from ",(0,i.jsx)(n.a,{href:"https://s3.amazonaws.com/amazon-reviews-pds/readme.html",children:"S3"})," in Parquet format and use it directly as an input for Spark ingesting it into Hudi table."]}),"\n",(0,i.jsx)(n.p,{children:"Launch Spark Shell"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"./bin/spark-shell --master 'local[4]' --driver-memory 8G --executor-memory 8G \\\n  --jars ../../packaging/hudi-spark-bundle/target/hudi-spark3-bundle_2.12-0.10.0.jar \\\n  --packages org.apache.spark:spark-avro_2.12:2.4.4 \\\n  --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer'\n"})}),"\n",(0,i.jsx)(n.p,{children:"Paste"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-scala",children:'import org.apache.hadoop.fs.{FileStatus, Path}\nimport scala.collection.JavaConversions._\nimport org.apache.spark.sql.SaveMode._\nimport org.apache.hudi.{DataSourceReadOptions, DataSourceWriteOptions}\nimport org.apache.hudi.DataSourceWriteOptions._\nimport org.apache.hudi.common.fs.FSUtils\nimport org.apache.hudi.common.table.HoodieTableMetaClient\nimport org.apache.hudi.common.util.ClusteringUtils\nimport org.apache.hudi.config.HoodieClusteringConfig\nimport org.apache.hudi.config.HoodieWriteConfig._\nimport org.apache.spark.sql.DataFrame\n\nimport java.util.stream.Collectors\n\nval layoutOptStrategy = "z-order"; // OR "hilbert"\n\nval inputPath = s"file:///${System.getProperty("user.home")}/datasets/amazon_reviews_parquet"\nval tableName = s"amazon_reviews_${layoutOptStrategy}"\nval outputPath = s"file:///tmp/hudi/$tableName"\n\n\ndef safeTableName(s: String) = s.replace(\'-\', \'_\')\n\nval commonOpts =\n  Map(\n    "hoodie.compact.inline" -> "false",\n    "hoodie.bulk_insert.shuffle.parallelism" -> "10"\n  )\n\n\n////////////////////////////////////////////////////////////////\n// Writing to Hudi\n////////////////////////////////////////////////////////////////\n\nval df = spark.read.parquet(inputPath)\n\ndf.write.format("hudi")\n  .option(DataSourceWriteOptions.TABLE_TYPE.key(), COW_TABLE_TYPE_OPT_VAL)\n  .option("hoodie.table.name", tableName)\n  .option(PRECOMBINE_FIELD.key(), "review_id")\n  .option(RECORDKEY_FIELD.key(), "review_id")\n  .option(DataSourceWriteOptions.PARTITIONPATH_FIELD.key(), "product_category")\n  .option("hoodie.clustering.inline", "true")\n  .option("hoodie.clustering.inline.max.commits", "1")\n  // NOTE: Small file limit is intentionally kept _ABOVE_ target file-size max threshold for Clustering,\n  // to force re-clustering\n  .option("hoodie.clustering.plan.strategy.small.file.limit", String.valueOf(1024 * 1024 * 1024)) // 1Gb\n  .option("hoodie.clustering.plan.strategy.target.file.max.bytes", String.valueOf(128 * 1024 * 1024)) // 128Mb\n  // NOTE: We\'re increasing cap on number of file-groups produced as part of the Clustering run to be able to accommodate for the \n  // whole dataset (~33Gb)\n  .option("hoodie.clustering.plan.strategy.max.num.groups", String.valueOf(4096))\n  .option(HoodieClusteringConfig.LAYOUT_OPTIMIZE_ENABLE.key, "true")\n  .option(HoodieClusteringConfig.LAYOUT_OPTIMIZE_STRATEGY.key, layoutOptStrategy)\n  .option(HoodieClusteringConfig.PLAN_STRATEGY_SORT_COLUMNS.key, "product_id,customer_id")\n  .option(DataSourceWriteOptions.OPERATION.key(), DataSourceWriteOptions.BULK_INSERT_OPERATION_OPT_VAL)\n  .option(BULK_INSERT_SORT_MODE.key(), "NONE")\n  .options(commonOpts)\n  .mode(ErrorIfExists)\n  \n'})}),"\n",(0,i.jsx)(n.h3,{id:"testing",children:"Testing"}),"\n",(0,i.jsx)(n.p,{children:"Please keep in mind, that each individual test is run in a separate spark-shell to avoid caching getting in the way of our measurements."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-scala",children:'////////////////////////////////////////////////////////////////\n// Reading\n///////////////////////////////////////////////////////////////\n\n// Temp Table w/ Data Skipping DISABLED\nval readDf: DataFrame =\n  spark.read.option(DataSourceReadOptions.ENABLE_DATA_SKIPPING.key(), "false").format("hudi").load(outputPath)\n\nval rawSnapshotTableName = safeTableName(s"${tableName}_sql_snapshot")\n\nreadDf.createOrReplaceTempView(rawSnapshotTableName)\n\n\n// Temp Table w/ Data Skipping ENABLED\nval readDfSkip: DataFrame =\n  spark.read.option(DataSourceReadOptions.ENABLE_DATA_SKIPPING.key(), "true").format("hudi").load(outputPath)\n\nval dataSkippingSnapshotTableName = safeTableName(s"${tableName}_sql_snapshot_skipping")\n\nreadDfSkip.createOrReplaceTempView(dataSkippingSnapshotTableName)\n\n// Query 1: Total votes by product_category, for 6 months\ndef runQuery1(tableName: String) = {\n  // Query 1: Total votes by product_category, for 6 months\n  spark.sql(s"SELECT sum(total_votes), product_category FROM $tableName WHERE review_date > \'2013-12-15\' AND review_date < \'2014-06-01\' GROUP BY product_category").show()\n}\n\n// Query 2: Average star rating by product_id, for some product\ndef runQuery2(tableName: String) = {\n  spark.sql(s"SELECT avg(star_rating), product_id FROM $tableName WHERE product_id in (\'B0184XC75U\') GROUP BY product_id").show()\n}\n\n// Query 3: Count number of reviews by customer_id for some 5 customers\ndef runQuery3(tableName: String) = {\n  spark.sql(s"SELECT count(*) as num_reviews, customer_id FROM $tableName WHERE customer_id in (\'53096570\',\'10046284\',\'53096576\',\'10000196\',\'21700145\') GROUP BY customer_id").show()\n}\n\n//\n// Query 1: Is a "wide" query and hence it\'s expected to touch a lot of files\n//\nscala> runQuery1(rawSnapshotTableName)\n+----------------+--------------------+\n|sum(total_votes)|    product_category|\n+----------------+--------------------+\n|         1050944|                  PC|\n|          867794|             Kitchen|\n|         1167489|                Home|\n|          927531|            Wireless|\n|            6861|               Video|\n|           39602| Digital_Video_Games|\n|          954924|Digital_Video_Dow...|\n|           81876|             Luggage|\n|          320536|         Video_Games|\n|          817679|              Sports|\n|           11451|  Mobile_Electronics|\n|          228739|  Home_Entertainment|\n|         3769269|Digital_Ebook_Pur...|\n|          252273|                Baby|\n|          735042|             Apparel|\n|           49101|    Major_Appliances|\n|          484732|             Grocery|\n|          285682|               Tools|\n|          459980|         Electronics|\n|          454258|            Outdoors|\n+----------------+--------------------+\nonly showing top 20 rows\n\nscala> runQuery1(dataSkippingSnapshotTableName)\n+----------------+--------------------+\n|sum(total_votes)|    product_category|\n+----------------+--------------------+\n|         1050944|                  PC|\n|          867794|             Kitchen|\n|         1167489|                Home|\n|          927531|            Wireless|\n|            6861|               Video|\n|           39602| Digital_Video_Games|\n|          954924|Digital_Video_Dow...|\n|           81876|             Luggage|\n|          320536|         Video_Games|\n|          817679|              Sports|\n|           11451|  Mobile_Electronics|\n|          228739|  Home_Entertainment|\n|         3769269|Digital_Ebook_Pur...|\n|          252273|                Baby|\n|          735042|             Apparel|\n|           49101|    Major_Appliances|\n|          484732|             Grocery|\n|          285682|               Tools|\n|          459980|         Electronics|\n|          454258|            Outdoors|\n+----------------+--------------------+\nonly showing top 20 rows\n\n//\n// Query 2: Is a "pointwise" query and hence it\'s expected that data-skipping should substantially reduce number \n// of files scanned (as compared to Baseline)\n//\n// NOTE: That Linear Ordering (as compared to Space-curve based on) will have similar effect on performance reducing\n// total # of Parquet files scanned, since we\'re querying on the prefix of the ordering key\n//\nscala> runQuery2(rawSnapshotTableName)\n+----------------+----------+\n|avg(star_rating)|product_id|\n+----------------+----------+\n|             1.0|B0184XC75U|\n+----------------+----------+\n\n\nscala> runQuery2(dataSkippingSnapshotTableName)\n+----------------+----------+\n|avg(star_rating)|product_id|\n+----------------+----------+\n|             1.0|B0184XC75U|\n+----------------+----------+\n\n//\n// Query 3: Similar to Q2, is a "pointwise" query, but querying other part of the ordering-key (product_id, customer_id)\n// and hence it\'s expected that data-skipping should substantially reduce number of files scanned (as compared to Baseline, Linear Ordering).\n//\n// NOTE: That Linear Ordering (as compared to Space-curve based on) will _NOT_ have similar effect on performance reducing\n// total # of Parquet files scanned, since we\'re NOT querying on the prefix of the ordering key\n//\nscala> runQuery3(rawSnapshotTableName)\n+-----------+-----------+\n|num_reviews|customer_id|\n+-----------+-----------+\n|         50|   53096570|\n|          3|   53096576|\n|         25|   10046284|\n|          1|   10000196|\n|         14|   21700145|\n+-----------+-----------+\n\nscala> runQuery3(dataSkippingSnapshotTableName)\n+-----------+-----------+\n|num_reviews|customer_id|\n+-----------+-----------+\n|         50|   53096570|\n|          3|   53096576|\n|         25|   10046284|\n|          1|   10000196|\n|         14|   21700145|\n+-----------+-----------+\n'})}),"\n",(0,i.jsx)(n.h3,{id:"results",children:"Results"}),"\n",(0,i.jsx)(n.p,{children:"We've summarized the measured performance metrics below:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:(0,i.jsx)(n.strong,{children:"Query"})}),(0,i.jsxs)(n.th,{children:[(0,i.jsx)(n.strong,{children:"Baseline (B)"})," duration (files scanned / size)"]}),(0,i.jsx)(n.th,{children:(0,i.jsx)(n.strong,{children:"Linear Sorting (S)"})}),(0,i.jsxs)(n.th,{children:[(0,i.jsx)(n.strong,{children:"Z-order (Z)"})," duration (scanned)"]}),(0,i.jsxs)(n.th,{children:[(0,i.jsx)(n.strong,{children:"Hilbert (H)"})," duration (scanned)"]})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Q1"}),(0,i.jsx)(n.td,{children:"14s (543 / 31.4Gb)"}),(0,i.jsx)(n.td,{children:"15s (533 / 28.8Gb)"}),(0,i.jsx)(n.td,{children:"15s (543 / 31.4Gb)"}),(0,i.jsx)(n.td,{children:"14s (541 / 31.3Gb)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Q2"}),(0,i.jsx)(n.td,{children:"21s (543 / 31.4Gb)"}),(0,i.jsx)(n.td,{children:"10s (533 / 28.8Gb)"}),(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.strong,{children:"8s"})," ",(0,i.jsx)(n.strong,{children:"(243 / 14.4Gb)"})]}),(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.strong,{children:"7s"})," ",(0,i.jsx)(n.strong,{children:"(237 / 13.9Gb)"})]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Q3"}),(0,i.jsx)(n.td,{children:"17s (543 / 31.4Gb)"}),(0,i.jsx)(n.td,{children:"15s (533 / 28.8Gb)"}),(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.strong,{children:"6s"})," ",(0,i.jsx)(n.strong,{children:"(224 / 12.4Gb)"})]}),(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.strong,{children:"6s"})," ",(0,i.jsx)(n.strong,{children:"(219 / 11.9Gb)"})]})]})]})]}),"\n",(0,i.jsx)(n.p,{children:"As you can see multi-column linear ordering is not very effective for the queries that do filtering by columns other than the first one (Q2, Q3)."}),"\n",(0,i.jsxs)(n.p,{children:["Which is a very clear contrast with space-filling curves (both Z-order and Hilbert) that allow to speed up query time by up to ",(0,i.jsx)(n.strong,{children:"3x!"})]}),"\n",(0,i.jsxs)(n.p,{children:["It's worth noting that the performance gains are heavily dependent on your underlying data and queries. In benchmarks on our internal data we were able to achieve queries performance improvements of more than ",(0,i.jsx)(n.strong,{children:"11x!"})]}),"\n",(0,i.jsx)(n.h3,{id:"epilogue",children:"Epilogue"}),"\n",(0,i.jsx)(n.p,{children:"Apache Hudi v0.10 brings new layout optimization capabilities Z-order and Hilbert to open source. Using these industry leading layout optimization techniques can bring substantial performance improvement and cost savings to your queries!"})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},71611:e=>{e.exports=JSON.parse('{"permalink":"/cn/blog/2021/12/29/hudi-zorder-and-hilbert-space-filling-curves","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-12-29-hudi-zorder-and-hilbert-space-filling-curves.md","source":"@site/blog/2021-12-29-hudi-zorder-and-hilbert-space-filling-curves.md","title":"Hudi Z-Order and Hilbert Space Filling Curves","description":"As of Hudi v0.10.0, we are excited to introduce support for an advanced Data Layout Optimization technique known in the database realm as Z-order and Hilbert space filling curves.","date":"2021-12-29T00:00:00.000Z","tags":[{"inline":true,"label":"design","permalink":"/cn/blog/tags/design"},{"inline":true,"label":"clustering","permalink":"/cn/blog/tags/clustering"},{"inline":true,"label":"data skipping","permalink":"/cn/blog/tags/data-skipping"},{"inline":true,"label":"apache hudi","permalink":"/cn/blog/tags/apache-hudi"}],"readingTime":8.64,"hasTruncateMarker":true,"authors":[{"name":"Alexey Kudinkin and Tao Meng","key":null,"page":null}],"frontMatter":{"title":"Hudi Z-Order and Hilbert Space Filling Curves","excerpt":"Explore the benefits of new Apache Hudi Z-Order and Hilbert Curves","author":"Alexey Kudinkin and Tao Meng","category":"blog","image":"/assets/images/zordercurve.png","tags":["design","clustering","data skipping","apache hudi"]},"unlisted":false,"prevItem":{"title":"The Art of Building Open Data Lakes with Apache Hudi, Kafka, Hive, and Debezium","permalink":"/cn/blog/2021/12/31/The-Art-of-Building-Open-Data-Lakes-with-Apache-Hudi-Kafka-Hive-and-Debezium"},"nextItem":{"title":"New features from Apache Hudi 0.7.0 and 0.8.0 available on Amazon EMR","permalink":"/cn/blog/2021/12/20/New-features-from-Apache-Hudi-0.7.0-and-0.8.0-available-on-Amazon-EMR"}}')}}]);