<!doctype html>
<html lang="cn" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.3">
<link rel="alternate" type="application/rss+xml" href="/cn/blog/rss.xml" title="Apache Hudi! Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/cn/blog/atom.xml" title="Apache Hudi! Blog Atom Feed">
<link rel="search" type="application/opensearchdescription+xml" title="Apache Hudi!" href="/cn/opensearch.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Comfortaa|Ubuntu|Roboto|Source+Code+Pro">
<link rel="stylesheet" href="https://at-ui.github.io/feather-font/css/iconfont.css"><title data-react-helmet="true">FAQs | Apache Hudi!</title><meta data-react-helmet="true" property="og:url" content="https://hudi.apache.org/cn/learn/faq"><meta data-react-helmet="true" name="docsearch:language" content="cn"><meta data-react-helmet="true" name="docsearch:version" content="current"><meta data-react-helmet="true" name="docsearch:docusaurus_tag" content="docs-learn-current"><meta data-react-helmet="true" property="og:title" content="FAQs | Apache Hudi!"><meta data-react-helmet="true" name="description" content="General"><meta data-react-helmet="true" property="og:description" content="General"><meta data-react-helmet="true" name="keywords" content="hudi,writing,reading"><link data-react-helmet="true" rel="shortcut icon" href="/cn/assets/images/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://hudi.apache.org/cn/learn/faq"><link data-react-helmet="true" rel="alternate" href="https://hudi.apache.org/learn/faq" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://hudi.apache.org/cn/learn/faq" hreflang="cn"><link data-react-helmet="true" rel="alternate" href="https://hudi.apache.org/learn/faq" hreflang="x-default"><link data-react-helmet="true" rel="preconnect" href="https://BH4D9OD16A-dsn.algolia.net" crossorigin="anonymous"><link rel="stylesheet" href="/cn/assets/css/styles.067d5900.css">
<link rel="preload" href="/cn/assets/js/runtime~main.b9689b19.js" as="script">
<link rel="preload" href="/cn/assets/js/main.d0765f1c.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_1oUP">Skip to main content</a></div><div class="announcementBar_3WsW" role="banner"><div class="announcementBarContent_3EUC announcementBarCloseable_3myR">⭐️ If you like Apache Hudi, give it a star on <a target="_blank" rel="noopener noreferrer" href="https://github.com/apache/hudi">GitHub</a>! ⭐</div><button type="button" class="announcementBarClose_38nx clean-btn" aria-label="Close"><span aria-hidden="true">×</span></button></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/cn/"><img src="/cn/assets/images/hudi.png" alt="Apache Hudi" class="themedImage_1VuW themedImage--light_3UqQ navbar__logo"><img src="/cn/assets/images/hudi.png" alt="Apache Hudi" class="themedImage_1VuW themedImage--dark_hz6m navbar__logo"></a><a class="navbar__item navbar__link" href="/cn/docs/quick-start-guide">Docs</a><div class="navbar__item dropdown dropdown--hoverable dropdown--left"><a class="navbar__item navbar__link">Learn</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/cn/blog">Blog</a></li><li><a class="dropdown__link" href="/cn/talks-articles">Talks &amp; Articles</a></li><li><a class="dropdown__link" href="/cn/learn/faq">FAQ</a></li><li><a href="https://cwiki.apache.org/confluence/display/HUDI" target="_blank" rel="noopener noreferrer" class="dropdown__link"><span>Technical Wiki<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable dropdown--left"><a class="navbar__item navbar__link">Contribute</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/cn/contribute/get-involved">Get Involved</a></li><li><a class="dropdown__link" href="/cn/contribute/team">Team</a></li><li><a class="dropdown__link" href="/cn/contribute/how-to-contribute">How to Contribute</a></li><li><a class="dropdown__link" href="/cn/contribute/developer-setup">Developer Setup</a></li><li><a class="dropdown__link" href="/cn/contribute/rfc-process">RFC Process</a></li><li><a class="dropdown__link" href="/cn/contribute/report-security-issues">Report Security Issues</a></li><li><a href="https://issues.apache.org/jira/projects/HUDI/summary" target="_blank" rel="noopener noreferrer" class="dropdown__link"><span>Report Issues<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><a class="navbar__item navbar__link" href="/cn/powered-by">Who&#x27;s Using</a><a class="navbar__item navbar__link" href="/cn/releases/download">Download</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a class="navbar__item navbar__link" href="/cn/docs/overview">0.9.0</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/cn/docs/next/overview">Next</a></li><li><a class="dropdown__link" href="/cn/docs/overview">0.9.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.8.0/overview">0.8.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.7.0/overview">0.7.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.6.0/quick-start-guide">0.6.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.5.3/quick-start-guide">0.5.3</a></li><li><a class="dropdown__link" href="/cn/docs/0.5.2/quick-start-guide">0.5.2</a></li><li><a class="dropdown__link" href="/cn/docs/0.5.1/quick-start-guide">0.5.1</a></li><li><a class="dropdown__link" href="/cn/docs/0.5.0/quick-start-guide">0.5.0</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" class="navbar__item navbar__link"><span><svg viewBox="0 0 20 20" width="20" height="20" aria-hidden="true" style="vertical-align:text-bottom;margin-right:5px"><path fill="currentColor" d="M19.753 10.909c-.624-1.707-2.366-2.726-4.661-2.726-.09 0-.176.002-.262.006l-.016-2.063 3.525-.607c.115-.019.133-.119.109-.231-.023-.111-.167-.883-.188-.976-.027-.131-.102-.127-.207-.109-.104.018-3.25.461-3.25.461l-.013-2.078c-.001-.125-.069-.158-.194-.156l-1.025.016c-.105.002-.164.049-.162.148l.033 2.307s-3.061.527-3.144.543c-.084.014-.17.053-.151.143.019.09.19 1.094.208 1.172.018.08.072.129.188.107l2.924-.504.035 2.018c-1.077.281-1.801.824-2.256 1.303-.768.807-1.207 1.887-1.207 2.963 0 1.586.971 2.529 2.328 2.695 3.162.387 5.119-3.06 5.769-4.715 1.097 1.506.256 4.354-2.094 5.98-.043.029-.098.129-.033.207l.619.756c.08.096.206.059.256.023 2.51-1.73 3.661-4.515 2.869-6.683zm-7.386 3.188c-.966-.121-.944-.914-.944-1.453 0-.773.327-1.58.876-2.156a3.21 3.21 0 011.229-.799l.082 4.277a2.773 2.773 0 01-1.243.131zm2.427-.553l.046-4.109c.084-.004.166-.01.252-.01.773 0 1.494.145 1.885.361.391.217-1.023 2.713-2.183 3.758zm-8.95-7.668a.196.196 0 00-.196-.145h-1.95a.194.194 0 00-.194.144L.008 16.916c-.017.051-.011.076.062.076h1.733c.075 0 .099-.023.114-.072l1.008-3.318h3.496l1.008 3.318c.016.049.039.072.113.072h1.734c.072 0 .078-.025.062-.076-.014-.05-3.083-9.741-3.494-11.04zm-2.618 6.318l1.447-5.25 1.447 5.25H3.226z"></path></svg><span>Chinese</span></span></a><ul class="dropdown__menu"><li><a href="/learn/faq" target="_self" rel="noopener noreferrer" class="dropdown__link" style="text-transform:capitalize">English</a></li><li><a href="/cn/learn/faq" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" style="text-transform:capitalize">Chinese</a></li></ul></div><a href="https://github.com/apache/hudi" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><a href="https://twitter.com/ApacheHudi" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-twitter-link" aria-label="Hudi Twitter Handle"></a><a href="https://join.slack.com/t/apache-hudi/shared_invite/enQtODYyNDAxNzc5MTg2LTE5OTBlYmVhYjM0N2ZhOTJjOWM4YzBmMWU2MjZjMGE4NDc5ZDFiOGQ2N2VkYTVkNzU3ZDQ4OTI1NmFmYWQ0NzE" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-slack-link" aria-label="Hudi Slack Channel"></a><div class="searchBox_xXbB"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/cn/"><img src="/cn/assets/images/hudi.png" alt="Apache Hudi" class="themedImage_1VuW themedImage--light_3UqQ navbar__logo"><img src="/cn/assets/images/hudi.png" alt="Apache Hudi" class="themedImage_1VuW themedImage--dark_hz6m navbar__logo"></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/cn/docs/quick-start-guide">Docs</a></li><li class="menu__list-item menu__list-item--collapsed"><a role="button" class="menu__link menu__link--sublist">Learn</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/cn/blog">Blog</a></li><li class="menu__list-item"><a class="menu__link" href="/cn/talks-articles">Talks &amp; Articles</a></li><li class="menu__list-item"><a class="menu__link" href="/cn/learn/faq">FAQ</a></li><li class="menu__list-item"><a href="https://cwiki.apache.org/confluence/display/HUDI" target="_blank" rel="noopener noreferrer" class="menu__link"><span>Technical Wiki<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a role="button" class="menu__link menu__link--sublist">Contribute</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/cn/contribute/get-involved">Get Involved</a></li><li class="menu__list-item"><a class="menu__link" href="/cn/contribute/team">Team</a></li><li class="menu__list-item"><a class="menu__link" href="/cn/contribute/how-to-contribute">How to Contribute</a></li><li class="menu__list-item"><a class="menu__link" href="/cn/contribute/developer-setup">Developer Setup</a></li><li class="menu__list-item"><a class="menu__link" href="/cn/contribute/rfc-process">RFC Process</a></li><li class="menu__list-item"><a class="menu__link" href="/cn/contribute/report-security-issues">Report Security Issues</a></li><li class="menu__list-item"><a href="https://issues.apache.org/jira/projects/HUDI/summary" target="_blank" rel="noopener noreferrer" class="menu__link"><span>Report Issues<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></li><li class="menu__list-item"><a class="menu__link" href="/cn/powered-by">Who&#x27;s Using</a></li><li class="menu__list-item"><a class="menu__link" href="/cn/releases/download">Download</a></li><li class="menu__list-item menu__list-item--collapsed"><a role="button" class="menu__link menu__link--sublist">Versions</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/cn/docs/next/overview">Next</a></li><li class="menu__list-item"><a class="menu__link" href="/cn/docs/overview">0.9.0</a></li><li class="menu__list-item"><a class="menu__link" href="/cn/docs/0.8.0/overview">0.8.0</a></li><li class="menu__list-item"><a class="menu__link" href="/cn/docs/0.7.0/overview">0.7.0</a></li><li class="menu__list-item"><a class="menu__link" href="/cn/docs/0.6.0/quick-start-guide">0.6.0</a></li><li class="menu__list-item"><a class="menu__link" href="/cn/docs/0.5.3/quick-start-guide">0.5.3</a></li><li class="menu__list-item"><a class="menu__link" href="/cn/docs/0.5.2/quick-start-guide">0.5.2</a></li><li class="menu__list-item"><a class="menu__link" href="/cn/docs/0.5.1/quick-start-guide">0.5.1</a></li><li class="menu__list-item"><a class="menu__link" href="/cn/docs/0.5.0/quick-start-guide">0.5.0</a></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a href="#" role="button" class="menu__link menu__link--sublist"><span><svg viewBox="0 0 20 20" width="20" height="20" aria-hidden="true" style="vertical-align:text-bottom;margin-right:5px"><path fill="currentColor" d="M19.753 10.909c-.624-1.707-2.366-2.726-4.661-2.726-.09 0-.176.002-.262.006l-.016-2.063 3.525-.607c.115-.019.133-.119.109-.231-.023-.111-.167-.883-.188-.976-.027-.131-.102-.127-.207-.109-.104.018-3.25.461-3.25.461l-.013-2.078c-.001-.125-.069-.158-.194-.156l-1.025.016c-.105.002-.164.049-.162.148l.033 2.307s-3.061.527-3.144.543c-.084.014-.17.053-.151.143.019.09.19 1.094.208 1.172.018.08.072.129.188.107l2.924-.504.035 2.018c-1.077.281-1.801.824-2.256 1.303-.768.807-1.207 1.887-1.207 2.963 0 1.586.971 2.529 2.328 2.695 3.162.387 5.119-3.06 5.769-4.715 1.097 1.506.256 4.354-2.094 5.98-.043.029-.098.129-.033.207l.619.756c.08.096.206.059.256.023 2.51-1.73 3.661-4.515 2.869-6.683zm-7.386 3.188c-.966-.121-.944-.914-.944-1.453 0-.773.327-1.58.876-2.156a3.21 3.21 0 011.229-.799l.082 4.277a2.773 2.773 0 01-1.243.131zm2.427-.553l.046-4.109c.084-.004.166-.01.252-.01.773 0 1.494.145 1.885.361.391.217-1.023 2.713-2.183 3.758zm-8.95-7.668a.196.196 0 00-.196-.145h-1.95a.194.194 0 00-.194.144L.008 16.916c-.017.051-.011.076.062.076h1.733c.075 0 .099-.023.114-.072l1.008-3.318h3.496l1.008 3.318c.016.049.039.072.113.072h1.734c.072 0 .078-.025.062-.076-.014-.05-3.083-9.741-3.494-11.04zm-2.618 6.318l1.447-5.25 1.447 5.25H3.226z"></path></svg><span>Languages</span></span></a><ul class="menu__list"><li class="menu__list-item"><a href="/learn/faq" target="_self" rel="noopener noreferrer" class="menu__link" style="text-transform:capitalize">English</a></li><li class="menu__list-item"><a href="/cn/learn/faq" target="_self" rel="noopener noreferrer" class="menu__link dropdown__link--active" style="text-transform:capitalize">Chinese</a></li></ul></li><li class="menu__list-item"><a href="https://github.com/apache/hudi" target="_blank" rel="noopener noreferrer" class="menu__link header-github-link" aria-label="GitHub repository"></a></li><li class="menu__list-item"><a href="https://twitter.com/ApacheHudi" target="_blank" rel="noopener noreferrer" class="menu__link header-twitter-link" aria-label="Hudi Twitter Handle"></a></li><li class="menu__list-item"><a href="https://join.slack.com/t/apache-hudi/shared_invite/enQtODYyNDAxNzc5MTg2LTE5OTBlYmVhYjM0N2ZhOTJjOWM4YzBmMWU2MjZjMGE4NDc5ZDFiOGQ2N2VkYTVkNzU3ZDQ4OTI1NmFmYWQ0NzE" target="_blank" rel="noopener noreferrer" class="menu__link header-slack-link" aria-label="Hudi Slack Channel"></a></li></ul></div></div></div></nav><div class="main-wrapper docs-wrapper doc-page"><div class="docPage_31aa"><aside class="docSidebarContainer_3Kbt"><div class="sidebar_15mo"><nav class="menu menu--responsive thin-scrollbar menu_Bmed menuWithAnnouncementBar_2WvA" aria-label="Sidebar navigation"><button aria-label="Open menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg class="sidebarMenuIcon_fgN0" width="24" height="24" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" href="/cn/learn/faq">FAQs</a></li></ul></nav></div></aside><main class="docMainContainer_3ufF"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_3FnS"><div class="docItemContainer_33ec"><article><div class="markdown"><header><h1 class="h1Heading_27L5">FAQs</h1></header><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="general"></a>General<a class="hash-link" href="#general" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="when-is-hudi-useful-for-me-or-my-organization"></a>When is Hudi useful for me or my organization?<a class="hash-link" href="#when-is-hudi-useful-for-me-or-my-organization" title="Direct link to heading">#</a></h3><p>If you are looking to quickly ingest data onto HDFS or cloud storage, Hudi can provide you tools to <a href="https://hudi.apache.org/docs/writing_data/" target="_blank" rel="noopener noreferrer">help</a>. Also, if you have ETL/hive/spark jobs which are slow/taking up a lot of resources, Hudi can potentially help by providing an incremental approach to reading and writing data.</p><p>As an organization, Hudi can help you build an <a href="https://docs.google.com/presentation/d/1FHhsvh70ZP6xXlHdVsAI0g__B_6Mpto5KQFlZ0b8-mM/edit#slide=id.p" target="_blank" rel="noopener noreferrer">efficient data lake</a>, solving some of the most complex, low-level storage management problems, while putting data into hands of your data analysts, engineers and scientists much quicker.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="what-are-some-non-goals-for-hudi"></a>What are some non-goals for Hudi?<a class="hash-link" href="#what-are-some-non-goals-for-hudi" title="Direct link to heading">#</a></h3><p>Hudi is not designed for any OLTP use-cases, where typically you are using existing NoSQL/RDBMS data stores. Hudi cannot replace your in-memory analytical database (at-least not yet!). Hudi support near-real time ingestion in the order of few minutes, trading off latency for efficient batching. If you truly desirable sub-minute processing delays, then stick with your favorite stream processing solution. </p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="what-is-incremental-processing-why-does-hudi-docstalks-keep-talking-about-it"></a>What is incremental processing? Why does Hudi docs/talks keep talking about it?<a class="hash-link" href="#what-is-incremental-processing-why-does-hudi-docstalks-keep-talking-about-it" title="Direct link to heading">#</a></h3><p>Incremental processing was first introduced by Vinoth Chandar, in the O&#x27;reilly <a href="https://www.oreilly.com/content/ubers-case-for-incremental-processing-on-hadoop/" target="_blank" rel="noopener noreferrer">blog</a>, that set off most of this effort. In purely technical terms, incremental processing merely refers to writing mini-batch programs in streaming processing style. Typical batch jobs consume <strong>all input</strong> and recompute <strong>all output</strong>, every few hours. Typical stream processing jobs consume some <strong>new input</strong> and recompute <strong>new/changes to output</strong>, continuously/every few seconds. While recomputing all output in batch fashion can be simpler, it&#x27;s wasteful and resource expensive. Hudi brings ability to author the same batch pipelines in streaming fashion, run every few minutes.</p><p>While we can merely refer to this as stream processing, we call it <em>incremental processing</em>, to distinguish from purely stream processing pipelines built using Apache Flink, Apache Apex or Apache Kafka Streams.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="what-is-the-difference-between-copy-on-write-cow-vs-merge-on-read-mor-storage-types"></a>What is the difference between copy-on-write (COW) vs merge-on-read (MOR) storage types?<a class="hash-link" href="#what-is-the-difference-between-copy-on-write-cow-vs-merge-on-read-mor-storage-types" title="Direct link to heading">#</a></h3><p><strong>Copy On Write</strong> - This storage type enables clients to ingest data on columnar file formats, currently parquet. Any new data that is written to the Hudi dataset using COW storage type, will write new parquet files. Updating an existing set of rows will result in a rewrite of the entire parquet files that collectively contain the affected rows being updated. Hence, all writes to such datasets are limited by parquet writing performance, the larger the parquet file, the higher is the time taken to ingest the data.</p><p><strong>Merge On Read</strong> - This storage type enables clients to  ingest data quickly onto row based data format such as avro. Any new data that is written to the Hudi dataset using MOR table type, will write new log/delta files that internally store the data as avro encoded bytes. A compaction process (configured as inline or asynchronous) will convert log file format to columnar file format (parquet). Two different InputFormats expose 2 different views of this data, Read Optimized view exposes columnar parquet reading performance while Realtime View exposes columnar and/or log reading performance respectively. Updating an existing set of rows will result in either a) a companion log/delta file for an existing base parquet file generated from a previous compaction or b) an update written to a log/delta file in case no compaction ever happened for it. Hence, all writes to such datasets are limited by avro/log file writing performance, much faster than parquet. Although, there is a higher cost to pay to read log/delta files vs columnar (parquet) files.</p><p>More details can be found <a href="https://hudi.apache.org/docs/concepts/" target="_blank" rel="noopener noreferrer">here</a> and also <a href="https://cwiki.apache.org/confluence/display/HUDI/Design+And+Architecture" target="_blank" rel="noopener noreferrer">Design And Architecture</a>.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-do-i-choose-a-storage-type-for-my-workload"></a>How do I choose a storage type for my workload?<a class="hash-link" href="#how-do-i-choose-a-storage-type-for-my-workload" title="Direct link to heading">#</a></h3><p>A key goal of Hudi is to provide <strong>upsert functionality</strong> that is orders of magnitude faster than rewriting entire tables or partitions.</p><p>Choose Copy-on-write storage if :</p><ul><li>You are looking for a simple alternative, that replaces your existing parquet tables without any need for real-time data.</li><li>Your current job is rewriting entire table/partition to deal with updates, while only a few files actually change in each partition.</li><li>You are happy keeping things operationally simpler (no compaction etc), with the ingestion/write performance bound by the <a href="https://hudi.apache.org/docs/configurations#hoodieparquetmaxfilesize" target="_blank" rel="noopener noreferrer">parquet file size</a> and the number of such files affected/dirtied by updates</li><li>Your workload is fairly well-understood and does not have sudden bursts of large amount of update or inserts to older partitions. COW absorbs all the merging cost on the writer side and thus these sudden changes can clog up your ingestion and interfere with meeting normal mode ingest latency targets.</li></ul><p>Choose merge-on-read storage if :</p><ul><li>You want the data to be ingested as quickly &amp; queryable as much as possible.</li><li>Your workload can have sudden spikes/changes in pattern (e.g bulk updates to older transactions in upstream database causing lots of updates to old partitions on DFS). Asynchronous compaction helps amortize the write amplification caused by such scenarios, while normal ingestion keeps up with incoming stream of changes.</li></ul><p>Immaterial of what you choose, Hudi provides</p><ul><li>Snapshot isolation and atomic write of batch of records</li><li>Incremental pulls</li><li>Ability to de-duplicate data</li></ul><p>Find more <a href="https://hudi.apache.org/docs/concepts/" target="_blank" rel="noopener noreferrer">here</a>.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="is-hudi-an-analytical-database"></a>Is Hudi an analytical database?<a class="hash-link" href="#is-hudi-an-analytical-database" title="Direct link to heading">#</a></h3><p>A typical database has a bunch of long running storage servers always running, which takes writes and reads. Hudi&#x27;s architecture is very different and for good reasons. It&#x27;s highly decoupled where writes and queries/reads can be scaled independently to be able to handle the scale challenges. So, it may not always seems like a database.</p><p>Nonetheless, Hudi is designed very much like a database and provides similar functionality (upserts, change capture) and semantics (transactional writes, snapshot isolated reads).</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-do-i-model-the-data-stored-in-hudi"></a>How do I model the data stored in Hudi?<a class="hash-link" href="#how-do-i-model-the-data-stored-in-hudi" title="Direct link to heading">#</a></h3><p>When writing data into Hudi, you model the records like how you would on a key-value store - specify a key field (unique for a single partition/across dataset), a partition field (denotes partition to place key into) and preCombine/combine logic that specifies how to handle duplicates in a batch of records written. This model enables Hudi to enforce primary key constraints like you would get on a database table. See <a href="https://hudi.apache.org/docs/writing_data/" target="_blank" rel="noopener noreferrer">here</a> for an example.</p><p>When querying/reading data, Hudi just presents itself as a json-like hierarchical table, everyone is used to querying using Hive/Spark/Presto over Parquet/Json/Avro. </p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="does-hudi-support-cloud-storageobject-stores"></a>Does Hudi support cloud storage/object stores?<a class="hash-link" href="#does-hudi-support-cloud-storageobject-stores" title="Direct link to heading">#</a></h3><p>Yes. Generally speaking, Hudi is able to provide its functionality on any Hadoop FileSystem implementation and thus can read and write datasets on <a href="https://hudi.apache.org/docs/cloud" target="_blank" rel="noopener noreferrer">Cloud stores</a> (Amazon S3 or Microsoft Azure or Google Cloud Storage). Over time, Hudi has also incorporated specific design aspects that make building Hudi datasets on the cloud easy, such as <a href="https://hudi.apache.org/docs/configurations#hoodieconsistencycheckenabled" target="_blank" rel="noopener noreferrer">consistency checks for s3</a>, Zero moves/renames involved for data files.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="what-versions-of-hivesparkhadoop-are-support-by-hudi"></a>What versions of Hive/Spark/Hadoop are support by Hudi?<a class="hash-link" href="#what-versions-of-hivesparkhadoop-are-support-by-hudi" title="Direct link to heading">#</a></h3><p>As of September 2019, Hudi can support Spark 2.1+, Hive 2.x, Hadoop 2.7+ (not Hadoop 3).</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-does-hudi-actually-store-data-inside-a-dataset"></a>How does Hudi actually store data inside a dataset?<a class="hash-link" href="#how-does-hudi-actually-store-data-inside-a-dataset" title="Direct link to heading">#</a></h3><p>At a high level, Hudi is based on MVCC design that writes data to versioned parquet/base files and log files that contain changes to the base file. All the files are stored under a partitioning scheme for the dataset, which closely resembles how Apache Hive tables are laid out on DFS. Please refer <a href="https://hudi.apache.org/docs/concepts/" target="_blank" rel="noopener noreferrer">here</a> for more details.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="using-hudi"></a>Using Hudi<a class="hash-link" href="#using-hudi" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="what-are-some-ways-to-write-a-hudi-dataset"></a>What are some ways to write a Hudi dataset?<a class="hash-link" href="#what-are-some-ways-to-write-a-hudi-dataset" title="Direct link to heading">#</a></h3><p>Typically, you obtain a set of partial updates/inserts from your source and issue <a href="https://hudi.apache.org/docs/writing_data/" target="_blank" rel="noopener noreferrer">write operations</a> against a Hudi dataset.  If you ingesting data from any of the standard sources like Kafka, or tailing DFS, the <a href="https://hudi.apache.org/docs/writing_data/#deltastreamer" target="_blank" rel="noopener noreferrer">delta streamer</a> tool is invaluable and provides an easy, self-managed solution to getting data written into Hudi. You can also write your own code to capture data from a custom source using the Spark datasource API and use a <a href="https://hudi.apache.org/docs/writing_data/#datasource-writer" target="_blank" rel="noopener noreferrer">Hudi datasource</a> to write into Hudi. </p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-is-a-hudi-job-deployed"></a>How is a Hudi job deployed?<a class="hash-link" href="#how-is-a-hudi-job-deployed" title="Direct link to heading">#</a></h3><p>The nice thing about Hudi writing is that it just runs like any other spark job would on a YARN/Mesos or even a K8S cluster. So you could simply use the Spark UI to get visibility into write operations.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-can-i-now-query-the-hudi-dataset-i-just-wrote"></a>How can I now query the Hudi dataset I just wrote?<a class="hash-link" href="#how-can-i-now-query-the-hudi-dataset-i-just-wrote" title="Direct link to heading">#</a></h3><p>Unless Hive sync is enabled, the dataset written by Hudi using one of the methods above can simply be queries via the Spark datasource like any other source. </p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly scala"><pre tabindex="0" class="prism-code language-scala codeBlock_23N8 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#F8F8F2"><span class="token plain">val hoodieROView = spark.read.format(&quot;org.apache.hudi&quot;).load(basePath + &quot;/path/to/partitions/*&quot;)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">val hoodieIncViewDF = spark.read().format(&quot;org.apache.hudi&quot;)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     .option(DataSourceReadOptions.VIEW_TYPE_OPT_KEY(), DataSourceReadOptions.VIEW_TYPE_INCREMENTAL_OPT_VAL())</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     .option(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY(), &lt;beginInstantTime&gt;)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     .load(basePath);</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly java"><pre tabindex="0" class="prism-code language-java codeBlock_23N8 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#F8F8F2"><span class="token plain">Limitations:</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Note that currently the reading realtime view natively out of the Spark datasource is not supported. Please use the Hive path below</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><p>if Hive Sync is enabled in the <a href="https://github.com/apache/hudi/blob/d3edac4612bde2fa9deca9536801dbc48961fb95/docker/demo/sparksql-incremental.commands#L50" target="_blank" rel="noopener noreferrer">deltastreamer</a> tool or <a href="https://hudi.apache.org/docs/configurations#hoodiedatasourcehive_syncenable" target="_blank" rel="noopener noreferrer">datasource</a>, the dataset is available in Hive as a couple of tables, that can now be read using HiveQL, Presto or SparkSQL. See <a href="https://hudi.apache.org/docs/querying_data/" target="_blank" rel="noopener noreferrer">here</a> for more.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-does-hudi-handle-duplicate-record-keys-in-an-input"></a>How does Hudi handle duplicate record keys in an input?<a class="hash-link" href="#how-does-hudi-handle-duplicate-record-keys-in-an-input" title="Direct link to heading">#</a></h3><p>When issuing an <code>upsert</code> operation on a dataset and the batch of records provided contains multiple entries for a given key, then all of them are reduced into a single final value by repeatedly calling payload class&#x27;s <a href="https://github.com/apache/hudi/blob/d3edac4612bde2fa9deca9536801dbc48961fb95/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordPayload.java#L40" target="_blank" rel="noopener noreferrer">preCombine()</a> method . By default, we pick the record with the greatest value (determined by calling .compareTo()) giving latest-write-wins style semantics. <a href="https://hudi.apache.org/learn/faq#can-i-implement-my-own-logic-for-how-input-records-are-merged-with-record-on-storage" target="_blank" rel="noopener noreferrer">This FAQ entry</a> shows the interface for HoodieRecordPayload if you are interested.</p><p>For an insert or bulk_insert operation, no such pre-combining is performed. Thus, if your input contains duplicates, the dataset would also contain duplicates. If you don&#x27;t want duplicate records either issue an upsert or consider specifying option to de-duplicate input in either <a href="https://hudi.apache.org/docs/configurations#hoodiedatasourcewriteinsertdropduplicates" target="_blank" rel="noopener noreferrer">datasource</a> or <a href="https://github.com/apache/hudi/blob/d3edac4612bde2fa9deca9536801dbc48961fb95/hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java#L229" target="_blank" rel="noopener noreferrer">deltastreamer</a>.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="can-i-implement-my-own-logic-for-how-input-records-are-merged-with-record-on-storage"></a>Can I implement my own logic for how input records are merged with record on storage?<a class="hash-link" href="#can-i-implement-my-own-logic-for-how-input-records-are-merged-with-record-on-storage" title="Direct link to heading">#</a></h3><p>Here is the payload interface that is used in Hudi to represent any hudi record. </p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly java"><pre tabindex="0" class="prism-code language-java codeBlock_23N8 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#F8F8F2"><span class="token plain">public interface HoodieRecordPayload&lt;T extends HoodieRecordPayload&gt; extends Serializable {</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> /**</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   * When more than one HoodieRecord have the same HoodieKey, this function combines them before attempting to insert/upsert by taking in a property map.</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   * Implementation can leverage the property to decide their business logic to do preCombine.</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   * @param another instance of another {@link HoodieRecordPayload} to be combined with.</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   * @param properties Payload related properties. For example pass the ordering field(s) name to extract from value in storage.</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   * @return the combined value</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   */</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  default T preCombine(T another, Properties properties);</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> </span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">/**</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   * This methods lets you write custom merging/combining logic to produce new values as a function of current value on storage and whats contained</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   * in this object. Implementations can leverage properties if required.</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   * &lt;p&gt;</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   * eg:</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   * 1) You are updating counters, you may want to add counts to currentValue and write back updated counts</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   * 2) You may be reading DB redo logs, and merge them with current image for a database row on storage</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   * &lt;/p&gt;</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   *</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   * @param currentValue Current value in storage, to merge/combine this payload with</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   * @param schema Schema used for record</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   * @param properties Payload related properties. For example pass the ordering field(s) name to extract from value in storage.</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   * @return new combined/merged value to be written back to storage. EMPTY to skip writing this record.</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   */</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  default Option&lt;IndexedRecord&gt; combineAndGetUpdateValue(IndexedRecord currentValue, Schema schema, Properties properties) throws IOException;</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   </span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">/**</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   * Generates an avro record out of the given HoodieRecordPayload, to be written out to storage. Called when writing a new value for the given</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   * HoodieKey, wherein there is no existing record in storage to be combined against. (i.e insert) Return EMPTY to skip writing this record.</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   * Implementations can leverage properties if required.</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   * @param schema Schema used for record</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   * @param properties Payload related properties. For example pass the ordering field(s) name to extract from value in storage.</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   * @return the {@link IndexedRecord} to be inserted.</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   */</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  @PublicAPIMethod(maturity = ApiMaturityLevel.STABLE)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  default Option&lt;IndexedRecord&gt; getInsertValue(Schema schema, Properties properties) throws IOException;</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> </span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">/**</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   * This method can be used to extract some metadata from HoodieRecordPayload. The metadata is passed to {@code WriteStatus.markSuccess()} and</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   * {@code WriteStatus.markFailure()} in order to compute some aggregate metrics using the metadata in the context of a write success or failure.</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   * @return the metadata in the form of Map&lt;String, String&gt; if any.</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   */</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  @PublicAPIMethod(maturity = ApiMaturityLevel.STABLE)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  default Option&lt;Map&lt;String, String&gt;&gt; getMetadata() {</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    return Option.empty();</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  }</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> </span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><p>As you could see, (<a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordPayload.java" target="_blank" rel="noopener noreferrer">combineAndGetUpdateValue(), getInsertValue()</a>) that control how the record on storage is combined with the incoming update/insert to generate the final value to be written back to storage. preCombine() is used to merge records within the same incoming batch. </p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-do-i-delete-records-in-the-dataset-using-hudi"></a>How do I delete records in the dataset using Hudi?<a class="hash-link" href="#how-do-i-delete-records-in-the-dataset-using-hudi" title="Direct link to heading">#</a></h3><p>GDPR has made deletes a must-have tool in everyone&#x27;s data management toolbox. Hudi supports both soft and hard deletes. For details on how to actually perform them, see <a href="https://hudi.apache.org/docs/writing_data/#deletes" target="_blank" rel="noopener noreferrer">here</a>.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="does-deleted-records-appear-in-hudis-incremental-query-results"></a>Does deleted records appear in Hudi&#x27;s incremental query results?<a class="hash-link" href="#does-deleted-records-appear-in-hudis-incremental-query-results" title="Direct link to heading">#</a></h3><p>Soft Deletes (unlike hard deletes) do appear in the incremental pull query results. So, if you need a mechanism to propagate deletes to downstream tables, you can use Soft deletes.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-do-i-migrate-my-data-to-hudi"></a>How do I migrate my data to Hudi?<a class="hash-link" href="#how-do-i-migrate-my-data-to-hudi" title="Direct link to heading">#</a></h3><p>Hudi provides built in support for rewriting your entire dataset into Hudi one-time using the HDFSParquetImporter tool available from the hudi-cli . You could also do this via a simple read and write of the dataset using the Spark datasource APIs. Once migrated, writes can be performed using normal means discussed <a href="https://hudi.apache.org/learn/faq#what-are-some-ways-to-write-a-hudi-dataset" target="_blank" rel="noopener noreferrer">here</a>. This topic is discussed in detail <a href="https://hudi.apache.org/docs/migration_guide/" target="_blank" rel="noopener noreferrer">here</a>, including ways to doing partial migrations.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-can-i-pass-hudi-configurations-to-my-spark-job"></a>How can I pass hudi configurations to my spark job?<a class="hash-link" href="#how-can-i-pass-hudi-configurations-to-my-spark-job" title="Direct link to heading">#</a></h3><p>Hudi configuration options covering the datasource and low level Hudi write client (which both deltastreamer &amp; datasource internally call) are <a href="https://hudi.apache.org/docs/configurations/" target="_blank" rel="noopener noreferrer">here</a>. Invoking <em>--help</em> on any tool such as DeltaStreamer would print all the usage options. A lot of the options that control upsert, file sizing behavior are defined at the write client level and below is how we pass them to different options available for writing data.</p><ul><li>For Spark DataSource, you can use the &quot;options&quot; API of DataFrameWriter to pass in these configs. </li></ul><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly scala"><pre tabindex="0" class="prism-code language-scala codeBlock_23N8 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#F8F8F2"><span class="token plain">inputDF.write().format(&quot;org.apache.hudi&quot;)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  .options(clientOpts) // any of the Hudi client opts can be passed in as well</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), &quot;_row_key&quot;)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  ...</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><ul><li><p>When using <code>HoodieWriteClient</code> directly, you can simply construct HoodieWriteConfig object with the configs in the link you mentioned.</p></li><li><p>When using HoodieDeltaStreamer tool to ingest, you can set the configs in properties file and pass the file as the cmdline argument &quot;<em>--props</em>&quot;</p></li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-to-create-hive-style-partition-folder-structure"></a>How to create Hive style partition folder structure?<a class="hash-link" href="#how-to-create-hive-style-partition-folder-structure" title="Direct link to heading">#</a></h3><p>By default Hudi creates the partition folders with just the partition values, but if would like to create partition folders similar to the way Hive will generate the structure, with paths that contain key value pairs, like country=us/… or datestr=2021-04-20. This is Hive style (or format) partitioning. The paths include both the names of the partition keys and the values that each path represents.</p><p>To enable hive style partitioning, you need to add this hoodie config when you write your data:</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly java"><pre tabindex="0" class="prism-code language-java codeBlock_23N8 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#F8F8F2"><span class="token plain">hoodie.datasource.write.hive_style_partitioning: true</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-do-i-pass-hudi-configurations-to-my-beeline-hive-queries"></a>How do I pass hudi configurations to my beeline Hive queries?<a class="hash-link" href="#how-do-i-pass-hudi-configurations-to-my-beeline-hive-queries" title="Direct link to heading">#</a></h3><p>If Hudi&#x27;s input format is not picked the returned results may be incorrect. To ensure correct inputformat is picked, please use <code>org.apache.hadoop.hive.ql.io.HiveInputFormat</code> or <code>org.apache.hudi.hadoop.hive.HoodieCombineHiveInputFormat</code> for <code>hive.input.format</code> config. This can be set like shown below:</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly java"><pre tabindex="0" class="prism-code language-java codeBlock_23N8 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#F8F8F2"><span class="token plain">set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><p>or</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly java"><pre tabindex="0" class="prism-code language-java codeBlock_23N8 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#F8F8F2"><span class="token plain">set hive.input.format=org.apache.hudi.hadoop.hive.HoodieCombineHiveInputFormat</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="can-i-register-my-hudi-dataset-with-apache-hive-metastore"></a>Can I register my Hudi dataset with Apache Hive metastore?<a class="hash-link" href="#can-i-register-my-hudi-dataset-with-apache-hive-metastore" title="Direct link to heading">#</a></h3><p>Yes. This can be performed either via the standalone <a href="https://hudi.apache.org/docs/writing_data/#syncing-to-hive" target="_blank" rel="noopener noreferrer">Hive Sync tool</a> or using options in <a href="https://github.com/apache/hudi/blob/d3edac4612bde2fa9deca9536801dbc48961fb95/docker/demo/sparksql-incremental.commands#L50" target="_blank" rel="noopener noreferrer">deltastreamer</a> tool or <a href="https://hudi.apache.org/docs/configurations#hoodiedatasourcehive_syncenable" target="_blank" rel="noopener noreferrer">datasource</a>.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-does-the-hudi-indexing-work--what-are-its-benefits"></a>How does the Hudi indexing work &amp; what are its benefits?<a class="hash-link" href="#how-does-the-hudi-indexing-work--what-are-its-benefits" title="Direct link to heading">#</a></h3><p>The indexing component is a key part of the Hudi writing and it maps a given recordKey to a fileGroup inside Hudi consistently. This enables faster identification of the file groups that are affected/dirtied by a given write operation.</p><p>Hudi supports a few options for indexing as below</p><ul><li><em>HoodieBloomIndex (default)</em> : Uses a bloom filter and ranges information placed in the footer of parquet/base files (and soon log files as well)</li><li><em>HoodieGlobalBloomIndex</em> : The default indexing only enforces uniqueness of a key inside a single partition i.e the user is expected to know the partition under which a given record key is stored. This helps the indexing scale very well for even <a href="https://eng.uber.com/uber-big-data-platform/" target="_blank" rel="noopener noreferrer">very large datasets</a>. However, in some cases, it might be necessary instead to do the de-duping/enforce uniqueness across all partitions and the global bloom index does exactly that. If this is used, incoming records are compared to files across the entire dataset and ensure a recordKey is only present in one partition.</li><li><em>HBaseIndex</em> : Apache HBase is a key value store, typically found in close proximity to HDFS. You can also store the index inside HBase, which could be handy if you are already operating HBase.</li></ul><p>You can implement your own index if you&#x27;d like, by subclassing the <code>HoodieIndex</code> class and configuring the index class name in configs. </p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="what-does-the-hudi-cleaner-do"></a>What does the Hudi cleaner do?<a class="hash-link" href="#what-does-the-hudi-cleaner-do" title="Direct link to heading">#</a></h3><p>The Hudi cleaner process often runs right after a commit and deltacommit and goes about deleting old files that are no longer needed. If you are using the incremental pull feature, then ensure you configure the cleaner to <a href="https://hudi.apache.org/docs/configurations#hoodiecleanercommitsretained" target="_blank" rel="noopener noreferrer">retain sufficient amount of last commits</a> to rewind. Another consideration is to provide sufficient time for your long running jobs to finish running. Otherwise, the cleaner could delete a file that is being or could be read by the job and will fail the job. Typically, the default configuration of 10 allows for an ingestion running every 30 mins to retain up-to 5 hours worth of data. If you run ingestion more frequently or if you want to give more running time for a query, consider increasing the  value for the config : <code>hoodie.cleaner.commits.retained</code></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="whats-hudis-schema-evolution-story"></a>What&#x27;s Hudi&#x27;s schema evolution story?<a class="hash-link" href="#whats-hudis-schema-evolution-story" title="Direct link to heading">#</a></h3><p>Hudi uses Avro as the internal canonical representation for records, primarily due to its nice <a href="https://docs.confluent.io/platform/current/schema-registry/avro.html" target="_blank" rel="noopener noreferrer">schema compatibility &amp; evolution</a> properties. This is a key aspect of having reliability in your ingestion or ETL pipelines. As long as the schema passed to Hudi (either explicitly in DeltaStreamer schema provider configs or implicitly by Spark Datasource&#x27;s Dataset schemas) is backwards compatible (e.g no field deletes, only appending new fields to schema), Hudi will seamlessly handle read/write of old and new data and also keep the Hive schema up-to date.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-do-i-run-compaction-for-a-mor-dataset"></a>How do I run compaction for a MOR dataset?<a class="hash-link" href="#how-do-i-run-compaction-for-a-mor-dataset" title="Direct link to heading">#</a></h3><p>Simplest way to run compaction on MOR dataset is to run the <a href="https://hudi.apache.org/docs/configurations#hoodiecompactinline" target="_blank" rel="noopener noreferrer">compaction inline</a>, at the cost of spending more time ingesting; This could be particularly useful, in common cases where you have small amount of late arriving data trickling into older partitions. In such a scenario, you may want to just aggressively compact the last N partitions while waiting for enough logs to accumulate for older partitions. The net effect is that you have converted most of the recent data, that is more likely to be queried to optimized columnar format.</p><p>That said, for obvious reasons of not blocking ingesting for compaction, you may want to run it asynchronously as well. This can be done either via a separate <a href="https://github.com/apache/hudi/blob/master/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactor.java" target="_blank" rel="noopener noreferrer">compaction job</a> that is scheduled by your workflow scheduler/notebook independently. If you are using delta streamer, then you can run in <a href="https://github.com/apache/hudi/blob/d3edac4612bde2fa9deca9536801dbc48961fb95/hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java#L241" target="_blank" rel="noopener noreferrer">continuous mode</a> where the ingestion and compaction are both managed concurrently in a single spark run time.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="what-performanceingest-latency-can-i-expect-for-hudi-writing"></a>What performance/ingest latency can I expect for Hudi writing?<a class="hash-link" href="#what-performanceingest-latency-can-i-expect-for-hudi-writing" title="Direct link to heading">#</a></h3><p>The speed at which you can write into Hudi depends on the <a href="https://hudi.apache.org/docs/writing_data/" target="_blank" rel="noopener noreferrer">write operation</a> and some trade-offs you make along the way like file sizing. Just like how databases incur overhead over direct/raw file I/O on disks,  Hudi operations may have overhead from supporting  database like features compared to reading/writing raw DFS files. That said, Hudi implements advanced techniques from database literature to keep these minimal. User is encouraged to have this perspective when trying to reason about Hudi performance. As the saying goes : there is no free lunch (not yet atleast)</p><table><thead><tr><th>Storage Type</th><th>Type of workload</th><th>Performance</th><th>Tips</th></tr></thead><tbody><tr><td>copy on write</td><td>bulk_insert</td><td>Should match vanilla spark writing + an additional sort to properly size files</td><td>properly size <a href="https://hudi.apache.org/docs/configurations#hoodiebulkinsertshuffleparallelism" target="_blank" rel="noopener noreferrer">bulk insert parallelism</a> to get right number of files. use insert if you want this auto tuned</td></tr><tr><td>copy on write</td><td>insert</td><td>Similar to bulk insert, except the file sizes are auto tuned requiring input to be cached into memory and custom partitioned.</td><td>Performance would be bound by how parallel you can write the ingested data. Tune <a href="https://hudi.apache.org/docs/configurations#hoodieinsertshuffleparallelism" target="_blank" rel="noopener noreferrer">this limit</a> up, if you see that writes are happening from only a few executors.</td></tr><tr><td>copy on write</td><td>upsert/ de-duplicate &amp; insert</td><td>Both of these would involve index lookup.  Compared to naively using Spark (or similar framework)&#x27;s JOIN to identify the affected records, Hudi indexing is often 7-10x faster as long as you have ordered keys (discussed below) or &lt;50% updates. Compared to naively overwriting entire partitions, Hudi write can be several magnitudes faster depending on how many files in a given partition is actually updated. For e.g, if a partition has 1000 files out of which only 100 is dirtied every ingestion run, then Hudi would only read/merge a total of 100 files and thus 10x faster than naively rewriting entire partition.</td><td>Ultimately performance would be bound by how quickly we can read and write a parquet file and that depends on the size of the parquet file, configured <a href="https://hudi.apache.org/docs/configurations#hoodieparquetmaxfilesize" target="_blank" rel="noopener noreferrer">here</a>. Also be sure to properly tune your <a href="https://hudi.apache.org/docs/configurations#Index-Configs" target="_blank" rel="noopener noreferrer">bloom filters</a>. <a href="https://issues.apache.org/jira/browse/HUDI-56" target="_blank" rel="noopener noreferrer">HUDI-56</a> will auto-tune this.</td></tr><tr><td>merge on read</td><td>bulk insert</td><td>Currently new data only goes to parquet files and thus performance here should be similar to copy_on_write bulk insert. This has the nice side-effect of getting data into parquet directly for query performance. <a href="https://issues.apache.org/jira/browse/HUDI-86" target="_blank" rel="noopener noreferrer">HUDI-86</a> will add support for logging inserts directly and this up drastically.</td><td></td></tr><tr><td>merge on read</td><td>insert</td><td>Similar to above</td><td></td></tr><tr><td>merge on read</td><td>upsert/ de-duplicate &amp; insert</td><td>Indexing performance would remain the same as copy-on-write, while ingest latency for updates (costliest I/O operation in copy_on_write) are sent to log files and thus with asynchronous compaction provides very very good ingest performance with low write amplification.</td><td></td></tr></tbody></table><p>Like with many typical system that manage time-series data, Hudi performs much better if your keys have a timestamp prefix or monotonically increasing/decreasing. You can almost always achieve this. Even if you have UUID keys, you can follow tricks like <a href="https://www.percona.com/blog/2014/12/19/store-uuid-optimized-way/" target="_blank" rel="noopener noreferrer">this</a> to get keys that are ordered. See also <a href="https://cwiki.apache.org/confluence/display/HUDI/Tuning+Guide" target="_blank" rel="noopener noreferrer">Tuning Guide</a> for more tips on JVM and other configurations. </p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="what-performance-can-i-expect-for-hudi-readingqueries"></a>What performance can I expect for Hudi reading/queries?<a class="hash-link" href="#what-performance-can-i-expect-for-hudi-readingqueries" title="Direct link to heading">#</a></h3><ul><li>For ReadOptimized views, you can expect the same best in-class columnar query performance as a standard parquet table in Hive/Spark/Presto</li><li>For incremental views, you can expect speed up relative to how much data usually changes in a given time window and how much time your entire scan takes. For e.g, if only 100 files changed in the last hour in a partition of 1000 files, then you can expect a speed of 10x using incremental pull in Hudi compared to full scanning the partition to find out new data.</li><li>For real time views, you can expect performance similar to the same avro backed table in Hive/Spark/Presto </li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-do-i-to-avoid-creating-tons-of-small-files"></a>How do I to avoid creating tons of small files?<a class="hash-link" href="#how-do-i-to-avoid-creating-tons-of-small-files" title="Direct link to heading">#</a></h3><p>A key design decision in Hudi was to avoid creating small files and always write properly sized files.</p><p>There are 2 ways to avoid creating tons of small files in Hudi and both of them have different trade-offs:</p><p>a) <strong>Auto Size small files during ingestion</strong>: This solution trades ingest/writing time to keep queries always efficient. Common approaches to writing very small files and then later stitching them together only solve for system scalability issues posed by small files and also let queries slow down by exposing small files to them anyway.</p><p>Hudi has the ability to maintain a configured target file size, when performing <strong>upsert/insert</strong> operations. (Note: <strong>bulk_insert</strong> operation does not provide this functionality and is designed as a simpler replacement for normal <code>spark.write.parquet</code>  )</p><p>For <strong>copy-on-write</strong>, this is as simple as configuring the <a href="https://hudi.apache.org/docs/configurations#hoodieparquetmaxfilesize" target="_blank" rel="noopener noreferrer">maximum size for a base/parquet file</a> and the <a href="https://hudi.apache.org/docs/configurations#hoodieparquetsmallfilelimit" target="_blank" rel="noopener noreferrer">soft limit</a> below which a file should be considered a small file. For the initial bootstrap to Hudi table, tuning record size estimate is also important to ensure sufficient records are bin-packed in a parquet file. For subsequent writes, Hudi automatically uses average record size based on previous commit. Hudi will try to add enough records to a small file at write time to get it to the configured maximum limit. For e.g , with <code>compactionSmallFileSize=100MB</code> and limitFileSize=120MB, Hudi will pick all files &lt; 100MB and try to get them upto 120MB.</p><p>For <strong>merge-on-read</strong>, there are few more configs to set. MergeOnRead works differently for different INDEX choices.</p><ul><li>Indexes with <strong>canIndexLogFiles = true</strong> : Inserts of new data go directly to log files. In this case, you can configure the <a href="https://hudi.apache.org/docs/configurations#hoodielogfilemaxsize" target="_blank" rel="noopener noreferrer">maximum log size</a> and a <a href="https://hudi.apache.org/docs/configurations#hoodielogfiletoparquetcompressionratio" target="_blank" rel="noopener noreferrer">factor</a> that denotes reduction in size when data moves from avro to parquet files.</li><li>Indexes with <strong>canIndexLogFiles = false</strong> : Inserts of new data go only to parquet files. In this case, the same configurations as above for the COPY_ON_WRITE case applies.</li></ul><p>NOTE : In either case, small files will be auto sized only if there is no PENDING compaction or associated log file for that particular file slice. For example, for case 1: If you had a log file and a compaction C1 was scheduled to convert that log file to parquet, no more inserts can go into that log file. For case 2: If you had a parquet file and an update ended up creating an associated delta log file, no more inserts can go into that parquet file. Only after the compaction has been performed and there are NO log files associated with the base parquet file, can new inserts be sent to auto size that parquet file.</p><p>b) <strong><a href="https://hudi.apache.org/blog/2021/01/27/hudi-clustering-intro" target="_blank" rel="noopener noreferrer">Clustering</a></strong> : This is a feature in Hudi to group small files into larger ones either synchronously or asynchronously. Since first solution of auto-sizing small files has a tradeoff on ingestion speed (since the small files are sized during ingestion), if your use-case is very sensitive to ingestion latency where you don&#x27;t want to compromise on ingestion speed which may end up creating a lot of small files, clustering comes to the rescue. Clustering can be scheduled through the ingestion job and an asynchronus job can stitch small files together in the background to generate larger files. NOTE that during this, ingestion can continue to run concurrently.</p><p><em>Please note that Hudi always creates immutable files on disk. To be able to do auto-sizing or clustering, Hudi will always create a newer version of the smaller file, resulting in 2 versions of the same file. The cleaner service will later kick in and delte the older version small file and keep the latest one.</em> </p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="why-does-hudi-retain-at-least-one-previous-commit-even-after-setting-hoodiecleanercommitsretained-1-"></a>Why does Hudi retain at-least one previous commit even after setting hoodie.cleaner.commits.retained&#x27;: 1 ?<a class="hash-link" href="#why-does-hudi-retain-at-least-one-previous-commit-even-after-setting-hoodiecleanercommitsretained-1-" title="Direct link to heading">#</a></h3><p>Hudi runs cleaner to remove old file versions as part of writing data either in inline or in asynchronous mode (0.6.0 onwards). Hudi Cleaner retains at-least one previous commit when cleaning old file versions. This is to prevent the case when concurrently running queries which are reading the latest file versions suddenly  see those files getting deleted by cleaner because a new file version got added . In other words, retaining at-least one previous commit is needed for ensuring snapshot isolation for readers.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-do-i-use-deltastreamer-or-spark-datasource-api-to-write-to-a-non-partitioned-hudi-dataset-"></a>How do I use DeltaStreamer or Spark DataSource API to write to a Non-partitioned Hudi dataset ?<a class="hash-link" href="#how-do-i-use-deltastreamer-or-spark-datasource-api-to-write-to-a-non-partitioned-hudi-dataset-" title="Direct link to heading">#</a></h3><p>Hudi supports writing to non-partitioned datasets. For writing to a non-partitioned Hudi dataset and performing hive table syncing, you need to set the below configurations in the properties passed:</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly java"><pre tabindex="0" class="prism-code language-java codeBlock_23N8 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#F8F8F2"><span class="token plain">hoodie.datasource.write.keygenerator.class=org.apache.hudi.keygen.NonpartitionedKeyGenerator</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.NonPartitionedExtractor</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="why-do-we-have-to-set-2-different-ways-of-configuring-spark-to-work-with-hudi"></a>Why do we have to set 2 different ways of configuring Spark to work with Hudi?<a class="hash-link" href="#why-do-we-have-to-set-2-different-ways-of-configuring-spark-to-work-with-hudi" title="Direct link to heading">#</a></h3><p>Non-Hive engines tend to do their own listing of DFS to query datasets. For e.g Spark starts reading the paths direct from the file system (HDFS or S3).</p><p>From Spark the calls would be as below:</p><ul><li>org.apache.spark.rdd.NewHadoopRDD.getPartitions</li><li>org.apache.parquet.hadoop.ParquetInputFormat.getSplits</li><li>org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits</li></ul><p>Without understanding of Hudi&#x27;s file layout, engines would just plainly reading all the parquet files and displaying the data within them, with massive amounts of duplicates in the result.</p><p>At a high level, there are two ways of configuring a query engine to properly read Hudi datasets</p><p>A) Making them invoke methods in <code>HoodieParquetInputFormat#getSplits</code> and <code>HoodieParquetInputFormat#getRecordReader</code></p><ul><li>Hive does this natively, since the InputFormat is the abstraction in Hive to plugin new table formats. HoodieParquetInputFormat extends MapredParquetInputFormat which is nothing but a input format for hive and we register Hudi tables to Hive metastore backed by these input formats</li><li>Presto also falls back to calling the input format when it sees a <code>UseFileSplitsFromInputFormat</code> annotation, to just obtain splits, but then goes on to use its own optimized/vectorized parquet reader for queries on Copy-on-Write tables</li><li>Spark can be forced into falling back to the HoodieParquetInputFormat class, using --conf spark.sql.hive.convertMetastoreParquet=false</li></ul><p>B) Making the engine invoke a path filter or other means to directly call Hudi classes to filter the files on DFS and pick out the latest file slice</p><ul><li>Even though we can force Spark to fallback to using the InputFormat class, we could lose ability to use Spark&#x27;s optimized parquet reader path by doing so. </li><li>To keep benefits of native parquet read performance, we set the  <code>HoodieROTablePathFilter</code> as a path filter, explicitly set this in the Spark Hadoop Configuration.There is logic in the file: to ensure that folders (paths) or files for Hoodie related files always ensures that latest file slice is selected. This filters out duplicate entries and shows latest entries for each record. </li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="i-have-an-existing-dataset-and-want-to-evaluate-hudi-using-portion-of-that-data-"></a>I have an existing dataset and want to evaluate Hudi using portion of that data ?<a class="hash-link" href="#i-have-an-existing-dataset-and-want-to-evaluate-hudi-using-portion-of-that-data-" title="Direct link to heading">#</a></h3><p>You can bulk import portion of that data to a new hudi table. For example, if you want to try on a month of data -</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly java"><pre tabindex="0" class="prism-code language-java codeBlock_23N8 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark.read.parquet(&quot;your_data_set/path/to/month&quot;)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     .write.format(&quot;org.apache.hudi&quot;)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     .option(&quot;hoodie.datasource.write.operation&quot;, &quot;bulk_insert&quot;)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     .option(&quot;hoodie.datasource.write.storage.type&quot;, &quot;storage_type&quot;) // COPY_ON_WRITE or MERGE_ON_READ</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     .option(RECORDKEY_FIELD_OPT_KEY, &quot;&lt;your key&gt;&quot;).</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     .option(PARTITIONPATH_FIELD_OPT_KEY, &quot;&lt;your_partition&gt;&quot;)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     ...</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     .mode(SaveMode.Append)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     .save(basePath);</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><p>Once you have the initial copy, you can simply run upsert operations on this by selecting some sample of data every round</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly java"><pre tabindex="0" class="prism-code language-java codeBlock_23N8 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark.read.parquet(&quot;your_data_set/path/to/month&quot;).limit(n) // Limit n records</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     .write.format(&quot;org.apache.hudi&quot;)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     .option(&quot;hoodie.datasource.write.operation&quot;, &quot;upsert&quot;)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     .option(RECORDKEY_FIELD_OPT_KEY, &quot;&lt;your key&gt;&quot;).</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     .option(PARTITIONPATH_FIELD_OPT_KEY, &quot;&lt;your_partition&gt;&quot;)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     ...</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     .mode(SaveMode.Append)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     .save(basePath);</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><p>For merge on read table, you may want to also try scheduling and running compaction jobs. You can run compaction directly using spark submit on org.apache.hudi.utilities.HoodieCompactor or by using <a href="https://hudi.apache.org/docs/deployment/#cli" target="_blank" rel="noopener noreferrer">HUDI CLI</a>.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="if-i-keep-my-file-versions-at-1-with-this-configuration-will-i-be-able-to-do-a-roll-back-to-the-last-commit-when-write-fail"></a>If I keep my file versions at 1, with this configuration will i be able to do a roll back (to the last commit) when write fail?<a class="hash-link" href="#if-i-keep-my-file-versions-at-1-with-this-configuration-will-i-be-able-to-do-a-roll-back-to-the-last-commit-when-write-fail" title="Direct link to heading">#</a></h3><p>Yes, Commits happen before cleaning. Any failed commits will not cause any side-effects and Hudi will guarantee snapshot isolation.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="does-aws-glue--support-hudi-"></a>Does AWS GLUE  support Hudi ?<a class="hash-link" href="#does-aws-glue--support-hudi-" title="Direct link to heading">#</a></h3><p>AWS Glue jobs can write, read and update Glue Data Catalog for hudi tables. In order to successfully integrate with Glue Data Catalog, you need to subscribe to one of the AWS provided Glue connectors named &quot;AWS Glue Connector for Apache Hudi&quot;. Glue job needs to have &quot;Use Glue data catalog as the Hive metastore&quot; option ticked. Detailed steps with a sample scripts is available on this article provided by AWS - <a href="https://aws.amazon.com/blogs/big-data/writing-to-apache-hudi-tables-using-aws-glue-connector/" target="_blank" rel="noopener noreferrer">https://aws.amazon.com/blogs/big-data/writing-to-apache-hudi-tables-using-aws-glue-connector/</a>.</p><p>In case if your using either notebooks or Zeppelin through Glue dev-endpoints, your script might not be able to integrate with Glue DataCatalog when writing to hudi tables.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-to-override-hudi-jars-in-emr"></a>How to override Hudi jars in EMR?<a class="hash-link" href="#how-to-override-hudi-jars-in-emr" title="Direct link to heading">#</a></h3><p>If you are looking to override Hudi jars in your EMR clusters one way to achieve this is by providing the Hudi jars through a bootstrap script.
Here are the example steps for overriding Hudi version 0.7.0 in EMR 0.6.2. </p><p><strong>Build Hudi Jars:</strong></p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly shell"><pre tabindex="0" class="prism-code language-shell codeBlock_23N8 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># Git clone</span><span class="token plain"></span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">git</span><span class="token plain"> clone https://github.com/apache/hudi.git </span><span class="token operator">&amp;&amp;</span><span class="token plain"> </span><span class="token builtin class-name" style="color:rgb(189, 147, 249)">cd</span><span class="token plain"> hudi   </span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Get version 0.7.0</span><span class="token plain"></span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">git</span><span class="token plain"> checkout --track origin/release-0.7.0</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Build jars with spark 3.0.0 and scala 2.12 (since emr 6.2.0 uses spark 3 which requires scala 2.12):</span><span class="token plain"></span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">mvn clean package -DskipTests -Dspark3  -Dscala-2.12 -T </span><span class="token number">30</span><span class="token plain"> </span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><p><strong>Copy jars to s3:</strong>
These are the jars we are interested in after build completes. Copy them to a temp location first.</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly shell"><pre tabindex="0" class="prism-code language-shell codeBlock_23N8 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#F8F8F2"><span class="token function" style="color:rgb(80, 250, 123)">mkdir</span><span class="token plain"> -p ~/Downloads/hudi-jars</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">cp</span><span class="token plain"> packaging/hudi-hadoop-mr-bundle/target/hudi-hadoop-mr-bundle-0.7.0.jar ~/Downloads/hudi-jars/</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">cp</span><span class="token plain"> packaging/hudi-hive-sync-bundle/target/hudi-hive-sync-bundle-0.7.0.jar ~/Downloads/hudi-jars/</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">cp</span><span class="token plain"> packaging/hudi-spark-bundle/target/hudi-spark-bundle_2.12-0.7.0.jar ~/Downloads/hudi-jars/</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">cp</span><span class="token plain"> packaging/hudi-timeline-server-bundle/target/hudi-timeline-server-bundle-0.7.0.jar ~/Downloads/hudi-jars/</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">cp</span><span class="token plain"> packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.12-0.7.0.jar ~/Downloads/hudi-jars/</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><p>Upload  all jars from ~/Downloads/hudi-jars/ to the s3 location s3://xxx/yyy/hudi-jars</p><p><strong>Include Hudi jars as part of the emr bootstrap script:</strong>
Below script downloads Hudi jars from above s3 location. Use this script as part <code>bootstrap-actions</code> when launching the EMR cluster to install the jars in each node.</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly shell"><pre tabindex="0" class="prism-code language-shell codeBlock_23N8 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#F8F8F2"><span class="token shebang important">#!/bin/bash</span><span class="token plain"></span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">sudo</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">mkdir</span><span class="token plain"> -p /mnt1/hudi-jars</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">sudo</span><span class="token plain"> aws s3 </span><span class="token function" style="color:rgb(80, 250, 123)">cp</span><span class="token plain"> s3://xxx/yyy/hudi-jars /mnt1/hudi-jars --recursive</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># create symlinks</span><span class="token plain"></span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token builtin class-name" style="color:rgb(189, 147, 249)">cd</span><span class="token plain"> /mnt1/hudi-jars</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">sudo</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">ln</span><span class="token plain"> -sf hudi-hadoop-mr-bundle-0.7.0.jar hudi-hadoop-mr-bundle.jar</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">sudo</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">ln</span><span class="token plain"> -sf hudi-hive-sync-bundle-0.7.0.jar hudi-hive-sync-bundle.jar</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">sudo</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">ln</span><span class="token plain"> -sf hudi-spark-bundle_2.12-0.7.0.jar hudi-spark-bundle.jar</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">sudo</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">ln</span><span class="token plain"> -sf hudi-timeline-server-bundle-0.7.0.jar hudi-timeline-server-bundle.jar</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">sudo</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">ln</span><span class="token plain"> -sf hudi-utilities-bundle_2.12-0.7.0.jar hudi-utilities-bundle.jar</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><p><strong>Using the overriden jar in Deltastreamer:</strong>
When invoking DeltaStreamer specify the above jar location as part of spark-submit command.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="why-partition-fields-are-also-stored-in-parquet-files-in-addition-to-the-partition-path-"></a>Why partition fields are also stored in parquet files in addition to the partition path ?<a class="hash-link" href="#why-partition-fields-are-also-stored-in-parquet-files-in-addition-to-the-partition-path-" title="Direct link to heading">#</a></h3><p>Hudi supports customizable partition values which could be a derived value of another field. Also, storing the partition value only as part of the field results in losing type information when queried by various query engines.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="i-am-seeing-lot-of-archive-files-how-do-i-control-the-number-of-archive-commit-files-generated"></a>I am seeing lot of archive files. How do I control the number of archive commit files generated?<a class="hash-link" href="#i-am-seeing-lot-of-archive-files-how-do-i-control-the-number-of-archive-commit-files-generated" title="Direct link to heading">#</a></h3><p>Please note that in cloud stores that do not support log append operations, Hudi is forced to create new archive files to archive old metadata operations.  You can increase hoodie.commits.archival.batch moving forward to increase the number of commits archived per archive file. In addition, you can increase the difference between the 2 watermark configurations : hoodie.keep.max.commits (default : 30) and hoodie.keep.min.commits (default : 20). This way, you can reduce the number of archive files created and also at the same time increase the number of metadata archived per archive file. Note that post 0.7.0 release where we are adding consolidated Hudi metadata (<a href="https://cwiki.apache.org/confluence/display/HUDI/RFC+-+15%3A+HUDI+File+Listing+Improvements" target="_blank" rel="noopener noreferrer">RFC-15</a>), the follow up work would involve re-organizing archival metadata so that we can do periodic compactions to control file-sizing of these archive files.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-do-i-configure-bloom-filter-when-bloomglobal_bloom-index-is-used"></a>How do I configure Bloom filter (when Bloom/Global_Bloom index is used)?<a class="hash-link" href="#how-do-i-configure-bloom-filter-when-bloomglobal_bloom-index-is-used" title="Direct link to heading">#</a></h3><p>Bloom filters are used in bloom indexes to look up the location of record keys in write path. Bloom filters are used only when the index type is chosen as “BLOOM” or “GLOBAL_BLOOM”. Hudi has few config knobs that users can use to tune their bloom filters.</p><p>On a high level, hudi has two types of blooms: Simple and Dynamic.</p><p>Simple, as the name suggests, is simple. Size is statically allocated based on few configs.</p><p><code>hoodie.bloom.index.filter.type</code>: SIMPLE</p><p><code>hoodie.index.bloom.num_entries</code> refers to the total number of entries per bloom filter, which refers to one file slice. Default value is 60000.</p><p><code>hoodie.index.bloom.fpp</code> refers to the false positive probability with the bloom filter. Default value: 1*10^-9.</p><p>Size of the bloom filter depends on these two values. This is statically allocated and here is the formula that determines the size of bloom. Until the total number of entries added to the bloom is within the configured <code>hoodie.index.bloom.num_entries</code> value, the fpp will be honored. i.e. with default values of 60k and 1*10^-9, bloom filter serialized size = 430kb. But if more entries are added, then the false positive probability will not be honored. Chances that more false positives could be returned if you add more number of entries than the configured value. So, users are expected to set the right values for both num_entries and fpp.</p><p>Hudi suggests to have roughly 100 to 120 mb sized files for better query performance. So, based on the record size, one could determine how many records could fit into one data file.</p><p>Lets say your data file max size is 128Mb and default avg record size is 1024 bytes. Hence, roughly this translates to 130k entries per data file. For this config, you should set num_entries to ~130k.</p><p>Dynamic bloom filter:</p><p><code>hoodie.bloom.index.filter.type</code> : DYNAMIC</p><p>This is an advanced version of the bloom filter which grows dynamically as the number of entries grows. So, users are expected to set two values wrt num_entries. <code>hoodie.index.bloom.num_entries</code> will determine the starting size of the bloom. <code>hoodie.bloom.index.filter.dynamic.max.entries</code> will determine the max size to which the bloom can grow upto. And fpp needs to be set similar to “Simple” bloom filter. Bloom size will be allotted based on the first config <code>hoodie.index.bloom.num_entries</code>. Once the number of entries reaches this value, bloom will dynamically grow its size to 2X. This will go on until the size reaches a max of <code>hoodie.bloom.index.filter.dynamic.max.entries</code> value. Until the size reaches this max value, fpp will be honored. If the entries added exceeds the max value, then the fpp may not be honored. </p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="how-to-tune-shuffle-parallelism-of-hudi-jobs-"></a>How to tune shuffle parallelism of Hudi jobs ?<a class="hash-link" href="#how-to-tune-shuffle-parallelism-of-hudi-jobs-" title="Direct link to heading">#</a></h3><p>First, let&#x27;s understand what the term parallelism means in the context of Hudi jobs. For any Hudi job using Spark, parallelism equals to the number of spark partitions that should be generated for a particular stage in the DAG. To understand more about spark partitions, read this <a href="https://www.dezyre.com/article/how-data-partitioning-in-spark-helps-achieve-more-parallelism/297" target="_blank" rel="noopener noreferrer">article</a>. In spark, each spark partition is mapped to a spark task that can be executed on an executor. Typically, for a spark application the following hierarchy holds true</p><p>(<em>Spark Application → N Spark Jobs → M Spark Stages → T Spark Tasks</em>) on (<em>E executors with C cores</em>)</p><p>A spark application can be given E number of executors to run the spark application on. Each executor might hold 1 or more spark cores. Every spark task will require atleast 1 core to execute, so imagine T number of tasks to be done in Z time depending on C cores. The higher C, Z is smaller.</p><p>With this understanding, if you want your DAG stage to run faster, <em>bring T as close or higher to C</em>. Additionally, this parallelism finally controls the number of output files you write using a Hudi based job. Let&#x27;s understand the different kinds of knobs available:</p><p><a href="https://hudi.apache.org/docs/configurations#hoodiebulkinsertshuffleparallelism" target="_blank" rel="noopener noreferrer">BulkInsertParallelism</a> → This is used to control the parallelism with which output files will be created by a Hudi job. The higher this parallelism, the more number of tasks are created and hence the more number of output files will eventually be created. Even if you define <a href="https://hudi.apache.org/docs/configurations#hoodieparquetmaxfilesize" target="_blank" rel="noopener noreferrer">parquet-max-file-size</a> to be of a high value, if you make parallelism really high, the max file size cannot be honored since the spark tasks are working on smaller amounts of data.</p><p><a href="https://hudi.apache.org/docs/configurations#hoodieupsertshuffleparallelism" target="_blank" rel="noopener noreferrer">Upsert</a> / <a href="https://hudi.apache.org/docs/configurations#hoodieinsertshuffleparallelism" target="_blank" rel="noopener noreferrer">Insert Parallelism</a> → This is used to control how fast the read process should be when reading data into the job. Find more details <a href="https://hudi.apache.org/docs/configurations/" target="_blank" rel="noopener noreferrer">here</a>.  </p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="int96-int64-and-timestamp-compatibility"></a>INT96, INT64 and timestamp compatibility<a class="hash-link" href="#int96-int64-and-timestamp-compatibility" title="Direct link to heading">#</a></h3><p><a href="https://hudi.apache.org/docs/configurations#hoodiedatasourcehive_syncsupport_timestamp" target="_blank" rel="noopener noreferrer">https://hudi.apache.org/docs/configurations#hoodiedatasourcehive_syncsupport_timestamp</a></p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="contributing-to-faq"></a>Contributing to FAQ<a class="hash-link" href="#contributing-to-faq" title="Direct link to heading">#</a></h2><p>A good and usable FAQ should be community-driven and crowd source questions/thoughts across everyone.</p><p>You can improve the FAQ by the following processes</p><ul><li>Raise a PR to spot inaccuracies, typos on this page and leave suggestions.</li><li>Raise a PR to propose new questions with answers.</li><li>Lean towards making it very understandable and simple, and heavily link to parts of documentation as needed</li><li>One committer on the project will review new questions and incorporate them upon review.</li></ul></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"></div><div class="pagination-nav__item pagination-nav__item--next"></div></nav></div></div><div class="col col--3"><div class="tableOfContents_35-E thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#general" class="table-of-contents__link">General</a><ul><li><a href="#when-is-hudi-useful-for-me-or-my-organization" class="table-of-contents__link">When is Hudi useful for me or my organization?</a></li><li><a href="#what-are-some-non-goals-for-hudi" class="table-of-contents__link">What are some non-goals for Hudi?</a></li><li><a href="#what-is-incremental-processing-why-does-hudi-docstalks-keep-talking-about-it" class="table-of-contents__link">What is incremental processing? Why does Hudi docs/talks keep talking about it?</a></li><li><a href="#what-is-the-difference-between-copy-on-write-cow-vs-merge-on-read-mor-storage-types" class="table-of-contents__link">What is the difference between copy-on-write (COW) vs merge-on-read (MOR) storage types?</a></li><li><a href="#how-do-i-choose-a-storage-type-for-my-workload" class="table-of-contents__link">How do I choose a storage type for my workload?</a></li><li><a href="#is-hudi-an-analytical-database" class="table-of-contents__link">Is Hudi an analytical database?</a></li><li><a href="#how-do-i-model-the-data-stored-in-hudi" class="table-of-contents__link">How do I model the data stored in Hudi?</a></li><li><a href="#does-hudi-support-cloud-storageobject-stores" class="table-of-contents__link">Does Hudi support cloud storage/object stores?</a></li><li><a href="#what-versions-of-hivesparkhadoop-are-support-by-hudi" class="table-of-contents__link">What versions of Hive/Spark/Hadoop are support by Hudi?</a></li><li><a href="#how-does-hudi-actually-store-data-inside-a-dataset" class="table-of-contents__link">How does Hudi actually store data inside a dataset?</a></li></ul></li><li><a href="#using-hudi" class="table-of-contents__link">Using Hudi</a><ul><li><a href="#what-are-some-ways-to-write-a-hudi-dataset" class="table-of-contents__link">What are some ways to write a Hudi dataset?</a></li><li><a href="#how-is-a-hudi-job-deployed" class="table-of-contents__link">How is a Hudi job deployed?</a></li><li><a href="#how-can-i-now-query-the-hudi-dataset-i-just-wrote" class="table-of-contents__link">How can I now query the Hudi dataset I just wrote?</a></li><li><a href="#how-does-hudi-handle-duplicate-record-keys-in-an-input" class="table-of-contents__link">How does Hudi handle duplicate record keys in an input?</a></li><li><a href="#can-i-implement-my-own-logic-for-how-input-records-are-merged-with-record-on-storage" class="table-of-contents__link">Can I implement my own logic for how input records are merged with record on storage?</a></li><li><a href="#how-do-i-delete-records-in-the-dataset-using-hudi" class="table-of-contents__link">How do I delete records in the dataset using Hudi?</a></li><li><a href="#does-deleted-records-appear-in-hudis-incremental-query-results" class="table-of-contents__link">Does deleted records appear in Hudi&#39;s incremental query results?</a></li><li><a href="#how-do-i-migrate-my-data-to-hudi" class="table-of-contents__link">How do I migrate my data to Hudi?</a></li><li><a href="#how-can-i-pass-hudi-configurations-to-my-spark-job" class="table-of-contents__link">How can I pass hudi configurations to my spark job?</a></li><li><a href="#how-to-create-hive-style-partition-folder-structure" class="table-of-contents__link">How to create Hive style partition folder structure?</a></li><li><a href="#how-do-i-pass-hudi-configurations-to-my-beeline-hive-queries" class="table-of-contents__link">How do I pass hudi configurations to my beeline Hive queries?</a></li><li><a href="#can-i-register-my-hudi-dataset-with-apache-hive-metastore" class="table-of-contents__link">Can I register my Hudi dataset with Apache Hive metastore?</a></li><li><a href="#how-does-the-hudi-indexing-work--what-are-its-benefits" class="table-of-contents__link">How does the Hudi indexing work &amp; what are its benefits?</a></li><li><a href="#what-does-the-hudi-cleaner-do" class="table-of-contents__link">What does the Hudi cleaner do?</a></li><li><a href="#whats-hudis-schema-evolution-story" class="table-of-contents__link">What&#39;s Hudi&#39;s schema evolution story?</a></li><li><a href="#how-do-i-run-compaction-for-a-mor-dataset" class="table-of-contents__link">How do I run compaction for a MOR dataset?</a></li><li><a href="#what-performanceingest-latency-can-i-expect-for-hudi-writing" class="table-of-contents__link">What performance/ingest latency can I expect for Hudi writing?</a></li><li><a href="#what-performance-can-i-expect-for-hudi-readingqueries" class="table-of-contents__link">What performance can I expect for Hudi reading/queries?</a></li><li><a href="#how-do-i-to-avoid-creating-tons-of-small-files" class="table-of-contents__link">How do I to avoid creating tons of small files?</a></li><li><a href="#why-does-hudi-retain-at-least-one-previous-commit-even-after-setting-hoodiecleanercommitsretained-1-" class="table-of-contents__link">Why does Hudi retain at-least one previous commit even after setting hoodie.cleaner.commits.retained&#39;: 1 ?</a></li><li><a href="#how-do-i-use-deltastreamer-or-spark-datasource-api-to-write-to-a-non-partitioned-hudi-dataset-" class="table-of-contents__link">How do I use DeltaStreamer or Spark DataSource API to write to a Non-partitioned Hudi dataset ?</a></li><li><a href="#why-do-we-have-to-set-2-different-ways-of-configuring-spark-to-work-with-hudi" class="table-of-contents__link">Why do we have to set 2 different ways of configuring Spark to work with Hudi?</a></li><li><a href="#i-have-an-existing-dataset-and-want-to-evaluate-hudi-using-portion-of-that-data-" class="table-of-contents__link">I have an existing dataset and want to evaluate Hudi using portion of that data ?</a></li><li><a href="#if-i-keep-my-file-versions-at-1-with-this-configuration-will-i-be-able-to-do-a-roll-back-to-the-last-commit-when-write-fail" class="table-of-contents__link">If I keep my file versions at 1, with this configuration will i be able to do a roll back (to the last commit) when write fail?</a></li><li><a href="#does-aws-glue--support-hudi-" class="table-of-contents__link">Does AWS GLUE  support Hudi ?</a></li><li><a href="#how-to-override-hudi-jars-in-emr" class="table-of-contents__link">How to override Hudi jars in EMR?</a></li><li><a href="#why-partition-fields-are-also-stored-in-parquet-files-in-addition-to-the-partition-path-" class="table-of-contents__link">Why partition fields are also stored in parquet files in addition to the partition path ?</a></li><li><a href="#i-am-seeing-lot-of-archive-files-how-do-i-control-the-number-of-archive-commit-files-generated" class="table-of-contents__link">I am seeing lot of archive files. How do I control the number of archive commit files generated?</a></li><li><a href="#how-do-i-configure-bloom-filter-when-bloomglobal_bloom-index-is-used" class="table-of-contents__link">How do I configure Bloom filter (when Bloom/Global_Bloom index is used)?</a></li><li><a href="#how-to-tune-shuffle-parallelism-of-hudi-jobs-" class="table-of-contents__link">How to tune shuffle parallelism of Hudi jobs ?</a></li><li><a href="#int96-int64-and-timestamp-compatibility" class="table-of-contents__link">INT96, INT64 and timestamp compatibility</a></li></ul></li><li><a href="#contributing-to-faq" class="table-of-contents__link">Contributing to FAQ</a></li></ul></div></div></div></div></main></div></div><footer class="footer"><div class="container"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">About</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/cn/blog/2021/07/21/streaming-data-lake-platform">Our Vision</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/concepts">Concepts</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/contribute/team">Team</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/releases/release-0.9.0">Releases</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/releases/download">Download</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/powered-by">Who&#x27;s Using</a></li></ul></div><div class="col footer__col"><div class="footer__title">Learn</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/cn/docs/quick-start-guide">Quick Start</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/docker_demo">Docker Demo</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/blog">Blog</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/talks-articles">Talks &amp; Articles</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/learn/faq">FAQ</a></li><li class="footer__item"><a href="https://cwiki.apache.org/confluence/display/HUDI" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Technical Wiki<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">Hudi On Cloud</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/cn/docs/s3_hoodie">AWS</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/gcs_hoodie">Google Cloud</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/oss_hoodie">Alibaba Cloud</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/azure_hoodie">Microsoft Azure</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/cos_hoodie">Tencent Cloud</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/ibm_cos_hoodie">IBM Cloud</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/cn/contribute/get-involved">Get Involved</a></li><li class="footer__item"><a href="https://join.slack.com/t/apache-hudi/shared_invite/enQtODYyNDAxNzc5MTg2LTE5OTBlYmVhYjM0N2ZhOTJjOWM4YzBmMWU2MjZjMGE4NDc5ZDFiOGQ2N2VkYTVkNzU3ZDQ4OTI1NmFmYWQ0NzE" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Slack<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://github.com/apache/hudi" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://twitter.com/ApacheHudi" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="mailto:dev-subscribe@hudi.apache.org?Subject=SubscribeToHudi" target="_blank" rel="noopener noreferrer" class="footer__link-item">Mailing List</a></li></ul></div><div class="col footer__col"><div class="footer__title">Apache</div><ul class="footer__items"><li class="footer__item"><a href="https://www.apache.org/events/current-event" target="_blank" rel="noopener noreferrer" class="footer__link-item">Events</a></li><li class="footer__item"><a href="https://www.apache.org/foundation/thanks.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Thanks</a></li><li class="footer__item"><a href="https://www.apache.org/licenses" target="_blank" rel="noopener noreferrer" class="footer__link-item">License</a></li><li class="footer__item"><a href="https://www.apache.org/security" target="_blank" rel="noopener noreferrer" class="footer__link-item">Security</a></li><li class="footer__item"><a href="https://www.apache.org/foundation/sponsorship.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Sponsorship</a></li><li class="footer__item"><a href="https://www.apache.org" target="_blank" rel="noopener noreferrer" class="footer__link-item">Foundation</a></li></ul></div></div><div class="footer__bottom text--center"><div class="margin-bottom--sm"><a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer" class="footerLogoLink_MyFc"><img src="/cn/assets/images/logo-big.png" alt="Apache Hudi™" class="themedImage_1VuW themedImage--light_3UqQ footer__logo"><img src="/cn/assets/images/logo-big.png" alt="Apache Hudi™" class="themedImage_1VuW themedImage--dark_hz6m footer__logo"></a></div><div class="footer__copyright">Copyright © 2021 <a href="https://apache.org">The Apache Software Foundation</a>, Licensed under the <a href="https://www.apache.org/licenses/LICENSE-2.0"> Apache License, Version 2.0</a>.
      Hudi, Apache and the Apache feather logo are trademarks of The Apache Software Foundation. <a href="/docs/privacy">Privacy Policy</a></div></div></div></footer></div>
<script src="/cn/assets/js/runtime~main.b9689b19.js"></script>
<script src="/cn/assets/js/main.d0765f1c.js"></script>
</body>
</html>