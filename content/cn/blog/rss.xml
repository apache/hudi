<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Apache Hudi: User-Facing Analytics</title>
        <link>https://hudi.apache.org/cn/blog</link>
        <description>Apache Hudi Blog</description>
        <lastBuildDate>Mon, 31 Mar 2025 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>cn</language>
        <item>
            <title><![CDATA[Powering Amazon Unit Economics at Scale Using Apache Hudi]]></title>
            <link>https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi</link>
            <guid>https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi</guid>
            <pubDate>Mon, 31 Mar 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Amazon‚Äôs Profit Intelligence team built Nexus, a configuration-driven platform powered by Apache Hudi, to scale unit economics across thousands of retail use cases. Nexus manages over 1,200 tables, processes hundreds of billions of rows daily, and handles ~1 petabyte of data churn each month. This blog dives into their data lakehouse journey, Nexus architecture, Hudi integration, and key operational learnings.]]></description>
            <content:encoded><![CDATA[<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>TL;DR</div><div class="admonitionContent_BuS1"><p>Amazon‚Äôs Profit Intelligence team built Nexus, a configuration-driven platform powered by Apache Hudi, to scale unit economics across thousands of retail use cases. Nexus manages over 1,200 tables, processes hundreds of billions of rows daily, and handles ~1 petabyte of data churn each month. This blog dives into their data lakehouse journey, Nexus architecture, Hudi integration, and key operational learnings.</p></div></div>
<p>Understanding and improving unit-level profitability at Amazon's scale is a massive challenge - one that requires flexibility, precision, and operational efficiency. In this blog, we walk through how Amazon‚Äôs Profit Intelligence team built a scalable, configuration-driven platform called Nexus, and how Apache Hudi became the cornerstone of its data lake architecture.</p>
<p>By combining declarative configuration with Hudi's advanced table management capabilities, the team has enabled thousands of retail business use cases to run seamlessly, allowing finance and pricing teams to self-serve insights on cost and profitability, without constantly relying on engineering intervention.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-business-need-profit-intelligence-and-unit-economics">The Business Need: Profit Intelligence and Unit Economics<a href="https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi#the-business-need-profit-intelligence-and-unit-economics" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>Within Amazon‚Äôs Worldwide Stores, the Selling Partner Services (SPS) team supports seller-facing operations. A key part of this effort is computing <strong>Contribution Profit</strong> - a granular metric that captures revenue, costs, and profitability at the unit level, such as <em>a shipped item to the customer</em>.</p>
<p>Contribution Profit powers decision-making for a range of downstream teams including:</p>
<ul>
<li>Pricing</li>
<li>Forecasting</li>
<li>Finance</li>
</ul>
<p>The challenge? Supporting the scale and diversity of retail use cases across Amazon's global business, while maintaining a data platform that's both extensible and maintainable.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="amazons-data-lakehouse-journey">Amazon‚Äôs Data Lakehouse Journey<a href="https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi#amazons-data-lakehouse-journey" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>Over the past decade, the architecture behind Contribution Profit has gone through several phases of evolution, driven by the need to better support Amazon‚Äôs growing and diverse retail business use cases.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="early-phase">Early Phase<a href="https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi#early-phase" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<img src="https://hudi.apache.org/assets/images/blog/fig1_amz.png" alt="redshift" width="800" align="middle">
<p>Initial implementations relied on ETL pipelines that published data to Redshift, often with unstructured job flows. Business logic could exist at various layers of the ETL and was written entirely in SQL, making it difficult to track, maintain, or modify. These pipelines lacked systematic enforcement of patterns, which led to fragmentation and technical debt.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="intermediate-phase">Intermediate Phase<a href="https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi#intermediate-phase" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<img src="https://hudi.apache.org/assets/images/blog/fig2_amz.png" alt="flink" width="800" align="middle">
<p>To improve scalability and support streaming workloads, the team transitioned to a setup involving Apache Flink and a custom-built data lake. Although this introduced broader data processing flexibility, it still had major drawbacks:</p>
<ul>
<li>Redshift-based ETLs remained in use.</li>
<li>Business logic and schema changes required engineering involvement.</li>
<li>There were ongoing scalability and maintainability issues with the custom data lake.</li>
<li>Flink introduced operational challenges of its own, such as handling version upgrades through AWS Managed Flink and providing done signal in batch operation.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="current-state-nexus--apache-hudi">Current State: Nexus + Apache Hudi<a href="https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi#current-state-nexus--apache-hudi" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>Each of the prior approaches came with tradeoffs, especially around business logic being tightly coupled with code, making it hard for non-engineers to simulate or change metrics for a specific retail business.</p>
<p>Recognizing the need for better abstraction and operational maturity, the team built Nexus - a configuration-driven platform for defining and orchestrating data pipelines. All lake interactions including ingestion, transformation, schema evolution, and table management now go through Nexus. Nexus is powered by <a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer"><strong>Apache Hudi</strong></a>, which provides the foundation for scalable ingestion, efficient upserts, schema evolution, and transactional guarantees on Amazon S3.</p>
<p>This new architecture enabled the team to decouple business logic from engineering code, allowing business teams to define logic declaratively. It also introduced standardization across workloads, eliminated redundant pipelines, and laid the groundwork for scaling unit economics calculations across thousands of use cases.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="key-modules-of-nexus">Key Modules of Nexus<a href="https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi#key-modules-of-nexus" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h4>
<img src="https://hudi.apache.org/assets/images/blog/fig3_amz.png" alt="nexus" width="800" align="middle">
<p>Nexus consists of four core components:</p>
<p><strong>Configuration Layer</strong></p>
<p>The topmost layer where users define their business logic in a declarative format. These configurations are typically generated and enriched with metadata by internal systems.</p>
<p><strong>NexusFlow (Orchestration)</strong></p>
<img src="https://hudi.apache.org/assets/images/blog/fig4_amz.png" alt="orch" width="1000" align="middle">
<p align="center"><em>Figure: Sample NexusFlow Config</em></p>
<p>Responsible for generating and executing workflows. It operates on two levels:</p>
<ul>
<li>Logical Layer: Comprising NexusETL jobs and other tasks.</li>
<li>Physical Layer: Implemented via AWS Step Functions to orchestrate EMR jobs and related dependencies. NexusFlow supports extensibility through a federated model and can execute diverse task types like Spark jobs, Redshift queries, wait conditions, and legacy ETLs.</li>
</ul>
<p><strong>NexusETL (Execution)</strong></p>
<img src="https://hudi.apache.org/assets/images/blog/fig5_amz.png" alt="etl" width="1000" align="middle">
<p align="center"><em>Figure: Sample NexusETL Config</em></p>
<p>Executes Spark-based data transformation jobs. Jobs are defined entirely in configuration, with support for:</p>
<ul>
<li>Built-in transforms like joins and filters</li>
<li>Custom UDFs</li>
<li>Source/Sink/Transform operators: It operates at the job abstraction level and is typically invoked by NexusFlow during orchestration.</li>
</ul>
<p><strong>NexusDataLake (Storage)</strong></p>
<img src="https://hudi.apache.org/assets/images/blog/fig5_amz.png" alt="datalake" width="1000" align="middle">
<p align="center"><em>Figure: Sample NexusDataLake Config</em></p>
<p>A storage abstraction layer built on Apache Hudi. NexusDataLake manages:</p>
<ul>
<li>Table creation</li>
<li>Schema inference and evolution</li>
<li>Catalog integration: All interactions with Hudi, such as inserts, upserts, table schema changes, and metadata syncs are funneled through NexusETL and NexusFlow, maintaining consistency across the platform.</li>
</ul>
<p>By standardizing how data is defined, processed, and stored, Nexus has enabled a scalable, maintainable, and extensible architecture. Every data lake interaction - from ingestion to table maintenance, is performed through this configuration-first model, which now powers hundreds of use cases across Amazon retail.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-apache-hudi">Why Apache Hudi?<a href="https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi#why-apache-hudi" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>Apache Hudi has been central to Nexus‚Äô success, providing the core data lake storage layer for scalable ingestion, updates, and metadata management. It enables fast, incremental updates at massive scale while maintaining transactional guarantees on top of Amazon S3.</p>
<p>In Amazon‚Äôs current architecture:</p>
<ul>
<li>Copy-on-Write (COW) table type is used for all Hudi tables.</li>
<li>Workloads generate hundreds of billions of row updates daily, with write patterns spanning concentrated single-partition updates and wide-range backfills across up to 90 partitions.</li>
<li>All Hudi interactions, including inserts, schema changes, and metadata syncs, are managed through Nexus.</li>
</ul>
<p><strong>Key Capabilities used with Apache Hudi</strong></p>
<ul>
<li>
<p><strong>Efficient Upserts</strong><br>
<!-- -->Hudi‚Äôs design primitives such as <a href="https://hudi.apache.org/docs/indexes" target="_blank" rel="noopener noreferrer">indexes</a> for Copy-on-Write (CoW) tables enable high-throughput update patterns by avoiding the need to join against the entire dataset to determine which files to rewrite, which is particularly critical for our daily workloads.</p>
</li>
<li>
<p><strong>Incremental Processing</strong><br>
<!-- -->By using Hudi‚Äôs native <a href="https://www.onehouse.ai/blog/getting-started-incrementally-process-data-with-apache-hudi" target="_blank" rel="noopener noreferrer">incremental pull</a> capabilities, downstream systems are able to consume only the changes between commits. This is essential for efficiently updating Contribution Profit metrics that power business decision-making.</p>
</li>
<li>
<p><strong>Metadata Table</strong><br>
<!-- -->Enabling the <a href="https://hudi.apache.org/docs/metadata" target="_blank" rel="noopener noreferrer">metadata table</a> (<code>hoodie.metadata.enable=true</code>) significantly reduced job runtimes by avoiding expensive file listings on S3. This is an important optimization given the scale at which we process updates across more than 1200 Hudi tables.</p>
</li>
<li>
<p><strong>Schema Evolution</strong><br>
<!-- -->Table creation and evolution are fully managed through configuration in Nexus. Hudi‚Äôs built-in support for <a href="https://hudi.apache.org/docs/schema_evolution" target="_blank" rel="noopener noreferrer">schema evolution</a> has allowed the team to onboard new use cases and adapt to changing schemas without requiring expensive rewrites or manual interventions.</p>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-learnings-from-operating-hudi-at-amazon-scale">Key Learnings from Operating Hudi at Amazon Scale<a href="https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi#key-learnings-from-operating-hudi-at-amazon-scale" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>Operating Apache Hudi at the scale and velocity required by Amazon‚Äôs Profit Intelligence workloads surfaced a set of hard-earned lessons, especially around concurrency, metadata handling, and cost optimization. These learnings reflect both architectural refinements and operational trade-offs that others adopting Hudi at large scale may find useful.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-concurrency-control">1. Concurrency Control<a href="https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi#1-concurrency-control" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>At Amazon‚Äôs ingestion scale - hundreds of billions of rows per day and thousands of concurrent table updates, multi-writer concurrency is a reality, not an edge case.</p>
<p>The team initially used Optimistic Concurrency Control (OCC), which works well in environments with low write conflicts. OCC assumes that concurrent writers rarely overlap, and when they do, the job retries after detecting a conflict. However, in high-contention scenarios, like multiple jobs writing to the same partition within a short time window, this led to frequent retries and job failures.</p>
<p>To resolve this, the team pivoted to a new table structure designed to minimize concurrent insertions. This change helped reduce contention by lowering the likelihood of multiple writers operating on overlapping partitions simultaneously. The updated design enabled using OCC while avoiding the excessive retries and failures we had initially encountered.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-metadata-table-management-async-vs-sync-trade-offs">2. Metadata Table Management: Async vs Sync Trade-Offs<a href="https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi#2-metadata-table-management-async-vs-sync-trade-offs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>Apache Hudi‚Äôs metadata table dramatically improves performance by avoiding expensive file listings on cloud object stores like S3. It maintains a persistent <em>index</em> <em>of files</em>, enabling faster operations such as file pruning, and data skipping.</p>
<p>The team enabled Hudi‚Äôs metadata table early (<code>hoodie.metadata.enable=true</code>) and started off with synchronous cleaning but switched to asynchronous cleaning to reduce job runtime. However, we ran into an issue when experimenting with asynchronous cleaning. Due to a <a href="https://github.com/apache/hudi/issues/11535" target="_blank" rel="noopener noreferrer">known issue (#11535)</a>, async cleaning wasn‚Äôt properly cleaning up metadata entries.</p>
<p>To ensure the metadata tables were properly cleaned, we switched all of  our Hudi workloads back to synchronous cleaning.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-cost-management">3. Cost Management<a href="https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi#3-cost-management" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>While Apache Hudi helped Amazon reduce data duplication and improve ingestion efficiency, we quickly realized that operational costs were not driven by storage - but by the API interaction patterns with S3.</p>
<p>Breakdown of the cost profile:</p>
<ul>
<li><strong>70% of total cost</strong> came from <code>PUT</code> requests (writes)</li>
<li>Combined <code>PUT + GET</code> operations accounted for <strong>80%</strong> of the bill</li>
<li>Storage cost remained a small fraction, even with 3+ PB of total data under management</li>
</ul>
<p>Their data ingestion patterns contributed to this:</p>
<ul>
<li>Daily workloads: Heavy concentration (99%) of updates into a single partition</li>
<li>Backfill workloads: Spread evenly across 30‚Äì90 partitions</li>
</ul>
<p>To manage this:</p>
<ul>
<li>We moved to <strong>S3 Intelligent-Tiering</strong> to reduce unused data storage costs</li>
<li>Enabled <strong>EMR cluster auto-scaling</strong> to dynamically adjust compute resources</li>
<li>Batched writes and carefully tuned Hudi configurations (e.g., <code>write.batch.size</code>, <code>compaction.small.file.limit</code>) to reduce unnecessary file churn</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="operational-scale-nexus-by-the-numbers">Operational Scale: Nexus by the Numbers<a href="https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi#operational-scale-nexus-by-the-numbers" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<table><thead><tr><th style="text-align:left">Metric</th><th style="text-align:left">Value</th></tr></thead><tbody><tr><td style="text-align:left">Tables Managed</td><td style="text-align:left">1200+ (5‚Äì15 updates/day per table)</td></tr><tr><td style="text-align:left">Legacy SQL Deprecated</td><td style="text-align:left">300,000+ lines</td></tr><tr><td style="text-align:left">Total Data Managed</td><td style="text-align:left">~3 Petabytes</td></tr><tr><td style="text-align:left">Monthly Data Changes</td><td style="text-align:left">~1 Petabyte added/deleted</td></tr><tr><td style="text-align:left">Daily Record Updates</td><td style="text-align:left">Hundreds of billions</td></tr><tr><td style="text-align:left">Developer Time Saved</td><td style="text-align:left">300+ days</td></tr></tbody></table>
<p>Nexus with Apache Hudi as the foundation has significantly improved the scale, modularity, and maintainability of the data lake operations at Amazon. As the business use cases scale, the team is also focused on managing the increasing complexity of the data lake, while ensuring that both technical and non-technical stakeholders can interact with Nexus effectively.</p>
<p>This blog is based on Amazon‚Äôs presentation at the Hudi Community Sync. If you are interested in watching the recorded version of the video, you can find it <a href="https://www.youtube.com/watch?v=rMXhlb7Uci8" target="_blank" rel="noopener noreferrer">here</a>.</p>
<hr>]]></content:encoded>
            <category>Apache Hudi</category>
            <category>Amazon</category>
            <category>Community</category>
        </item>
        <item>
            <title><![CDATA[Building an Amazon Sales Analytics Pipeline with Apache Hudi on Databricks]]></title>
            <link>https://hudi.apache.org/cn/blog/2025/03/13/hudi-on-dbr</link>
            <guid>https://hudi.apache.org/cn/blog/2025/03/13/hudi-on-dbr</guid>
            <pubDate>Thu, 13 Mar 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://www.linkedin.com/pulse/building-amazon-sales-analytics-pipeline-apache-hudi-databricks-ruotf/">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>apache hudi</category>
            <category>aws</category>
            <category>databricks</category>
        </item>
        <item>
            <title><![CDATA[From Transactional Bottlenecks to Lightning-Fast Analytics]]></title>
            <link>https://hudi.apache.org/cn/blog/2025/03/13/lightning-fast-analytics</link>
            <guid>https://hudi.apache.org/cn/blog/2025/03/13/lightning-fast-analytics</guid>
            <pubDate>Thu, 13 Mar 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://aakashsankritya.medium.com/from-transactional-bottlenecks-to-lightning-fast-analytics-74e0d3fff1c0">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>apache hudi</category>
            <category>kafka</category>
            <category>debezium</category>
            <category>S3</category>
        </item>
        <item>
            <title><![CDATA[21 Unique Reasons Why Apache Hudi Should Be Your Next Data Lakehouse]]></title>
            <link>https://hudi.apache.org/cn/blog/2025/03/05/hudi-21-unique-differentiators</link>
            <guid>https://hudi.apache.org/cn/blog/2025/03/05/hudi-21-unique-differentiators</guid>
            <pubDate>Wed, 05 Mar 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Apache Hudi is continuously redefining the data lakehouse, pushing the technical boundaries and offering cutting-edge features to handle data quickly and efficiently. If you have ever wondered how Apache Hudi has sustained its position over the years as the most comprehensive, open, high-performance data lakehouse project, this blog aims to give you some concise answers. Below, we shine a light on some unique capabilities in Hudi, that go beyond the lowest-common-denominator across the different projects in the space.]]></description>
            <content:encoded><![CDATA[<p>Apache Hudi is continuously <a href="https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0" target="_blank" rel="noopener noreferrer">redefining</a> the data lakehouse, pushing the technical boundaries and offering cutting-edge features to handle data quickly and efficiently. If you have ever wondered how Apache Hudi has sustained its position over the years as the most comprehensive, open, high-performance data lakehouse project, this blog aims to give you some concise answers. Below, we shine a light on some unique capabilities in Hudi, that go beyond the lowest-common-denominator across the different projects in the space.</p>
<p><strong>1. Well-Balanced Storage Format</strong></p>
<p>Hudi‚Äôs <a href="https://hudi.apache.org/docs/storage_layouts" target="_blank" rel="noopener noreferrer">storage format</a> <em>perfectly balances write speed</em> (record-level changes) and <em>query performance</em> (scan+lookup optimized), at the cost of additional storage space to track indexes. In contrast, Apache Iceberg/Delta Lake formats produce storage layouts aimed at vanilla scans, focus more on metadata to help scale/prune the scans. Recent effots that adopt LSM tree structures to improve write performance, inevitably sacrifice query performance. See <a href="https://www.codementor.io/@arpitbhayani/the-rum-conjecture-16z2ckqte9" target="_blank" rel="noopener noreferrer">RUM conjecture</a>.</p>
<p><strong>2. Database-like Secondary Indexes</strong></p>
<p>In a long line of unique technical contributions to the lakehouse tech, Hudi recently added <a href="https://hudi.apache.org/docs/indexes#multi-modal-indexing" target="_blank" rel="noopener noreferrer">secondary indexes</a> (record level, bloom filters, ‚Ä¶), with support for even creating indexes on expressions on columns. Features heavily inspired by relational databases like Postgres, that can <em>unlock completely new use-cases</em> on the data lakehouse like <a href="https://en.wikipedia.org/wiki/Hybrid_transactional/analytical_processing" target="_blank" rel="noopener noreferrer">HTAP</a> or <a href="https://planetscale.com/learn/courses/mysql-for-developers/queries/indexing-joins" target="_blank" rel="noopener noreferrer">index-joins</a>.</p>
<p><strong>3. Efficient Merge-on-Read (MoR) Design</strong></p>
<p>Hudi‚Äôs <a href="https://hudi.apache.org/docs/table_types#merge-on-read-table" target="_blank" rel="noopener noreferrer">optimized MoR design</a> <em>minimizes read/write amplification</em>, by a range of techniques like file grouping and partial updates. Grouping helps cut down the amount of update blocks/deletion blocks/vectors to be scanned to serve snapshot queries. It also helps <em>preserve temporal locality</em> of data that dramatically improves time-based access for e.g building dashboards based on time - last hour, last day, last week, ‚Ä¶ - that are table stakes for warehouse/lakehouse users.</p>
<p><strong>4. Scalable Metadata for Large-Scale Datasets</strong></p>
<p>Hudi‚Äôs <a href="https://hudi.apache.org/docs/metadata" target="_blank" rel="noopener noreferrer">metadata table</a> efficiently handles <em>millions of files</em>, by storing them <em>efficiently</em> in an indexed <a href="https://www.scylladb.com/glossary/sstable" target="_blank" rel="noopener noreferrer">SSTable</a> based file format. Similarly, Hudi also indexes other metadata like column statistics, such that query planning scales linearly with <em>O(number_of_columns_in_query)</em>, as opposed to flat-file storage like avro that scales poorly with size of tables, large number of files or wide-columns.</p>
<p><strong>5. Built-In Table Services</strong></p>
<p>Hudi comes <em>loaded with automated <a href="https://hudi.apache.org/docs/write_operations#write-path" target="_blank" rel="noopener noreferrer">table services</a></em> like compaction, clustering, indexer, de-duplication, archiver, TTL enforcement and cleaning, that are scheduled, executed, retried, automatically with every write without requiring any external orchestration or manual SQL commands for table maintenance. Hudi‚Äôs <a href="https://hudi.apache.org/docs/markers/" target="_blank" rel="noopener noreferrer">marker mechanism</a> efficiently cleans up uncomitted/orphaned files during writes without requiring full-listing of cloud storage to identify such files (can take hours or even timeout forever).</p>
<p><strong>6. Data Management Smarts</strong></p>
<p>Stepping in level deeper, Hudi fully manages everything around storage : <a href="https://hudi.apache.org/docs/overview" target="_blank" rel="noopener noreferrer">file sizes, partitions and metadata maintenance</a> automatically on each write, to provide consistent, dependable read/write performance. Further more,  Hudi provides <em>advanced <a href="https://hudi.apache.org/docs/clustering" target="_blank" rel="noopener noreferrer">sorting/clustering</a> capabilities</em>, that can be <em>incrementally</em> run with new writes, to keep tables optimized.</p>
<p><strong>7. Concurrency Control Purpose-built For the Lake</strong></p>
<p>Hudi‚Äôs <a href="https://hudi.apache.org/blog/2025/01/28/concurrency-control" target="_blank" rel="noopener noreferrer">concurrency control</a> is carefully designed to deliver high throughput for data lakehouse workloads, without blindly rehashing approaches that work for OLTP databases. Hudi brings novel MVCC based approaches and <a href="https://hudi.apache.org/docs/concurrency_control#non-blocking-concurrency-control" target="_blank" rel="noopener noreferrer">non-blocking concurrency control</a>. Data pipelines/SQL ETLs and table services won‚Äôt fail/livelock each other eliminating wastage of compute cycles, improving data freshness and reducing cloud bills. Even on optimistic concurrency control model (L.C.D across projects), Hudi provides <em>early conflict detection</em> to pre-emptively abort writes that will eventually fail due to conflicts, saving countless compute hours.</p>
<p><strong>8. Performance at Scale</strong></p>
<p>Hudi stands out on the <em>toughest workloads</em> you should be testing first before deciding your lakehouse stack : CDC ingest, expensive SQL merges or TB-PB scale streaming data. Hudi provides about <a href="https://hudi.apache.org/docs/indexes#additional-writer-side-indexes" target="_blank" rel="noopener noreferrer">half a dozen writer side indexes</a> including advanced record level indexes, range indexes built on interval trees or consistent-hashed bucket indexes to scale writes for such workloads. Hudi is the <em>only lakehouse project</em>, that can rapidly ingest/write and handle small-file compaction without blocking those writes.</p>
<p><strong>9. Out-of-box CDC/Streaming Ingestion</strong></p>
<p>Hudi provides <em>powerful, fully-production ready  ingestion</em> <a href="https://hudi.apache.org/docs/hoodie_streaming_ingestion" target="_blank" rel="noopener noreferrer">tools</a> for both Spark/Flink/Kafka users, that help users build data lakehouses from their data, with a single-command. In fact, many many Hudi users blissfully use these tools, unaware of all the underlying machinery balancing write/read performance or table maintenance. This way, Hudi provides a self-managing runtime environment, for your data lakehouse pipelines, without having to pay for closed-services from vendors. Hudi ingest tools natively support popular CDC formats like Debezium/AWS DMS/Mongo and sources like S3, GCS, Kafka, Pulsar and the like.</p>
<p><strong>10. First-Class Support for Keys</strong></p>
<p>Hudi treats record <a href="https://hudi.apache.org/docs/key_generation" target="_blank" rel="noopener noreferrer">keys</a> as first-class citizen, used everywhere from indexing, de-duplication, clustering, compaction to consistently track/control movement of records within a table, across files. Additionally, Hudi also tracks <a href="https://www.onehouse.ai/blog/hudi-metafields-demystified" target="_blank" rel="noopener noreferrer">necessary record-level metadata</a> that help implement powerful features like incremental queries, in conjunction with queries. Ingest tools seamlessly map source primary keys to Hudi primary keys or auto-generate <em>highly-compressible</em> keys to aid these capabilities.</p>
<p><strong>11. Streaming-First Design</strong></p>
<p>Hudi was born out of a need to bridge the gap between batch processing and stream processing models. Thus, naturally, Hudi offers <em>best-in-class and unique capabilities</em> around handling streaming data. Hudi supports <a href="https://hudi.apache.org/docs/record_merger#event_time_ordering" target="_blank" rel="noopener noreferrer">event time ordering</a> and late data handling natively in storage where MoR is employed heavily. RecordPayload/RecordMerger APIs let you merge updates in the database LSN order compared to other approaches, avoiding cases like tables going back in (event) time, if the input is out-of-order/late-arriving (which is more the norm/nor an exception).</p>
<p><strong>12. Efficient Incremental Processing</strong></p>
<p>All roads in Hudi, lead to efficiency in storage and compute. Storage by <em>reducing</em> the amount of <em>data stored/accessed</em>, compute by reducing the <em>time needed write/read</em>. Hudi supports unique <a href="https://www.onehouse.ai/blog/getting-started-incrementally-process-data-with-apache-hudi" target="_blank" rel="noopener noreferrer">incremental queries</a>, along with CDC queries to allow downstream data consumers to quickly obtain changes to a table, between two time intervals. Owing to scalable metadata design, a LSM-tree backed timeline history and record-level change tracking, Hudi is able to support near infinite retention for such streams, provide very useful when dealing with transactional data/logs.</p>
<p><strong>13. Powerful Apache Spark Implementation</strong></p>
<p>Hudi comes with a very feature-rich, advanced integration with Apache Spark - across SQL, DataSource, RDD APIs, Structured Streaming and Spark Streaming. When combined together, <em>Hudi + Spark</em> almost gives users a <a href="https://github.com/apache/hudi/blob/master/rfc/rfc-69/rfc-69.md" target="_blank" rel="noopener noreferrer">database</a> - with built-in data management, ingestion, streaming/batch APIs, ANSI SQL and programmatic access from Python/JVM. Much like a database, the write/read implementation paths automatically pick the right storage layout to optimize storage at rest or do necessary index pruning to speed up queries.</p>
<p><strong>14. Next-Gen Flink Writer for Streaming Pipelines</strong></p>
<p><a href="https://www.onehouse.ai/blog/intro-to-hudi-and-flink" target="_blank" rel="noopener noreferrer">Hudi and Flink</a> have the best impedance match when it comes to handling streaming data. Hudi Flink sink is built on a <em>deep integration</em> between the two project capabilities, by leveraging Flink‚Äôs state backends as an writer side index in Hudi. With the combination of non-blocking concurrency and partial updates, Hudi is the only lakehouse storage sink for Flink, that can allow <em>multiple streaming writers</em> concurrently write a table (without having to fail one). Just like Spark, Flink writer comes with built-in table services, akin to a ‚Äústreaming database‚Äù for the lakehouse.</p>
<p><strong>15. Avoid Compute Lockins</strong></p>
<p>Don‚Äôt let the noise fool you. Hudi is <a href="https://hudi.apache.org/ecosystem" target="_blank" rel="noopener noreferrer"><em>widely supported</em></a> across cloud warehouses (Redshift, BigQuery), open-source query/processing engines (Spark, Presto, Trino, Flink, Hive, Clickhouse, Starrocks, Doris) and also hosted offering of those open-source engines (AWS Athena, EMR, DataProc, Databricks). This means, you have the power to fully control <em>not just the open format</em> you store data in, but also the end-end ingestion, transformation and optimizations of your tables, avoiding any ‚Äúcompute lockin‚Äù with these engines.</p>
<p><strong>16. Seamless Interop Iceberg/Delta Lake and Catalog Syncs</strong></p>
<p>To make the point above really easy, Hudi also ships with a <a href="https://hudi.apache.org/docs/syncing_aws_glue_data_catalog" target="_blank" rel="noopener noreferrer">catalog sync</a> mechanism, that supports about <em>6 different data catalogs</em> to keep your table definitions in sync over time. Hudi tables can be readily queried as external tables on cloud data warehouses. And, with the <a href="https://github.com/apache/xtable" target="_blank" rel="noopener noreferrer">Apache XTable</a> (Incubating) catalog sync, Hudi enables interoperability with Iceberg and Delta Lake table format, without the need to duplicate data storage or processing. Thus, Hudi offers the most open way to manage your data on the cloud.</p>
<p><strong>17. Truly Open and Community-Driven</strong></p>
<p>Apache Hudi is an <a href="https://hudi.apache.org/community" target="_blank" rel="noopener noreferrer">open-source project</a>, actively developed by a diverse global <a href="https://ossinsight.io/analyze/apache/hudi#contributors" target="_blank" rel="noopener noreferrer">community</a>. In fact, the grass-roots nature of the project and its community have been the crucial reason for the lasting success Hudi has had in the industry, inspite 100-1000x bigger vendor teams marketing/selling users in other directions. Project has an established track record of truly, collaborative way of software development, the <a href="https://www.apache.org/theapacheway/" target="_blank" rel="noopener noreferrer">apache way</a>.</p>
<p><strong>18. Massive Adoption Across Industries</strong></p>
<p>For system/infrastructure software like Hudi, it‚Äôs very important to gain/prove maturity by clocking massive amounts of server hours. Hudi is used at massive scale at much of the Fortune 100s and large organizations like  <a href="https://hudi.apache.org/powered-by" target="_blank" rel="noopener noreferrer">Uber, AWS, ByteDance, Peloton, Huawei, Alibaba, and more</a>, adding immense value in terms of a steady stream of  high-quality bug reports and feature asks shaping the projects roadmap. This way, Hudi users get highly capable lakehouse software, that can address a diverse range of use-cases.</p>
<p><strong>19. Proven Reliability in High-Pressure Workloads</strong></p>
<p>Hudi has been pressure-tested at some of the most demanding worloads there is, on the data lakehouse. From <a href="https://www.uber.com/blog/uber-big-data-platform/" target="_blank" rel="noopener noreferrer">minute-level latency</a> on petabytes to storing ingesting &gt; 100GB/s or just very <a href="https://aws.amazon.com/blogs/big-data/how-amazon-transportation-service-enabled-near-real-time-event-analytics-at-petabyte-scale-using-aws-glue-with-apache-hudi/" target="_blank" rel="noopener noreferrer">tough random write</a> workloads, that test even the best OLTP databases out there. Hudi has been deployed industry-wide for very critical data processing needs like financial clearing jobs, ride-sharing payments or transactional reconciliation.</p>
<p><strong>20. Cloud-Native and Lakehouse-Ready</strong></p>
<p>Don‚Äôt let the origins from a Hadoop mislead you either. Hudi has long evolved past HDFS and works seamlessly with <a href="https://hudi.apache.org/docs/cloud" target="_blank" rel="noopener noreferrer">S3, GCS, Azure, Alibaba, Huawei and many other cloud storage</a> systems. Together with the <a href="https://www.onehouse.ai/blog/apache-hudi-native-aws-integrations" target="_blank" rel="noopener noreferrer">cloud-native</a> integrations or just via <a href="https://www.onehouse.ai/blog/apache-hudi-on-microsoft-azure" target="_blank" rel="noopener noreferrer">easy integrations</a> outside of Cloud-native services, Hudi provides a very portable (cross-engine, format, cloud) way for building cloud data lakehouses.</p>
<p><strong>21. Future-Proof and Actively Evolving</strong></p>
<p>Hudi‚Äôs community boasts about 40-50 monthly active developers, which is growing even more with efforts like <a href="https://github.com/apache/hudi-rs" target="_blank" rel="noopener noreferrer">hudi-rs</a>. Hudi‚Äôs <a href="https://github.com/apache/hudi" target="_blank" rel="noopener noreferrer">rapid development</a> ensures constant improvements and cutting-edge features on one hand, while the openness of the community to truly work across the entire cloud data ecosystem on the other, ensure your data stays as open as possible.</p>
<p>In summary, there is no secret sauce. The answer to the original question is simply how these design and implementation differences have compounded over time into unmatched technical capabilities that data engineers across the industry widely recognize. These have resulted from 6+ years of evolution, hardening and iteration from an OSS community. And, it's always a moving target, given the amount of innovation that is still ahead of us, in the data lakehouse space. By the time some of these differences make it to other projects, the community might have innovated 21 more reasons.</p>
<p>Apache Hudi is the <strong>best-in-class open-source data lakehouse platform</strong> ‚Äîpowerful, efficient, and future-proof. Start exploring it today! üöÄ</p>]]></content:encoded>
            <category>Data Lake</category>
            <category>Data Lakehouse</category>
            <category>Apache Hudi</category>
            <category>Apache Iceberg</category>
            <category>Delta Lake</category>
            <category>Table Format</category>
        </item>
        <item>
            <title><![CDATA[Record Mergers in Apache Hudi]]></title>
            <link>https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi</link>
            <guid>https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi</guid>
            <pubDate>Mon, 03 Mar 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[The Challenge of Unordered Streams of Events]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-challenge-of-unordered-streams-of-events">The Challenge of Unordered Streams of Events<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#the-challenge-of-unordered-streams-of-events" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>One of the primary challenges associated with streaming workloads is the unordered nature of incoming events. In a typical streaming scenario, events can arrive out of sequence due to network latency, processing delays, or other factors. With the increasing volume and velocity of data being ingested from various sources‚Äîespecially in mobile applications and IoT platforms‚Äîdata processing frameworks must be equipped to handle mutations (i.e., changes to records) and out-of-order events.
Traditional data storage systems and file formats, such as those optimized for batch processing, often struggle to manage these scenarios effectively. Hudi steps in with features specifically designed to handle such challenges.
When events or changes to a record arrive at different times, they may not be in the same order in which they were originally generated. For example, in a smart city traffic monitoring system, sensors may report vehicle speeds at various intersections in real-time. However, due to network issues or delays, some sensor data might arrive later than others, possibly out of order. To handle this, the system needs to merge the new incoming data with existing records efficiently. Just like how Hudi‚Äôs merge modes control the merging of records with the same key in a storage system, ensuring consistency and accuracy, it ensures that the final traffic data reflects the correct event times, even when some data arrives with a delay. These merge modes help maintain a consistent, deterministic result under heavy load, making sure that late data updates the right records without causing inconsistencies.
This can lead to several issues:</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="data-integrity">Data Integrity<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#data-integrity" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h4>
<p>When events are processed out of order, it can result in incorrect or inconsistent data states. For example, if an event representing a transaction is processed before the event that indicates the account balance, the resulting data may not accurately reflect the true state of the system.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="complexity-in-processing">Complexity in Processing<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#complexity-in-processing" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h4>
<p>Handling unordered events often requires additional logic to ensure that data is processed in the correct sequence. This can complicate the data pipeline and increase the likelihood of errors.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-record-mergers">What are Record Mergers<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#what-are-record-mergers" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>With the new api introduced with version 1.0.0, Hudi supports three primary merge modes, each suited to different stages of data processing: writing, compaction, and querying.
4 places/points of data processing [Subheader]</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="1-merging-input-data-before-writing--combining-change-records-during-writes">1. Merging input data before writing : Combining Change Records During Writes<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#1-merging-input-data-before-writing--combining-change-records-during-writes" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h4>
<p>When new data arrives for an existing record, Hudi performs deduplication on the input dataset. This process involves combining multiple change records for the same record key before the write phase. This is an optimization that also helps reduce the number of records written to the log files (in case of MOR). By merging changes upfront, Hudi reduces unnecessary records, improving the efficiency of both query and write operations.
This step is crucial for handling stream data in real-time, where changes may arrive rapidly, and ensuring that only the final version of the record is written into the system. Normally these out of order events come together commonly in the same batch,  With processing engines like spark, which deals with micro-batches, merging the input changes helps in reduces the number of records which needs to be written.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="2-merging-final-change-record-in-cow-copy-on-write-tables-applying-changes-to-existing-records">2. Merging Final Change Record in CoW (Copy-on-Write) Tables: Applying Changes to Existing Records<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#2-merging-final-change-record-in-cow-copy-on-write-tables-applying-changes-to-existing-records" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h4>
<p>In Copy-on-Write (CoW) tables, changes are applied by creating new file versions for the records. When an update, partial update, or delete operation occurs, Hudi will merge this final change with the existing record in the storage. The merge mode controls how these updates are applied, ensuring that only the most recent changes are reflected and the table‚Äôs data remains consistent.
This is especially important in CoW tables, as they preserve immutability of historical data by writing new versions of the records instead of overwriting the existing data. The merge mode ensures that the new version of the record is consistent with all previous changes.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="3-compaction-merge-in-mor-merge-on-read-tables--merging-log-files-with-base-files">3. Compaction Merge in MoR (Merge-on-Read) Tables : Merging Log Files with Base Files<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#3-compaction-merge-in-mor-merge-on-read-tables--merging-log-files-with-base-files" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h4>
<p>Hudi uses a concept of log files (delta logs) and base files (original data). As changes to records accumulate over time, Hudi‚Äôs compaction service merges the change records stored in the log files with the base files to keep the data consistent and query-optimized. The merge mode defines how these log records are merged with base files during the compaction process.
Compaction helps maintain storage efficiency and ensures that queries run faster by reducing the number of small log files that might need to be read.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="4-query-time-merge-merging-log-files-with-base-files-in-mor-merge-on-read-tables">4. Query-Time Merge: Merging Log Files with Base Files in MoR (Merge-on-Read) Tables<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#4-query-time-merge-merging-log-files-with-base-files-in-mor-merge-on-read-tables" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h4>
<p>In Merge-on-Read (MoR) tables, the data is stored in both log files and base files. When a query is executed, Hudi merges the change records in the log files with the base files based on the merge mode. The merge operation occurs at query time to provide the final, consistent view of the data.
By merging records at query time, Hudi ensures that queries reflect the most recent changes while maintaining query performance.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="implementation">Implementation<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#implementation" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>In common scenarios, the input data contains a field that can be used to identify the latest record. Typically, tables have fields like updated_at or other ordering columns. If no such column is present in the input, we are limited to relying on the incoming order.</p>
<p>After the release of Hudi 1.0.0, a new configuration, <a href="https://hudi.apache.org/docs/configurations/#hoodierecordmergemode" target="_blank" rel="noopener noreferrer">hoodie.record.merge.mode</a> was introduced to define the merge modes responsible for handling record updates. These merge modes dictate how records with the same key are processed at different stages of the pipeline, from data ingestion to query results.
It can have the following three values:</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="1-commit_time_ordering">1. COMMIT_TIME_ORDERING<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#1-commit_time_ordering" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h4>
<p>This merge mode is used when no field is available in the input data to explicitly determine which record is the latest. The system will rely on the order of ingestion (commit time) to determine the order of records. Hudi expects records to arrive in strict order of their commits. So, the most recent record (in terms of ingestion time) is assumed to be the latest version of the record. This mode is typically used when there is no dedicated column like updated_at, timestamp, or versioning field that can indicate the order of the records.
The merging logic here simply picks the latest write based on the ingestion order (commit time). In a way, it's equivalent to overwriting semantics where only the most recent record is considered.
Example -</p>
<div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">SET</span><span class="token plain"> hoodie</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">sql</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">insert</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">into</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">operation</span><span class="token operator">=</span><span class="token plain">upsert</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">TABLE</span><span class="token plain"> hudi_table </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ts </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">BIGINT</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    uuid STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    rider STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    driver STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    fare </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">DOUBLE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    city STRING</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">USING</span><span class="token plain"> HUDI TBLPROPERTIES </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">primaryKey </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'uuid'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> hoodie</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">record</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">merge</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">mode</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">'COMMIT_TIME_ORDERING'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INSERT</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INTO</span><span class="token plain"> hudi_table</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">VALUES</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">3</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'334e26e9-8355-45cc-97c6-c31daf0df330'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'rider-A'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'driver-K'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">19.10</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'san_francisco'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'334e26e9-8355-45cc-97c6-c31daf0df330'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'rider-C'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'driver-M'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">27.70</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'san_francisco'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">select</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> hudi_table</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">-- Result - 20250106162911278	20250106162911278_0_0	334e26e9-8355-45cc-97c6-c31daf0df330		08218473-f72a-480d-90e6-c6764f062e5c-0_0-43-47_20250106162911278.parquet	1695091554788	334e26e9-8355-45cc-97c6-c31daf0df330	rider-C	driver-M	27.7	san_francisco</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INSERT</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INTO</span><span class="token plain"> hudi_table</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">VALUES</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'334e26e9-8355-45cc-97c6-c31daf0df330'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'rider-D'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'driver-K'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">19.10</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'san_francisco'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">select</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> hudi_table</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">-- Result - 20250106163449812	20250106163449812_0_0	334e26e9-8355-45cc-97c6-c31daf0df330		08218473-f72a-480d-90e6-c6764f062e5c-0_0-71-68_20250106163449812.parquet	1	334e26e9-8355-45cc-97c6-c31daf0df330	rider-D	driver-K	19.1	san_francisco</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>In the example above, we created the table using the COMMIT_TIME_ORDERING merge mode. When using this mode, there is no need to specify a precombine or ordering field.
During the first insert, two records with the same record key are provided. The system will deduplicate them and keep the record that is processed later.
In the second insert, a new record with the same record key is inserted. As expected, the table is updated with the new record because it is committed later, regardless of the values in any of the fields.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="2-event_time_ordering-default">2. EVENT_TIME_ORDERING (DEFAULT)<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#2-event_time_ordering-default" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h4>
<p>This merge mode is used when you do have a field in the input data that can be used to determine the order of events (such as a timestamp field like updated_at or a version number). If your records contain a field that can be used to track when the record was last updated (e.g., updated_at, last_modified, or a sequence number), Hudi will use this field to determine which record is the latest.
In this case, Hudi does not rely on the ingestion order but instead uses the value of the ordering field (updated_at, for example) to decide the correct record.
This approach is ideal when you have temporal or event-driven data, and you want to maintain the "latest" record according to an event timestamp.
Example -</p>
<div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">DROP</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">TABLE</span><span class="token plain"> hudi_table</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">SET</span><span class="token plain"> hoodie</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">sql</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">insert</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">into</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">operation</span><span class="token operator">=</span><span class="token plain">upsert</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">TABLE</span><span class="token plain"> hudi_table </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ts </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">BIGINT</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    uuid STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    rider STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    driver STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    fare </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">DOUBLE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    city STRING</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">USING</span><span class="token plain"> HUDI TBLPROPERTIES </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">primaryKey </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'uuid'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">preCombineField </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'ts'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> hoodie</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">record</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">merge</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">mode</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">'EVENT_TIME_ORDERING'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INSERT</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INTO</span><span class="token plain"> hudi_table</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">VALUES</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">3</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'334e26e9-8355-45cc-97c6-c31daf0df330'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'rider-A'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'driver-K'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">19.10</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'san_francisco'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'334e26e9-8355-45cc-97c6-c31daf0df330'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'rider-C'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'driver-M'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">27.70</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'san_francisco'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">select</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> hudi_table</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">-- Result - 20250106165902806	20250106165902806_0_0	334e26e9-8355-45cc-97c6-c31daf0df330		568ce7bc-9b71-4e15-b557-cbaeb5b4d2ea-0_0-56-57_20250106165902806.parquet	3	334e26e9-8355-45cc-97c6-c31daf0df330	rider-A	driver-K	19.1	san_francisco</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INSERT</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INTO</span><span class="token plain"> hudi_table</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">VALUES</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'334e26e9-8355-45cc-97c6-c31daf0df330'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'rider-D'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'driver-K'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">18.00</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'san_francisco'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">select</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> hudi_table</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">-- Result - 20250106165902806	20250106165902806_0_0	334e26e9-8355-45cc-97c6-c31daf0df330		568ce7bc-9b71-4e15-b557-cbaeb5b4d2ea-0_0-84-78_20250106165918731.parquet	3	334e26e9-8355-45cc-97c6-c31daf0df330	rider-A	driver-K	19.1	san_francisco</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>In the example above, we created the table using the EVENT_TIME_ORDERING merge mode. When using this mode, we need to specify the precombineField. In this case we are specifying ts as the precombineField.
During the first insert, two records with the same record key are provided. The system will deduplicate them and keep the record that is processed later.
In the second insert, a new record with the same record key is inserted. As expected, the table is updated with the new record because it is committed later, regardless of the values in any of the fields.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="3-custom">3. CUSTOM<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#3-custom" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h4>
<p>For more complex use-case sometimes prior discussed merging modes won‚Äôt work. We may need to implement a use-case specific merging logic.
The details for the implementation is provided here  - <a href="https://hudi.apache.org/docs/record_merger/#custom" target="_blank" rel="noopener noreferrer">https://hudi.apache.org/docs/record_merger/#custom</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="record-payloads">Record Payloads<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#record-payloads" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>Pre 1.0.0, Hudi uses the legacy Record Payload API, Please refer to the <a href="https://hudi.apache.org/docs/record_merger/#record-payloads" target="_blank" rel="noopener noreferrer">Record Payloads</a> section to know about the implementation and some of the existing record payloads.</p>
<p>Along with the existing payloads, Hudi provides flexibility to implement the custom record payload by implementing the <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordPayload.java" target="_blank" rel="noopener noreferrer">HoodieRecordPayload</a> interface</p>
<p>The following example demonstrates the use of Record Payload, which achieves a similar outcome to what EVENT_TIME_ORDERING does. We‚Äôve used the same example as above to illustrate how this functionality works.</p>
<div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">DROP</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">TABLE</span><span class="token plain"> hudi_table</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">SET</span><span class="token plain"> hoodie</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">sql</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">insert</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">into</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">operation</span><span class="token operator">=</span><span class="token plain">upsert</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">TABLE</span><span class="token plain"> hudi_table </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ts </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">BIGINT</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    uuid STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    rider STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    driver STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    fare </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">DOUBLE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    city STRING</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">USING</span><span class="token plain"> HUDI TBLPROPERTIES </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">primaryKey </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'uuid'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">preCombineField </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'ts'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> hoodie</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">datasource</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">write</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">payload</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">class</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">'org.apache.hudi.common.model.DefaultHoodieRecordPayload'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INSERT</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INTO</span><span class="token plain"> hudi_table</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">VALUES</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">3</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'334e26e9-8355-45cc-97c6-c31daf0df330'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'rider-A'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'driver-K'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">19.10</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'san_francisco'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'334e26e9-8355-45cc-97c6-c31daf0df330'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'rider-C'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'driver-M'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">27.70</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'san_francisco'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">select</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> hudi_table</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">-- Result - 20250203164444124	20250203164444124_0_0	334e26e9-8355-45cc-97c6-c31daf0df330		4549ed8e-0346-4d59-8878-9e047fb6c651-0_0-14-17_20250203164444124.parquet	3	334e26e9-8355-45cc-97c6-c31daf0df330	rider-A	driver-K	19.1	san_francisco</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INSERT</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INTO</span><span class="token plain"> hudi_table</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">VALUES</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'334e26e9-8355-45cc-97c6-c31daf0df330'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'rider-D'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'driver-K'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">18.00</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'san_francisco'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">select</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> hudi_table</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">-- Result - 20250203164444124	20250203164444124_0_0	334e26e9-8355-45cc-97c6-c31daf0df330		4549ed8e-0346-4d59-8878-9e047fb6c651-0_0-53-51_20250203164537068.parquet	3	334e26e9-8355-45cc-97c6-c31daf0df330	rider-A	driver-K	19.1	san_francisco</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#conclusion" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>In conclusion, managing late-arriving and out-of-order data is a critical challenge in modern data processing systems, especially when dealing with large-scale, real-time data pipelines. Tools like Hudi provide powerful merge modes that ensure data consistency, accuracy, and efficiency by handling record updates intelligently across different stages of the pipeline. Whether you're working with streaming data, IoT sensors, or social media posts, understanding how to configure and use these merge modes can greatly improve the performance and reliability of your data storage and query processes. By leveraging the right merge strategy, you can ensure that your system remains robust, even under heavy load and with delayed data, ultimately enabling better decision-making and insights from your data.</p>]]></content:encoded>
            <category>Data Lake</category>
            <category>Data Lakehouse</category>
            <category>Apache Hudi</category>
            <category>Record Mergers</category>
            <category>Record payloads</category>
            <category>Late Arriving Data</category>
        </item>
        <item>
            <title><![CDATA[Curious Engineering Facts ( Trace Agents | Hudi| Daft : 1) : March Release 18 : 25]]></title>
            <link>https://hudi.apache.org/cn/blog/2025/02/25/curious-engineering-facts-trace-agents-hudi-daft-1</link>
            <guid>https://hudi.apache.org/cn/blog/2025/02/25/curious-engineering-facts-trace-agents-hudi-daft-1</guid>
            <pubDate>Tue, 25 Feb 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://medium.com/@kkgsanjeewac77/curious-engineering-facts-trace-agents-hudi-daft-1-march-release-18-25-bedc00e05ecd">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>apache hudi</category>
            <category>daft</category>
            <category>trace agents</category>
            <category>openai</category>
            <category>llm</category>
            <category>medium</category>
        </item>
        <item>
            <title><![CDATA[Building a Lakehouse Architecture on AWS with Terraform]]></title>
            <link>https://hudi.apache.org/cn/blog/2025/02/24/building-a-lakehouse-architecture-on-aws-with-terraform</link>
            <guid>https://hudi.apache.org/cn/blog/2025/02/24/building-a-lakehouse-architecture-on-aws-with-terraform</guid>
            <pubDate>Mon, 24 Feb 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://medium.com/@juanfelipear97/building-a-lakehouse-architecture-on-aws-with-terraform-139c079ec385">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>apache hudi</category>
            <category>aws</category>
            <category>terraform</category>
            <category>lakehouse</category>
            <category>medium</category>
        </item>
        <item>
            <title><![CDATA[Curious Engineering Facts (Lakehouse | Apache Hudi | Daft |Positional argument|) : March Release 19 : 25]]></title>
            <link>https://hudi.apache.org/cn/blog/2025/02/23/curious-engineering-facts-lakehouse-apache-hudi-daft-positional-argument</link>
            <guid>https://hudi.apache.org/cn/blog/2025/02/23/curious-engineering-facts-lakehouse-apache-hudi-daft-positional-argument</guid>
            <pubDate>Sun, 23 Feb 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://medium.com/@kkgsanjeewac77/curious-engineering-facts-lakehouse-apache-hudi-daft-positional-argument-march-release-d0fee8151736">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>apache hudi</category>
            <category>daft</category>
            <category>streamlit</category>
            <category>cow</category>
            <category>medium</category>
        </item>
        <item>
            <title><![CDATA[An intro to Hudi with MinIO]]></title>
            <link>https://hudi.apache.org/cn/blog/2025/01/30/an-intro-to-hudi-with-minio</link>
            <guid>https://hudi.apache.org/cn/blog/2025/01/30/an-intro-to-hudi-with-minio</guid>
            <pubDate>Thu, 30 Jan 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://dataxplorer.medium.com/an-intro-to-hudi-with-minio-i-75536fe75b4c">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>apache hudi</category>
            <category>minio</category>
            <category>medium</category>
        </item>
        <item>
            <title><![CDATA[Concurrency Control in Open Data Lakehouse]]></title>
            <link>https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control</link>
            <guid>https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control</guid>
            <pubDate>Tue, 28 Jan 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Introduction]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#introduction" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>Concurrency control is critical in database management systems to ensure consistent and safe access to shared data by multiple users. Relational databases (RDBMS) such as <a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-locking-transaction-model.html" target="_blank" rel="noopener noreferrer">MySQL (InnoDB)</a> and analytical databases (such as data warehouses) have been offering robust concurrency control mechanisms to effectively deal with this. As data grows in scale and complexity, managing concurrent access becomes more challenging, especially in large distributed systems like Data Lakes or <a href="https://hudi.apache.org/blog/2024/07/11/what-is-a-data-lakehouse/" target="_blank" rel="noopener noreferrer">Lakehouses</a>, which are expected to handle different types of workloads in the analytics realm. While data lakes have traditionally struggled with concurrent operations due to the lack of a <a href="https://hudi.apache.org/docs/hudi_stack#storage-engine" target="_blank" rel="noopener noreferrer">storage engine</a> and ACID guarantees, lakehouse architectures with open table formats like Apache Hudi, Apache Iceberg, and Delta Lake take inspiration from some of the widely used concurrency control methods to support high concurrent workloads.</p>
<p>This blog goes into the fundamentals of concurrency control, explores why it is essential for lakehouses, and examines how open table formats such as Apache Hudi enable strong concurrency control mechanisms to uphold the ACID properties and deal with varied workloads.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="concurrency-control-foundations">Concurrency Control Foundations<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#concurrency-control-foundations" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>At the core of concurrency control are the concepts of Isolation and Serializability, which define the expected behavior for concurrent transactions and ensure the <strong>"I"</strong> in ACID properties. Let‚Äôs quickly go over these concepts from a general database system perspective.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="isolation-and-serializability">Isolation and Serializability<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#isolation-and-serializability" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>In transactional systems, Isolation ensures that each transaction operates independently of others, as if it were executed in a single-user environment. This means a transaction should be "all by itself," free from interference by other concurrent operations, preventing concurrency anomalies like dirty reads or lost updates. This isolation allows end users (such as developers or analysts) to understand the impact of a transaction without worrying about conflicts from other simultaneous operations.</p>
<p>Serializability takes this idea further by defining the correct execution order for concurrent transactions. It guarantees that the outcome of executing transactions concurrently will be the same as if they had been executed serially, one after the other. In other words, even if transactions are interleaved, their combined effect should appear as though there were no parallel execution at all. Serializability is thus a rigorous correctness criterion that concurrency control models in databases strive to enforce, providing a predictable environment for transactional workloads.</p>
<p>For example, imagine an online concert ticketing system where multiple customers are attempting to purchase tickets for the same concert at the same time. Suppose there are only 5 tickets left, and two customers - Customer A and Customer B try to buy 3 tickets each simultaneously. Without proper concurrency control, these transactions might interfere with each other, leading to scenarios where more tickets are "sold" than available in inventory, resulting in inconsistencies. To maintain serializability, the system must ensure that the outcome of processing these transactions concurrently is the same as if they were processed one at a time (serially), i.e. no more than 5 tickets are sold, ensuring inventory consistency.</p>
<p>Concurrency control methods can be broadly classified into three approaches: Pessimistic Concurrency Control, Optimistic Concurrency Control, and Multi-Version Concurrency Control (MVCC).</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="pessimistic-concurrency-control-2pl">Pessimistic Concurrency Control (2PL)<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#pessimistic-concurrency-control-2pl" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h4>
<p>Pessimistic Concurrency Control assumes that conflicts between transactions can happen often and avoids having ‚Äòproblems‚Äô in the first place. The most commonly used method, Strict Two-Phase Locking (2PL), works in this way:</p>
<ul>
<li>Transactions acquire a shared lock before reading data and an exclusive lock before writing.</li>
<li>Locks are held until the transaction commits or aborts but releases immediately after the commit command executes, ensuring serializability.</li>
</ul>
<img src="https://hudi.apache.org/assets/images/blog/concurrency_control/2PL.png" alt="2PL" width="1000" align="middle">
<p>If we take our online concert ticketing system example, where we have 5 tickets left and Customer A and Customer B both attempt to buy 3 tickets simultaneously. With Strict Two-Phase Locking (2PL), Transaction T1 (Customer A‚Äôs purchase) acquires an exclusive lock on the inventory, preventing Transaction T2 (Customer B‚Äôs purchase) from accessing it until T1 completes. T1 checks the inventory, deducts 3 tickets for Customer A, reducing the count to 2, and then releases the lock. Only then can T2 proceed, locking the inventory, seeing the updated 2 tickets, and completing the purchase for Customer B. This ensures serializability by isolating transactions through locking, yielding the same result as if the transactions had run one after the other.</p>
<p>While Strict 2PL guarantees correctness, it comes with some downsides:</p>
<ul>
<li>Transactions waiting to acquire locks may be blocked for long durations, especially in high-contention scenarios, leading to reduced throughput.</li>
<li>If two transactions hold locks on different resources and wait for each other to release them, a deadlock occurs, requiring intervention (e.g., by aborting one transaction).</li>
<li>The strict correctness requirements can lead to long transaction times, making it less suitable for high-concurrency workloads.</li>
</ul>
<p>Strict 2PL is present in relational database systems such as PostgreSQL, and Oracle Database.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="optimistic-concurrency-control-occ">Optimistic Concurrency Control (OCC)<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#optimistic-concurrency-control-occ" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h4>
<p>Optimistic concurrency control takes the opposite approach - it assumes that conflicts happen rarely, and if there are such scenarios, then it would deal with it at the time of the conflict. OCC works this way:</p>
<ul>
<li>Transactions track read and write operations and, upon completion, validate these changes to check for conflicts.</li>
<li>If conflicts are detected, one or more conflicting transactions are rolled back and can be retried if needed be.</li>
</ul>
<p>OCC is particularly effective in low-contention environments, where conflicts between transactions are infrequent. However, in scenarios with frequent conflicts, such as multiple transactions attempting to modify the same data, OCC may result in a high number of rollbacks, reducing its efficiency. Its ability to allow multiple transactions to proceed without locking makes it a good choice for workloads where contention is low and throughput is prioritized over strict blocking mechanisms.</p>
<img src="https://hudi.apache.org/assets/images/blog/concurrency_control/OCC.png" alt="OCC" align="middle">
<p>For our example, with OCC, both transactions will proceed, each reading the initial count of 5 tickets and preparing to deduct 3. When they try to commit, a conflict check (history) will reveal that reducing by 3 tickets would oversell the inventory. As a result, one transaction (e.g., Customer B‚Äôs) is rolled back, allowing Customer A to complete their purchase, reducing the inventory to 2. Customer B then retries, sees only 2 tickets left, and adjusts accordingly.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="multi-version-concurrency-control-mvcc">Multi-Version Concurrency Control (MVCC)<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#multi-version-concurrency-control-mvcc" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h4>
<p>MVCC enables concurrent transactions by maintaining multiple versions of each data item, allowing transactions to read data as it appeared at a specific point in time. Here‚Äôs how MVCC works at a high-level:</p>
<ul>
<li>Each transaction is split into a "read set" and a "write set." This separation of read and write sets enhances concurrency by reducing conflicts.</li>
<li>All reads in a transaction operate as if they are accessing a single, consistent ‚Äòsnapshot‚Äô of the data at a particular moment.</li>
<li>Writes are applied as if they are part of a ‚Äòlater snapshot‚Äô, ensuring that any changes made by the transaction are isolated from other concurrent transactions until the transaction completes.</li>
</ul>
<img src="https://hudi.apache.org/assets/images/blog/concurrency_control/MVCC.png" alt="MVCC" align="middle">
<p>In our example, with MVCC, each customer sees a consistent snapshot of 5 tickets when they start. Customer A completes their purchase first, reducing the inventory to 2 tickets. When Customer B finishes, they commit their transaction based on the latest snapshot, seeing only 2 tickets left and adjusting their purchase accordingly.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="concurrency-control-in-open-table-formats">Concurrency Control in Open Table Formats<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#concurrency-control-in-open-table-formats" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>Data lakes were built for scalable storage, cheaper cost, and to address some of the limitations of data warehouses (such as handling varied data types), but they lack the transactional storage engine needed to enforce ACID guarantees. We learnt in our previous section how isolation (the "I" in ACID) plays a critical role in managing concurrency by ensuring that each transaction operates independently without unintended interference from others. This level of isolation is essential for preventing concurrency anomalies like dirty reads, lost updates, and other issues that can compromise data integrity. Data lakehouse architecture with open table formats such as Apache Hudi, Apache Iceberg, and Delta Lake as the foundation for the storage layer addresses this problem by applying some of the concurrency control methods available in the database systems.</p>
<p>Let‚Äôs take a look at what type of concurrency control methods are available within these formats with a focus on <strong>Apache Hudi</strong>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="apache-hudi">Apache Hudi<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#apache-hudi" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>Most of the concurrency control implementations today in lakehouse table formats focus on optimistically handling conflicts. OCC relies on the assumption that conflicts are rare, making it suitable for simple, append-only jobs but inadequate for scenarios that require frequent updates or deletes. In OCC, each job typically takes a table-level lock to check for conflicts by determining if there are overlapping files that multiple jobs have impacted. If a conflict is detected, the job will abort its operation <em>entirely</em>. This could be a problem with certain types of workloads. For example, an ingest job writing data every 30 minutes and a deletion job running every two hours may often conflict, causing the deletion job to fail. In such cases especially with long-running transactions, OCC is problematic because the chance of conflicts increases over time.</p>
<img src="https://hudi.apache.org/assets/images/blog/concurrency_control/concur_blog.png" alt="Hudi concurrency control methods" width="900" align="middle">
<p>Apache Hudi‚Äôs uniqueness lies in the fact that it clearly distinguishes the different actors interacting with the format, i.e. writer processes (that issue user‚Äôs upserts/deletes), table services (such as clustering, compaction) and readers (that execute queries and read data). Hudi provides <a href="https://en.wikipedia.org/wiki/Snapshot_isolation" target="_blank" rel="noopener noreferrer">Snapshot Isolation</a> between all three types of processes, meaning they all operate on a consistent snapshot of the table. For writers, Hudi implements a variant of Serializable <a href="https://distributed-computing-musings.com/2022/02/transactions-serializable-snapshot-isolation/" target="_blank" rel="noopener noreferrer">Snapshot Isolation (SSI)</a>. Here‚Äôs how Hudi supports different types of concurrency control methods, offering fine-grained control over concurrent data access and updates.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="occ-multi-writers">OCC (Multi Writers)<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#occ-multi-writers" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h4>
<p>OCC is primarily used to manage concurrent writer processes in Hudi. For example, two different Spark jobs interacting with the same Hudi table to perform updates. Hudi‚Äôs OCC workflow involves a series of checks to detect and handle conflicts, ensuring that only one writer can successfully commit changes to a particular file group at any given time. Here‚Äôs a quick summary of what file groups and slices mean in Hudi.</p>
<p><em>File group: Groups multiple versions of a base file (e.g. Parquet). The file group is uniquely identified by a File id. Each version corresponds to the commit's timestamp recording updates to records in the file.</em></p>
<p><em>File slice: A File group can further be split into multiple slices. Each file slice within the file-group is uniquely identified by the commit's timestamp that created it.</em></p>
<p>OCC works in three phases - read, validate and write. When a writer begins a transaction, it first makes the changes, i.e. commits in isolation. During the validation phase, writers compare their proposed changes against existing file groups in the timeline to detect conflicts. Finally, in the write phase, the changes are either committed if no conflicts are found or rolled back if conflicts are detected.</p>
<p>For multi-writing scenarios, when a writer begins the commit process, it acquires a short-duration lock from the lock provider, typically implemented with an external service such as Zookeeper, Hive Metastore, or DynamoDB. Once the lock is secured, the writer loads the <a href="https://hudi.apache.org/docs/next/timeline" target="_blank" rel="noopener noreferrer">current timeline</a> to check for previously <code>completed</code> actions on the targeted file group. After that, it scans for any instances marked as completed with a timestamp greater than the target file slice's timestamp. If any such completed instances are found, it indicates that another writer has already modified the target file group, leading to a conflict. In this case, Hudi‚Äôs OCC logic prevents the current transaction from proceeding by aborting the writer‚Äôs operation, ensuring that only one writer‚Äôs updates are committed. If no conflicting instant exists, the transaction is allowed to proceed, and the writer completes the write operation, adding a new file slice to the timeline. Finally, Hudi updates the timeline with the location of the new file slice and releases the table lock, allowing other transactions to proceed. This approach adheres to the ACID principles providing consistency guarantees.</p>
<p>It is important to note that Hudi acquires locks <strong>only</strong> at critical points, such as during the commit or while scheduling table services, rather than across the entire transaction. This approach significantly improves concurrency by allowing writers to work in parallel without contention.</p>
<p>Additionally, Hudi‚Äôs OCC operates at the file level, meaning conflicts are detected and resolved based on the files being modified. For instance, when two writers work on non-overlapping files, both writes are allowed to succeed. However, if their operations overlap and modify the same set of files, only one transaction will succeed, and the other will be rolled back. This file-level granularity is a significant advantage in many real-world scenarios, as it enables multiple writers to proceed without issues as long as they are working on different files, improving concurrency and overall throughput.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="mvcc-writer-table-service-and-table-service-table-service">MVCC (Writer-Table Service and Table Service-Table Service)<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#mvcc-writer-table-service-and-table-service-table-service" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h4>
<p>Apache Hudi provides support for Multiversion Concurrency Control (MVCC) between writers and table-services (for example, an update Spark job and <a href="https://hudi.apache.org/docs/clustering" target="_blank" rel="noopener noreferrer">clustering</a>) and between different table services (such as <a href="https://hudi.apache.org/docs/compaction" target="_blank" rel="noopener noreferrer">compaction</a> and clustering). Similar to OCC, the Hudi timeline is instrumental in Hudi‚Äôs MVCC implementation, which keeps a track of all the events (instants) happening in a particular Hudi table. Every writer and reader relies on the file system‚Äôs state to decide where to carry out the operations, thereby providing read-write isolation.</p>
<p>When a write operation begins, Hudi marks the action as either <code>requested</code> or <code>inflight</code> on the timeline, making all processes aware of the ongoing operation. This ensures that table management operations such as compaction and clustering are aware of active writes and do not include the file slices currently being modified. With Hudi 1.0's new <a href="https://hudi.apache.org/docs/timeline/" target="_blank" rel="noopener noreferrer">timeline</a> design, compaction and clustering operations are now based on both the requested and completion times of actions, treating these timestamps as <em>intervals</em> to dynamically determine file slices. This means a service like compaction no longer needs to block ongoing writes and can be scheduled at any instant without interfering with active operations.</p>
<p>Under the new design, file slicing includes only those file slices whose completion times precede the start of the compaction or clustering process. This intelligent slicing mechanism ensures that these table management services work only on finalized data while new writes seamlessly continue without impacting the base files being compacted. By decoupling the scheduling of table services from active writes, Hudi 1.0 eliminates the need for strict scheduling sequences or blocking behaviors.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="non-blocking-concurrency-control-multi-writers">Non-Blocking Concurrency Control (Multi Writers)<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#non-blocking-concurrency-control-multi-writers" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h4>
<p>In a generic sense, Non-Blocking Concurrency Control (NBCC) allows multiple transactions to proceed simultaneously without locking, reducing delays and improving throughput in high-concurrency environments. <a href="https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0" target="_blank" rel="noopener noreferrer">Hudi 1.0</a> introduces a new concurrency mode, <code>NON_BLOCKING_CONCURRENCY_CONTROL</code>, where, unlike OCC, multiple writers can operate on the same table simultaneously with non-blocking conflict resolution. This approach eliminates the need for explicit locks to serialize writes, enabling higher concurrency. Instead of requiring each writer to wait, NBCC allows concurrent writes to proceed, making it ideal for real-time applications that demand faster data ingestion.</p>
<p>In NBCC, the only lock required is for writing the commit metadata to the Hudi timeline, which ensures that the order and state of completed transactions is tracked accurately. With the release of version 1.0, Hudi introduces <a href="https://hudi.apache.org/docs/timeline#truetime-generation" target="_blank" rel="noopener noreferrer">TrueTime</a> semantics for instant times on the timeline, ensuring unique and monotonically increasing instant values. Each action on the Hudi timeline now includes both a <em>requested time</em> and a <em>completion time</em>, enabling these actions to be treated as intervals. This allows for more precise conflict detection by reasoning about overlapping actions within these time intervals.  The final serialization of writes in NBCC is determined by the <em>completion</em> times. This means multiple writers can modify the same file group, with conflicts resolved automatically by query readers and the compactor. NBCC is available with the new Hudi 1.0 release, thereby providing more controls to balance speed with data consistency, even under heavy concurrent workloads.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="concurrency-control-deployment-modes-in-hudi">Concurrency Control Deployment Modes in Hudi<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#concurrency-control-deployment-modes-in-hudi" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>Hudi offers several deployment models to handle different concurrency needs, allowing users to optimize for performance, simplicity, or high-concurrency scenarios depending on the requirements.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="single-writer-with-inline-table-services">Single Writer with Inline Table Services<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#single-writer-with-inline-table-services" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h4>
<p>In this model, only one writer handles data ingestion or updates, with table services (such as cleaning, compaction, and clustering) running inline sequentially after every write. This approach <em>eliminates</em> the need for concurrency control as all operations occur in a single process. MVCC in Hudi guarantees that readers see consistent snapshots, isolating them from ongoing writes and table services. This model is ideal for straightforward use cases where the focus is on getting data into the lakehouse without the complexity of managing multiple writers.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="single-writer-with-async-table-services">Single Writer with Async Table Services<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#single-writer-with-async-table-services" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h4>
<p>For workloads that require higher throughput without blocking writers, Hudi supports asynchronous table services. In this model, a single writer continuously ingests data, while table services such as compaction and clustering run asynchronously in the same process. MVCC allows these background jobs to operate concurrently with ingestion without creating conflicts, as they coordinate to avoid race conditions. This model suits applications where ingestion speed is essential, as async services help optimize the table in the background, reducing operational complexity without the need for external orchestration.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="multi-writer-configuration">Multi-Writer Configuration<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#multi-writer-configuration" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h4>
<p>In cases where multiple writer jobs need to access the same table, Hudi supports multi-writer setups. This model allows disparate processes, such as multiple ingestion writers or a mix of ingestion and separate table service jobs to write concurrently. To manage conflicts, Hudi uses OCC with file-level conflict resolution, allowing non-overlapping writes to proceed while conflicting writes are resolved by allowing only one to succeed. For these types of multi-writer setups, <a href="https://hudi.apache.org/docs/concurrency_control#external-locking-and-lock-providers" target="_blank" rel="noopener noreferrer"><em>external</em></a> lock providers like Amazon DynamoDB, Zookeeper, or Hive Metastore are required to coordinate concurrent access. This setup is ideal for production-level, high-concurrency environments where different processes need to modify the table simultaneously.</p>
<p>Note that while Hudi provides OCC to deal with multiple writers, table services can still run asynchronously and without locks if they operate in the same process as the writer. This is because Hudi intelligently differentiates between the different types of actors (writers, table services) that interact with the table.</p>
<p>You will need to set the following properties to activate OCC with locks.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">hoodie.write.concurrency.mode=optimistic_concurrency_control</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">hoodie.write.lock.provider=&lt;lock-provider-classname&gt;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">hoodie.cleaner.policy.failed.writes=LAZY</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><code>Hoodie.write.lock.provider</code> defines the lock provider class that manages locks for concurrent writes. Default is <code>org.apache.hudi.client.transaction.lock.ZookeeperBasedLockProvider</code></p>
<p>The <code>LAZY</code> mode cleans failed writes only after a heartbeat timeout when the cleaning service runs and is recommended when using multiple writers.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-use-occ-with-apache-hudi-and-apache-spark">How to use OCC with Apache Hudi and Apache Spark<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#how-to-use-occ-with-apache-hudi-and-apache-spark" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>This is a simple example where we configure OCC by setting the <code>hoodie.write.concurrency.mode</code> to <code>optimistic_concurrency_control</code>. We also specify a lock provider (in this case, Zookeeper) to manage concurrent access, along with essential table options like the precombine field, record key, and partition path.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> pyspark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">sql </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> SparkSession</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Initialize Spark session</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark </span><span class="token operator">=</span><span class="token plain"> SparkSession</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">builder \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">appName</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"Hudi Example with OCC"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">config</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"spark.serializer"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"org.apache.spark.serializer.KryoSerializer"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">getOrCreate</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Sample DataFrame</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">inputDF </span><span class="token operator">=</span><span class="token plain"> spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">createDataFrame</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"2024-11-19 10:00:00"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"A"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"partition1"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"2024-11-19 10:05:00"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"B"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"partition1"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"uuid"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"ts"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"value"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"partitionpath"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">tableName </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"my_hudi_table"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">basePath </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"s3://path-to-your-hudi-table"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Write DataFrame to Hudi with OCC and Zookeeper lock provider</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">inputDF</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">write</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token builtin" style="color:rgb(189, 147, 249)">format</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"hudi"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">option</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"hoodie.datasource.write.precombine.field"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"ts"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">option</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"hoodie.cleaner.policy.failed.writes"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"LAZY"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">option</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"hoodie.write.concurrency.mode"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"optimistic_concurrency_control"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">option</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"hoodie.write.lock.provider"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"org.apache.hudi.client.transaction.lock.ZookeeperBasedLockProvider"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">option</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"hoodie.write.lock.zookeeper.url"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"zk-cs.hudi-infra.svc.cluster.local"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">option</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"hoodie.write.lock.zookeeper.port"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"2181"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">option</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"hoodie.write.lock.zookeeper.base_path"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"/test"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">option</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"hoodie.datasource.write.recordkey.field"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"uuid"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">option</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"hoodie.datasource.write.partitionpath.field"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"partitionpath"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">option</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"hoodie.table.name"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> tableName</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">mode</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"overwrite"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">save</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">basePath</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">stop</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="apache-iceberg">Apache Iceberg<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#apache-iceberg" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>Apache Iceberg supports multiple concurrent writes through Optimistic Concurrency Control (OCC). The most important part to note here is that Iceberg needs a <em>catalog</em> component to adhere to the ACID guarantees. Each writer assumes it is the only one making changes, generating new table metadata for its operation. When a writer completes its updates, it attempts to commit the changes by performing an <em>atomic swap</em> of the latest <code>metadata.json</code> file in the catalog, replacing the existing metadata file with the new one.</p>
<p>If this atomic swap fails (due to another writer committing changes in the meantime), the writer‚Äôs commit is rejected. The writer then retries the entire process by creating a new metadata tree based on the latest state of the table and attempting the atomic swap again.</p>
<p>When it comes to table maintenance tasks, such as optimizations (e.g., compaction) or large delete jobs, Iceberg treats these as regular writes. These operations can overlap with ingestion jobs, but they follow the same OCC principles - conflicts are resolved by retrying based on the latest table state. Users are recommended to schedule such jobs during official maintenance periods to avoid contention, as frequent retries due to conflicts can impact performance.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="delta-lake">Delta Lake<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#delta-lake" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>Delta Lake provides concurrency control through Optimistic Concurrency Control (OCC) for transactional guarantees between writes. OCC allows multiple writers to attempt changes independently, assuming conflicts are infrequent. When a writer tries to commit, it checks for any conflicting updates from other transactions in the <a href="https://www.databricks.com/blog/2019/08/21/diving-into-delta-lake-unpacking-the-transaction-log.html" target="_blank" rel="noopener noreferrer">transaction log</a>. If a conflict is found, the transaction is rolled back, and the writer retries based on the latest version of the data.</p>
<p>Additionally, Delta Lake employs Multi-Version Concurrency Control (MVCC) within the file system to separate reads from writes. By keeping data objects and the transaction log immutable, MVCC allows readers to access a consistent snapshot of the data, even as new writes are added. This not only protects existing data from modification during concurrent transactions but also enables time-travel queries, allowing users to query historical snapshots.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#conclusion" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>Concurrency control is critical for Open lakehouse architectures, especially when your architecture has multiple concurrent pipelines interacting with the same table. Open table formats such as Apache Hudi bring well-established concurrency control methods from traditional database systems into the Lakehouse architecture to handle these operations while maintaining data consistency and scalability. Apache Hudi‚Äôs unique design to distinguish between writers, table services, and readers ensures snapshot isolation across all three processes. By supporting multiple concurrency control methods, such as OCC for managing writer conflicts, MVCC for isolating background table services and writers, and a novel NBCC for non-blocking, real-time ingestion, Hudi offers greater flexibility with complex workloads.</p>
<hr>]]></content:encoded>
            <category>multi-writer</category>
            <category>concurrency</category>
            <category>concurrency-control</category>
            <category>non-blocking concurrency-control</category>
            <category>Apache Hudi</category>
            <category>Apache Iceberg</category>
            <category>Delta Lake</category>
            <category>blog</category>
            <category>design</category>
        </item>
        <item>
            <title><![CDATA[Apache Hudi 1.0 Now Generally Available]]></title>
            <link>https://hudi.apache.org/cn/blog/2025/01/18/apache-hudi-1-0-now-generally-available</link>
            <guid>https://hudi.apache.org/cn/blog/2025/01/18/apache-hudi-1-0-now-generally-available</guid>
            <pubDate>Sat, 18 Jan 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://www.infoq.com/news/2025/01/apache-hudi/">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>apache hudi</category>
            <category>hudi 1.0.0</category>
            <category>infoq</category>
        </item>
        <item>
            <title><![CDATA[Out of the box Key Generators in Apache Hudi]]></title>
            <link>https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi</link>
            <guid>https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi</guid>
            <pubDate>Wed, 15 Jan 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Introduction]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi#introduction" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>The goal of Apache Hudi is to bring database-like features to data lakes. This addresses the main shortcoming of traditional data lakes: the inability to easily perform row-level updates or deletions.By integrating database-like management capabilities into data lakes, Hudi revolutionizes how it handles and processes large volumes of data, enabling out-of-the-box upserts and deletes that facilitate efficient record level updating and deletion.
One of Hudi's key innovations is the ability for users to explicitly define a Record Key, similar to a unique key in traditional databases, along with a Partition Key that aligns with the data lake paradigm. These two keys make the <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieKey.java" target="_blank" rel="noopener noreferrer">HoodieKey</a> that aligns with the data lake paradigm. These two keys make the HoodieKey which is similar to the primary key which uniquely defines each row. This enables hudi to do the upsert based on Hoodiekey. The upsert operation works by utilizing the HoodieKey to locate the exact file group where the data associated with that key resides.  When a new record is ingested into the Hudi table, the system first derives  the HoodieKey of the incoming record based on the unique key and partitioning schema configured. This key is used to determine which file group (a logical grouping of files) the record should be associated with which is usually achieved via an <a href="https://hudi.apache.org/docs/indexes" target="_blank" rel="noopener noreferrer">indexing</a> mechanism.
In this blog, we will explore the concept of Key Generators in Apache Hudi, how they enhance data management, and their role in enabling efficient data operations in modern data lakes.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="challenge">Challenge<a href="https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi#challenge" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>The biggest challenge in defining the record key and partition key on a table is  the columns in input data does not naturally lend itself to being used as a primary key or partition key directly. In the realm of databases, we often have below cases -</p>
<ul>
<li>Need to have multiple fields that serve as primary key commonly known as composite keys in the database.</li>
<li>It is necessary to preprocess the data to derive a specific field that can serve as a primary key before loading it into the database.</li>
<li>Sometimes we have to generate unique ids also. Common use case is surrogate key.</li>
</ul>
<p>Similarly, for partition columns also in datalakes, most of the time the raw field can‚Äôt be used as a partition key.</p>
<ul>
<li>Partition columns often have time grain like month level or year level partition but input data mostly contain timestamp and date.</li>
<li>Nested primary keys are very common, and necessitates multiple partition columns.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="approaches-to-handling-this-in-data-pipelines">Approaches to Handling this in Data Pipelines<a href="https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi#approaches-to-handling-this-in-data-pipelines" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>Data Lake and Lakehouse technologies typically address such scenarios by preprocessing the data. For example, if date-based partitioning is required and a timestamp column is available, the data must be processed using Spark SQL date functions to extract relevant components (e.g., year, month, day). These derived columns are then used for partitioning. However, this process can become cumbersome at scale, especially when multiple data streams are writing to the same Hudi table. The same extraction logic needs to be applied to all streams, and any table maintenance activities (such as bootstrapping or backfilling) also require this logic to be reapplied. This repetition is error-prone and can lead to data consistency issues if the logic is incorrectly applied.
Hudi addresses these challenges with a built-in solution: key generators. These can be configured at the table level, eliminating the need to repeatedly apply the same logic. With key generators, Hudi automatically handles the conversion process every time, ensuring consistency and reducing the risk of errors.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-key-generators-in-apache-hudi">What are Key Generators in Apache Hudi<a href="https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi#what-are-key-generators-in-apache-hudi" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p><a href="https://hudi.apache.org/docs/key_generation" target="_blank" rel="noopener noreferrer">Key generators</a> in Apache Hudi are essential components responsible for creating record keys and partition keys for records within a dataset. Hudi uses key generators to extract the Hudi record key, which is a combination of the record key and the partition key, from the incoming record fields. This process allows Hudi to efficiently prepare the hoodie key on which updates can occur. During upserts, Hudi identifies the file group that contains the specified hoodie key using an index and updates the corresponding file group accordingly.
Hudi offers several built-in key generator implementations that cover common use cases, such as generating record keys based on fields from the input data. However, to provide flexibility and support for more complex use cases, Hudi also offers a pluggable interface. This allows users to implement custom key generators tailored to their specific requirements.
To create a custom key generator, you can extend the <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/keygen/BaseKeyGenerator.java" target="_blank" rel="noopener noreferrer">BaseKeyGenerator</a> class which itself extends the <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/keygen/KeyGenerator.java" target="_blank" rel="noopener noreferrer">KeyGenerator</a>  class and implement methods such as getRecordKey and getPartitionKey. This enables you to define the specific logic required for calculating record and partition keys tailored to your dataset's requirements. Additionally, Hudi includes a variety of built-in key generators that address many common scenarios discussed in the previous section, streamlining the process of key generation for users.
The key generator is configured at the table level and stored in the hoodie.properties file, which resides within the .hoodie directory. This file contains all the table-level configurations, including the key generation settings. Once a table is created with a particular key generator we can‚Äôt change it. It can be set using the configuration hoodie.datasource.write.keygenerator.class</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="out-of-the-box-key-generators">Out of the Box Key Generators<a href="https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi#out-of-the-box-key-generators" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="simplekeygenerator">SimpleKeyGenerator<a href="https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi#simplekeygenerator" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>The SimpleKeyGenerator is a basic key generator used in Apache Hudi when direct fields from the input dataset can serve as both the record key and partition key. It maps a specific column in the DataFrame to the record key and another column to the partition path. This widely-used generator interprets values as-is from the DataFrame and converts them to strings, making it ideal for straightforward data structures.
Please note that this is the default key generator for the partitioned datasets.</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.recordkey.field": "id",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.partitionpath.field": "date",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.keygenerator.class": "org.apache.hudi.keygen.SimpleKeyGenerator"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="nonpartitionedkeygenerator">NonpartitionedKeyGenerator<a href="https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi#nonpartitionedkeygenerator" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>The NonpartitionedKeyGenerator is a key generator in Apache Hudi designed specifically for non-partitioned datasets. Unlike the SimpleKeyGenerator, which uses a field to determine the partition path for the data, the NonpartitionedKeyGenerator does not assign a partition key to the records. Instead, it returns an empty string as the partition key for all records. This is because the dataset is non-partitioned, meaning all records are stored in a single partition.</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.recordkey.field": "id",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.keygenerator.class": "org.apache.hudi.keygen.NonpartitionedKeyGenerator"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="complexkeygenerator">ComplexKeyGenerator<a href="https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi#complexkeygenerator" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>This key generator is used when multiple fields are used to create the record key or partition key. We can provide the comma separated list of the columns. In the output, the hoodie record key is generated using the format key1<!-- -->:value1<!-- -->,key2<!-- -->:value2<!-- -->. If any one of the partition key or record key contains multiple fields, then we have to use ComplexKeyGenerator.</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.keygenerator.class" : "org.apache.hudi.keygen.ComplexKeyGenerator",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.recordkey.field" = "key1,key2",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.partitionpath.field" = "country,state,city"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="timestampbasedkeygenerator">TimestampBasedKeygenerator<a href="https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi#timestampbasedkeygenerator" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>The TimestampBasedKeyGenerator allows you to generate partition keys based on timestamp fields in your data. This is especially useful when you want to partition your data by date, month, or year, depending on your use case. The key generator can transform timestamps into different formats, enabling you to create partitions that suit your analytical needs.</p>
<p>Relevant Configurations</p>
<ul>
<li>
<p><strong>hoodie.datasource.write.keygenerator.class</strong>
To use this key generator, The key gen class should be <code>org.apache.hudi.keygen.TimestampBasedKeyGenerator</code></p>
</li>
<li>
<p><strong>hoodie.deltastreamer.keygen.timebased.timestamp.type</strong>
This config determines the nature of the value of input. Below can be the possible values for this -
<strong>DATE_STRING</strong>: Use this when the input value is in string format.</p>
<ul>
<li>
<p>MIXED: This option allows for a combination of formats.</p>
</li>
<li>
<p>UNIX_TIMESTAMP: Select this when the input value is in epoch timestamp format (long type) measured in seconds.</p>
</li>
<li>
<p>EPOCHMILLISECONDS: Use this when the input value is in epoch timestamp format (long type) measured in milliseconds.</p>
</li>
<li>
<p>SCALAR: This option is for epoch timestamp values (long type) where you can specify any time unit.</p>
</li>
</ul>
</li>
<li>
<p><strong>hoodie.deltastreamer.keygen.timebased.timestamp.scalar.time.unit</strong>
When using the SCALAR timestamp type, you can define the unit of the epoch time. Valid options include NANOSECONDS, MICROSECONDS, MILLISECONDS, SECONDS, MINUTES, HOURS, DAYS</p>
</li>
<li>
<p><strong>hoodie.keygen.timebased.input.dateformat</strong>
When the timestamp type is DATE_STRING or MIXED, this config can be defined to specify the date format in which the field is coming in input.</p>
</li>
<li>
<p><strong>hoodie.keygen.timebased.output.dateformat</strong>
When the timestamp type is set to DATE_STRING or MIXED, this configuration defines the desired date format for the output field. It allows you to specify how the date should be formatted when it is generated or output.</p>
</li>
<li>
<p><strong>hoodie.deltastreamer.keygen.timebased.input.timezone</strong>
This setting specifies the timezone for the input date field derived from the raw data. The default value is UTC.</p>
</li>
<li>
<p><strong>hoodie.deltastreamer.keygen.timebased.output.timezone</strong>
This setting defines the timezone for the output date field that will be used to populate the partition column. The default value is UTC.</p>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="common-use-cases">Common Use Cases<a href="https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi#common-use-cases" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h4>
<ul>
<li>Data Contains Timestamp Field and We Want Date Level Partitions
In this scenario, you have a dataset with a timestamp field, and you want to partition the data by the date (i.e., year-month-day).</li>
</ul>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.keygenerator.class":     "org.apache.hudi.keygen.TimestampBasedKeyGenerator",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.deltastreamer.keygen.timebased.timestamp.type": "DATE_STRING",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.keygen.timebased.input.dateformat":"yyyy-MM-dd'T'HH:mm:ss.SSSSSSZ",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.keygen.timebased.output.dateformat":"yyyy-MM-dd",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.partitionpath.field": "event_time"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<ul>
<li>Data Contains Date Field but We Want to Have Month or Year Level Partitions
Here, you have a dataset with a date field, but you want to create partitions at a higher granularity, such as by month or year.</li>
</ul>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.keygenerator.class":     "org.apache.hudi.keygen.TimestampBasedKeyGenerator",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.deltastreamer.keygen.timebased.timestamp.type": "DATE_STRING",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.keygen.timebased.input.dateformat":"yyyy-MM-dd",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.keygen.timebased.output.dateformat":"yyyyMM",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.partitionpath.field": "event_date"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>In the example above, if we have an input with a date column named event_date in the format 'yyyy-MM-dd', the configurations will convert this format to a monthly level in the format 'yyyyMM' and use it as the partition column.</p>
<p>We can refer <a href="https://hudi.apache.org/docs/0.10.0/key_generation/#timestampbasedkeygenerator" target="_blank" rel="noopener noreferrer">TimestampBasedKeyGenerator</a> for more examples</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="customkeygenerator">CustomKeyGenerator<a href="https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi#customkeygenerator" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>In typical use cases, using the same key generator for both the record key and the partition key often does not meet the requirements. For such scenarios, a Custom Key Generator is particularly useful, as it allows for the use of different key generators for different fields.
A common use case arises when the partition key consists of multiple fields, and you also need to extract date or month-level partitions from a timestamp field. In these situations, it is essential to utilize both the TimestampBasedKeyGenerator and the ComplexKeyGenerator. However, since you cannot specify two different key generator classes simultaneously, the CustomKeyGenerator serves as an effective solution. We can configure it as list of comma separated fields with the key generator separated by colon. Example - key1<!-- -->:Timestamp<!-- -->,key2<!-- -->:SIMPLE<!-- -->,key3<!-- -->:SIMPLE<!-- -->
When we pass the partition column, we can also provide which key generator to use. The configurations below enable you to use SimpleKeyGenerator to extract the country field and TimestampBasedKeygenerator to transform the event_date field to use only month level partitions.</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.keygenerator.class":     "org.apache.hudi.keygen.TimestampBasedKeyGenerator",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.deltastreamer.keygen.timebased.timestamp.type": "DATE_STRING",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.keygen.timebased.input.dateformat":"yyyy-MM-dd",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.keygen.timebased.output.dateformat":"yyyyMM",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.partitionpath.field": "country:SIMPLE,event_date:TIMESTAMP"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi#conclusion" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>Key generators in Hudi are vital components that enable efficient record identification, partitioning, and data operations in large datasets. Whether you're performing upserts, deletes, or managing time-series data, choosing the right key generator ensures that Hudi can handle the data efficiently, while aligning with your business logic. By addressing challenges like composite keys, timestamp-based partitioning, and complex use cases, Apache Hudi revolutionizes how data lakes handle evolving data, providing database-like management capabilities that are scalable and flexible.</p>]]></content:encoded>
            <category>Data Lake</category>
            <category>Data Lakehouse</category>
            <category>Apache Hudi</category>
            <category>Key Generators</category>
            <category>partition</category>
        </item>
        <item>
            <title><![CDATA[Apache Iceberg vs Delta Lake vs Apache Hudi]]></title>
            <link>https://hudi.apache.org/cn/blog/2025/01/09/apache-iceberg-vs-delta-lake-vs-apache-hudi</link>
            <guid>https://hudi.apache.org/cn/blog/2025/01/09/apache-iceberg-vs-delta-lake-vs-apache-hudi</guid>
            <pubDate>Thu, 09 Jan 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://medium.com/@algodaysindia/apache-iceberg-vs-delta-lake-vs-apache-hudi-f987fee8dbe1">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>apache hudi</category>
            <category>apache iceberg</category>
            <category>delta lake</category>
            <category>comparison</category>
            <category>medium</category>
        </item>
        <item>
            <title><![CDATA[The Future of Data Lakehouses: A Fireside Chat with Vinoth Chandar - Founder CEO Onehouse & PMC Chair of Apache Hudi]]></title>
            <link>https://hudi.apache.org/cn/blog/2025/01/08/the-future-of-data-lakehouses-a-fireside</link>
            <guid>https://hudi.apache.org/cn/blog/2025/01/08/the-future-of-data-lakehouses-a-fireside</guid>
            <pubDate>Wed, 08 Jan 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://www.dataengineeringweekly.com/p/the-future-of-data-lakehouses-a-fireside">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>apache hudi</category>
            <category>data lakehouse</category>
            <category>lakehouse</category>
            <category>dataengineeringweekly</category>
        </item>
        <item>
            <title><![CDATA[How to Use the New Hudi Streamer with Hudi 1.0.0 on EMR Serverless 7.5.0 | Hands-on Labs]]></title>
            <link>https://hudi.apache.org/cn/blog/2025/01/05/how-use-new-hudi-streamer-100-emr-serverless-750-hands-on</link>
            <guid>https://hudi.apache.org/cn/blog/2025/01/05/how-use-new-hudi-streamer-100-emr-serverless-750-hands-on</guid>
            <pubDate>Sun, 05 Jan 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://www.linkedin.com/pulse/how-use-new-hudi-streamer-100-emr-serverless-750-hands-on-soumil-shah-fxrae/">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>how-to</category>
            <category>apache hudi</category>
            <category>hudi 1.0.0</category>
            <category>hudi streamer</category>
            <category>amazon emr</category>
            <category>linkedin</category>
        </item>
        <item>
            <title><![CDATA[Indexing in Apache Hudi]]></title>
            <link>https://hudi.apache.org/cn/blog/2024/12/31/indexing-in-apache-hudi</link>
            <guid>https://hudi.apache.org/cn/blog/2024/12/31/indexing-in-apache-hudi</guid>
            <pubDate>Tue, 31 Dec 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://medium.com/@sanjeets1900/indexing-in-apache-hudi-674f9481796e">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>apache hudi</category>
            <category>indexing</category>
            <category>medium</category>
        </item>
        <item>
            <title><![CDATA[The Architect‚Äôs Guide to Open Table Formats and Object Storage]]></title>
            <link>https://hudi.apache.org/cn/blog/2024/12/31/the-architects-guide-to-open-table-formats-and-object-storage</link>
            <guid>https://hudi.apache.org/cn/blog/2024/12/31/the-architects-guide-to-open-table-formats-and-object-storage</guid>
            <pubDate>Tue, 31 Dec 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://thenewstack.io/the-architects-guide-to-open-table-formats-and-object-storage/">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>apache hudi</category>
            <category>apache iceberg</category>
            <category>delta lake</category>
            <category>data lakehouse</category>
            <category>lakehouse</category>
            <category>table formats</category>
            <category>thenewstack</category>
        </item>
        <item>
            <title><![CDATA[Apache Hudi 2024: A Year In Review]]></title>
            <link>https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review</link>
            <guid>https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review</guid>
            <pubDate>Sun, 29 Dec 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[As we wrap up another remarkable year for Apache Hudi, I am thrilled to reflect on the tremendous achievements and milestones that have defined 2024. This year has been particularly special as we achieved several significant milestones, including the landmark release of Hudi 1.0, the publication of comprehensive books, and the introduction of new tools that expand Hudi's ecosystem.]]></description>
            <content:encoded><![CDATA[<img src="https://hudi.apache.org/assets/images/blog/2024-12-29-a-year-in-review-2024/cover.jpg" alt="drawing" style="width:80%;display:block;margin-left:auto;margin-right:auto;margin-top:18pt;margin-bottom:18pt">
<p>As we wrap up another remarkable year for Apache Hudi, I am thrilled to reflect on the tremendous achievements and milestones that have defined 2024. This year has been particularly special as we achieved several significant milestones, including the landmark release of Hudi 1.0, the publication of comprehensive books, and the introduction of new tools that expand Hudi's ecosystem.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="community-growth-and-engagement">Community Growth and Engagement<a href="https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review#community-growth-and-engagement" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>The Apache Hudi community continued its impressive growth trajectory in 2024. The number of new PRs has remained stable, indicating a consistent level of development activities:</p>
<img src="https://hudi.apache.org/assets/images/blog/2024-12-29-a-year-in-review-2024/pr-history.svg" alt="drawing" style="width:80%;display:block;margin-left:auto;margin-right:auto;margin-top:18pt;margin-bottom:18pt">
<p>Our community presence expanded significantly across various platforms:</p>
<ul>
<li>The community grew to over 10,500 followers on LinkedIn</li>
<li>Added 8,755 new followers in the last 365 days</li>
<li>Generated 441,402 content impressions</li>
<li>Received 6,555 reactions and 493 comments across platforms</li>
<li>Our Slack community remained vibrant with rich technical discussions and knowledge sharing</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="major-milestones">Major Milestones<a href="https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review#major-milestones" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="apache-hudi-10-release">Apache Hudi 1.0 Release<a href="https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review#apache-hudi-10-release" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>2024 marked a historic moment with the <a href="https://hudi.apache.org/releases/release-1.0.0" target="_blank" rel="noopener noreferrer">release of Apache Hudi 1.0</a>, representing a major evolution in data lakehouse technology. This release brought several groundbreaking features:</p>
<ul>
<li><strong>Secondary Indexing</strong>: First of its kind in lakehouses, enabling database-like query acceleration with demonstrated 95% latency reduction on 10TB TPC-DS for low-moderate selectivity queries</li>
<li><strong>Logical Partitioning via Expression Indexes</strong>: Introducing PostgreSQL-style expression indexes for more efficient partition management</li>
<li><strong>Partial Updates</strong>: Achieving 2.6x performance improvement and 85% reduction in bytes written for update-heavy workloads</li>
<li><strong>Non-blocking Concurrency Control (NBCC)</strong>: An industry-first feature allowing simultaneous writing from multiple writers</li>
<li><strong>Merge Modes</strong>: First-class support for both <code>commit_time_ordering</code> and <code>event_time_ordering</code></li>
<li><strong>LSM Timeline</strong>: Revamped timeline storage as a scalable LSM tree for extended table history retention</li>
<li><strong>TrueTime</strong>: Strengthened time semantics ensuring forward-moving clocks in distributed processes</li>
</ul>
<p>Please check out the <a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0">announcement blog</a>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="launch-of-hudi-rs">Launch of Hudi-rs<a href="https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review#launch-of-hudi-rs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>A significant expansion of the Hudi ecosystem occurred with the <a href="https://github.com/apache/hudi-rs" target="_blank" rel="noopener noreferrer">release of Hudi-rs</a>, the native Rust implementation for Apache Hudi with Python API bindings. This new project enables:</p>
<ul>
<li>Reading Hudi Tables without Spark or JVM dependencies</li>
<li>Integration with Apache Arrow for enhanced compatibility</li>
<li>Support for Copy-on-Write (CoW) table snapshots and time-travel reads</li>
<li>Cloud storage support across AWS, Azure, and GCP</li>
<li>Native integration with Apache DataFusion, Ray, Daft, etc</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="published-books-and-educational-content">Published Books and Educational Content<a href="https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review#published-books-and-educational-content" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>2024 saw the release of two comprehensive guides to Apache Hudi:</p>
<ul>
<li><a href="https://learning.oreilly.com/library/view/apache-hudi-the/9781098173821/" target="_blank" rel="noopener noreferrer"><strong>"Apache Hudi: The Definitive Guide"</strong></a> (O'Reilly) - Released in early access, <a href="https://www.onehouse.ai/whitepaper/apache-hudi-the-definitive-guide" target="_blank" rel="noopener noreferrer">free copy available</a>, providing comprehensive coverage of:<!-- -->
<ul>
<li>Distributed query engines</li>
<li>Snapshot and time travel queries</li>
<li>Incremental queries</li>
<li>Change-data-capture modes</li>
<li>End-to-end ingestion with Hudi Streamer</li>
</ul>
</li>
</ul>
<img src="https://hudi.apache.org/assets/images/blog/2024-12-29-a-year-in-review-2024/hudi-tdg.jpg" alt="drawing" style="width:80%;display:block;margin-left:auto;margin-right:auto;margin-top:18pt;margin-bottom:18pt">
<ul>
<li><a href="https://blog.datumagic.com/p/apache-hudi-from-zero-to-one-110" target="_blank" rel="noopener noreferrer"><strong>"Apache Hudi: From Zero to One"</strong></a> - A 10-part blog series turned into <a href="https://www.onehouse.ai/whitepaper/ebook-apache-hudi---zero-to-one" target="_blank" rel="noopener noreferrer">an ebook</a>, offering deep technical insights into Hudi's architecture and capabilities, covering:<!-- -->
<ul>
<li>Storage format and operations</li>
<li>Read and write flows</li>
<li>Table services and indexing</li>
<li>Incremental processing</li>
<li>Hudi 1.0 features</li>
</ul>
</li>
</ul>
<img src="https://hudi.apache.org/assets/images/blog/2024-12-29-a-year-in-review-2024/hudi0to1.png" alt="drawing" style="width:80%;display:block;margin-left:auto;margin-right:auto;margin-top:18pt;margin-bottom:18pt">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="community-events-and-sharing">Community Events and Sharing<a href="https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review#community-events-and-sharing" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>The Apache Hudi community maintained a strong presence at major industry events throughout 2024:</p>
<img src="https://hudi.apache.org/assets/images/blog/2024-12-29-a-year-in-review-2024/community-events.png" alt="drawing" style="width:80%;display:block;margin-left:auto;margin-right:auto;margin-top:18pt;margin-bottom:18pt">
<ul>
<li>Databricks' Data+AI Summit - Presenting Apache Hudi's role in the lakehouse ecosystem and its interoperability with other table formats through XTable, an open-source project enabling seamless conversion between Hudi, Delta Lake, and Iceberg</li>
<li>Confluent's Current 2024 - Demonstrating Hudi's powerful CDC capabilities with Apache Flink, showcasing real-time data pipelines and the innovative Non-Blocking Concurrency Control (NBCC) for high-volume streaming workloads</li>
<li>Trino Fest 2024 - Showcasing Hudi connector's evolution and innovations in Trino, including multi-modal indexing capabilities and the roadmap for enhanced query performance through Alluxio-powered caching and expanded DDL/DML support</li>
<li>Bangalore Lakehouse Days - Deep dive into Apache Hudi 1.0's groundbreaking features including LSM-based timeline, functional indexes, and non-blocking concurrency control, demonstrating Hudi's continued innovation in the lakehouse space</li>
</ul>
<p>Additionally, the community launched several new initiatives to foster learning and knowledge sharing:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="lakehouse-chronicles-with-apache-hudi"><a href="https://www.youtube.com/playlist?list=PLxSSOLH2WRMNQetyPU98B2dHnYv91R6Y8" target="_blank" rel="noopener noreferrer">Lakehouse Chronicles with Apache Hudi</a><a href="https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review#lakehouse-chronicles-with-apache-hudi" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>A new community series with 4 episodes released.</p>
<img src="https://hudi.apache.org/assets/images/blog/2024-12-29-a-year-in-review-2024/lakehouse-chronicles.png" alt="drawing" style="width:80%;display:block;margin-left:auto;margin-right:auto;margin-top:18pt;margin-bottom:18pt">
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="hudi-newsletter"><a href="https://hudinewsletter.substack.com/" target="_blank" rel="noopener noreferrer">Hudi Newsletter</a><a href="https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review#hudi-newsletter" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>9 editions published, keeping the community informed about latest developments.</p>
<img src="https://hudi.apache.org/assets/images/blog/2024-12-29-a-year-in-review-2024/newsletter.png" alt="drawing" style="width:80%;display:block;margin-left:auto;margin-right:auto;margin-top:18pt;margin-bottom:18pt">
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="community-syncs"><a href="https://www.youtube.com/@apachehudi" target="_blank" rel="noopener noreferrer">Community Syncs</a><a href="https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review#community-syncs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>Featured 8 user stories from major organizations including Amazon, Peloton, Shopee and Uber.</p>
<img src="https://hudi.apache.org/assets/images/blog/2024-12-29-a-year-in-review-2024/community-syncs.png" alt="drawing" style="width:80%;display:block;margin-left:auto;margin-right:auto;margin-top:18pt;margin-bottom:18pt">
<ul>
<li><a href="https://www.youtube.com/watch?v=rMXhlb7Uci8" target="_blank" rel="noopener noreferrer">Powering Amazon Unit Economics with Configurations and Hudi</a></li>
<li><a href="https://www.youtube.com/watch?v=-Pyid5K9dyU" target="_blank" rel="noopener noreferrer">Modernizing Data Infrastructure at Peleton using Apache Hudi</a></li>
<li><a href="https://www.youtube.com/watch?v=fqhr-4jXi6I" target="_blank" rel="noopener noreferrer">Innovative Solution for Real-time Analytics at Scale using Apache Hudi (Shopee)</a></li>
<li><a href="https://www.youtube.com/watch?v=VpdimpH_nsI" target="_blank" rel="noopener noreferrer">Scaling Complex Data Workflows using Apache Hudi (Uber)</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="notable-user-stories-and-technical-content">Notable User Stories and Technical Content<a href="https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review#notable-user-stories-and-technical-content" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>Throughout 2024, several organizations shared their Hudi implementation experiences:</p>
<ul>
<li><a href="https://www.notion.com/blog/building-and-scaling-notions-data-lake" target="_blank" rel="noopener noreferrer">Notion's transition from Snowflake to Hudi</a></li>
<li><a href="https://engineering.grab.com/enabling-near-realtime-data-analytics" target="_blank" rel="noopener noreferrer">Grab's implementation of near-realtime data analytics</a></li>
<li><a href="https://aws.amazon.com/blogs/big-data/use-aws-data-exchange-to-seamlessly-share-apache-hudi-datasets/" target="_blank" rel="noopener noreferrer">AWS's data sharing capabilities with AWS Data Exchange</a></li>
<li><a href="https://www.y.uno/post/how-apache-hudi-transformed-yunos-data-lake" target="_blank" rel="noopener noreferrer">Yuno's data lake transformation</a></li>
<li><a href="https://blogs.halodoc.io/data-lake-cost-optimisation-strategies/" target="_blank" rel="noopener noreferrer">Halodoc's cost optimization strategies</a></li>
<li><a href="https://medium.com/upstox-engineering/navigating-the-future-the-evolutionary-journey-of-upstoxs-data-platform-92dc10ff22ae" target="_blank" rel="noopener noreferrer">Upstox's data platform evolution</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="looking-ahead-to-2025">Looking Ahead to 2025<a href="https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review#looking-ahead-to-2025" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>As we look forward to 2025, Apache Hudi's roadmap includes several exciting developments:</p>
<ul>
<li>Enhanced core engine with modernized write paths and advanced indexing (bitmap, vector search)</li>
<li>Multi-modal data support with improved storage engine APIs and cross-format interoperability</li>
<li>Enterprise-grade features including multi-table transactions and advanced caching</li>
<li>Robust platform services with Data Lakehouse Management System (DLMS) components</li>
<li>Broader adoption of Hudi-rs across the ecosystem</li>
<li>Continued focus on stability and seamless migration path for the community</li>
</ul>
<p>These initiatives reflect our commitment to advancing data lakehouse technology while ensuring reliability and user experience.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="get-involved">Get Involved<a href="https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review#get-involved" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>Join our thriving community:</p>
<ul>
<li>Contribute to the project on GitHub: <a href="https://github.com/apache/hudi" target="_blank" rel="noopener noreferrer">Hudi</a> &amp; <a href="https://github.com/apache/hudi-rs" target="_blank" rel="noopener noreferrer">Hudi-rs</a></li>
<li>Join our <a href="https://apache-hudi.slack.com/join/shared_invite/zt-2ggm1fub8-_yt4Reu9djwqqVRFC7X49g" target="_blank" rel="noopener noreferrer">Slack community</a></li>
<li>Follow us on <a href="https://www.linkedin.com/company/apache-hudi/" target="_blank" rel="noopener noreferrer">LinkedIn</a> and <a href="https://x.com/apachehudi" target="_blank" rel="noopener noreferrer">X (Twitter)</a></li>
<li>Subscribe to our <a href="https://www.youtube.com/@apachehudi" target="_blank" rel="noopener noreferrer">YouTube channel</a></li>
<li>Participate in our <a href="https://hudi.apache.org/community/syncs" target="_blank" rel="noopener noreferrer">community syncs</a> and <a href="https://hudi.apache.org/community/office_hours" target="_blank" rel="noopener noreferrer">office hours</a>.</li>
<li>Subscribe to the dev mailing list by sending an empty email to <code>dev-subscribe@hudi.apache.org</code></li>
</ul>
<p>The success of Apache Hudi in 2024 wouldn't have been possible without our dedicated community of contributors, users, and supporters. As we celebrate these achievements, we look forward to another year of innovation and growth in 2025.</p>]]></content:encoded>
            <category>apache hudi</category>
            <category>community</category>
        </item>
        <item>
            <title><![CDATA[How lakehouse handles concurrent Read and Writes]]></title>
            <link>https://hudi.apache.org/cn/blog/2024/12/28/how-lakehouse-handles-concurrent-read-and-writes</link>
            <guid>https://hudi.apache.org/cn/blog/2024/12/28/how-lakehouse-handles-concurrent-read-and-writes</guid>
            <pubDate>Sat, 28 Dec 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://medium.com/@sanjeets1900/how-lakehouse-handles-concurrent-read-and-writes-b4423fecfe81">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>apache hudi</category>
            <category>concurrency</category>
            <category>concurrency-control</category>
            <category>non-blocking concurrency-control</category>
            <category>nbcc</category>
            <category>medium</category>
        </item>
        <item>
            <title><![CDATA[Announcing Apache Hudi 1.0 and the Next Generation of Data Lakehouses]]></title>
            <link>https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0</link>
            <guid>https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0</guid>
            <pubDate>Mon, 16 Dec 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Overview]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview">Overview<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#overview" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>We are thrilled to announce the release of Apache Hudi 1.0, a landmark achievement for our vibrant community that defines what the next generation of data lakehouses should achieve. Hudi pioneered <em><strong>transactional data lakes</strong></em> in 2017, and today, we live in a world where this technology category is mainstream as the ‚Äú<em><strong>Data Lakehouse‚Äù</strong></em>. The Hudi community has made several key, original, and first-of-its-kind contributions to this category, as shown below, compared to when other OSS alternatives emerged. This is an incredibly rare feat for a relatively small OSS community to sustain in a fiercely competitive commercial data ecosystem. On the other hand, it also demonstrates the value of deeply understanding the technology category within a focused open-source community. So, I first want to thank/congratulate the Hudi community and the <strong>60+ contributors</strong> for making 1.0 happen.</p>
<div style="text-align:center"><img src="https://hudi.apache.org/assets/images/blog/hudi-innovation-timeline.jpg" alt="innovation timeline"></div>
<p>This <a href="https://hudi.apache.org/cn/releases/release-1.0.0">release</a> is more than just a version increment‚Äîit advances the breadth of Hudi‚Äôs feature set and its architecture's robustness while bringing fresh innovation to shape the future. This post reflects on how technology and the surrounding ecosystem have evolved, making a case for a holistic ‚Äú<em><strong>Data Lakehouse Management System</strong></em>‚Äù (<em><strong>DLMS</strong></em>) as the new Northstar. For most of this post, we will deep dive into the latest capabilities of Hudi 1.0 that make this evolution possible.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="evolution-of-the-data-lakehouse">Evolution of the Data Lakehouse<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#evolution-of-the-data-lakehouse" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>Technologies must constantly evolve‚Äî<a href="https://en.wikipedia.org/wiki/Web3" target="_blank" rel="noopener noreferrer">Web 3.0</a>, <a href="https://en.wikipedia.org/wiki/List_of_wireless_network_technologies" target="_blank" rel="noopener noreferrer">cellular tech</a>, <a href="https://en.wikipedia.org/wiki/Programming_language_generations" target="_blank" rel="noopener noreferrer">programming language generations</a>‚Äîbased on emerging needs. Data lakehouses are no exception. This section explores the hierarchy of such needs for data lakehouse users. The most basic need is the ‚Äú<strong>table format</strong>‚Äù functionality, the foundation for data lakehouses. Table format organizes the collection of files/objects into tables with snapshots, schema, and statistics tracking, enabling higher abstraction. Furthermore, table format dictates the organization of files within each snapshot, encoding deletes/updates and metadata about how the table changes over time. Table format also provides protocols for various readers and writers and table management processes to handle concurrent access and provide ACID transactions safely. In the last five years, leading data warehouse and cloud vendors have integrated their proprietary SQL warehouse stack with open table formats. While they mostly default to their closed table formats and the compute engines remain closed, this welcome move provides users an open alternative for their data.</p>
<p>However, the benefits of a format end there, and now a table format is just the tip of the iceberg. Users require an <a href="https://www.onehouse.ai/blog/open-table-formats-and-the-open-data-lakehouse-in-perspective" target="_blank" rel="noopener noreferrer">end-to-end open data lakehouse</a>, and modern data lakehouse features need a sophisticated layer of <em><strong>open-source software</strong></em> operating on data stored in open table formats. For example, Optimized writers can balance cost and performance by carefully managing file sizes using the statistics maintained in the table format or catalog syncing service that can make data in Hudi readily available to half a dozen catalogs open and closed out there. Hudi shines by providing a high-performance open table format as well as a comprehensive open-source software stack that can ingest, store, optimize and effectively self-manage a data lakehouse. This distinction between open formats and open software is often lost in translation inside the large vendor ecosystem in which Hudi operates. Still, it has been and remains a key consideration for Hudi‚Äôs <a href="https://hudi.apache.org/cn/powered-by">users</a> to avoid compute-lockin to any given data vendor. The Hudi streamer tool, e.g., powers hundreds of data lakes by ingesting data seamlessly from various sources at the convenience of a single command in a terminal.</p>
<div style="text-align:center;width:90%;height:auto"><img src="https://hudi.apache.org/assets/images/blog/dlms-hierarchy.png" alt="dlms hierarchy"></div>
<p>Moving forward with 1.0, the community has <a href="https://github.com/apache/hudi/pull/8679" target="_blank" rel="noopener noreferrer">debated</a> these key points and concluded that we need more open-source ‚Äú<strong>software capabilities</strong>‚Äù that are directly comparable with DBMSes for two main reasons.</p>
<p><strong>Significantly expand the technical capabilities of a data lakehouse</strong>: Many design decisions in Hudi have been inspired by databases (see <a href="https://github.com/apache/hudi/blob/master/rfc/rfc-69/rfc-69.md#hudi-1x" target="_blank" rel="noopener noreferrer">here</a> for a layer-by-layer mapping) and have delivered significant benefits to the community. For example, Hudi‚Äôs indexing mechanisms deliver the fast update performance the project has come to be known for.  We want to generalize such features across writers and queries and introduce new capabilities like fast metastores for query planning, support for unstructured/multimodal data and caching mechanisms that can be deeply integrated into (at least) open-source query engines in the ecosystem. We also need concurrency control that works for lakehouse workloads instead of employing techniques applicable to OLTP databases at the surface level.</p>
<p><strong>We also need a database-like experience</strong>: We originally designed Hudi as a software library that can be embedded into different query/processing engines for reading/writing/managing tables. This model has been a great success within the existing data ecosystem, which is familiar with scheduling jobs and employing multiple engines for ETL and interactive queries. However, for a new user wanting to explore data lakehouses, there is no piece of software to easily install and explore all functionality packaged coherently. Such data lakehouse functionality packaged and delivered like a typical database system unlocks new use cases. For example, with such a system, we could bring HTAP capabilities to the data lakehouses on faster cloud storage/row-oriented formats, finally making it a low-latency data serving layer.</p>
<p>If combined, we would gain a powerful database built on top of the data lake(house) architecture‚Äîa <em><strong>data</strong></em> <em><strong>lakehouse</strong></em> <em><strong>management</strong></em> <em><strong>system</strong></em> <em><strong>(DLMS)</strong></em>‚Äîthat we believe the industry needs.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-features-in-hudi-10">Key Features in Hudi 1.0<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#key-features-in-hudi-10" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>In Hudi 1.0, we‚Äôve delivered a significant expansion of data lakehouse technical capabilities discussed above inside Hudi‚Äôs <a href="https://en.wikipedia.org/wiki/Database_engine" target="_blank" rel="noopener noreferrer">storage engine</a> layer.  Storage engines (a.k.a database engines) are standard database components that sit on top of the storage/file/table format and are wrapped by the DBMS layer above, handling the core read/write/management functionality. In the figure below, we map the Hudi components with the seminal <a href="https://dsf.berkeley.edu/papers/fntdb07-architecture.pdf" target="_blank" rel="noopener noreferrer">Architecture of a Database System</a> paper (see page 4) to illustrate the standard layering discussed. If the layering is implemented correctly, we can deliver the benefits of the storage engine to even other table formats, which may lack such fully-developed open-source software for table management or achieving high performance, via interop standards defined in projects like <a href="https://xtable.apache.org/" target="_blank" rel="noopener noreferrer">Apache XTable (Incubating)</a>.</p>
<div style="text-align:center;width:80%;height:auto"><img src="https://hudi.apache.org/assets/images/hudi-stack-1-x.png" alt="Hudi DB Architecture"><p align="center">Figure: Apache Hudi Database Architecture</p></div>
<p>Regarding full-fledged DLMS functionality, the closest experience Hudi 1.0 offers is through Apache Spark. Users can deploy a Spark server (or Spark Connect) with Hudi 1.0 installed, submit SQL/jobs, orchestrate table services via SQL commands, and enjoy new secondary index functionality to speed up queries like a DBMS. Subsequent releases in the 1.x release line and beyond will continuously add new features and improve this experience.</p>
<p>In the following sections, let‚Äôs dive into what makes Hudi 1.0 a standout release.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="new-time-and-timeline">New Time and Timeline<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#new-time-and-timeline" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>For the familiar user, time is a key concept in Hudi. Hudi‚Äôs original notion of time was instantaneous, i.e., actions that modify the table appear to take effect at a given instant. This was limiting when designing features like non-blocking concurrency control across writers, which needs to reason about actions more as an ‚Äúinterval‚Äù to detect other conflicting actions. Every action on the Hudi timeline now gets a <em>requested</em> and a <em>completion</em> time; Thus, the timeline layout version has bumped up in the 1.0 release. Furthermore, to ease the understanding and bring consistency around time generation for users and implementors, we have formalized the adoption of <a href="https://hudi.apache.org/cn/docs/timeline#truetime-generation">TrueTime</a> semantics. The default implementation assures forward-moving clocks even with distributed processes, assuming a maximum tolerable clock skew similar to <a href="https://cockroachlabs.com/blog/living-without-atomic-clocks/" target="_blank" rel="noopener noreferrer">OLTP/NoSQL</a> stores adopting TrueTime.</p>
<div style="text-align:center"><img src="https://hudi.apache.org/assets/images/hudi-timeline-actions.png" alt="Timeline actions"><p align="center">Figure: Showing actions in Hudi 1.0 modeled as an interval of two instants: requested and completed</p></div>
<p>Hudi tables are frequently updated, and users also want to retain a more extended action history associated with the table. Before Hudi 1.0, the older action history in a table was archived for audit access. But, due to the lack of support for cloud storage appends, access might become cumbersome due to tons of small files. In Hudi 1.0, we have redesigned the timeline as an <a href="https://en.wikipedia.org/wiki/Log-structured_merge-tree" target="_blank" rel="noopener noreferrer">LSM tree</a>, which is widely adopted for cases where good write performance on temporal data is desired.</p>
<p>In the Hudi 1.0 release, the <a href="https://hudi.apache.org/cn/docs/timeline#lsm-timeline-history">LSM timeline</a> is heavily used in the query planning to map requested and completion times across Apache Spark, Apache Flink and Apache Hive. Future releases plan to leverage this to unify the timeline's active and history components, providing infinite retention of table history. Micro benchmarks show that the LSM timeline can be pretty efficient, even committing every <em><strong>30 seconds for 10 years with about 10M instants</strong></em>, further cementing Hudi‚Äôs table format as the most suited for frequently written tables.</p>
<table><thead><tr><th style="text-align:left">Number of actions</th><th style="text-align:left">Instant Batch Size</th><th style="text-align:left">Read cost (just times)</th><th style="text-align:left">Read cost (along with action metadata)</th><th style="text-align:left">Total file size</th></tr></thead><tbody><tr><td style="text-align:left">10000</td><td style="text-align:left">10</td><td style="text-align:left">32ms</td><td style="text-align:left">150ms</td><td style="text-align:left">8.39MB</td></tr><tr><td style="text-align:left">20000</td><td style="text-align:left">10</td><td style="text-align:left">51ms</td><td style="text-align:left">188ms</td><td style="text-align:left">16.8MB</td></tr><tr><td style="text-align:left">10000000</td><td style="text-align:left">1000</td><td style="text-align:left">3400ms</td><td style="text-align:left">162s</td><td style="text-align:left">8.4GB</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="secondary-indexing-for-faster-lookups">Secondary Indexing for Faster Lookups<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#secondary-indexing-for-faster-lookups" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>Indexes are core to Hudi‚Äôs design, so much so that even the first pre-open-source version of Hudi shipped with <a href="https://hudi.apache.org/cn/docs/indexes#additional-writer-side-indexes">indexes</a> to speed up writes. However, these indexes were limited to the writer's side, except for record indexes in 0.14+ above, which were also integrated with Spark SQL queries. Hudi 1.0 generalizes indexes closer to the indexing functionality found in relational databases, supporting indexes on any secondary column across both writer and readers. Hudi 1.0 also supports near-standard <a href="https://hudi.apache.org/cn/docs/sql_ddl#create-index">SQL syntax</a> for creating/dropping indexes on different columns via Spark SQL, along with an asynchronous indexing table service to build indexes without interrupting the writers.</p>
<div style="text-align:center;padding-left:10%;width:70%;height:auto"><img src="https://hudi.apache.org/assets/images/hudi-stack-indexes.png" alt="Indexes"><p align="center">Figure: the indexing subsystem in Hudi 1.0, showing different types of indexes</p></div>
<p>With secondary indexes, queries and DMLs scan a much-reduced amount of files from cloud storage, dramatically reducing costs (e.g., on engines like AWS Athena, which price by data scanned) and improving query performance for queries with low to even moderate amount of selectivity. On a benchmark of a query on <em>web_sales</em> table (from <em><strong>10 TB tpc-ds dataset</strong></em>), with file groups - 286,603, total records - 7,198,162,544 and cardinality of secondary index column in the ~ 1:150 ranges, we see a remarkable <em><strong>~95% decrease in latency</strong></em>.</p>
<table><thead><tr><th style="text-align:left">Run 1</th><th style="text-align:left">Total Query Latency w/o indexing skipping (secs)</th><th style="text-align:left">Total Query Latency with secondary index skipping (secs)</th><th style="text-align:left">% decrease</th></tr></thead><tbody><tr><td style="text-align:left">1</td><td style="text-align:left">252</td><td style="text-align:left">31</td><td style="text-align:left">~88%</td></tr><tr><td style="text-align:left">2</td><td style="text-align:left">214</td><td style="text-align:left">10</td><td style="text-align:left">~95%</td></tr><tr><td style="text-align:left">3</td><td style="text-align:left">204</td><td style="text-align:left">9</td><td style="text-align:left">~95%</td></tr></tbody></table>
<p>In Hudi 1.0, secondary indexes are only supported for Apache Spark, with planned support for other engines in Hudi 1.1, starting with Flink, Presto and Trino.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bloom-filter-indexes">Bloom Filter indexes<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#bloom-filter-indexes" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>Bloom filter indexes have existed on the Hudi writers for a long time. It is one of the most performant and versatile indexes users prefer for ‚Äúneedle-in-a-haystack‚Äù deletes/updates or de-duplication. The index works by storing special footers in base files around min/max key ranges and a dynamic bloom filter that adapts to the file size and can automatically handle partitioning/skew on the writer's path. Hudi 1.0 introduces a newer kind of bloom filter index for Spark SQL while retaining the writer-side index as-is. The new index stores bloom filters in the Hudi metadata table and other secondary/record indexes for scalable access, even for huge tables, since the index is stored in fewer files compared to being stored alongside data files. It can be created using standard <a href="https://hudi.apache.org/cn/docs/sql_ddl#create-bloom-filter-index">SQL syntax</a>, as shown below. Subsequent queries on the indexed columns will use the bloom filters to speed up queries.</p>
<div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)">-- Create a bloom filter index on the driver column of the table `hudi_table`</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INDEX</span><span class="token plain"> idx_bloom_driver </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">ON</span><span class="token plain"> hudi_indexed_table </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">USING</span><span class="token plain"> bloom_filters</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">driver</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">-- Create a bloom filter index on the column derived from expression `lower(rider)` of the table `hudi_table`</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INDEX</span><span class="token plain"> idx_bloom_rider </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">ON</span><span class="token plain"> hudi_indexed_table </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">USING</span><span class="token plain"> bloom_filters</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">rider</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> OPTIONS</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">expr</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">'lower'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>In future releases of Hudi, we aim to fully integrate the benefits of the older writer-side index into the new bloom index. Nonetheless, this demonstrates the adaptability of Hudi‚Äôs indexing system to handle different types of indexes on the table.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="partitioning-replaced-by-expression-indexes">Partitioning replaced by Expression Indexes<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#partitioning-replaced-by-expression-indexes" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>An astute reader may have noticed above that the indexing is supported on a function/expression on a column. Hudi 1.0 introduces expression indexes similar to <a href="https://www.postgresql.org/docs/current/indexes-expressional.html" target="_blank" rel="noopener noreferrer">Postgres</a> to generalize a two-decade-old relic in the data lake ecosystem - partitioning! At a high level, partitioning on the data lake divides the table into folders based on a column or a mapping function (partitioning function). When queries or operations are performed against the table, they can efficiently skip entire partitions (folders), reducing the amount of metadata and data involved. This is very effective since data lake tables span 100s of thousands of files. But, as simple as it sounds, this is one of the <a href="https://www.onehouse.ai/blog/knowing-your-data-partitioning-vices-on-the-data-lakehouse" target="_blank" rel="noopener noreferrer">most common pitfalls</a> around performance on the data lake, where new users use it like an index by partitioning based on a high cardinality column, resulting in lots of storage partitions/tiny files and abysmal write/query performance for no good reason. Further, tying storage organization to partitioning makes it inflexible to changes.</p>
<div style="text-align:center"><img src="https://hudi.apache.org/assets/images/expression-index-date-partitioning.png" alt="Timeline actions"><p align="center">Figure: Shows index on a date expression when a different column physically partitions data</p></div>
<p>Hudi 1.0 treats partitions as a <a href="https://hudi.apache.org/cn/docs/sql_queries#query-using-column-stats-expression-index">coarse-grained index</a> on a column value or an expression of a column, as they should have been. To support the efficiency of skipping entire storage paths/folders, Hudi 1.0 introduces partition stats indexes that aggregate these statistics on the storage partition path level, in addition to doing so at the file level. Now, users can create different types of indexes on columns to achieve the effects of partitioning in a streamlined fashion using fewer concepts to achieve the same results. Along with support for other 1.x features, partition stats and expression indexes support will be extended to other engines like Presto, Trino, Apache Doris, and Starrocks with the 1.1 release.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="efficient-partial-updates">Efficient Partial Updates<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#efficient-partial-updates" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>Managing large-scale datasets often involves making fine-grained changes to records. Hudi has long supported <a href="https://hudi.apache.org/cn/docs/0.15.0/record_payload#partialupdateavropayload">partial updates</a> to records via the record payload interface. However, this usually comes at the cost of sacrificing engine-native performance by moving away from specific objects used by engines to represent rows. As users have embraced Hudi for incremental SQL pipelines on top of dbt/Spark or Flink Dynamic Tables, there was a rise in interest in making this much more straightforward and mainstream. Hudi 1.0 introduces first-class support for <strong>partial updates</strong> at the log format level, enabling <em>MERGE INTO</em> SQL statements to modify only the changed fields of a record instead of rewriting/reprocessing the entire row.</p>
<p>Partial updates improve query and write performance simultaneously by reducing write amplification for writes and the amount of data read by Merge-on-Read snapshot queries. It also achieves much better storage utilization due to fewer bytes stored and improved compute efficiency over existing partial update support by retaining vectorized engine-native processing. Using the 1TB Brooklyn benchmark for write performance, we observe about <strong>2.6x</strong> improvement in Merge-on-Read query performance due to an <strong>85%</strong> reduction in write amplification. For random write workloads, the gains can be much more pronounced. Below shows a second benchmark for partial updates, 1TB MOR table, 1000 partitions, 80% random updates. 3/100 columns randomly updated.</p>
<table><thead><tr><th style="text-align:left"></th><th style="text-align:left">Full Record Update</th><th style="text-align:left">Partial Update</th><th style="text-align:left">Gains</th></tr></thead><tbody><tr><td style="text-align:left"><strong>Update latency (s)</strong></td><td style="text-align:left">2072</td><td style="text-align:left">1429</td><td style="text-align:left">1.4x</td></tr><tr><td style="text-align:left"><strong>Bytes written (GB)</strong></td><td style="text-align:left">891.7</td><td style="text-align:left">12.7</td><td style="text-align:left">70.2x</td></tr><tr><td style="text-align:left"><strong>Query latency (s)</strong></td><td style="text-align:left">164</td><td style="text-align:left">29</td><td style="text-align:left">5.7x</td></tr></tbody></table>
<p>This also lays the foundation for managing unstructured and multimodal data inside a Hudi table and supporting <a href="https://github.com/apache/hudi/pull/11733" target="_blank" rel="noopener noreferrer">wide tables</a> efficiently for machine learning use cases.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="merge-modes-and-custom-mergers">Merge Modes and Custom Mergers<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#merge-modes-and-custom-mergers" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>One of the most unique capabilities Hudi provides is how it helps process streaming data. Specifically, Hudi has, since the very beginning, supported merging records pre-write (to reduce write amplification), during write (against an existing record in storage with the same record key) and reads (for MoR snapshot queries), using a <em>precombine</em> or <em>ordering</em> field. This helps implement <a href="https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/" target="_blank" rel="noopener noreferrer">event time processing</a> semantics, widely supported by stream processing systems, on data lakehouse storage. This helps integrate late-arriving data into Hudi tables without causing weird movement of record state back in time. For example, if an older database CDC record arrives late and gets committed as the new value, the state of the record would be incorrect even though the writes to the table themselves were serialized in some order.</p>
<p><img decoding="async" loading="lazy" alt="event time ordering" src="https://hudi.apache.org/cn/assets/images/event-time-ordering-merge-mode-c8164e035840388bf4290fa81ac6262a.png" width="1360" height="490" class="img_ev3q">
</p><p align="center">Figure: Shows EVENT_TIME_ORDERING where merging reconciles state based on the highest event_time</p><p></p>
<p>Prior Hudi versions supported this functionality through the record payload interface with built-in support for a pre-combine field on the default payloads. Hudi 1.0 makes these two styles of processing and merging changes first class by introducing <a href="https://hudi.apache.org/cn/docs/record_merger">merge modes</a> within Hudi.</p>
<table><thead><tr><th style="text-align:left">Merge Mode</th><th style="text-align:left">What does it do?</th></tr></thead><tbody><tr><td style="text-align:left">COMMIT_TIME_ORDERING</td><td style="text-align:left">Picks record with highest completion time/instant as final merge result  i.e., standard relational semantics or arrival time processing</td></tr><tr><td style="text-align:left">EVENT_TIME_ORDERING</td><td style="text-align:left">Default (for now, to ease migration).Picks record with the highest value for a user-specified ordering/precombine field as the final merge result.</td></tr><tr><td style="text-align:left">CUSTOM</td><td style="text-align:left">Uses a user-provided RecordMerger implementation to produce final merge result (similar to stream processing processor APIs)</td></tr></tbody></table>
<p>Like partial update support, the new <em>RecordMerger</em> API provides a more efficient engine-native alternative to the older RecordPayload interface through native objects and vectorized processing on EVENT_TIME_ORDERING merge modes. In future versions, we intend to change the default to COMMIT_TIME_ORDERING to provide simple, out-of-the-box relational table semantics.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="non-blocking-concurrency-control-for-streaming-writes">Non-Blocking Concurrency Control for Streaming Writes<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#non-blocking-concurrency-control-for-streaming-writes" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>We have expressed dissatisfaction with the optimistic concurrency control approaches employed on the data lakehouse since they appear to paint the problem with a broad brush without paying attention to the nuances of the lakehouse workloads. Specifically, contention is much more common in data lakehouses, even for Hudi, the only data lakehouse storage project capable of asynchronously compacting delta updates without failing or causing retries on the writer. Ultimately, data lakehouses are high-throughput systems, and failing concurrent writers to handle contention can waste expensive compute clusters. Streaming and high-frequency writes often require fine-grained concurrency control to prevent bottlenecks.</p>
<p>Hudi 1.0 introduces a new <strong>non-blocking concurrency control (NBCC)</strong> designed explicitly for data lakehouse workloads, using years of experience gained supporting some of the largest data lakes on the planet in the Hudi community. NBCC enables simultaneous writing from multiple writers and compaction of the same record without blocking any involved processes. This is achieved by simply lightweight distributed locks and TrueTime semantics discussed above. (see <a href="https://github.com/apache/hudi/blob/master/rfc/rfc-66/rfc-66.md" target="_blank" rel="noopener noreferrer">RFC-66</a> for more)</p>
<div style="text-align:center"><img src="https://hudi.apache.org/assets/images/nbcc_partial_updates.gif" alt="NBCC"><p align="center">Figure: Two streaming jobs in action writing to the same records concurrently on different columns.</p></div>
<p>NBCC operates with streaming semantics, tying together concepts from previous sections. Data necessary to compute table updates are emitted from an upstream source, and changes and partial updates can be merged in any of the merge modes above. For example, in the figure above, two independent Flink jobs enrich different table columns in parallel, a pervasive pattern seen in stream processing use cases. Check out this <a href="https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control" target="_blank" rel="noopener noreferrer">blog</a> for a full demo. We also expect to support NBCC across other compute engines in future releases.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="backwards-compatible-writing">Backwards Compatible Writing<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#backwards-compatible-writing" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h3>
<p>If you are wondering: ‚ÄúAll of this sounds cool, but how do I upgrade?‚Äù we have put a lot of thought into making that seamless. Hudi has always supported backward-compatible reads to older table versions. Table versions are stored in table properties unrelated to the software binary version. The supported way of upgrading has been to first migrate readers/query engines to new software binary versions and then upgrade the writers, which will auto-upgrade the table if there is a table version change between the old and new software binary versions. Upon community feedback, users expressed the need to be able to do upgrades on the writers without waiting on the reader side upgrades and reduce any additional coordination necessary within different teams.</p>
<p><img decoding="async" loading="lazy" alt="Indexes" src="https://hudi.apache.org/cn/assets/images/backwards-compat-writing-6299b055646e2577964069b755ee1f3d.png" width="1481" height="825" class="img_ev3q">
</p><p align="center">Figure: 4-step process for painless rolling upgrades to Hudi 1.0</p><p></p>
<p>Hudi 1.0 introduces backward-compatible writing to achieve this in 4 steps, as described above. Hudi 1.0 also automatically handles any checkpoint translation necessary as we switch to completion time-based processing semantics for incremental and CDC queries. The Hudi metadata table has to be temporarily disabled during this upgrade process but can be turned on once the upgrade is completed successfully. Please read the <a href="https://hudi.apache.org/cn/releases/release-1.0.0">release notes</a> carefully to plan your migration.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="whats-next">What‚Äôs Next?<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#whats-next" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>Hudi 1.0 is a testament to the power of open-source collaboration. This release embodies the contributions of 60+ developers, maintainers, and users who have actively shaped its roadmap. We sincerely thank the Apache Hudi community for their passion, feedback, and unwavering support.</p>
<p>The release of Hudi 1.0 is just the beginning. Our current <a href="https://hudi.apache.org/cn/roadmap">roadmap</a> includes exciting developments across the following planned releases:</p>
<ul>
<li><strong>1.0.1</strong>: First bug fix, patch release on top of 1.0, which hardens the functionality above and makes it easier. We intend to publish additional patch releases to aid migration to 1.0 as the bridge release for the community from 0.x.</li>
<li><strong>1.1</strong>:  Faster writer code path rewrite, new indexes like bitmap/vector search, granular record-level change encoding, Hudi storage engine APIs, abstractions for cross-format interop.</li>
<li><strong>1.2</strong>: Multi-table transactions, platform services for reverse streaming from Hudi etc., Multi-modal data + indexing, NBCC clustering</li>
<li><strong>2.0</strong>: Server components for DLMS, caching and metaserver functionality.</li>
</ul>
<p>Hudi releases are drafted collaboratively by the community. If you don‚Äôt see something you like here, please help shape the roadmap together.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="get-started-with-apache-hudi-10">Get Started with Apache Hudi 1.0<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#get-started-with-apache-hudi-10" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">‚Äã</a></h2>
<p>Are you ready to experience the future of data lakehouses? Here‚Äôs how you can dive into Hudi 1.0:</p>
<ul>
<li>Documentation: Explore Hudi‚Äôs <a href="https://hudi.apache.org/cn/docs/overview">Documentation</a> and learn the <a href="https://hudi.apache.org/cn/docs/hudi_stack">concepts</a>.</li>
<li>Quickstart Guide: Follow the <a href="https://hudi.apache.org/cn/docs/quick-start-guide">Quickstart Guide</a> to set up your first Hudi project.</li>
<li>Upgrading from a previous version?  Follow the <a href="https://hudi.apache.org/cn/releases/release-1.0.0#migration-guide">migration guide</a> and contact the Hudi OSS community for help.</li>
<li>Join the Community: Participate in discussions on the <a href="https://hudi.apache.org/community/get-involved/" target="_blank" rel="noopener noreferrer">Hudi Mailing List</a>, <a href="https://join.slack.com/t/apache-hudi/shared_invite/zt-2ggm1fub8-_yt4Reu9djwqqVRFC7X49g" target="_blank" rel="noopener noreferrer">Slack</a> and <a href="https://github.com/apache/hudi/issues" target="_blank" rel="noopener noreferrer">GitHub</a>.</li>
<li>Follow us on social media: <a href="https://www.linkedin.com/company/apache-hudi/?viewAsMember=true" target="_blank" rel="noopener noreferrer">Linkedin</a>, <a href="https://twitter.com/ApacheHudi" target="_blank" rel="noopener noreferrer">X/Twitter</a>.</li>
</ul>
<p>We can‚Äôt wait to see what you build with Apache Hudi 1.0. Let‚Äôs work together to shape the future of data lakehouses!</p>
<p>Crafted with passion for the Apache Hudi community.</p>]]></content:encoded>
            <category>timeline</category>
            <category>design</category>
            <category>release</category>
            <category>streaming ingestion</category>
            <category>multi-writer</category>
            <category>concurrency-control</category>
            <category>blog</category>
        </item>
    </channel>
</rss>