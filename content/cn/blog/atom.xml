<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://hudi.apache.org/cn/blog</id>
    <title>Apache Hudi: User-Facing Analytics</title>
    <updated>2025-07-15T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://hudi.apache.org/cn/blog"/>
    <subtitle>Apache Hudi Blog</subtitle>
    <icon>https://hudi.apache.org/cn/assets/images/favicon.ico</icon>
    <entry>
        <title type="html"><![CDATA[Modernizing Data Infrastructure at Peloton Using Apache Hudi]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi</id>
        <link href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi"/>
        <updated>2025-07-15T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Peloton re-architected its data platform using Apache Hudi to overcome snapshot delays, rigid service coupling, and high operational costs. By adopting CDC-based ingestion from PostgreSQL and DynamoDB, moving from CoW to MoR tables, and leveraging asynchronous services with fine-grained schema control, Peloton achieved 10-minute ingestion cycles, reduced compute/storage overhead, and enabled time travel and GDPR compliance.]]></summary>
        <content type="html"><![CDATA[<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>TL;DR</div><div class="admonitionContent_BuS1"><p>Peloton re-architected its data platform using Apache Hudi to overcome snapshot delays, rigid service coupling, and high operational costs. By adopting CDC-based ingestion from PostgreSQL and DynamoDB, moving from CoW to MoR tables, and leveraging asynchronous services with fine-grained schema control, Peloton achieved 10-minute ingestion cycles, reduced compute/storage overhead, and enabled time travel and GDPR compliance.</p></div></div>
<p>Peloton is a global interactive fitness platform that delivers connected, instructor-led fitness experiences to millions of members worldwide. Known for its immersive classes and cutting-edge equipment, Peloton combines software, hardware, and data to create personalized workout journeys. With a growing member base and increasing product diversity, data has become central to how Peloton delivers value. The <em>Data Platform</em> team at Peloton is responsible for building and maintaining the core infrastructure that powers analytics, reporting, and real-time data applications. Their work ensures that data flows seamlessly from transactional systems to the data lake, enabling teams across the organization to make timely, data-driven decisions.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-challenge-data-growth-latency-and-operational-bottlenecks">The Challenge: Data Growth, Latency, and Operational Bottlenecks<a href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi#the-challenge-data-growth-latency-and-operational-bottlenecks" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>As Peloton evolved into a global interactive fitness platform, its data infrastructure was challenged by the growing need for timely insights, agile service migrations, and cost-effective analytics. Daily operations, recommendation systems, and compliance requirements demanded an architecture that could support near real-time access, high-frequency updates, and scalable service boundaries.</p>
<p>However, the team faced persistent bottlenecks with the existing setup:</p>
<ul>
<li>Reporting pipelines were gated by the completion of full snapshot jobs.</li>
<li>Recommender systems could only function on daily refreshed datasets.</li>
<li>The analytics platform was tightly coupled with operational systems.</li>
<li>Microservice migrations were constrained to all-at-once shifts.</li>
<li>Database read replicas incurred high infrastructure costs.</li>
</ul>
<p>These limitations made it difficult to meet SLA expectations, scale workloads efficiently, and adapt the platform to new user and product needs.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-legacy-architecture">The Legacy Architecture<a href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi#the-legacy-architecture" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig1.png" alt="challenge" width="1000" align="middle">
<p>Peloton's earlier architecture relied on daily snapshots from a monolithic <strong>PostgreSQL</strong> database. The analytics systems would consume these snapshots, often waiting hours for completion. This not only delayed reporting but also introduced downstream rigidity.</p>
<p>Because the same data platform supported both online and analytical workloads, any schema or service migration required significant planning and coordination. Database read replicas, used to scale reads, increased cost overhead. Moreover, recommendation systems that depended on data freshness were constrained by the snapshot interval, limiting personalization capabilities. This architecture struggled to support a fast-moving product roadmap, near real-time analytics, and the data agility needed to experiment and iterate.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="reimagining-the-data-platform-with-apache-hudi">Reimagining the Data Platform with Apache Hudi<a href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi#reimagining-the-data-platform-with-apache-hudi" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig2.png" alt="challenge" width="1000" align="middle">
<p>To address these challenges, the data platform team introduced Apache Hudi as the foundation of its modern data lake. The architecture was rebuilt to support Change Data Capture (CDC) ingestion from both PostgreSQL and DynamoDB using Debezium, with Kafka acting as the transport layer. A custom-built Hudi writer was developed to ingest CDC records into S3 using Apache Spark on EMR (version 6.12.0 with Hudi 0.13.1).</p>
<p>Peloton initially chose Copy-on-Write (CoW) table formats to support querying via Redshift Spectrum and simplify adoption. However, performance and cost bottlenecks prompted a transition to Merge-on-Read (MoR) tables with asynchronous table services for cleaning and compaction.</p>
<p>Key architectural enhancements included:</p>
<ul>
<li><strong>Support for GDPR compliance</strong> through structured delete propagation.</li>
<li><strong>Time travel queries</strong> for recommender model training and data recovery.</li>
<li><strong>Phased migration support</strong> for microservices via decoupled ingestion.</li>
</ul>
<p>Peloton's broader data platform tech stack supports this architecture with a range of tools for orchestration, analytics, and governance. This includes EMR for compute, Redshift for querying, DBT for data transformations, Looker for BI and visualization, Airflow for orchestration, and DataHub for metadata management. These components complement Apache Hudi in forming a modular and production-ready lakehouse stack.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig3.png" alt="challenge" width="1000" align="middle">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="learnings-from-running-hudi-at-scale">Learnings from Running Hudi at Scale<a href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi#learnings-from-running-hudi-at-scale" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>With Hudi now integrated into Peloton's data lake, the team began to observe and address new operational and architectural challenges that emerged at scale. This section outlines the major lessons learned while maintaining high-ingestion throughput, ensuring data reliability, and keeping infrastructure costs under control.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cow-vs-mor-performance-trade-offs">CoW vs MoR: Performance Trade-offs<a href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi#cow-vs-mor-performance-trade-offs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Initially, Copy-on-Write (CoW) tables were chosen to simplify deployment and ensure compatibility with Redshift Spectrum. However, as ingestion frequency increased and update volumes spanned hundreds of partitions, performance became a bottleneck. Some high-frequency tables with updates across 256 partitions took nearly an hour to process per run. Additionally, retaining 30 days of commits for training recommender models significantly inflated storage requirements, reaching into the hundreds of gigabytes.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig4.png" alt="challenge" width="1000" align="middle">
<p>To resolve this, the team migrated to Hudi’s Merge-on-Read (MoR) tables and reduced commit retention to 7 days. With ingestion jobs now running every 10 minutes, latency dropped significantly, and storage and compute usage became more efficient.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="async-vs-inline-table-services">Async vs Inline Table Services<a href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi#async-vs-inline-table-services" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>To improve write throughput and meet low-latency ingestion goals, the Peloton team initially configured Apache Hudi with asynchronous cleaner and compactor services. This approach worked well across most tables, allowing ingestion pipelines to run every 10 minutes with minimal blocking but introduced some operational edge cases. Some of the challenges encountered included:</p>
<ul>
<li>Concurrent execution of writer and cleaner jobs, leading to conflicts. These were mitigated by introducing DynamoDB-based locks to serialize access.</li>
<li>Reader-cleaner race conditions, where time travel queries intermittently failed with <code>"File Not Found"</code> errors - traced back to cleaners deleting files mid-read.</li>
<li>Compaction disruptions caused by EMR node terminations, which led to orphaned files when jobs failed mid-way.</li>
</ul>
<p>These edge cases were largely due to the operational complexity of managing concurrent workloads at Peloton’s scale. After weighing reliability against latency, the team opted to switch to inline table services for compaction and cleaning, augmented with custom logic to control when these actions would run. This change improved system stability while maintaining acceptable latency trade-offs.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="glue-schema-version-limits">Glue Schema Version Limits<a href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi#glue-schema-version-limits" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>As schema evolution continued, the team used Hudi's <code>META_SYNC_ENABLED</code> to sync schema updates with AWS Glue. Over time, high-frequency schema updates pushed the number of <code>TABLE_VERSION</code> resources in Glue beyond the <em>1 million</em> limit. This caused jobs to fail in ways that were initially difficult to trace.</p>
<p>One such failure manifested as the following error:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">ERROR Client: Application diagnostics message: User class threw exception:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">java.lang.NoSuchMethodError: 'org.apache.hudi.exception.HoodieException </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">org.apache.hudi.sync.common.util.SyncUtilHelpers.getExceptionFromList(java.util.Collection)'</span><br></span></code></pre></div></div>
<p>After significant debugging, the issue was traced to AWS Glue limits. The team implemented a multi-step fix:</p>
<ul>
<li>Worked with AWS to temporarily raise resource limits.</li>
<li>Developed a Python service to identify and delete outdated table versions, removing over 1 million entries.</li>
<li>Added an Airflow job to schedule weekly cleanup tasks.</li>
<li>Improved schema sync logic to trigger only when the schema changed.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="debezium--toast-handling">Debezium &amp; TOAST Handling<a href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi#debezium--toast-handling" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>PostgreSQL CDC ingestion posed unique challenges due to the database’s handling of large fields using TOAST (The Oversized-Attribute Storage Technique). When fields over 8KB were unchanged, Debezium emitted a placeholder value <code>__debezium_unavailable_value</code>, making it impossible to determine whether the value had changed.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig5.png" alt="challenge" width="1000" align="middle">
<p>To address this, Peloton:</p>
<ul>
<li>Populated initial data using PostgreSQL snapshots.</li>
<li>Implemented self-joins between incoming CDC records and existing Hudi records to fill in missing values.</li>
<li>Separated inserts, updates, and deletes within Spark batch processing.</li>
<li>Used the <code>ts</code> field as the precombine key to ensure only the latest record state was retained.</li>
</ul>
<p>A reconciliation pipeline was also developed to heal data inconsistencies caused by multiple operations on the same key within a batch (e.g., create-delete-create).</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="data-validation-and-quality-enforcement">Data Validation and Quality Enforcement<a href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi#data-validation-and-quality-enforcement" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Data quality was critical to ensure trust in the newly established data lake. The team developed several internal libraries and checks:</p>
<ul>
<li>A Crypto Shredding Library to encrypt <code>user_id</code> and other PII fields before storage.</li>
<li>A Data Validation Framework that compared records in the lake against snapshot data.</li>
<li>A Data Quality Library that enforced column-level thresholds. These checks integrated with DataHub and were tied to Airflow sensors to halt downstream jobs on failures.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="dynamodb-ingestion-and-schema-challenges">DynamoDB Ingestion and Schema Challenges<a href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi#dynamodb-ingestion-and-schema-challenges" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Some Peloton services relied on DynamoDB for operational workloads (NoSQL). To ingest these datasets into the lake, the team used DynamoDB Streams and a Kafka Connector, allowing reuse of the existing Kafka-based Hudi ingestion path.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig6.png" alt="challenge" width="1000" align="middle">
<p>However, the NoSQL nature of DynamoDB introduced schema management challenges. Two strategies were evaluated:</p>
<ol>
<li>Stakeholder-defined schemas, using SUPER-type fields.</li>
<li>Dynamic schema inference, where incoming JSON records were parsed, and the evolving schema was inferred and reconciled.</li>
</ol>
<p>The team opted for dynamic inference despite increased processing time, as it enabled better support for exploratory workloads. Daily snapshots and reconciliation steps helped clean up inconsistent schema states.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="reducing-operational-costs">Reducing Operational Costs<a href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi#reducing-operational-costs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig7.png" alt="challenge" width="1000" align="middle">
<p>As the system matured, cost optimization became a priority. The team used <a href="https://github.com/ganglia/" target="_blank" rel="noopener noreferrer">Ganglia</a> to analyze job profiles and identify areas for improvement:</p>
<ul>
<li>EMR resources were gradually right-sized based on CPU and memory usage.</li>
<li>Conditional Hive syncing was introduced to avoid unnecessary sync operations during each run.</li>
<li>A Spark-side inefficiency was discovered where archived timelines were unnecessarily loaded, causing jobs to take 4x longer. Fixing this reduced overall latency and compute resource usage.</li>
</ul>
<p>These operational refinements significantly reduced idle times and improved the cost-efficiency of the platform.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="gains-from-hudi-adoption">Gains from Hudi Adoption<a href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi#gains-from-hudi-adoption" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Peloton's transition to Apache Hudi led to measurable performance, operational, and cost-related improvements across its modern data platform.</p>
<p>Peloton's transition to Apache Hudi yielded several measurable improvements:</p>
<ul>
<li>Ingestion frequency increased from once daily to every 10 minutes.</li>
<li>Reduced snapshot job durations from an hour to under 15 minutes.</li>
<li>Cost savings by eliminating read replicas and optimizing EMR cluster usage.</li>
<li>Time travel support enabled retrospective analysis and model re-training.</li>
<li>Improved compliance posture through structured deletes and encrypted PII.</li>
</ul>
<p>The modernization laid the groundwork for future evolution, including real-time streaming ingestion using Apache Flink and continued improvements in data freshness, latency, and governance.</p>
<p>This blog is based on Peloton’s presentation at the Apache Hudi Community Sync. If you are interested in watching the recorded version of the video, you can find it <a href="https://youtu.be/-Pyid5K9dyU?feature=shared" target="_blank" rel="noopener noreferrer">here</a>.</p>
<hr>]]></content>
        <author>
            <name>Amaresh Bingumalla, Thinh Kenny Vu, Gabriel Wang, Arun Vasudevan in collaboration with Dipankar Mazumdar</name>
        </author>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="Peloton" term="Peloton"/>
        <category label="Community" term="Community"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[How PayU built a secure enterprise AI assistant using Amazon Bedrock]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/07/15/PayU-built-a-secure-enterprise-AI-assistant</id>
        <link href="https://hudi.apache.org/cn/blog/2025/07/15/PayU-built-a-secure-enterprise-AI-assistant"/>
        <updated>2025-07-15T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://aws.amazon.com/blogs/machine-learning/how-payu-built-a-secure-enterprise-ai-assistant-using-amazon-bedrock/">here</a></span>]]></content>
        <author>
            <name>Deepesh Dhapola, Mudit Chopra, Rahmat Khan, Rahul Ghosh, Saikat Dey, and Sandeep Kumar Veerlapati</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="AWS" term="AWS"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Building a RAG-based AI Recommender (Part 1/2)]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/07/10/building-a-rag-based-ai-recommender</id>
        <link href="https://hudi.apache.org/cn/blog/2025/07/10/building-a-rag-based-ai-recommender"/>
        <updated>2025-07-10T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://blog.datumagic.ai/p/building-a-rag-based-ai-recommender">here</a></span>]]></content>
        <author>
            <name>Shiyan Xu</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="AI" term="AI"/>
        <category label="RAG" term="RAG"/>
        <category label="Artificial Intelligence" term="Artificial Intelligence"/>
        <category label="data lakehouse" term="data lakehouse"/>
        <category label="Lakehouse" term="Lakehouse"/>
        <category label="use-case" term="use-case"/>
        <category label="datumagic" term="datumagic"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Stifel built a modern data platform using AWS Glue and an event-driven domain architecture]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/07/07/how-stifel-built-a-modern-data-platform-using-aws-glue-and-an-event-driven-domain-architecture</id>
        <link href="https://hudi.apache.org/cn/blog/2025/07/07/how-stifel-built-a-modern-data-platform-using-aws-glue-and-an-event-driven-domain-architecture"/>
        <updated>2025-07-07T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://aws.amazon.com/blogs/big-data/how-stifel-built-a-modern-data-platform-using-aws-glue-and-an-event-driven-domain-architecture/">here</a></span>]]></content>
        <author>
            <name>Amit Maindola and Srinivas Kandi, Hossein Johari, Ahmad Rawashdeh, Lei Meng</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="aws" term="aws"/>
        <category label="AWS Glue" term="AWS Glue"/>
        <category label="AWS Blogs" term="AWS Blogs"/>
        <category label="Amazon EMR" term="Amazon EMR"/>
        <category label="AWS Lake Formation" term="AWS Lake Formation"/>
        <category label="Data Governance" term="Data Governance"/>
        <category label="Lakehouse" term="Lakehouse"/>
        <category label="use-case" term="use-case"/>
        <category label="det" term="det"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why Uber Built Hudi: The Strategic Decision Behind a Custom Table Format]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/07/03/why-uber-built-hudi-the-strategic-decision-behind-a-custom-table-format</id>
        <link href="https://hudi.apache.org/cn/blog/2025/07/03/why-uber-built-hudi-the-strategic-decision-behind-a-custom-table-format"/>
        <updated>2025-07-03T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://thamizhelango.medium.com/why-uber-built-hudi-the-strategic-decision-behind-a-custom-table-format-f57db68b0cb9">here</a></span>]]></content>
        <author>
            <name>ThamizhElango Natarajan</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="Apache Iceberg" term="Apache Iceberg"/>
        <category label="Lakehouse" term="Lakehouse"/>
        <category label="use-case" term="use-case"/>
        <category label="Uber" term="Uber"/>
        <category label="det" term="det"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lakehouse Architecture - Apache Hudi and Apache Iceberg]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/07/02/Lakehouse-Architecture-apache-hudi-and-apache-iceberg</id>
        <link href="https://hudi.apache.org/cn/blog/2025/07/02/Lakehouse-Architecture-apache-hudi-and-apache-iceberg"/>
        <updated>2025-07-02T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://www.linkedin.com/pulse/lakehouse-architecture-apache-hudi-iceberg-becloudready-4b1ac/">here</a></span>]]></content>
        <author>
            <name>beCloudReady</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="Apache Iceberg" term="Apache Iceberg"/>
        <category label="Lakehouse" term="Lakehouse"/>
        <category label="use-case" term="use-case"/>
        <category label="det" term="det"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Complex Data Workflows at Uber Using Apache Hudi]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/06/30/uber-hudi</id>
        <link href="https://hudi.apache.org/cn/blog/2025/06/30/uber-hudi"/>
        <updated>2025-06-30T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Uber’s trip and order collection pipelines grew highly complex, with long runtimes, massive DAGs, and rigid SQL logic that hampered scalability and maintainability. By adopting Apache Hudi, Uber re-architected these pipelines to enable incremental processing, custom merge behavior, and rule-based functional transformations. This reduced runtime from 20 hours to 4 hours, improved test coverage to 95%, cut costs by 60%, and delivered a composable, explainable, and scalable data workflow architecture.]]></summary>
        <content type="html"><![CDATA[<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>TL;DR</div><div class="admonitionContent_BuS1"><p>Uber’s trip and order collection pipelines grew highly complex, with long runtimes, massive DAGs, and rigid SQL logic that hampered scalability and maintainability. By adopting Apache Hudi, Uber re-architected these pipelines to enable incremental processing, custom merge behavior, and rule-based functional transformations. This reduced runtime from 20 hours to 4 hours, improved test coverage to 95%, cut costs by 60%, and delivered a composable, explainable, and scalable data workflow architecture.</p></div></div>
<p>At Uber, the Core Services Data Engineering team supports a wide range of use cases across products like Uber Mobility and Uber Eats. One critical use case is computing the collection - the net payable amount - from a trip or an order. While this sounds straightforward at first, it quickly becomes a complex data problem when you factor in real-world scenarios like refunds, tips, driver disputes, location updates, and settlement adjustments across multiple verticals.</p>
<p>To solve this problem at scale, Uber re-architected their pipelines using <a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer">Apache Hudi</a> to enable low-latency, incremental, and rule-based processing. This post outlines the challenges they faced, the architectural shifts they made, and the measurable outcomes they achieved in production.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-challenge-scale-latency-and-complexity">The Challenge: Scale, Latency, and Complexity<a href="https://hudi.apache.org/cn/blog/2025/06/30/uber-hudi#the-challenge-scale-latency-and-complexity" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<img src="https://hudi.apache.org/assets/images/blog/figure2_uber.png" alt="challenge" width="800" align="middle">
<p>Our original data pipelines were processing nearly 90 million records a day, but the nature of updates made them inefficient. For instance, a trip taken three years ago could still be updated due to a late settlement. Our statistical analysis showed most updates occur within 180 days, so we designed the system to read and write a 180-day window every day - leading to severe read and write amplification.</p>
<p>The pipeline itself was a massive DAG with over 50–60 tasks, taking close to 20 hours to complete. These long runtimes made recovery difficult and introduced operational risks. Making a change meant tracing the logic across this sprawling DAG, which affected developer productivity and increased the chances of regressions.</p>
<p>Despite the large window, we still missed updates that fell outside the 180-day mark, leading to data quality issues. The long development cycles and heavy debugging effort further hindered our ability to iterate and maintain the system.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="rigid-sql-and-tight-coupling">Rigid SQL and Tight Coupling<a href="https://hudi.apache.org/cn/blog/2025/06/30/uber-hudi#rigid-sql-and-tight-coupling" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Digging deeper, we identified multiple underlying causes. The pipeline relied heavily on SQL for all transformations. But expressing the evolving business rules for different Uber products in SQL was limiting. The logic had grown too complex to be managed effectively, and granular transformations led to a proliferation of intermediate stages. This made unit testing and debugging difficult, and the absence of structured logging made observability poor.</p>
<img src="https://hudi.apache.org/assets/images/blog/figure3_uber.png" alt="redshift" width="800" align="middle">
<p>Additionally, data and logic were tightly coupled. The system often required joining tables at very fine granularities, introducing redundancy and making logic harder to reason about. Complex joins, table scans, and late-arriving data amplified processing costs. It was also difficult to trace how a specific row was transformed through the DAG, making explainability a real challenge.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-we-solved-it">How We Solved It?<a href="https://hudi.apache.org/cn/blog/2025/06/30/uber-hudi#how-we-solved-it" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ol>
<li><strong>Solving Read Amplification</strong></li>
</ol>
<p>The first step in addressing inefficiencies was eliminating the brute-force strategy of scanning and processing a 180-day window of data on every pipeline run. With the help of Apache Hudi’s <a href="https://hudi.apache.org/docs/table_types#incremental-queries" target="_blank" rel="noopener noreferrer"><em>incremental</em> <em>read</em></a> capabilities, we restructured the ingestion layer to read only the records that had mutated since the last checkpoint.</p>
<img src="https://hudi.apache.org/assets/images/blog/fig4_uber.png" alt="redshift" width="800" align="middle">
<p>We introduced an intermediate Hudi table that consolidated all related records for a trip or order into a single row, using complex data types such as structs, lists, and maps. This model allowed us to capture the complete state of a trip - including all updates, tips, disputes, and refunds in one place, without scattering information across multiple joins.</p>
<p>By using this intermediate table as the foundation, all downstream logic could operate on change-driven inputs. The result was a pipeline that avoided unnecessary scans, improved correctness by processing all real changes (not just those in a time window), and reduced overall I/O dramatically.</p>
<ol start="2">
<li><strong>Eliminating Self Joins with Custom Payloads</strong></li>
</ol>
<p>Self joins - especially for reconciling updates to the same trip were one of the costliest operations in our original pipeline.</p>
<img src="https://hudi.apache.org/assets/images/blog/fig5_uber.png" alt="redshift" width="800" align="middle">
<p>To solve this, we implemented a custom Hudi payload class that allows us to control how updates are applied during the merge phase. This class overrides methods such as <code>combineAndGetUpdateValue</code> and <code>getInsertValue</code>, and executes the merge logic as part of the write path, eliminating the need for a full table scan or shuffle.</p>
<p>This approach helped us efficiently handle updates to complex, nested records in the intermediate Hudi table, and dramatically reduced the cost associated with self joins.</p>
<ol start="3">
<li><strong>Simplifying Processing with a Rule-Based Framework</strong></li>
</ol>
<p>To move away from the rigidity of SQL, we designed a rule engine framework based on functional programming principles.</p>
<p>Instead of expressing business logic as large, monolithic SQL queries, we cast each input row (from the intermediate table) into a strongly typed object (e.g., a Trip object). These objects were then passed through a series of declarative rules - each consisting of a condition and an action.</p>
<img src="https://hudi.apache.org/assets/images/blog/fig6_uber.png" alt="redshift" width="800" align="middle">
<p>This framework was implemented as a custom <a href="https://hudi.apache.org/docs/hoodie_streaming_ingestion#transformers" target="_blank" rel="noopener noreferrer"><em>transformer</em></a> plugged into <a href="https://hudi.apache.org/docs/hoodie_streaming_ingestion" target="_blank" rel="noopener noreferrer">HudiStreamer</a>. The transformer intercepts the ingested data, applies the rule engine logic, and emits the transformed object to the final Hudi output table. We also built in capabilities for:</p>
<ul>
<li>Logging and observability (for metrics and debugging)</li>
<li>Unreachable state detection (flagging invalid rows)</li>
<li>Unit testing support for each rule independently</li>
</ul>
<p>This architecture replaced the huge DAG with modular, testable, and composable rule definitions, dramatically improving developer productivity and data pipeline clarity.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="final-architecture">Final Architecture<a href="https://hudi.apache.org/cn/blog/2025/06/30/uber-hudi#final-architecture" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<img src="https://hudi.apache.org/assets/images/blog/fig7_uber.png" alt="redshift" width="800" align="middle">
<p>The redesigned system follows a clean, composable structure:</p>
<ul>
<li>Incremental ingestion from the data lake is done using HudiStreamer, which writes to an intermediate Hudi table.</li>
<li>The intermediate table consolidates all records for a trip using complex types, serving as the central input for downstream processing.</li>
<li>A custom Transformer intercepts the records, casts them into typed domain objects, and passes them through a rule engine.</li>
<li>The rule engine applies business logic declaratively and emits fully processed objects.</li>
<li>The output is written to a final Hudi table that supports efficient, incremental consumption.</li>
</ul>
<p>This design eliminates redundant scans, reduces shuffle overhead, enables full test coverage, and offers detailed observability across all transformation stages.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-wins-with-hudi">The Wins with Hudi<a href="https://hudi.apache.org/cn/blog/2025/06/30/uber-hudi#the-wins-with-hudi" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>The improvements were substantial and measurable:</p>
<ul>
<li>Runtime reduced from ~20 hours to ~4 hours (~75% improvement)</li>
<li>Test coverage increased to 95% for transformation logic</li>
<li>Single run cost reduced by 60%</li>
<li>Improved data completeness, processing all updates—not just those in a statistical window</li>
<li>Reusable and modular logic, reducing DAG complexity</li>
<li>Higher developer productivity, with isolated unit testing and simplified debugging</li>
<li>Improved self-join performance through custom payloads</li>
<li>A generic rule engine design, portable across Spark and Flink</li>
</ul>
<p>Apache Hudi has been central to Nexus’ success, providing the core data lake storage layer for scalable ingestion, updates, and metadata management. It enables fast, incremental updates at massive scale while maintaining transactional guarantees on top of Amazon S3.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://hudi.apache.org/cn/blog/2025/06/30/uber-hudi#conclusion" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>By redesigning the system around Apache Hudi and adopting functional, rule-based processing, Uber was able to transform a brittle, long-running pipeline into a maintainable and efficient architecture. The changes allowed them to scale their data workflows to meet the needs of complex, multi-product use cases without compromising on performance, observability, or data quality.</p>
<p>This work highlights the power of pairing the right storage format with a principled architectural approach. Apache Hudi was instrumental in helping achieve these outcomes and continues to play a key role in Uber’s evolving data platform.</p>
<p>This blog is based on Uber’s presentation at the Apache Hudi Community Sync. If you are interested in watching the recorded version of the video, you can find it <a href="https://www.youtube.com/watch?v=VpdimpH_nsI" target="_blank" rel="noopener noreferrer">here</a>.</p>
<hr>]]></content>
        <author>
            <name>Ankit Shrivastava in collaboration with Dipankar</name>
        </author>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="Uber" term="Uber"/>
        <category label="Community" term="Community"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Apache Hudi does XYZ (1/10): File pruning with multi-modal index]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/06/16/Apache-Hudi-does-XYZ-110</id>
        <link href="https://hudi.apache.org/cn/blog/2025/06/16/Apache-Hudi-does-XYZ-110"/>
        <updated>2025-06-16T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://blog.datumagic.ai/p/apache-hudi-does-xyz-110">here</a></span>]]></content>
        <author>
            <name>Shiyan Xu</name>
        </author>
        <category label="hudi" term="hudi"/>
        <category label="spark" term="spark"/>
        <category label="blog" term="blog"/>
        <category label="course" term="course"/>
        <category label="tutorial" term="tutorial"/>
        <category label="datumagic" term="datumagic"/>
        <category label="data lake" term="data lake"/>
        <category label="lakehouse" term="lakehouse"/>
        <category label="apache hudi" term="apache hudi"/>
        <category label="apache spark" term="apache spark"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Apache Hudi’s New Log-Structured Merge (LSM) Timeline]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline</id>
        <link href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline"/>
        <updated>2025-05-29T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Apache Hudi 1.0 introduces a new LSM Timeline to scale metadata management for long-lived tables. By restructuring timeline storage into a compacted, versioned tree layout, Hudi enables faster metadata access, snapshot isolation, and support for Non-Blocking Concurrency Control.]]></summary>
        <content type="html"><![CDATA[<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>TL;DR</div><div class="admonitionContent_BuS1"><p>Apache Hudi 1.0 introduces a new LSM Timeline to scale metadata management for long-lived tables. By restructuring timeline storage into a compacted, versioned tree layout, Hudi enables faster metadata access, snapshot isolation, and support for Non-Blocking Concurrency Control.</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="apache-hudis-timeline">Apache Hudi’s Timeline<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#apache-hudis-timeline" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>At the heart of Apache Hudi’s architecture is the <a href="https://hudi.apache.org/docs/timeline" target="_blank" rel="noopener noreferrer">Timeline</a> - a log-structured system that acts as the single source of truth for the table’s state at any point in time. The timeline records every change and operation performed on a Hudi table, encompassing writes, schema evolutions, compactions, cleanings, and clustering operations. This meticulous record-keeping empowers Hudi to deliver <a href="https://www.onehouse.ai/blog/acid-transactions-in-an-open-data-lakehouse" target="_blank" rel="noopener noreferrer">ACID guarantees</a>, robust <a href="https://hudi.apache.org/blog/2025/01/28/concurrency-control" target="_blank" rel="noopener noreferrer">concurrency control</a>, and advanced capabilities such as incremental processing, rollback/recovery, and time travel.</p>
<p>In essence, the timeline functions like a <a href="https://en.wikipedia.org/wiki/Write-ahead_logging" target="_blank" rel="noopener noreferrer">Write-Ahead Log (WAL)</a>, maintaining a sequence of immutable actions. Each action is recorded as a unique <em>instant</em> - a unit of work identified by its action type (e.g., commit, clean, compaction), a timestamp that marks when the action was initiated, and its lifecycle state. In Hudi, an <em>instant</em> refers to this combination of action, timestamp, and state (REQUESTED, INFLIGHT, or COMPLETED), and serves as the atomic unit of change on the timeline. These timeline entries are the backbone of Hudi’s transactional integrity, ensuring that every table change is atomically recorded and timeline-consistent. Every operation progresses through a lifecycle of <em>states</em>:</p>
<ul>
<li>REQUESTED: The action is planned and registered but not yet started.</li>
<li>INFLIGHT: The action is actively being performed, modifying table state.</li>
<li>COMPLETED: The action has successfully executed, and all data/metadata updates are finalized.</li>
</ul>
<p>These <em>instants</em> serve as both log entries and transaction markers, defining exactly what data is valid and visible at any given time. Whether you're issuing a snapshot query for the latest view, running an incremental query to fetch changes since the last checkpoint, or rolling back to a prior state, the timeline ensures that each action’s impact is precisely tracked. Every <em>action</em>, such as commit, clean, compaction, or rollback is explicitly recorded, allowing compute engines and tools to reason precisely about the table’s state transitions and history. This strict sequencing and lifecycle management also underpin Hudi’s ability to provide serializable isolation (the “I” in ACID) guarantees, ensuring that readers only observe committed data and consistent snapshots.</p>
<p>To optimize both performance and long-term storage scalability, Apache Hudi splits the timeline into two distinct components that work together to provide fast access to recent actions while ensuring historical records are retained efficiently. Let’s understand these in detail.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="active-timeline">Active Timeline<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#active-timeline" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>The <a href="https://hudi.apache.org/docs/timeline#active-timeline" target="_blank" rel="noopener noreferrer">Active timeline</a> is the front line of Hudi’s transaction log. It contains the most recent and in-progress actions that are critical for building a consistent and up-to-date view of the table. Every time a new operation, such as a data write, compaction, clean, or rollback is initiated, it is immediately recorded here as a new instant file under the <code>.hoodie/</code> directory. Each of these files holds metadata about the action’s lifecycle, moving through the standard states of REQUESTED → INFLIGHT → COMPLETED.</p>
<p>The active timeline is consulted constantly - whether you are issuing a query, running compaction, or planning a new write operation. Compute engines read from the active timeline to determine what data files are valid and visible, making it the source of truth for the table’s latest state. To maintain performance, Hudi enforces a retention policy on the active timeline, i.e. it deliberately keeps only a window of the most recent actions, ensuring the timeline remains lightweight and quick to scan.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="archived-timeline">Archived Timeline<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#archived-timeline" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Tables naturally accumulate many more actions over time, especially in high-ingestion or update environments. As the number of instants grows, the active timeline can become bloated if left unchecked, introducing latency and performance penalties during reads and writes.</p>
<p>To solve this, Hudi implements an archival process. Once the number of active instants crosses a configured threshold, older actions are offloaded from the active timeline into the Archived Timeline stored in the <code>.hoodie/archive/</code> directory. This design ensures that while the active timeline remains lean and fast for day-to-day operations, the complete transactional history of the table is still preserved for auditing, recovery, and time travel purposes.</p>
<p>Although the archived timeline is optimized for long-term retention, accessing deep history can incur higher latency and overhead, especially in workloads with a large number of archived instants. This limitation is precisely what set the stage for the LSM Timeline innovation introduced in Hudi 1.0.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="problem-statement---why-move-to-an-lsm-timeline">Problem Statement - Why move to an LSM Timeline?<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#problem-statement---why-move-to-an-lsm-timeline" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Apache Hudi’s original timeline design served well for many workloads. By maintaining a lightweight active timeline for fast operations and offloading historical instants to the archive, Hudi struck a balance between performance and durability. However, there were some aspects to think about with the previous timeline design.</p>
<ul>
<li>
<p><strong>Linear Growth</strong>: The timeline grows linearly with each table action, whether it’s a commit, compaction, clustering, or rollback. Although Hudi’s archival process offloads older instants to keep the active timeline lean, the total number of instants (active + archived) continues to grow unbounded in long-lived tables. Over time, the accumulation of these instants can inflate metadata size, leading to slower scans and degraded query planning performance, especially for use cases like time travel and incremental queries.</p>
</li>
<li>
<p><strong>Latency &amp; Cost</strong>: Accessing the archived timeline, which is often required for time-travel, or recovery operations introduces high read latencies. This is because the archival format was optimized for durability and storage efficiency (many small Avro files), not for fast access. As the number of archived instants balloons, reading deep history involves scanning and deserializing large volumes of metadata, increasing both latency and compute cost. This can noticeably slow down operations like incremental syncs and historical audits.</p>
</li>
<li>
<p><strong>Cloud Storage Limitations</strong>: In cloud object stores like S3 or GCS, appending to existing files is not supported (or is highly inefficient). As a result, every new archival batch creates new small files, leading to a small-file problem. Over time, these fragmented archives accumulate, creating operational challenges in storage management and performance bottlenecks during metadata access, especially when files must be scanned individually across large object stores.</p>
</li>
<li>
<p><strong>Emerging Use Cases</strong>: Apache Hudi has evolved to support next-generation features such as non-blocking concurrency control (NBCC), infinite time travel, and fine-grained transaction metadata. These capabilities place heavier demands on the timeline architecture, requiring high-throughput writes and faster lookups across both recent and historical data.</p>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introducing-the-lsm-timeline">Introducing the LSM Timeline<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#introducing-the-lsm-timeline" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>To overcome the scaling challenges of the original timeline architecture, Apache Hudi 1.0 introduced the <a href="https://hudi.apache.org/docs/timeline#timeline-components" target="_blank" rel="noopener noreferrer">LSM (Log-Structured Merge)</a> Timeline - a fundamentally new way to store and manage timeline metadata. This redesign brings together principles of <a href="https://en.wikipedia.org/wiki/Log-structured_merge-tree" target="_blank" rel="noopener noreferrer">log-structured storage</a>, tiered compaction, and snapshot versioning to deliver a highly scalable, cloud-native solution for tracking table history.</p>
<p>Hudi introduces a critical change in how time is represented on the timeline. Previously, Hudi treated time as instantaneous, i.e. each action appeared to take effect at a single instant. While effective for basic operations, this model proved limiting when implementing certain advanced features like <a href="https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control/" target="_blank" rel="noopener noreferrer">Non-Blocking Concurrency Control (NBCC)</a>, which require reasoning about actions as intervals of time to detect overlaps and resolve conflicts.</p>
<img src="https://hudi.apache.org/assets/images/blog/lsm_1.png" alt="index" width="800" align="middle">
<p>To address this, every action on the Hudi timeline now records both a <em>requested time</em> (when the action is initiated) and a <em>completion time</em> (when it finishes). This allows Hudi to track not just when an action was scheduled, but also how it interacts with other concurrent actions over time. To ensure global consistency across distributed processes, Hudi formalized the use of <a href="https://hudi.apache.org/docs/timeline#truetime-generation" target="_blank" rel="noopener noreferrer">TrueTime semantics</a>, guaranteeing that all instant times are monotonically increasing and globally ordered. This is a foundational requirement for precise conflict detection and robust transaction isolation.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-it-works--design">How It Works / Design<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#how-it-works--design" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<img src="https://hudi.apache.org/assets/images/blog/lsm_2.png" alt="index" width="800" align="middle">
<p>At its core, the LSM timeline replaces the flat archival model with a layered tree structure, allowing Hudi to manage metadata for millions of historical instants efficiently, without compromising on read performance or consistency. Here’s how it’s designed:</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="file-organization">File Organization<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#file-organization" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<ul>
<li>Metadata files are organized into layers (L0, L1, L2, …) following a Log-Structured Merge (LSM) tree layout.</li>
<li>Each file is a Parquet file that stores a batch of timeline instants. Their metadata entries are sorted chronologically by timestamp.</li>
<li>The files follow a precise naming convention: <code>${min_instant}_${max_instant}_${level}.parquet</code> where <code>min_instant</code> and <code>max_instant</code> represent the range of instants in the file and <code>level</code> denotes the layer (e.g., L0, L1, L2).</li>
<li>Files in the same layer may have overlapping time ranges, but the system tracks them via manifest files (more on that below).</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="compaction-strategy">Compaction Strategy<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#compaction-strategy" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<ul>
<li>The LSM timeline uses a universal compaction strategy, similar to designs seen in modern databases.</li>
<li>Whenever N files (default: 10) accumulate in a given layer (e.g., L0), they are merged and flushed into the next layer (e.g., L1).</li>
<li>Compaction is governed by a size-based policy (default max file size ~1 GB), ensuring that write amplification is controlled and files stay within optimal size limits.</li>
<li>There’s no hard limit on the number of layers. The LSM tree naturally scales to handle massive tables with deep histories.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="version--manifest-management-snapshot-isolation">Version &amp; Manifest Management: Snapshot Isolation<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#version--manifest-management-snapshot-isolation" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<ul>
<li>The LSM timeline introduces manifest files that record the current valid set of Parquet files representing the latest snapshot of the timeline.</li>
<li>Version files are generated alongside manifest files to maintain snapshot isolation, ensuring that readers and writers do not conflict.</li>
<li>This system supports multiple valid snapshot versions simultaneously (default: 3), enabling:<!-- -->
<ul>
<li>Consistent reads even during compaction.</li>
<li>Seamless evolution of the timeline without impacting query correctness.</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="reader-workflow">Reader Workflow<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#reader-workflow" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<ul>
<li>When a query is made on the timeline:<!-- -->
<ul>
<li>The engine first fetches the latest version file.</li>
<li>It reads the corresponding manifest file to get the list of valid data files.</li>
<li>It scans only the relevant Parquet files, often using timestamp-based filtering to skip irrelevant data early.</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="cleaning-strategy">Cleaning Strategy<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#cleaning-strategy" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<ul>
<li>The LSM timeline performs cleaning only after successful compaction, ensuring that no active snapshot is disrupted.</li>
<li>By default, Hudi retains 3 valid snapshot versions to support concurrent readers/writers.</li>
<li>Files are retained for at least 3 archival trigger intervals, providing a grace period before old data is purged.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="what-it-brings-to-the-table-benefits">What It Brings to the Table (Benefits)<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#what-it-brings-to-the-table-benefits" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>The LSM timeline unlocks significant advancements in how Apache Hudi handles metadata, providing both performance improvements and new capabilities.</p>
<ul>
<li>
<p><strong>Scalability:</strong> The LSM timeline architecture allows Hudi to manage virtually infinite timeline history while keeping both read and write performance predictable. Whether it's thousands or millions of instants, the layered compaction model ensures stable metadata performance over time, supporting efficient query and metadata access even as tables grow in size and history length.</p>
</li>
<li>
<p><strong>Efficient Reads:</strong> Readers benefit from manifest-guided lookups, allowing them to scan only the specific set of files relevant to their query. By using Parquet’s columnar format and timestamp-based filtering, Hudi dramatically reduces the overhead of accessing deep historical metadata.</p>
</li>
<li>
<p><strong>Non-Blocking Concurrency Control (NBCC):</strong> One of the most powerful capabilities enabled by the LSM timeline is Non-Blocking Concurrency Control, allowing multiple writers to operate concurrently on the same table (and even the same file group) without the need for explicit locks - except during final commit metadata updates.</p>
</li>
<li>
<p><strong>Cloud-Native Optimization</strong>: By compacting small files into large Parquet files, the LSM timeline avoids the small-file problem common in cloud storage systems like Amazon S3 or Google Cloud Storage. This improves both query performance and storage cost efficiency.</p>
</li>
<li>
<p><strong>Snapshot Isolation &amp; Consistency</strong>: The manifest + version file mechanism ensures that concurrent operations remain isolated and consistent, even as background compaction and cleaning occur. This provides strong transactional guarantees without sacrificing performance.</p>
</li>
<li>
<p><strong>Maintenance-Free Scalability</strong>: The universal compaction and smart cleaning strategies keep the timeline healthy over time, requiring minimal manual tuning, while ensuring that old data is cleaned up safely only after valid snapshots are no longer in use.</p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="performance">Performance<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#performance" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Micro-benchmarks show that the LSM Timeline scales efficiently even as the number of timeline actions grows by orders of magnitude. Reading just the instant times for <code>10,000</code> actions takes around <code>32ms</code>, while fetching full metadata takes <code>150ms</code>. At larger scales, such as <code>10 million</code> actions, metadata reads completes in about <code>162 seconds</code>.</p>
<p>These results demonstrate that Hudi's LSM timeline can handle decades of high-frequency commits (e.g., one every 30 seconds for 10+ years) while keeping metadata access performant.</p>
<p>The LSM timeline represents a natural progression in Apache Hudi’s timeline architecture, designed to address the growing demands of large-scale and long-lived tables. Hudi’s timeline has been foundational for transactional integrity, time travel, and incremental processing capabilities. The new LSM-based design enhances scalability and operational efficiency by introducing a layered, compacted structure with manifest-driven snapshot isolation. This improvement allows Hudi to manage extensive metadata histories more efficiently, maintain predictable performance, and better support advanced use cases such as non-blocking concurrency control.</p>
<hr>]]></content>
        <author>
            <name>Dipankar Mazumdar</name>
        </author>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="LSM Tree" term="LSM Tree"/>
        <category label="Performance" term="Performance"/>
        <category label="Non-Blocking Concurrency Control" term="Non-Blocking Concurrency Control"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Doris + Hudi Turned the Impossible Into the Everyday]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/04/14/doris-hudi-making-impossible-possible</id>
        <link href="https://hudi.apache.org/cn/blog/2025/04/14/doris-hudi-making-impossible-possible"/>
        <updated>2025-04-14T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://dzone.com/articles/doris-hudi-making-impossible-possible">here</a></span>]]></content>
        <author>
            <name>Zen Hua</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="Apache Doris" term="Apache Doris"/>
        <category label="use-case" term="use-case"/>
        <category label="federated querying" term="federated querying"/>
        <category label="dzone" term="dzone"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why Walmart Chose Apache Hudi for Their Lakehouse]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/04/09/why-walmart-chose-apache-hudi-for-their-lakehouse</id>
        <link href="https://hudi.apache.org/cn/blog/2025/04/09/why-walmart-chose-apache-hudi-for-their-lakehouse"/>
        <updated>2025-04-09T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://blog.det.life/why-walmart-chose-apache-hudi-for-their-lakehouse-c0a3574db0ba">here</a></span>]]></content>
        <author>
            <name>Vu Trinh</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="use-case" term="use-case"/>
        <category label="det" term="det"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[ From Swamp to Stream: How Apache Hudi Transforms the Modern Data Lake]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/04/06/from-swamp-to-stream-how-apache-hudi-transforms-the-modern-data-lake</id>
        <link href="https://hudi.apache.org/cn/blog/2025/04/06/from-swamp-to-stream-how-apache-hudi-transforms-the-modern-data-lake"/>
        <updated>2025-04-06T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://medium.com/aimonks/from-swamp-to-stream-how-apache-hudi-transforms-the-modern-data-lake-8a938f517ea1">here</a></span>]]></content>
        <author>
            <name>Everton Gomede</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="real-time datalake" term="real-time datalake"/>
        <category label="incremental processing" term="incremental processing"/>
        <category label="upserts" term="upserts"/>
        <category label="medium" term="medium"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integrating Apache Doris and Hudi for Data Querying and Migration]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/04/03/integrate-apache-doris-hudi-data-querying-migration</id>
        <link href="https://hudi.apache.org/cn/blog/2025/04/03/integrate-apache-doris-hudi-data-querying-migration"/>
        <updated>2025-04-03T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://dzone.com/articles/integrate-apache-doris-hudi-data-querying-migration">here</a></span>]]></content>
        <author>
            <name>li yy</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="Apache Doris" term="Apache Doris"/>
        <category label="real-time query" term="real-time query"/>
        <category label="how-to" term="how-to"/>
        <category label="dzone" term="dzone"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing Secondary Index in Apache Hudi Lakehouse Platform]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/04/02/secondary-index</id>
        <link href="https://hudi.apache.org/cn/blog/2025/04/02/secondary-index"/>
        <updated>2025-04-02T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Apache Hudi 1.0 introduces Secondary Indexes, enabling faster queries on non-primary key fields. This improves data retrieval in Lakehouse architectures by reducing data scans. Hudi also offers asynchronous indexing for scalability and efficient index maintenance without disrupting data ingestion. By the end of this blog, you'll understand how these features enhance Hudi's capabilities as a high-performance lakehouse platform.]]></summary>
        <content type="html"><![CDATA[<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>TL;DR</div><div class="admonitionContent_BuS1"><p>Apache Hudi 1.0 introduces Secondary Indexes, enabling faster queries on non-primary key fields. This improves data retrieval in Lakehouse architectures by reducing data scans. Hudi also offers asynchronous indexing for scalability and efficient index maintenance without disrupting data ingestion. By the end of this blog, you'll understand how these features enhance Hudi's capabilities as a high-performance lakehouse platform.</p></div></div>
<p>Indexes are a fundamental data structure that enables efficient data retrieval by eliminating the need to scan the entire dataset for every query. In the context of a Lakehouse, where records are written as immutable data files (such as Parquet) at scale, indexing becomes crucial in reducing lookup times. Otherwise, a lot of time will be spent by the compute engine on finding out where exactly a particular record exists amongst thousands of files in the data lake storage, which is computationally expensive at scale. Indexing is not only important for <em>reads</em> in a lakehouse architecture, but also for <em>writes</em>, such as upserts and deletes, as you need to know where the record is to update it.</p>
<p>One of the standout design choices in Apache Hudi that separates it from other lakehouse formats is its <a href="https://hudi.apache.org/docs/next/indexes/" target="_blank" rel="noopener noreferrer">indexing</a> capability, which has been central to its architecture from the beginning. Hudi is heavily optimized to handle mutable change streams with varying write patterns, and indexing plays a pivotal role in making upserts and deletes efficient.</p>
<p>Hudi's indexing mechanism is designed to efficiently manage record lookups and updates by maintaining a structured mapping between records and file groups. Here's how it works:</p>
<ul>
<li>
<p>The first time a record is ingested into Hudi, it is assigned to a <a href="https://hudi.apache.org/tech-specs/#file-layout-hierarchy" target="_blank" rel="noopener noreferrer">File Group</a> - a logical grouping of files. This assignment typically remains unchanged throughout the record's lifecycle. However, in cases such as clustering or cross-partition updates, the record may be remapped to a different file group. Even in such scenarios, Hudi ensures that a given record key is associated with exactly one file group at any completed instant on the timeline</p>
</li>
<li>
<p>Hudi maintains a mapping between the incoming <a href="https://hudi.apache.org/docs/key_generation" target="_blank" rel="noopener noreferrer">record’s key</a> (unique identifier) and the File Group where it resides.</p>
</li>
<li>
<p>The index is responsible for quickly locating records based on this File Group mapping, eliminating the need for full dataset scans.</p>
</li>
</ul>
<p>This strategy allows Hudi to determine whether a record exists and pinpoint its exact location, enabling faster upserts and deletes.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="apache-hudis-multi-modal-indexing-system">Apache Hudi's Multi-Modal Indexing System<a href="https://hudi.apache.org/cn/blog/2025/04/02/secondary-index#apache-hudis-multi-modal-indexing-system" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>While Hudi’s indexes have set a benchmark for fast writes, bringing those advantages to queries was equally important. This led to the design of a generalized indexing subsystem that enhances performance in the lakehouse. Hudi’s <a href="https://www.onehouse.ai/blog/introducing-multi-modal-index-for-the-lakehouse-in-apache-hudi" target="_blank" rel="noopener noreferrer">multi-modal indexing</a> redefines indexing in data lakes by employing multiple index types, each optimized for different workloads and query patterns. It is built on scalable metadata that supports multiple index types without extra overhead, ACID-compliant updates to keep indexes in sync with the data table, and optimized lookups that minimize full scans for low-latency queries on large datasets.</p>
<p>At the core of Hudi’s indexing design is its <a href="https://hudi.apache.org/docs/metadata" target="_blank" rel="noopener noreferrer">metadata table</a>, a specialized Merge-on-Read table that houses multiple index types as separate partitions. These indexes serve various purposes, improving the efficiency of reads, writes, and upserts.</p>
<img src="https://hudi.apache.org/assets/images/blog/hudi-stack-indexes.png" alt="index" width="800" align="middle">
<p>Some key indexes within Hudi’s metadata table include:</p>
<ul>
<li>File Index - Stores a compact listing of files, reducing the overhead of expensive file system operations.</li>
<li>Column Stats Index - Tracks min/max statistics for each column, enabling more effective data pruning.</li>
<li>Bloom Filter Index - Stores precomputed bloom filters for all data files, optimizing record lookups.</li>
<li>Partition Stats Index - Stores aggregated partition-related information which helps in efficient partition pruning by skipping entire folders very quickly.</li>
<li>Record-Level Index - Maintains direct mappings to individual records, facilitating faster upserts and deletes.</li>
<li>Secondary Index - Allow users to create indexes on columns that are not part of record key columns in Hudi tables.</li>
</ul>
<p>By structuring these indexes as individual partitions within the metadata table, Hudi ensures efficient retrieval, quick lookups, and scalability, even as the data volume grows. In this blog, we will focus on secondary indexes and understand how it can help accelerate query performance in a lakehouse.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introducing-secondary-index">Introducing Secondary Index<a href="https://hudi.apache.org/cn/blog/2025/04/02/secondary-index#introducing-secondary-index" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>A secondary index is an indexing mechanism commonly used in database systems to provide efficient access to records based on non-primary key attributes. Unlike primary indexes, which enforce uniqueness and define the main data layout, secondary indexes serve as auxiliary data structures that accelerate lookups on fields that are frequently queried but are not the primary key.</p>
<p>For example, in an OLTP (Online Transaction Processing) database, a primary index might be defined on a unique <code>order_id</code>, whereas a secondary index could be created on <code>customer_id</code> to quickly fetch all orders placed by a specific customer. Secondary indexes enhance query performance by reducing the need for full table scans, especially in analytical workloads that involve complex filtering or joins.</p>
<p>With <a href="https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0/" target="_blank" rel="noopener noreferrer">Hudi 1.0</a>, Apache Hudi introduces <a href="https://hudi.apache.org/docs/next/indexes#secondary-index" target="_blank" rel="noopener noreferrer">secondary indexes</a>, bringing database-style indexing capabilities to the Lakehouse. Secondary indexes allow queries to scan significantly fewer files, reducing query latency and compute costs. This is especially beneficial for cloud-based query engines (such as AWS Athena), where pricing is based on the amount of data scanned. A secondary index in Hudi allows users to index any column beyond the record key (primary key), making queries on non-primary key fields much faster. This extends Hudi’s existing <a href="https://hudi.apache.org/blog/2023/11/01/record-level-index/" target="_blank" rel="noopener noreferrer">record-level index</a>, which optimizes writes and reads based on the record key.</p>
<img src="https://hudi.apache.org/assets/images/blog/secondary_index.png" alt="sec_index" width="800" align="middle">
<p>Here is how the secondary index works in Hudi.</p>
<ul>
<li>Indexes Non-Primary Key Columns: Unlike the record-level index, which tracks record keys, secondary indexes help accelerate queries on fields outside the primary key.</li>
<li>Stores Mappings Between Secondary and Primary Keys: Hudi maintains a mapping between secondary keys (e.g., city, driver) and record keys, enabling fast lookups for non-primary key queries.</li>
<li>Minimizes Data Scans via Index-Aware Query Execution: During query execution, the secondary index enables data skipping, allowing Hudi to prune unnecessary files before scanning.</li>
<li>SQL-Based Index Management: Users can create, drop, and manage indexes using SQL, making secondary indexes easily accessible.</li>
</ul>
<p>Hudi supports hash-based secondary indexes, which are horizontally scalable by distributing keys across shards for fast writes and lookups.</p>
<p>If you are interested in the implementation details of secondary indexes, you can read more <a href="https://hudi.apache.org/tech-specs-1point0/#secondary-index" target="_blank" rel="noopener noreferrer">here</a>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="creating-a-secondary-index-in-hudi">Creating a Secondary Index in Hudi<a href="https://hudi.apache.org/cn/blog/2025/04/02/secondary-index#creating-a-secondary-index-in-hudi" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>In Hudi 1.0, secondary indexes are supported currently in Apache Spark, with future support planned for Flink, Presto, and Trino in Hudi 1.1.</p>
<p>Let’s see an example of creating a Hudi table with a secondary index.</p>
<p>First, let’s create a table with a record index enabled. The record index maintains mappings of record keys (<code>id</code>) to file groups, enabling fast updates, deletes, and lookups.</p>
<div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">DROP</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">TABLE</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">IF</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">EXISTS</span><span class="token plain"> hudi_table</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">TABLE</span><span class="token plain"> hudi_table </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ts </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">BIGINT</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    id STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    rider STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    driver STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    fare </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">DOUBLE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    city STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    state STRING</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">USING</span><span class="token plain"> hudi</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">OPTIONS </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    primaryKey </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'id'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    hoodie</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">metadata</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">record</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">index</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">enable</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'true'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)">-- Enable record index</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    hoodie</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">write</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">record</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">merge</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">mode</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"COMMIT_TIME_ORDERING"</span><span class="token plain"> </span><span class="token comment" style="color:rgb(98, 114, 164)">-- Only Required for 1.0.0 version</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">PARTITIONED </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">BY</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">city</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> state</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">LOCATION </span><span class="token string" style="color:rgb(255, 121, 198)">'file:///tmp/hudi_test_table'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><br></span></code></pre></div></div>
<p>Now we can create a secondary index on the <code>city</code> field to optimize queries filtering on this column.</p>
<div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INDEX</span><span class="token plain"> idx_city </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">ON</span><span class="token plain"> hudi_table </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">USING</span><span class="token plain"> secondary_index</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">city</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><br></span></code></pre></div></div>
<p>Now, when executing a query such as:</p>
<div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">SELECT</span><span class="token plain"> rider </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">FROM</span><span class="token plain"> hudi_table </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">WHERE</span><span class="token plain"> city </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'SFO'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><br></span></code></pre></div></div>
<p>✅ Hudi first checks the secondary index to determine which records match the filter condition.<br>
<!-- -->✅ It then uses the record index to locate the exact file group for retrieval.<br>
<!-- -->✅ Data skipping is applied, reducing the number of files read from cloud storage.</p>
<p>Users can also create secondary indexes using the Spark DataSource API by setting the following configurations:</p>
<table><thead><tr><th style="text-align:left">Config Name</th><th style="text-align:left">Default</th><th style="text-align:left">Description</th></tr></thead><tbody><tr><td style="text-align:left"><code>hoodie.metadata.index.secondary.enable</code></td><td style="text-align:left">true</td><td style="text-align:left">Enables secondary index maintenance. When true, Hudi writers automatically maintain all secondary indexes within the metadata table. When disabled, secondary indexes must be created manually using SQL.</td></tr><tr><td style="text-align:left"><code>hoodie.datasource.write.secondarykey.column</code></td><td style="text-align:left">(N/A)</td><td style="text-align:left">Specifies the columns to be used as secondary keys. Supports dot notation for nested fields (e.g., <code>customer.region</code>).</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="asynchronous-indexing-in-hudi">Asynchronous Indexing in Hudi<a href="https://hudi.apache.org/cn/blog/2025/04/02/secondary-index#asynchronous-indexing-in-hudi" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>A notable thing about Hudi’s indexing system is that it offers <a href="https://www.onehouse.ai/blog/asynchronous-indexing-using-hudi" target="_blank" rel="noopener noreferrer">asynchronous indexing</a> as a service. Traditional indexing approaches often introduce performance bottlenecks, as index maintenance needs to be performed synchronously with writes. Hudi’s asynchronous indexing service eliminates the performance bottlenecks of traditional indexing by decoupling index maintenance from ingestion. Instead of requiring synchronous updates that slow down writes, Hudi builds indexes in the background, ensuring ingestion remains uninterrupted.</p>
<p>A key aspect of this design is timeline-consistent indexing, where a new indexing action is introduced in Hudi’s transactional <a href="https://hudi.apache.org/docs/timeline" target="_blank" rel="noopener noreferrer">timeline</a>. The indexer service schedules indexing by adding an <code>indexing.requested</code> instant, moves it to <code>inflight</code> during execution, and finally marks it <code>completed</code> once indexing is done, without locking index file writes. This enables a scalable indexing framework, allowing indexes to be dynamically added or removed without downtime as datasets grow. Async indexing also supports multiple index types, including secondary indexes.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="benchmarking">Benchmarking<a href="https://hudi.apache.org/cn/blog/2025/04/02/secondary-index#benchmarking" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>We ran a simple benchmark using the TPCDS 1TB dataset, created the index on one of the fact table <code>web_sales</code> and ran a complex join query with lookup on customer id.</p>
<p><strong>Setup:</strong></p>
<ul>
<li>Uses 1TB TPCDS public dataset.</li>
<li>Apache Spark version -  3.5.5 installed on EMR cluster</li>
<li>Apache Hudi version - 1.0.1</li>
<li>Table on which secondary index is created - <code>web_sales</code></li>
<li>Column on which Secondary Index is created - <code>ws_ship_customer_sk</code></li>
<li>Cluster Configurations<!-- -->
<ul>
<li>Nodes: m5.xlarge (10 executors)</li>
</ul>
</li>
</ul>
<p>To evaluate performance, we executed the same query multiple times within the same Spark session. The table below demonstrates an approximately <strong>33%</strong> improvement in the first run and a <strong>58%</strong> improvement in the second run. Additionally, the amount of data scanned was reduced by <strong>90%</strong> when using the secondary index.</p>
<table><thead><tr><th style="text-align:left"></th><th style="text-align:left">Run 1</th><th style="text-align:left">Run 2</th><th style="text-align:left">Files Read</th><th style="text-align:left">File Size Read</th><th style="text-align:left">Rows Scanned</th></tr></thead><tbody><tr><td style="text-align:left">Without Secondary index</td><td style="text-align:left">32 sec</td><td style="text-align:left">14 sec</td><td style="text-align:left">5000</td><td style="text-align:left">67 GB</td><td style="text-align:left">719 M</td></tr><tr><td style="text-align:left">With Secondary Index</td><td style="text-align:left">22 sec</td><td style="text-align:left">6 sec</td><td style="text-align:left">521</td><td style="text-align:left">7 GB</td><td style="text-align:left">75 M</td></tr></tbody></table>
<p><strong>Read Query used for Benchmarking:</strong></p>
<div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">SELECT</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   ws</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ws_order_number</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   ws</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ws_item_sk</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   ws</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ws_quantity</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   ws</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ws_sales_price</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   c</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">c_customer_id</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   c</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">c_first_name</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   c</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">c_last_name</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   d</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">d_date</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   wp</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">wp_web_page_id</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">FROM</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   web_sales ws</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">JOIN</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   tpcds_hudi_1tb</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">customer c </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">ON</span><span class="token plain"> ws</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ws_ship_customer_sk </span><span class="token operator">=</span><span class="token plain"> c</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">c_customer_sk</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">JOIN</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   tpcds_hudi_1tb</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">date_dim d </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">ON</span><span class="token plain"> ws</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ws_ship_date_sk </span><span class="token operator">=</span><span class="token plain"> d</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">d_date_sk</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">JOIN</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   tpcds_hudi_1tb</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">web_page wp </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">ON</span><span class="token plain"> ws</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ws_web_page_sk </span><span class="token operator">=</span><span class="token plain"> wp</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">wp_web_page_sk</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">WHERE</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   ws</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ws_ship_customer_sk </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'647632'</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">ORDER</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">BY</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   ws</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">ws_order_number</span><br></span></code></pre></div></div>
<p>As shown in the DAG below, there is a significant difference in the amount of data scanned and other metrics (see the highlighted part) for the websales table, both with and without the secondary index.</p>
<p><strong>Spark SQL Stats  with Secondary index</strong></p>
<img src="https://hudi.apache.org/assets/images/blog/sec_index_spark1.png" alt="orch" width="600" align="middle">
<p><strong>Spark SQL Stats  without Secondary index</strong></p>
<img src="https://hudi.apache.org/assets/images/blog/sec_index_spark2.png" alt="orch" width="600" align="middle">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://hudi.apache.org/cn/blog/2025/04/02/secondary-index#conclusion" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Indexing has been a core component of Apache Hudi since its inception, enabling efficient upserts and deletes at scale. With Hudi 1.0, the introduction of secondary indexing expands these capabilities by allowing queries to efficiently filter and retrieve records based on <em>non-primary key</em> fields, significantly reducing data scans and improving query performance. Looking ahead, secondary indexing in Hudi opens new possibilities for further optimizations, such as accelerating complex joins and MERGE INTO operations.</p>
<p>Additionally, to ensure that index maintenance does not introduce bottlenecks, Hudi’s <em>asynchronous indexing</em> service decouples index updates from ingestion, enabling seamless scaling while keeping indexes timeline-consistent and ACID-compliant. These advancements further solidify Hudi’s role as a high-performance lakehouse platform, making data structures such as secondary indexes more accessible.</p>
<hr>]]></content>
        <author>
            <name>Dipankar Mazumdar, Aditya Goenka</name>
        </author>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="Indexing" term="Indexing"/>
        <category label="Performance" term="Performance"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Powering Amazon Unit Economics at Scale Using Apache Hudi]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi</id>
        <link href="https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi"/>
        <updated>2025-03-31T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Amazon’s Profit Intelligence team built Nexus, a configuration-driven platform powered by Apache Hudi, to scale unit economics across thousands of retail use cases. Nexus manages over 1,200 tables, processes hundreds of billions of rows daily, and handles ~1 petabyte of data churn each month. This blog dives into their data lakehouse journey, Nexus architecture, Hudi integration, and key operational learnings.]]></summary>
        <content type="html"><![CDATA[<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>TL;DR</div><div class="admonitionContent_BuS1"><p>Amazon’s Profit Intelligence team built Nexus, a configuration-driven platform powered by Apache Hudi, to scale unit economics across thousands of retail use cases. Nexus manages over 1,200 tables, processes hundreds of billions of rows daily, and handles ~1 petabyte of data churn each month. This blog dives into their data lakehouse journey, Nexus architecture, Hudi integration, and key operational learnings.</p></div></div>
<p>Understanding and improving unit-level profitability at Amazon's scale is a massive challenge - one that requires flexibility, precision, and operational efficiency. In this blog, we walk through how Amazon’s Profit Intelligence team built a scalable, configuration-driven platform called Nexus, and how Apache Hudi became the cornerstone of its data lake architecture.</p>
<p>By combining declarative configuration with Hudi's advanced table management capabilities, the team has enabled thousands of retail business use cases to run seamlessly, allowing finance and pricing teams to self-serve insights on cost and profitability, without constantly relying on engineering intervention.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-business-need-profit-intelligence-and-unit-economics">The Business Need: Profit Intelligence and Unit Economics<a href="https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi#the-business-need-profit-intelligence-and-unit-economics" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Within Amazon’s Worldwide Stores, the Selling Partner Services (SPS) team supports seller-facing operations. A key part of this effort is computing <strong>Contribution Profit</strong> - a granular metric that captures revenue, costs, and profitability at the unit level, such as <em>a shipped item to the customer</em>.</p>
<p>Contribution Profit powers decision-making for a range of downstream teams including:</p>
<ul>
<li>Pricing</li>
<li>Forecasting</li>
<li>Finance</li>
</ul>
<p>The challenge? Supporting the scale and diversity of retail use cases across Amazon's global business, while maintaining a data platform that's both extensible and maintainable.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="amazons-data-lakehouse-journey">Amazon’s Data Lakehouse Journey<a href="https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi#amazons-data-lakehouse-journey" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Over the past decade, the architecture behind Contribution Profit has gone through several phases of evolution, driven by the need to better support Amazon’s growing and diverse retail business use cases.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="early-phase">Early Phase<a href="https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi#early-phase" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<img src="https://hudi.apache.org/assets/images/blog/fig1_amz.png" alt="redshift" width="800" align="middle">
<p>Initial implementations relied on ETL pipelines that published data to Redshift, often with unstructured job flows. Business logic could exist at various layers of the ETL and was written entirely in SQL, making it difficult to track, maintain, or modify. These pipelines lacked systematic enforcement of patterns, which led to fragmentation and technical debt.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="intermediate-phase">Intermediate Phase<a href="https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi#intermediate-phase" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<img src="https://hudi.apache.org/assets/images/blog/fig2_amz.png" alt="flink" width="800" align="middle">
<p>To improve scalability and support streaming workloads, the team transitioned to a setup involving Apache Flink and a custom-built data lake. Although this introduced broader data processing flexibility, it still had major drawbacks:</p>
<ul>
<li>Redshift-based ETLs remained in use.</li>
<li>Business logic and schema changes required engineering involvement.</li>
<li>There were ongoing scalability and maintainability issues with the custom data lake.</li>
<li>Flink introduced operational challenges of its own, such as handling version upgrades through AWS Managed Flink and providing done signal in batch operation.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="current-state-nexus--apache-hudi">Current State: Nexus + Apache Hudi<a href="https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi#current-state-nexus--apache-hudi" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Each of the prior approaches came with tradeoffs, especially around business logic being tightly coupled with code, making it hard for non-engineers to simulate or change metrics for a specific retail business.</p>
<p>Recognizing the need for better abstraction and operational maturity, the team built Nexus - a configuration-driven platform for defining and orchestrating data pipelines. All lake interactions including ingestion, transformation, schema evolution, and table management now go through Nexus. Nexus is powered by <a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer"><strong>Apache Hudi</strong></a>, which provides the foundation for scalable ingestion, efficient upserts, schema evolution, and transactional guarantees on Amazon S3.</p>
<p>This new architecture enabled the team to decouple business logic from engineering code, allowing business teams to define logic declaratively. It also introduced standardization across workloads, eliminated redundant pipelines, and laid the groundwork for scaling unit economics calculations across thousands of use cases.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="key-modules-of-nexus">Key Modules of Nexus<a href="https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi#key-modules-of-nexus" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<img src="https://hudi.apache.org/assets/images/blog/fig3_amz.png" alt="nexus" width="800" align="middle">
<p>Nexus consists of four core components:</p>
<p><strong>Configuration Layer</strong></p>
<p>The topmost layer where users define their business logic in a declarative format. These configurations are typically generated and enriched with metadata by internal systems.</p>
<p><strong>NexusFlow (Orchestration)</strong></p>
<img src="https://hudi.apache.org/assets/images/blog/fig4_amz.png" alt="orch" width="1000" align="middle">
<p align="center"><em>Figure: Sample NexusFlow Config</em></p>
<p>Responsible for generating and executing workflows. It operates on two levels:</p>
<ul>
<li>Logical Layer: Comprising NexusETL jobs and other tasks.</li>
<li>Physical Layer: Implemented via AWS Step Functions to orchestrate EMR jobs and related dependencies. NexusFlow supports extensibility through a federated model and can execute diverse task types like Spark jobs, Redshift queries, wait conditions, and legacy ETLs.</li>
</ul>
<p><strong>NexusETL (Execution)</strong></p>
<img src="https://hudi.apache.org/assets/images/blog/fig5_amz.png" alt="etl" width="1000" align="middle">
<p align="center"><em>Figure: Sample NexusETL Config</em></p>
<p>Executes Spark-based data transformation jobs. Jobs are defined entirely in configuration, with support for:</p>
<ul>
<li>Built-in transforms like joins and filters</li>
<li>Custom UDFs</li>
<li>Source/Sink/Transform operators: It operates at the job abstraction level and is typically invoked by NexusFlow during orchestration.</li>
</ul>
<p><strong>NexusDataLake (Storage)</strong></p>
<img src="https://hudi.apache.org/assets/images/blog/fig5_amz.png" alt="datalake" width="1000" align="middle">
<p align="center"><em>Figure: Sample NexusDataLake Config</em></p>
<p>A storage abstraction layer built on Apache Hudi. NexusDataLake manages:</p>
<ul>
<li>Table creation</li>
<li>Schema inference and evolution</li>
<li>Catalog integration: All interactions with Hudi, such as inserts, upserts, table schema changes, and metadata syncs are funneled through NexusETL and NexusFlow, maintaining consistency across the platform.</li>
</ul>
<p>By standardizing how data is defined, processed, and stored, Nexus has enabled a scalable, maintainable, and extensible architecture. Every data lake interaction - from ingestion to table maintenance, is performed through this configuration-first model, which now powers hundreds of use cases across Amazon retail.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-apache-hudi">Why Apache Hudi?<a href="https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi#why-apache-hudi" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Apache Hudi has been central to Nexus’ success, providing the core data lake storage layer for scalable ingestion, updates, and metadata management. It enables fast, incremental updates at massive scale while maintaining transactional guarantees on top of Amazon S3.</p>
<p>In Amazon’s current architecture:</p>
<ul>
<li>Copy-on-Write (COW) table type is used for all Hudi tables.</li>
<li>Workloads generate hundreds of billions of row updates daily, with write patterns spanning concentrated single-partition updates and wide-range backfills across up to 90 partitions.</li>
<li>All Hudi interactions, including inserts, schema changes, and metadata syncs, are managed through Nexus.</li>
</ul>
<p><strong>Key Capabilities used with Apache Hudi</strong></p>
<ul>
<li>
<p><strong>Efficient Upserts</strong><br>
<!-- -->Hudi’s design primitives such as <a href="https://hudi.apache.org/docs/indexes" target="_blank" rel="noopener noreferrer">indexes</a> for Copy-on-Write (CoW) tables enable high-throughput update patterns by avoiding the need to join against the entire dataset to determine which files to rewrite, which is particularly critical for our daily workloads.</p>
</li>
<li>
<p><strong>Incremental Processing</strong><br>
<!-- -->By using Hudi’s native <a href="https://www.onehouse.ai/blog/getting-started-incrementally-process-data-with-apache-hudi" target="_blank" rel="noopener noreferrer">incremental pull</a> capabilities, downstream systems are able to consume only the changes between commits. This is essential for efficiently updating Contribution Profit metrics that power business decision-making.</p>
</li>
<li>
<p><strong>Metadata Table</strong><br>
<!-- -->Enabling the <a href="https://hudi.apache.org/docs/metadata" target="_blank" rel="noopener noreferrer">metadata table</a> (<code>hoodie.metadata.enable=true</code>) significantly reduced job runtimes by avoiding expensive file listings on S3. This is an important optimization given the scale at which we process updates across more than 1200 Hudi tables.</p>
</li>
<li>
<p><strong>Schema Evolution</strong><br>
<!-- -->Table creation and evolution are fully managed through configuration in Nexus. Hudi’s built-in support for <a href="https://hudi.apache.org/docs/schema_evolution" target="_blank" rel="noopener noreferrer">schema evolution</a> has allowed the team to onboard new use cases and adapt to changing schemas without requiring expensive rewrites or manual interventions.</p>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-learnings-from-operating-hudi-at-amazon-scale">Key Learnings from Operating Hudi at Amazon Scale<a href="https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi#key-learnings-from-operating-hudi-at-amazon-scale" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Operating Apache Hudi at the scale and velocity required by Amazon’s Profit Intelligence workloads surfaced a set of hard-earned lessons, especially around concurrency, metadata handling, and cost optimization. These learnings reflect both architectural refinements and operational trade-offs that others adopting Hudi at large scale may find useful.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-concurrency-control">1. Concurrency Control<a href="https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi#1-concurrency-control" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>At Amazon’s ingestion scale - hundreds of billions of rows per day and thousands of concurrent table updates, multi-writer concurrency is a reality, not an edge case.</p>
<p>The team initially used Optimistic Concurrency Control (OCC), which works well in environments with low write conflicts. OCC assumes that concurrent writers rarely overlap, and when they do, the job retries after detecting a conflict. However, in high-contention scenarios, like multiple jobs writing to the same partition within a short time window, this led to frequent retries and job failures.</p>
<p>To resolve this, the team pivoted to a new table structure designed to minimize concurrent insertions. This change helped reduce contention by lowering the likelihood of multiple writers operating on overlapping partitions simultaneously. The updated design enabled using OCC while avoiding the excessive retries and failures we had initially encountered.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-metadata-table-management-async-vs-sync-trade-offs">2. Metadata Table Management: Async vs Sync Trade-Offs<a href="https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi#2-metadata-table-management-async-vs-sync-trade-offs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Apache Hudi’s metadata table dramatically improves performance by avoiding expensive file listings on cloud object stores like S3. It maintains a persistent <em>index</em> <em>of files</em>, enabling faster operations such as file pruning, and data skipping.</p>
<p>The team enabled Hudi’s metadata table early (<code>hoodie.metadata.enable=true</code>) and started off with synchronous cleaning but switched to asynchronous cleaning to reduce job runtime. However, we ran into an issue when experimenting with asynchronous cleaning. Due to a <a href="https://github.com/apache/hudi/issues/11535" target="_blank" rel="noopener noreferrer">known issue (#11535)</a>, async cleaning wasn’t properly cleaning up metadata entries.</p>
<p>To ensure the metadata tables were properly cleaned, we switched all of  our Hudi workloads back to synchronous cleaning.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-cost-management">3. Cost Management<a href="https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi#3-cost-management" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>While Apache Hudi helped Amazon reduce data duplication and improve ingestion efficiency, we quickly realized that operational costs were not driven by storage - but by the API interaction patterns with S3.</p>
<p>Breakdown of the cost profile:</p>
<ul>
<li><strong>70% of total cost</strong> came from <code>PUT</code> requests (writes)</li>
<li>Combined <code>PUT + GET</code> operations accounted for <strong>80%</strong> of the bill</li>
<li>Storage cost remained a small fraction, even with 3+ PB of total data under management</li>
</ul>
<p>Their data ingestion patterns contributed to this:</p>
<ul>
<li>Daily workloads: Heavy concentration (99%) of updates into a single partition</li>
<li>Backfill workloads: Spread evenly across 30–90 partitions</li>
</ul>
<p>To manage this:</p>
<ul>
<li>We moved to <strong>S3 Intelligent-Tiering</strong> to reduce unused data storage costs</li>
<li>Enabled <strong>EMR cluster auto-scaling</strong> to dynamically adjust compute resources</li>
<li>Batched writes and carefully tuned Hudi configurations (e.g., <code>write.batch.size</code>, <code>compaction.small.file.limit</code>) to reduce unnecessary file churn</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="operational-scale-nexus-by-the-numbers">Operational Scale: Nexus by the Numbers<a href="https://hudi.apache.org/cn/blog/2025/03/31/amazon-hudi#operational-scale-nexus-by-the-numbers" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<table><thead><tr><th style="text-align:left">Metric</th><th style="text-align:left">Value</th></tr></thead><tbody><tr><td style="text-align:left">Tables Managed</td><td style="text-align:left">1200+ (5–15 updates/day per table)</td></tr><tr><td style="text-align:left">Legacy SQL Deprecated</td><td style="text-align:left">300,000+ lines</td></tr><tr><td style="text-align:left">Total Data Managed</td><td style="text-align:left">~3 Petabytes</td></tr><tr><td style="text-align:left">Monthly Data Changes</td><td style="text-align:left">~1 Petabyte added/deleted</td></tr><tr><td style="text-align:left">Daily Record Updates</td><td style="text-align:left">Hundreds of billions</td></tr><tr><td style="text-align:left">Developer Time Saved</td><td style="text-align:left">300+ days</td></tr></tbody></table>
<p>Nexus with Apache Hudi as the foundation has significantly improved the scale, modularity, and maintainability of the data lake operations at Amazon. As the business use cases scale, the team is also focused on managing the increasing complexity of the data lake, while ensuring that both technical and non-technical stakeholders can interact with Nexus effectively.</p>
<p>This blog is based on Amazon’s presentation at the Hudi Community Sync. If you are interested in watching the recorded version of the video, you can find it <a href="https://www.youtube.com/watch?v=rMXhlb7Uci8" target="_blank" rel="noopener noreferrer">here</a>.</p>
<hr>]]></content>
        <author>
            <name>Jason, Abhishek, Sethu in collaboration with Dipankar</name>
        </author>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="Amazon" term="Amazon"/>
        <category label="Community" term="Community"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[ACID Transactions in an Open Data Lakehouse]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/03/26/acid-transactions</id>
        <link href="https://hudi.apache.org/cn/blog/2025/03/26/acid-transactions"/>
        <updated>2025-03-26T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://www.onehouse.ai/blog/acid-transactions-in-an-open-data-lakehouse">here</a></span>]]></content>
        <author>
            <name>Dipankar Mazumdar</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="Apache Iceberg" term="Apache Iceberg"/>
        <category label="Delta Lake" term="Delta Lake"/>
        <category label="ACID" term="ACID"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is Clustering in an Open Data Lakehouse?]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/03/26/clustering</id>
        <link href="https://hudi.apache.org/cn/blog/2025/03/26/clustering"/>
        <updated>2025-03-26T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://www.onehouse.ai/blog/what-is-clustering-in-an-open-data-lakehouse">here</a></span>]]></content>
        <author>
            <name>Dipankar Mazumdar</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="Apache Iceberg" term="Apache Iceberg"/>
        <category label="Delta Lake" term="Delta Lake"/>
        <category label="Clustering" term="Clustering"/>
        <category label="Z-order" term="Z-order"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Deduplication Strategies in an Open Lakehouse Architecture]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/03/26/dedupe</id>
        <link href="https://hudi.apache.org/cn/blog/2025/03/26/dedupe"/>
        <updated>2025-03-26T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://www.onehouse.ai/blog/data-deduplication-strategies-in-an-open-lakehouse-architecture">here</a></span>]]></content>
        <author>
            <name>Dipankar Mazumdar, Aditya Goenka</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="Apache Iceberg" term="Apache Iceberg"/>
        <category label="Delta Lake" term="Delta Lake"/>
        <category label="Deduplication" term="Deduplication"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Transactional Bottlenecks to Lightning-Fast Analytics]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/03/26/uptycs</id>
        <link href="https://hudi.apache.org/cn/blog/2025/03/26/uptycs"/>
        <updated>2025-03-26T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://medium.com/allthatscales/from-transactional-bottlenecks-to-lightning-fast-analytics-74e0d3fff1c0">here</a></span>]]></content>
        <author>
            <name>Akash, Anudeep, Rohan</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="CDC" term="CDC"/>
        <category label="Debezium" term="Debezium"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Building an Amazon Sales Analytics Pipeline with Apache Hudi on Databricks]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/03/13/hudi-on-dbr</id>
        <link href="https://hudi.apache.org/cn/blog/2025/03/13/hudi-on-dbr"/>
        <updated>2025-03-13T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://www.linkedin.com/pulse/building-amazon-sales-analytics-pipeline-apache-hudi-databricks-ruotf/">here</a></span>]]></content>
        <author>
            <name>Sameer Shaik</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="apache hudi" term="apache hudi"/>
        <category label="aws" term="aws"/>
        <category label="databricks" term="databricks"/>
    </entry>
</feed>