<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://hudi.apache.org/cn/blog</id>
    <title>Apache Hudi: User-Facing Analytics</title>
    <updated>2025-10-22T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://hudi.apache.org/cn/blog"/>
    <subtitle>Apache Hudi Blog</subtitle>
    <icon>https://hudi.apache.org/cn/assets/images/favicon.ico</icon>
    <entry>
        <title type="html"><![CDATA[Partition Stats: Enhancing Column Stats in Hudi 1.0]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/10/22/Partition_Stats_Enhancing_Column_Stats_in_Hudi_1.0</id>
        <link href="https://hudi.apache.org/cn/blog/2025/10/22/Partition_Stats_Enhancing_Column_Stats_in_Hudi_1.0"/>
        <updated>2025-10-22T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[For those tracking Apache Hudi's performance enhancements, the introduction of the column stats index was a significant development, as detailed in this blog. It represented a major advancement for query optimization by implementing a straightforward yet highly effective concept: storing lightweight, file-level statistics (such as min/max values and null counts) for specific columns. This provided Hudi's query engine a substantial performance improvement.]]></summary>
        <content type="html"><![CDATA[<p>For those tracking Apache Hudi's performance enhancements, the introduction of the column stats index was a significant development, as <a href="https://www.onehouse.ai/blog/hudis-column-stats-index-and-data-skipping-feature-help-speed-up-queries-by-an-orders-of-magnitude" target="_blank" rel="noopener noreferrer">detailed in this blog</a>. It represented a major advancement for query optimization by implementing a straightforward yet highly effective concept: storing lightweight, file-level statistics (such as min/max values and null counts) for specific columns. This provided Hudi's query engine a substantial performance improvement.</p>
<p><img decoding="async" loading="lazy" alt="cover" src="https://hudi.apache.org/cn/assets/images/fig1-103edc705ab1254fb8b23ba25db76fd6.jpg" width="1944" height="1654" class="img_ev3q"></p>
<p>Instead of blindly scanning every single file for a query, the engine could first peek at the index entries—which is far more efficient than reading all the Parquet footers—to determine which files <em>couldn't</em> possibly contain the relevant data. This data-skipping capability meant engines could bypass large amounts of irrelevant data, slashing query latency. But that skipping process is conducted at the file level—what if we could apply a similar skipping logic at the partition level? Since a single physical partition can contain thousands of data files, applying this logic at the partition level can further amplify the performance gains by only considering files in the relevant partitions. This is precisely the capability that Hudi 1.0’s partition stats index introduces.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="multimodal-indexing">Multimodal Indexing<a href="https://hudi.apache.org/cn/blog/2025/10/22/Partition_Stats_Enhancing_Column_Stats_in_Hudi_1.0#multimodal-indexing" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Hudi’s <a href="https://hudi.apache.org/docs/indexes#multi-modal-indexing" target="_blank" rel="noopener noreferrer">multimodal indexing subsystem</a> enhances both read and write performance in data lakehouses by supporting versatile index types optimized for different workloads. This subsystem is built on a scalable, internal metadata table that ensures ACID-compliant updates and efficient lookups, which in turn reduces full data scans. It houses various indexes—such as the files, column stats, and partition stats—which work together to improve efficiency in reads, writes, and upserts, providing scalable, low-latency query performance for large datasets in the lakehouse.</p>
<p>The partition stats index is built on top of the column stats index by aggregating its file-level statistics up to the partition level. As we've covered, the column stats index tracks statistics (min, max, null counts) for <em>individual files</em>, enabling fine-grained file pruning. The partition stats index, in contrast, summarizes these same statistics across <em>all files</em> within a single partition.</p>
<p>This partition-level aggregation allows Hudi to efficiently prune entire physical partitions before even examining file-level indexes, leading to faster query planning and execution by skipping large chunks of irrelevant data early in the process. In other words, the partition stats index provides a coarse-grained, high-level pruning layer on top of the fine-grained, file-level pruning enabled by the column stats index.</p>
<p>Because partition-level pruning happens first, it narrows down the scope of files that the column stats index needs to inspect, improving overall query performance and reducing overhead on large datasets. The diagram below illustrates the file pruning process:</p>
<p><img decoding="async" loading="lazy" alt="file pruning process" src="https://hudi.apache.org/cn/assets/images/fig2-468ec18846bf7194631d838fd9824bcf.png" width="981" height="706" class="img_ev3q"></p>
<p>During query planning, the Hudi integration for the query engine takes the predicates parsed from user queries and queries the indexes within the metadata table.</p>
<ul>
<li>The files index is queried first to return an initial list of all partitions in the table.</li>
<li>The partition stats index then filters this partition list by checking if each partition’s min/max values for the indexed columns fall within the predicate's range. For example, with a predicate of <code>A = 100</code>, the index skips any partition whose <code>min(A)</code> is greater than 100 or whose <code>max(A)</code> is less than 100.</li>
<li>The files index is queried again to retrieve a list of all files <em>within</em> these pruned partitions.</li>
<li>This file list is then passed to the column stats index, which performs the final, fine-grained pruning by applying the query predicates to the file-level statistics.</li>
<li>Finally, this pruned list of files is returned to the query engine to complete query planning.</li>
</ul>
<p>This dual-layer pruning strategy is especially impactful in production systems managing large amounts of data. By complementing the fine-grained column stats index with this coarse-grained partition skipping, Hudi’s metadata table significantly reduces I/O, computation, and cost. For end-users, this translates directly into a better experience, turning queries that once took minutes into operations that complete in seconds.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="example-us-shipping-addresses">Example: US Shipping Addresses<a href="https://hudi.apache.org/cn/blog/2025/10/22/Partition_Stats_Enhancing_Column_Stats_in_Hudi_1.0#example-us-shipping-addresses" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>To understand the impact, let's use the example table below, which stores US shipping addresses for online orders and is partitioned by <code>state</code>. This table could contain billions of records, and we want to run a query filtering on the <code>zip_code</code> column.</p>
<p>By default, the files, column stats, and partition stats indexes are all enabled in Hudi 1.0. You can create the Hudi table using Spark SQL, for example, without needing additional configs to enable column stats and partition stats:</p>
<div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">TABLE</span><span class="token plain"> shipping_address </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    order_id STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    state STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    zip_code STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">USING</span><span class="token plain"> HUDI</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">TBLPROPERTIES </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    primaryKey </span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">'order_id'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    hoodie</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">metadata</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">index</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">column</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">stats</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">column</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">list </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'zip_code'</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">PARTITIONED </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">BY</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">state</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><br></span></code></pre></div></div>
<p>Note that, in practice, you would most likely want to use <code>hoodie.metadata.index.column.stats.column.list</code> to indicate which column(s) to index according to your business use case, otherwise, the first 32 columns in the table schema will be indexed by default, which probably won’t be optimal. The specified columns apply to both the column stats and partition stats indexes.</p>
<p>Without the column and partition stats indexes, a query for a specific ZIP code (e.g., <code>zip_code = '90001'</code>) would force the query engine to perform a full table scan. This is highly inefficient, leading to high query latency and excessive resource consumption.</p>
<p>With the indexes enabled, the process is drastically different.</p>
<ol>
<li>During write operations, the Hudi writer tracks statistics for the <code>zip_code</code> column. The column stats index stores min/max values for each data file, and the partition stats index aggregates and stores the min/max <code>zip_code</code> for each <code>state</code>.</li>
<li>At query time, suppose the partition stats index shows that the "California" partition contains ZIP codes from "90000" to "96199", while the "New York" partition contains ZIP codes from "10000" to "14999". When the query for <code>zip_code = '90001'</code> is executed, the query planner first consults the partition stats index. It sees that "90001" falls within the "California" partition's range but outside the "New York" partition's range.</li>
<li>The engine can therefore skip the entire "New York" partition (and any other partition like "Texas" or "Florida" whose ZIP code range doesn't include "90001"). The query proceeds by only reading data from the "California" partition—the only one that could possibly contain the data.</li>
</ol>
<p>This ability to prune entire partitions before reading any files is what provides such a significant performance gain.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="results-the-data-skipping-effect">Results: the Data Skipping Effect<a href="https://hudi.apache.org/cn/blog/2025/10/22/Partition_Stats_Enhancing_Column_Stats_in_Hudi_1.0#results-the-data-skipping-effect" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>We conducted a focused benchmarking exercise using a synthetic dataset generated by the open-source tool <a href="https://github.com/onehouseinc/lake-loader" target="_blank" rel="noopener noreferrer">lake_loader</a>. Specifically, we created a 1 TB table for the US shipping addresses example and built both the column stats and partition stats indexes on this dataset.</p>
<p>The benchmarking objective was to evaluate the performance impact from the two indexes for data skipping. To do this, we executed the following query in two scenarios:</p>
<div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">select</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">count</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> shipping_address </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">where</span><span class="token plain"> zip_code </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'10001'</span><br></span></code></pre></div></div>
<p>One with the column and partition stats indexes enabled (default), and one with both indexes disabled for reads, which forced a full table scan.</p>
<p>The Spark job was configured with:</p>
<ul>
<li>Executor cores = 4</li>
<li>Executor memory = 10g</li>
<li>Number of executors = 60</li>
</ul>
<p>The Spark DAGs for the two scenarios show the file pruning effect:</p>
<p><img decoding="async" loading="lazy" alt="Spark DAGs comparison" src="https://hudi.apache.org/cn/assets/images/fig3-2a993e3d03e054e6e2697772a56e673f.png" width="3456" height="1992" class="img_ev3q"></p>
<p>With both column stats and partition stats indexes enabled (the left-side DAG), the number of files read was 19,304. In contrast, the disabled setup (the right-side DAG) resulted in reading 393,360 files—about 20 times more.</p>
<p>The runtime comparison chart below shows the query time difference (shorter is better):</p>
<p><img decoding="async" loading="lazy" alt="perf run time chart" src="https://hudi.apache.org/cn/assets/images/fig4-8df00f82c190ca1663131f7dbb6ddf8d.jpg" width="2428" height="1720" class="img_ev3q"></p>
<p>Enabling data skipping with both the column stats and partition stats indexes for the Hudi table delivers approximately a 93% reduction in query runtime compared to the full scan (no data skipping).</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://hudi.apache.org/cn/blog/2025/10/22/Partition_Stats_Enhancing_Column_Stats_in_Hudi_1.0#conclusion" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>The new partition stats index is a powerful addition to Hudi's multimodal indexing subsystem, directly addressing the challenge of query performance on large-scale partitioned tables. By working in concert with the existing column stats index, it provides a crucial layer of coarse-grained pruning, allowing the query engine to eliminate entire partitions from consideration <em>before</em> inspecting individual files. As our benchmark showed, this two-level pruning strategy—first by partition, then by file—is not just a minor tweak. It results in a dramatic reduction in I/O, slashing query runtimes by over 93% and enabling near-interactive query speeds. This feature solidifies Hudi's data-skipping capabilities, making it even more efficient to run demanding analytical queries directly on the data lakehouse, saving both time and computation costs.</p>]]></content>
        <author>
            <name>Aditya Goenka and Shiyan Xu</name>
        </author>
        <category label="hudi" term="hudi"/>
        <category label="indexing" term="indexing"/>
        <category label="data lakehouse" term="data lakehouse"/>
        <category label="data skipping" term="data skipping"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modernizing Upstox's Data Platform with Apache Hudi, dbt, and EMR Serverless]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless</id>
        <link href="https://hudi.apache.org/cn/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless"/>
        <updated>2025-10-16T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Introduction]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="https://hudi.apache.org/cn/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless#introduction" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>In <a href="https://www.youtube.com/watch?v=dAM2zOvnPmw" target="_blank" rel="noopener noreferrer">this community sharing session</a>, Manish Gaurav from Upstox shared insights into the complexities of managing data ingestion at scale. Drawing from the company’s experience as a leading online trading platform in India, the discussion highlighted challenges around file-level upserts, ensuring atomic operations, and handling small files effectively. Upstox shared how they built a modern data platform using Apache Hudi and dbt to address these issues. In this blog post, we’ll break down their solution and why it matters.</p>
<p>Upstox is a leading online trading platform that enables millions of users to invest in equities, commodities, derivatives, and currencies. With over 12 million customers generating 300,000 data requests daily, the company's data team is responsible for delivering the real-time insights that power key products, including:</p>
<ul>
<li>Search functionality</li>
<li>A customer service chatbot (powered by OpenAI)</li>
<li>Personalized portfolio recommendations</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://hudi.apache.org/cn/assets/images/fig1-3baa485e75ef728786f15b45d2d97d6b.png" width="1999" height="1312" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="data-sources">Data Sources<a href="https://hudi.apache.org/cn/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless#data-sources" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Upstox ingests 250–300 GB of structured and semi-structured data per day from a variety of sources:</p>
<ul>
<li>Order and transaction data from exchanges</li>
<li>Microservice telemetry from Cloudflare</li>
<li>Customer support data from platforms like Freshdesk and SquadStack</li>
<li>Behavioral analytics from Mixpanel</li>
<li>Data from operational databases (MongoDB, MySQL, and MS SQL) via AWS DMS</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-challenges-with-initial-data-platform">The Challenges with Initial Data Platform<a href="https://hudi.apache.org/cn/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless#the-challenges-with-initial-data-platform" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>As Upstox grew, so did the complexity of its data operations. Here are some of the early bottlenecks the company faced:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="data-ingestion-issues">Data Ingestion Issues<a href="https://hudi.apache.org/cn/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless#data-ingestion-issues" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Prior to 2023, Upstox relied on no-code ingestion platforms like Hevo. While easy to adopt, these platforms introduced several limitations, including high licensing costs and a lack of fine-grained control over ingestion logic. File-level upserts required complex joins between incoming CDC (change data capture) datasets and target tables. Additionally, a lack of atomicity often led to inconsistent data writes, and small-file issues were rampant. To combat these problems, the team had to implement time-consuming re-partitioning and coalescing, along with complex salting strategies to distribute data evenly.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="downstream-consumption-struggles">Downstream Consumption Struggles<a href="https://hudi.apache.org/cn/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless#downstream-consumption-struggles" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Analytics queries were primarily served through Amazon Athena, which presented several key limitations. For instance, it frequently timed out when querying large datasets and often exceeded the maximum number of partitions it could handle. Additionally, Athena's lack of support for stored procedures made it challenging to manage and reuse complex query logic. Attempts to improve performance with bucketing often created more small files, and the lack of native support for incremental queries further complicated their analytics workflow.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-modern-lakehouse-architecture">The Modern Lakehouse Architecture<a href="https://hudi.apache.org/cn/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless#the-modern-lakehouse-architecture" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p><img decoding="async" loading="lazy" src="https://hudi.apache.org/cn/assets/images/fig2-a2a9161b7ad75628a36b03514ae4a9c4.png" width="1934" height="1016" class="img_ev3q"></p>
<p>To tackle these problems, Upstox implemented a medallion architecture, organizing data into bronze, silver, and gold layers:</p>
<ul>
<li><strong>Bronze (Raw Data):</strong> Data is ingested and stored in its raw format as Parquet files.</li>
<li><strong>Silver (Cleaned and Filtered):</strong> Data is cleaned, filtered, and stored in Apache Hudi tables, which are updated incrementally.</li>
<li><strong>Gold (Business-Ready):</strong> Data is aggregated for specific business use cases, modeled with dbt, and stored in Hudi.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-solution-a-modern-stack-with-hudi-dbt-and-emr-serverless">The Solution: A Modern Stack with Hudi, dbt, and EMR Serverless<a href="https://hudi.apache.org/cn/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless#the-solution-a-modern-stack-with-hudi-dbt-and-emr-serverless" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Upstox re-architected its platform using Apache Hudi as the core data lake technology, dbt for transformations, and EMR Serverless for scalable compute. Airflow was used to orchestrate the entire workflow. Here's how this new stack addressed their challenges:</p>
<p><strong>Simplified Data Updates:</strong> Hudi provides built-in support for record-level upserts with atomic guarantees and snapshot isolation. This helped Upstox overcome the challenge of ensuring consistent updates to their fact and dimension tables.</p>
<p><strong>Improved Upsert Performance:</strong> To optimize upsert performance, the team leveraged Bloom index, especially for transaction-heavy fact tables. Indexing strategies were chosen based on data characteristics to balance latency and efficiency.</p>
<p><strong>Resolved Small-File Issues:</strong> Small files, which are common in streaming workloads, were mitigated using clustering jobs supported by Hudi. This process was scheduled to run weekly and ensured efficient file sizes and reduced storage overhead without manual intervention.</p>
<p><strong>Enabled Incremental Processing:</strong> Incremental joins allowed Upstox to process only new data daily. This enabled timely updates to the aggregated tables in the gold layer that power user-facing dashboards—a task that was not feasible with traditional Athena queries.</p>
<p><strong>Managed Metadata Growth:</strong> The accumulation of commit and metadata files in the Hudi table’s `.hoodie/` directory increased S3 listing costs and slowed down operations. Hudi's archival feature helped manage this by archiving older commits after a certain threshold, keeping metadata lean and efficient.</p>
<p><strong>Streamlined Data Modeling:</strong> The team used dbt on EMR Serverless to create materialized views over the Hudi datasets. This enabled the creation of efficient transformation layers (silver and gold) using familiar SQL workflows and managed compute.</p>
<p><strong>Flexible Data Materialization:</strong> dbt supported a variety of model types, including tables, views, and ephemeral models (Common Table Expressions, or CTEs). This gave teams the flexibility to optimize for performance, reuse, or simplicity, depending on the use case.</p>
<p><strong>Out-of-the-Box Lineage and Documentation:</strong> dbt helps visualize how data flows from one table to another, making it easier to debug and understand dependencies. The glossary feature allows teams to document column meanings and transformations clearly.</p>
<p><strong>Enforced Data Quality:</strong> With dbt, specific data quality rules can be added to individual tables or pipelines. This adds an extra layer of validation beyond the basic checks performed during data ingestion.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cicd-and-orchestration">CI/CD and Orchestration<a href="https://hudi.apache.org/cn/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless#cicd-and-orchestration" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p><img decoding="async" loading="lazy" src="https://hudi.apache.org/cn/assets/images/fig3-3a9b031ca307c28c17434af09a0ee7bc.png" width="1932" height="882" class="img_ev3q"></p>
<p>Upstox uses Apache Airflow for orchestration, with dbt pipelines deployed via a Git-based CI/CD process. Merging a pull request in GitLab triggers the CI/CD pipeline, which automatically builds a new dbt image and publishes the updated data catalog. Airflow then runs the corresponding dbt jobs daily or on-demand, automating the entire transformation workflow.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-impact">The Impact<a href="https://hudi.apache.org/cn/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless#the-impact" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>The adoption of this modern data stack had a significant impact on Upstox's data platform. The company achieved extremely high data availability and consistency for critical datasets, reducing SLA breaches for complex joins by 70%. Furthermore, pipeline costs dropped by 40%, and query performance improved drastically thanks to Hudi's clustering and optimized joins.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://hudi.apache.org/cn/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless#conclusion" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>By leveraging Apache Hudi, dbt, and EMR Serverless, Upstox built a robust and cost-efficient data platform to serve its 12M+ customers, overcoming the significant challenges of data ingestion and analytics at scale. This transformation resolved critical issues like inconsistent data writes, small-file problems, and query timeouts, leading to tangible improvements in both performance and efficiency. With a 70% reduction in SLA breaches and a 40% drop in pipeline costs, the new architecture has empowered their BI and ML teams to move faster. Ultimately, this success story demonstrates how a modern data stack can not only solve immediate technical bottlenecks but also lay the groundwork for a scalable, self-service future that enables continued innovation.</p>]]></content>
        <author>
            <name>The Hudi Community</name>
        </author>
        <category label="hudi" term="hudi"/>
        <category label="upstox" term="upstox"/>
        <category label="dbt" term="dbt"/>
        <category label="data lakehouse" term="data lakehouse"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-Time Cloud Security Graphs with Apache Hudi and PuppyGraph]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph</id>
        <link href="https://hudi.apache.org/cn/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph"/>
        <updated>2025-10-02T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[CrowdStrike’s 2025 Global Threat Report puts average eCrime breakout time at 48 minutes, with the fastest at 51 seconds. This means that by the time security teams are even alerted about the potential breach, attackers have already long infiltrated the system. And that’s assuming they even get alerted. Cloud environments generate massive amounts of access logs, configuration changes, alerts, and telemetry. Reviewing these events in isolation rarely surfaces patterns like lateral movement or privilege escalation.]]></summary>
        <content type="html"><![CDATA[<p><a href="https://www.crowdstrike.com/en-us/global-threat-report/" target="_blank" rel="noopener noreferrer">CrowdStrike’s 2025 Global Threat Report</a> puts average eCrime breakout time at 48 minutes, with the fastest at 51 seconds. This means that by the time security teams are even alerted about the potential breach, attackers have already long infiltrated the system. And that’s assuming they even get alerted. Cloud environments generate massive amounts of access logs, configuration changes, alerts, and telemetry. Reviewing these events in isolation rarely surfaces patterns like lateral movement or privilege escalation.</p>
<p>Security tools such as SIEM, CSPM, and cloud workload protection need relationship-based analysis. It is not only a login attempt or a policy change, but also who acted, which systems were touched, what privileges were active, and what happened next. Event-centric methods struggle to answer those questions at scale. Graph analysis fits better because it captures paths and context across entities.</p>
<p>To keep up, the data pipeline must support:</p>
<ul>
<li>Continuous upserts with low lag so detections run on the latest state</li>
<li>Incremental consumption so analytics read only “what changed since T”</li>
<li>A rewindable timeline so responders can review state during investigations</li>
</ul>
<p>With Apache Hudi and PuppyGraph, this becomes straightforward. Hudi tables support fast upserts and incremental processing. PuppyGraph queries relationships in place using openCypher or Gremlin. In this blog, we explore how to get started with real-time security graph analytics at scale using the data already stored in your Hudi lakehouse tables.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-apache-hudi-for-cybersecurity-data">Why Apache Hudi for Cybersecurity Data?<a href="https://hudi.apache.org/cn/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph#why-apache-hudi-for-cybersecurity-data" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p><a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer">Apache Hudi</a> is an open data lakehouse platform that brings ACID transaction guarantees to data lakes. It enables efficient, record-level updates and deletes on massive datasets, which makes it a strong foundation for storing and analyzing cybersecurity data such as logs, telemetry, and threat intelligence. Its combination of performance, flexibility, and broad ecosystem integration is well-suited for threat detection, forensic investigation, and compliance work.</p>
<p>Hudi speeds up large-scale security analytics through features that keep tables both current and query-efficient. Hudi writers excel at handling continuous, mutable workloads without requiring costly full rewrites. Hudi’s <a href="https://hudi.apache.org/docs/metadata" target="_blank" rel="noopener noreferrer">multi-modal indexing subsystem</a>, backed by its internal metadata table, offers efficient lookups and data skipping, dramatically accelerating queries that scan massive log sets to isolate suspicious activity. Hudi keeps tables updatable and queryable as they change, with time-travel and incremental reads for point-in-time forensic analysis.</p>
<p>Even as data volumes grow, operations remain manageable. Hudi tracks every commit on a timeline, enabling powerful time-travel queries for historical investigations. Asynchronous table services like <a href="https://hudi.apache.org/docs/compaction" target="_blank" rel="noopener noreferrer">compaction</a>, <a href="https://hudi.apache.org/docs/clustering" target="_blank" rel="noopener noreferrer">clustering</a>, <a href="https://hudi.apache.org/docs/cleaning" target="_blank" rel="noopener noreferrer">cleaning</a>, and <a href="https://hudi.apache.org/docs/metadata_indexing" target="_blank" rel="noopener noreferrer">indexing</a> run in the background to maintain peak performance and storage health while minimizing disruption to ingestion pipelines. Furthermore, its consistent commit and delete semantics support the creation of reliable audit trails, simplify data retention policies, and help meet privacy requirements.</p>
<figure><p><img decoding="async" loading="lazy" src="https://hudi.apache.org/cn/assets/images/hudi-stack-1-x-86c60e4c27bcc3af1fdf1e78ed48e42d.png" width="1989" height="1344" class="img_ev3q">
</p><figcaption>The <a href="https://hudi.apache.org/docs/hudi_stack" target="_blank" rel="noopener noreferrer">Apache Hudi Stack</a></figcaption><p></p></figure>
<p>Hudi also integrates seamlessly with the tools security teams already use. You can stream data from Apache Kafka or Debezium CDC into Hudi, register tables in Hive Metastore or AWS Glue Catalog, and query them from popular query engines like Apache Spark, Apache Flink, Presto, Trino, or Amazon Athena. PuppyGraph connects to the same Hudi tables and runs openCypher or Gremlin queries directly on them via the user access layer, so you get real-time graph analytics on the lake with no ETL and no data duplication.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-puppygraph-for-cybersecurity-data">Why PuppyGraph for Cybersecurity Data?<a href="https://hudi.apache.org/cn/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph#why-puppygraph-for-cybersecurity-data" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p><a href="https://puppygraph.com/" target="_blank" rel="noopener noreferrer">PuppyGraph</a> is the first real-time, zero-ETL graph query engine. It lets data teams query existing relational stores as a single graph and get up and running in under 10 minutes, avoiding the cost, latency, and maintenance of a separate graph database. To understand why this is so important, let’s take a look at the status quo.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="traditional-analytics-on-the-lake">Traditional Analytics on the Lake<a href="https://hudi.apache.org/cn/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph#traditional-analytics-on-the-lake" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Security teams already store logs, configs, and alerts in a lakehouse. SQL engines are great for counts, filters, rollups, and point lookups. They struggle when questions depend on relationships. Lateral movement, privilege escalation, and blast radius span many tables and time windows. Each new join adds complexity, pushes latency up, and breaks easily when schemas evolve or events arrive late. You can stitch context with views and pipelines, but it is fragile and slow to adapt.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="dedicated-graph-databases">Dedicated Graph Databases<a href="https://hudi.apache.org/cn/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph#dedicated-graph-databases" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Graphs make paths and neighborhoods first class. Graph queries let you answer “what connects to what” in a way that makes sense, without the need for confusing data joins. The tradeoff is operations and freshness. Most graph databases want their own storage. That means ETL, a second copy, and lag between source and graph. Continuous upserts are heavy because every change can touch nodes, edges, and multiple indexes. Running a separate cluster adds backups, upgrades, sizing, and vendor-specific tuning. During an incident, that overhead shows up as stale data and slower investigations.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-puppygraph-helps">How PuppyGraph Helps<a href="https://hudi.apache.org/cn/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph#how-puppygraph-helps" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>PuppyGraph is not a traditional graph database but a graph query engine designed to run directly on top of your existing data infrastructure without costly and complex ETL (Extract, Transform, Load) processes. This "zero-ETL" approach is its core differentiator, allowing you to query relational data in data warehouses, data lakes, and databases as a unified graph model in minutes.</p>
<p>Instead of migrating data into a specialized store, PuppyGraph connects to sources including <a href="https://www.puppygraph.com/blog/postgresql-graph-database" target="_blank" rel="noopener noreferrer">PostgreSQL</a>, <a href="https://docs.puppygraph.com/connecting/connecting-to-iceberg/?h=ice" target="_blank" rel="noopener noreferrer">Apache Iceberg</a>, <a href="https://docs.puppygraph.com/connecting/connecting-to-apache-hudi/" target="_blank" rel="noopener noreferrer">Apache Hudi</a>, <a href="https://docs.puppygraph.com/connecting/connecting-to-bigquery/?h=bigq" target="_blank" rel="noopener noreferrer">BigQuery</a>, and others, then builds a virtual graph layer over them. Graph models are defined through simple JSON schema files, making it easy to update, version, or switch graph views without touching the underlying data. From there, you can quickly begin exploring your data with graph queries written in Gremlin or openCypher.</p>
<figure><p><img decoding="async" loading="lazy" src="https://hudi.apache.org/cn/assets/images/fig-2-PuppyGraph_Supported_Data_Sources-530816b1bc98047b2113f8f1fb791b8c.png" width="1313" height="745" class="img_ev3q">
</p><figcaption>PuppyGraph Supported Data Sources</figcaption><p></p></figure>
<figure><p><img decoding="async" loading="lazy" src="https://hudi.apache.org/cn/assets/images/fig-3-Architecture-with-Graph-Database-vs-with-PuppyGraph-c6601e01f1e7ae2fcd8ffcc58aec90ac.png" width="1497" height="843" class="img_ev3q">
</p><figcaption>Architecture with Graph Database vs. with PuppyGraph</figcaption><p></p></figure>
<p>This approach aligns with the broader shift in modern data stacks to separate compute from storage. You keep data where it belongs and scale query power independently, which supports petabyte-level workloads without duplicating data or managing fragile pipelines.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="real-world-use-case">Real-World Use Case<a href="https://hudi.apache.org/cn/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph#real-world-use-case" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>We have shown why cloud security benefits from a relationship-first view of identities, resources, and events. In this demo, we’ll show how easy it is to begin querying your cloud security data as a graph. Apache Hudi keeps those tables current with streaming upserts and an investigation-friendly timeline. PuppyGraph lets you query your existing lake tables as a graph. Together they give you real-time security graph analytics on the data you already store.</p>
<p>Getting started is straightforward. You will deploy the stack, load security data into Hudi, connect PuppyGraph to your catalog, define a graph view, and run a few queries. All in a matter of minutes.</p>
<figure><p><img decoding="async" loading="lazy" src="https://hudi.apache.org/cn/assets/images/fig-4-Sample-Architecture-of-PuppyGraph-Hudi-adcd64f4ffaf1a8c9b6e13f7d9d07e4a.png" width="858" height="1100" class="img_ev3q">
</p><figcaption>Sample Architecture of PuppyGraph + Hudi</figcaption><p></p></figure>
<p>The components of this demo project include:</p>
<ul>
<li>Storage: MinIO/S3 – Object store for Hudi data</li>
<li>Data Lakehouse: Apache Hudi – Brings database functionality to your data lakes</li>
<li>Catalog: Hive Metastore – Backed by Postgres</li>
<li>Compute engines:<!-- -->
<ul>
<li>Spark – Initial table writes</li>
<li>PuppyGraph – Graph query engine for complex, multi-hop graph queries</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="prerequisites">Prerequisites<a href="https://hudi.apache.org/cn/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph#prerequisites" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>This tutorial assumes that you have the following:</p>
<ol>
<li><strong>Docker</strong> and <strong>Docker</strong> <strong>Compose</strong> (for setting up the Docker container)</li>
<li><strong>Python 3</strong> (for managing dependencies)</li>
<li><a href="https://github.com/puppygraph/puppygraph-getting-started/tree/main/integration-demos/hudi-demo" target="_blank" rel="noopener noreferrer">PuppyGraph-Hudi Demo Repository</a></li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="data-preparation">Data Preparation<a href="https://hudi.apache.org/cn/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph#data-preparation" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>Before we can load our data into our Hudi tables, we need to make sure they’re in the correct file format. Hudi currently supports Parquet and ORC for base files, and we’ll be going with Parquet for this demo:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">python3 -m venv demo</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">source demo/bin/activate</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">pip install -r requirements.txt</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">python3 CsvToParquet.py ./csv_data ./parquet_data</span><br></span></code></pre></div></div>
<p>Since we’ll be connecting to the Hudi Catalog via the Hive Metastore (HMS), we also have to install the following dependencies:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">mkdir -p lib</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">curl -L -o lib/postgresql-42.5.1.jar \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">https://repo1.maven.org/maven2/org/postgresql/postgresql/42.5.1/postgresql-42.5.1.jar</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">curl -L -o lib/hadoop-aws-3.3.4.jar \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">curl -L -o lib/aws-java-sdk-bundle-1.12.262.jar \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="loading-data">Loading Data<a href="https://hudi.apache.org/cn/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph#loading-data" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>With all our dependencies installed and data prepared, we can launch the required services:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">docker compose up -d</span><br></span></code></pre></div></div>
<p>Once everything is up and running, we can finally populate the tables with our data:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">docker compose exec spark /opt/spark/bin/spark-sql -f /init.sql</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="modeling-the-graph">Modeling the Graph<a href="https://hudi.apache.org/cn/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph#modeling-the-graph" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>Now that our data is loaded in, we can log into the PuppyGraph Web UI at <a href="http://localhost:8081/" target="_blank" rel="noopener noreferrer">http://localhost:8081</a> with the default credentials (username: puppygraph, password: puppygraph123)</p>
<figure><p><img decoding="async" loading="lazy" src="https://hudi.apache.org/cn/assets/images/fig-5-PuppyGraph-Login-Page-c4b9d16af5789a65c1f796e0411dfd40.png" width="3024" height="1710" class="img_ev3q">
</p><figcaption>PuppyGraph Login Page</figcaption><p></p></figure>
<p>To model your data as a graph, you can simply select the file `schema.json` in the Upload Graph Schema JSON section and click on Upload.</p>
<figure><p><img decoding="async" loading="lazy" src="https://hudi.apache.org/cn/assets/images/fig-6-Schema-Page-in-PuppyGraph-UI-ff0eb0aac444ad00b098145a9c84fe69.png" width="1600" height="907" class="img_ev3q">
</p><figcaption>Schema Page in PuppyGraph UI</figcaption><p></p></figure>
<p>Once you see your graph schema loaded in, you’re ready to start querying your data as a graph.</p>
<figure><p><img decoding="async" loading="lazy" src="https://hudi.apache.org/cn/assets/images/fig-7-Loaded-Schema-in-PuppyGraph-UI-b32e7eb53b88d6290b5f463308445bfa.png" width="1600" height="907" class="img_ev3q">
</p><figcaption>Loaded Schema in PuppyGraph UI</figcaption><p></p></figure>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="sample-queries">Sample Queries<a href="https://hudi.apache.org/cn/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph#sample-queries" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>By modeling the network infrastructure as a graph, users can identify potential security risks, such as:</p>
<ul>
<li>Public IP addresses exposed to the internet</li>
<li>Network interfaces not protected by any security group</li>
<li>Roles granted excessive access permissions</li>
<li>Security groups with overly permissive ingress rules</li>
</ul>
<p>Listed below are some sample queries you can try running to explore the data:</p>
<ol>
<li>Tracing Admin Access Paths from Users to Internet Gateways</li>
</ol>
<div class="language-javascript codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-javascript codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">g</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token constant" style="color:rgb(189, 147, 249)">V</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">hasLabel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'User'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword module" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'user'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">outE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'ACCESS'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">has</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'access_level'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'admin'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword module" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'edge'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">inV</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">path</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre></div></div>
<p><img decoding="async" loading="lazy" src="https://hudi.apache.org/cn/assets/images/fig-8-PuppyGraph-Query-1-9d56715c417b1f35667d91d153df9547.png" width="1600" height="907" class="img_ev3q"></p>
<ol start="2">
<li>Find all public IP addresses exposed to the internet, along with their associated virtual machine instances, security groups, subnets, VPCs, internet gateways, and users, displaying all these entities in the traversal path.</li>
</ol>
<div class="language-javascript codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-javascript codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain"> g</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token constant" style="color:rgb(189, 147, 249)">V</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">hasLabel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'PublicIP'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword module" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'ip'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">in</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'HAS_PUBLIC_IP'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword module" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'ni'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">in</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'PROTECTS'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">hasLabel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'SecurityGroup'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword module" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'sg'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">out</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'HAS_RULE'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">hasLabel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'IngressRule'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword module" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'rule'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">where</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   __</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">out</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'ALLOWS_TRAFFIC_FROM'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">hasLabel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'InternetGateway'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">select</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'ni'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">out</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'ATTACHED_TO'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">hasLabel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'VMInstance'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword module" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'vm'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">select</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'ni'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">in</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'HOSTS_INTERFACE'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">hasLabel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'Subnet'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword module" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'subnet'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">in</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'CONTAINS'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">hasLabel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'VPC'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword module" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'vpc'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">in</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'GATEWAY_TO'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">hasLabel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'InternetGateway'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword module" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'igw'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">in</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'ACCESS'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">hasLabel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'User'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword module" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'user'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">path</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">limit</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">1000</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre></div></div>
<p><img decoding="async" loading="lazy" src="https://hudi.apache.org/cn/assets/images/fig-9-PuppyGraph-Query-2-59da47f7532f00f65a89d7ac108865a8.png" width="1600" height="907" class="img_ev3q"></p>
<ol start="3">
<li>Find roles that have been granted excessive access permissions, along with their associated virtual machine instances.</li>
</ol>
<div class="language-javascript codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-javascript codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">g</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token constant" style="color:rgb(189, 147, 249)">V</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">hasLabel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'Role'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword module" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'role'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">where</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  __</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">out</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'ALLOWS_ACCESS_TO'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">count</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">is</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token function" style="color:rgb(80, 250, 123)">gt</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">4L</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">out</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'ALLOWS_ACCESS_TO'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">hasLabel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'Resource'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword module" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'resource'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">select</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'role'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">in</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'ASSIGNED_ROLE'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">hasLabel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'VMInstance'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword module" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'vm'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">path</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre></div></div>
<p><img decoding="async" loading="lazy" src="https://hudi.apache.org/cn/assets/images/fig-10-PuppyGraph-Query-3-c324707053b02056cc824229fd1d1ac0.png" width="1600" height="907" class="img_ev3q"></p>
<ol start="4">
<li>Find security groups that have ingress rules permitting traffic from any IP address (0.0.0.0/0) to sensitive ports (22 or 3389), and retrieve the associated ingress rules, network interfaces, and virtual machine instances in the traversal path.</li>
</ol>
<div class="language-javascript codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-javascript codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">g</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token constant" style="color:rgb(189, 147, 249)">V</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">hasLabel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'SecurityGroup'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword module" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'sg'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">out</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'HAS_RULE'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">has</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'source'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'0.0.0.0/0'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">has</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'port_range'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token constant" style="color:rgb(189, 147, 249)">P</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">within</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'22'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'3389'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">hasLabel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'IngressRule'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword module" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'rule'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">in</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'HAS_RULE'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword module" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'sg'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">out</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'PROTECTS'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">hasLabel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'NetworkInterface'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword module" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'ni'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">out</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'ATTACHED_TO'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">hasLabel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'VMInstance'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword module" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">'vm'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token method function property-access" style="color:rgb(80, 250, 123)">path</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre></div></div>
<p><img decoding="async" loading="lazy" src="https://hudi.apache.org/cn/assets/images/fig-11-PuppyGraph-Query-4-66848df36086049fd3ac42b78a5de47c.png" width="1600" height="907" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://hudi.apache.org/cn/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph#conclusion" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Real-time security work comes down to two needs: fresh tables and connected questions. Apache Hudi keeps lakehouse data current with streaming upserts, incremental reads, and a rewindable timeline. PuppyGraph reads those same tables in place and runs multi-hop graph queries with openCypher or Gremlin. One data copy. No ETL.</p>
<p>The result is faster investigations and clearer decisions. You can trace attack paths, size blast radius, and correlate alerts to recent changes while keeping governance and access controls in a single lake. When you need to look back, time travel gives you point-in-time views without rebuilding pipelines.</p>]]></content>
        <author>
            <name>Jaz Samantha Ku, in collaboration with Shiyan Xu</name>
        </author>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="PuppyGraph" term="PuppyGraph"/>
        <category label="security" term="security"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Record Key Generation in Apache Hudi]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/09/17/hudi-auto-gen-keys</id>
        <link href="https://hudi.apache.org/cn/blog/2025/09/17/hudi-auto-gen-keys"/>
        <updated>2025-09-17T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[In database systems, the primary key is a foundational design principle for managing data at the record level. Its function is to provide each record with a unique and stable logical identifier, which decouples the record's identity from its physical location on storage. While using direct physical address pointers (e.g., position inside a file being used as a key) can be convenient, the physical address can change when records are moved around within the table for things like clustering or z-ordering (called out here).]]></summary>
        <content type="html"><![CDATA[<p>In database systems, the primary key is a foundational design principle for managing data at the record level. Its function is to provide each record with a unique and stable logical identifier, which decouples the record's identity from its physical location on storage. While using direct physical address pointers (e.g., position inside a file being used as a key) can be convenient, the physical address can change when records are moved around within the table for things like clustering or z-ordering (<a href="https://x.com/apachehudi/status/1641572485325017089" target="_blank" rel="noopener noreferrer">called out here</a>).</p>
<p>By using a primary key that is stable across record movement, a system can efficiently perform operations like updates and deletes, enabling critical features like relational integrity.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="first-class-support-of-record-keys">First-Class Support of Record Keys<a href="https://hudi.apache.org/cn/blog/2025/09/17/hudi-auto-gen-keys#first-class-support-of-record-keys" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Apache Hudi was the first lakehouse storage project to introduce the notion of record keys. For mutable workloads, this addressed a significant architectural challenge. In a typical data lake table, updating records usually required rewriting entire partitions—a process that is slow and expensive. By supporting the record key as the stable identifier for every record, Hudi offered unique and advanced capabilities among lakehouse frameworks:</p>
<ul>
<li>Hudi supports <a href="https://hudi.apache.org/blog/2023/11/01/record-level-index/" target="_blank" rel="noopener noreferrer">record-level indexing</a> for directly locating records in <a href="https://hudi.apache.org/docs/storage_layouts" target="_blank" rel="noopener noreferrer">file groups</a> for highly efficient upserts and queries, and <a href="https://hudi.apache.org/blog/2025/04/02/secondary-index/" target="_blank" rel="noopener noreferrer">secondary indexes</a> that enable performant lookups for predicates on non-record key fields.</li>
<li>Hudi implements <a href="https://hudi.apache.org/blog/2025/03/03/record-mergers-in-hudi/" target="_blank" rel="noopener noreferrer">merge modes</a>, standardizing record-merging semantics to handle requirements such as unordered events, duplicate records, and custom merge logic.</li>
<li>By materializing record keys along with other <a href="https://www.onehouse.ai/blog/hudi-metafields-demystified" target="_blank" rel="noopener noreferrer">record-level meta-fields</a>, Hudi unlocks features such as efficient <a href="https://hudi.apache.org/blog/2024/07/30/data-lake-cdc/" target="_blank" rel="noopener noreferrer">change data capture (CDC)</a> that serves record-level change streams, near-infinite history for time-travel queries, and the <a href="https://hudi.apache.org/docs/clustering" target="_blank" rel="noopener noreferrer">clustering table service</a> that can significantly optimize file sizes.</li>
</ul>
<figure><p><img decoding="async" loading="lazy" src="https://hudi.apache.org/cn/assets/images/2025-09-17-hudi-auto-gen-keys.fig1-fb5004b3f1cd1832795f39f6c7255411.jpg" width="647" height="351" class="img_ev3q">
</p><figcaption>Replicating operational databases to a Hudi lakehouse using CDC</figcaption><p></p></figure>
<p>Append-only writes are very common in the data lakehouse, such as ingesting application logs streamed continuously from numerous servers or capturing clickstream events from user interactions on a website. Even for this kind of scenario, having record keys is beneficial in scenarios like concurrently running data-fixing backfill writers (e.g., a GDPR deletion process) with ongoing writers to the same table. Without record keys, engineers typically had to coordinate the backfill to run on different partitions than the active writes to avoid conflicts. With record keys and the support provided by Hudi’s <a href="https://hudi.apache.org/docs/concurrency_control" target="_blank" rel="noopener noreferrer">concurrency control</a> and merge modes, this restriction can be lifted, with Hudi handling the concurrent writes properly.</p>
<p>Given the advantages of supporting record keys, Hudi required users to set one or multiple record key fields when creating a table prior to <a href="https://hudi.apache.org/releases/release-0.14.0" target="_blank" rel="noopener noreferrer">release 0.14</a>. However, this requirement created friction for users in cases where there were no natural record keys in the incoming stream for simply setting another config variable. Even for users who understood the benefits of record keys, they had to put careful thought into their record key generation to ensure uniqueness and idempotency. The initial friction of generating keys was a barrier to adoption for teams who simply wanted to land their append-only workloads in a lakehouse with as few lines of code and configuration as possible.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="automatic-key-generation">Automatic Key Generation<a href="https://hudi.apache.org/cn/blog/2025/09/17/hudi-auto-gen-keys#automatic-key-generation" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>With the release of version 0.14 (this is actually old news), Hudi has introduced automatic record key generation, a feature designed to simplify the user experience with append-only writes. This enhancement eliminates the mandatory requirement to specify record key fields for every write operation.</p>
<figure><p><img decoding="async" loading="lazy" src="https://hudi.apache.org/cn/assets/images/2025-09-17-hudi-auto-gen-keys.fig2-760500605f2a1ecfa253caffaa013c4a.jpg" width="639" height="316" class="img_ev3q">
</p><figcaption>Hudi's auto key generation for append-only writes</figcaption><p></p></figure>
<p>Now, to perform append-only writes, you can simply omit the <code>primaryKey</code> property in <code>CREATE TABLE</code> statements (see the example below) or skip setting the <code>hoodie.datasource.write.recordkey.field</code> or <code>hoodie.table.recordkey.fields</code> configurations.</p>
<div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">TABLE</span><span class="token plain"> hudi_table </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ts </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">BIGINT</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    uuid STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    rider STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    driver STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    fare </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">DOUBLE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    city STRING</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">USING</span><span class="token plain"> HUDI</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">PARTITIONED </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">BY</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">city</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><br></span></code></pre></div></div>
<p>In this example, you’re creating a Copy-on-Write table partitioned by <code>city</code>. Because the <code>primaryKey</code> property is not present, Hudi automatically detects the omission and engages the auto key generation feature.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="design-considerations">Design Considerations<a href="https://hudi.apache.org/cn/blog/2025/09/17/hudi-auto-gen-keys#design-considerations" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Designing a key generation mechanism that operates efficiently at petabyte scale requires careful thought. We established five core requirements for the auto-generated keys:</p>
<ol>
<li><strong>Global Uniqueness:</strong> Keys must be unique across the entire table to maintain the integrity of a primary key.</li>
<li><strong>Low Storage Footprint:</strong> The keys should be highly compressible to add minimal storage overhead.</li>
<li><strong>Computational Efficiency:</strong> The encoding and decoding process must be lightweight so as not to slow down the write process.</li>
<li><strong>Idempotency:</strong> The generation process must be resilient to task retries, producing the same key for the same record every time.</li>
<li><strong>Engine Agnostic:</strong> The logic must be reusable and implemented consistently across different execution engines like Spark and Flink.</li>
</ol>
<p>These principles guided the technical design. To align with primary key semantics, global uniqueness was non-negotiable. To minimize storage footprint, the generated keys needed to be compact and highly compressible, especially for tables with billions of records. The computational cost was also critical; any expensive operation would be amplified by the number of records, creating a significant performance overhead. Furthermore, in distributed systems where task failures and retries are common, the key generation process had to be idempotent—ensuring the same input record always produces the exact same key. Finally, the solution needed to be engine-agnostic to provide consistent behavior, whether data is written via Spark, Flink, or another supported engine.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="determining-the-format">Determining the Format<a href="https://hudi.apache.org/cn/blog/2025/09/17/hudi-auto-gen-keys#determining-the-format" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Based on the requirements mentioned previously, we eliminated several common ID generation techniques. For instance, we cannot use simple auto-incrementing IDs for each batch of writes, as it will not satisfy global uniqueness in the table across different writes. We also rule out using the <code>monotonically_increasing_id</code> function in Spark, as it does not guarantee global uniqueness either. Furthermore, using such functions violates the rule of being engine-agnostic. We do not use random ID generation such as UUID (v4, v6, and v7) and ULID, which do not satisfy the idempotency requirement. The final format that we chose is a deterministic, composite key with the following structure:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">&lt;write action start time&gt;-&lt;workload partition ID&gt;-&lt;record sequence ID&gt;</span><br></span></code></pre></div></div>
<p>Each component serves a specific purpose:</p>
<ul>
<li><strong>Write Action Start Time:</strong> The timestamp from the Hudi timeline that marks the beginning of a write transaction.</li>
<li><strong>Workload Partition ID:</strong> An internal identifier that execution engines use to track the specific data split being processed by a given distributed write task.</li>
<li><strong>Record Sequence ID:</strong> A counter that uniquely identifies each record within that data split.</li>
</ul>
<p>Together, these three components—all readily accessible during the write process—form a record identifier that satisfies the requirements of global uniqueness, idempotency, and being engine-agnostic.</p>
<p>Next, we evaluate the generated keys against the requirements of low storage footprint and computational efficiency. The following tables highlight some experiment numbers based on the <a href="https://github.com/apache/hudi/blob/master/rfc/rfc-76/rfc-76.md" target="_blank" rel="noopener noreferrer">RFC document</a> of the auto key generation feature.</p>
<p>For storage efficiency, we compare the original strings with UUID v6/7, Base64, and ASCII encoding schemes:</p>
<table><thead><tr><th style="text-align:left">Format</th><th style="text-align:left">Uncompressed size (bytes)</th><th style="text-align:left">Compressed size (bytes)</th><th style="text-align:left">Compression ratio</th></tr></thead><tbody><tr><td style="text-align:left">Original string</td><td style="text-align:left">4,000,185</td><td style="text-align:left">244,373</td><td style="text-align:left">11.1</td></tr><tr><td style="text-align:left">UUID v6/7</td><td style="text-align:left">4,000,184</td><td style="text-align:left">1,451,897</td><td style="text-align:left">2.74</td></tr><tr><td style="text-align:left">Base64</td><td style="text-align:left">2,400,184</td><td style="text-align:left">202,095</td><td style="text-align:left">11.9</td></tr><tr><td style="text-align:left">ASCII</td><td style="text-align:left">1,900,185</td><td style="text-align:left">176,606</td><td style="text-align:left">10.8</td></tr></tbody></table>
<p>We also compare their compute efficiency using the original string format as the baseline:</p>
<table><thead><tr><th style="text-align:left">Format</th><th style="text-align:left">Average runtime (ms)</th><th style="text-align:left">Ratio to baseline</th></tr></thead><tbody><tr><td style="text-align:left">Original string</td><td style="text-align:left">0.00001</td><td style="text-align:left">1</td></tr><tr><td style="text-align:left">UUID v6/7</td><td style="text-align:left">0.0001</td><td style="text-align:left">10</td></tr><tr><td style="text-align:left">Base64</td><td style="text-align:left">0.004</td><td style="text-align:left">400</td></tr><tr><td style="text-align:left">ASCII</td><td style="text-align:left">0.004</td><td style="text-align:left">400</td></tr></tbody></table>
<p>Based on the micro-benchmarking results, UUID v6/7 resulted in a much larger and undesired compressed size compared to others. Base64 and ASCII encoding had a lower storage footprint compared to the original string, with around 17% and 28% reduction respectively. However, both Base64 and ASCII require 400x more CPU power for encoding than the original string format. Given that write performance is often more critical than marginal storage savings in high-throughput data systems, we opted for the original string format for auto-generating record keys.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="summary">Summary<a href="https://hudi.apache.org/cn/blog/2025/09/17/hudi-auto-gen-keys#summary" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Hudi’s first-class support for record keys provides a database-like experience for lakehouses, enabling powerful features such as record-level indexing, merge modes, and CDC. The introduction of automatic record key generation thoughtfully extends the record key support, removing a barrier for teams performing append-only writes. By following the design principles of uniqueness, idempotency, and efficiency, the feature allows more users to easily adopt Hudi and benefit from its rich set of lakehouse capabilities without the initial overhead of manual key generation. This enhancement reinforces Hudi’s position as a versatile and user-friendly platform for building modern data lakehouses.</p>]]></content>
        <author>
            <name>Shiyan Xu</name>
        </author>
        <category label="hudi" term="hudi"/>
        <category label="record key generation" term="record key generation"/>
        <category label="database" term="database"/>
        <category label="data lakehouse" term="data lakehouse"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Building a RAG-based AI Recommender (2/2)]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/08/29/building-a-rag-based-ai-recommender-2</id>
        <link href="https://hudi.apache.org/cn/blog/2025/08/29/building-a-rag-based-ai-recommender-2"/>
        <updated>2025-08-29T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://blog.datumagic.ai/p/building-a-rag-based-ai-recommender-147">here</a></span>]]></content>
        <author>
            <name>Shiyan Xu</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="AI" term="AI"/>
        <category label="RAG" term="RAG"/>
        <category label="Artificial Intelligence" term="Artificial Intelligence"/>
        <category label="data lakehouse" term="data lakehouse"/>
        <category label="Lakehouse" term="Lakehouse"/>
        <category label="use-case" term="use-case"/>
        <category label="datumagic" term="datumagic"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Dive on Merge-on-Read (MoR) in Lakehouse Table Formats]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison</id>
        <link href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison"/>
        <updated>2025-07-21T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[TL;DR]]></summary>
        <content type="html"><![CDATA[<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>TL;DR</p><ul>
<li>Merge-on-Read tables help manage updates on immutable files without constant rewrites.</li>
<li>Apache Hudi’s MoR tables, with delta logs, file groups, asynchronous compaction, and event-time merging, are well-suited for update-heavy, low-latency streaming and CDC workloads.</li>
<li>Iceberg and Delta Lake also support MoR, but with design differences around delete files and deletion vectors.</li>
</ul></div></div>
<p>As <a href="https://www.onehouse.ai/blog/open-table-formats-and-the-open-data-lakehouse-in-perspective" target="_blank" rel="noopener noreferrer">open table formats</a> like Apache Hudi, Apache Iceberg, and Delta Lake become foundational to modern data lakes, understanding how data is written and read becomes critical for designing high-performance pipelines. One such key dimension is the table's write mechanism, specifically, what happens when <em>updates or deletes</em> are made to these lakehouse tables.</p>
<p>This is where <a href="https://hudi.apache.org/docs/table_types#copy-on-write-table" target="_blank" rel="noopener noreferrer">Copy-on-Write (CoW)</a> and <a href="https://hudi.apache.org/docs/table_types#merge-on-read-table" target="_blank" rel="noopener noreferrer">Merge-on-Read (MoR)</a> table types come into play. These terms were popularized by <a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer">Apache Hudi</a>, in the <a href="https://www.uber.com/blog/hoodie/" target="_blank" rel="noopener noreferrer">original blog</a> from Uber Engineering, when the project was open-sourced in 2017. These strategies exist to overcome a fundamental limitation: data file formats like Parquet and ORC are immutable in nature. Therefore, any update or delete operation that is executed on these files (managed by a lakehouse table format) requires a specific way to deal with it - either by merging changes right away during writes, rewriting entire files (CoW) or maintaining a differential log or delete index that can  be merged at read time (MoR).</p>
<p>Viewed through the lens of the <a href="https://substack.com/home/post/p-159031300?utm_campaign=post&amp;utm_medium=web" target="_blank" rel="noopener noreferrer">RUM Conjecture</a> - which states that optimizing for two of Read, Update, and Memory inevitably requires trading off the third. CoW and MoR emerge as two natural design responses to the trade-offs in lakehouse table formats:</p>
<ul>
<li>
<p>Copy-on-Write tables optimize for read performance. They rewrite Parquet files entirely when a change is made, ensuring clean, columnar files with no extra merge logic at query time. This suits batch-style, read-optimized analytics workloads where write frequency is low.</p>
</li>
<li>
<p>Merge-on-Read, in contrast, introduces flexibility for write-intensive and latency-sensitive workloads by avoiding expensive writes. Instead of rewriting files for every change, MoR tables store updates in delta logs (Hudi), delete files (Iceberg V2), or deletion vectors (Delta Lake). Reads then stitch together the base data files with these changes to present an up-to-date view. This tradeoff favors streaming or near real-time workloads where low write latency is critical.</p>
</li>
</ul>
<p>Here is a generic comparison table between CoW and MoR tables.</p>
<table><thead><tr><th>Trade-Off</th><th>CoW</th><th>MoR</th></tr></thead><tbody><tr><td>Write latency</td><td>Higher</td><td>Lower</td></tr><tr><td>Query latency</td><td>Lower</td><td>Higher</td></tr><tr><td>Update cost</td><td>High</td><td>Low</td></tr><tr><td>File Size Guidance</td><td>Base files should be smaller to keep rewrites manageable</td><td>Base files can be larger, as updates don’t rewrite them directly</td></tr><tr><td>Read Amplification</td><td>Minimal - all changes are already materialized into base files</td><td>Higher - readers must combine base files with change logs or metadata (e.g., delete files or vectors)</td></tr><tr><td>Write Amplification</td><td>Higher - changes often rewrite full files, even for small updates</td><td>Lower - only incremental data (e.g., updates/deletes) is written as separate files or metadata</td></tr></tbody></table>
<p>In this blog, we will understand how various lakehouse table formats implement <strong>MoR</strong> strategy and how the design influences performance and other related factors.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-merge-on-read-works-across-table-formats">How Merge-on-Read Works Across Table Formats<a href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison#how-merge-on-read-works-across-table-formats" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Although Merge-on-Read is a shared concept across open table formats, each system implements it using different techniques, influenced by their internal design philosophy and read-write optimization goals. Here’s a breakdown of how Apache Hudi, Apache Iceberg, and Delta Lake enable Merge-on-Read behavior.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="apache-hudi">Apache Hudi<a href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison#apache-hudi" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Hudi implements Merge-on-Read as one of its two core table types (along with Copy-on-Write), offering a trade-off between read and write costs by maintaining base files alongside delta log files. Instead of rewriting columnar files for every update or delete, MoR tables maintain a combination of base files and log files that encode delta updates/deletes to the base file, enabling fast ingestion and deferred file merging via asynchronous <a href="https://hudi.apache.org/docs/compaction" target="_blank" rel="noopener noreferrer">compaction</a>. This design is particularly suited for streaming ingestion and update-heavy workloads, where minimizing write amplification and achieving high throughput are critical, without any downtime whatsoever for the writers.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="storage-layout">Storage Layout<a href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison#storage-layout" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>At the physical level, a Hudi MoR table stores data in <a href="https://hudi.apache.org/tech-specs/#file-layout-hierarchy" target="_blank" rel="noopener noreferrer"><strong>File Groups</strong></a>, each uniquely identified by a <code>fileId</code>. A file group consists of:</p>
<ul>
<li>Base File (<code>.parquet, .orc</code>): Stores the base snapshot of records in columnar format.</li>
<li>Delta Log Files (<code>.log</code>): Append-only files that capture incremental updates, inserts, and deletes since the last compaction, in either row-oriented data formats like Apache Avro, Hudi’s native SSTable format or columnar-formats like Apache Parquet</li>
</ul>
<p>This hybrid design enables fast writes and defers expensive columnar file writing to asynchronous compaction.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="write-path">Write Path<a href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison#write-path" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>In a Merge-on-Read table, insert and update operations are handled differently to strike a balance between write efficiency and read performance.</p>
<ul>
<li>
<p>Insert operations behave similarly to those in Copy-on-Write tables. New records are written to freshly created <em>base files</em>, aligned to a configured block size. In some cases, these inserts may be merged into the smallest existing base file in the partition to control file counts and sizes.</p>
</li>
<li>
<p>Update operations, however, are written to <em>log files</em> associated with the corresponding file group. These updates in the log files are written using Hudi’s <a href="https://github.com/apache/hudi/blob/45312d437a51ccd1d8c75ba0bd8af21a47dbb9e0/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkMergeOnReadTable.java#L205" target="_blank" rel="noopener noreferrer"><code>HoodieAppendHandle</code></a> class. At runtime, a new instance of <code>HoodieAppendHandle</code> is created with the target <em>partition</em> and <em>file ID</em>. The update records are passed to its <code>write()</code> method, which processes and appends them to the active <em>log file</em> associated with that file group. This mechanism avoids rewriting large Parquet base files and instead accumulates changes in a rolling log structure associated with each base file.</p>
</li>
</ul>
<div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">HoodieAppendHandle appendHandle = new HoodieAppendHandle(config, instantTime, this,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    partitionPath, fileId, recordMap.values().iterator(), taskContextSupplier, header);</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">appendHandle.write(recordMap);</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">List&lt;WriteStatus&gt; writeStatuses = appendHandle.close();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">return Collections.singletonList(writeStatuses).iterator();</span><br></span></code></pre></div></div>
<ul>
<li>Delete operations are also appended to log files as either delete keys or deleted vector positions, to refer to the base file records that were deleted. These delete entries are not applied to the base files immediately. Instead, they are taken into account during snapshot reads, which merge the base and log files to produce the latest view, and during compaction, which merges the accumulated log files (including deletes) into new base files.</li>
</ul>
<p>This design ensures that write operations remain lightweight and fast, regardless of the size of the base files. Writers are not blocked by background compaction or cleanup operations, making the system well-suited for streaming and CDC workloads.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="read-path">Read Path<a href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison#read-path" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>Hudi MoR tables offer flexible read semantics by supporting both <a href="https://hudi.apache.org/docs/sql_queries/#snapshot-query" target="_blank" rel="noopener noreferrer">snapshot queries</a> and <a href="https://hudi.apache.org/docs/table_types#query-types" target="_blank" rel="noopener noreferrer">read-optimized queries</a>, depending on the user's performance and freshness requirements.</p>
<ul>
<li>
<p>Snapshot queries provide the most current view of the dataset by dynamically merging base files with their corresponding log files at read time. The system selects between different reader types based on the nature of the query and the presence of log files:</p>
<ul>
<li>A <strong>full-schema</strong> reader reads the complete row data to ensure correct application of updates and deletes.</li>
<li>A <strong>required-schema</strong> reader projects only the needed columns to reduce I/O, while still applying log file merges.</li>
<li>A <strong>skip-merging</strong> reader is used when log files are absent for a file group, allowing the query engine to read directly from base files without incurring merge costs.</li>
</ul>
</li>
<li>
<p>Read-optimized queries, in contrast, skip reading the delta log files altogether. These queries only scan the base Parquet files, providing faster response times at the cost of not reflecting the latest un-compacted changes. This mode is suitable for applications where slightly stale data is acceptable or where performance is critical.</p>
</li>
</ul>
<p>Together, these two read strategies allow Hudi MoR tables to serve both real-time and interactive queries from the same dataset, adjusting behavior depending on the workload and latency constraints.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="compaction">Compaction<a href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison#compaction" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>As log files accumulate new updates and deletes, Hudi triggers a compaction operation to merge these log files back into columnar base files. This process is <a href="https://hudi.apache.org/docs/compaction#async--offline-compaction-models" target="_blank" rel="noopener noreferrer">configurable and asynchronous</a>, and plays a key role in balancing write and read performance.</p>
<p>Compaction in Hudi is triggered based on thresholds that can be configured by the user, such as the <em>number of commits (NUM_COMMITS)</em>. During compaction, all log files associated with a file group are read and merged with the existing base file to produce a new compacted base file.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="apache-iceberg">Apache Iceberg<a href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison#apache-iceberg" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Apache Iceberg supports Merge-on-Read (MoR) semantics by maintaining immutable base data files and tracking updates and deletions through separate <a href="https://iceberg.apache.org/spec/#delete-formats" target="_blank" rel="noopener noreferrer"><em>delete files</em></a>. This design avoids rewriting data files for every update or delete operation. Instead, these changes are applied at query time by merging delete files with the base files to produce an up-to-date view.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="storage-layout-1">Storage Layout<a href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison#storage-layout-1" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>An Iceberg table consists of:</p>
<ul>
<li>Base Data Files: Immutable Parquet, ORC, or Avro files that contain the primary data.</li>
<li>Delete Files: Auxiliary files that record row-level deletions.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="write-path-1">Write Path<a href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison#write-path-1" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>In Iceberg’s MoR tables, write operations implement row-level updates by encoding them as a delete of the old record and an insert of the new one. Rather than modifying existing Parquet base files directly, Iceberg maintains a clear separation between new data and logical deletes by introducing delete files alongside new data files.</p>
<ul>
<li>Inserts behave in the same way as CoW tables. The new data is appended to the table as part of a new snapshot.</li>
<li>For delete operations, Iceberg writes a delete file containing rows to be logically removed across multiple base files. Delete files are of two types:<!-- -->
<ul>
<li>Position Deletes: Reference row positions in a specific data file.</li>
<li>Equality Deletes: Encode a predicate that matches rows based on one or more column values.</li>
</ul>
</li>
</ul>
<p>Equality deletes are typically not favored in performance sensitive data platforms, since it forces predicate evaluation against every single base file during snapshot reads.</p>
<p>The <code>DeleteFile</code> interface captures these semantics:</p>
<div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">public interface DeleteFile extends ContentFile&lt;StructLike&gt; {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  enum DeleteType {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    EQUALITY, POSITION</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre></div></div>
<p><strong>Note:</strong> Iceberg v3 introduces Deletion Vectors as a more efficient alternative to positional deletes. Deletion vectors attach a <em>bitmap</em> to a data file to indicate deleted rows, allowing query engines to skip over deleted rows at read time. Deletion Vectors are already supported by Delta Lake and Hudi and this is now borrowed into the Iceberg spec as well.</p>
<ul>
<li>For update operations, Iceberg uses a two-step process. An update is implemented as a delete + insert pattern. First, a delete file is created to logically remove the old record, using either a position or equality delete. Then, a new data file is written that contains the full image of the updated record. Both the delete file and the new data file are added in a single atomic commit, creating a new snapshot of the table. This behavior is implemented via the <code>RowDelta</code> interface:</li>
</ul>
<div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">RowDelta rowDelta = table.newRowDelta()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    .addDeletes(deleteFile)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    .addRows(dataFile);</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">rowDelta.commit();</span><br></span></code></pre></div></div>
<p>All write operations, whether adding new data files or new delete files, produce a new snapshot in Iceberg’s timeline. This guarantees consistent isolation across readers and writers while avoiding any rewriting of immutable data files.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="read-path-1">Read Path<a href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison#read-path-1" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>During query execution, Iceberg performs a Merge-on-Read query by combining the immutable base data files with any relevant delete files to present a consistent and up-to-date view. Before reading, the scan planning logic identifies which delete files apply to each data file, ensuring that deletes are correctly associated with their targets.</p>
<p>This planning step guarantees that any row marked for deletion through either position deletes or equality deletes is filtered out of the final results, while the original base files remain unchanged. The merging of base data with delete files is applied dynamically by the query engine, allowing Iceberg to preserve the immutable file structure and still deliver row-level updates.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="delta-lake">Delta Lake<a href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison#delta-lake" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Delta Lake supports Merge-on-Read semantics using <a href="https://docs.delta.io/latest/delta-deletion-vectors.html" target="_blank" rel="noopener noreferrer"><em>Deletion Vectors (DVs)</em></a>, a feature that allows rows to be logically removed from a dataset without rewriting the base Parquet files. This enables efficient row-level delete   while preserving immutability of data files. For updates, Delta Lake encodes changes as a combination of DELETE and INSERT operations, i.e. the old row is marked as deleted, and a new row with updated values is appended.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="storage-layout-2">Storage Layout<a href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison#storage-layout-2" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>In Delta Lake, the storage layout consists of:</p>
<ul>
<li>Base Data Files: Immutable Parquet files that hold the core data</li>
<li>Deletion Vectors: Structures that track rows that should be considered deleted during reads, instead of physically removing them from Parquet</li>
</ul>
<p>A deletion vector is described by a descriptor which captures its storage type (inline, on-disk, or UUID-based), its physical location or inline data, an offset if stored on disk, its size in bytes, and the cardinality (number of rows it marks as deleted).</p>
<p>Small deletion vectors can be embedded directly into the Delta transaction log (inline), while larger ones are stored as separate files, with the UUIDs referencing them by a unique identifier.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="write-path-2">Write Path<a href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison#write-path-2" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>When a DELETE, UPDATE, or MERGE operation is performed on a Delta table, Delta Lake does not rewrite the affected base Parquet files. Instead, it generates a deletion vector that identifies which rows are logically removed. These deletion vectors are built as compressed bitmap structures (using Roaring Bitmaps), which efficiently encode the positions of the deleted rows.</p>
<p>Smaller deletion vectors are kept inline in the transaction log for quick lookup, while larger ones are persisted as separate deletion vector files. All write operations that affect rows in this way update the metadata to track the associated deletion vectors, maintaining a consistent and atomic snapshot view for downstream reads.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="read-path-2">Read Path<a href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison#read-path-2" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>During query execution, Delta Lake consults any deletion vectors attached to the current snapshot. The query execution loads these deletion vectors and applies them dynamically, filtering out rows marked as deleted before returning results to the user. This happens without rewriting or modifying the base Parquet files, preserving their immutability while still providing correct row-level semantics.</p>
<p>This Merge-on-Read approach allows Delta Lake to combine efficient write operations with the ability to serve up-to-date views, ensuring that queries see a consistent, deletion-aware representation of the dataset.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="comparative-design-analysis">Comparative Design Analysis<a href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison#comparative-design-analysis" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Merge-on-Read semantics are implemented differently across open table formats, with each approach reflecting distinct trade-offs that influence workload performance, complexity, and operational flexibility. MoR is generally well-suited for high-throughput, low-latency streaming ingestion scenarios in a lakehouse, where frequent updates and late-arriving data are expected. In contrast, Copy-on-Write (CoW) tables often work best for simpler, batch-oriented workloads where updates are infrequent and read-optimized behavior is a priority.</p>
<p>In this section, we focus on Apache Hudi and Apache Iceberg table formats and explore how their MoR designs influence real-world workloads.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="streaming-data-support--event-time-ordering">Streaming Data Support &amp; Event-Time Ordering<a href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison#streaming-data-support--event-time-ordering" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Hudi’s Merge-on-Read design supports event-time ordering and late-arriving data for streaming workloads by providing <a href="https://hudi.apache.org/docs/record_merger#record-payloads" target="_blank" rel="noopener noreferrer"><code>RecordPayload</code></a> and <a href="https://hudi.apache.org/docs/record_merger" target="_blank" rel="noopener noreferrer"><code>RecordMerger</code></a> APIs. These allow updates to be merged based on database sequence numbers or event timestamps, so that if data arrives out of order or has late arriving data, the final state is still correct from a temporal perspective.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig1.png" alt="index" width="1000" align="middle">
<p>Iceberg uses a last-writer-wins approach, where the most recent commit determines record values regardless of event time. This design may be tricky to deal with late-arriving data  in streaming workloads or CDC ingestion. For e.g. if the source stream is ever repositioned to an earlier time, it will cause the table to move backwards in time where older record values from the replayed stream overwrite newer record images in the table.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="scalable-incremental-write-costs">Scalable Incremental Write Costs<a href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison#scalable-incremental-write-costs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>One of the main goals of MoR is to reduce write costs and latencies by avoiding full file rewrites. Hudi achieves this by appending changes to <em>delta logs</em> and using <a href="https://hudi.apache.org/docs/indexes" target="_blank" rel="noopener noreferrer"><strong>indexing</strong></a> to quickly identify which file group an incoming update belongs to. Hudi supports different index types to accelerate this lookup process, so it does not need to scan the entire table on every update. This ensures that even if you are updating a relatively small amount of data - for example, 1GB of changes into a 1TB table every five to ten minutes, the system can efficiently target only the affected files.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig2.png" alt="index" width="1000" align="middle">
<p>Iceberg handles row-level updates and deletes by recording them as <a href="https://iceberg.apache.org/spec/#delete-formats" target="_blank" rel="noopener noreferrer"><em>delete files</em></a>. To identify which records to update or delete, Iceberg relies on scanning table metadata, and in some cases file-level data, to locate affected rows. This design uses a simple metadata approach but if partitioning is not highly selective, this lookup step can become a bottleneck for write performance on large tables with frequent small updates.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="asynchronous-compaction-during-merge">Asynchronous Compaction during ‘Merge’<a href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison#asynchronous-compaction-during-merge" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Hudi employs <a href="https://hudi.apache.org/blog/2025/01/28/concurrency-control#occ-multi-writers" target="_blank" rel="noopener noreferrer">optimistic concurrency control</a> (OCC) between writers and maintains blocking-free <a href="https://hudi.apache.org/blog/2025/01/28/concurrency-control#mvcc-writer-table-service-and-table-service-table-service" target="_blank" rel="noopener noreferrer">multi-version concurrency control</a> (MVCC) between writers and its asynchronous compaction process. This means writers can continue appending updates to the same records while earlier versions are being compacted in the background. Compaction operates <em>asynchronously</em>, creating new base files from accumulated log files, without interfering with active writers. This ensures great data freshness as well as better compression ratio and thus excellent query performance for columnar files longer term.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig3.png" alt="index" width="1000" align="middle">
<p>Iceberg maintains consistent snapshots across all operations, but it does not separate a dedicated compaction action from other write operations. As a result, if both a writer and a maintenance process try to modify overlapping data, standard snapshot conflict resolution ensures only one succeeds and might require retries in some concurrent write scenarios, but there is no asynchronous way to run compaction services. This could lead to livelocking between the writer and table maintenance, where one of them continuously causes the other to fail.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="non-blocking-concurrency-control-nbcc-for-real-time-applications">Non-Blocking Concurrency Control (NBCC) for Real-time applications<a href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison#non-blocking-concurrency-control-nbcc-for-real-time-applications" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Hudi 1.0 further extends its concurrency model to allow multiple writers to safely update the same record at the same time with <a href="https://hudi.apache.org/blog/2025/01/28/concurrency-control#non-blocking-concurrency-control-multi-writers" target="_blank" rel="noopener noreferrer">non-blocking conflict resolution</a>. It supports serializability guarantees based on write completion timestamps (arrival-time processing), while also allowing record merging according to event-time order if required. This flexible concurrency strategy enables concurrent writes to proceed, without the need to wait, making it ideal for real-time applications that demand faster ingestion.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig4.png" alt="index" width="700" align="middle">
<p>Iceberg applies OCC through its snapshot approach, where writers commit updates against the latest known snapshot, and if conflicts are detected, retries are required. There is no explicit distinction between arrival-time and event-time semantics for concurrent record updates.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="minimizing-read-costs">Minimizing Read Costs<a href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison#minimizing-read-costs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Hudi organizes records into <em>file groups,</em> ensuring that updates are consistently routed back to the same group where the original records were stored. This approach means that when a query is executed, it only needs to scan the base file and any delta log files within that specific file group, reducing the data that must be read and merged at query time. By tying updates and inserts to a consistent file group, Hudi preserves locality and limits merge complexity.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig5.png" alt="index" width="1000" align="middle">
<p>Iceberg applies updates and deletes using <em>delete files</em>, and these can reference any row in any base file. As a result, readers must examine all relevant delete files along with all associated base data files during scan planning and execution, which can increase I/O and metadata processing requirements for large tables.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="performant-read-side-merge">Performant Read-Side Merge<a href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison#performant-read-side-merge" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Hudi’s MoR implementation uses <em>key-based</em> merging to reconcile delta log records with base files, which allows query engines to push down filters and still correctly merge updates based on record keys. This selective merging reduces unnecessary I/O and improves performance for queries that only need a subset of columns or rows.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig6.png" alt="index" width="800" align="middle">
<p>Iceberg historically required readers (particularly Spark readers) to load entire base files when applying positional deletes. This was because pushing down filters could change the order or number of rows returned by the Parquet reader, making positional delete applications incorrect. As a result, filter pushdowns could not be safely applied, forcing a full file scan to maintain correctness. There has been ongoing work in the Iceberg community to address this limitation by improving how positional information is tracked through filtered reads.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="efficient-compaction-planning">Efficient Compaction Planning<a href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison#efficient-compaction-planning" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Hudi’s compaction strategy operates at the level of individual file groups, which means it can plan and execute small, predictable units of compaction work. This fine-grained approach allows compaction to proceed <em>incrementally</em> and avoids large, unpredictable workloads.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig7.png" alt="index" width="800" align="middle">
<p>In Iceberg, compaction must consider all base files and their related delete files together, because delete files reference rows in the base data files. This creates a dependency graph where all related files must be handled in a coordinated way. As delete files accumulate over time, these compaction operations can become increasingly large and complex to plan, making it harder to schedule resources efficiently. If compaction falls behind, the amount of data that must be compacted in future operations continues to grow, potentially making the problem worse.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="temporal-and-spatial-locality-for-event-time-filters">Temporal and Spatial Locality for Event-Time Filters<a href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison#temporal-and-spatial-locality-for-event-time-filters" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Hudi maintains temporal and spatial locality by ensuring that updates and deletes are routed back to the same file group where the original record was first stored. This preserves the time-based clustering or ordering of records, which is especially beneficial for queries filtering by event time or operating within specific time windows. By keeping related records together, Hudi enables efficient pruning of file groups along with partition pruning, during time-based queries.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig8.png" alt="index" width="1000" align="middle">
<p>Iceberg handles updates by deleting the existing record and inserting a new one, which may place the updated record in a different data file. Over time, this can scatter records that belong to the same logical or temporal group across multiple files, reducing the effectiveness of partition pruning and requiring periodic clustering or optimization to restore temporal locality.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="partial-updates-for-performant-merge">Partial Updates for Performant Merge<a href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison#partial-updates-for-performant-merge" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Hudi supports partial updates by encoding only the columns that have changed into its delta log files. This means the cost of merging updates is proportional to the number of columns actually modified, rather than the total width of the record. For columnar datasets with wide schemas, this can significantly reduce write amplification and improve merge performance.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-21-mor-comparison/mor_fig9.png" alt="index" width="800" align="middle">
<p>In Iceberg, updates are implemented as a delete plus a full-row insert, which requires rewriting the entire record even if only a single column has changed. As a result, update costs in Iceberg scale with the total number of columns in the record, increasing I/O and storage requirements for wide tables with frequent column-level updates.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://hudi.apache.org/cn/blog/2025/07/21/mor-comparison#conclusion" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Merge-on-Read (MoR) table type provides an alternative approach to managing updates and deletes on immutable columnar files in a lakehouse. While multiple open table formats support MoR semantics, their design choices significantly affect suitability for real-time and change-data driven workloads.</p>
<p>Apache Hudi’s MoR implementation specifically addresses the needs of high-ingestion, update-heavy pipelines. By appending changes to delta logs, preserving file-group-based data locality, supporting event-time ordering, and enabling asynchronous, non-blocking compaction, Hudi minimizes write amplification and supports low-latency data availability. These design primitives directly align with streaming and CDC patterns, where data arrives frequently and potentially out of order. Iceberg and Delta Lake also implement MoR semantics in their own ways to address transactional consistency and immutable storage goals.</p>
<hr>]]></content>
        <author>
            <name>Dipankar Mazumdar</name>
        </author>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="Merge-on-Read (MoR)" term="Merge-on-Read (MoR)"/>
        <category label="Streaming" term="Streaming"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modernizing Data Infrastructure at Peloton Using Apache Hudi]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi</id>
        <link href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi"/>
        <updated>2025-07-15T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Peloton re-architected its data platform using Apache Hudi to overcome snapshot delays, rigid service coupling, and high operational costs. By adopting CDC-based ingestion from PostgreSQL and DynamoDB, moving from CoW to MoR tables, and leveraging asynchronous services with fine-grained schema control, Peloton achieved 10-minute ingestion cycles, reduced compute/storage overhead, and enabled time travel and GDPR compliance.]]></summary>
        <content type="html"><![CDATA[<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>TL;DR</div><div class="admonitionContent_BuS1"><p>Peloton re-architected its data platform using Apache Hudi to overcome snapshot delays, rigid service coupling, and high operational costs. By adopting CDC-based ingestion from PostgreSQL and DynamoDB, moving from CoW to MoR tables, and leveraging asynchronous services with fine-grained schema control, Peloton achieved 10-minute ingestion cycles, reduced compute/storage overhead, and enabled time travel and GDPR compliance.</p></div></div>
<p>Peloton is a global interactive fitness platform that delivers connected, instructor-led fitness experiences to millions of members worldwide. Known for its immersive classes and cutting-edge equipment, Peloton combines software, hardware, and data to create personalized workout journeys. With a growing member base and increasing product diversity, data has become central to how Peloton delivers value. The <em>Data Platform</em> team at Peloton is responsible for building and maintaining the core infrastructure that powers analytics, reporting, and real-time data applications. Their work ensures that data flows seamlessly from transactional systems to the data lake, enabling teams across the organization to make timely, data-driven decisions.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-challenge-data-growth-latency-and-operational-bottlenecks">The Challenge: Data Growth, Latency, and Operational Bottlenecks<a href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi#the-challenge-data-growth-latency-and-operational-bottlenecks" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>As Peloton evolved into a global interactive fitness platform, its data infrastructure was challenged by the growing need for timely insights, agile service migrations, and cost-effective analytics. Daily operations, recommendation systems, and compliance requirements demanded an architecture that could support near real-time access, high-frequency updates, and scalable service boundaries.</p>
<p>However, the team faced persistent bottlenecks with the existing setup:</p>
<ul>
<li>Reporting pipelines were gated by the completion of full snapshot jobs.</li>
<li>Recommender systems could only function on daily refreshed datasets.</li>
<li>The analytics platform was tightly coupled with operational systems.</li>
<li>Microservice migrations were constrained to all-at-once shifts.</li>
<li>Database read replicas incurred high infrastructure costs.</li>
</ul>
<p>These limitations made it difficult to meet SLA expectations, scale workloads efficiently, and adapt the platform to new user and product needs.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-legacy-architecture">The Legacy Architecture<a href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi#the-legacy-architecture" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig1.png" alt="challenge" width="1000" align="middle">
<p>Peloton's earlier architecture relied on daily snapshots from a monolithic <strong>PostgreSQL</strong> database. The analytics systems would consume these snapshots, often waiting hours for completion. This not only delayed reporting but also introduced downstream rigidity.</p>
<p>Because the same data platform supported both online and analytical workloads, any schema or service migration required significant planning and coordination. Database read replicas, used to scale reads, increased cost overhead. Moreover, recommendation systems that depended on data freshness were constrained by the snapshot interval, limiting personalization capabilities. This architecture struggled to support a fast-moving product roadmap, near real-time analytics, and the data agility needed to experiment and iterate.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="reimagining-the-data-platform-with-apache-hudi">Reimagining the Data Platform with Apache Hudi<a href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi#reimagining-the-data-platform-with-apache-hudi" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig2.png" alt="challenge" width="1000" align="middle">
<p>To address these challenges, the data platform team introduced Apache Hudi as the foundation of its modern data lake. The architecture was rebuilt to support Change Data Capture (CDC) ingestion from both PostgreSQL and DynamoDB using Debezium, with Kafka acting as the transport layer. A custom-built Hudi writer was developed to ingest CDC records into S3 using Apache Spark on EMR (version 6.12.0 with Hudi 0.13.1).</p>
<p>Peloton initially chose Copy-on-Write (CoW) table formats to support querying via Redshift Spectrum and simplify adoption. However, performance and cost bottlenecks prompted a transition to Merge-on-Read (MoR) tables with asynchronous table services for cleaning and compaction.</p>
<p>Key architectural enhancements included:</p>
<ul>
<li><strong>Support for GDPR compliance</strong> through structured delete propagation.</li>
<li><strong>Time travel queries</strong> for recommender model training and data recovery.</li>
<li><strong>Phased migration support</strong> for microservices via decoupled ingestion.</li>
</ul>
<p>Peloton's broader data platform tech stack supports this architecture with a range of tools for orchestration, analytics, and governance. This includes EMR for compute, Redshift for querying, DBT for data transformations, Looker for BI and visualization, Airflow for orchestration, and DataHub for metadata management. These components complement Apache Hudi in forming a modular and production-ready lakehouse stack.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig3.png" alt="challenge" width="1000" align="middle">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="learnings-from-running-hudi-at-scale">Learnings from Running Hudi at Scale<a href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi#learnings-from-running-hudi-at-scale" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>With Hudi now integrated into Peloton's data lake, the team began to observe and address new operational and architectural challenges that emerged at scale. This section outlines the major lessons learned while maintaining high-ingestion throughput, ensuring data reliability, and keeping infrastructure costs under control.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cow-vs-mor-performance-trade-offs">CoW vs MoR: Performance Trade-offs<a href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi#cow-vs-mor-performance-trade-offs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Initially, Copy-on-Write (CoW) tables were chosen to simplify deployment and ensure compatibility with Redshift Spectrum. However, as ingestion frequency increased and update volumes spanned hundreds of partitions, performance became a bottleneck. Some high-frequency tables with updates across 256 partitions took nearly an hour to process per run. Additionally, retaining 30 days of commits for training recommender models significantly inflated storage requirements, reaching into the hundreds of gigabytes.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig4.png" alt="challenge" width="1000" align="middle">
<p>To resolve this, the team migrated to Hudi’s Merge-on-Read (MoR) tables and reduced commit retention to 7 days. With ingestion jobs now running every 10 minutes, latency dropped significantly, and storage and compute usage became more efficient.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="async-vs-inline-table-services">Async vs Inline Table Services<a href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi#async-vs-inline-table-services" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>To improve write throughput and meet low-latency ingestion goals, the Peloton team initially configured Apache Hudi with asynchronous cleaner and compactor services. This approach worked well across most tables, allowing ingestion pipelines to run every 10 minutes with minimal blocking but introduced some operational edge cases. Some of the challenges encountered included:</p>
<ul>
<li>Concurrent execution of writer and cleaner jobs, leading to conflicts. These were mitigated by introducing DynamoDB-based locks to serialize access.</li>
<li>Reader-cleaner race conditions, where time travel queries intermittently failed with <code>"File Not Found"</code> errors - traced back to cleaners deleting files mid-read.</li>
<li>Compaction disruptions caused by EMR node terminations, which led to orphaned files when jobs failed mid-way.</li>
</ul>
<p>These edge cases were largely due to the operational complexity of managing concurrent workloads at Peloton’s scale. After weighing reliability against latency, the team opted to switch to inline table services for compaction and cleaning, augmented with custom logic to control when these actions would run. This change improved system stability while maintaining acceptable latency trade-offs.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="glue-schema-version-limits">Glue Schema Version Limits<a href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi#glue-schema-version-limits" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>As schema evolution continued, the team used Hudi's <code>META_SYNC_ENABLED</code> to sync schema updates with AWS Glue. Over time, high-frequency schema updates pushed the number of <code>TABLE_VERSION</code> resources in Glue beyond the <em>1 million</em> limit. This caused jobs to fail in ways that were initially difficult to trace.</p>
<p>One such failure manifested as the following error:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">ERROR Client: Application diagnostics message: User class threw exception:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">java.lang.NoSuchMethodError: 'org.apache.hudi.exception.HoodieException </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">org.apache.hudi.sync.common.util.SyncUtilHelpers.getExceptionFromList(java.util.Collection)'</span><br></span></code></pre></div></div>
<p>After significant debugging, the issue was traced to AWS Glue limits. The team implemented a multi-step fix:</p>
<ul>
<li>Worked with AWS to temporarily raise resource limits.</li>
<li>Developed a Python service to identify and delete outdated table versions, removing over 1 million entries.</li>
<li>Added an Airflow job to schedule weekly cleanup tasks.</li>
<li>Improved schema sync logic to trigger only when the schema changed.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="debezium--toast-handling">Debezium &amp; TOAST Handling<a href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi#debezium--toast-handling" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>PostgreSQL CDC ingestion posed unique challenges due to the database’s handling of large fields using TOAST (The Oversized-Attribute Storage Technique). When fields over 8KB were unchanged, Debezium emitted a placeholder value <code>__debezium_unavailable_value</code>, making it impossible to determine whether the value had changed.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig5.png" alt="challenge" width="1000" align="middle">
<p>To address this, Peloton:</p>
<ul>
<li>Populated initial data using PostgreSQL snapshots.</li>
<li>Implemented self-joins between incoming CDC records and existing Hudi records to fill in missing values.</li>
<li>Separated inserts, updates, and deletes within Spark batch processing.</li>
<li>Used the <code>ts</code> field as the precombine key to ensure only the latest record state was retained.</li>
</ul>
<p>A reconciliation pipeline was also developed to heal data inconsistencies caused by multiple operations on the same key within a batch (e.g., create-delete-create).</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="data-validation-and-quality-enforcement">Data Validation and Quality Enforcement<a href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi#data-validation-and-quality-enforcement" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Data quality was critical to ensure trust in the newly established data lake. The team developed several internal libraries and checks:</p>
<ul>
<li>A Crypto Shredding Library to encrypt <code>user_id</code> and other PII fields before storage.</li>
<li>A Data Validation Framework that compared records in the lake against snapshot data.</li>
<li>A Data Quality Library that enforced column-level thresholds. These checks integrated with DataHub and were tied to Airflow sensors to halt downstream jobs on failures.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="dynamodb-ingestion-and-schema-challenges">DynamoDB Ingestion and Schema Challenges<a href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi#dynamodb-ingestion-and-schema-challenges" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Some Peloton services relied on DynamoDB for operational workloads (NoSQL). To ingest these datasets into the lake, the team used DynamoDB Streams and a Kafka Connector, allowing reuse of the existing Kafka-based Hudi ingestion path.</p>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig6.png" alt="challenge" width="1000" align="middle">
<p>However, the NoSQL nature of DynamoDB introduced schema management challenges. Two strategies were evaluated:</p>
<ol>
<li>Stakeholder-defined schemas, using SUPER-type fields.</li>
<li>Dynamic schema inference, where incoming JSON records were parsed, and the evolving schema was inferred and reconciled.</li>
</ol>
<p>The team opted for dynamic inference despite increased processing time, as it enabled better support for exploratory workloads. Daily snapshots and reconciliation steps helped clean up inconsistent schema states.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="reducing-operational-costs">Reducing Operational Costs<a href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi#reducing-operational-costs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<img src="https://hudi.apache.org/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig7.png" alt="challenge" width="1000" align="middle">
<p>As the system matured, cost optimization became a priority. The team used <a href="https://github.com/ganglia/" target="_blank" rel="noopener noreferrer">Ganglia</a> to analyze job profiles and identify areas for improvement:</p>
<ul>
<li>EMR resources were gradually right-sized based on CPU and memory usage.</li>
<li>Conditional Hive syncing was introduced to avoid unnecessary sync operations during each run.</li>
<li>A Spark-side inefficiency was discovered where archived timelines were unnecessarily loaded, causing jobs to take 4x longer. Fixing this reduced overall latency and compute resource usage.</li>
</ul>
<p>These operational refinements significantly reduced idle times and improved the cost-efficiency of the platform.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="gains-from-hudi-adoption">Gains from Hudi Adoption<a href="https://hudi.apache.org/cn/blog/2025/07/15/modernizing-datainfra-peloton-hudi#gains-from-hudi-adoption" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Peloton's transition to Apache Hudi led to measurable performance, operational, and cost-related improvements across its modern data platform.</p>
<p>Peloton's transition to Apache Hudi yielded several measurable improvements:</p>
<ul>
<li>Ingestion frequency increased from once daily to every 10 minutes.</li>
<li>Reduced snapshot job durations from an hour to under 15 minutes.</li>
<li>Cost savings by eliminating read replicas and optimizing EMR cluster usage.</li>
<li>Time travel support enabled retrospective analysis and model re-training.</li>
<li>Improved compliance posture through structured deletes and encrypted PII.</li>
</ul>
<p>The modernization laid the groundwork for future evolution, including real-time streaming ingestion using Apache Flink and continued improvements in data freshness, latency, and governance.</p>
<p>This blog is based on Peloton’s presentation at the Apache Hudi Community Sync. If you are interested in watching the recorded version of the video, you can find it <a href="https://youtu.be/-Pyid5K9dyU?feature=shared" target="_blank" rel="noopener noreferrer">here</a>.</p>
<hr>]]></content>
        <author>
            <name>Amaresh Bingumalla, Thinh Kenny Vu, Gabriel Wang, Arun Vasudevan in collaboration with Dipankar Mazumdar</name>
        </author>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="Peloton" term="Peloton"/>
        <category label="Community" term="Community"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[How PayU built a secure enterprise AI assistant using Amazon Bedrock]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/07/15/PayU-built-a-secure-enterprise-AI-assistant</id>
        <link href="https://hudi.apache.org/cn/blog/2025/07/15/PayU-built-a-secure-enterprise-AI-assistant"/>
        <updated>2025-07-15T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://aws.amazon.com/blogs/machine-learning/how-payu-built-a-secure-enterprise-ai-assistant-using-amazon-bedrock/">here</a></span>]]></content>
        <author>
            <name>Deepesh Dhapola, Mudit Chopra, Rahmat Khan, Rahul Ghosh, Saikat Dey, and Sandeep Kumar Veerlapati</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="AWS" term="AWS"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Building a RAG-based AI Recommender (Part 1/2)]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/07/10/building-a-rag-based-ai-recommender</id>
        <link href="https://hudi.apache.org/cn/blog/2025/07/10/building-a-rag-based-ai-recommender"/>
        <updated>2025-07-10T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://blog.datumagic.ai/p/building-a-rag-based-ai-recommender">here</a></span>]]></content>
        <author>
            <name>Shiyan Xu</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="AI" term="AI"/>
        <category label="RAG" term="RAG"/>
        <category label="Artificial Intelligence" term="Artificial Intelligence"/>
        <category label="data lakehouse" term="data lakehouse"/>
        <category label="Lakehouse" term="Lakehouse"/>
        <category label="use-case" term="use-case"/>
        <category label="datumagic" term="datumagic"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Stifel built a modern data platform using AWS Glue and an event-driven domain architecture]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/07/07/how-stifel-built-a-modern-data-platform-using-aws-glue-and-an-event-driven-domain-architecture</id>
        <link href="https://hudi.apache.org/cn/blog/2025/07/07/how-stifel-built-a-modern-data-platform-using-aws-glue-and-an-event-driven-domain-architecture"/>
        <updated>2025-07-07T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://aws.amazon.com/blogs/big-data/how-stifel-built-a-modern-data-platform-using-aws-glue-and-an-event-driven-domain-architecture/">here</a></span>]]></content>
        <author>
            <name>Amit Maindola and Srinivas Kandi, Hossein Johari, Ahmad Rawashdeh, Lei Meng</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="aws" term="aws"/>
        <category label="AWS Glue" term="AWS Glue"/>
        <category label="AWS Blogs" term="AWS Blogs"/>
        <category label="Amazon EMR" term="Amazon EMR"/>
        <category label="AWS Lake Formation" term="AWS Lake Formation"/>
        <category label="Data Governance" term="Data Governance"/>
        <category label="Lakehouse" term="Lakehouse"/>
        <category label="use-case" term="use-case"/>
        <category label="det" term="det"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why Uber Built Hudi: The Strategic Decision Behind a Custom Table Format]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/07/03/why-uber-built-hudi-the-strategic-decision-behind-a-custom-table-format</id>
        <link href="https://hudi.apache.org/cn/blog/2025/07/03/why-uber-built-hudi-the-strategic-decision-behind-a-custom-table-format"/>
        <updated>2025-07-03T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://thamizhelango.medium.com/why-uber-built-hudi-the-strategic-decision-behind-a-custom-table-format-f57db68b0cb9">here</a></span>]]></content>
        <author>
            <name>ThamizhElango Natarajan</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="Apache Iceberg" term="Apache Iceberg"/>
        <category label="Lakehouse" term="Lakehouse"/>
        <category label="use-case" term="use-case"/>
        <category label="Uber" term="Uber"/>
        <category label="det" term="det"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lakehouse Architecture - Apache Hudi and Apache Iceberg]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/07/02/Lakehouse-Architecture-apache-hudi-and-apache-iceberg</id>
        <link href="https://hudi.apache.org/cn/blog/2025/07/02/Lakehouse-Architecture-apache-hudi-and-apache-iceberg"/>
        <updated>2025-07-02T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://www.linkedin.com/pulse/lakehouse-architecture-apache-hudi-iceberg-becloudready-4b1ac/">here</a></span>]]></content>
        <author>
            <name>beCloudReady</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="Apache Iceberg" term="Apache Iceberg"/>
        <category label="Lakehouse" term="Lakehouse"/>
        <category label="use-case" term="use-case"/>
        <category label="det" term="det"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Complex Data Workflows at Uber Using Apache Hudi]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/06/30/uber-hudi</id>
        <link href="https://hudi.apache.org/cn/blog/2025/06/30/uber-hudi"/>
        <updated>2025-06-30T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Uber’s trip and order collection pipelines grew highly complex, with long runtimes, massive DAGs, and rigid SQL logic that hampered scalability and maintainability. By adopting Apache Hudi, Uber re-architected these pipelines to enable incremental processing, custom merge behavior, and rule-based functional transformations. This reduced runtime from 20 hours to 4 hours, improved test coverage to 95%, cut costs by 60%, and delivered a composable, explainable, and scalable data workflow architecture.]]></summary>
        <content type="html"><![CDATA[<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>TL;DR</div><div class="admonitionContent_BuS1"><p>Uber’s trip and order collection pipelines grew highly complex, with long runtimes, massive DAGs, and rigid SQL logic that hampered scalability and maintainability. By adopting Apache Hudi, Uber re-architected these pipelines to enable incremental processing, custom merge behavior, and rule-based functional transformations. This reduced runtime from 20 hours to 4 hours, improved test coverage to 95%, cut costs by 60%, and delivered a composable, explainable, and scalable data workflow architecture.</p></div></div>
<p>At Uber, the Core Services Data Engineering team supports a wide range of use cases across products like Uber Mobility and Uber Eats. One critical use case is computing the collection - the net payable amount - from a trip or an order. While this sounds straightforward at first, it quickly becomes a complex data problem when you factor in real-world scenarios like refunds, tips, driver disputes, location updates, and settlement adjustments across multiple verticals.</p>
<p>To solve this problem at scale, Uber re-architected their pipelines using <a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer">Apache Hudi</a> to enable low-latency, incremental, and rule-based processing. This post outlines the challenges they faced, the architectural shifts they made, and the measurable outcomes they achieved in production.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-challenge-scale-latency-and-complexity">The Challenge: Scale, Latency, and Complexity<a href="https://hudi.apache.org/cn/blog/2025/06/30/uber-hudi#the-challenge-scale-latency-and-complexity" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<img src="https://hudi.apache.org/assets/images/blog/figure2_uber.png" alt="challenge" width="800" align="middle">
<p>Our original data pipelines were processing nearly 90 million records a day, but the nature of updates made them inefficient. For instance, a trip taken three years ago could still be updated due to a late settlement. Our statistical analysis showed most updates occur within 180 days, so we designed the system to read and write a 180-day window every day - leading to severe read and write amplification.</p>
<p>The pipeline itself was a massive DAG with over 50–60 tasks, taking close to 20 hours to complete. These long runtimes made recovery difficult and introduced operational risks. Making a change meant tracing the logic across this sprawling DAG, which affected developer productivity and increased the chances of regressions.</p>
<p>Despite the large window, we still missed updates that fell outside the 180-day mark, leading to data quality issues. The long development cycles and heavy debugging effort further hindered our ability to iterate and maintain the system.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="rigid-sql-and-tight-coupling">Rigid SQL and Tight Coupling<a href="https://hudi.apache.org/cn/blog/2025/06/30/uber-hudi#rigid-sql-and-tight-coupling" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Digging deeper, we identified multiple underlying causes. The pipeline relied heavily on SQL for all transformations. But expressing the evolving business rules for different Uber products in SQL was limiting. The logic had grown too complex to be managed effectively, and granular transformations led to a proliferation of intermediate stages. This made unit testing and debugging difficult, and the absence of structured logging made observability poor.</p>
<img src="https://hudi.apache.org/assets/images/blog/figure3_uber.png" alt="redshift" width="800" align="middle">
<p>Additionally, data and logic were tightly coupled. The system often required joining tables at very fine granularities, introducing redundancy and making logic harder to reason about. Complex joins, table scans, and late-arriving data amplified processing costs. It was also difficult to trace how a specific row was transformed through the DAG, making explainability a real challenge.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-we-solved-it">How We Solved It?<a href="https://hudi.apache.org/cn/blog/2025/06/30/uber-hudi#how-we-solved-it" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<ol>
<li><strong>Solving Read Amplification</strong></li>
</ol>
<p>The first step in addressing inefficiencies was eliminating the brute-force strategy of scanning and processing a 180-day window of data on every pipeline run. With the help of Apache Hudi’s <a href="https://hudi.apache.org/docs/table_types#incremental-queries" target="_blank" rel="noopener noreferrer"><em>incremental</em> <em>read</em></a> capabilities, we restructured the ingestion layer to read only the records that had mutated since the last checkpoint.</p>
<img src="https://hudi.apache.org/assets/images/blog/fig4_uber.png" alt="redshift" width="800" align="middle">
<p>We introduced an intermediate Hudi table that consolidated all related records for a trip or order into a single row, using complex data types such as structs, lists, and maps. This model allowed us to capture the complete state of a trip - including all updates, tips, disputes, and refunds in one place, without scattering information across multiple joins.</p>
<p>By using this intermediate table as the foundation, all downstream logic could operate on change-driven inputs. The result was a pipeline that avoided unnecessary scans, improved correctness by processing all real changes (not just those in a time window), and reduced overall I/O dramatically.</p>
<ol start="2">
<li><strong>Eliminating Self Joins with Custom Payloads</strong></li>
</ol>
<p>Self joins - especially for reconciling updates to the same trip were one of the costliest operations in our original pipeline.</p>
<img src="https://hudi.apache.org/assets/images/blog/fig5_uber.png" alt="redshift" width="800" align="middle">
<p>To solve this, we implemented a custom Hudi payload class that allows us to control how updates are applied during the merge phase. This class overrides methods such as <code>combineAndGetUpdateValue</code> and <code>getInsertValue</code>, and executes the merge logic as part of the write path, eliminating the need for a full table scan or shuffle.</p>
<p>This approach helped us efficiently handle updates to complex, nested records in the intermediate Hudi table, and dramatically reduced the cost associated with self joins.</p>
<ol start="3">
<li><strong>Simplifying Processing with a Rule-Based Framework</strong></li>
</ol>
<p>To move away from the rigidity of SQL, we designed a rule engine framework based on functional programming principles.</p>
<p>Instead of expressing business logic as large, monolithic SQL queries, we cast each input row (from the intermediate table) into a strongly typed object (e.g., a Trip object). These objects were then passed through a series of declarative rules - each consisting of a condition and an action.</p>
<img src="https://hudi.apache.org/assets/images/blog/fig6_uber.png" alt="redshift" width="800" align="middle">
<p>This framework was implemented as a custom <a href="https://hudi.apache.org/docs/hoodie_streaming_ingestion#transformers" target="_blank" rel="noopener noreferrer"><em>transformer</em></a> plugged into <a href="https://hudi.apache.org/docs/hoodie_streaming_ingestion" target="_blank" rel="noopener noreferrer">HudiStreamer</a>. The transformer intercepts the ingested data, applies the rule engine logic, and emits the transformed object to the final Hudi output table. We also built in capabilities for:</p>
<ul>
<li>Logging and observability (for metrics and debugging)</li>
<li>Unreachable state detection (flagging invalid rows)</li>
<li>Unit testing support for each rule independently</li>
</ul>
<p>This architecture replaced the huge DAG with modular, testable, and composable rule definitions, dramatically improving developer productivity and data pipeline clarity.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="final-architecture">Final Architecture<a href="https://hudi.apache.org/cn/blog/2025/06/30/uber-hudi#final-architecture" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<img src="https://hudi.apache.org/assets/images/blog/fig7_uber.png" alt="redshift" width="800" align="middle">
<p>The redesigned system follows a clean, composable structure:</p>
<ul>
<li>Incremental ingestion from the data lake is done using HudiStreamer, which writes to an intermediate Hudi table.</li>
<li>The intermediate table consolidates all records for a trip using complex types, serving as the central input for downstream processing.</li>
<li>A custom Transformer intercepts the records, casts them into typed domain objects, and passes them through a rule engine.</li>
<li>The rule engine applies business logic declaratively and emits fully processed objects.</li>
<li>The output is written to a final Hudi table that supports efficient, incremental consumption.</li>
</ul>
<p>This design eliminates redundant scans, reduces shuffle overhead, enables full test coverage, and offers detailed observability across all transformation stages.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-wins-with-hudi">The Wins with Hudi<a href="https://hudi.apache.org/cn/blog/2025/06/30/uber-hudi#the-wins-with-hudi" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>The improvements were substantial and measurable:</p>
<ul>
<li>Runtime reduced from ~20 hours to ~4 hours (~75% improvement)</li>
<li>Test coverage increased to 95% for transformation logic</li>
<li>Single run cost reduced by 60%</li>
<li>Improved data completeness, processing all updates—not just those in a statistical window</li>
<li>Reusable and modular logic, reducing DAG complexity</li>
<li>Higher developer productivity, with isolated unit testing and simplified debugging</li>
<li>Improved self-join performance through custom payloads</li>
<li>A generic rule engine design, portable across Spark and Flink</li>
</ul>
<p>Apache Hudi has been central to Nexus’ success, providing the core data lake storage layer for scalable ingestion, updates, and metadata management. It enables fast, incremental updates at massive scale while maintaining transactional guarantees on top of Amazon S3.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://hudi.apache.org/cn/blog/2025/06/30/uber-hudi#conclusion" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>By redesigning the system around Apache Hudi and adopting functional, rule-based processing, Uber was able to transform a brittle, long-running pipeline into a maintainable and efficient architecture. The changes allowed them to scale their data workflows to meet the needs of complex, multi-product use cases without compromising on performance, observability, or data quality.</p>
<p>This work highlights the power of pairing the right storage format with a principled architectural approach. Apache Hudi was instrumental in helping achieve these outcomes and continues to play a key role in Uber’s evolving data platform.</p>
<p>This blog is based on Uber’s presentation at the Apache Hudi Community Sync. If you are interested in watching the recorded version of the video, you can find it <a href="https://www.youtube.com/watch?v=VpdimpH_nsI" target="_blank" rel="noopener noreferrer">here</a>.</p>
<hr>]]></content>
        <author>
            <name>Ankit Shrivastava in collaboration with Dipankar</name>
        </author>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="Uber" term="Uber"/>
        <category label="Community" term="Community"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Apache Hudi does XYZ (1/10): File pruning with multi-modal index]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/06/16/Apache-Hudi-does-XYZ-110</id>
        <link href="https://hudi.apache.org/cn/blog/2025/06/16/Apache-Hudi-does-XYZ-110"/>
        <updated>2025-06-16T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://blog.datumagic.ai/p/apache-hudi-does-xyz-110">here</a></span>]]></content>
        <author>
            <name>Shiyan Xu</name>
        </author>
        <category label="hudi" term="hudi"/>
        <category label="spark" term="spark"/>
        <category label="blog" term="blog"/>
        <category label="course" term="course"/>
        <category label="tutorial" term="tutorial"/>
        <category label="datumagic" term="datumagic"/>
        <category label="data lake" term="data lake"/>
        <category label="lakehouse" term="lakehouse"/>
        <category label="apache hudi" term="apache hudi"/>
        <category label="apache spark" term="apache spark"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Apache Hudi Workflows: Automation for Clustering, Resizing & Concurrency]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/06/13/Optimizing-Apache-Hudi-Workflows-Automation-for-Clustering-Resizing-Concurrency</id>
        <link href="https://hudi.apache.org/cn/blog/2025/06/13/Optimizing-Apache-Hudi-Workflows-Automation-for-Clustering-Resizing-Concurrency"/>
        <updated>2025-06-13T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://blogs.halodoc.io/optimizing-apache-hudi-workflows-automation-for-clustering-resizing-concurrency/">here</a></span>]]></content>
        <author>
            <name>Halodoc, Apache Hudi</name>
        </author>
        <category label="hudi" term="hudi"/>
        <category label="blog" term="blog"/>
        <category label="halodoc" term="halodoc"/>
        <category label="data lake" term="data lake"/>
        <category label="lakehouse" term="lakehouse"/>
        <category label="apache hudi" term="apache hudi"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Apache Hudi’s New Log-Structured Merge (LSM) Timeline]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline</id>
        <link href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline"/>
        <updated>2025-05-29T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Apache Hudi 1.0 introduces a new LSM Timeline to scale metadata management for long-lived tables. By restructuring timeline storage into a compacted, versioned tree layout, Hudi enables faster metadata access, snapshot isolation, and support for Non-Blocking Concurrency Control.]]></summary>
        <content type="html"><![CDATA[<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>TL;DR</div><div class="admonitionContent_BuS1"><p>Apache Hudi 1.0 introduces a new LSM Timeline to scale metadata management for long-lived tables. By restructuring timeline storage into a compacted, versioned tree layout, Hudi enables faster metadata access, snapshot isolation, and support for Non-Blocking Concurrency Control.</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="apache-hudis-timeline">Apache Hudi’s Timeline<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#apache-hudis-timeline" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>At the heart of Apache Hudi’s architecture is the <a href="https://hudi.apache.org/docs/timeline" target="_blank" rel="noopener noreferrer">Timeline</a> - a log-structured system that acts as the single source of truth for the table’s state at any point in time. The timeline records every change and operation performed on a Hudi table, encompassing writes, schema evolutions, compactions, cleanings, and clustering operations. This meticulous record-keeping empowers Hudi to deliver <a href="https://www.onehouse.ai/blog/acid-transactions-in-an-open-data-lakehouse" target="_blank" rel="noopener noreferrer">ACID guarantees</a>, robust <a href="https://hudi.apache.org/blog/2025/01/28/concurrency-control" target="_blank" rel="noopener noreferrer">concurrency control</a>, and advanced capabilities such as incremental processing, rollback/recovery, and time travel.</p>
<p>In essence, the timeline functions like a <a href="https://en.wikipedia.org/wiki/Write-ahead_logging" target="_blank" rel="noopener noreferrer">Write-Ahead Log (WAL)</a>, maintaining a sequence of immutable actions. Each action is recorded as a unique <em>instant</em> - a unit of work identified by its action type (e.g., commit, clean, compaction), a timestamp that marks when the action was initiated, and its lifecycle state. In Hudi, an <em>instant</em> refers to this combination of action, timestamp, and state (REQUESTED, INFLIGHT, or COMPLETED), and serves as the atomic unit of change on the timeline. These timeline entries are the backbone of Hudi’s transactional integrity, ensuring that every table change is atomically recorded and timeline-consistent. Every operation progresses through a lifecycle of <em>states</em>:</p>
<ul>
<li>REQUESTED: The action is planned and registered but not yet started.</li>
<li>INFLIGHT: The action is actively being performed, modifying table state.</li>
<li>COMPLETED: The action has successfully executed, and all data/metadata updates are finalized.</li>
</ul>
<p>These <em>instants</em> serve as both log entries and transaction markers, defining exactly what data is valid and visible at any given time. Whether you're issuing a snapshot query for the latest view, running an incremental query to fetch changes since the last checkpoint, or rolling back to a prior state, the timeline ensures that each action’s impact is precisely tracked. Every <em>action</em>, such as commit, clean, compaction, or rollback is explicitly recorded, allowing compute engines and tools to reason precisely about the table’s state transitions and history. This strict sequencing and lifecycle management also underpin Hudi’s ability to provide serializable isolation (the “I” in ACID) guarantees, ensuring that readers only observe committed data and consistent snapshots.</p>
<p>To optimize both performance and long-term storage scalability, Apache Hudi splits the timeline into two distinct components that work together to provide fast access to recent actions while ensuring historical records are retained efficiently. Let’s understand these in detail.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="active-timeline">Active Timeline<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#active-timeline" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>The <a href="https://hudi.apache.org/docs/timeline#active-timeline" target="_blank" rel="noopener noreferrer">Active timeline</a> is the front line of Hudi’s transaction log. It contains the most recent and in-progress actions that are critical for building a consistent and up-to-date view of the table. Every time a new operation, such as a data write, compaction, clean, or rollback is initiated, it is immediately recorded here as a new instant file under the <code>.hoodie/</code> directory. Each of these files holds metadata about the action’s lifecycle, moving through the standard states of REQUESTED → INFLIGHT → COMPLETED.</p>
<p>The active timeline is consulted constantly - whether you are issuing a query, running compaction, or planning a new write operation. Compute engines read from the active timeline to determine what data files are valid and visible, making it the source of truth for the table’s latest state. To maintain performance, Hudi enforces a retention policy on the active timeline, i.e. it deliberately keeps only a window of the most recent actions, ensuring the timeline remains lightweight and quick to scan.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="archived-timeline">Archived Timeline<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#archived-timeline" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Tables naturally accumulate many more actions over time, especially in high-ingestion or update environments. As the number of instants grows, the active timeline can become bloated if left unchecked, introducing latency and performance penalties during reads and writes.</p>
<p>To solve this, Hudi implements an archival process. Once the number of active instants crosses a configured threshold, older actions are offloaded from the active timeline into the Archived Timeline stored in the <code>.hoodie/archive/</code> directory. This design ensures that while the active timeline remains lean and fast for day-to-day operations, the complete transactional history of the table is still preserved for auditing, recovery, and time travel purposes.</p>
<p>Although the archived timeline is optimized for long-term retention, accessing deep history can incur higher latency and overhead, especially in workloads with a large number of archived instants. This limitation is precisely what set the stage for the LSM Timeline innovation introduced in Hudi 1.0.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="problem-statement---why-move-to-an-lsm-timeline">Problem Statement - Why move to an LSM Timeline?<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#problem-statement---why-move-to-an-lsm-timeline" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Apache Hudi’s original timeline design served well for many workloads. By maintaining a lightweight active timeline for fast operations and offloading historical instants to the archive, Hudi struck a balance between performance and durability. However, there were some aspects to think about with the previous timeline design.</p>
<ul>
<li>
<p><strong>Linear Growth</strong>: The timeline grows linearly with each table action, whether it’s a commit, compaction, clustering, or rollback. Although Hudi’s archival process offloads older instants to keep the active timeline lean, the total number of instants (active + archived) continues to grow unbounded in long-lived tables. Over time, the accumulation of these instants can inflate metadata size, leading to slower scans and degraded query planning performance, especially for use cases like time travel and incremental queries.</p>
</li>
<li>
<p><strong>Latency &amp; Cost</strong>: Accessing the archived timeline, which is often required for time-travel, or recovery operations introduces high read latencies. This is because the archival format was optimized for durability and storage efficiency (many small Avro files), not for fast access. As the number of archived instants balloons, reading deep history involves scanning and deserializing large volumes of metadata, increasing both latency and compute cost. This can noticeably slow down operations like incremental syncs and historical audits.</p>
</li>
<li>
<p><strong>Cloud Storage Limitations</strong>: In cloud object stores like S3 or GCS, appending to existing files is not supported (or is highly inefficient). As a result, every new archival batch creates new small files, leading to a small-file problem. Over time, these fragmented archives accumulate, creating operational challenges in storage management and performance bottlenecks during metadata access, especially when files must be scanned individually across large object stores.</p>
</li>
<li>
<p><strong>Emerging Use Cases</strong>: Apache Hudi has evolved to support next-generation features such as non-blocking concurrency control (NBCC), infinite time travel, and fine-grained transaction metadata. These capabilities place heavier demands on the timeline architecture, requiring high-throughput writes and faster lookups across both recent and historical data.</p>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introducing-the-lsm-timeline">Introducing the LSM Timeline<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#introducing-the-lsm-timeline" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>To overcome the scaling challenges of the original timeline architecture, Apache Hudi 1.0 introduced the <a href="https://hudi.apache.org/docs/timeline#timeline-components" target="_blank" rel="noopener noreferrer">LSM (Log-Structured Merge)</a> Timeline - a fundamentally new way to store and manage timeline metadata. This redesign brings together principles of <a href="https://en.wikipedia.org/wiki/Log-structured_merge-tree" target="_blank" rel="noopener noreferrer">log-structured storage</a>, tiered compaction, and snapshot versioning to deliver a highly scalable, cloud-native solution for tracking table history.</p>
<p>Hudi introduces a critical change in how time is represented on the timeline. Previously, Hudi treated time as instantaneous, i.e. each action appeared to take effect at a single instant. While effective for basic operations, this model proved limiting when implementing certain advanced features like <a href="https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control/" target="_blank" rel="noopener noreferrer">Non-Blocking Concurrency Control (NBCC)</a>, which require reasoning about actions as intervals of time to detect overlaps and resolve conflicts.</p>
<img src="https://hudi.apache.org/assets/images/blog/lsm_1.png" alt="index" width="800" align="middle">
<p>To address this, every action on the Hudi timeline now records both a <em>requested time</em> (when the action is initiated) and a <em>completion time</em> (when it finishes). This allows Hudi to track not just when an action was scheduled, but also how it interacts with other concurrent actions over time. To ensure global consistency across distributed processes, Hudi formalized the use of <a href="https://hudi.apache.org/docs/timeline#truetime-generation" target="_blank" rel="noopener noreferrer">TrueTime semantics</a>, guaranteeing that all instant times are monotonically increasing and globally ordered. This is a foundational requirement for precise conflict detection and robust transaction isolation.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-it-works--design">How It Works / Design<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#how-it-works--design" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<img src="https://hudi.apache.org/assets/images/blog/lsm_2.png" alt="index" width="800" align="middle">
<p>At its core, the LSM timeline replaces the flat archival model with a layered tree structure, allowing Hudi to manage metadata for millions of historical instants efficiently, without compromising on read performance or consistency. Here’s how it’s designed:</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="file-organization">File Organization<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#file-organization" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<ul>
<li>Metadata files are organized into layers (L0, L1, L2, …) following a Log-Structured Merge (LSM) tree layout.</li>
<li>Each file is a Parquet file that stores a batch of timeline instants. Their metadata entries are sorted chronologically by timestamp.</li>
<li>The files follow a precise naming convention: <code>${min_instant}_${max_instant}_${level}.parquet</code> where <code>min_instant</code> and <code>max_instant</code> represent the range of instants in the file and <code>level</code> denotes the layer (e.g., L0, L1, L2).</li>
<li>Files in the same layer may have overlapping time ranges, but the system tracks them via manifest files (more on that below).</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="compaction-strategy">Compaction Strategy<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#compaction-strategy" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<ul>
<li>The LSM timeline uses a universal compaction strategy, similar to designs seen in modern databases.</li>
<li>Whenever N files (default: 10) accumulate in a given layer (e.g., L0), they are merged and flushed into the next layer (e.g., L1).</li>
<li>Compaction is governed by a size-based policy (default max file size ~1 GB), ensuring that write amplification is controlled and files stay within optimal size limits.</li>
<li>There’s no hard limit on the number of layers. The LSM tree naturally scales to handle massive tables with deep histories.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="version--manifest-management-snapshot-isolation">Version &amp; Manifest Management: Snapshot Isolation<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#version--manifest-management-snapshot-isolation" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<ul>
<li>The LSM timeline introduces manifest files that record the current valid set of Parquet files representing the latest snapshot of the timeline.</li>
<li>Version files are generated alongside manifest files to maintain snapshot isolation, ensuring that readers and writers do not conflict.</li>
<li>This system supports multiple valid snapshot versions simultaneously (default: 3), enabling:<!-- -->
<ul>
<li>Consistent reads even during compaction.</li>
<li>Seamless evolution of the timeline without impacting query correctness.</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="reader-workflow">Reader Workflow<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#reader-workflow" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<ul>
<li>When a query is made on the timeline:<!-- -->
<ul>
<li>The engine first fetches the latest version file.</li>
<li>It reads the corresponding manifest file to get the list of valid data files.</li>
<li>It scans only the relevant Parquet files, often using timestamp-based filtering to skip irrelevant data early.</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="cleaning-strategy">Cleaning Strategy<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#cleaning-strategy" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<ul>
<li>The LSM timeline performs cleaning only after successful compaction, ensuring that no active snapshot is disrupted.</li>
<li>By default, Hudi retains 3 valid snapshot versions to support concurrent readers/writers.</li>
<li>Files are retained for at least 3 archival trigger intervals, providing a grace period before old data is purged.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="what-it-brings-to-the-table-benefits">What It Brings to the Table (Benefits)<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#what-it-brings-to-the-table-benefits" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>The LSM timeline unlocks significant advancements in how Apache Hudi handles metadata, providing both performance improvements and new capabilities.</p>
<ul>
<li>
<p><strong>Scalability:</strong> The LSM timeline architecture allows Hudi to manage virtually infinite timeline history while keeping both read and write performance predictable. Whether it's thousands or millions of instants, the layered compaction model ensures stable metadata performance over time, supporting efficient query and metadata access even as tables grow in size and history length.</p>
</li>
<li>
<p><strong>Efficient Reads:</strong> Readers benefit from manifest-guided lookups, allowing them to scan only the specific set of files relevant to their query. By using Parquet’s columnar format and timestamp-based filtering, Hudi dramatically reduces the overhead of accessing deep historical metadata.</p>
</li>
<li>
<p><strong>Non-Blocking Concurrency Control (NBCC):</strong> One of the most powerful capabilities enabled by the LSM timeline is Non-Blocking Concurrency Control, allowing multiple writers to operate concurrently on the same table (and even the same file group) without the need for explicit locks - except during final commit metadata updates.</p>
</li>
<li>
<p><strong>Cloud-Native Optimization</strong>: By compacting small files into large Parquet files, the LSM timeline avoids the small-file problem common in cloud storage systems like Amazon S3 or Google Cloud Storage. This improves both query performance and storage cost efficiency.</p>
</li>
<li>
<p><strong>Snapshot Isolation &amp; Consistency</strong>: The manifest + version file mechanism ensures that concurrent operations remain isolated and consistent, even as background compaction and cleaning occur. This provides strong transactional guarantees without sacrificing performance.</p>
</li>
<li>
<p><strong>Maintenance-Free Scalability</strong>: The universal compaction and smart cleaning strategies keep the timeline healthy over time, requiring minimal manual tuning, while ensuring that old data is cleaned up safely only after valid snapshots are no longer in use.</p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="performance">Performance<a href="https://hudi.apache.org/cn/blog/2025/05/29/lsm-timeline#performance" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Micro-benchmarks show that the LSM Timeline scales efficiently even as the number of timeline actions grows by orders of magnitude. Reading just the instant times for <code>10,000</code> actions takes around <code>32ms</code>, while fetching full metadata takes <code>150ms</code>. At larger scales, such as <code>10 million</code> actions, metadata reads completes in about <code>162 seconds</code>.</p>
<p>These results demonstrate that Hudi's LSM timeline can handle decades of high-frequency commits (e.g., one every 30 seconds for 10+ years) while keeping metadata access performant.</p>
<p>The LSM timeline represents a natural progression in Apache Hudi’s timeline architecture, designed to address the growing demands of large-scale and long-lived tables. Hudi’s timeline has been foundational for transactional integrity, time travel, and incremental processing capabilities. The new LSM-based design enhances scalability and operational efficiency by introducing a layered, compacted structure with manifest-driven snapshot isolation. This improvement allows Hudi to manage extensive metadata histories more efficiently, maintain predictable performance, and better support advanced use cases such as non-blocking concurrency control.</p>
<hr>]]></content>
        <author>
            <name>Dipankar Mazumdar</name>
        </author>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="LSM Tree" term="LSM Tree"/>
        <category label="Performance" term="Performance"/>
        <category label="Non-Blocking Concurrency Control" term="Non-Blocking Concurrency Control"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Doris + Hudi Turned the Impossible Into the Everyday]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/04/14/doris-hudi-making-impossible-possible</id>
        <link href="https://hudi.apache.org/cn/blog/2025/04/14/doris-hudi-making-impossible-possible"/>
        <updated>2025-04-14T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://dzone.com/articles/doris-hudi-making-impossible-possible">here</a></span>]]></content>
        <author>
            <name>Zen Hua</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="Apache Doris" term="Apache Doris"/>
        <category label="use-case" term="use-case"/>
        <category label="federated querying" term="federated querying"/>
        <category label="dzone" term="dzone"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why Walmart Chose Apache Hudi for Their Lakehouse]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/04/09/why-walmart-chose-apache-hudi-for-their-lakehouse</id>
        <link href="https://hudi.apache.org/cn/blog/2025/04/09/why-walmart-chose-apache-hudi-for-their-lakehouse"/>
        <updated>2025-04-09T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://blog.det.life/why-walmart-chose-apache-hudi-for-their-lakehouse-c0a3574db0ba">here</a></span>]]></content>
        <author>
            <name>Vu Trinh</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="use-case" term="use-case"/>
        <category label="det" term="det"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[ From Swamp to Stream: How Apache Hudi Transforms the Modern Data Lake]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/04/06/from-swamp-to-stream-how-apache-hudi-transforms-the-modern-data-lake</id>
        <link href="https://hudi.apache.org/cn/blog/2025/04/06/from-swamp-to-stream-how-apache-hudi-transforms-the-modern-data-lake"/>
        <updated>2025-04-06T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://medium.com/aimonks/from-swamp-to-stream-how-apache-hudi-transforms-the-modern-data-lake-8a938f517ea1">here</a></span>]]></content>
        <author>
            <name>Everton Gomede</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="real-time datalake" term="real-time datalake"/>
        <category label="incremental processing" term="incremental processing"/>
        <category label="upserts" term="upserts"/>
        <category label="medium" term="medium"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integrating Apache Doris and Hudi for Data Querying and Migration]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/04/03/integrate-apache-doris-hudi-data-querying-migration</id>
        <link href="https://hudi.apache.org/cn/blog/2025/04/03/integrate-apache-doris-hudi-data-querying-migration"/>
        <updated>2025-04-03T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://dzone.com/articles/integrate-apache-doris-hudi-data-querying-migration">here</a></span>]]></content>
        <author>
            <name>li yy</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="Apache Doris" term="Apache Doris"/>
        <category label="real-time query" term="real-time query"/>
        <category label="how-to" term="how-to"/>
        <category label="dzone" term="dzone"/>
    </entry>
</feed>