<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://hudi.apache.org/cn/blog</id>
    <title>Apache Hudi: User-Facing Analytics</title>
    <updated>2025-03-03T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://hudi.apache.org/cn/blog"/>
    <subtitle>Apache Hudi Blog</subtitle>
    <icon>https://hudi.apache.org/cn/assets/images/favicon.ico</icon>
    <entry>
        <title type="html"><![CDATA[Record Mergers in Apache Hudi]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi</id>
        <link href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi"/>
        <updated>2025-03-03T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The Challenge of Unordered Streams of Events]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-challenge-of-unordered-streams-of-events">The Challenge of Unordered Streams of Events<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#the-challenge-of-unordered-streams-of-events" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>One of the primary challenges associated with streaming workloads is the unordered nature of incoming events. In a typical streaming scenario, events can arrive out of sequence due to network latency, processing delays, or other factors. With the increasing volume and velocity of data being ingested from various sources—especially in mobile applications and IoT platforms—data processing frameworks must be equipped to handle mutations (i.e., changes to records) and out-of-order events.
Traditional data storage systems and file formats, such as those optimized for batch processing, often struggle to manage these scenarios effectively. Hudi steps in with features specifically designed to handle such challenges.
When events or changes to a record arrive at different times, they may not be in the same order in which they were originally generated. For example, in a smart city traffic monitoring system, sensors may report vehicle speeds at various intersections in real-time. However, due to network issues or delays, some sensor data might arrive later than others, possibly out of order. To handle this, the system needs to merge the new incoming data with existing records efficiently. Just like how Hudi’s merge modes control the merging of records with the same key in a storage system, ensuring consistency and accuracy, it ensures that the final traffic data reflects the correct event times, even when some data arrives with a delay. These merge modes help maintain a consistent, deterministic result under heavy load, making sure that late data updates the right records without causing inconsistencies.
This can lead to several issues:</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="data-integrity">Data Integrity<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#data-integrity" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>When events are processed out of order, it can result in incorrect or inconsistent data states. For example, if an event representing a transaction is processed before the event that indicates the account balance, the resulting data may not accurately reflect the true state of the system.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="complexity-in-processing">Complexity in Processing<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#complexity-in-processing" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>Handling unordered events often requires additional logic to ensure that data is processed in the correct sequence. This can complicate the data pipeline and increase the likelihood of errors.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-record-mergers">What are Record Mergers<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#what-are-record-mergers" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>With the new api introduced with version 1.0.0, Hudi supports three primary merge modes, each suited to different stages of data processing: writing, compaction, and querying.
4 places/points of data processing [Subheader]</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="1-merging-input-data-before-writing--combining-change-records-during-writes">1. Merging input data before writing : Combining Change Records During Writes<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#1-merging-input-data-before-writing--combining-change-records-during-writes" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>When new data arrives for an existing record, Hudi performs deduplication on the input dataset. This process involves combining multiple change records for the same record key before the write phase. This is an optimization that also helps reduce the number of records written to the log files (in case of MOR). By merging changes upfront, Hudi reduces unnecessary records, improving the efficiency of both query and write operations.
This step is crucial for handling stream data in real-time, where changes may arrive rapidly, and ensuring that only the final version of the record is written into the system. Normally these out of order events come together commonly in the same batch,  With processing engines like spark, which deals with micro-batches, merging the input changes helps in reduces the number of records which needs to be written.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="2-merging-final-change-record-in-cow-copy-on-write-tables-applying-changes-to-existing-records">2. Merging Final Change Record in CoW (Copy-on-Write) Tables: Applying Changes to Existing Records<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#2-merging-final-change-record-in-cow-copy-on-write-tables-applying-changes-to-existing-records" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>In Copy-on-Write (CoW) tables, changes are applied by creating new file versions for the records. When an update, partial update, or delete operation occurs, Hudi will merge this final change with the existing record in the storage. The merge mode controls how these updates are applied, ensuring that only the most recent changes are reflected and the table’s data remains consistent.
This is especially important in CoW tables, as they preserve immutability of historical data by writing new versions of the records instead of overwriting the existing data. The merge mode ensures that the new version of the record is consistent with all previous changes.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="3-compaction-merge-in-mor-merge-on-read-tables--merging-log-files-with-base-files">3. Compaction Merge in MoR (Merge-on-Read) Tables : Merging Log Files with Base Files<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#3-compaction-merge-in-mor-merge-on-read-tables--merging-log-files-with-base-files" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>Hudi uses a concept of log files (delta logs) and base files (original data). As changes to records accumulate over time, Hudi’s compaction service merges the change records stored in the log files with the base files to keep the data consistent and query-optimized. The merge mode defines how these log records are merged with base files during the compaction process.
Compaction helps maintain storage efficiency and ensures that queries run faster by reducing the number of small log files that might need to be read.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="4-query-time-merge-merging-log-files-with-base-files-in-mor-merge-on-read-tables">4. Query-Time Merge: Merging Log Files with Base Files in MoR (Merge-on-Read) Tables<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#4-query-time-merge-merging-log-files-with-base-files-in-mor-merge-on-read-tables" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>In Merge-on-Read (MoR) tables, the data is stored in both log files and base files. When a query is executed, Hudi merges the change records in the log files with the base files based on the merge mode. The merge operation occurs at query time to provide the final, consistent view of the data.
By merging records at query time, Hudi ensures that queries reflect the most recent changes while maintaining query performance.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="implementation">Implementation<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#implementation" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>In common scenarios, the input data contains a field that can be used to identify the latest record. Typically, tables have fields like updated_at or other ordering columns. If no such column is present in the input, we are limited to relying on the incoming order.</p>
<p>After the release of Hudi 1.0.0, a new configuration, <a href="https://hudi.apache.org/docs/configurations/#hoodierecordmergemode" target="_blank" rel="noopener noreferrer">hoodie.record.merge.mode</a> was introduced to define the merge modes responsible for handling record updates. These merge modes dictate how records with the same key are processed at different stages of the pipeline, from data ingestion to query results.
It can have the following three values:</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="1-commit_time_ordering">1. COMMIT_TIME_ORDERING<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#1-commit_time_ordering" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>This merge mode is used when no field is available in the input data to explicitly determine which record is the latest. The system will rely on the order of ingestion (commit time) to determine the order of records. Hudi expects records to arrive in strict order of their commits. So, the most recent record (in terms of ingestion time) is assumed to be the latest version of the record. This mode is typically used when there is no dedicated column like updated_at, timestamp, or versioning field that can indicate the order of the records.
The merging logic here simply picks the latest write based on the ingestion order (commit time). In a way, it's equivalent to overwriting semantics where only the most recent record is considered.
Example -</p>
<div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">SET</span><span class="token plain"> hoodie</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">sql</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">insert</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">into</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">operation</span><span class="token operator">=</span><span class="token plain">upsert</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">TABLE</span><span class="token plain"> hudi_table </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ts </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">BIGINT</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    uuid STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    rider STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    driver STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    fare </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">DOUBLE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    city STRING</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">USING</span><span class="token plain"> HUDI TBLPROPERTIES </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">primaryKey </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'uuid'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> hoodie</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">record</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">merge</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">mode</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">'COMMIT_TIME_ORDERING'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INSERT</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INTO</span><span class="token plain"> hudi_table</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">VALUES</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">3</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'334e26e9-8355-45cc-97c6-c31daf0df330'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'rider-A'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'driver-K'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">19.10</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'san_francisco'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'334e26e9-8355-45cc-97c6-c31daf0df330'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'rider-C'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'driver-M'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">27.70</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'san_francisco'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">select</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> hudi_table</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">-- Result - 20250106162911278	20250106162911278_0_0	334e26e9-8355-45cc-97c6-c31daf0df330		08218473-f72a-480d-90e6-c6764f062e5c-0_0-43-47_20250106162911278.parquet	1695091554788	334e26e9-8355-45cc-97c6-c31daf0df330	rider-C	driver-M	27.7	san_francisco</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INSERT</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INTO</span><span class="token plain"> hudi_table</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">VALUES</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'334e26e9-8355-45cc-97c6-c31daf0df330'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'rider-D'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'driver-K'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">19.10</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'san_francisco'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">select</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> hudi_table</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">-- Result - 20250106163449812	20250106163449812_0_0	334e26e9-8355-45cc-97c6-c31daf0df330		08218473-f72a-480d-90e6-c6764f062e5c-0_0-71-68_20250106163449812.parquet	1	334e26e9-8355-45cc-97c6-c31daf0df330	rider-D	driver-K	19.1	san_francisco</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>In the example above, we created the table using the COMMIT_TIME_ORDERING merge mode. When using this mode, there is no need to specify a precombine or ordering field.
During the first insert, two records with the same record key are provided. The system will deduplicate them and keep the record that is processed later.
In the second insert, a new record with the same record key is inserted. As expected, the table is updated with the new record because it is committed later, regardless of the values in any of the fields.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="2-event_time_ordering-default">2. EVENT_TIME_ORDERING (DEFAULT)<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#2-event_time_ordering-default" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>This merge mode is used when you do have a field in the input data that can be used to determine the order of events (such as a timestamp field like updated_at or a version number). If your records contain a field that can be used to track when the record was last updated (e.g., updated_at, last_modified, or a sequence number), Hudi will use this field to determine which record is the latest.
In this case, Hudi does not rely on the ingestion order but instead uses the value of the ordering field (updated_at, for example) to decide the correct record.
This approach is ideal when you have temporal or event-driven data, and you want to maintain the "latest" record according to an event timestamp.
Example -</p>
<div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">DROP</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">TABLE</span><span class="token plain"> hudi_table</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">SET</span><span class="token plain"> hoodie</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">sql</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">insert</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">into</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">operation</span><span class="token operator">=</span><span class="token plain">upsert</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">TABLE</span><span class="token plain"> hudi_table </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ts </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">BIGINT</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    uuid STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    rider STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    driver STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    fare </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">DOUBLE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    city STRING</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">USING</span><span class="token plain"> HUDI TBLPROPERTIES </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">primaryKey </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'uuid'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">preCombineField </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'ts'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> hoodie</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">record</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">merge</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">mode</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">'EVENT_TIME_ORDERING'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INSERT</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INTO</span><span class="token plain"> hudi_table</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">VALUES</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">3</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'334e26e9-8355-45cc-97c6-c31daf0df330'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'rider-A'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'driver-K'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">19.10</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'san_francisco'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'334e26e9-8355-45cc-97c6-c31daf0df330'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'rider-C'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'driver-M'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">27.70</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'san_francisco'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">select</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> hudi_table</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">-- Result - 20250106165902806	20250106165902806_0_0	334e26e9-8355-45cc-97c6-c31daf0df330		568ce7bc-9b71-4e15-b557-cbaeb5b4d2ea-0_0-56-57_20250106165902806.parquet	3	334e26e9-8355-45cc-97c6-c31daf0df330	rider-A	driver-K	19.1	san_francisco</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INSERT</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INTO</span><span class="token plain"> hudi_table</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">VALUES</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'334e26e9-8355-45cc-97c6-c31daf0df330'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'rider-D'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'driver-K'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">18.00</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'san_francisco'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">select</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> hudi_table</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">-- Result - 20250106165902806	20250106165902806_0_0	334e26e9-8355-45cc-97c6-c31daf0df330		568ce7bc-9b71-4e15-b557-cbaeb5b4d2ea-0_0-84-78_20250106165918731.parquet	3	334e26e9-8355-45cc-97c6-c31daf0df330	rider-A	driver-K	19.1	san_francisco</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>In the example above, we created the table using the EVENT_TIME_ORDERING merge mode. When using this mode, we need to specify the precombineField. In this case we are specifying ts as the precombineField.
During the first insert, two records with the same record key are provided. The system will deduplicate them and keep the record that is processed later.
In the second insert, a new record with the same record key is inserted. As expected, the table is updated with the new record because it is committed later, regardless of the values in any of the fields.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="3-custom">3. CUSTOM<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#3-custom" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>For more complex use-case sometimes prior discussed merging modes won’t work. We may need to implement a use-case specific merging logic.
The details for the implementation is provided here  - <a href="https://hudi.apache.org/docs/record_merger/#custom" target="_blank" rel="noopener noreferrer">https://hudi.apache.org/docs/record_merger/#custom</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="record-payloads">Record Payloads<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#record-payloads" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Pre 1.0.0, Hudi uses the legacy Record Payload API, Please refer to the <a href="https://hudi.apache.org/docs/record_merger/#record-payloads" target="_blank" rel="noopener noreferrer">Record Payloads</a> section to know about the implementation and some of the existing record payloads.</p>
<p>Along with the existing payloads, Hudi provides flexibility to implement the custom record payload by implementing the <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordPayload.java" target="_blank" rel="noopener noreferrer">HoodieRecordPayload</a> interface</p>
<p>The following example demonstrates the use of Record Payload, which achieves a similar outcome to what EVENT_TIME_ORDERING does. We’ve used the same example as above to illustrate how this functionality works.</p>
<div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">DROP</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">TABLE</span><span class="token plain"> hudi_table</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">SET</span><span class="token plain"> hoodie</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">sql</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">insert</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">into</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">operation</span><span class="token operator">=</span><span class="token plain">upsert</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">TABLE</span><span class="token plain"> hudi_table </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ts </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">BIGINT</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    uuid STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    rider STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    driver STRING</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    fare </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">DOUBLE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    city STRING</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">USING</span><span class="token plain"> HUDI TBLPROPERTIES </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">primaryKey </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'uuid'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain">preCombineField </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'ts'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> hoodie</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">datasource</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">write</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">payload</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">class</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">'org.apache.hudi.common.model.DefaultHoodieRecordPayload'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INSERT</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INTO</span><span class="token plain"> hudi_table</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">VALUES</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">3</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'334e26e9-8355-45cc-97c6-c31daf0df330'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'rider-A'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'driver-K'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">19.10</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'san_francisco'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'334e26e9-8355-45cc-97c6-c31daf0df330'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'rider-C'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'driver-M'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">27.70</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'san_francisco'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">select</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> hudi_table</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">-- Result - 20250203164444124	20250203164444124_0_0	334e26e9-8355-45cc-97c6-c31daf0df330		4549ed8e-0346-4d59-8878-9e047fb6c651-0_0-14-17_20250203164444124.parquet	3	334e26e9-8355-45cc-97c6-c31daf0df330	rider-A	driver-K	19.1	san_francisco</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INSERT</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INTO</span><span class="token plain"> hudi_table</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">VALUES</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'334e26e9-8355-45cc-97c6-c31daf0df330'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'rider-D'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'driver-K'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token number">18.00</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token string" style="color:rgb(255, 121, 198)">'san_francisco'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">select</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> hudi_table</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">-- Result - 20250203164444124	20250203164444124_0_0	334e26e9-8355-45cc-97c6-c31daf0df330		4549ed8e-0346-4d59-8878-9e047fb6c651-0_0-53-51_20250203164537068.parquet	3	334e26e9-8355-45cc-97c6-c31daf0df330	rider-A	driver-K	19.1	san_francisco</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://hudi.apache.org/cn/blog/2025/03/03/record-mergers-in-hudi#conclusion" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>In conclusion, managing late-arriving and out-of-order data is a critical challenge in modern data processing systems, especially when dealing with large-scale, real-time data pipelines. Tools like Hudi provide powerful merge modes that ensure data consistency, accuracy, and efficiency by handling record updates intelligently across different stages of the pipeline. Whether you're working with streaming data, IoT sensors, or social media posts, understanding how to configure and use these merge modes can greatly improve the performance and reliability of your data storage and query processes. By leveraging the right merge strategy, you can ensure that your system remains robust, even under heavy load and with delayed data, ultimately enabling better decision-making and insights from your data.</p>]]></content>
        <author>
            <name>Aditya Goenka</name>
        </author>
        <category label="Data Lake" term="Data Lake"/>
        <category label="Data Lakehouse" term="Data Lakehouse"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="Record Mergers" term="Record Mergers"/>
        <category label="Record payloads" term="Record payloads"/>
        <category label="Late Arriving Data" term="Late Arriving Data"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Curious Engineering Facts ( Trace Agents | Hudi| Daft : 1) : March Release 18 : 25]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/02/25/curious-engineering-facts-trace-agents-hudi-daft-1</id>
        <link href="https://hudi.apache.org/cn/blog/2025/02/25/curious-engineering-facts-trace-agents-hudi-daft-1"/>
        <updated>2025-02-25T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://medium.com/@kkgsanjeewac77/curious-engineering-facts-trace-agents-hudi-daft-1-march-release-18-25-bedc00e05ecd">here</a></span>]]></content>
        <author>
            <name>Gayan Sanjeewa</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="apache hudi" term="apache hudi"/>
        <category label="daft" term="daft"/>
        <category label="trace agents" term="trace agents"/>
        <category label="openai" term="openai"/>
        <category label="llm" term="llm"/>
        <category label="medium" term="medium"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Building a Lakehouse Architecture on AWS with Terraform]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/02/24/building-a-lakehouse-architecture-on-aws-with-terraform</id>
        <link href="https://hudi.apache.org/cn/blog/2025/02/24/building-a-lakehouse-architecture-on-aws-with-terraform"/>
        <updated>2025-02-24T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://medium.com/@juanfelipear97/building-a-lakehouse-architecture-on-aws-with-terraform-139c079ec385">here</a></span>]]></content>
        <author>
            <name>Juanfelipear</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="apache hudi" term="apache hudi"/>
        <category label="aws" term="aws"/>
        <category label="terraform" term="terraform"/>
        <category label="lakehouse" term="lakehouse"/>
        <category label="medium" term="medium"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Curious Engineering Facts (Lakehouse | Apache Hudi | Daft |Positional argument|) : March Release 19 : 25]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/02/23/curious-engineering-facts-lakehouse-apache-hudi-daft-positional-argument</id>
        <link href="https://hudi.apache.org/cn/blog/2025/02/23/curious-engineering-facts-lakehouse-apache-hudi-daft-positional-argument"/>
        <updated>2025-02-23T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://medium.com/@kkgsanjeewac77/curious-engineering-facts-lakehouse-apache-hudi-daft-positional-argument-march-release-d0fee8151736">here</a></span>]]></content>
        <author>
            <name>Gayan Sanjeewa</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="apache hudi" term="apache hudi"/>
        <category label="daft" term="daft"/>
        <category label="streamlit" term="streamlit"/>
        <category label="cow" term="cow"/>
        <category label="medium" term="medium"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[An intro to Hudi with MinIO]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/01/30/an-intro-to-hudi-with-minio</id>
        <link href="https://hudi.apache.org/cn/blog/2025/01/30/an-intro-to-hudi-with-minio"/>
        <updated>2025-01-30T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://dataxplorer.medium.com/an-intro-to-hudi-with-minio-i-75536fe75b4c">here</a></span>]]></content>
        <author>
            <name>Simbu</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="apache hudi" term="apache hudi"/>
        <category label="minio" term="minio"/>
        <category label="medium" term="medium"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Concurrency Control in Open Data Lakehouse]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control</id>
        <link href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control"/>
        <updated>2025-01-28T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Introduction]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#introduction" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Concurrency control is critical in database management systems to ensure consistent and safe access to shared data by multiple users. Relational databases (RDBMS) such as <a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-locking-transaction-model.html" target="_blank" rel="noopener noreferrer">MySQL (InnoDB)</a> and analytical databases (such as data warehouses) have been offering robust concurrency control mechanisms to effectively deal with this. As data grows in scale and complexity, managing concurrent access becomes more challenging, especially in large distributed systems like Data Lakes or <a href="https://hudi.apache.org/blog/2024/07/11/what-is-a-data-lakehouse/" target="_blank" rel="noopener noreferrer">Lakehouses</a>, which are expected to handle different types of workloads in the analytics realm. While data lakes have traditionally struggled with concurrent operations due to the lack of a <a href="https://hudi.apache.org/docs/hudi_stack#storage-engine" target="_blank" rel="noopener noreferrer">storage engine</a> and ACID guarantees, lakehouse architectures with open table formats like Apache Hudi, Apache Iceberg, and Delta Lake take inspiration from some of the widely used concurrency control methods to support high concurrent workloads.</p>
<p>This blog goes into the fundamentals of concurrency control, explores why it is essential for lakehouses, and examines how open table formats such as Apache Hudi enable strong concurrency control mechanisms to uphold the ACID properties and deal with varied workloads.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="concurrency-control-foundations">Concurrency Control Foundations<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#concurrency-control-foundations" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>At the core of concurrency control are the concepts of Isolation and Serializability, which define the expected behavior for concurrent transactions and ensure the <strong>"I"</strong> in ACID properties. Let’s quickly go over these concepts from a general database system perspective.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="isolation-and-serializability">Isolation and Serializability<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#isolation-and-serializability" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>In transactional systems, Isolation ensures that each transaction operates independently of others, as if it were executed in a single-user environment. This means a transaction should be "all by itself," free from interference by other concurrent operations, preventing concurrency anomalies like dirty reads or lost updates. This isolation allows end users (such as developers or analysts) to understand the impact of a transaction without worrying about conflicts from other simultaneous operations.</p>
<p>Serializability takes this idea further by defining the correct execution order for concurrent transactions. It guarantees that the outcome of executing transactions concurrently will be the same as if they had been executed serially, one after the other. In other words, even if transactions are interleaved, their combined effect should appear as though there were no parallel execution at all. Serializability is thus a rigorous correctness criterion that concurrency control models in databases strive to enforce, providing a predictable environment for transactional workloads.</p>
<p>For example, imagine an online concert ticketing system where multiple customers are attempting to purchase tickets for the same concert at the same time. Suppose there are only 5 tickets left, and two customers - Customer A and Customer B try to buy 3 tickets each simultaneously. Without proper concurrency control, these transactions might interfere with each other, leading to scenarios where more tickets are "sold" than available in inventory, resulting in inconsistencies. To maintain serializability, the system must ensure that the outcome of processing these transactions concurrently is the same as if they were processed one at a time (serially), i.e. no more than 5 tickets are sold, ensuring inventory consistency.</p>
<p>Concurrency control methods can be broadly classified into three approaches: Pessimistic Concurrency Control, Optimistic Concurrency Control, and Multi-Version Concurrency Control (MVCC).</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="pessimistic-concurrency-control-2pl">Pessimistic Concurrency Control (2PL)<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#pessimistic-concurrency-control-2pl" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>Pessimistic Concurrency Control assumes that conflicts between transactions can happen often and avoids having ‘problems’ in the first place. The most commonly used method, Strict Two-Phase Locking (2PL), works in this way:</p>
<ul>
<li>Transactions acquire a shared lock before reading data and an exclusive lock before writing.</li>
<li>Locks are held until the transaction commits or aborts but releases immediately after the commit command executes, ensuring serializability.</li>
</ul>
<img src="https://hudi.apache.org/assets/images/blog/concurrency_control/2PL.png" alt="2PL" width="1000" align="middle">
<p>If we take our online concert ticketing system example, where we have 5 tickets left and Customer A and Customer B both attempt to buy 3 tickets simultaneously. With Strict Two-Phase Locking (2PL), Transaction T1 (Customer A’s purchase) acquires an exclusive lock on the inventory, preventing Transaction T2 (Customer B’s purchase) from accessing it until T1 completes. T1 checks the inventory, deducts 3 tickets for Customer A, reducing the count to 2, and then releases the lock. Only then can T2 proceed, locking the inventory, seeing the updated 2 tickets, and completing the purchase for Customer B. This ensures serializability by isolating transactions through locking, yielding the same result as if the transactions had run one after the other.</p>
<p>While Strict 2PL guarantees correctness, it comes with some downsides:</p>
<ul>
<li>Transactions waiting to acquire locks may be blocked for long durations, especially in high-contention scenarios, leading to reduced throughput.</li>
<li>If two transactions hold locks on different resources and wait for each other to release them, a deadlock occurs, requiring intervention (e.g., by aborting one transaction).</li>
<li>The strict correctness requirements can lead to long transaction times, making it less suitable for high-concurrency workloads.</li>
</ul>
<p>Strict 2PL is present in relational database systems such as PostgreSQL, and Oracle Database.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="optimistic-concurrency-control-occ">Optimistic Concurrency Control (OCC)<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#optimistic-concurrency-control-occ" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>Optimistic concurrency control takes the opposite approach - it assumes that conflicts happen rarely, and if there are such scenarios, then it would deal with it at the time of the conflict. OCC works this way:</p>
<ul>
<li>Transactions track read and write operations and, upon completion, validate these changes to check for conflicts.</li>
<li>If conflicts are detected, one or more conflicting transactions are rolled back and can be retried if needed be.</li>
</ul>
<p>OCC is particularly effective in low-contention environments, where conflicts between transactions are infrequent. However, in scenarios with frequent conflicts, such as multiple transactions attempting to modify the same data, OCC may result in a high number of rollbacks, reducing its efficiency. Its ability to allow multiple transactions to proceed without locking makes it a good choice for workloads where contention is low and throughput is prioritized over strict blocking mechanisms.</p>
<img src="https://hudi.apache.org/assets/images/blog/concurrency_control/OCC.png" alt="OCC" align="middle">
<p>For our example, with OCC, both transactions will proceed, each reading the initial count of 5 tickets and preparing to deduct 3. When they try to commit, a conflict check (history) will reveal that reducing by 3 tickets would oversell the inventory. As a result, one transaction (e.g., Customer B’s) is rolled back, allowing Customer A to complete their purchase, reducing the inventory to 2. Customer B then retries, sees only 2 tickets left, and adjusts accordingly.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="multi-version-concurrency-control-mvcc">Multi-Version Concurrency Control (MVCC)<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#multi-version-concurrency-control-mvcc" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>MVCC enables concurrent transactions by maintaining multiple versions of each data item, allowing transactions to read data as it appeared at a specific point in time. Here’s how MVCC works at a high-level:</p>
<ul>
<li>Each transaction is split into a "read set" and a "write set." This separation of read and write sets enhances concurrency by reducing conflicts.</li>
<li>All reads in a transaction operate as if they are accessing a single, consistent ‘snapshot’ of the data at a particular moment.</li>
<li>Writes are applied as if they are part of a ‘later snapshot’, ensuring that any changes made by the transaction are isolated from other concurrent transactions until the transaction completes.</li>
</ul>
<img src="https://hudi.apache.org/assets/images/blog/concurrency_control/MVCC.png" alt="MVCC" align="middle">
<p>In our example, with MVCC, each customer sees a consistent snapshot of 5 tickets when they start. Customer A completes their purchase first, reducing the inventory to 2 tickets. When Customer B finishes, they commit their transaction based on the latest snapshot, seeing only 2 tickets left and adjusting their purchase accordingly.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="concurrency-control-in-open-table-formats">Concurrency Control in Open Table Formats<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#concurrency-control-in-open-table-formats" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Data lakes were built for scalable storage, cheaper cost, and to address some of the limitations of data warehouses (such as handling varied data types), but they lack the transactional storage engine needed to enforce ACID guarantees. We learnt in our previous section how isolation (the "I" in ACID) plays a critical role in managing concurrency by ensuring that each transaction operates independently without unintended interference from others. This level of isolation is essential for preventing concurrency anomalies like dirty reads, lost updates, and other issues that can compromise data integrity. Data lakehouse architecture with open table formats such as Apache Hudi, Apache Iceberg, and Delta Lake as the foundation for the storage layer addresses this problem by applying some of the concurrency control methods available in the database systems.</p>
<p>Let’s take a look at what type of concurrency control methods are available within these formats with a focus on <strong>Apache Hudi</strong>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="apache-hudi">Apache Hudi<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#apache-hudi" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Most of the concurrency control implementations today in lakehouse table formats focus on optimistically handling conflicts. OCC relies on the assumption that conflicts are rare, making it suitable for simple, append-only jobs but inadequate for scenarios that require frequent updates or deletes. In OCC, each job typically takes a table-level lock to check for conflicts by determining if there are overlapping files that multiple jobs have impacted. If a conflict is detected, the job will abort its operation <em>entirely</em>. This could be a problem with certain types of workloads. For example, an ingest job writing data every 30 minutes and a deletion job running every two hours may often conflict, causing the deletion job to fail. In such cases especially with long-running transactions, OCC is problematic because the chance of conflicts increases over time.</p>
<img src="https://hudi.apache.org/assets/images/blog/concurrency_control/concur_blog.png" alt="Hudi concurrency control methods" width="900" align="middle">
<p>Apache Hudi’s uniqueness lies in the fact that it clearly distinguishes the different actors interacting with the format, i.e. writer processes (that issue user’s upserts/deletes), table services (such as clustering, compaction) and readers (that execute queries and read data). Hudi provides <a href="https://en.wikipedia.org/wiki/Snapshot_isolation" target="_blank" rel="noopener noreferrer">Snapshot Isolation</a> between all three types of processes, meaning they all operate on a consistent snapshot of the table. For writers, Hudi implements a variant of Serializable <a href="https://distributed-computing-musings.com/2022/02/transactions-serializable-snapshot-isolation/" target="_blank" rel="noopener noreferrer">Snapshot Isolation (SSI)</a>. Here’s how Hudi supports different types of concurrency control methods, offering fine-grained control over concurrent data access and updates.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="occ-multi-writers">OCC (Multi Writers)<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#occ-multi-writers" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>OCC is primarily used to manage concurrent writer processes in Hudi. For example, two different Spark jobs interacting with the same Hudi table to perform updates. Hudi’s OCC workflow involves a series of checks to detect and handle conflicts, ensuring that only one writer can successfully commit changes to a particular file group at any given time. Here’s a quick summary of what file groups and slices mean in Hudi.</p>
<p><em>File group: Groups multiple versions of a base file (e.g. Parquet). The file group is uniquely identified by a File id. Each version corresponds to the commit's timestamp recording updates to records in the file.</em></p>
<p><em>File slice: A File group can further be split into multiple slices. Each file slice within the file-group is uniquely identified by the commit's timestamp that created it.</em></p>
<p>OCC works in three phases - read, validate and write. When a writer begins a transaction, it first makes the changes, i.e. commits in isolation. During the validation phase, writers compare their proposed changes against existing file groups in the timeline to detect conflicts. Finally, in the write phase, the changes are either committed if no conflicts are found or rolled back if conflicts are detected.</p>
<p>For multi-writing scenarios, when a writer begins the commit process, it acquires a short-duration lock from the lock provider, typically implemented with an external service such as Zookeeper, Hive Metastore, or DynamoDB. Once the lock is secured, the writer loads the <a href="https://hudi.apache.org/docs/next/timeline" target="_blank" rel="noopener noreferrer">current timeline</a> to check for previously <code>completed</code> actions on the targeted file group. After that, it scans for any instances marked as completed with a timestamp greater than the target file slice's timestamp. If any such completed instances are found, it indicates that another writer has already modified the target file group, leading to a conflict. In this case, Hudi’s OCC logic prevents the current transaction from proceeding by aborting the writer’s operation, ensuring that only one writer’s updates are committed. If no conflicting instant exists, the transaction is allowed to proceed, and the writer completes the write operation, adding a new file slice to the timeline. Finally, Hudi updates the timeline with the location of the new file slice and releases the table lock, allowing other transactions to proceed. This approach adheres to the ACID principles providing consistency guarantees.</p>
<p>It is important to note that Hudi acquires locks <strong>only</strong> at critical points, such as during the commit or while scheduling table services, rather than across the entire transaction. This approach significantly improves concurrency by allowing writers to work in parallel without contention.</p>
<p>Additionally, Hudi’s OCC operates at the file level, meaning conflicts are detected and resolved based on the files being modified. For instance, when two writers work on non-overlapping files, both writes are allowed to succeed. However, if their operations overlap and modify the same set of files, only one transaction will succeed, and the other will be rolled back. This file-level granularity is a significant advantage in many real-world scenarios, as it enables multiple writers to proceed without issues as long as they are working on different files, improving concurrency and overall throughput.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="mvcc-writer-table-service-and-table-service-table-service">MVCC (Writer-Table Service and Table Service-Table Service)<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#mvcc-writer-table-service-and-table-service-table-service" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>Apache Hudi provides support for Multiversion Concurrency Control (MVCC) between writers and table-services (for example, an update Spark job and <a href="https://hudi.apache.org/docs/clustering" target="_blank" rel="noopener noreferrer">clustering</a>) and between different table services (such as <a href="https://hudi.apache.org/docs/compaction" target="_blank" rel="noopener noreferrer">compaction</a> and clustering). Similar to OCC, the Hudi timeline is instrumental in Hudi’s MVCC implementation, which keeps a track of all the events (instants) happening in a particular Hudi table. Every writer and reader relies on the file system’s state to decide where to carry out the operations, thereby providing read-write isolation.</p>
<p>When a write operation begins, Hudi marks the action as either <code>requested</code> or <code>inflight</code> on the timeline, making all processes aware of the ongoing operation. This ensures that table management operations such as compaction and clustering are aware of active writes and do not include the file slices currently being modified. With Hudi 1.0's new <a href="https://hudi.apache.org/docs/timeline/" target="_blank" rel="noopener noreferrer">timeline</a> design, compaction and clustering operations are now based on both the requested and completion times of actions, treating these timestamps as <em>intervals</em> to dynamically determine file slices. This means a service like compaction no longer needs to block ongoing writes and can be scheduled at any instant without interfering with active operations.</p>
<p>Under the new design, file slicing includes only those file slices whose completion times precede the start of the compaction or clustering process. This intelligent slicing mechanism ensures that these table management services work only on finalized data while new writes seamlessly continue without impacting the base files being compacted. By decoupling the scheduling of table services from active writes, Hudi 1.0 eliminates the need for strict scheduling sequences or blocking behaviors.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="non-blocking-concurrency-control-multi-writers">Non-Blocking Concurrency Control (Multi Writers)<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#non-blocking-concurrency-control-multi-writers" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>In a generic sense, Non-Blocking Concurrency Control (NBCC) allows multiple transactions to proceed simultaneously without locking, reducing delays and improving throughput in high-concurrency environments. <a href="https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0" target="_blank" rel="noopener noreferrer">Hudi 1.0</a> introduces a new concurrency mode, <code>NON_BLOCKING_CONCURRENCY_CONTROL</code>, where, unlike OCC, multiple writers can operate on the same table simultaneously with non-blocking conflict resolution. This approach eliminates the need for explicit locks to serialize writes, enabling higher concurrency. Instead of requiring each writer to wait, NBCC allows concurrent writes to proceed, making it ideal for real-time applications that demand faster data ingestion.</p>
<p>In NBCC, the only lock required is for writing the commit metadata to the Hudi timeline, which ensures that the order and state of completed transactions is tracked accurately. With the release of version 1.0, Hudi introduces <a href="https://hudi.apache.org/docs/timeline#truetime-generation" target="_blank" rel="noopener noreferrer">TrueTime</a> semantics for instant times on the timeline, ensuring unique and monotonically increasing instant values. Each action on the Hudi timeline now includes both a <em>requested time</em> and a <em>completion time</em>, enabling these actions to be treated as intervals. This allows for more precise conflict detection by reasoning about overlapping actions within these time intervals.  The final serialization of writes in NBCC is determined by the <em>completion</em> times. This means multiple writers can modify the same file group, with conflicts resolved automatically by query readers and the compactor. NBCC is available with the new Hudi 1.0 release, thereby providing more controls to balance speed with data consistency, even under heavy concurrent workloads.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="concurrency-control-deployment-modes-in-hudi">Concurrency Control Deployment Modes in Hudi<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#concurrency-control-deployment-modes-in-hudi" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Hudi offers several deployment models to handle different concurrency needs, allowing users to optimize for performance, simplicity, or high-concurrency scenarios depending on the requirements.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="single-writer-with-inline-table-services">Single Writer with Inline Table Services<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#single-writer-with-inline-table-services" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>In this model, only one writer handles data ingestion or updates, with table services (such as cleaning, compaction, and clustering) running inline sequentially after every write. This approach <em>eliminates</em> the need for concurrency control as all operations occur in a single process. MVCC in Hudi guarantees that readers see consistent snapshots, isolating them from ongoing writes and table services. This model is ideal for straightforward use cases where the focus is on getting data into the lakehouse without the complexity of managing multiple writers.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="single-writer-with-async-table-services">Single Writer with Async Table Services<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#single-writer-with-async-table-services" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>For workloads that require higher throughput without blocking writers, Hudi supports asynchronous table services. In this model, a single writer continuously ingests data, while table services such as compaction and clustering run asynchronously in the same process. MVCC allows these background jobs to operate concurrently with ingestion without creating conflicts, as they coordinate to avoid race conditions. This model suits applications where ingestion speed is essential, as async services help optimize the table in the background, reducing operational complexity without the need for external orchestration.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="multi-writer-configuration">Multi-Writer Configuration<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#multi-writer-configuration" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>In cases where multiple writer jobs need to access the same table, Hudi supports multi-writer setups. This model allows disparate processes, such as multiple ingestion writers or a mix of ingestion and separate table service jobs to write concurrently. To manage conflicts, Hudi uses OCC with file-level conflict resolution, allowing non-overlapping writes to proceed while conflicting writes are resolved by allowing only one to succeed. For these types of multi-writer setups, <a href="https://hudi.apache.org/docs/concurrency_control#external-locking-and-lock-providers" target="_blank" rel="noopener noreferrer"><em>external</em></a> lock providers like Amazon DynamoDB, Zookeeper, or Hive Metastore are required to coordinate concurrent access. This setup is ideal for production-level, high-concurrency environments where different processes need to modify the table simultaneously.</p>
<p>Note that while Hudi provides OCC to deal with multiple writers, table services can still run asynchronously and without locks if they operate in the same process as the writer. This is because Hudi intelligently differentiates between the different types of actors (writers, table services) that interact with the table.</p>
<p>You will need to set the following properties to activate OCC with locks.</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">hoodie.write.concurrency.mode=optimistic_concurrency_control</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">hoodie.write.lock.provider=&lt;lock-provider-classname&gt;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">hoodie.cleaner.policy.failed.writes=LAZY</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><code>Hoodie.write.lock.provider</code> defines the lock provider class that manages locks for concurrent writes. Default is <code>org.apache.hudi.client.transaction.lock.ZookeeperBasedLockProvider</code></p>
<p>The <code>LAZY</code> mode cleans failed writes only after a heartbeat timeout when the cleaning service runs and is recommended when using multiple writers.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-use-occ-with-apache-hudi-and-apache-spark">How to use OCC with Apache Hudi and Apache Spark<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#how-to-use-occ-with-apache-hudi-and-apache-spark" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>This is a simple example where we configure OCC by setting the <code>hoodie.write.concurrency.mode</code> to <code>optimistic_concurrency_control</code>. We also specify a lock provider (in this case, Zookeeper) to manage concurrent access, along with essential table options like the precombine field, record key, and partition path.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> pyspark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">sql </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> SparkSession</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Initialize Spark session</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark </span><span class="token operator">=</span><span class="token plain"> SparkSession</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">builder \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">appName</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"Hudi Example with OCC"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">config</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"spark.serializer"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"org.apache.spark.serializer.KryoSerializer"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">getOrCreate</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Sample DataFrame</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">inputDF </span><span class="token operator">=</span><span class="token plain"> spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">createDataFrame</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"2024-11-19 10:00:00"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"A"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"partition1"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"2024-11-19 10:05:00"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"B"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"partition1"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"uuid"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"ts"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"value"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"partitionpath"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">tableName </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"my_hudi_table"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">basePath </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"s3://path-to-your-hudi-table"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Write DataFrame to Hudi with OCC and Zookeeper lock provider</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">inputDF</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">write</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token builtin" style="color:rgb(189, 147, 249)">format</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"hudi"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">option</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"hoodie.datasource.write.precombine.field"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"ts"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">option</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"hoodie.cleaner.policy.failed.writes"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"LAZY"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">option</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"hoodie.write.concurrency.mode"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"optimistic_concurrency_control"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">option</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"hoodie.write.lock.provider"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"org.apache.hudi.client.transaction.lock.ZookeeperBasedLockProvider"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">option</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"hoodie.write.lock.zookeeper.url"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"zk-cs.hudi-infra.svc.cluster.local"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">option</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"hoodie.write.lock.zookeeper.port"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"2181"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">option</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"hoodie.write.lock.zookeeper.base_path"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"/test"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">option</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"hoodie.datasource.write.recordkey.field"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"uuid"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">option</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"hoodie.datasource.write.partitionpath.field"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"partitionpath"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">option</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"hoodie.table.name"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> tableName</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">mode</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">"overwrite"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">save</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">basePath</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">stop</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="apache-iceberg">Apache Iceberg<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#apache-iceberg" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Apache Iceberg supports multiple concurrent writes through Optimistic Concurrency Control (OCC). The most important part to note here is that Iceberg needs a <em>catalog</em> component to adhere to the ACID guarantees. Each writer assumes it is the only one making changes, generating new table metadata for its operation. When a writer completes its updates, it attempts to commit the changes by performing an <em>atomic swap</em> of the latest <code>metadata.json</code> file in the catalog, replacing the existing metadata file with the new one.</p>
<p>If this atomic swap fails (due to another writer committing changes in the meantime), the writer’s commit is rejected. The writer then retries the entire process by creating a new metadata tree based on the latest state of the table and attempting the atomic swap again.</p>
<p>When it comes to table maintenance tasks, such as optimizations (e.g., compaction) or large delete jobs, Iceberg treats these as regular writes. These operations can overlap with ingestion jobs, but they follow the same OCC principles - conflicts are resolved by retrying based on the latest table state. Users are recommended to schedule such jobs during official maintenance periods to avoid contention, as frequent retries due to conflicts can impact performance.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="delta-lake">Delta Lake<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#delta-lake" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Delta Lake provides concurrency control through Optimistic Concurrency Control (OCC) for transactional guarantees between writes. OCC allows multiple writers to attempt changes independently, assuming conflicts are infrequent. When a writer tries to commit, it checks for any conflicting updates from other transactions in the <a href="https://www.databricks.com/blog/2019/08/21/diving-into-delta-lake-unpacking-the-transaction-log.html" target="_blank" rel="noopener noreferrer">transaction log</a>. If a conflict is found, the transaction is rolled back, and the writer retries based on the latest version of the data.</p>
<p>Additionally, Delta Lake employs Multi-Version Concurrency Control (MVCC) within the file system to separate reads from writes. By keeping data objects and the transaction log immutable, MVCC allows readers to access a consistent snapshot of the data, even as new writes are added. This not only protects existing data from modification during concurrent transactions but also enables time-travel queries, allowing users to query historical snapshots.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://hudi.apache.org/cn/blog/2025/01/28/concurrency-control#conclusion" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Concurrency control is critical for Open lakehouse architectures, especially when your architecture has multiple concurrent pipelines interacting with the same table. Open table formats such as Apache Hudi bring well-established concurrency control methods from traditional database systems into the Lakehouse architecture to handle these operations while maintaining data consistency and scalability. Apache Hudi’s unique design to distinguish between writers, table services, and readers ensures snapshot isolation across all three processes. By supporting multiple concurrency control methods, such as OCC for managing writer conflicts, MVCC for isolating background table services and writers, and a novel NBCC for non-blocking, real-time ingestion, Hudi offers greater flexibility with complex workloads.</p>
<hr>]]></content>
        <author>
            <name>Dipankar Mazumdar</name>
        </author>
        <category label="multi-writer" term="multi-writer"/>
        <category label="concurrency" term="concurrency"/>
        <category label="concurrency-control" term="concurrency-control"/>
        <category label="non-blocking concurrency-control" term="non-blocking concurrency-control"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="Apache Iceberg" term="Apache Iceberg"/>
        <category label="Delta Lake" term="Delta Lake"/>
        <category label="blog" term="blog"/>
        <category label="design" term="design"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Apache Hudi 1.0 Now Generally Available]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/01/18/apache-hudi-1-0-now-generally-available</id>
        <link href="https://hudi.apache.org/cn/blog/2025/01/18/apache-hudi-1-0-now-generally-available"/>
        <updated>2025-01-18T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://www.infoq.com/news/2025/01/apache-hudi/">here</a></span>]]></content>
        <author>
            <name>Renato Losio</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="apache hudi" term="apache hudi"/>
        <category label="hudi 1.0.0" term="hudi 1.0.0"/>
        <category label="infoq" term="infoq"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out of the box Key Generators in Apache Hudi]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi</id>
        <link href="https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi"/>
        <updated>2025-01-15T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Introduction]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi#introduction" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>The goal of Apache Hudi is to bring database-like features to data lakes. This addresses the main shortcoming of traditional data lakes: the inability to easily perform row-level updates or deletions.By integrating database-like management capabilities into data lakes, Hudi revolutionizes how it handles and processes large volumes of data, enabling out-of-the-box upserts and deletes that facilitate efficient record level updating and deletion.
One of Hudi's key innovations is the ability for users to explicitly define a Record Key, similar to a unique key in traditional databases, along with a Partition Key that aligns with the data lake paradigm. These two keys make the <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieKey.java" target="_blank" rel="noopener noreferrer">HoodieKey</a> that aligns with the data lake paradigm. These two keys make the HoodieKey which is similar to the primary key which uniquely defines each row. This enables hudi to do the upsert based on Hoodiekey. The upsert operation works by utilizing the HoodieKey to locate the exact file group where the data associated with that key resides.  When a new record is ingested into the Hudi table, the system first derives  the HoodieKey of the incoming record based on the unique key and partitioning schema configured. This key is used to determine which file group (a logical grouping of files) the record should be associated with which is usually achieved via an <a href="https://hudi.apache.org/docs/indexes" target="_blank" rel="noopener noreferrer">indexing</a> mechanism.
In this blog, we will explore the concept of Key Generators in Apache Hudi, how they enhance data management, and their role in enabling efficient data operations in modern data lakes.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="challenge">Challenge<a href="https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi#challenge" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>The biggest challenge in defining the record key and partition key on a table is  the columns in input data does not naturally lend itself to being used as a primary key or partition key directly. In the realm of databases, we often have below cases -</p>
<ul>
<li>Need to have multiple fields that serve as primary key commonly known as composite keys in the database.</li>
<li>It is necessary to preprocess the data to derive a specific field that can serve as a primary key before loading it into the database.</li>
<li>Sometimes we have to generate unique ids also. Common use case is surrogate key.</li>
</ul>
<p>Similarly, for partition columns also in datalakes, most of the time the raw field can’t be used as a partition key.</p>
<ul>
<li>Partition columns often have time grain like month level or year level partition but input data mostly contain timestamp and date.</li>
<li>Nested primary keys are very common, and necessitates multiple partition columns.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="approaches-to-handling-this-in-data-pipelines">Approaches to Handling this in Data Pipelines<a href="https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi#approaches-to-handling-this-in-data-pipelines" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Data Lake and Lakehouse technologies typically address such scenarios by preprocessing the data. For example, if date-based partitioning is required and a timestamp column is available, the data must be processed using Spark SQL date functions to extract relevant components (e.g., year, month, day). These derived columns are then used for partitioning. However, this process can become cumbersome at scale, especially when multiple data streams are writing to the same Hudi table. The same extraction logic needs to be applied to all streams, and any table maintenance activities (such as bootstrapping or backfilling) also require this logic to be reapplied. This repetition is error-prone and can lead to data consistency issues if the logic is incorrectly applied.
Hudi addresses these challenges with a built-in solution: key generators. These can be configured at the table level, eliminating the need to repeatedly apply the same logic. With key generators, Hudi automatically handles the conversion process every time, ensuring consistency and reducing the risk of errors.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-key-generators-in-apache-hudi">What are Key Generators in Apache Hudi<a href="https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi#what-are-key-generators-in-apache-hudi" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p><a href="https://hudi.apache.org/docs/key_generation" target="_blank" rel="noopener noreferrer">Key generators</a> in Apache Hudi are essential components responsible for creating record keys and partition keys for records within a dataset. Hudi uses key generators to extract the Hudi record key, which is a combination of the record key and the partition key, from the incoming record fields. This process allows Hudi to efficiently prepare the hoodie key on which updates can occur. During upserts, Hudi identifies the file group that contains the specified hoodie key using an index and updates the corresponding file group accordingly.
Hudi offers several built-in key generator implementations that cover common use cases, such as generating record keys based on fields from the input data. However, to provide flexibility and support for more complex use cases, Hudi also offers a pluggable interface. This allows users to implement custom key generators tailored to their specific requirements.
To create a custom key generator, you can extend the <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/keygen/BaseKeyGenerator.java" target="_blank" rel="noopener noreferrer">BaseKeyGenerator</a> class which itself extends the <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/keygen/KeyGenerator.java" target="_blank" rel="noopener noreferrer">KeyGenerator</a>  class and implement methods such as getRecordKey and getPartitionKey. This enables you to define the specific logic required for calculating record and partition keys tailored to your dataset's requirements. Additionally, Hudi includes a variety of built-in key generators that address many common scenarios discussed in the previous section, streamlining the process of key generation for users.
The key generator is configured at the table level and stored in the hoodie.properties file, which resides within the .hoodie directory. This file contains all the table-level configurations, including the key generation settings. Once a table is created with a particular key generator we can’t change it. It can be set using the configuration hoodie.datasource.write.keygenerator.class</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="out-of-the-box-key-generators">Out of the Box Key Generators<a href="https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi#out-of-the-box-key-generators" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="simplekeygenerator">SimpleKeyGenerator<a href="https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi#simplekeygenerator" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>The SimpleKeyGenerator is a basic key generator used in Apache Hudi when direct fields from the input dataset can serve as both the record key and partition key. It maps a specific column in the DataFrame to the record key and another column to the partition path. This widely-used generator interprets values as-is from the DataFrame and converts them to strings, making it ideal for straightforward data structures.
Please note that this is the default key generator for the partitioned datasets.</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.recordkey.field": "id",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.partitionpath.field": "date",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.keygenerator.class": "org.apache.hudi.keygen.SimpleKeyGenerator"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="nonpartitionedkeygenerator">NonpartitionedKeyGenerator<a href="https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi#nonpartitionedkeygenerator" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>The NonpartitionedKeyGenerator is a key generator in Apache Hudi designed specifically for non-partitioned datasets. Unlike the SimpleKeyGenerator, which uses a field to determine the partition path for the data, the NonpartitionedKeyGenerator does not assign a partition key to the records. Instead, it returns an empty string as the partition key for all records. This is because the dataset is non-partitioned, meaning all records are stored in a single partition.</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.recordkey.field": "id",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.keygenerator.class": "org.apache.hudi.keygen.NonpartitionedKeyGenerator"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="complexkeygenerator">ComplexKeyGenerator<a href="https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi#complexkeygenerator" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>This key generator is used when multiple fields are used to create the record key or partition key. We can provide the comma separated list of the columns. In the output, the hoodie record key is generated using the format key1<!-- -->:value1<!-- -->,key2<!-- -->:value2<!-- -->. If any one of the partition key or record key contains multiple fields, then we have to use ComplexKeyGenerator.</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.keygenerator.class" : "org.apache.hudi.keygen.ComplexKeyGenerator",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.recordkey.field" = "key1,key2",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.partitionpath.field" = "country,state,city"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="timestampbasedkeygenerator">TimestampBasedKeygenerator<a href="https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi#timestampbasedkeygenerator" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>The TimestampBasedKeyGenerator allows you to generate partition keys based on timestamp fields in your data. This is especially useful when you want to partition your data by date, month, or year, depending on your use case. The key generator can transform timestamps into different formats, enabling you to create partitions that suit your analytical needs.</p>
<p>Relevant Configurations</p>
<ul>
<li>
<p><strong>hoodie.datasource.write.keygenerator.class</strong>
To use this key generator, The key gen class should be <code>org.apache.hudi.keygen.TimestampBasedKeyGenerator</code></p>
</li>
<li>
<p><strong>hoodie.deltastreamer.keygen.timebased.timestamp.type</strong>
This config determines the nature of the value of input. Below can be the possible values for this -
<strong>DATE_STRING</strong>: Use this when the input value is in string format.</p>
<ul>
<li>
<p>MIXED: This option allows for a combination of formats.</p>
</li>
<li>
<p>UNIX_TIMESTAMP: Select this when the input value is in epoch timestamp format (long type) measured in seconds.</p>
</li>
<li>
<p>EPOCHMILLISECONDS: Use this when the input value is in epoch timestamp format (long type) measured in milliseconds.</p>
</li>
<li>
<p>SCALAR: This option is for epoch timestamp values (long type) where you can specify any time unit.</p>
</li>
</ul>
</li>
<li>
<p><strong>hoodie.deltastreamer.keygen.timebased.timestamp.scalar.time.unit</strong>
When using the SCALAR timestamp type, you can define the unit of the epoch time. Valid options include NANOSECONDS, MICROSECONDS, MILLISECONDS, SECONDS, MINUTES, HOURS, DAYS</p>
</li>
<li>
<p><strong>hoodie.keygen.timebased.input.dateformat</strong>
When the timestamp type is DATE_STRING or MIXED, this config can be defined to specify the date format in which the field is coming in input.</p>
</li>
<li>
<p><strong>hoodie.keygen.timebased.output.dateformat</strong>
When the timestamp type is set to DATE_STRING or MIXED, this configuration defines the desired date format for the output field. It allows you to specify how the date should be formatted when it is generated or output.</p>
</li>
<li>
<p><strong>hoodie.deltastreamer.keygen.timebased.input.timezone</strong>
This setting specifies the timezone for the input date field derived from the raw data. The default value is UTC.</p>
</li>
<li>
<p><strong>hoodie.deltastreamer.keygen.timebased.output.timezone</strong>
This setting defines the timezone for the output date field that will be used to populate the partition column. The default value is UTC.</p>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="common-use-cases">Common Use Cases<a href="https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi#common-use-cases" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<ul>
<li>Data Contains Timestamp Field and We Want Date Level Partitions
In this scenario, you have a dataset with a timestamp field, and you want to partition the data by the date (i.e., year-month-day).</li>
</ul>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.keygenerator.class":     "org.apache.hudi.keygen.TimestampBasedKeyGenerator",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.deltastreamer.keygen.timebased.timestamp.type": "DATE_STRING",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.keygen.timebased.input.dateformat":"yyyy-MM-dd'T'HH:mm:ss.SSSSSSZ",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.keygen.timebased.output.dateformat":"yyyy-MM-dd",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.partitionpath.field": "event_time"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<ul>
<li>Data Contains Date Field but We Want to Have Month or Year Level Partitions
Here, you have a dataset with a date field, but you want to create partitions at a higher granularity, such as by month or year.</li>
</ul>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.keygenerator.class":     "org.apache.hudi.keygen.TimestampBasedKeyGenerator",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.deltastreamer.keygen.timebased.timestamp.type": "DATE_STRING",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.keygen.timebased.input.dateformat":"yyyy-MM-dd",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.keygen.timebased.output.dateformat":"yyyyMM",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.partitionpath.field": "event_date"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>In the example above, if we have an input with a date column named event_date in the format 'yyyy-MM-dd', the configurations will convert this format to a monthly level in the format 'yyyyMM' and use it as the partition column.</p>
<p>We can refer <a href="https://hudi.apache.org/docs/0.10.0/key_generation/#timestampbasedkeygenerator" target="_blank" rel="noopener noreferrer">TimestampBasedKeyGenerator</a> for more examples</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="customkeygenerator">CustomKeyGenerator<a href="https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi#customkeygenerator" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>In typical use cases, using the same key generator for both the record key and the partition key often does not meet the requirements. For such scenarios, a Custom Key Generator is particularly useful, as it allows for the use of different key generators for different fields.
A common use case arises when the partition key consists of multiple fields, and you also need to extract date or month-level partitions from a timestamp field. In these situations, it is essential to utilize both the TimestampBasedKeyGenerator and the ComplexKeyGenerator. However, since you cannot specify two different key generator classes simultaneously, the CustomKeyGenerator serves as an effective solution. We can configure it as list of comma separated fields with the key generator separated by colon. Example - key1<!-- -->:Timestamp<!-- -->,key2<!-- -->:SIMPLE<!-- -->,key3<!-- -->:SIMPLE<!-- -->
When we pass the partition column, we can also provide which key generator to use. The configurations below enable you to use SimpleKeyGenerator to extract the country field and TimestampBasedKeygenerator to transform the event_date field to use only month level partitions.</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.keygenerator.class":     "org.apache.hudi.keygen.TimestampBasedKeyGenerator",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.deltastreamer.keygen.timebased.timestamp.type": "DATE_STRING",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.keygen.timebased.input.dateformat":"yyyy-MM-dd",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.keygen.timebased.output.dateformat":"yyyyMM",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.partitionpath.field": "country:SIMPLE,event_date:TIMESTAMP"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://hudi.apache.org/cn/blog/2025/01/15/outofbox-key-generators-in-hudi#conclusion" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Key generators in Hudi are vital components that enable efficient record identification, partitioning, and data operations in large datasets. Whether you're performing upserts, deletes, or managing time-series data, choosing the right key generator ensures that Hudi can handle the data efficiently, while aligning with your business logic. By addressing challenges like composite keys, timestamp-based partitioning, and complex use cases, Apache Hudi revolutionizes how data lakes handle evolving data, providing database-like management capabilities that are scalable and flexible.</p>]]></content>
        <author>
            <name>Aditya Goenka</name>
        </author>
        <category label="Data Lake" term="Data Lake"/>
        <category label="Data Lakehouse" term="Data Lakehouse"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
        <category label="Key Generators" term="Key Generators"/>
        <category label="partition" term="partition"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Apache Iceberg vs Delta Lake vs Apache Hudi]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/01/09/apache-iceberg-vs-delta-lake-vs-apache-hudi</id>
        <link href="https://hudi.apache.org/cn/blog/2025/01/09/apache-iceberg-vs-delta-lake-vs-apache-hudi"/>
        <updated>2025-01-09T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://medium.com/@algodaysindia/apache-iceberg-vs-delta-lake-vs-apache-hudi-f987fee8dbe1">here</a></span>]]></content>
        <author>
            <name>AlgoDays</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="apache hudi" term="apache hudi"/>
        <category label="apache iceberg" term="apache iceberg"/>
        <category label="delta lake" term="delta lake"/>
        <category label="comparison" term="comparison"/>
        <category label="medium" term="medium"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Future of Data Lakehouses: A Fireside Chat with Vinoth Chandar - Founder CEO Onehouse & PMC Chair of Apache Hudi]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/01/08/the-future-of-data-lakehouses-a-fireside</id>
        <link href="https://hudi.apache.org/cn/blog/2025/01/08/the-future-of-data-lakehouses-a-fireside"/>
        <updated>2025-01-08T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://www.dataengineeringweekly.com/p/the-future-of-data-lakehouses-a-fireside">here</a></span>]]></content>
        <author>
            <name>Ananth Packkildurai</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="apache hudi" term="apache hudi"/>
        <category label="data lakehouse" term="data lakehouse"/>
        <category label="lakehouse" term="lakehouse"/>
        <category label="dataengineeringweekly" term="dataengineeringweekly"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Use the New Hudi Streamer with Hudi 1.0.0 on EMR Serverless 7.5.0 | Hands-on Labs]]></title>
        <id>https://hudi.apache.org/cn/blog/2025/01/05/how-use-new-hudi-streamer-100-emr-serverless-750-hands-on</id>
        <link href="https://hudi.apache.org/cn/blog/2025/01/05/how-use-new-hudi-streamer-100-emr-serverless-750-hands-on"/>
        <updated>2025-01-05T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://www.linkedin.com/pulse/how-use-new-hudi-streamer-100-emr-serverless-750-hands-on-soumil-shah-fxrae/">here</a></span>]]></content>
        <author>
            <name>Soumil Shah</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="how-to" term="how-to"/>
        <category label="apache hudi" term="apache hudi"/>
        <category label="hudi 1.0.0" term="hudi 1.0.0"/>
        <category label="hudi streamer" term="hudi streamer"/>
        <category label="amazon emr" term="amazon emr"/>
        <category label="linkedin" term="linkedin"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Indexing in Apache Hudi]]></title>
        <id>https://hudi.apache.org/cn/blog/2024/12/31/indexing-in-apache-hudi</id>
        <link href="https://hudi.apache.org/cn/blog/2024/12/31/indexing-in-apache-hudi"/>
        <updated>2024-12-31T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://medium.com/@sanjeets1900/indexing-in-apache-hudi-674f9481796e">here</a></span>]]></content>
        <author>
            <name>Sanjeet Shukla</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="apache hudi" term="apache hudi"/>
        <category label="indexing" term="indexing"/>
        <category label="medium" term="medium"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Architect’s Guide to Open Table Formats and Object Storage]]></title>
        <id>https://hudi.apache.org/cn/blog/2024/12/31/the-architects-guide-to-open-table-formats-and-object-storage</id>
        <link href="https://hudi.apache.org/cn/blog/2024/12/31/the-architects-guide-to-open-table-formats-and-object-storage"/>
        <updated>2024-12-31T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://thenewstack.io/the-architects-guide-to-open-table-formats-and-object-storage/">here</a></span>]]></content>
        <author>
            <name>Brenna Buuck</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="apache hudi" term="apache hudi"/>
        <category label="apache iceberg" term="apache iceberg"/>
        <category label="delta lake" term="delta lake"/>
        <category label="data lakehouse" term="data lakehouse"/>
        <category label="lakehouse" term="lakehouse"/>
        <category label="table formats" term="table formats"/>
        <category label="thenewstack" term="thenewstack"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Apache Hudi 2024: A Year In Review]]></title>
        <id>https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review</id>
        <link href="https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review"/>
        <updated>2024-12-29T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[As we wrap up another remarkable year for Apache Hudi, I am thrilled to reflect on the tremendous achievements and milestones that have defined 2024. This year has been particularly special as we achieved several significant milestones, including the landmark release of Hudi 1.0, the publication of comprehensive books, and the introduction of new tools that expand Hudi's ecosystem.]]></summary>
        <content type="html"><![CDATA[<img src="https://hudi.apache.org/assets/images/blog/2024-12-29-a-year-in-review-2024/cover.jpg" alt="drawing" style="width:80%;display:block;margin-left:auto;margin-right:auto;margin-top:18pt;margin-bottom:18pt">
<p>As we wrap up another remarkable year for Apache Hudi, I am thrilled to reflect on the tremendous achievements and milestones that have defined 2024. This year has been particularly special as we achieved several significant milestones, including the landmark release of Hudi 1.0, the publication of comprehensive books, and the introduction of new tools that expand Hudi's ecosystem.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="community-growth-and-engagement">Community Growth and Engagement<a href="https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review#community-growth-and-engagement" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>The Apache Hudi community continued its impressive growth trajectory in 2024. The number of new PRs has remained stable, indicating a consistent level of development activities:</p>
<img src="https://hudi.apache.org/assets/images/blog/2024-12-29-a-year-in-review-2024/pr-history.svg" alt="drawing" style="width:80%;display:block;margin-left:auto;margin-right:auto;margin-top:18pt;margin-bottom:18pt">
<p>Our community presence expanded significantly across various platforms:</p>
<ul>
<li>The community grew to over 10,500 followers on LinkedIn</li>
<li>Added 8,755 new followers in the last 365 days</li>
<li>Generated 441,402 content impressions</li>
<li>Received 6,555 reactions and 493 comments across platforms</li>
<li>Our Slack community remained vibrant with rich technical discussions and knowledge sharing</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="major-milestones">Major Milestones<a href="https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review#major-milestones" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="apache-hudi-10-release">Apache Hudi 1.0 Release<a href="https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review#apache-hudi-10-release" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>2024 marked a historic moment with the <a href="https://hudi.apache.org/releases/release-1.0.0" target="_blank" rel="noopener noreferrer">release of Apache Hudi 1.0</a>, representing a major evolution in data lakehouse technology. This release brought several groundbreaking features:</p>
<ul>
<li><strong>Secondary Indexing</strong>: First of its kind in lakehouses, enabling database-like query acceleration with demonstrated 95% latency reduction on 10TB TPC-DS for low-moderate selectivity queries</li>
<li><strong>Logical Partitioning via Expression Indexes</strong>: Introducing PostgreSQL-style expression indexes for more efficient partition management</li>
<li><strong>Partial Updates</strong>: Achieving 2.6x performance improvement and 85% reduction in bytes written for update-heavy workloads</li>
<li><strong>Non-blocking Concurrency Control (NBCC)</strong>: An industry-first feature allowing simultaneous writing from multiple writers</li>
<li><strong>Merge Modes</strong>: First-class support for both <code>commit_time_ordering</code> and <code>event_time_ordering</code></li>
<li><strong>LSM Timeline</strong>: Revamped timeline storage as a scalable LSM tree for extended table history retention</li>
<li><strong>TrueTime</strong>: Strengthened time semantics ensuring forward-moving clocks in distributed processes</li>
</ul>
<p>Please check out the <a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0">announcement blog</a>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="launch-of-hudi-rs">Launch of Hudi-rs<a href="https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review#launch-of-hudi-rs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>A significant expansion of the Hudi ecosystem occurred with the <a href="https://github.com/apache/hudi-rs" target="_blank" rel="noopener noreferrer">release of Hudi-rs</a>, the native Rust implementation for Apache Hudi with Python API bindings. This new project enables:</p>
<ul>
<li>Reading Hudi Tables without Spark or JVM dependencies</li>
<li>Integration with Apache Arrow for enhanced compatibility</li>
<li>Support for Copy-on-Write (CoW) table snapshots and time-travel reads</li>
<li>Cloud storage support across AWS, Azure, and GCP</li>
<li>Native integration with Apache DataFusion, Ray, Daft, etc</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="published-books-and-educational-content">Published Books and Educational Content<a href="https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review#published-books-and-educational-content" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>2024 saw the release of two comprehensive guides to Apache Hudi:</p>
<ul>
<li><a href="https://learning.oreilly.com/library/view/apache-hudi-the/9781098173821/" target="_blank" rel="noopener noreferrer"><strong>"Apache Hudi: The Definitive Guide"</strong></a> (O'Reilly) - Released in early access, <a href="https://www.onehouse.ai/whitepaper/apache-hudi-the-definitive-guide" target="_blank" rel="noopener noreferrer">free copy available</a>, providing comprehensive coverage of:<!-- -->
<ul>
<li>Distributed query engines</li>
<li>Snapshot and time travel queries</li>
<li>Incremental queries</li>
<li>Change-data-capture modes</li>
<li>End-to-end ingestion with Hudi Streamer</li>
</ul>
</li>
</ul>
<img src="https://hudi.apache.org/assets/images/blog/2024-12-29-a-year-in-review-2024/hudi-tdg.jpg" alt="drawing" style="width:80%;display:block;margin-left:auto;margin-right:auto;margin-top:18pt;margin-bottom:18pt">
<ul>
<li><a href="https://blog.datumagic.com/p/apache-hudi-from-zero-to-one-110" target="_blank" rel="noopener noreferrer"><strong>"Apache Hudi: From Zero to One"</strong></a> - A 10-part blog series turned into <a href="https://www.onehouse.ai/whitepaper/ebook-apache-hudi---zero-to-one" target="_blank" rel="noopener noreferrer">an ebook</a>, offering deep technical insights into Hudi's architecture and capabilities, covering:<!-- -->
<ul>
<li>Storage format and operations</li>
<li>Read and write flows</li>
<li>Table services and indexing</li>
<li>Incremental processing</li>
<li>Hudi 1.0 features</li>
</ul>
</li>
</ul>
<img src="https://hudi.apache.org/assets/images/blog/2024-12-29-a-year-in-review-2024/hudi0to1.png" alt="drawing" style="width:80%;display:block;margin-left:auto;margin-right:auto;margin-top:18pt;margin-bottom:18pt">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="community-events-and-sharing">Community Events and Sharing<a href="https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review#community-events-and-sharing" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>The Apache Hudi community maintained a strong presence at major industry events throughout 2024:</p>
<img src="https://hudi.apache.org/assets/images/blog/2024-12-29-a-year-in-review-2024/community-events.png" alt="drawing" style="width:80%;display:block;margin-left:auto;margin-right:auto;margin-top:18pt;margin-bottom:18pt">
<ul>
<li>Databricks' Data+AI Summit - Presenting Apache Hudi's role in the lakehouse ecosystem and its interoperability with other table formats through XTable, an open-source project enabling seamless conversion between Hudi, Delta Lake, and Iceberg</li>
<li>Confluent's Current 2024 - Demonstrating Hudi's powerful CDC capabilities with Apache Flink, showcasing real-time data pipelines and the innovative Non-Blocking Concurrency Control (NBCC) for high-volume streaming workloads</li>
<li>Trino Fest 2024 - Showcasing Hudi connector's evolution and innovations in Trino, including multi-modal indexing capabilities and the roadmap for enhanced query performance through Alluxio-powered caching and expanded DDL/DML support</li>
<li>Bangalore Lakehouse Days - Deep dive into Apache Hudi 1.0's groundbreaking features including LSM-based timeline, functional indexes, and non-blocking concurrency control, demonstrating Hudi's continued innovation in the lakehouse space</li>
</ul>
<p>Additionally, the community launched several new initiatives to foster learning and knowledge sharing:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="lakehouse-chronicles-with-apache-hudi"><a href="https://www.youtube.com/playlist?list=PLxSSOLH2WRMNQetyPU98B2dHnYv91R6Y8" target="_blank" rel="noopener noreferrer">Lakehouse Chronicles with Apache Hudi</a><a href="https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review#lakehouse-chronicles-with-apache-hudi" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>A new community series with 4 episodes released.</p>
<img src="https://hudi.apache.org/assets/images/blog/2024-12-29-a-year-in-review-2024/lakehouse-chronicles.png" alt="drawing" style="width:80%;display:block;margin-left:auto;margin-right:auto;margin-top:18pt;margin-bottom:18pt">
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="hudi-newsletter"><a href="https://hudinewsletter.substack.com/" target="_blank" rel="noopener noreferrer">Hudi Newsletter</a><a href="https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review#hudi-newsletter" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>9 editions published, keeping the community informed about latest developments.</p>
<img src="https://hudi.apache.org/assets/images/blog/2024-12-29-a-year-in-review-2024/newsletter.png" alt="drawing" style="width:80%;display:block;margin-left:auto;margin-right:auto;margin-top:18pt;margin-bottom:18pt">
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="community-syncs"><a href="https://www.youtube.com/@apachehudi" target="_blank" rel="noopener noreferrer">Community Syncs</a><a href="https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review#community-syncs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Featured 8 user stories from major organizations including Amazon, Peloton, Shopee and Uber.</p>
<img src="https://hudi.apache.org/assets/images/blog/2024-12-29-a-year-in-review-2024/community-syncs.png" alt="drawing" style="width:80%;display:block;margin-left:auto;margin-right:auto;margin-top:18pt;margin-bottom:18pt">
<ul>
<li><a href="https://www.youtube.com/watch?v=rMXhlb7Uci8" target="_blank" rel="noopener noreferrer">Powering Amazon Unit Economics with Configurations and Hudi</a></li>
<li><a href="https://www.youtube.com/watch?v=-Pyid5K9dyU" target="_blank" rel="noopener noreferrer">Modernizing Data Infrastructure at Peleton using Apache Hudi</a></li>
<li><a href="https://www.youtube.com/watch?v=fqhr-4jXi6I" target="_blank" rel="noopener noreferrer">Innovative Solution for Real-time Analytics at Scale using Apache Hudi (Shopee)</a></li>
<li><a href="https://www.youtube.com/watch?v=VpdimpH_nsI" target="_blank" rel="noopener noreferrer">Scaling Complex Data Workflows using Apache Hudi (Uber)</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="notable-user-stories-and-technical-content">Notable User Stories and Technical Content<a href="https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review#notable-user-stories-and-technical-content" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Throughout 2024, several organizations shared their Hudi implementation experiences:</p>
<ul>
<li><a href="https://www.notion.com/blog/building-and-scaling-notions-data-lake" target="_blank" rel="noopener noreferrer">Notion's transition from Snowflake to Hudi</a></li>
<li><a href="https://engineering.grab.com/enabling-near-realtime-data-analytics" target="_blank" rel="noopener noreferrer">Grab's implementation of near-realtime data analytics</a></li>
<li><a href="https://aws.amazon.com/blogs/big-data/use-aws-data-exchange-to-seamlessly-share-apache-hudi-datasets/" target="_blank" rel="noopener noreferrer">AWS's data sharing capabilities with AWS Data Exchange</a></li>
<li><a href="https://www.y.uno/post/how-apache-hudi-transformed-yunos-data-lake" target="_blank" rel="noopener noreferrer">Yuno's data lake transformation</a></li>
<li><a href="https://blogs.halodoc.io/data-lake-cost-optimisation-strategies/" target="_blank" rel="noopener noreferrer">Halodoc's cost optimization strategies</a></li>
<li><a href="https://medium.com/upstox-engineering/navigating-the-future-the-evolutionary-journey-of-upstoxs-data-platform-92dc10ff22ae" target="_blank" rel="noopener noreferrer">Upstox's data platform evolution</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="looking-ahead-to-2025">Looking Ahead to 2025<a href="https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review#looking-ahead-to-2025" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>As we look forward to 2025, Apache Hudi's roadmap includes several exciting developments:</p>
<ul>
<li>Enhanced core engine with modernized write paths and advanced indexing (bitmap, vector search)</li>
<li>Multi-modal data support with improved storage engine APIs and cross-format interoperability</li>
<li>Enterprise-grade features including multi-table transactions and advanced caching</li>
<li>Robust platform services with Data Lakehouse Management System (DLMS) components</li>
<li>Broader adoption of Hudi-rs across the ecosystem</li>
<li>Continued focus on stability and seamless migration path for the community</li>
</ul>
<p>These initiatives reflect our commitment to advancing data lakehouse technology while ensuring reliability and user experience.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="get-involved">Get Involved<a href="https://hudi.apache.org/cn/blog/2024/12/29/apache-hudi-2024-a-year-in-review#get-involved" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Join our thriving community:</p>
<ul>
<li>Contribute to the project on GitHub: <a href="https://github.com/apache/hudi" target="_blank" rel="noopener noreferrer">Hudi</a> &amp; <a href="https://github.com/apache/hudi-rs" target="_blank" rel="noopener noreferrer">Hudi-rs</a></li>
<li>Join our <a href="https://apache-hudi.slack.com/join/shared_invite/zt-2ggm1fub8-_yt4Reu9djwqqVRFC7X49g" target="_blank" rel="noopener noreferrer">Slack community</a></li>
<li>Follow us on <a href="https://www.linkedin.com/company/apache-hudi/" target="_blank" rel="noopener noreferrer">LinkedIn</a> and <a href="https://x.com/apachehudi" target="_blank" rel="noopener noreferrer">X (Twitter)</a></li>
<li>Subscribe to our <a href="https://www.youtube.com/@apachehudi" target="_blank" rel="noopener noreferrer">YouTube channel</a></li>
<li>Participate in our <a href="https://hudi.apache.org/community/syncs" target="_blank" rel="noopener noreferrer">community syncs</a> and <a href="https://hudi.apache.org/community/office_hours" target="_blank" rel="noopener noreferrer">office hours</a>.</li>
<li>Subscribe to the dev mailing list by sending an empty email to <code>dev-subscribe@hudi.apache.org</code></li>
</ul>
<p>The success of Apache Hudi in 2024 wouldn't have been possible without our dedicated community of contributors, users, and supporters. As we celebrate these achievements, we look forward to another year of innovation and growth in 2025.</p>]]></content>
        <author>
            <name>Shiyan Xu</name>
        </author>
        <category label="apache hudi" term="apache hudi"/>
        <category label="community" term="community"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[How lakehouse handles concurrent Read and Writes]]></title>
        <id>https://hudi.apache.org/cn/blog/2024/12/28/how-lakehouse-handles-concurrent-read-and-writes</id>
        <link href="https://hudi.apache.org/cn/blog/2024/12/28/how-lakehouse-handles-concurrent-read-and-writes"/>
        <updated>2024-12-28T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://medium.com/@sanjeets1900/how-lakehouse-handles-concurrent-read-and-writes-b4423fecfe81">here</a></span>]]></content>
        <author>
            <name>Sanjeet Shukla</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="apache hudi" term="apache hudi"/>
        <category label="concurrency" term="concurrency"/>
        <category label="concurrency-control" term="concurrency-control"/>
        <category label="non-blocking concurrency-control" term="non-blocking concurrency-control"/>
        <category label="nbcc" term="nbcc"/>
        <category label="medium" term="medium"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Announcing Apache Hudi 1.0 and the Next Generation of Data Lakehouses]]></title>
        <id>https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0</id>
        <link href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0"/>
        <updated>2024-12-16T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Overview]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview">Overview<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#overview" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>We are thrilled to announce the release of Apache Hudi 1.0, a landmark achievement for our vibrant community that defines what the next generation of data lakehouses should achieve. Hudi pioneered <em><strong>transactional data lakes</strong></em> in 2017, and today, we live in a world where this technology category is mainstream as the “<em><strong>Data Lakehouse”</strong></em>. The Hudi community has made several key, original, and first-of-its-kind contributions to this category, as shown below, compared to when other OSS alternatives emerged. This is an incredibly rare feat for a relatively small OSS community to sustain in a fiercely competitive commercial data ecosystem. On the other hand, it also demonstrates the value of deeply understanding the technology category within a focused open-source community. So, I first want to thank/congratulate the Hudi community and the <strong>60+ contributors</strong> for making 1.0 happen.</p>
<div style="text-align:center"><img src="https://hudi.apache.org/assets/images/blog/hudi-innovation-timeline.jpg" alt="innovation timeline"></div>
<p>This <a href="https://hudi.apache.org/cn/releases/release-1.0.0">release</a> is more than just a version increment—it advances the breadth of Hudi’s feature set and its architecture's robustness while bringing fresh innovation to shape the future. This post reflects on how technology and the surrounding ecosystem have evolved, making a case for a holistic “<em><strong>Data Lakehouse Management System</strong></em>” (<em><strong>DLMS</strong></em>) as the new Northstar. For most of this post, we will deep dive into the latest capabilities of Hudi 1.0 that make this evolution possible.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="evolution-of-the-data-lakehouse">Evolution of the Data Lakehouse<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#evolution-of-the-data-lakehouse" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Technologies must constantly evolve—<a href="https://en.wikipedia.org/wiki/Web3" target="_blank" rel="noopener noreferrer">Web 3.0</a>, <a href="https://en.wikipedia.org/wiki/List_of_wireless_network_technologies" target="_blank" rel="noopener noreferrer">cellular tech</a>, <a href="https://en.wikipedia.org/wiki/Programming_language_generations" target="_blank" rel="noopener noreferrer">programming language generations</a>—based on emerging needs. Data lakehouses are no exception. This section explores the hierarchy of such needs for data lakehouse users. The most basic need is the “<strong>table format</strong>” functionality, the foundation for data lakehouses. Table format organizes the collection of files/objects into tables with snapshots, schema, and statistics tracking, enabling higher abstraction. Furthermore, table format dictates the organization of files within each snapshot, encoding deletes/updates and metadata about how the table changes over time. Table format also provides protocols for various readers and writers and table management processes to handle concurrent access and provide ACID transactions safely. In the last five years, leading data warehouse and cloud vendors have integrated their proprietary SQL warehouse stack with open table formats. While they mostly default to their closed table formats and the compute engines remain closed, this welcome move provides users an open alternative for their data.</p>
<p>However, the benefits of a format end there, and now a table format is just the tip of the iceberg. Users require an <a href="https://www.onehouse.ai/blog/open-table-formats-and-the-open-data-lakehouse-in-perspective" target="_blank" rel="noopener noreferrer">end-to-end open data lakehouse</a>, and modern data lakehouse features need a sophisticated layer of <em><strong>open-source software</strong></em> operating on data stored in open table formats. For example, Optimized writers can balance cost and performance by carefully managing file sizes using the statistics maintained in the table format or catalog syncing service that can make data in Hudi readily available to half a dozen catalogs open and closed out there. Hudi shines by providing a high-performance open table format as well as a comprehensive open-source software stack that can ingest, store, optimize and effectively self-manage a data lakehouse. This distinction between open formats and open software is often lost in translation inside the large vendor ecosystem in which Hudi operates. Still, it has been and remains a key consideration for Hudi’s <a href="https://hudi.apache.org/cn/powered-by">users</a> to avoid compute-lockin to any given data vendor. The Hudi streamer tool, e.g., powers hundreds of data lakes by ingesting data seamlessly from various sources at the convenience of a single command in a terminal.</p>
<div style="text-align:center;width:90%;height:auto"><img src="https://hudi.apache.org/assets/images/blog/dlms-hierarchy.png" alt="dlms hierarchy"></div>
<p>Moving forward with 1.0, the community has <a href="https://github.com/apache/hudi/pull/8679" target="_blank" rel="noopener noreferrer">debated</a> these key points and concluded that we need more open-source “<strong>software capabilities</strong>” that are directly comparable with DBMSes for two main reasons.</p>
<p><strong>Significantly expand the technical capabilities of a data lakehouse</strong>: Many design decisions in Hudi have been inspired by databases (see <a href="https://github.com/apache/hudi/blob/master/rfc/rfc-69/rfc-69.md#hudi-1x" target="_blank" rel="noopener noreferrer">here</a> for a layer-by-layer mapping) and have delivered significant benefits to the community. For example, Hudi’s indexing mechanisms deliver the fast update performance the project has come to be known for.  We want to generalize such features across writers and queries and introduce new capabilities like fast metastores for query planning, support for unstructured/multimodal data and caching mechanisms that can be deeply integrated into (at least) open-source query engines in the ecosystem. We also need concurrency control that works for lakehouse workloads instead of employing techniques applicable to OLTP databases at the surface level.</p>
<p><strong>We also need a database-like experience</strong>: We originally designed Hudi as a software library that can be embedded into different query/processing engines for reading/writing/managing tables. This model has been a great success within the existing data ecosystem, which is familiar with scheduling jobs and employing multiple engines for ETL and interactive queries. However, for a new user wanting to explore data lakehouses, there is no piece of software to easily install and explore all functionality packaged coherently. Such data lakehouse functionality packaged and delivered like a typical database system unlocks new use cases. For example, with such a system, we could bring HTAP capabilities to the data lakehouses on faster cloud storage/row-oriented formats, finally making it a low-latency data serving layer.</p>
<p>If combined, we would gain a powerful database built on top of the data lake(house) architecture—a <em><strong>data</strong></em> <em><strong>lakehouse</strong></em> <em><strong>management</strong></em> <em><strong>system</strong></em> <em><strong>(DLMS)</strong></em>—that we believe the industry needs.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-features-in-hudi-10">Key Features in Hudi 1.0<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#key-features-in-hudi-10" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>In Hudi 1.0, we’ve delivered a significant expansion of data lakehouse technical capabilities discussed above inside Hudi’s <a href="https://en.wikipedia.org/wiki/Database_engine" target="_blank" rel="noopener noreferrer">storage engine</a> layer.  Storage engines (a.k.a database engines) are standard database components that sit on top of the storage/file/table format and are wrapped by the DBMS layer above, handling the core read/write/management functionality. In the figure below, we map the Hudi components with the seminal <a href="https://dsf.berkeley.edu/papers/fntdb07-architecture.pdf" target="_blank" rel="noopener noreferrer">Architecture of a Database System</a> paper (see page 4) to illustrate the standard layering discussed. If the layering is implemented correctly, we can deliver the benefits of the storage engine to even other table formats, which may lack such fully-developed open-source software for table management or achieving high performance, via interop standards defined in projects like <a href="https://xtable.apache.org/" target="_blank" rel="noopener noreferrer">Apache XTable (Incubating)</a>.</p>
<div style="text-align:center;width:80%;height:auto"><img src="https://hudi.apache.org/assets/images/hudi-stack-1-x.png" alt="Hudi DB Architecture"><p align="center">Figure: Apache Hudi Database Architecture</p></div>
<p>Regarding full-fledged DLMS functionality, the closest experience Hudi 1.0 offers is through Apache Spark. Users can deploy a Spark server (or Spark Connect) with Hudi 1.0 installed, submit SQL/jobs, orchestrate table services via SQL commands, and enjoy new secondary index functionality to speed up queries like a DBMS. Subsequent releases in the 1.x release line and beyond will continuously add new features and improve this experience.</p>
<p>In the following sections, let’s dive into what makes Hudi 1.0 a standout release.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="new-time-and-timeline">New Time and Timeline<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#new-time-and-timeline" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>For the familiar user, time is a key concept in Hudi. Hudi’s original notion of time was instantaneous, i.e., actions that modify the table appear to take effect at a given instant. This was limiting when designing features like non-blocking concurrency control across writers, which needs to reason about actions more as an “interval” to detect other conflicting actions. Every action on the Hudi timeline now gets a <em>requested</em> and a <em>completion</em> time; Thus, the timeline layout version has bumped up in the 1.0 release. Furthermore, to ease the understanding and bring consistency around time generation for users and implementors, we have formalized the adoption of <a href="https://hudi.apache.org/cn/docs/timeline#truetime-generation">TrueTime</a> semantics. The default implementation assures forward-moving clocks even with distributed processes, assuming a maximum tolerable clock skew similar to <a href="https://cockroachlabs.com/blog/living-without-atomic-clocks/" target="_blank" rel="noopener noreferrer">OLTP/NoSQL</a> stores adopting TrueTime.</p>
<div style="text-align:center"><img src="https://hudi.apache.org/assets/images/hudi-timeline-actions.png" alt="Timeline actions"><p align="center">Figure: Showing actions in Hudi 1.0 modeled as an interval of two instants: requested and completed</p></div>
<p>Hudi tables are frequently updated, and users also want to retain a more extended action history associated with the table. Before Hudi 1.0, the older action history in a table was archived for audit access. But, due to the lack of support for cloud storage appends, access might become cumbersome due to tons of small files. In Hudi 1.0, we have redesigned the timeline as an <a href="https://en.wikipedia.org/wiki/Log-structured_merge-tree" target="_blank" rel="noopener noreferrer">LSM tree</a>, which is widely adopted for cases where good write performance on temporal data is desired.</p>
<p>In the Hudi 1.0 release, the <a href="https://hudi.apache.org/cn/docs/timeline#lsm-timeline-history">LSM timeline</a> is heavily used in the query planning to map requested and completion times across Apache Spark, Apache Flink and Apache Hive. Future releases plan to leverage this to unify the timeline's active and history components, providing infinite retention of table history. Micro benchmarks show that the LSM timeline can be pretty efficient, even committing every <em><strong>30 seconds for 10 years with about 10M instants</strong></em>, further cementing Hudi’s table format as the most suited for frequently written tables.</p>
<table><thead><tr><th style="text-align:left">Number of actions</th><th style="text-align:left">Instant Batch Size</th><th style="text-align:left">Read cost (just times)</th><th style="text-align:left">Read cost (along with action metadata)</th><th style="text-align:left">Total file size</th></tr></thead><tbody><tr><td style="text-align:left">10000</td><td style="text-align:left">10</td><td style="text-align:left">32ms</td><td style="text-align:left">150ms</td><td style="text-align:left">8.39MB</td></tr><tr><td style="text-align:left">20000</td><td style="text-align:left">10</td><td style="text-align:left">51ms</td><td style="text-align:left">188ms</td><td style="text-align:left">16.8MB</td></tr><tr><td style="text-align:left">10000000</td><td style="text-align:left">1000</td><td style="text-align:left">3400ms</td><td style="text-align:left">162s</td><td style="text-align:left">8.4GB</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="secondary-indexing-for-faster-lookups">Secondary Indexing for Faster Lookups<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#secondary-indexing-for-faster-lookups" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Indexes are core to Hudi’s design, so much so that even the first pre-open-source version of Hudi shipped with <a href="https://hudi.apache.org/cn/docs/indexes#additional-writer-side-indexes">indexes</a> to speed up writes. However, these indexes were limited to the writer's side, except for record indexes in 0.14+ above, which were also integrated with Spark SQL queries. Hudi 1.0 generalizes indexes closer to the indexing functionality found in relational databases, supporting indexes on any secondary column across both writer and readers. Hudi 1.0 also supports near-standard <a href="https://hudi.apache.org/cn/docs/sql_ddl#create-index">SQL syntax</a> for creating/dropping indexes on different columns via Spark SQL, along with an asynchronous indexing table service to build indexes without interrupting the writers.</p>
<div style="text-align:center;padding-left:10%;width:70%;height:auto"><img src="https://hudi.apache.org/assets/images/hudi-stack-indexes.png" alt="Indexes"><p align="center">Figure: the indexing subsystem in Hudi 1.0, showing different types of indexes</p></div>
<p>With secondary indexes, queries and DMLs scan a much-reduced amount of files from cloud storage, dramatically reducing costs (e.g., on engines like AWS Athena, which price by data scanned) and improving query performance for queries with low to even moderate amount of selectivity. On a benchmark of a query on <em>web_sales</em> table (from <em><strong>10 TB tpc-ds dataset</strong></em>), with file groups - 286,603, total records - 7,198,162,544 and cardinality of secondary index column in the ~ 1:150 ranges, we see a remarkable <em><strong>~95% decrease in latency</strong></em>.</p>
<table><thead><tr><th style="text-align:left">Run 1</th><th style="text-align:left">Total Query Latency w/o indexing skipping (secs)</th><th style="text-align:left">Total Query Latency with secondary index skipping (secs)</th><th style="text-align:left">% decrease</th></tr></thead><tbody><tr><td style="text-align:left">1</td><td style="text-align:left">252</td><td style="text-align:left">31</td><td style="text-align:left">~88%</td></tr><tr><td style="text-align:left">2</td><td style="text-align:left">214</td><td style="text-align:left">10</td><td style="text-align:left">~95%</td></tr><tr><td style="text-align:left">3</td><td style="text-align:left">204</td><td style="text-align:left">9</td><td style="text-align:left">~95%</td></tr></tbody></table>
<p>In Hudi 1.0, secondary indexes are only supported for Apache Spark, with planned support for other engines in Hudi 1.1, starting with Flink, Presto and Trino.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bloom-filter-indexes">Bloom Filter indexes<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#bloom-filter-indexes" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Bloom filter indexes have existed on the Hudi writers for a long time. It is one of the most performant and versatile indexes users prefer for “needle-in-a-haystack” deletes/updates or de-duplication. The index works by storing special footers in base files around min/max key ranges and a dynamic bloom filter that adapts to the file size and can automatically handle partitioning/skew on the writer's path. Hudi 1.0 introduces a newer kind of bloom filter index for Spark SQL while retaining the writer-side index as-is. The new index stores bloom filters in the Hudi metadata table and other secondary/record indexes for scalable access, even for huge tables, since the index is stored in fewer files compared to being stored alongside data files. It can be created using standard <a href="https://hudi.apache.org/cn/docs/sql_ddl#create-bloom-filter-index">SQL syntax</a>, as shown below. Subsequent queries on the indexed columns will use the bloom filters to speed up queries.</p>
<div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)">-- Create a bloom filter index on the driver column of the table `hudi_table`</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INDEX</span><span class="token plain"> idx_bloom_driver </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">ON</span><span class="token plain"> hudi_indexed_table </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">USING</span><span class="token plain"> bloom_filters</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">driver</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">-- Create a bloom filter index on the column derived from expression `lower(rider)` of the table `hudi_table`</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INDEX</span><span class="token plain"> idx_bloom_rider </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">ON</span><span class="token plain"> hudi_indexed_table </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">USING</span><span class="token plain"> bloom_filters</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">rider</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> OPTIONS</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">expr</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">'lower'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>In future releases of Hudi, we aim to fully integrate the benefits of the older writer-side index into the new bloom index. Nonetheless, this demonstrates the adaptability of Hudi’s indexing system to handle different types of indexes on the table.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="partitioning-replaced-by-expression-indexes">Partitioning replaced by Expression Indexes<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#partitioning-replaced-by-expression-indexes" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>An astute reader may have noticed above that the indexing is supported on a function/expression on a column. Hudi 1.0 introduces expression indexes similar to <a href="https://www.postgresql.org/docs/current/indexes-expressional.html" target="_blank" rel="noopener noreferrer">Postgres</a> to generalize a two-decade-old relic in the data lake ecosystem - partitioning! At a high level, partitioning on the data lake divides the table into folders based on a column or a mapping function (partitioning function). When queries or operations are performed against the table, they can efficiently skip entire partitions (folders), reducing the amount of metadata and data involved. This is very effective since data lake tables span 100s of thousands of files. But, as simple as it sounds, this is one of the <a href="https://www.onehouse.ai/blog/knowing-your-data-partitioning-vices-on-the-data-lakehouse" target="_blank" rel="noopener noreferrer">most common pitfalls</a> around performance on the data lake, where new users use it like an index by partitioning based on a high cardinality column, resulting in lots of storage partitions/tiny files and abysmal write/query performance for no good reason. Further, tying storage organization to partitioning makes it inflexible to changes.</p>
<div style="text-align:center"><img src="https://hudi.apache.org/assets/images/expression-index-date-partitioning.png" alt="Timeline actions"><p align="center">Figure: Shows index on a date expression when a different column physically partitions data</p></div>
<p>Hudi 1.0 treats partitions as a <a href="https://hudi.apache.org/cn/docs/sql_queries#query-using-column-stats-expression-index">coarse-grained index</a> on a column value or an expression of a column, as they should have been. To support the efficiency of skipping entire storage paths/folders, Hudi 1.0 introduces partition stats indexes that aggregate these statistics on the storage partition path level, in addition to doing so at the file level. Now, users can create different types of indexes on columns to achieve the effects of partitioning in a streamlined fashion using fewer concepts to achieve the same results. Along with support for other 1.x features, partition stats and expression indexes support will be extended to other engines like Presto, Trino, Apache Doris, and Starrocks with the 1.1 release.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="efficient-partial-updates">Efficient Partial Updates<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#efficient-partial-updates" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Managing large-scale datasets often involves making fine-grained changes to records. Hudi has long supported <a href="https://hudi.apache.org/cn/docs/0.15.0/record_payload#partialupdateavropayload">partial updates</a> to records via the record payload interface. However, this usually comes at the cost of sacrificing engine-native performance by moving away from specific objects used by engines to represent rows. As users have embraced Hudi for incremental SQL pipelines on top of dbt/Spark or Flink Dynamic Tables, there was a rise in interest in making this much more straightforward and mainstream. Hudi 1.0 introduces first-class support for <strong>partial updates</strong> at the log format level, enabling <em>MERGE INTO</em> SQL statements to modify only the changed fields of a record instead of rewriting/reprocessing the entire row.</p>
<p>Partial updates improve query and write performance simultaneously by reducing write amplification for writes and the amount of data read by Merge-on-Read snapshot queries. It also achieves much better storage utilization due to fewer bytes stored and improved compute efficiency over existing partial update support by retaining vectorized engine-native processing. Using the 1TB Brooklyn benchmark for write performance, we observe about <strong>2.6x</strong> improvement in Merge-on-Read query performance due to an <strong>85%</strong> reduction in write amplification. For random write workloads, the gains can be much more pronounced. Below shows a second benchmark for partial updates, 1TB MOR table, 1000 partitions, 80% random updates. 3/100 columns randomly updated.</p>
<table><thead><tr><th style="text-align:left"></th><th style="text-align:left">Full Record Update</th><th style="text-align:left">Partial Update</th><th style="text-align:left">Gains</th></tr></thead><tbody><tr><td style="text-align:left"><strong>Update latency (s)</strong></td><td style="text-align:left">2072</td><td style="text-align:left">1429</td><td style="text-align:left">1.4x</td></tr><tr><td style="text-align:left"><strong>Bytes written (GB)</strong></td><td style="text-align:left">891.7</td><td style="text-align:left">12.7</td><td style="text-align:left">70.2x</td></tr><tr><td style="text-align:left"><strong>Query latency (s)</strong></td><td style="text-align:left">164</td><td style="text-align:left">29</td><td style="text-align:left">5.7x</td></tr></tbody></table>
<p>This also lays the foundation for managing unstructured and multimodal data inside a Hudi table and supporting <a href="https://github.com/apache/hudi/pull/11733" target="_blank" rel="noopener noreferrer">wide tables</a> efficiently for machine learning use cases.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="merge-modes-and-custom-mergers">Merge Modes and Custom Mergers<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#merge-modes-and-custom-mergers" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>One of the most unique capabilities Hudi provides is how it helps process streaming data. Specifically, Hudi has, since the very beginning, supported merging records pre-write (to reduce write amplification), during write (against an existing record in storage with the same record key) and reads (for MoR snapshot queries), using a <em>precombine</em> or <em>ordering</em> field. This helps implement <a href="https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/" target="_blank" rel="noopener noreferrer">event time processing</a> semantics, widely supported by stream processing systems, on data lakehouse storage. This helps integrate late-arriving data into Hudi tables without causing weird movement of record state back in time. For example, if an older database CDC record arrives late and gets committed as the new value, the state of the record would be incorrect even though the writes to the table themselves were serialized in some order.</p>
<p><img decoding="async" loading="lazy" alt="event time ordering" src="https://hudi.apache.org/cn/assets/images/event-time-ordering-merge-mode-c8164e035840388bf4290fa81ac6262a.png" width="1360" height="490" class="img_ev3q">
</p><p align="center">Figure: Shows EVENT_TIME_ORDERING where merging reconciles state based on the highest event_time</p><p></p>
<p>Prior Hudi versions supported this functionality through the record payload interface with built-in support for a pre-combine field on the default payloads. Hudi 1.0 makes these two styles of processing and merging changes first class by introducing <a href="https://hudi.apache.org/cn/docs/record_merger">merge modes</a> within Hudi.</p>
<table><thead><tr><th style="text-align:left">Merge Mode</th><th style="text-align:left">What does it do?</th></tr></thead><tbody><tr><td style="text-align:left">COMMIT_TIME_ORDERING</td><td style="text-align:left">Picks record with highest completion time/instant as final merge result  i.e., standard relational semantics or arrival time processing</td></tr><tr><td style="text-align:left">EVENT_TIME_ORDERING</td><td style="text-align:left">Default (for now, to ease migration).Picks record with the highest value for a user-specified ordering/precombine field as the final merge result.</td></tr><tr><td style="text-align:left">CUSTOM</td><td style="text-align:left">Uses a user-provided RecordMerger implementation to produce final merge result (similar to stream processing processor APIs)</td></tr></tbody></table>
<p>Like partial update support, the new <em>RecordMerger</em> API provides a more efficient engine-native alternative to the older RecordPayload interface through native objects and vectorized processing on EVENT_TIME_ORDERING merge modes. In future versions, we intend to change the default to COMMIT_TIME_ORDERING to provide simple, out-of-the-box relational table semantics.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="non-blocking-concurrency-control-for-streaming-writes">Non-Blocking Concurrency Control for Streaming Writes<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#non-blocking-concurrency-control-for-streaming-writes" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>We have expressed dissatisfaction with the optimistic concurrency control approaches employed on the data lakehouse since they appear to paint the problem with a broad brush without paying attention to the nuances of the lakehouse workloads. Specifically, contention is much more common in data lakehouses, even for Hudi, the only data lakehouse storage project capable of asynchronously compacting delta updates without failing or causing retries on the writer. Ultimately, data lakehouses are high-throughput systems, and failing concurrent writers to handle contention can waste expensive compute clusters. Streaming and high-frequency writes often require fine-grained concurrency control to prevent bottlenecks.</p>
<p>Hudi 1.0 introduces a new <strong>non-blocking concurrency control (NBCC)</strong> designed explicitly for data lakehouse workloads, using years of experience gained supporting some of the largest data lakes on the planet in the Hudi community. NBCC enables simultaneous writing from multiple writers and compaction of the same record without blocking any involved processes. This is achieved by simply lightweight distributed locks and TrueTime semantics discussed above. (see <a href="https://github.com/apache/hudi/blob/master/rfc/rfc-66/rfc-66.md" target="_blank" rel="noopener noreferrer">RFC-66</a> for more)</p>
<div style="text-align:center"><img src="https://hudi.apache.org/assets/images/nbcc_partial_updates.gif" alt="NBCC"><p align="center">Figure: Two streaming jobs in action writing to the same records concurrently on different columns.</p></div>
<p>NBCC operates with streaming semantics, tying together concepts from previous sections. Data necessary to compute table updates are emitted from an upstream source, and changes and partial updates can be merged in any of the merge modes above. For example, in the figure above, two independent Flink jobs enrich different table columns in parallel, a pervasive pattern seen in stream processing use cases. Check out this <a href="https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control" target="_blank" rel="noopener noreferrer">blog</a> for a full demo. We also expect to support NBCC across other compute engines in future releases.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="backwards-compatible-writing">Backwards Compatible Writing<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#backwards-compatible-writing" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>If you are wondering: “All of this sounds cool, but how do I upgrade?” we have put a lot of thought into making that seamless. Hudi has always supported backward-compatible reads to older table versions. Table versions are stored in table properties unrelated to the software binary version. The supported way of upgrading has been to first migrate readers/query engines to new software binary versions and then upgrade the writers, which will auto-upgrade the table if there is a table version change between the old and new software binary versions. Upon community feedback, users expressed the need to be able to do upgrades on the writers without waiting on the reader side upgrades and reduce any additional coordination necessary within different teams.</p>
<p><img decoding="async" loading="lazy" alt="Indexes" src="https://hudi.apache.org/cn/assets/images/backwards-compat-writing-6299b055646e2577964069b755ee1f3d.png" width="1481" height="825" class="img_ev3q">
</p><p align="center">Figure: 4-step process for painless rolling upgrades to Hudi 1.0</p><p></p>
<p>Hudi 1.0 introduces backward-compatible writing to achieve this in 4 steps, as described above. Hudi 1.0 also automatically handles any checkpoint translation necessary as we switch to completion time-based processing semantics for incremental and CDC queries. The Hudi metadata table has to be temporarily disabled during this upgrade process but can be turned on once the upgrade is completed successfully. Please read the <a href="https://hudi.apache.org/cn/releases/release-1.0.0">release notes</a> carefully to plan your migration.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="whats-next">What’s Next?<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#whats-next" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Hudi 1.0 is a testament to the power of open-source collaboration. This release embodies the contributions of 60+ developers, maintainers, and users who have actively shaped its roadmap. We sincerely thank the Apache Hudi community for their passion, feedback, and unwavering support.</p>
<p>The release of Hudi 1.0 is just the beginning. Our current <a href="https://hudi.apache.org/cn/roadmap">roadmap</a> includes exciting developments across the following planned releases:</p>
<ul>
<li><strong>1.0.1</strong>: First bug fix, patch release on top of 1.0, which hardens the functionality above and makes it easier. We intend to publish additional patch releases to aid migration to 1.0 as the bridge release for the community from 0.x.</li>
<li><strong>1.1</strong>:  Faster writer code path rewrite, new indexes like bitmap/vector search, granular record-level change encoding, Hudi storage engine APIs, abstractions for cross-format interop.</li>
<li><strong>1.2</strong>: Multi-table transactions, platform services for reverse streaming from Hudi etc., Multi-modal data + indexing, NBCC clustering</li>
<li><strong>2.0</strong>: Server components for DLMS, caching and metaserver functionality.</li>
</ul>
<p>Hudi releases are drafted collaboratively by the community. If you don’t see something you like here, please help shape the roadmap together.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="get-started-with-apache-hudi-10">Get Started with Apache Hudi 1.0<a href="https://hudi.apache.org/cn/blog/2024/12/16/announcing-hudi-1-0-0#get-started-with-apache-hudi-10" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Are you ready to experience the future of data lakehouses? Here’s how you can dive into Hudi 1.0:</p>
<ul>
<li>Documentation: Explore Hudi’s <a href="https://hudi.apache.org/cn/docs/overview">Documentation</a> and learn the <a href="https://hudi.apache.org/cn/docs/hudi_stack">concepts</a>.</li>
<li>Quickstart Guide: Follow the <a href="https://hudi.apache.org/cn/docs/quick-start-guide">Quickstart Guide</a> to set up your first Hudi project.</li>
<li>Upgrading from a previous version?  Follow the <a href="https://hudi.apache.org/cn/releases/release-1.0.0#migration-guide">migration guide</a> and contact the Hudi OSS community for help.</li>
<li>Join the Community: Participate in discussions on the <a href="https://hudi.apache.org/community/get-involved/" target="_blank" rel="noopener noreferrer">Hudi Mailing List</a>, <a href="https://join.slack.com/t/apache-hudi/shared_invite/zt-2ggm1fub8-_yt4Reu9djwqqVRFC7X49g" target="_blank" rel="noopener noreferrer">Slack</a> and <a href="https://github.com/apache/hudi/issues" target="_blank" rel="noopener noreferrer">GitHub</a>.</li>
<li>Follow us on social media: <a href="https://www.linkedin.com/company/apache-hudi/?viewAsMember=true" target="_blank" rel="noopener noreferrer">Linkedin</a>, <a href="https://twitter.com/ApacheHudi" target="_blank" rel="noopener noreferrer">X/Twitter</a>.</li>
</ul>
<p>We can’t wait to see what you build with Apache Hudi 1.0. Let’s work together to shape the future of data lakehouses!</p>
<p>Crafted with passion for the Apache Hudi community.</p>]]></content>
        <author>
            <name>Vinoth Chandar</name>
        </author>
        <category label="timeline" term="timeline"/>
        <category label="design" term="design"/>
        <category label="release" term="release"/>
        <category label="streaming ingestion" term="streaming ingestion"/>
        <category label="multi-writer" term="multi-writer"/>
        <category label="concurrency-control" term="concurrency-control"/>
        <category label="blog" term="blog"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing Hudi's Non-blocking Concurrency Control for streaming, high-frequency writes]]></title>
        <id>https://hudi.apache.org/cn/blog/2024/12/06/non-blocking-concurrency-control</id>
        <link href="https://hudi.apache.org/cn/blog/2024/12/06/non-blocking-concurrency-control"/>
        <updated>2024-12-06T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Introduction]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="https://hudi.apache.org/cn/blog/2024/12/06/non-blocking-concurrency-control#introduction" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>In streaming ingestion scenarios, there are plenty of use cases that require concurrent ingestion from multiple streaming sources.
The user can union all the upstream source inputs into one downstream table to collect the records for unified access across federated queries.
Another very common scenario is multiple stream sources joined together to supplement dimensions of the records to build a wide-dimension table where each source
stream is taking records with partial table schema fields. Common and strong demand for multi-stream concurrent ingestion has always been there.
The Hudi community has collected so many feedbacks from users ever since the day Hudi supported streaming ingestion and processing.</p>
<p>Starting from <a href="https://hudi.apache.org/releases/release-1.0.0" target="_blank" rel="noopener noreferrer">Hudi 1.0.0</a>, we are thrilled to announce a new general-purpose
concurrency model for Apache Hudi - the Non-blocking Concurrency Control (NBCC)- aimed at the stream processing or high-contention/frequent writing scenarios.
In contrast to <a href="https://hudi.apache.org/cn/blog/2021/12/16/lakehouse-concurrency-control-are-we-too-optimistic/">Optimistic Concurrency Control</a>, where writers abort the transaction
if there is a hint of contention, this innovation allows multiple streaming writes to the same Hudi table without any overhead of conflict resolution, while
keeping the semantics of <a href="https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/" target="_blank" rel="noopener noreferrer">event-time ordering</a> found in streaming systems, along with
asynchronous table service such as compaction, archiving and cleaning.</p>
<p>NBCC works seamlessly without any new infrastructure or operational overhead. In the subsequent sections of this blog, we will give a brief introduction to Hudi's internals
about the data file layout and TrueTime semantics for time generation, a pre-requisite for discussing NBCC. Following that, we will delve into the design and workflows of NBCC,
and then a simple SQL demo to show the NBCC related config options. The blog will conclude with insights into future work for NBCC.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="older-design">Older Design<a href="https://hudi.apache.org/cn/blog/2024/12/06/non-blocking-concurrency-control#older-design" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>It's important to understand the Hudi <a href="https://hudi.apache.org/cn/docs/next/storage_layouts">storage layout</a> and it evolves/manages data versions. In older release before 1.0.0,
Hudi organizes the data files with units as <code>FileGroup</code>. Each file group contains multiple <code>FileSlice</code>s. Every compaction on this file group generates a new file slice.
Each file slice may comprise an optional base file(columnar file format like Apache Parquet or ORC) and multiple log files(row file format in Apache Avro or Parquet).</p>
<img src="https://hudi.apache.org/assets/images/blog/non-blocking-concurrency-control/legacy_file_layout.png" alt="Legacy file layout" width="800" align="middle">
<p>The timestamp in the base file name is the instant time of the compaction that writes it, it is also called as "requested instant time" in Hudi's notion.
The timestamp in the log file name is the same timestamp as the current file slice base instant time. Data files with the same instant time belong to one file slice.
In effect, a file group represented a linear ordered sequence of base files (checkpoints) followed by logs files (deltas), followed by base files (checkpoints).</p>
<p>The instant time naming convention in log files becomes a hash limitation in concurrency mode. Each log file contains incremental changes from
multiple commits. Each writer needs to query the file layout to get the base instant time and figure out the full file name before flushing the records.
A more severe problem is the base instant time can be variable with the async compaction pushing forward. In order to make the base instant time deterministic for the log writers, Hudi
forces the schedule sequence between a write commit and compaction scheduling: a compaction can be scheduled only if there is no ongoing ingestion into the Hudi table. Without this, a log file
can be written with a wrong base instant time which could introduce data loss. This means a compaction scheduling could block all the writers in concurrency mode.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="nbcc-design">NBCC Design<a href="https://hudi.apache.org/cn/blog/2024/12/06/non-blocking-concurrency-control#nbcc-design" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>In order to resolve these pains, since 1.0.0, Hudi introduces a new storage layout based on both requested and completion times of actions, viewing them as an interval.
Each commit in 1.x Hudi has two <a href="https://hudi.apache.org/cn/docs/next/timeline">important notions of time</a>: instant time(or requested time) and completion time.
All the generated timestamp are globally monotonically increasing. Instead of putting the base instant time in the log file name, Hudi now just uses the requested instant time
of the write. During file slicing, Hudi queries the completion time for each log file with the instant time, and we have a new rule for file slicing:</p>
<p><em>A log file belongs to the file slice with the maximum base requested time smaller than(or equals with) it's completion time.</em>[^1]</p>
<img src="https://hudi.apache.org/assets/images/blog/non-blocking-concurrency-control/new_file_layout.png" alt="New file layout" width="800" align="middle">
<p>With the flexibility of the new file layout, the overhead of querying base instant time is eliminated for log writers and a compaction can be scheduled anywhere with any instant time.
See <a href="https://github.com/apache/hudi/blob/master/rfc/rfc-66/rfc-66.md" target="_blank" rel="noopener noreferrer">RFC-66</a> for more.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="true-time-api">True Time API<a href="https://hudi.apache.org/cn/blog/2024/12/06/non-blocking-concurrency-control#true-time-api" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>In order to ensure the monotonicity of timestamp generation, Hudi introduces the "<a href="https://hudi.apache.org/cn/docs/next/timeline#timeline-components">TrueTime API</a>" since 1.x release.
Basically there are two ways to make the time generation monotonically increasing, inline with TrueTime semantics:</p>
<ul>
<li>A global lock to guard the time generation with mutex, along with a wait for an estimated max allowed clock skew on distributed hosts;</li>
<li>Globally synchronized time generation service, e.g. Google Spanner Time Service, the service itself can ensure the monotonicity.</li>
</ul>
<p>Hudi now implements the "TrueTime" semantics with the first solution, a configurable max waiting time is supported.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="lsm-timeline">LSM timeline<a href="https://hudi.apache.org/cn/blog/2024/12/06/non-blocking-concurrency-control#lsm-timeline" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>The new file layout requires efficient queries from instant time to get the completion time. Hudi re-implements the archived timeline since 1.x, the
new archived timeline data files are organized as <a href="https://hudi.apache.org/cn/docs/next/timeline#lsm-timeline-history">an LSM tree</a> to support fast time range filtering queries with instant time data-skipping on it.</p>
<img src="https://hudi.apache.org/assets/images/blog/non-blocking-concurrency-control/lsm_archive_timeline.png" alt="LSM archive timeline" align="middle">
<p>With the powerful new file layout, it is quite straight-forward to implement non-blocking concurrency control. The function is implemented with the simple bucket index on MOR table for Flink.
The bucket index ensures fixed record key to file group mappings for multiple workloads. The log writer writes the records into avro logs and the compaction table service would take care of
the conflict resolution. Because each log file name contains the instant time and each record contains the event time ordering field, Hudi reader can merge the records either
with natural order(processing time sequence) or event time order.</p>
<p>The concurrency mode should be configured as <code>NON_BLOCKING_CONCURRENCY_CONTROL</code>, you can enable the table services on one job and disable it for the others.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="flink-sql-demo">Flink SQL demo<a href="https://hudi.apache.org/cn/blog/2024/12/06/non-blocking-concurrency-control#flink-sql-demo" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Here is a demo to show 2 pipelines that ingest into the same downstream table, the two sink table views share the same table path.</p>
<div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)">-- NB-CC demo</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">-- The source table</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">TABLE</span><span class="token plain"> sourceT </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  uuid </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">varchar</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">20</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  name </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">varchar</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">10</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  age </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">int</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  ts </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">timestamp</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">3</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">partition</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'par1'</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">WITH</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'connector'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'datagen'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'rows-per-second'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'200'</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">-- table view for writer1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">create</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">table</span><span class="token plain"> t1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  uuid </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">varchar</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">20</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  name </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">varchar</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">10</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  age </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">int</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  ts </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">timestamp</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">3</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">partition</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">varchar</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">20</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">with</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'connector'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'hudi'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'path'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'/Users/chenyuzhao/workspace/hudi-demo/t1'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'table.type'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'MERGE_ON_READ'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'index.type'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'BUCKET'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'hoodie.write.concurrency.mode'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'NON_BLOCKING_CONCURRENCY_CONTROL'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'write.tasks'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'2'</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">insert</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">into</span><span class="token plain"> t1</span><span class="token comment" style="color:rgb(98, 114, 164)">/*+options('metadata.enabled'='true')*/</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">select</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> sourceT</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">-- table view for writer2</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">-- compaction and cleaning are disabled because writer1 has taken care of it.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">create</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">table</span><span class="token plain"> t1_2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  uuid </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">varchar</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">20</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  name </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">varchar</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">10</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  age </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">int</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  ts </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">timestamp</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">3</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">partition</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">varchar</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">20</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">with</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'connector'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'hudi'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'path'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'/Users/chenyuzhao/workspace/hudi-demo/t1'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'table.type'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'MERGE_ON_READ'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'index.type'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'BUCKET'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'hoodie.write.concurrency.mode'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'NON_BLOCKING_CONCURRENCY_CONTROL'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'write.tasks'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'2'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'compaction.schedule.enabled'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'false'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'compaction.async.enabled'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'false'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'clean.async.enabled'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'false'</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">-- executes the ingestion workloads</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">insert</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">into</span><span class="token plain"> t1 </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">select</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> sourceT</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">insert</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">into</span><span class="token plain"> t1_2 </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">select</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> sourceT</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="future-roadmap">Future Roadmap<a href="https://hudi.apache.org/cn/blog/2024/12/06/non-blocking-concurrency-control#future-roadmap" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>While non-blocking concurrency control is a very powerful feature for streaming users, it is a general solution for multiple writer conflict resolution,
here are some plans that improve the Hudi core features:</p>
<ul>
<li>NBCC support for metadata table</li>
<li>NBCC for clustering</li>
<li>NBCC for other index type</li>
</ul>
<hr>
<p>[^1] <a href="https://github.com/apache/hudi/blob/master/rfc/rfc-66/rfc-66.md" target="_blank" rel="noopener noreferrer">RFC-66</a> well-explained the completion time based file slicing with a pseudocode.</p>]]></content>
        <author>
            <name>Danny Chan</name>
        </author>
        <category label="design" term="design"/>
        <category label="streaming ingestion" term="streaming ingestion"/>
        <category label="multi-writer" term="multi-writer"/>
        <category label="concurrency-control" term="concurrency-control"/>
        <category label="blog" term="blog"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Use open table format libraries on AWS Glue 5.0 for Apache Spark]]></title>
        <id>https://hudi.apache.org/cn/blog/2024/12/04/use-open-table-format-libraries-on-aws-glue-5-0-for-apache-spark</id>
        <link href="https://hudi.apache.org/cn/blog/2024/12/04/use-open-table-format-libraries-on-aws-glue-5-0-for-apache-spark"/>
        <updated>2024-12-04T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://aws.amazon.com/blogs/big-data/use-open-table-format-libraries-on-aws-glue-5-0-for-apache-spark/">here</a></span>]]></content>
        <author>
            <name>Sotaro Hikita and  Noritaka Sekiyama</name>
        </author>
        <category label="announcement" term="announcement"/>
        <category label="blog" term="blog"/>
        <category label="apache hudi" term="apache hudi"/>
        <category label="aws glue" term="aws glue"/>
        <category label="apache spark" term="apache spark"/>
        <category label="table format" term="table format"/>
        <category label="amazon" term="amazon"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Apache Iceberg vs Hudi: Key Features, Performance & Use Cases]]></title>
        <id>https://hudi.apache.org/cn/blog/2024/12/03/apache-iceberg-vs-apache-hudi</id>
        <link href="https://hudi.apache.org/cn/blog/2024/12/03/apache-iceberg-vs-apache-hudi"/>
        <updated>2024-12-03T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Redirecting... please wait!!]]></summary>
        <content type="html"><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://estuary.dev/apache-iceberg-vs-apache-hudi/">here</a></span>]]></content>
        <author>
            <name>Dani Pálma</name>
        </author>
        <category label="blog" term="blog"/>
        <category label="apache hudi" term="apache hudi"/>
        <category label="apache iceberg" term="apache iceberg"/>
        <category label="comparison" term="comparison"/>
        <category label="estuary" term="estuary"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hudi’s Automatic File Sizing Delivers Unmatched Performance]]></title>
        <id>https://hudi.apache.org/cn/blog/2024/11/19/automated-small-file-handling</id>
        <link href="https://hudi.apache.org/cn/blog/2024/11/19/automated-small-file-handling"/>
        <updated>2024-11-19T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Introduction]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="https://hudi.apache.org/cn/blog/2024/11/19/automated-small-file-handling#introduction" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>In today’s data-driven world, managing large volumes of data efficiently is crucial. One of the standout features of Apache Hudi is its ability to handle small files during data writes, which significantly optimizes both performance and cost. In this post, we’ll explore how Hudi’s auto file sizing, powered by a unique bin packing algorithm, can transform your data processing workflows.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="understanding-small-file-challenges">Understanding Small File Challenges<a href="https://hudi.apache.org/cn/blog/2024/11/19/automated-small-file-handling#understanding-small-file-challenges" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>In big data environments, small files can pose a major challenge. Some major use-cases which can create lot of small files -</p>
<ul>
<li><strong>Streaming Workloads</strong> :
When data is ingested in micro-batches, as is common in streaming workloads, the resulting files tend to be small. This can lead to a significant number of small files, especially for high-throughput streaming applications.</li>
<li><strong>High-Cardinality Partitioning</strong> :
Excessive partitioning, particularly on columns with high cardinality, can create a large number of small files. This can be especially problematic when dealing with large datasets and complex data schemas.</li>
</ul>
<p>These small files can lead to several inefficiencies that can include increased metadata overhead, degraded read performance, and higher storage costs, particularly when using cloud storage solutions like Amazon S3.</p>
<ul>
<li><strong>Increased Metadata Overhead</strong> :
Metadata is data about data, including information such as file names, sizes, creation dates, and other attributes that help systems manage and locate files. Each file, no matter how small, requires metadata to be tracked and managed. In environments where numerous small files are created, the amount of metadata generated can skyrocket. For instance, if a dataset consists of thousands of tiny files, the system must maintain metadata for each of these files. This can overwhelm metadata management systems, leading to longer lookup times and increased latency when accessing files.</li>
<li><strong>Degraded Read Performance</strong> :
Reading data from storage typically involves input/output (I/O) operations, which can be costly in terms of time and resources. When files are small, the number of I/O operations increases, as each small file needs to be accessed individually. This scenario can create bottlenecks, particularly in analytical workloads where speed is critical. Querying a large number of small files may result in significant delays, as the system spends more time opening and reading each file than processing the data itself.</li>
<li><strong>Higher Cloud Costs</strong> :
Many cloud storage solutions, like Amazon S3, charge based on the total amount of data stored as well as the number of requests made. With numerous small files, not only does the total storage requirement increase, but the number of requests to access these files also grows. Each small file incurs additional costs due to the overhead associated with managing and accessing them. This can add up quickly, leading to unexpectedly high storage bills.</li>
<li><strong>High Query Load</strong> :
Multiple teams are querying these tables for various dashboards, ad-hoc analyses, and machine learning tasks. This leads to a high number of concurrent queries, including Spark jobs, which can significantly impact performance. All those queries/jobs will take a hit on both performance and cost.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="impact-of-small-file">Impact of Small File<a href="https://hudi.apache.org/cn/blog/2024/11/19/automated-small-file-handling#impact-of-small-file" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>To demonstrate the impact of small files, we conducted a benchmarking using AWS EMR.
Dataset Used - TPC-DS 1 TB dataset ( <a href="https://www.tpc.org/tpcds/" target="_blank" rel="noopener noreferrer">https://www.tpc.org/tpcds/</a> )
Cluster Configurations - 10 nodes (m5.4xlarge)
Spark Configurations - Executors: 10 (16 cores 32 GB memory)
Dataset Generation - We generated two types of datasets in parquet format</p>
<ul>
<li>Optimized File Sizes which had ~100 MB sized files</li>
<li>Small File Sizes which had ~5-10 MB sized files
Execution and Results</li>
<li>We executed 3 rounds of 99 standard TPC-DS queries on both datasets and measured the time taken by the queries.</li>
<li>The results indicated that queries executed on small files were, on average, 30% slower compared to those executed on optimized file sizes.</li>
</ul>
<p>The following chart illustrates the average runtimes for the 99 queries across each round.</p>
<p><img decoding="async" loading="lazy" alt="Impact of Small Files" src="https://hudi.apache.org/cn/assets/images/2024-11-19-automated-small-file-handling-benchmarks-5340e7e5e0e586c3803f6e06796b5daf.png" width="3188" height="1844" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-table-formats-solve-this-problem">How table formats solve this problem<a href="https://hudi.apache.org/cn/blog/2024/11/19/automated-small-file-handling#how-table-formats-solve-this-problem" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>When it comes to managing small files in table formats, there are two primary strategies:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ingesting-data-as-is-and-optimizing-post-ingestion-"><strong>Ingesting Data As-Is and Optimizing Post-Ingestion</strong> :<a href="https://hudi.apache.org/cn/blog/2024/11/19/automated-small-file-handling#ingesting-data-as-is-and-optimizing-post-ingestion-" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>In this approach, data, including small files, is initially ingested without immediate processing. After ingestion, various technologies provide functionalities to merge these small files into larger, more efficient partitions:</p>
<ul>
<li>Hudi uses clustering to manage small files.</li>
<li>Delta Lake utilizes the OPTIMIZE command.</li>
<li>Iceberg offers the rewrite_data_files function.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="pros">Pros:<a href="https://hudi.apache.org/cn/blog/2024/11/19/automated-small-file-handling#pros" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<ul>
<li>Writing small files directly accelerates the ingestion process, enabling quick data availability—especially beneficial for real-time or near-real-time applications.</li>
<li>The initial write phase involves less data manipulation, as small files are simply appended. This streamlines workflows and eases the management of incoming data streams.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="cons">Cons:<a href="https://hudi.apache.org/cn/blog/2024/11/19/automated-small-file-handling#cons" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<ul>
<li>Until clustering or optimization is performed, small files may be exposed to readers, which can significantly slow down queries and potentially violate read SLAs.</li>
<li>Just like with read performance, exposing small files to readers can lead to a high number of cloud storage API calls, which can increase cloud costs significantly.</li>
<li>Managing table service jobs can become cumbersome. These jobs often can't run in parallel with ingestion tasks, leading to potential delays and resource contention.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="managing-small-files-during-ingestion-only-"><strong>Managing Small Files During Ingestion Only</strong> :<a href="https://hudi.apache.org/cn/blog/2024/11/19/automated-small-file-handling#managing-small-files-during-ingestion-only-" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Hudi offers a unique functionality that can handle small files during the ingestion only, ensuring that only larger files are stored in the table. This not only optimizes read performance but also significantly reduces storage costs.
By eliminating small files from the lake, Hudi addresses key challenges associated with data management, providing a streamlined solution that enhances both performance and cost efficiency.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-hudi-helps-in-small-file-handling-during-ingestion">How Hudi helps in small file handling during ingestion<a href="https://hudi.apache.org/cn/blog/2024/11/19/automated-small-file-handling#how-hudi-helps-in-small-file-handling-during-ingestion" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Hudi automatically manages file sizing during insert and upsert operations. It employs a bin packing algorithm to handle small files effectively. A bin packing algorithm is a technique used to optimize file storage by grouping files of varying sizes into fixed-size containers, often referred to as "bins." This strategy aims to minimize the number of bins required to store all files efficiently. When writing data, Hudi identifies file groups of small files and merges new data into the same  group, resulting in optimized file sizes.</p>
<p>The diagram above illustrates how Hudi employs a bin packing algorithm to manage small files while using default parameters: a small file limit of 100 MB and a maximum file size of 120 MB.</p>
<p><img decoding="async" loading="lazy" alt="  " src="https://hudi.apache.org/cn/assets/images/2024-11-19-automated-small-file-handling-process-676b9be484af36088162dfaf6a219a1f.png" width="1350" height="632" class="img_ev3q"></p>
<p>Initially, the table contains the following files: F1 (110 MB), F2 (60 MB), F3 (20 MB), and F4 (20 MB).
After processing a batch-1 of 150 MB, F2, F3, and F4 will all be classified as small files since they each fall below the 100 MB threshold. The first 60 MB will be allocated to F2, increasing its size to 120 MB. The remaining 90 MB will be assigned to F3, bringing its total to 110 MB.
After processing batch-2 of 150 MB, only F4 will be classified as a small file. F3, now at 110 MB, will not be considered a small file since it exceeds the 100 MB limit. Therefore, an additional 100 MB will be allocated to F4, increasing its size to 120 MB, while the remaining 50 MB will create a new file of 50 MB.
We can refer this blog for in-depth details of the functionality  - <a href="https://hudi.apache.org/blog/2021/03/01/hudi-file-sizing/" target="_blank" rel="noopener noreferrer">https://hudi.apache.org/blog/2021/03/01/hudi-file-sizing/</a></p>
<p>We use following configs to configure this -</p>
<ul>
<li>
<p><strong>hoodie.parquet.max.file.size (Default 128 MB)</strong>
This setting specifies the target size, in bytes, for Parquet files generated during Hudi write phases. The writer will attempt to create files that approach this target size. For example, if an existing file is 80 MB, the writer will allocate only 40 MB to that particular file group.</p>
</li>
<li>
<p><strong>hoodie.parquet.small.file.limit (Default 100 MB)</strong>
This setting defines the maximum file size for a data file to be classified as a small file. Files below this threshold are considered small files, prompting the system to allocate additional records to their respective file groups in subsequent write phases.</p>
</li>
<li>
<p><strong>hoodie.copyonwrite.record.size.estimate (Default 1024)</strong>
This setting represents the estimated average size of a record. If not explicitly specified, Hudi will dynamically compute this estimate based on commit metadata. Accurate record size estimation is essential for determining insert parallelism and efficiently bin-packing inserts into smaller files.</p>
</li>
<li>
<p><strong>hoodie.copyonwrite.insert.split.size (Default 500000)</strong>
This setting determines the number of records inserted into each partition or bucket during a write operation. The default value is based on the assumption of 100MB files with at least 1KB records, resulting in approximately 100,000 records per file. To accommodate potential variations, we overprovision to 500,000 records. As long as auto-tuning of splits is turned on, this only affects the first write, where there is no history to learn record sizes from.</p>
</li>
<li>
<p><strong>hoodie.merge.small.file.group.candidates.limit (Default1)</strong>
This setting specifies the maximum number of file groups whose base files meet the small-file limit that can be considered for appending records during an upsert operation. This parameter is applicable only to Merge-On-Read (MOR) tables.</p>
</li>
</ul>
<p>We can refer this blog to understand internal functionality how it works -
<a href="https://hudi.apache.org/blog/2021/03/01/hudi-file-sizing/#during-write-vs-after-write" target="_blank" rel="noopener noreferrer">https://hudi.apache.org/blog/2021/03/01/hudi-file-sizing/#during-write-vs-after-write</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://hudi.apache.org/cn/blog/2024/11/19/automated-small-file-handling#conclusion" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Hudi's innovative approach to managing small files during ingestion positions it as a compelling choice in the lakehouse landscape. By automatically merging small files at the time of ingestion, it optimizes storage costs and enhances read performance, and alleviates users from the operational burden of maintaining their tables in an optimized state.</p>
<p>Unleash the power of Apache Hudi for your big data challenges! Head over to <a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer">https://hudi.apache.org/</a> and dive into the quickstarts to get started. Want to learn more? Join our vibrant Hudi community! Attend the monthly Community Call or hop into the Apache Hudi Slack to ask questions and gain deeper insights.</p>]]></content>
        <author>
            <name>Aditya Goenka</name>
        </author>
        <category label="Data Lake" term="Data Lake"/>
        <category label="Apache Hudi" term="Apache Hudi"/>
    </entry>
</feed>