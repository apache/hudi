<!doctype html>
<html lang="cn" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.14">
<link rel="alternate" type="application/rss+xml" href="/cn/blog/rss.xml" title="Apache Hudi: User-Facing Analytics RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/cn/blog/atom.xml" title="Apache Hudi: User-Facing Analytics Atom Feed">
<link rel="alternate" type="application/json" href="/cn/blog/feed.json" title="Apache Hudi: User-Facing Analytics JSON Feed">
<link rel="search" type="application/opensearchdescription+xml" title="Apache Hudi" href="/cn/opensearch.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Comfortaa|Ubuntu|Roboto|Source+Code+Pro">
<link rel="stylesheet" href="https://at-ui.github.io/feather-font/css/iconfont.css"><title data-react-helmet="true">Incremental Processing on the Data Lake | Apache Hudi</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://hudi.apache.org/cn/blog/2020/08/18/hudi-incremental-processing-on-data-lakes"><meta data-react-helmet="true" name="docsearch:language" content="cn"><meta data-react-helmet="true" name="docsearch:docusaurus_tag" content="default"><meta data-react-helmet="true" property="og:title" content="Incremental Processing on the Data Lake | Apache Hudi"><meta data-react-helmet="true" name="description" content="NOTE: This article is a translation of the infoq.cn article, found here, with minor edits"><meta data-react-helmet="true" property="og:description" content="NOTE: This article is a translation of the infoq.cn article, found here, with minor edits"><meta data-react-helmet="true" property="og:image" content="https://hudi.apache.org/cn/assets/images/blog/incr-processing/image7.png"><meta data-react-helmet="true" name="twitter:image" content="https://hudi.apache.org/cn/assets/images/blog/incr-processing/image7.png"><meta data-react-helmet="true" property="og:type" content="article"><meta data-react-helmet="true" property="article:published_time" content="2020-08-18T00:00:00.000Z"><link data-react-helmet="true" rel="icon" href="/cn/assets/images/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://hudi.apache.org/cn/blog/2020/08/18/hudi-incremental-processing-on-data-lakes"><link data-react-helmet="true" rel="alternate" href="https://hudi.apache.org/blog/2020/08/18/hudi-incremental-processing-on-data-lakes" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://hudi.apache.org/cn/blog/2020/08/18/hudi-incremental-processing-on-data-lakes" hreflang="cn"><link data-react-helmet="true" rel="alternate" href="https://hudi.apache.org/blog/2020/08/18/hudi-incremental-processing-on-data-lakes" hreflang="x-default"><link data-react-helmet="true" rel="preconnect" href="https://BH4D9OD16A-dsn.algolia.net" crossorigin="anonymous"><link rel="stylesheet" href="/cn/assets/css/styles.975cb223.css">
<link rel="preload" href="/cn/assets/js/runtime~main.31bd3d9c.js" as="script">
<link rel="preload" href="/cn/assets/js/main.58422f7b.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><div class="announcementBar_axC9" role="banner"><div class="announcementBarPlaceholder_xYHE"></div><div class="announcementBarContent_6uhP">⭐️ If you like Apache Hudi, give it a star on <a target="_blank" rel="noopener noreferrer" href="https://github.com/apache/hudi">GitHub</a>! ⭐</div><button type="button" class="clean-btn close announcementBarClose_A3A1" aria-label="Close"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/cn/"><div class="navbar__logo"><img src="/cn/assets/images/hudi.png" alt="Apache Hudi" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/cn/assets/images/hudi.png" alt="Apache Hudi" class="themedImage_TMUO themedImage--dark_uzRr"></div></a><a class="navbar__item navbar__link" href="/cn/docs/overview">Docs</a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" class="navbar__link">Learn</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/cn/talks">Talks</a></li><li><a class="dropdown__link" href="/cn/docs/faq">FAQ</a></li><li><a href="https://cwiki.apache.org/confluence/display/HUDI" target="_blank" rel="noopener noreferrer" class="dropdown__link"><span>Technical Wiki<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" class="navbar__link">Contribute</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/cn/contribute/how-to-contribute">How to Contribute</a></li><li><a class="dropdown__link" href="/cn/contribute/developer-setup">Developer Setup</a></li><li><a class="dropdown__link" href="/cn/contribute/rfc-process">RFC Process</a></li><li><a class="dropdown__link" href="/cn/contribute/report-security-issues">Report Security Issues</a></li><li><a href="https://issues.apache.org/jira/projects/HUDI/summary" target="_blank" rel="noopener noreferrer" class="dropdown__link"><span>Report Issues<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" class="navbar__link">Community</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/cn/community/get-involved">Get Involved</a></li><li><a class="dropdown__link" href="/cn/community/syncs">Community Syncs</a></li><li><a class="dropdown__link" href="/cn/community/team">Team</a></li></ul></div><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/cn/blog">Blog</a><a class="navbar__item navbar__link" href="/cn/powered-by">Who&#x27;s Using</a><a class="navbar__item navbar__link" href="/cn/roadmap">Roadmap</a><a class="navbar__item navbar__link" href="/cn/releases/download">Download</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a class="navbar__link" href="/cn/docs/overview">0.11.1</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/cn/docs/next/overview">Next</a></li><li><a class="dropdown__link" href="/cn/docs/overview">0.11.1</a></li><li><a class="dropdown__link" href="/cn/docs/0.11.0/overview">0.11.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.10.1/overview">0.10.1</a></li><li><a class="dropdown__link" href="/cn/docs/0.10.0/overview">0.10.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.9.0/overview">0.9.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.8.0/overview">0.8.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.7.0/overview">0.7.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.6.0/quick-start-guide">0.6.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.5.3/quick-start-guide">0.5.3</a></li><li><a class="dropdown__link" href="/cn/docs/0.5.2/quick-start-guide">0.5.2</a></li><li><a class="dropdown__link" href="/cn/docs/0.5.1/quick-start-guide">0.5.1</a></li><li><a class="dropdown__link" href="/cn/docs/0.5.0/quick-start-guide">0.5.0</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" class="navbar__link"><span><svg viewBox="0 0 20 20" width="20" height="20" aria-hidden="true" class="iconLanguage_EbrZ"><path fill="currentColor" d="M19.753 10.909c-.624-1.707-2.366-2.726-4.661-2.726-.09 0-.176.002-.262.006l-.016-2.063 3.525-.607c.115-.019.133-.119.109-.231-.023-.111-.167-.883-.188-.976-.027-.131-.102-.127-.207-.109-.104.018-3.25.461-3.25.461l-.013-2.078c-.001-.125-.069-.158-.194-.156l-1.025.016c-.105.002-.164.049-.162.148l.033 2.307s-3.061.527-3.144.543c-.084.014-.17.053-.151.143.019.09.19 1.094.208 1.172.018.08.072.129.188.107l2.924-.504.035 2.018c-1.077.281-1.801.824-2.256 1.303-.768.807-1.207 1.887-1.207 2.963 0 1.586.971 2.529 2.328 2.695 3.162.387 5.119-3.06 5.769-4.715 1.097 1.506.256 4.354-2.094 5.98-.043.029-.098.129-.033.207l.619.756c.08.096.206.059.256.023 2.51-1.73 3.661-4.515 2.869-6.683zm-7.386 3.188c-.966-.121-.944-.914-.944-1.453 0-.773.327-1.58.876-2.156a3.21 3.21 0 011.229-.799l.082 4.277a2.773 2.773 0 01-1.243.131zm2.427-.553l.046-4.109c.084-.004.166-.01.252-.01.773 0 1.494.145 1.885.361.391.217-1.023 2.713-2.183 3.758zm-8.95-7.668a.196.196 0 00-.196-.145h-1.95a.194.194 0 00-.194.144L.008 16.916c-.017.051-.011.076.062.076h1.733c.075 0 .099-.023.114-.072l1.008-3.318h3.496l1.008 3.318c.016.049.039.072.113.072h1.734c.072 0 .078-.025.062-.076-.014-.05-3.083-9.741-3.494-11.04zm-2.618 6.318l1.447-5.25 1.447 5.25H3.226z"></path></svg><span>Chinese</span></span></a><ul class="dropdown__menu"><li><a href="/blog/2020/08/18/hudi-incremental-processing-on-data-lakes" target="_self" rel="noopener noreferrer" class="dropdown__link">English</a></li><li><a href="/cn/blog/2020/08/18/hudi-incremental-processing-on-data-lakes" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active">Chinese</a></li></ul></div><a href="https://github.com/apache/hudi" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><a href="https://twitter.com/ApacheHudi" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-twitter-link" aria-label="Hudi Twitter Handle"></a><a href="https://join.slack.com/t/apache-hudi/shared_invite/zt-1d5zjsfl3-d_TefVaGyvEe16EANrxz6Q" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-slack-link" aria-label="Hudi Slack Channel"></a><div class="searchBox_Utm0"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-post-page"><div class="container margin-vert--lg"><div class="row"><main class="col col--9 col--offset-2" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="blogPostTitle_RC3s" itemprop="headline"><h1 itemprop="headline">Incremental Processing on the Data Lake</h1></h1><div class="blogPostText_jBA8 row margin-top--sm margin-bottom--sm &#x27;margin-vert--md&#x27;"><time datetime="2020-08-18T00:00:00.000Z" itemprop="datePublished">August 18, 2020</time><div><div class="avatar margin-bottom--sm"><div><a itemprop="url"><span class="blogPostAuthorsList_dlEG" itemprop="name">vinoyang</span></a></div></div></div></div><div class="blogPostData_A2Le margin-vert--md">18 min read</div></header><div class="markdown" itemprop="articleBody"><h3 class="anchor anchorWithStickyNavbar_y2LR" id="note-this-article-is-a-translation-of-the-infoqcn-article-found-here-with-minor-edits">NOTE: This article is a translation of the infoq.cn article, found <a href="https://www.infoq.cn/article/CAgIDpfJBVcJHKJLSbhe" target="_blank" rel="noopener noreferrer">here</a>, with minor edits<a class="hash-link" href="#note-this-article-is-a-translation-of-the-infoqcn-article-found-here-with-minor-edits" title="Direct link to heading">​</a></h3><p>Apache Hudi is a data lake framework which provides the ability to ingest, manage and query large analytical data sets on a distributed file system/cloud stores.
Hudi joined the Apache incubator for incubation in January 2019, and was promoted to the top Apache project in May 2020. This article mainly discusses the importance
of Hudi to the data lake from the perspective of &quot;incremental processing&quot;. More information about Apache Hudi&#x27;s framework functions, features, usage scenarios, and
latest developments can be found at <a href="https://qconplus.infoq.cn/2020/shanghai/presentation/2646" target="_blank" rel="noopener noreferrer">QCon Global Software Development Conference (Shanghai Station) 2020</a>.</p><p>Throughout the development of big data technology, Hadoop has steadily seized the opportunities of this era and has become the de-facto standard for enterprises to build big data infrastructure.
Among them, the distributed file system HDFS that supports the Hadoop ecosystem almost naturally has become the standard interface for big data storage systems. In recent years, with the rise of
cloud-native architectures, we have seen a wave of newer models embracing low-cost cloud storage emerging, a number of data lake frameworks compatible with HDFS interfaces
embracing cloud vendor storage have emerged in the industry as well. </p><p>However, we are still processing data pretty much in the same way we did 10 years ago. This article will try to talk about its importance to the data lake from the perspective of &quot;incremental processing&quot;.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="traditional-data-lakes-lack-the-primitives-for-incremental-processing">Traditional data lakes lack the primitives for incremental processing<a class="hash-link" href="#traditional-data-lakes-lack-the-primitives-for-incremental-processing" title="Direct link to heading">​</a></h2><p>In the era of mobile Internet and Internet of Things, delayed arrival of data is very common.
Here we are involved in the definition of two time semantics: <a href="https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/" target="_blank" rel="noopener noreferrer">event time and processing time</a>. </p><p>As the name suggests:</p><ul><li><strong>Event time:</strong> the time when the event actually occurred;</li><li><strong>Processing time:</strong> the time when an event is observed (processed) in the system;</li></ul><p>Ideally, the event time and the processing time are the same, but in reality, they may have more or less deviation, which we often call &quot;Time Skew&quot;.
Whether for low-latency stream computing or common batch processing, the processing of event time and processing time and late data is a common and difficult problem.
In general, in order to ensure correctness, when we strictly follow the &quot;event time&quot; semantics, late data will trigger the
<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/stream/operators/windows#late-elements-considerations" target="_blank" rel="noopener noreferrer">recalculation of the time window</a>
(usually Hive partitions for batch processing), although the results of these &quot;windows&quot; may have been calculated or even interacted with the end user.
For recalculation, the extensible key-value storage structure is usually used in streaming processing, which is processed incrementally at the record/event level and optimized
based on point queries and updates. However, in data lakes, recalculating usually means rewriting the entire (immutable) Hive partition (or simply a folder in DFS), and
re-triggering the recalculation of cascading tasks that have consumed that Hive partition.</p><p>With data lakes supporting massive amounts of data, many long-tail businesses still have a strong demand for updating cold data. However, for a long time,
the data in a single partition in the data lake was designed to be non-updatable. If it needs to be updated, the entire partition needs to be rewritten.
This will seriously damage the efficiency of the entire ecosystem. From the perspective of latency and resource utilization, these operations on Hadoop will incur expensive overhead.
Besides, this overhead is usually also cascaded to the entire Hadoop data processing pipeline, which ultimately leads to an increase in latency by several hours.</p><p>In response to the two problems mentioned above, if the data lake supports fine-grained incremental processing, we can incorporate changes into existing Hive partitions
more effectively, and provide a way for downstream table consumers to obtain only the changed data. For effectively supporting incremental processing, we can decompose it into the
following two primitive operations:</p><ul><li><p><strong>Update insert (upsert):</strong> Conceptually, rewriting the entire partition can be regarded as a very inefficient upsert operation, which will eventually write much more data than the
original data itself. Therefore, support for (bulk) upsert is considered a very important feature. <a href="https://research.google/pubs/pub42851/" target="_blank" rel="noopener noreferrer">Google&#x27;s Mesa</a> (Google&#x27;s data warehouse system) also
talks about several techniques that can be applied to rapid data ingestion scenarios.</p></li><li><p><strong>Incremental consumption:</strong> Although upsert can solve the problem of quickly releasing new data to a partition, downstream data consumers do not know
which data has been changed from which time in the past. Usually, consumers can only know the changed data by scanning the entire partition/data table and
recalculating all the data, which requires considerable time and resources. Therefore, we also need a mechanism to more efficiently obtain data records that
have changed since the last time the partition was consumed.</p></li></ul><p>With the above two primitive operations, you can upsert a data set, and then incrementally consume from it, and create another (also incremental) data set to solve the two problems
we mentioned above and support many common cases, so as to support end-to-end incremental processing and reduce end-to-end latency. These two primitives combine with each other,
unlocking the ability of stream/incremental processing based on DFS abstraction.</p><p>The storage scale of the data lake far exceeds that of the data warehouse. Although the two have different focuses on the definition of functions,
there is still a considerable intersection (of course, there are still disputes and deviations from definition and implementation.
This is not the topic this article tries to discuss). In any case, the data lake will support larger analytical data sets with cheaper storage,
so incremental processing is also very important for it. Next let&#x27;s discuss the significance of incremental processing for the data lake.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="the-significance-of-incremental-processing-for-the-data-lake">The significance of incremental processing for the data lake<a class="hash-link" href="#the-significance-of-incremental-processing-for-the-data-lake" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_y2LR" id="streaming-semantics">Streaming Semantics<a class="hash-link" href="#streaming-semantics" title="Direct link to heading">​</a></h3><p>It has long been stated that there is a &quot;<a href="https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying" target="_blank" rel="noopener noreferrer">dualism</a>&quot;
between the change log (that is, the &quot;flow&quot; in the conventional sense we understand) and the table.</p><p><img alt="dualism" src="/cn/assets/images/image4-e0b30b97adaffc9f1a9cf2eb8f0a9c52.jpg"></p><p>The core of this discussion is: if there is a change log, you can use these changes to generate a data table and get the current status. If you update a table,
you can record these changes and publish all &quot;change logs&quot; to the table&#x27;s status information. This interchangeable nature is called &quot;stream table duality&quot; for short.</p><p>A more general understanding of &quot;stream table duality&quot;: when the business system is modifying the data in the MySQL table, MySQL will reflect these changes as Binlog,
if we publish these continuous Binlog (stream) to Kafka, and then let the downstream processing system subscribe to the Kafka, and use the state store to gradually
accumulate the intermediate results. Then the current state of this intermediate result can reflects the current snapshot of the table.</p><p>If the two primitives mentioned above that support incremental processing can be introduced to the data lake, the above pipeline, which can reflect the
&quot;stream table duality&quot;, is also applicable on the data lake. Based on the first primitive, the data lake can also ingest the Binlog log streams in Kafka,
and then store these Binlog log streams into &quot;tables&quot; on the data lake. Based on the second primitive, these tables recognize the changed records as &quot;Binlog&quot;
streams to support the incremental consumption of subsequent cascading tasks.</p><p>Of course, as the data in the data lake needs to be landed on the final file/object storage, considering the trade-off between throughput and write performance,
Binlog on the data lake reacts to a small batch of change logs over a period of time on the stream. For example, the Apache Hudi community is further trying to
provide an incremental view similar to Binlog for different Commits (a Commit refers to a batch of data write commit),
as shown in the following figure:</p><p><img alt="idu" src="/cn/assets/images/image1-3639ab8c9d27f6e461866dc83e8346c0.png"></p><p>Remarks in the &quot;Flag&quot; column:</p><p>I: Insert;
D: Delete;
U: After image of Update;
X: Before image of Update;</p><p>Based on the above discussion, we can think that incremental processing and stream are naturally compatible, and we can naturally connect them on the data lake.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="warehousing-needs-incremental-processing">Warehousing needs Incremental Processing<a class="hash-link" href="#warehousing-needs-incremental-processing" title="Direct link to heading">​</a></h3><p>In the data warehouse, whether it is dimensional modeling or relational modeling theory, it is usually constructed based on the <a href="https://en.wikipedia.org/wiki/Data_warehouse#Design_methods" target="_blank" rel="noopener noreferrer">layered design ideas</a>.
In terms of technical implementation, multiple stages (steps) of a long pipeline are formed by connecting multiple levels of ETL tasks through a workflow scheduling engine,
as shown in the following figure:</p><p><img alt="image2" src="/cn/assets/images/image2-971db03016b54c2da63fec8a2df4f412.png"></p><p>As the main application of the data warehouse, in the OLAP field, for the conventional business scenarios(for no or few changes), there are already some frameworks in the industry
that focus on the scenarios where they are good at providing efficient analysis capabilities. However, in the Hadoop data warehouse/data lake ecosystem,
there is still no good solution for the analysis scenario of frequent changes of business data.</p><p>For example, let’s consider the scenario of updating the order status of a travel business. This scenario has a typical long-tail effect:
you cannot know whether an order will be billed tomorrow, one month later, or one year later. In this scenario, the order table is the main data table,
but usually we will derive other derived tables based on this table to support the modeling of various business scenarios.
The initial update takes place in the order table at the ODS level, but the derived tables need to be updated in cascade.</p><p>For this scenario, in the past, once there is a change, people usually need to find the partition where the data to be updated is located in the Hive order
table of the ODS layer, and update the entire partition, besides, the partition of the relevant data of the derived table needs to be updated in cascade.</p><p>Yes, someone will definitely think of that Kudu&#x27;s support for Upsert can solve the problem of the old version of Hive missing the first incremental primitive.
But the Kudu storage engine has its own limitations:</p><ol><li>Performance: additional requirements for the hardware itself;</li><li>Ecologically: In terms of adapting to mainstream big data computing frameworks and machine learning frameworks, it is far less advantageous than Hive;</li><li>Cost: requires special maintenance costs and expenses;</li><li>Did not solve the second primitive of incremental processing mentioned above: the problem of incremental consumption.</li></ol><p>In summary, incremental processing has the following advantages on the data lake:</p><p><strong>Performance improvement:</strong> Ingesting data usually needs to handle updates, deletes, and enforce unique key constraints. Since incremental primitives support record-level updates,
it can bring orders of magnitude performance improvements to these operations. </p><p><strong>Faster ETL/derived Pipelines:</strong> An ubiquitous next step, once the data has been ingested from external sources is to build derived data pipelines using
Apache Spark/Apache Hive or any other data processing framework to ETL the ingested data for a variety of use-cases like data warehouse,
machine learning, or even just analytics. Typically, such processes again rely on batch processing jobs expressed in code or SQL. Such data pipelines can be speed up dramatically,
by querying one or more input tables using an incremental query instead of a regular snapshot query, resulting in only processing the incremental changes from upstream tables and
then upsert or delete the target derived table.Similar to raw data ingestion, in order to reduce the data delay of the modelled table, the ETL job only needs to gradually extract the
changed data from the original table and update the previously derived output table instead of rebuilding the entire output table every few hours .</p><p><strong>Unified storage:</strong> Based on the above two advantages, faster and lighter processing on the existing data lake means that only for the purpose of accessing near real-time data,
no special storage or data mart is needed.</p><p>Next, we use two simple examples to illustrate how <a href="https://www.oreilly.com/content/ubers-case-for-incremental-processing-on-hadoop/" target="_blank" rel="noopener noreferrer">incremental processing</a> can speed up the processing
of pipelines in analytical scenarios. First of all, data projection is the most common and easy to understand case:</p><p><img alt="image7" src="/cn/assets/images/image7-b669c80a71be700ccbd8bc0945b5d762.png"></p><p>This simple example shows that: by upserting new changes into table_1 and establishing a simple projected table (projected_table) through incremental consumption, we can
operate simpler with lower latency more efficiently projection.</p><p>Next, for a more complex scenario, we can use incremental processing to support the stream and batch connections supported by the stream computing framework,
and stream-stream connections (just need to add some additional logic to align window) :</p><p><img alt="image6" src="/cn/assets/images/image6-3f3cbe07ee5f79cce9b6f18241058961.png"></p><p>The example in the figure above connects a fact table to multiple dimension tables to create a connected table. This case is one of the rare scenarios where we can save hardware
costs while significantly reducing latency.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="quasi-real-time-scenarios-resourceefficiency-trade-offs">Quasi-real-time scenarios, resource/efficiency trade-offs<a class="hash-link" href="#quasi-real-time-scenarios-resourceefficiency-trade-offs" title="Direct link to heading">​</a></h3><p>Incremental processing of new data in mini batches can use resources more efficiently. Let&#x27;s refer to a specific example. We have a Kafka event stream that is pouring in
at a rate of 10,000 per second. We want to count the number of messages in some dimensions over the past 15 minutes. Many stream processing pipelines use an external/internal
result state store (such as RocksDB, Cassandra, ElasticSearch) to save the aggregated count results, and run the containers in resource managers such as YARN/Mesos continuously,
which is very reasonable in less than a five-minute delay window scene. In fact, the YARN container itself has some startup overhead. In addition, in order to improve the
performance of writing to result storage system, we usually cache the results before performing batch updates. This kind of protocol requires the container to run continuously.</p><p>However, in quasi-real-time processing scenarios, these options may not be optimal. To achieve the same effect, you can use short-life containers and optimize overall
resource utilization. For example, a streaming processor may need to perform six million updates to the result storage system in 15 minutes. However, in the incremental
batch mode, we only need to perform an in-memory merge on the accumulated data and update the result storage system only once, then only use the resource container for
five minutes. Compared with the pure stream processing mode, the incremental batch processing mode has several times the CPU efficiency improvement, and there are several
orders of magnitude efficiency improvement in updating to the result storage. Basically, this processing method obtains resources on demand, instead of swallowing CPU and
memory while waiting for data to be calculated in real time.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="incremental-processing-facilitates-unified-data-lake-architecture">Incremental processing facilitates unified data lake architecture<a class="hash-link" href="#incremental-processing-facilitates-unified-data-lake-architecture" title="Direct link to heading">​</a></h3><p>Whether in the data warehouse or in the data lake, data processing is an unavoidable problem. Data processing involves the selection of computing engines and
the design of architectures. There are currently two mainstream architectures in the industry: Lambda and Kappa architectures. Each architecture has its own
characteristics and existing problems. Derivative versions of these architectures are also <a href="https://www.infoq.cn/article/Uo4pFswlMzBVhq*Y2tB9" target="_blank" rel="noopener noreferrer">emerging endlessly</a>.</p><p>In reality, many enterprises still maintain the implementation of the <a href="https://en.wikipedia.org/wiki/Lambda_architecture" target="_blank" rel="noopener noreferrer">Lambda architecture</a>.
The typical Lambda architecture has two modules for the data processing part: the speed layer and the batch layer.</p><p><img alt="image5" src="/cn/assets/images/image5-58bbba66797e915ba7518ad7e61bcd56.png"></p><p>They are usually two independent implementations (from code to infrastructure). For example, Flink (formerly Storm) is a popular option on the speed layer,
while MapReduce/Spark can serve as a batch layer. In fact, people often rely on the speed layer to provide updated results (which may not be accurate), and
once the data is considered complete, the results of the speed layer are corrected at a later time through the batch layer. With incremental processing,
we have the opportunity to implement the Lambda architecture for batch processing and quasi-real-time processing at the code level and infrastructure level in
a unified manner. It typically looks like below:</p><p><img alt="image3" src="/cn/assets/images/image3-8f35b19f5afc8d6b571f4479eb024189.png"></p><p>As we said, you can use SQL or a batch processing framework like Spark to consistently implement your processing logic. The result table is built incrementally,
and SQL is executed on &quot;new data&quot; like streaming to produce a quick view of the results. The same SQL can be executed periodically on the full amount of data to
correct any inaccurate results (remember, join operations are always tricky!) and produce a more &quot;complete&quot; view of the results. In both cases, we will use the
same infrastructure to perform calculations, which can reduce overall operating costs and complexity.</p><p>Setting aside the Lambda architecture, even in the Kappa architecture, the first primitive of incremental processing (upsert) also plays an important role.
Uber <a href="https://www.slideshare.net/FlinkForward/flink-forward-san-francisco-2019-moving-from-lambda-and-kappa-architectures-to-kappa-at-uber-roshan-naik" target="_blank" rel="noopener noreferrer">proposed</a> the Kappa + architecture
based on this. The Kappa architecture advocates a single stream computing layer sufficient to become a general solution
for data processing. Although the batch layer is removed in this model, there are still two problems in the service layer:</p><p>Now days many stream processing engines support row-level data processing, which requires that our service layer should also support row-level updates;
The trade-offs between data ingestion delay, scanning performance and computing resources and operational complexity are unavoidable.</p><p><img alt="image8" src="/cn/assets/images/image8-953168c35d108f42143d2942a7197941.png"></p><p>However, if our business scenarios have low latency requirements, for example, we can accept a delay of about 10 minutes. And if we can quickly ingest and prepare data on DFS,
effectively connect and propagate updates to the upper-level modeling data set, Speed Serving in the service layer is unnecessary. Then the service layer can be unified,
greatly reducing the overall complexity and resource consumption of the system.</p><p>Above, we introduced the significance of incremental processing for the data lake. Next, we introduce the implementation and support of incremental processing.
Among the three open source data lake frameworks (Apache Hudi/Iceberg, Delta Lake), only Apache Hudi provides good support for incremental processing.
This is completely rooted in a framework developed by Uber at the time when it encountered the pain points of data analysis on the Hadoop data lake.
So, next, let&#x27;s introduce how Hudi supports incremental processing.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="hudis-support-for-incremental-processing">Hudi&#x27;s support for incremental processing<a class="hash-link" href="#hudis-support-for-incremental-processing" title="Direct link to heading">​</a></h2><p>Apache Hudi (Hadoop Upserts Deletes and Incrementals) is a top-level project of the Apache Foundation. It allows you to process very large-scale data on
top of Hadoop-compatible storage, and it also provides two primitives that enable stream processing on the data lake in addition to classic batch processing.</p><p>From the naming of the letter &quot;I&quot; denotes &quot;Incremental Processing&quot;, we can see that it will support incremental processing as a first class citizen.
The two primitives we mentioned at the beginning of this article that support incremental processing are reflected in the following two aspects in Apache Hudi:</p><p>Update/Delete operation:Hudi provides support for updating/deleting records, using fine-grained file/record level indexes while providing transactional guarantees
for the write operation. Queries process the last such committed snapshot, to produce results..</p><p>Change stream: Hudi also provides first-class support for obtaining an incremental stream of all the records that were updated/inserted/deleted in a given table, from a given point-in-time.</p><p>The specific implementation of the change flow is &quot;incremental view&quot;. Hudi is the only one of the three open source data lake frameworks that supports
the incremental query feature, with support for record level change streams. The following sample code snippet shows us how to query the incremental view:</p><div class="codeBlockContainer_J+bg language-java theme-code-block"><div class="codeBlockContent_csEI java"><pre tabindex="0" class="prism-code language-java codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">// spark-shell</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">// reload data</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  read.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  format(&quot;hudi&quot;).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  load(basePath + &quot;/*/*/*/*&quot;).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  createOrReplaceTempView(&quot;hudi_trips_snapshot&quot;)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">val commits = spark.sql(&quot;select distinct(_hoodie_commit_time) as commitTime from  hudi_trips_snapshot order by commitTime&quot;).map(k =&gt; k.getString(0)).take(50)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">val beginTime = commits(commits.length - 2) // commit time we are interested in</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">// incrementally query data</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">val tripsIncrementalDF = spark.read.format(&quot;hudi&quot;).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  option(QUERY_TYPE_OPT_KEY, QUERY_TYPE_INCREMENTAL_OPT_VAL).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  option(BEGIN_INSTANTTIME_OPT_KEY, beginTime).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  load(basePath)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">tripsIncrementalDF.createOrReplaceTempView(&quot;hudi_trips_incremental&quot;)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark.sql(&quot;select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from  hudi_trips_incremental where fare &gt; 20.0&quot;).show()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>The code snippet above creates a Hudi trip increment table (hudi_trips_incremental), and then queries all the change records in the increment table after the &quot;beginTime&quot; submission time
and the &quot;cost&quot;  is greater than 20.0. Based on this query, you can create incremental data pipelines on batch data.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="summary">Summary<a class="hash-link" href="#summary" title="Direct link to heading">​</a></h2><p>In this article, we first elaborated many problems caused by the lack of incremental processing primitives in the traditional Hadoop data warehouse due to the trade-off between data integrity
and latency, and some long-tail applications that rely heavily on updates. Next, we argued that to support incremental processing, we must have at least two primitives: upsert and
incremental consumption, and explained why these two primitives can solve the problems explained above.</p><p>Then, we introduced why incremental processing is also important to the data lake. There are many common parts in data processing between the data lake and the data warehouse.
In the data warehouse, some &quot;pain points&quot; caused by the lack of incremental processing also exist in the data lake. We elaborated its significance to the data lake from four
aspects: incremental processing of semantics of natural fit flow, the need for analytical scenarios, quasi-real-time scene resource/efficiency trade-offs, and unified lake architecture.</p><p>Finally, we introduced the open source data lake storage framework Apache Hudi&#x27;s support for incremental processing and simple cases.</p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/cn/blog/2020/08/20/efficient-migration-of-large-parquet-tables"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">« <!-- -->Efficient Migration of Large Parquet Tables to Apache Hudi</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/cn/blog/2020/08/04/PrestoDB-and-Apache-Hudi"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">PrestoDB and Apache Hudi<!-- --> »</div></a></div></nav></main><div class="col col--2"><div class="tableOfContents_vrFS thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#note-this-article-is-a-translation-of-the-infoqcn-article-found-here-with-minor-edits" class="table-of-contents__link toc-highlight">NOTE: This article is a translation of the infoq.cn article, found here, with minor edits</a></li><li><a href="#traditional-data-lakes-lack-the-primitives-for-incremental-processing" class="table-of-contents__link toc-highlight">Traditional data lakes lack the primitives for incremental processing</a></li><li><a href="#the-significance-of-incremental-processing-for-the-data-lake" class="table-of-contents__link toc-highlight">The significance of incremental processing for the data lake</a><ul><li><a href="#streaming-semantics" class="table-of-contents__link toc-highlight">Streaming Semantics</a></li><li><a href="#warehousing-needs-incremental-processing" class="table-of-contents__link toc-highlight">Warehousing needs Incremental Processing</a></li><li><a href="#quasi-real-time-scenarios-resourceefficiency-trade-offs" class="table-of-contents__link toc-highlight">Quasi-real-time scenarios, resource/efficiency trade-offs</a></li><li><a href="#incremental-processing-facilitates-unified-data-lake-architecture" class="table-of-contents__link toc-highlight">Incremental processing facilitates unified data lake architecture</a></li></ul></li><li><a href="#hudis-support-for-incremental-processing" class="table-of-contents__link toc-highlight">Hudi&#39;s support for incremental processing</a></li><li><a href="#summary" class="table-of-contents__link toc-highlight">Summary</a></li></ul></div></div></div></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">About</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/cn/blog/2021/07/21/streaming-data-lake-platform">Our Vision</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/concepts">Concepts</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/community/team">Team</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/releases/release-0.11.1">Releases</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/releases/download">Download</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/powered-by">Who&#x27;s Using</a></li></ul></div><div class="col footer__col"><div class="footer__title">Learn</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/cn/docs/quick-start-guide">Quick Start</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/docker_demo">Docker Demo</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/blog">Blog</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/talks">Talks</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/faq">FAQ</a></li><li class="footer__item"><a href="https://cwiki.apache.org/confluence/display/HUDI" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Technical Wiki<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">Hudi On Cloud</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/cn/docs/s3_hoodie">AWS</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/gcs_hoodie">Google Cloud</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/oss_hoodie">Alibaba Cloud</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/azure_hoodie">Microsoft Azure</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/cos_hoodie">Tencent Cloud</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/ibm_cos_hoodie">IBM Cloud</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/cn/contribute/get-involved">Get Involved</a></li><li class="footer__item"><a href="https://join.slack.com/t/apache-hudi/shared_invite/zt-1d5zjsfl3-d_TefVaGyvEe16EANrxz6Q" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Slack<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://github.com/apache/hudi" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://twitter.com/ApacheHudi" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="mailto:dev-subscribe@hudi.apache.org?Subject=SubscribeToHudi" target="_blank" rel="noopener noreferrer" class="footer__link-item">Mailing List</a></li></ul></div><div class="col footer__col"><div class="footer__title">Apache</div><ul class="footer__items"><li class="footer__item"><a href="https://www.apache.org/events/current-event" target="_blank" rel="noopener noreferrer" class="footer__link-item">Events</a></li><li class="footer__item"><a href="https://www.apache.org/foundation/thanks.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Thanks</a></li><li class="footer__item"><a href="https://www.apache.org/licenses" target="_blank" rel="noopener noreferrer" class="footer__link-item">License</a></li><li class="footer__item"><a href="https://www.apache.org/security" target="_blank" rel="noopener noreferrer" class="footer__link-item">Security</a></li><li class="footer__item"><a href="https://www.apache.org/foundation/sponsorship.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Sponsorship</a></li><li class="footer__item"><a href="https://www.apache.org" target="_blank" rel="noopener noreferrer" class="footer__link-item">Foundation</a></li></ul></div></div><div class="footer__bottom text--center"><div class="margin-bottom--sm"><a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer" class="footerLogoLink_SRtH"><img src="/cn/assets/images/logo-big.png" alt="Apache Hudi™" class="themedImage_TMUO themedImage--light_4Vu1 footer__logo"><img src="/cn/assets/images/logo-big.png" alt="Apache Hudi™" class="themedImage_TMUO themedImage--dark_uzRr footer__logo"></a></div><div class="footer__copyright">Copyright © 2021 <a href="https://apache.org">The Apache Software Foundation</a>, Licensed under the <a href="https://www.apache.org/licenses/LICENSE-2.0"> Apache License, Version 2.0</a>.
      Hudi, Apache and the Apache feather logo are trademarks of The Apache Software Foundation. <a href="/docs/privacy">Privacy Policy</a></div></div></div></footer></div>
<script src="/cn/assets/js/runtime~main.31bd3d9c.js"></script>
<script src="/cn/assets/js/main.58422f7b.js"></script>
</body>
</html>