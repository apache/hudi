<!doctype html>
<html lang="cn" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-1.0.2 docs-doc-page docs-custom-styles docs-doc-id-basic_configurations" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">Basic Configurations | Apache Hudi</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://hudi.apache.org/cn/docs/basic_configurations"><meta data-rh="true" property="og:locale" content="cn"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" name="docusaurus_locale" content="cn"><meta data-rh="true" name="docsearch:language" content="cn"><meta data-rh="true" name="keywords" content="apache hudi, data lake, lakehouse, big data, apache spark, apache flink, presto, trino, analytics, data engineering"><meta data-rh="true" name="docusaurus_version" content="1.0.2"><meta data-rh="true" name="docusaurus_tag" content="docs-default-1.0.2"><meta data-rh="true" name="docsearch:version" content="1.0.2"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-1.0.2"><meta data-rh="true" property="og:title" content="Basic Configurations | Apache Hudi"><meta data-rh="true" name="description" content="This page covers the basic configurations you may use to write/read Hudi tables. This page only features a subset of the most frequently used configurations. For a full list of all configs, please visit the All Configurations page."><meta data-rh="true" property="og:description" content="This page covers the basic configurations you may use to write/read Hudi tables. This page only features a subset of the most frequently used configurations. For a full list of all configs, please visit the All Configurations page."><link data-rh="true" rel="icon" href="/cn/assets/images/favicon.ico"><link data-rh="true" rel="canonical" href="https://hudi.apache.org/cn/docs/basic_configurations"><link data-rh="true" rel="alternate" href="https://hudi.apache.org/docs/basic_configurations" hreflang="en"><link data-rh="true" rel="alternate" href="https://hudi.apache.org/cn/docs/basic_configurations" hreflang="cn"><link data-rh="true" rel="alternate" href="https://hudi.apache.org/docs/basic_configurations" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://BH4D9OD16A-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Basic Configurations","item":"https://hudi.apache.org/cn/docs/basic_configurations"}]}</script><link rel="alternate" type="application/rss+xml" href="/cn/blog/rss.xml" title="Apache Hudi: User-Facing Analytics RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/cn/blog/atom.xml" title="Apache Hudi: User-Facing Analytics Atom Feed">
<link rel="alternate" type="application/json" href="/cn/blog/feed.json" title="Apache Hudi: User-Facing Analytics JSON Feed">




<link rel="search" type="application/opensearchdescription+xml" title="Apache Hudi" href="/cn/opensearch.xml">
<link rel="alternate" type="application/rss+xml" href="/cn/videos/rss.xml" title="Apache Hudi RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/cn/videos/atom.xml" title="Apache Hudi Atom Feed">





<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Comfortaa|Ubuntu|Roboto|Source+Code+Pro">
<link rel="stylesheet" href="https://at-ui.github.io/feather-font/css/iconfont.css">
<script src="https://widget.kapa.ai/kapa-widget.bundle.js" data-website-id="9e4444ba-93cc-45ea-b143-783ae0fbeb6f" data-project-name="Apache Hudi" data-project-color="#FFFFFF" data-project-logo="https://hudi.apache.org/assets/images/logo-big.png" data-modal-disclaimer="This AI assistant answers Apache Hudi questions using your [documentation](https://hudi.apache.org/docs/quick-start-guide/), [dev setup](https://hudi.apache.org/contribute/developer-setup/), the [tech specs](https://hudi.apache.org/tech-specs-1point0/) and open GitHub Issues from the last year." data-modal-title="Apache Hudi AI Assistant" data-modal-example-questions-title="Try asking me..." data-modal-example-questions="How can I convert an existing COW table to MOR?,How do I set up incremental queries with Hudi tables?" data-modal-image="https://hudi.apache.org/assets/images/logo-big-2.png" data-modal-image-ask-ai="https://hudi.apache.org/assets/images/logo-big-2.png" data-modal-header-min-height="64px" data-modal-image-height="40" data-modal-image-width="40" data-modal-header-bg-color="#ffffff" data-modal-title-color="rgb(13, 177, 249)" data-button-text-color="rgb(41, 85, 122)" data-button-text="Ask AI" data-consent-required="true" data-consent-screen-title="Help us improve our AI assistant" data-consent-screen-disclaimer="By clicking &amp;quot;Allow tracking&amp;quot;, you consent to the use of the AI assistant in accordance with kapa.ai&#39;s [Privacy Policy](https://www.kapa.ai/content/privacy-policy). This service uses reCAPTCHA, which requires your consent to Google&#39;s [Privacy Policy](https://policies.google.com/privacy) and [Terms of Service](https://policies.google.com/terms). By proceeding, you explicitly agree to both kapa.ai&#39;s and Google&#39;s privacy policies." data-consent-screen-accept-button-text="Allow tracking" data-query-input-placeholder-text-color="rgb(41, 85, 122)" data-submit-query-button-bg-color="#000" data-user-analytics-cookie-enabled="false" async></script><link rel="stylesheet" href="/cn/assets/css/styles.d44ac021.css">
<script src="/cn/assets/js/runtime~main.deca6468.js" defer="defer"></script>
<script src="/cn/assets/js/main.b5f2a391.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="theme-announcement-bar announcementBar_mb4j" role="banner"><div class="content_knG7 announcementBarContent_xLdY">⭐️ If you like <b>Apache Hudi</b>, give it a star on <a target="_blank" rel="noopener noreferrer" href="https://github.com/apache/hudi"><b>GitHub!<svg xmlns="http://www.w3.org/2000/svg\" width="16" height="16" fill="currentColor" class="bi bi-github" viewBox="0 -2 16 16"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"/></svg></b></a> ⭐</div></div><nav aria-label="Main" class="navbar navbar--fixed-top navbarWrapper_j_uY"><div class="navbar__inner navbarInnerStyle_KoMw"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/cn/"><div class="navbar__logo navbarLogo_aghy"><img src="/cn/assets/images/hudi.png" alt="Apache Hudi" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/cn/assets/images/hudi.png" alt="Apache Hudi" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a class="navbar__item navbar__link" href="/cn/docs/overview">Docs</a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Learn</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/cn/learn/tutorial-series">Tutorial Series</a></li><li><a class="dropdown__link" href="/cn/talks">Talks</a></li><li><a class="dropdown__link" href="/cn/videos">Video Guides</a></li><li><a class="dropdown__link" href="/cn/docs/faq">FAQ</a></li><li><a class="dropdown__link" href="/cn/tech-specs">Tech Specs</a></li><li><a class="dropdown__link" href="/cn/tech-specs-1point0">Tech Specs 1.0</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Contribute</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/cn/contribute/developer-sync-call">Developer Sync Call</a></li><li><a class="dropdown__link" href="/cn/contribute/how-to-contribute">How to Contribute</a></li><li><a class="dropdown__link" href="/cn/contribute/developer-setup">Developer Setup</a></li><li><a class="dropdown__link" href="/cn/contribute/rfc-process">RFC Process</a></li><li><a class="dropdown__link" href="/cn/contribute/report-security-issues">Report Issues</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Community</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/cn/community/get-involved">Get Involved</a></li><li><a class="dropdown__link" href="/cn/community/syncs">Community Syncs</a></li><li><a class="dropdown__link" href="/cn/community/office_hours">Office Hours</a></li><li><a class="dropdown__link" href="/cn/community/team">Team</a></li><li><a href="https://join.slack.com/t/apache-hudi/shared_invite/zt-33fabmxb7-Q7QSUtNOHYCwUdYM8LbauA" target="_blank" rel="noopener noreferrer" class="dropdown__link">Join Our Slack Space<svg width="12" height="12" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><a class="navbar__item navbar__link" href="/cn/ecosystem">Ecosystem</a><a class="navbar__item navbar__link" href="/cn/blog">Blog</a><a class="navbar__item navbar__link" href="/cn/powered-by">Who&#x27;s Using</a><a class="navbar__item navbar__link" href="/cn/roadmap">Roadmap</a><a class="navbar__item navbar__link" href="/cn/releases/download">Download</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a class="navbar__link" aria-haspopup="true" aria-expanded="false" role="button" href="/cn/docs/basic_configurations">1.0.2</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/cn/docs/next/basic_configurations">Next</a></li><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/cn/docs/basic_configurations">1.0.2</a></li><li><a class="dropdown__link" href="/cn/docs/1.0.1/basic_configurations">1.0.1</a></li><li><a class="dropdown__link" href="/cn/docs/1.0.0/basic_configurations">1.0.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.15.0/basic_configurations">0.15.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.14.1/basic_configurations">0.14.1</a></li><li><a class="dropdown__link" href="/cn/docs/0.14.0/basic_configurations">0.14.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.13.1/basic_configurations">0.13.1</a></li><li><a class="dropdown__link" href="/cn/docs/0.13.0/basic_configurations">0.13.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.12.3/basic_configurations">0.12.3</a></li><li><a class="dropdown__link" href="/cn/docs/0.12.2/basic_configurations">0.12.2</a></li><li><a class="dropdown__link" href="/cn/docs/0.12.1/basic_configurations">0.12.1</a></li><li><a class="dropdown__link" href="/cn/docs/0.12.0/basic_configurations">0.12.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.11.1/basic_configurations">0.11.1</a></li><li><a class="dropdown__link" href="/cn/docs/0.11.0/overview">0.11.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.10.1/overview">0.10.1</a></li><li><a class="dropdown__link" href="/cn/docs/0.10.0/overview">0.10.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.9.0/overview">0.9.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.8.0/overview">0.8.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.7.0/overview">0.7.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.6.0/quick-start-guide">0.6.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.5.3/quick-start-guide">0.5.3</a></li><li><a class="dropdown__link" href="/cn/docs/0.5.2/quick-start-guide">0.5.2</a></li><li><a class="dropdown__link" href="/cn/docs/0.5.1/quick-start-guide">0.5.1</a></li><li><a class="dropdown__link" href="/cn/docs/0.5.0/quick-start-guide">0.5.0</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link locale-dropdown-wrapper"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>Chinese<svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" fill="none" viewBox="0 0 14 14" class="globeIcon_RxQM"><g clip-path="url(#a)"><path fill="#1C1E21" d="M14 6.457a6.84 6.84 0 0 0-7-6.02 6.843 6.843 0 0 0-7 6.02v1.085a6.843 6.843 0 0 0 7 6.02 6.843 6.843 0 0 0 7-6.02zm-1.094 0h-2.625a10 10 0 0 0-.376-2.222 6.7 6.7 0 0 0 1.531-.875 5.25 5.25 0 0 1 1.444 3.097zm-8.032 0a8.5 8.5 0 0 1 .324-1.872 7.4 7.4 0 0 0 3.63 0c.175.61.284 1.239.325 1.872zm4.305 1.085a8.4 8.4 0 0 1-.324 1.873 7.46 7.46 0 0 0-3.658 0 8.5 8.5 0 0 1-.323-1.873zm.35-4.375A10.3 10.3 0 0 0 8.75 1.75c.627.194 1.218.49 1.75.875a5.8 5.8 0 0 1-.998.577zM7.254 1.54A8.8 8.8 0 0 1 8.46 3.552c-.48.11-.97.165-1.461.167-.492-.001-.982-.057-1.461-.167.308-.722.715-1.4 1.207-2.012zM4.498 3.202a5.8 5.8 0 0 1-.998-.577 6 6 0 0 1 1.75-.875c-.294.46-.546.947-.753 1.452m-1.873.15c.47.358.984.652 1.531.874A9.6 9.6 0 0 0 3.78 6.45H1.155a5.25 5.25 0 0 1 1.47-3.098M1.12 7.541h2.625c.038.753.164 1.5.376 2.223a6.7 6.7 0 0 0-1.531.875 5.25 5.25 0 0 1-1.47-3.098m3.377 3.255q.311.76.753 1.453a6 6 0 0 1-1.75-.875q.47-.34.997-.578m2.25 1.663a8.6 8.6 0 0 1-1.208-2.013 6.5 6.5 0 0 1 2.922 0 8.5 8.5 0 0 1-1.207 2.013zm2.755-1.663q.552.235 1.042.578a6.3 6.3 0 0 1-1.75.875q.413-.697.708-1.453m1.873-.148a6.7 6.7 0 0 0-1.531-.875 9.5 9.5 0 0 0 .376-2.223h2.625a5.25 5.25 0 0 1-1.47 3.098"></path></g><defs><clipPath id="a"><path fill="#fff" d="M0 0h14v14H0z"></path></clipPath></defs></svg></a><ul class="dropdown__menu"><li><a href="/docs/basic_configurations" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/cn/docs/basic_configurations" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="cn">Chinese</a></li></ul></div><a href="https://github.com/apache/hudi" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><a href="https://x.com/ApacheHudi" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-twitter-link" aria-label="Hudi Twitter Handle"></a><a href="https://join.slack.com/t/apache-hudi/shared_invite/zt-33fabmxb7-Q7QSUtNOHYCwUdYM8LbauA" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-slack-link" aria-label="Hudi Slack Channel"></a><a href="https://www.youtube.com/channel/UCs7AhE0BWaEPZSChrBR-Muw" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-youtube-link" aria-label="Hudi YouTube Channel"></a><a href="https://www.linkedin.com/company/apache-hudi/?viewAsMember=true" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-linkedin-link" aria-label="Hudi Linkedin Page"></a><div class="navbarSearchContainer_Bca1"><div><div role="button" class="searchButton_o6KI" aria-label="Search"><span class="searchText_sHOJ">Search</span><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" fill="none" viewBox="0 0 14 14"><circle cx="6.864" cy="6.864" r="5.243" stroke="#1C1E21" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5"></circle><path stroke="#1C1E21" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m10.51 10.783 2.056 2.05"></path></svg></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar navbarSideMenu_TODO"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/cn/"><div class="navbar__logo navbarLogo_aghy"><img src="/cn/assets/images/hudi.png" alt="Apache Hudi" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/cn/assets/images/hudi.png" alt="Apache Hudi" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><button type="button" aria-label="Close navigation bar" class="clean-btn navbar-sidebar__close"><svg viewBox="0 0 15 15" width="21" height="21"><g stroke="var(--ifm-color-emphasis-600)" stroke-width="1.2"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><div class="navbar-sidebar__items"><div class="navbar-sidebar__item menu"><ul class="menu__list"><li class="menu__list-item"><a class="menu__link navbarFontSize_pR5Q" href="/cn/docs/overview">Docs</a></li><li class="menu__list-item menu__list-item--collapsed"><a role="button" class="dropdownNavbarItemMobile_JUhd menu__link menu__link--sublist menu__link--sublist-caret navbarFontSize_pR5Q">Learn</a></li><li class="menu__list-item menu__list-item--collapsed"><a role="button" class="dropdownNavbarItemMobile_JUhd menu__link menu__link--sublist menu__link--sublist-caret navbarFontSize_pR5Q">Contribute</a></li><li class="menu__list-item menu__list-item--collapsed"><a role="button" class="dropdownNavbarItemMobile_JUhd menu__link menu__link--sublist menu__link--sublist-caret navbarFontSize_pR5Q">Community</a></li><li class="menu__list-item"><a class="menu__link navbarFontSize_pR5Q" href="/cn/ecosystem">Ecosystem</a></li><li class="menu__list-item"><a class="menu__link navbarFontSize_pR5Q" href="/cn/blog">Blog</a></li><li class="menu__list-item"><a class="menu__link navbarFontSize_pR5Q" href="/cn/powered-by">Who&#x27;s Using</a></li><li class="menu__list-item"><a class="menu__link navbarFontSize_pR5Q" href="/cn/roadmap">Roadmap</a></li><li class="menu__list-item"><a class="menu__link navbarFontSize_pR5Q" href="/cn/releases/download">Download</a></li><li class="menu__list-item menu__list-item--collapsed"><a role="button" class="dropdownNavbarItemMobile_JUhd menu__link menu__link--sublist menu__link--sublist-caret navbarFontSize_pR5Q">Versions</a></li><li class="menu__list-item menu__list-item--collapsed"><a role="button" class="dropdownNavbarItemMobile_JUhd menu__link menu__link--sublist menu__link--sublist-caret locale-dropdown-wrapper"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>Languages<svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" fill="none" viewBox="0 0 14 14" class="globeIcon_RxQM"><g clip-path="url(#a)"><path fill="#1C1E21" d="M14 6.457a6.84 6.84 0 0 0-7-6.02 6.843 6.843 0 0 0-7 6.02v1.085a6.843 6.843 0 0 0 7 6.02 6.843 6.843 0 0 0 7-6.02zm-1.094 0h-2.625a10 10 0 0 0-.376-2.222 6.7 6.7 0 0 0 1.531-.875 5.25 5.25 0 0 1 1.444 3.097zm-8.032 0a8.5 8.5 0 0 1 .324-1.872 7.4 7.4 0 0 0 3.63 0c.175.61.284 1.239.325 1.872zm4.305 1.085a8.4 8.4 0 0 1-.324 1.873 7.46 7.46 0 0 0-3.658 0 8.5 8.5 0 0 1-.323-1.873zm.35-4.375A10.3 10.3 0 0 0 8.75 1.75c.627.194 1.218.49 1.75.875a5.8 5.8 0 0 1-.998.577zM7.254 1.54A8.8 8.8 0 0 1 8.46 3.552c-.48.11-.97.165-1.461.167-.492-.001-.982-.057-1.461-.167.308-.722.715-1.4 1.207-2.012zM4.498 3.202a5.8 5.8 0 0 1-.998-.577 6 6 0 0 1 1.75-.875c-.294.46-.546.947-.753 1.452m-1.873.15c.47.358.984.652 1.531.874A9.6 9.6 0 0 0 3.78 6.45H1.155a5.25 5.25 0 0 1 1.47-3.098M1.12 7.541h2.625c.038.753.164 1.5.376 2.223a6.7 6.7 0 0 0-1.531.875 5.25 5.25 0 0 1-1.47-3.098m3.377 3.255q.311.76.753 1.453a6 6 0 0 1-1.75-.875q.47-.34.997-.578m2.25 1.663a8.6 8.6 0 0 1-1.208-2.013 6.5 6.5 0 0 1 2.922 0 8.5 8.5 0 0 1-1.207 2.013zm2.755-1.663q.552.235 1.042.578a6.3 6.3 0 0 1-1.75.875q.413-.697.708-1.453m1.873-.148a6.7 6.7 0 0 0-1.531-.875 9.5 9.5 0 0 0 .376-2.223h2.625a5.25 5.25 0 0 1-1.47 3.098"></path></g><defs><clipPath id="a"><path fill="#fff" d="M0 0h14v14H0z"></path></clipPath></defs></svg></a></li><li class="menu__list-item"><a href="https://github.com/apache/hudi" target="_blank" rel="noopener noreferrer" class="menu__link header-github-link" aria-label="GitHub repository"></a></li><li class="menu__list-item"><a href="https://x.com/ApacheHudi" target="_blank" rel="noopener noreferrer" class="menu__link header-twitter-link" aria-label="Hudi Twitter Handle"></a></li><li class="menu__list-item"><a href="https://join.slack.com/t/apache-hudi/shared_invite/zt-33fabmxb7-Q7QSUtNOHYCwUdYM8LbauA" target="_blank" rel="noopener noreferrer" class="menu__link header-slack-link" aria-label="Hudi Slack Channel"></a></li><li class="menu__list-item"><a href="https://www.youtube.com/channel/UCs7AhE0BWaEPZSChrBR-Muw" target="_blank" rel="noopener noreferrer" class="menu__link header-youtube-link" aria-label="Hudi YouTube Channel"></a></li><li class="menu__list-item"><a href="https://www.linkedin.com/company/apache-hudi/?viewAsMember=true" target="_blank" rel="noopener noreferrer" class="menu__link header-linkedin-link" aria-label="Hudi Linkedin Page"></a></li></ul></div><div class="navbar-sidebar__item menu"><button type="button" class="clean-btn navbar-sidebar__back">← Back to main menu</button></div></div></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_bSxm"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_cWv0"><aside class="theme-doc-sidebar-container docSidebarContainer_RSuS"><div class="sidebarViewport_pYEE"><div class="sidebar_njMd"><nav aria-label="Sidebar navigation" class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" href="/cn/docs/overview">Getting Started</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cn/docs/overview">Overview</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cn/docs/quick-start-guide">Spark Quick Start</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cn/docs/flink-quick-start-guide">Flink 指南</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cn/docs/python-rust-quick-start-guide">Python/Rust Quick Start</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cn/docs/docker_demo">Docker Demo</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cn/docs/use_cases">使用案例</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/cn/docs/hudi_stack">Design &amp; Concepts</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/cn/docs/hoodie_streaming_ingestion">Ingestion</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/cn/docs/sql_ddl">Writing Tables</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/cn/docs/sql_queries">Reading Tables</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/cn/docs/cleaning">Table Services</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/cn/docs/snapshot_exporter">Platform &amp; Tools</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/cn/docs/performance">Operating Hudi</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/cn/docs/basic_configurations">Configurations</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/cn/docs/basic_configurations">Basic Configurations</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cn/docs/configurations">配置</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/cn/docs/cloud">Storage Configurations</a></div></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cn/docs/privacy">隐私协议</a></li></ul></nav></div></div></aside><main class="docMainContainer_hjYf"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/cn/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Configurations</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Basic Configurations</span></li></ul></nav><span class="theme-doc-version-badge badge badge--secondary">Version: 1.0.2</span><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Basic Configurations</h1></header><p>This page covers the basic configurations you may use to write/read Hudi tables. This page only features a subset of the most frequently used configurations. For a full list of all configs, please visit the <a href="/cn/docs/configurations">All Configurations</a> page.</p>
<ul>
<li><a href="#TABLE_CONFIG"><strong>Hudi Table Config</strong></a>: Basic Hudi Table configuration parameters.</li>
<li><a href="#SPARK_DATASOURCE"><strong>Spark Datasource Configs</strong></a>: These configs control the Hudi Spark Datasource, providing ability to define keys/partitioning, pick out the write operation, specify how to merge records or choosing query type to read.</li>
<li><a href="#FLINK_SQL"><strong>Flink Sql Configs</strong></a>: These configs control the Hudi Flink SQL source/sink connectors, providing ability to define record keys, pick out the write operation, specify how to merge records, enable/disable asynchronous compaction or choosing query type to read.</li>
<li><a href="#WRITE_CLIENT"><strong>Write Client Configs</strong></a>: Internally, the Hudi datasource uses a RDD based HoodieWriteClient API to actually perform writes to storage. These configs provide deep control over lower level aspects like file sizing, compression, parallelism, compaction, write schema, cleaning etc. Although Hudi provides sane defaults, from time-time these configs may need to be tweaked to optimize for specific workloads.</li>
<li><a href="#META_SYNC"><strong>Metastore and Catalog Sync Configs</strong></a>: Configurations used by the Hudi to sync metadata to external metastores and catalogs.</li>
<li><a href="#METRICS"><strong>Metrics Configs</strong></a>: These set of configs are used to enable monitoring and reporting of key Hudi stats and metrics.</li>
<li><a href="#KAFKA_CONNECT"><strong>Kafka Connect Configs</strong></a>: These set of configs are used for Kafka Connect Sink Connector for writing Hudi Tables</li>
<li><a href="#HUDI_STREAMER"><strong>Hudi Streamer Configs</strong></a>: These set of configs are used for Hudi Streamer utility which provides the way to ingest from different sources such as DFS or Kafka.</li>
</ul>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>In the tables below <strong>(N/A)</strong> means there is no default value set</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="TABLE_CONFIG">Hudi Table Config<a href="#TABLE_CONFIG" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Basic Hudi Table configuration parameters.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="Hudi-Table-Basic-Configs">Hudi Table Basic Configs<a href="#Hudi-Table-Basic-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Configurations of the Hudi Table like type of ingestion, storage formats, hive table name etc. Configurations are loaded from hoodie.properties, these properties are usually set during initializing a path as hoodie base path and never changes during the lifetime of a hoodie table.</p>
<p><a href="#Hudi-Table-Basic-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiebootstrapbasepath">hoodie.bootstrap.base.path</a></td><td>(N/A)</td><td>Base path of the dataset that needs to be bootstrapped as a Hudi table<br><code>Config Param: BOOTSTRAP_BASE_PATH</code></td></tr><tr><td><a href="#hoodiecompactionpayloadclass">hoodie.compaction.payload.class</a></td><td>(N/A)</td><td>Payload class to use for performing merges, compactions, i.e merge delta logs with current base file and then  produce a new base file.<br><code>Config Param: PAYLOAD_CLASS_NAME</code></td></tr><tr><td><a href="#hoodiedatabasename">hoodie.database.name</a></td><td>(N/A)</td><td>Database name. If different databases have the same table name during incremental query, we can set it to limit the table name under a specific database<br><code>Config Param: DATABASE_NAME</code></td></tr><tr><td><a href="#hoodierecordmergemode">hoodie.record.merge.mode</a></td><td>(N/A)</td><td>org.apache.hudi.common.config.RecordMergeMode: Determines the logic of merging updates     COMMIT_TIME_ORDERING: Using transaction time to merge records, i.e., the record from later transaction overwrites the earlier record with the same key.     EVENT_TIME_ORDERING: Using event time as the ordering to merge records, i.e., the record with the larger event time overwrites the record with the smaller event time on the same key, regardless of transaction time. The event time or preCombine field needs to be specified by the user.     CUSTOM: Using custom merging logic specified by the user.<br><code>Config Param: RECORD_MERGE_MODE</code><br><code>Since Version: 1.0.0</code></td></tr><tr><td><a href="#hoodierecordmergestrategyid">hoodie.record.merge.strategy.id</a></td><td>(N/A)</td><td>Id of merger strategy. Hudi will pick HoodieRecordMerger implementations in <code>hoodie.write.record.merge.custom.implementation.classes</code> which has the same merger strategy id<br><code>Config Param: RECORD_MERGE_STRATEGY_ID</code><br><code>Since Version: 0.13.0</code></td></tr><tr><td><a href="#hoodietablechecksum">hoodie.table.checksum</a></td><td>(N/A)</td><td>Table checksum is used to guard against partial writes in HDFS. It is added as the last entry in hoodie.properties and then used to validate while reading table config.<br><code>Config Param: TABLE_CHECKSUM</code><br><code>Since Version: 0.11.0</code></td></tr><tr><td><a href="#hoodietablecreateschema">hoodie.table.create.schema</a></td><td>(N/A)</td><td>Schema used when creating the table<br><code>Config Param: CREATE_SCHEMA</code></td></tr><tr><td><a href="#hoodietableindexdefspath">hoodie.table.index.defs.path</a></td><td>(N/A)</td><td>Relative path to table base path where the index definitions are stored<br><code>Config Param: RELATIVE_INDEX_DEFINITION_PATH</code><br><code>Since Version: 1.0.0</code></td></tr><tr><td><a href="#hoodietablekeygeneratorclass">hoodie.table.keygenerator.class</a></td><td>(N/A)</td><td>Key Generator class property for the hoodie table<br><code>Config Param: KEY_GENERATOR_CLASS_NAME</code></td></tr><tr><td><a href="#hoodietablekeygeneratortype">hoodie.table.keygenerator.type</a></td><td>(N/A)</td><td>Key Generator type to determine key generator class<br><code>Config Param: KEY_GENERATOR_TYPE</code><br><code>Since Version: 1.0.0</code></td></tr><tr><td><a href="#hoodietablemetadatapartitions">hoodie.table.metadata.partitions</a></td><td>(N/A)</td><td>Comma-separated list of metadata partitions that have been completely built and in-sync with data table. These partitions are ready for use by the readers<br><code>Config Param: TABLE_METADATA_PARTITIONS</code><br><code>Since Version: 0.11.0</code></td></tr><tr><td><a href="#hoodietablemetadatapartitionsinflight">hoodie.table.metadata.partitions.inflight</a></td><td>(N/A)</td><td>Comma-separated list of metadata partitions whose building is in progress. These partitions are not yet ready for use by the readers.<br><code>Config Param: TABLE_METADATA_PARTITIONS_INFLIGHT</code><br><code>Since Version: 0.11.0</code></td></tr><tr><td><a href="#hoodietablename">hoodie.table.name</a></td><td>(N/A)</td><td>Table name that will be used for registering with Hive. Needs to be same across runs.<br><code>Config Param: NAME</code></td></tr><tr><td><a href="#hoodietablepartitionfields">hoodie.table.partition.fields</a></td><td>(N/A)</td><td>Comma separated field names used to partition the table. These field names also include the partition type which is used by custom key generators<br><code>Config Param: PARTITION_FIELDS</code></td></tr><tr><td><a href="#hoodietableprecombinefield">hoodie.table.precombine.field</a></td><td>(N/A)</td><td>Field used in preCombining before actual write. By default, when two records have the same key value, the largest value for the precombine field determined by Object.compareTo(..), is picked.<br><code>Config Param: PRECOMBINE_FIELD</code></td></tr><tr><td><a href="#hoodietablerecordkeyfields">hoodie.table.recordkey.fields</a></td><td>(N/A)</td><td>Columns used to uniquely identify the table. Concatenated values of these fields are used as  the record key component of HoodieKey.<br><code>Config Param: RECORDKEY_FIELDS</code></td></tr><tr><td><a href="#hoodietablesecondaryindexesmetadata">hoodie.table.secondary.indexes.metadata</a></td><td>(N/A)</td><td>The metadata of secondary indexes<br><code>Config Param: SECONDARY_INDEXES_METADATA</code><br><code>Since Version: 0.13.0</code></td></tr><tr><td><a href="#hoodietimelinelayoutversion">hoodie.timeline.layout.version</a></td><td>(N/A)</td><td>Version of timeline used, by the table.<br><code>Config Param: TIMELINE_LAYOUT_VERSION</code></td></tr><tr><td><a href="#hoodiearchivelogfolder">hoodie.archivelog.folder</a></td><td>archived</td><td>path under the meta folder, to store archived timeline instants at.<br><code>Config Param: ARCHIVELOG_FOLDER</code></td></tr><tr><td><a href="#hoodiebootstrapindexclass">hoodie.bootstrap.index.class</a></td><td>org.apache.hudi.common.bootstrap.index.hfile.HFileBootstrapIndex</td><td>Implementation to use, for mapping base files to bootstrap base file, that contain actual data.<br><code>Config Param: BOOTSTRAP_INDEX_CLASS_NAME</code></td></tr><tr><td><a href="#hoodiebootstrapindexenable">hoodie.bootstrap.index.enable</a></td><td>true</td><td>Whether or not, this is a bootstrapped table, with bootstrap base data and an mapping index defined, default true.<br><code>Config Param: BOOTSTRAP_INDEX_ENABLE</code></td></tr><tr><td><a href="#hoodiebootstrapindextype">hoodie.bootstrap.index.type</a></td><td>HFILE</td><td>Bootstrap index type determines which implementation to use, for mapping base files to bootstrap base file, that contain actual data.<br><code>Config Param: BOOTSTRAP_INDEX_TYPE</code><br><code>Since Version: 1.0.0</code></td></tr><tr><td><a href="#hoodiedatasourcewritehive_style_partitioning">hoodie.datasource.write.hive_style_partitioning</a></td><td>false</td><td>Flag to indicate whether to use Hive style partitioning. If set true, the names of partition folders follow &lt;partition_column_name&gt;=&lt;partition_value&gt; format. By default false (the names of partition folders are only partition values)<br><code>Config Param: HIVE_STYLE_PARTITIONING_ENABLE</code></td></tr><tr><td><a href="#hoodiepartitionmetafileusebaseformat">hoodie.partition.metafile.use.base.format</a></td><td>false</td><td>If true, partition metafiles are saved in the same format as base-files for this dataset (e.g. Parquet / ORC). If false (default) partition metafiles are saved as properties files.<br><code>Config Param: PARTITION_METAFILE_USE_BASE_FORMAT</code></td></tr><tr><td><a href="#hoodiepopulatemetafields">hoodie.populate.meta.fields</a></td><td>true</td><td>When enabled, populates all meta fields. When disabled, no meta fields are populated and incremental queries will not be functional. This is only meant to be used for append only/immutable data for batch processing<br><code>Config Param: POPULATE_META_FIELDS</code></td></tr><tr><td><a href="#hoodietablebasefileformat">hoodie.table.base.file.format</a></td><td>PARQUET</td><td>Base file format to store all the base file data.<br><code>Config Param: BASE_FILE_FORMAT</code></td></tr><tr><td><a href="#hoodietablecdcenabled">hoodie.table.cdc.enabled</a></td><td>false</td><td>When enable, persist the change data if necessary, and can be queried as a CDC query mode.<br><code>Config Param: CDC_ENABLED</code><br><code>Since Version: 0.13.0</code></td></tr><tr><td><a href="#hoodietablecdcsupplementalloggingmode">hoodie.table.cdc.supplemental.logging.mode</a></td><td>DATA_BEFORE_AFTER</td><td>org.apache.hudi.common.table.cdc.HoodieCDCSupplementalLoggingMode: Change log capture supplemental logging mode. The supplemental log is used for accelerating the generation of change log details.     OP_KEY_ONLY: Only keeping record keys in the supplemental logs, so the reader needs to figure out the update before image and after image.     DATA_BEFORE: Keeping the before images in the supplemental logs, so the reader needs to figure out the update after images.     DATA_BEFORE_AFTER(default): Keeping the before and after images in the supplemental logs, so the reader can generate the details directly from the logs.<br><code>Config Param: CDC_SUPPLEMENTAL_LOGGING_MODE</code><br><code>Since Version: 0.13.0</code></td></tr><tr><td><a href="#hoodietableinitialversion">hoodie.table.initial.version</a></td><td>EIGHT</td><td>Initial Version of table when the table was created. Used for upgrade/downgrade to identify what upgrade/downgrade paths happened on the table. This is only configured when the table is initially setup.<br><code>Config Param: INITIAL_VERSION</code><br><code>Since Version: 1.0.0</code></td></tr><tr><td><a href="#hoodietablelogfileformat">hoodie.table.log.file.format</a></td><td>HOODIE_LOG</td><td>Log format used for the delta logs.<br><code>Config Param: LOG_FILE_FORMAT</code></td></tr><tr><td><a href="#hoodietablemultiplebasefileformatsenable">hoodie.table.multiple.base.file.formats.enable</a></td><td>false</td><td>When set to true, the table can support reading and writing multiple base file formats.<br><code>Config Param: MULTIPLE_BASE_FILE_FORMATS_ENABLE</code><br><code>Since Version: 1.0.0</code></td></tr><tr><td><a href="#hoodietabletimelinetimezone">hoodie.table.timeline.timezone</a></td><td>LOCAL</td><td>User can set hoodie commit timeline timezone, such as utc, local and so on. local is default<br><code>Config Param: TIMELINE_TIMEZONE</code></td></tr><tr><td><a href="#hoodietabletype">hoodie.table.type</a></td><td>COPY_ON_WRITE</td><td>The table type for the underlying data.<br><code>Config Param: TYPE</code></td></tr><tr><td><a href="#hoodietableversion">hoodie.table.version</a></td><td>EIGHT</td><td>Version of table, used for running upgrade/downgrade steps between releases with potentially breaking/backwards compatible changes.<br><code>Config Param: VERSION</code></td></tr><tr><td><a href="#hoodietimelinehistorypath">hoodie.timeline.history.path</a></td><td>history</td><td>path under the meta folder, to store timeline history at.<br><code>Config Param: TIMELINE_HISTORY_PATH</code></td></tr><tr><td><a href="#hoodietimelinepath">hoodie.timeline.path</a></td><td>timeline</td><td>path under the meta folder, to store timeline instants at.<br><code>Config Param: TIMELINE_PATH</code></td></tr></tbody></table>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="SPARK_DATASOURCE">Spark Datasource Configs<a href="#SPARK_DATASOURCE" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>These configs control the Hudi Spark Datasource, providing ability to define keys/partitioning, pick out the write operation, specify how to merge records or choosing query type to read.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="Read-Options">Read Options<a href="#Read-Options" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Options useful for reading tables via <code>read.format.option(...)</code></p>
<p><a href="#Read-Options-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiedatasourcereadbegininstanttime">hoodie.datasource.read.begin.instanttime</a></td><td>(N/A)</td><td>Required when <code>hoodie.datasource.query.type</code> is set to <code>incremental</code>. Represents the completion time to start incrementally pulling data from. The completion time here need not necessarily correspond to an instant on the timeline. New data written with completion_time &gt;= START_COMMIT are fetched out. For e.g: ‘20170901080000’ will get all new data written on or after Sep 1, 2017 08:00AM.<br><code>Config Param: START_COMMIT</code></td></tr><tr><td><a href="#hoodiedatasourcereadendinstanttime">hoodie.datasource.read.end.instanttime</a></td><td>(N/A)</td><td>Used when <code>hoodie.datasource.query.type</code> is set to <code>incremental</code>. Represents the completion time to limit incrementally fetched data to. When not specified latest commit completion time from timeline is assumed by default. When specified, new data written with completion_time &lt;= END_COMMIT are fetched out. Point in time type queries make more sense with begin and end completion times specified.<br><code>Config Param: END_COMMIT</code></td></tr><tr><td><a href="#hoodiedatasourcereadincrtableversion">hoodie.datasource.read.incr.table.version</a></td><td>(N/A)</td><td>The table version assumed for incremental read<br><code>Config Param: INCREMENTAL_READ_TABLE_VERSION</code></td></tr><tr><td><a href="#hoodiedatasourcereadstreamingtableversion">hoodie.datasource.read.streaming.table.version</a></td><td>(N/A)</td><td>The table version assumed for streaming read<br><code>Config Param: STREAMING_READ_TABLE_VERSION</code></td></tr><tr><td><a href="#hoodiedatasourcewriteprecombinefield">hoodie.datasource.write.precombine.field</a></td><td>(N/A)</td><td>Field used in preCombining before actual write. When two records have the same key value, we will pick the one with the largest value for the precombine field, determined by Object.compareTo(..)<br><code>Config Param: READ_PRE_COMBINE_FIELD</code></td></tr><tr><td><a href="#hoodiedatasourcequerytype">hoodie.datasource.query.type</a></td><td>snapshot</td><td>Whether data needs to be read, in <code>incremental</code> mode (new data since an instantTime) (or) <code>read_optimized</code> mode (obtain latest view, based on base files) (or) <code>snapshot</code> mode (obtain latest view, by merging base and (if any) log files)<br><code>Config Param: QUERY_TYPE</code></td></tr></tbody></table>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="Write-Options">Write Options<a href="#Write-Options" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>You can pass down any of the WriteClient level configs directly using <code>options()</code> or <code>option(k,v)</code> methods.</p>
<div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">inputDF.write()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">.format(&quot;org.apache.hudi&quot;)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">.options(clientOpts) // any of the Hudi client opts can be passed in as well</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">.option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), &quot;_row_key&quot;)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">.option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), &quot;partition&quot;)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">.option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), &quot;timestamp&quot;)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">.option(HoodieWriteConfig.TABLE_NAME, tableName)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">.mode(SaveMode.Append)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">.save(basePath);</span><br></span></code></pre></div></div>
<p>Options useful for writing tables via <code>write.format.option(...)</code></p>
<p><a href="#Write-Options-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiedatasourcehive_syncmode">hoodie.datasource.hive_sync.mode</a></td><td>(N/A)</td><td>Mode to choose for Hive ops. Valid values are hms, jdbc and hiveql.<br><code>Config Param: HIVE_SYNC_MODE</code></td></tr><tr><td><a href="#hoodiedatasourcewritepartitionpathfield">hoodie.datasource.write.partitionpath.field</a></td><td>(N/A)</td><td>Partition path field. Value to be used at the partitionPath component of HoodieKey. Actual value obtained by invoking .toString()<br><code>Config Param: PARTITIONPATH_FIELD</code></td></tr><tr><td><a href="#hoodiedatasourcewriteprecombinefield">hoodie.datasource.write.precombine.field</a></td><td>(N/A)</td><td>Field used in preCombining before actual write. When two records have the same key value, we will pick the one with the largest value for the precombine field, determined by Object.compareTo(..)<br><code>Config Param: PRECOMBINE_FIELD</code></td></tr><tr><td><a href="#hoodiedatasourcewriterecordkeyfield">hoodie.datasource.write.recordkey.field</a></td><td>(N/A)</td><td>Record key field. Value to be used as the <code>recordKey</code> component of <code>HoodieKey</code>. Actual value will be obtained by invoking .toString() on the field value. Nested fields can be specified using the dot notation eg: <code>a.b.c</code><br><code>Config Param: RECORDKEY_FIELD</code></td></tr><tr><td><a href="#hoodiedatasourcewritesecondarykeycolumn">hoodie.datasource.write.secondarykey.column</a></td><td>(N/A)</td><td>Columns that constitute the secondary key component. Actual value will be obtained by invoking .toString() on the field value. Nested fields can be specified using the dot notation eg: <code>a.b.c</code><br><code>Config Param: SECONDARYKEY_COLUMN_NAME</code></td></tr><tr><td><a href="#hoodiewriterecordmergemode">hoodie.write.record.merge.mode</a></td><td>(N/A)</td><td>org.apache.hudi.common.config.RecordMergeMode: Determines the logic of merging updates     COMMIT_TIME_ORDERING: Using transaction time to merge records, i.e., the record from later transaction overwrites the earlier record with the same key.     EVENT_TIME_ORDERING: Using event time as the ordering to merge records, i.e., the record with the larger event time overwrites the record with the smaller event time on the same key, regardless of transaction time. The event time or preCombine field needs to be specified by the user.     CUSTOM: Using custom merging logic specified by the user.<br><code>Config Param: RECORD_MERGE_MODE</code><br><code>Since Version: 1.0.0</code></td></tr><tr><td><a href="#hoodieclusteringasyncenabled">hoodie.clustering.async.enabled</a></td><td>false</td><td>Enable running of clustering service, asynchronously as inserts happen on the table.<br><code>Config Param: ASYNC_CLUSTERING_ENABLE</code><br><code>Since Version: 0.7.0</code></td></tr><tr><td><a href="#hoodieclusteringinline">hoodie.clustering.inline</a></td><td>false</td><td>Turn on inline clustering - clustering will be run after each write operation is complete<br><code>Config Param: INLINE_CLUSTERING_ENABLE</code><br><code>Since Version: 0.7.0</code></td></tr><tr><td><a href="#hoodiedatasourcehive_syncenable">hoodie.datasource.hive_sync.enable</a></td><td>false</td><td>When set to true, register/sync the table to Apache Hive metastore.<br><code>Config Param: HIVE_SYNC_ENABLED</code></td></tr><tr><td><a href="#hoodiedatasourcehive_syncjdbcurl">hoodie.datasource.hive_sync.jdbcurl</a></td><td>jdbc:hive2://localhost:10000</td><td>Hive metastore url<br><code>Config Param: HIVE_URL</code></td></tr><tr><td><a href="#hoodiedatasourcehive_syncmetastoreuris">hoodie.datasource.hive_sync.metastore.uris</a></td><td>thrift://localhost:9083</td><td>Hive metastore url<br><code>Config Param: METASTORE_URIS</code></td></tr><tr><td><a href="#hoodiedatasourcemetasyncenable">hoodie.datasource.meta.sync.enable</a></td><td>false</td><td>Enable Syncing the Hudi Table with an external meta store or data catalog.<br><code>Config Param: META_SYNC_ENABLED</code></td></tr><tr><td><a href="#hoodiedatasourcewritehive_style_partitioning">hoodie.datasource.write.hive_style_partitioning</a></td><td>false</td><td>Flag to indicate whether to use Hive style partitioning. If set true, the names of partition folders follow &lt;partition_column_name&gt;=&lt;partition_value&gt; format. By default false (the names of partition folders are only partition values)<br><code>Config Param: HIVE_STYLE_PARTITIONING</code></td></tr><tr><td><a href="#hoodiedatasourcewriteoperation">hoodie.datasource.write.operation</a></td><td>upsert</td><td>Whether to do upsert, insert or bulk_insert for the write operation. Use bulk_insert to load new data into a table, and there on use upsert/insert. bulk insert uses a disk based write path to scale to load large inputs without need to cache it.<br><code>Config Param: OPERATION</code></td></tr><tr><td><a href="#hoodiedatasourcewritetabletype">hoodie.datasource.write.table.type</a></td><td>COPY_ON_WRITE</td><td>The table type for the underlying data, for this write. This can’t change between writes.<br><code>Config Param: TABLE_TYPE</code></td></tr></tbody></table>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="FLINK_SQL">Flink Sql Configs<a href="#FLINK_SQL" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>These configs control the Hudi Flink SQL source/sink connectors, providing ability to define record keys, pick out the write operation, specify how to merge records, enable/disable asynchronous compaction or choosing query type to read.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="Flink-Options">Flink Options<a href="#Flink-Options" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Flink jobs using the SQL can be configured through the options in WITH clause. The actual datasource level configs are listed below.</p>
<p><a href="#Flink-Options-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiedatabasename">hoodie.database.name</a></td><td>(N/A)</td><td>Database name to register to Hive metastore<br> <code>Config Param: DATABASE_NAME</code></td></tr><tr><td><a href="#hoodietablename">hoodie.table.name</a></td><td>(N/A)</td><td>Table name to register to Hive metastore<br> <code>Config Param: TABLE_NAME</code></td></tr><tr><td><a href="#path">path</a></td><td>(N/A)</td><td>Base path for the target hoodie table. The path would be created if it does not exist, otherwise a Hoodie table expects to be initialized successfully<br> <code>Config Param: PATH</code></td></tr><tr><td><a href="#readcommitslimit">read.commits.limit</a></td><td>(N/A)</td><td>The maximum number of commits allowed to read in each instant check, if it is streaming read, the avg read instants number per-second would be &#x27;read.commits.limit&#x27;/&#x27;read.streaming.check-interval&#x27;, by default no limit<br> <code>Config Param: READ_COMMITS_LIMIT</code></td></tr><tr><td><a href="#readend-commit">read.end-commit</a></td><td>(N/A)</td><td>End commit instant for reading, the commit time format should be &#x27;yyyyMMddHHmmss&#x27;<br> <code>Config Param: READ_END_COMMIT</code></td></tr><tr><td><a href="#readstart-commit">read.start-commit</a></td><td>(N/A)</td><td>Start commit instant for reading, the commit time format should be &#x27;yyyyMMddHHmmss&#x27;, by default reading from the latest instant for streaming read<br> <code>Config Param: READ_START_COMMIT</code></td></tr><tr><td><a href="#archivemax_commits">archive.max_commits</a></td><td>50</td><td>Max number of commits to keep before archiving older commits into a sequential log, default 50<br> <code>Config Param: ARCHIVE_MAX_COMMITS</code></td></tr><tr><td><a href="#archivemin_commits">archive.min_commits</a></td><td>40</td><td>Min number of commits to keep before archiving older commits into a sequential log, default 40<br> <code>Config Param: ARCHIVE_MIN_COMMITS</code></td></tr><tr><td><a href="#cdcenabled">cdc.enabled</a></td><td>false</td><td>When enable, persist the change data if necessary, and can be queried as a CDC query mode<br> <code>Config Param: CDC_ENABLED</code></td></tr><tr><td><a href="#cdcsupplementalloggingmode">cdc.supplemental.logging.mode</a></td><td>DATA_BEFORE_AFTER</td><td>Setting &#x27;op_key_only&#x27; persists the &#x27;op&#x27; and the record key only, setting &#x27;data_before&#x27; persists the additional &#x27;before&#x27; image, and setting &#x27;data_before_after&#x27; persists the additional &#x27;before&#x27; and &#x27;after&#x27; images.<br> <code>Config Param: SUPPLEMENTAL_LOGGING_MODE</code></td></tr><tr><td><a href="#changelogenabled">changelog.enabled</a></td><td>false</td><td>Whether to keep all the intermediate changes, we try to keep all the changes of a record when enabled: 1). The sink accept the UPDATE_BEFORE message; 2). The source try to emit every changes of a record. The semantics is best effort because the compaction job would finally merge all changes of a record into one.  default false to have UPSERT semantics<br> <code>Config Param: CHANGELOG_ENABLED</code></td></tr><tr><td><a href="#cleanasyncenabled">clean.async.enabled</a></td><td>true</td><td>Whether to cleanup the old commits immediately on new commits, enabled by default<br> <code>Config Param: CLEAN_ASYNC_ENABLED</code></td></tr><tr><td><a href="#cleanretain_commits">clean.retain_commits</a></td><td>30</td><td>Number of commits to retain. So data will be retained for num_of_commits * time_between_commits (scheduled). This also directly translates into how much you can incrementally pull on this table, default 30<br> <code>Config Param: CLEAN_RETAIN_COMMITS</code></td></tr><tr><td><a href="#clusteringasyncenabled">clustering.async.enabled</a></td><td>false</td><td>Async Clustering, default false<br> <code>Config Param: CLUSTERING_ASYNC_ENABLED</code></td></tr><tr><td><a href="#clusteringplanstrategysmallfilelimit">clustering.plan.strategy.small.file.limit</a></td><td>600</td><td>Files smaller than the size specified here are candidates for clustering, default 600 MB<br> <code>Config Param: CLUSTERING_PLAN_STRATEGY_SMALL_FILE_LIMIT</code></td></tr><tr><td><a href="#clusteringplanstrategytargetfilemaxbytes">clustering.plan.strategy.target.file.max.bytes</a></td><td>1073741824</td><td>Each group can produce &#x27;N&#x27; (CLUSTERING_MAX_GROUP_SIZE/CLUSTERING_TARGET_FILE_SIZE) output file groups, default 1 GB<br> <code>Config Param: CLUSTERING_PLAN_STRATEGY_TARGET_FILE_MAX_BYTES</code></td></tr><tr><td><a href="#compactionasyncenabled">compaction.async.enabled</a></td><td>true</td><td>Async Compaction, enabled by default for MOR<br> <code>Config Param: COMPACTION_ASYNC_ENABLED</code></td></tr><tr><td><a href="#compactiondelta_commits">compaction.delta_commits</a></td><td>5</td><td>Max delta commits needed to trigger compaction, default 5 commits<br> <code>Config Param: COMPACTION_DELTA_COMMITS</code></td></tr><tr><td><a href="#hive_syncenabled">hive_sync.enabled</a></td><td>false</td><td>Asynchronously sync Hive meta to HMS, default false<br> <code>Config Param: HIVE_SYNC_ENABLED</code></td></tr><tr><td><a href="#hive_syncjdbc_url">hive_sync.jdbc_url</a></td><td>jdbc:hive2://localhost:10000</td><td>Jdbc URL for hive sync, default &#x27;jdbc:hive2://localhost:10000&#x27;<br> <code>Config Param: HIVE_SYNC_JDBC_URL</code></td></tr><tr><td><a href="#hive_syncmetastoreuris">hive_sync.metastore.uris</a></td><td></td><td>Metastore uris for hive sync, default &#x27;&#x27;<br> <code>Config Param: HIVE_SYNC_METASTORE_URIS</code></td></tr><tr><td><a href="#hive_syncmode">hive_sync.mode</a></td><td>HMS</td><td>Mode to choose for Hive ops. Valid values are hms, jdbc and hiveql, default &#x27;hms&#x27;<br> <code>Config Param: HIVE_SYNC_MODE</code></td></tr><tr><td><a href="#hoodiedatasourcequerytype">hoodie.datasource.query.type</a></td><td>snapshot</td><td>Decides how data files need to be read, in 1) Snapshot mode (obtain latest view, based on row &amp; columnar data); 2) incremental mode (new data since an instantTime); 3) Read Optimized mode (obtain latest view, based on columnar data) .Default: snapshot<br> <code>Config Param: QUERY_TYPE</code></td></tr><tr><td><a href="#hoodiedatasourcewritehive_style_partitioning">hoodie.datasource.write.hive_style_partitioning</a></td><td>false</td><td>Whether to use Hive style partitioning. If set true, the names of partition folders follow &lt;partition_column_name&gt;=&lt;partition_value&gt; format. By default false (the names of partition folders are only partition values)<br> <code>Config Param: HIVE_STYLE_PARTITIONING</code></td></tr><tr><td><a href="#hoodiedatasourcewritepartitionpathfield">hoodie.datasource.write.partitionpath.field</a></td><td></td><td>Partition path field. Value to be used at the <code>partitionPath</code> component of <code>HoodieKey</code>. Actual value obtained by invoking .toString(), default &#x27;&#x27;<br> <code>Config Param: PARTITION_PATH_FIELD</code></td></tr><tr><td><a href="#hoodiedatasourcewriterecordkeyfield">hoodie.datasource.write.recordkey.field</a></td><td>uuid</td><td>Record key field. Value to be used as the <code>recordKey</code> component of <code>HoodieKey</code>. Actual value will be obtained by invoking .toString() on the field value. Nested fields can be specified using the dot notation eg: <code>a.b.c</code><br> <code>Config Param: RECORD_KEY_FIELD</code></td></tr><tr><td><a href="#indextype">index.type</a></td><td>FLINK_STATE</td><td>Index type of Flink write job, default is using state backed index.<br> <code>Config Param: INDEX_TYPE</code></td></tr><tr><td><a href="#lookupjoincachettl">lookup.join.cache.ttl</a></td><td>PT1H</td><td>The cache TTL (e.g. 10min) for the build table in lookup join.<br> <code>Config Param: LOOKUP_JOIN_CACHE_TTL</code></td></tr><tr><td><a href="#metadatacompactiondelta_commits">metadata.compaction.delta_commits</a></td><td>10</td><td>Max delta commits for metadata table to trigger compaction, default 10<br> <code>Config Param: METADATA_COMPACTION_DELTA_COMMITS</code></td></tr><tr><td><a href="#metadataenabled">metadata.enabled</a></td><td>true</td><td>Enable the internal metadata table which serves table metadata like level file listings, default enabled<br> <code>Config Param: METADATA_ENABLED</code></td></tr><tr><td><a href="#precombinefield">precombine.field</a></td><td>ts</td><td>Field used in preCombining before actual write. When two records have the same key value, we will pick the one with the largest value for the precombine field, determined by Object.compareTo(..)<br> <code>Config Param: PRECOMBINE_FIELD</code></td></tr><tr><td><a href="#readstreamingenabled">read.streaming.enabled</a></td><td>false</td><td>Whether to read as streaming source, default false<br> <code>Config Param: READ_AS_STREAMING</code></td></tr><tr><td><a href="#readstreamingskip_insertoverwrite">read.streaming.skip_insertoverwrite</a></td><td>false</td><td>Whether to skip insert overwrite instants to avoid reading base files of insert overwrite operations for streaming read. In streaming scenarios, insert overwrite is usually used to repair data, here you can control the visibility of downstream streaming read.<br> <code>Config Param: READ_STREAMING_SKIP_INSERT_OVERWRITE</code></td></tr><tr><td><a href="#tabletype">table.type</a></td><td>COPY_ON_WRITE</td><td>Type of table to write. COPY_ON_WRITE (or) MERGE_ON_READ<br> <code>Config Param: TABLE_TYPE</code></td></tr><tr><td><a href="#writeoperation">write.operation</a></td><td>upsert</td><td>The write operation, that this write should do<br> <code>Config Param: OPERATION</code></td></tr><tr><td><a href="#writeparquetmaxfilesize">write.parquet.max.file.size</a></td><td>120</td><td>Target size for parquet files produced by Hudi write phases. For DFS, this needs to be aligned with the underlying filesystem block size for optimal performance.<br> <code>Config Param: WRITE_PARQUET_MAX_FILE_SIZE</code></td></tr></tbody></table>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="WRITE_CLIENT">Write Client Configs<a href="#WRITE_CLIENT" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Internally, the Hudi datasource uses a RDD based HoodieWriteClient API to actually perform writes to storage. These configs provide deep control over lower level aspects like file sizing, compression, parallelism, compaction, write schema, cleaning etc. Although Hudi provides sane defaults, from time-time these configs may need to be tweaked to optimize for specific workloads.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="Common-Configurations">Common Configurations<a href="#Common-Configurations" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>The following set of configurations are common across Hudi.</p>
<p><a href="#Common-Configurations-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiebasepath">hoodie.base.path</a></td><td>(N/A)</td><td>Base path on lake storage, under which all the table data is stored. Always prefix it explicitly with the storage scheme (e.g hdfs://, s3:// etc). Hudi stores all the main meta-data about commits, savepoints, cleaning audit logs etc in .hoodie directory under this base path directory.<br><code>Config Param: BASE_PATH</code></td></tr></tbody></table>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="Metadata-Configs">Metadata Configs<a href="#Metadata-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Configurations used by the Hudi Metadata Table. This table maintains the metadata about a given Hudi table (e.g file listings)  to avoid overhead of accessing cloud storage, during queries.</p>
<p><a href="#Metadata-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodieindexname">hoodie.index.name</a></td><td>(N/A)</td><td>Name of the expression index. This is also used for the partition name in the metadata table.<br><code>Config Param: SECONDARY_INDEX_NAME</code><br><code>Since Version: 1.0.0</code></td></tr><tr><td><a href="#hoodieindexname">hoodie.index.name</a></td><td>(N/A)</td><td>Name of the expression index. This is also used for the partition name in the metadata table.<br><code>Config Param: EXPRESSION_INDEX_NAME</code><br><code>Since Version: 1.0.0</code></td></tr><tr><td><a href="#hoodiemetadataindexdrop">hoodie.metadata.index.drop</a></td><td>(N/A)</td><td>Drop the specified index. The value should be the name of the index to delete. You can check index names using <code>SHOW INDEXES</code> command. The index name either starts with or matches exactly can be one of the following: files, column_stats, bloom_filters, record_index, expr_index_, secondary_index_, partition_stats, files<br><code>Config Param: DROP_METADATA_INDEX</code><br><code>Since Version: 1.0.1</code></td></tr><tr><td><a href="#hoodieexpressionindextype">hoodie.expression.index.type</a></td><td>COLUMN_STATS</td><td>Type of the expression index. Default is <code>column_stats</code> if there are no functions and expressions in the command. Valid options could be BITMAP, COLUMN_STATS, LUCENE, etc. If index_type is not provided, and there are functions or expressions in the command then a expression index using column stats will be created.<br><code>Config Param: EXPRESSION_INDEX_TYPE</code><br><code>Since Version: 1.0.0</code></td></tr><tr><td><a href="#hoodiemetadataenable">hoodie.metadata.enable</a></td><td>true</td><td>Enable the internal metadata table which serves table metadata like level file listings<br><code>Config Param: ENABLE</code><br><code>Since Version: 0.7.0</code></td></tr><tr><td><a href="#hoodiemetadataindexbloomfilterenable">hoodie.metadata.index.bloom.filter.enable</a></td><td>false</td><td>Enable indexing bloom filters of user data files under metadata table. When enabled, metadata table will have a partition to store the bloom filter index and will be used during the index lookups.<br><code>Config Param: ENABLE_METADATA_INDEX_BLOOM_FILTER</code><br><code>Since Version: 0.11.0</code></td></tr><tr><td><a href="#hoodiemetadataindexcolumnstatsenable">hoodie.metadata.index.column.stats.enable</a></td><td>false</td><td>Enable indexing column ranges of user data files under metadata table key lookups. When enabled, metadata table will have a partition to store the column ranges and will be used for pruning files during the index lookups.<br><code>Config Param: ENABLE_METADATA_INDEX_COLUMN_STATS</code><br><code>Since Version: 0.11.0</code></td></tr><tr><td><a href="#hoodiemetadataindexexpressionenable">hoodie.metadata.index.expression.enable</a></td><td>false</td><td>Enable expression index within the metadata table.  When this configuration property is enabled (<code>true</code>), the Hudi writer automatically  keeps all expression indexes consistent with the data table.  When disabled (<code>false</code>), all expression indexes are deleted.  Note that individual expression index can only be created through a <code>CREATE INDEX</code>  and deleted through a <code>DROP INDEX</code> statement in Spark SQL.<br><code>Config Param: EXPRESSION_INDEX_ENABLE_PROP</code><br><code>Since Version: 1.0.0</code></td></tr><tr><td><a href="#hoodiemetadataindexpartitionstatsenable">hoodie.metadata.index.partition.stats.enable</a></td><td>false</td><td>Enable aggregating stats for each column at the storage partition level. Enabling this can improve query performance by leveraging partition and column stats for (partition) filtering. Important: The default value for this configuration is dynamically set based on the effective value of hoodie.metadata.index.column.stats.enable. If column stats index is enabled (default for Spark engine), partition stats indexing will also be enabled by default. Conversely, if column stats indexing is disabled (default for Flink and Java engines), partition stats indexing will also be disabled by default.<br><code>Config Param: ENABLE_METADATA_INDEX_PARTITION_STATS</code><br><code>Since Version: 1.0.0</code></td></tr><tr><td><a href="#hoodiemetadataindexsecondaryenable">hoodie.metadata.index.secondary.enable</a></td><td>true</td><td>Enable secondary index within the metadata table.  When this configuration property is enabled (<code>true</code>), the Hudi writer automatically  keeps all secondary indexes consistent with the data table.  When disabled (<code>false</code>), all secondary indexes are deleted.  Note that individual secondary index can only be created through a <code>CREATE INDEX</code>  and deleted through a <code>DROP INDEX</code> statement in Spark SQL. <br><code>Config Param: SECONDARY_INDEX_ENABLE_PROP</code><br><code>Since Version: 1.0.0</code></td></tr></tbody></table>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="Storage-Configs">Storage Configs<a href="#Storage-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Configurations that control aspects around writing, sizing, reading base and log files.</p>
<p><a href="#Storage-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodieparquetcompressioncodec">hoodie.parquet.compression.codec</a></td><td>gzip</td><td>Compression Codec for parquet files<br><code>Config Param: PARQUET_COMPRESSION_CODEC_NAME</code></td></tr><tr><td><a href="#hoodieparquetmaxfilesize">hoodie.parquet.max.file.size</a></td><td>125829120</td><td>Target size in bytes for parquet files produced by Hudi write phases. For DFS, this needs to be aligned with the underlying filesystem block size for optimal performance.<br><code>Config Param: PARQUET_MAX_FILE_SIZE</code></td></tr></tbody></table>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="Archival-Configs">Archival Configs<a href="#Archival-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Configurations that control archival.</p>
<p><a href="#Archival-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiekeepmaxcommits">hoodie.keep.max.commits</a></td><td>30</td><td>Archiving service moves older entries from timeline into an archived log after each write, to keep the metadata overhead constant, even as the table size grows. This config controls the maximum number of instants to retain in the active timeline. <br><code>Config Param: MAX_COMMITS_TO_KEEP</code></td></tr><tr><td><a href="#hoodiekeepmincommits">hoodie.keep.min.commits</a></td><td>20</td><td>Similar to hoodie.keep.max.commits, but controls the minimum number of instants to retain in the active timeline.<br><code>Config Param: MIN_COMMITS_TO_KEEP</code></td></tr></tbody></table>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="Bootstrap-Configs">Bootstrap Configs<a href="#Bootstrap-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Configurations that control how you want to bootstrap your existing tables for the first time into hudi. The bootstrap operation can flexibly avoid copying data over before you can use Hudi and support running the existing  writers and new hudi writers in parallel, to validate the migration.</p>
<p><a href="#Bootstrap-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiebootstrapbasepath">hoodie.bootstrap.base.path</a></td><td>(N/A)</td><td>Base path of the dataset that needs to be bootstrapped as a Hudi table<br><code>Config Param: BASE_PATH</code><br><code>Since Version: 0.6.0</code></td></tr></tbody></table>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="Clean-Configs">Clean Configs<a href="#Clean-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Cleaning (reclamation of older/unused file groups/slices).</p>
<p><a href="#Clean-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiecleanasyncenabled">hoodie.clean.async.enabled</a></td><td>false</td><td>Only applies when hoodie.clean.automatic is turned on. When turned on runs cleaner async with writing, which can speed up overall write performance.<br><code>Config Param: ASYNC_CLEAN</code></td></tr><tr><td><a href="#hoodiecleancommitsretained">hoodie.clean.commits.retained</a></td><td>10</td><td>When KEEP_LATEST_COMMITS cleaning policy is used, the number of commits to retain, without cleaning. This will be retained for num_of_commits * time_between_commits (scheduled). This also directly translates into how much data retention the table supports for incremental queries.<br><code>Config Param: CLEANER_COMMITS_RETAINED</code></td></tr></tbody></table>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="Clustering-Configs">Clustering Configs<a href="#Clustering-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Configurations that control the clustering table service in hudi, which optimizes the storage layout for better query performance by sorting and sizing data files.</p>
<p><a href="#Clustering-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodieclusteringasyncenabled">hoodie.clustering.async.enabled</a></td><td>false</td><td>Enable running of clustering service, asynchronously as inserts happen on the table.<br><code>Config Param: ASYNC_CLUSTERING_ENABLE</code><br><code>Since Version: 0.7.0</code></td></tr><tr><td><a href="#hoodieclusteringinline">hoodie.clustering.inline</a></td><td>false</td><td>Turn on inline clustering - clustering will be run after each write operation is complete<br><code>Config Param: INLINE_CLUSTERING</code><br><code>Since Version: 0.7.0</code></td></tr><tr><td><a href="#hoodieclusteringplanstrategysmallfilelimit">hoodie.clustering.plan.strategy.small.file.limit</a></td><td>314572800</td><td>Files smaller than the size in bytes specified here are candidates for clustering<br><code>Config Param: PLAN_STRATEGY_SMALL_FILE_LIMIT</code><br><code>Since Version: 0.7.0</code></td></tr><tr><td><a href="#hoodieclusteringplanstrategytargetfilemaxbytes">hoodie.clustering.plan.strategy.target.file.max.bytes</a></td><td>1073741824</td><td>Each group can produce &#x27;N&#x27; (CLUSTERING_MAX_GROUP_SIZE/CLUSTERING_TARGET_FILE_SIZE) output file groups<br><code>Config Param: PLAN_STRATEGY_TARGET_FILE_MAX_BYTES</code><br><code>Since Version: 0.7.0</code></td></tr></tbody></table>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="Compaction-Configs">Compaction Configs<a href="#Compaction-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Configurations that control compaction (merging of log files onto a new base files).</p>
<p><a href="#Compaction-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiecompactinline">hoodie.compact.inline</a></td><td>false</td><td>When set to true, compaction service is triggered after each write. While being  simpler operationally, this adds extra latency on the write path.<br><code>Config Param: INLINE_COMPACT</code></td></tr><tr><td><a href="#hoodiecompactinlinemaxdeltacommits">hoodie.compact.inline.max.delta.commits</a></td><td>5</td><td>Number of delta commits after the last compaction, before scheduling of a new compaction is attempted. This config takes effect only for the compaction triggering strategy based on the number of commits, i.e., NUM_COMMITS, NUM_COMMITS_AFTER_LAST_REQUEST, NUM_AND_TIME, and NUM_OR_TIME.<br><code>Config Param: INLINE_COMPACT_NUM_DELTA_COMMITS</code></td></tr></tbody></table>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="Error-table-Configs">Error table Configs<a href="#Error-table-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Configurations that are required for Error table configs</p>
<p><a href="#Error-table-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodieerrortablebasepath">hoodie.errortable.base.path</a></td><td>(N/A)</td><td>Base path for error table under which all error records would be stored.<br><code>Config Param: ERROR_TABLE_BASE_PATH</code></td></tr><tr><td><a href="#hoodieerrortabletargettablename">hoodie.errortable.target.table.name</a></td><td>(N/A)</td><td>Table name to be used for the error table<br><code>Config Param: ERROR_TARGET_TABLE</code></td></tr><tr><td><a href="#hoodieerrortablewriteclass">hoodie.errortable.write.class</a></td><td>(N/A)</td><td>Class which handles the error table writes. This config is used to configure a custom implementation for Error Table Writer. Specify the full class name of the custom error table writer as a value for this config<br><code>Config Param: ERROR_TABLE_WRITE_CLASS</code></td></tr><tr><td><a href="#hoodieerrortableenable">hoodie.errortable.enable</a></td><td>false</td><td>Config to enable error table. If the config is enabled, all the records with processing error in DeltaStreamer are transferred to error table.<br><code>Config Param: ERROR_TABLE_ENABLED</code></td></tr><tr><td><a href="#hoodieerrortableinsertshuffleparallelism">hoodie.errortable.insert.shuffle.parallelism</a></td><td>200</td><td>Config to set insert shuffle parallelism. The config is similar to hoodie.insert.shuffle.parallelism config but applies to the error table.<br><code>Config Param: ERROR_TABLE_INSERT_PARALLELISM_VALUE</code></td></tr><tr><td><a href="#hoodieerrortablesourcerddpersist">hoodie.errortable.source.rdd.persist</a></td><td>false</td><td>Enabling this config, persists the sourceRDD to disk which helps in faster processing of data table + error table write DAG<br><code>Config Param: ERROR_TABLE_PERSIST_SOURCE_RDD</code></td></tr><tr><td><a href="#hoodieerrortableupsertshuffleparallelism">hoodie.errortable.upsert.shuffle.parallelism</a></td><td>200</td><td>Config to set upsert shuffle parallelism. The config is similar to hoodie.upsert.shuffle.parallelism config but applies to the error table.<br><code>Config Param: ERROR_TABLE_UPSERT_PARALLELISM_VALUE</code></td></tr><tr><td><a href="#hoodieerrortablevalidaterecordcreationenable">hoodie.errortable.validate.recordcreation.enable</a></td><td>true</td><td>Records that fail to be created due to keygeneration failure or other issues will be sent to the Error Table<br><code>Config Param: ERROR_ENABLE_VALIDATE_RECORD_CREATION</code><br><code>Since Version: 0.15.0</code></td></tr><tr><td><a href="#hoodieerrortablevalidatetargetschemaenable">hoodie.errortable.validate.targetschema.enable</a></td><td>false</td><td>Records with schema mismatch with Target Schema are sent to Error Table.<br><code>Config Param: ERROR_ENABLE_VALIDATE_TARGET_SCHEMA</code></td></tr><tr><td><a href="#hoodieerrortablewritefailurestrategy">hoodie.errortable.write.failure.strategy</a></td><td>ROLLBACK_COMMIT</td><td>The config specifies the failure strategy if error table write fails. Use one of - [ROLLBACK_COMMIT (Rollback the corresponding base table write commit for which the error events were triggered) , LOG_ERROR (Error is logged but the base table write succeeds) ]<br><code>Config Param: ERROR_TABLE_WRITE_FAILURE_STRATEGY</code></td></tr><tr><td><a href="#hoodieerrortablewriteunionenable">hoodie.errortable.write.union.enable</a></td><td>false</td><td>Enable error table union with data table when writing for improved commit performance. By default it is disabled meaning data table and error table writes are sequential<br><code>Config Param: ENABLE_ERROR_TABLE_WRITE_UNIFICATION</code></td></tr></tbody></table>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="Write-Configurations">Write Configurations<a href="#Write-Configurations" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Configurations that control write behavior on Hudi tables. These can be directly passed down from even higher level frameworks (e.g Spark datasources, Flink sink) and utilities (e.g Hudi Streamer).</p>
<p><a href="#Write-Configurations-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiebasepath">hoodie.base.path</a></td><td>(N/A)</td><td>Base path on lake storage, under which all the table data is stored. Always prefix it explicitly with the storage scheme (e.g hdfs://, s3:// etc). Hudi stores all the main meta-data about commits, savepoints, cleaning audit logs etc in .hoodie directory under this base path directory.<br><code>Config Param: BASE_PATH</code></td></tr><tr><td><a href="#hoodiedatasourcewriteprecombinefield">hoodie.datasource.write.precombine.field</a></td><td>(N/A)</td><td>Field used in preCombining before actual write. When two records have the same key value, we will pick the one with the largest value for the precombine field, determined by Object.compareTo(..)<br><code>Config Param: PRECOMBINE_FIELD_NAME</code></td></tr><tr><td><a href="#hoodietablename">hoodie.table.name</a></td><td>(N/A)</td><td>Table name that will be used for registering with metastores like HMS. Needs to be same across runs.<br><code>Config Param: TBL_NAME</code></td></tr><tr><td><a href="#hoodiewriterecordmergemode">hoodie.write.record.merge.mode</a></td><td>(N/A)</td><td>org.apache.hudi.common.config.RecordMergeMode: Determines the logic of merging updates     COMMIT_TIME_ORDERING: Using transaction time to merge records, i.e., the record from later transaction overwrites the earlier record with the same key.     EVENT_TIME_ORDERING: Using event time as the ordering to merge records, i.e., the record with the larger event time overwrites the record with the smaller event time on the same key, regardless of transaction time. The event time or preCombine field needs to be specified by the user.     CUSTOM: Using custom merging logic specified by the user.<br><code>Config Param: RECORD_MERGE_MODE</code><br><code>Since Version: 1.0.0</code></td></tr><tr><td><a href="#hoodiefailjobonduplicatedatafiledetection">hoodie.fail.job.on.duplicate.data.file.detection</a></td><td>false</td><td>If config is enabled, entire job is failed on invalid file detection<br><code>Config Param: FAIL_JOB_ON_DUPLICATE_DATA_FILE_DETECTION</code></td></tr><tr><td><a href="#hoodieinstant_statetimeline_server_basedenabled">hoodie.instant_state.timeline_server_based.enabled</a></td><td>false</td><td>If enabled, writers get instant state from timeline server rather than requesting DFS directly<br><code>Config Param: INSTANT_STATE_TIMELINE_SERVER_BASED</code><br><code>Since Version: 1.0.0</code></td></tr><tr><td><a href="#hoodieinstant_statetimeline_server_basedforce_refreshrequestnumber">hoodie.instant_state.timeline_server_based.force_refresh.request.number</a></td><td>100</td><td>Number of requests to trigger instant state cache refreshing<br><code>Config Param: INSTANT_STATE_TIMELINE_SERVER_BASED_FORCE_REFRESH_REQUEST_NUMBER</code><br><code>Since Version: 1.0.0</code></td></tr><tr><td><a href="#hoodiewriteautoupgrade">hoodie.write.auto.upgrade</a></td><td>true</td><td>If enabled, writers automatically migrate the table to the specified write table version if the current table version is lower.<br><code>Config Param: AUTO_UPGRADE_VERSION</code><br><code>Since Version: 1.0.0</code></td></tr><tr><td><a href="#hoodiewriteconcurrencymode">hoodie.write.concurrency.mode</a></td><td>SINGLE_WRITER</td><td>org.apache.hudi.common.model.WriteConcurrencyMode: Concurrency modes for write operations.     SINGLE_WRITER(default): Only one active writer to the table. Maximizes throughput.     OPTIMISTIC_CONCURRENCY_CONTROL: Multiple writers can operate on the table with lazy conflict resolution using locks. This means that only one writer succeeds if multiple writers write to the same file group.     NON_BLOCKING_CONCURRENCY_CONTROL: Multiple writers can operate on the table with non-blocking conflict resolution. The writers can write into the same file group with the conflicts resolved automatically by the query reader and the compactor.<br><code>Config Param: WRITE_CONCURRENCY_MODE</code></td></tr><tr><td><a href="#hoodiewritetableversion">hoodie.write.table.version</a></td><td>8</td><td>The table version this writer is storing the table in. This should match the current table version.<br><code>Config Param: WRITE_TABLE_VERSION</code><br><code>Since Version: 1.0.0</code></td></tr></tbody></table>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="LOCK">Lock Configs<a href="#LOCK" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Configurations that control locking mechanisms required for concurrency control  between writers to a Hudi table. Concurrency between Hudi&#x27;s own table services  are auto managed internally.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="Common-Lock-Configurations">Common Lock Configurations<a href="#Common-Lock-Configurations" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p><a href="#Common-Lock-Configurations-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiewritelockheartbeat_interval_ms">hoodie.write.lock.heartbeat_interval_ms</a></td><td>60000</td><td>Heartbeat interval in ms, to send a heartbeat to indicate that hive client holding locks.<br><code>Config Param: LOCK_HEARTBEAT_INTERVAL_MS</code><br><code>Since Version: 0.15.0</code></td></tr></tbody></table>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="KEY_GENERATOR">Key Generator Configs<a href="#KEY_GENERATOR" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Hudi maintains keys (record key + partition path) for uniquely identifying a particular record. These configs allow developers to setup the Key generator class that extracts these out of incoming records.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="Key-Generator-Options">Key Generator Options<a href="#Key-Generator-Options" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p><a href="#Key-Generator-Options-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiedatasourcewritepartitionpathfield">hoodie.datasource.write.partitionpath.field</a></td><td>(N/A)</td><td>Partition path field. Value to be used at the partitionPath component of HoodieKey. Actual value obtained by invoking .toString()<br><code>Config Param: PARTITIONPATH_FIELD_NAME</code></td></tr><tr><td><a href="#hoodiedatasourcewriterecordkeyfield">hoodie.datasource.write.recordkey.field</a></td><td>(N/A)</td><td>Record key field. Value to be used as the <code>recordKey</code> component of <code>HoodieKey</code>. Actual value will be obtained by invoking .toString() on the field value. Nested fields can be specified using the dot notation eg: <code>a.b.c</code><br><code>Config Param: RECORDKEY_FIELD_NAME</code></td></tr><tr><td><a href="#hoodiedatasourcewritesecondarykeycolumn">hoodie.datasource.write.secondarykey.column</a></td><td>(N/A)</td><td>Columns that constitute the secondary key component. Actual value will be obtained by invoking .toString() on the field value. Nested fields can be specified using the dot notation eg: <code>a.b.c</code><br><code>Config Param: SECONDARYKEY_COLUMN_NAME</code></td></tr><tr><td><a href="#hoodiedatasourcewritehive_style_partitioning">hoodie.datasource.write.hive_style_partitioning</a></td><td>false</td><td>Flag to indicate whether to use Hive style partitioning. If set true, the names of partition folders follow &lt;partition_column_name&gt;=&lt;partition_value&gt; format. By default false (the names of partition folders are only partition values)<br><code>Config Param: HIVE_STYLE_PARTITIONING_ENABLE</code></td></tr></tbody></table>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="INDEX">Index Configs<a href="#INDEX" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Configurations that control indexing behavior, which tags incoming records as either inserts or updates to older records.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="Common-Index-Configs">Common Index Configs<a href="#Common-Index-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p><a href="#Common-Index-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodieexpressionindexfunction">hoodie.expression.index.function</a></td><td>(N/A)</td><td>Function to be used for building the expression index.<br><code>Config Param: INDEX_FUNCTION</code><br><code>Since Version: 1.0.0</code></td></tr><tr><td><a href="#hoodieindexname">hoodie.index.name</a></td><td>(N/A)</td><td>Name of the expression index. This is also used for the partition name in the metadata table.<br><code>Config Param: INDEX_NAME</code><br><code>Since Version: 1.0.0</code></td></tr><tr><td><a href="#hoodietablechecksum">hoodie.table.checksum</a></td><td>(N/A)</td><td>Index definition checksum is used to guard against partial writes in HDFS. It is added as the last entry in index.properties and then used to validate while reading table config.<br><code>Config Param: INDEX_DEFINITION_CHECKSUM</code><br><code>Since Version: 1.0.0</code></td></tr><tr><td><a href="#hoodieexpressionindextype">hoodie.expression.index.type</a></td><td>COLUMN_STATS</td><td>Type of the expression index. Default is <code>column_stats</code> if there are no functions and expressions in the command. Valid options could be BITMAP, COLUMN_STATS, LUCENE, etc. If index_type is not provided, and there are functions or expressions in the command then a expression index using column stats will be created.<br><code>Config Param: INDEX_TYPE</code><br><code>Since Version: 1.0.0</code></td></tr></tbody></table>
<hr>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="Common-Index-Configs">Common Index Configs<a href="#Common-Index-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p><a href="#Common-Index-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodieindextype">hoodie.index.type</a></td><td>(N/A)</td><td>org.apache.hudi.index.HoodieIndex$IndexType: Determines how input records are indexed, i.e., looked up based on the key for the location in the existing table. Default is SIMPLE on Spark engine, and INMEMORY on Flink and Java engines.     HBASE: uses an external managed Apache HBase table to store record key to location mapping. HBase index is a global index, enforcing key uniqueness across all partitions in the table.     INMEMORY: Uses in-memory hashmap in Spark and Java engine and Flink in-memory state in Flink for indexing.     BLOOM: Employs bloom filters built out of the record keys, optionally also pruning candidate files using record key ranges. Key uniqueness is enforced inside partitions.     GLOBAL_BLOOM: Employs bloom filters built out of the record keys, optionally also pruning candidate files using record key ranges. Key uniqueness is enforced across all partitions in the table.      SIMPLE: Performs a lean join of the incoming update/delete records against keys extracted from the table on storage.Key uniqueness is enforced inside partitions.     GLOBAL_SIMPLE: Performs a lean join of the incoming update/delete records against keys extracted from the table on storage.Key uniqueness is enforced across all partitions in the table.     BUCKET: locates the file group containing the record fast by using bucket hashing, particularly beneficial in large scale. Use <code>hoodie.index.bucket.engine</code> to choose bucket engine type, i.e., how buckets are generated.     FLINK_STATE: Internal Config for indexing based on Flink state.     RECORD_INDEX: Index which saves the record key to location mappings in the HUDI Metadata Table. Record index is a global index, enforcing key uniqueness across all partitions in the table. Supports sharding to achieve very high scale.<br><code>Config Param: INDEX_TYPE</code></td></tr><tr><td><a href="#hoodiebucketindexquerypruning">hoodie.bucket.index.query.pruning</a></td><td>true</td><td>Control if table with bucket index use bucket query or not<br><code>Config Param: BUCKET_QUERY_INDEX</code></td></tr></tbody></table>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="META_SYNC">Metastore and Catalog Sync Configs<a href="#META_SYNC" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>Configurations used by the Hudi to sync metadata to external metastores and catalogs.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="Common-Metadata-Sync-Configs">Common Metadata Sync Configs<a href="#Common-Metadata-Sync-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p><a href="#Common-Metadata-Sync-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiedatasourcemetasyncenable">hoodie.datasource.meta.sync.enable</a></td><td>false</td><td>Enable Syncing the Hudi Table with an external meta store or data catalog.<br><code>Config Param: META_SYNC_ENABLED</code></td></tr></tbody></table>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="Glue-catalog-sync-based-client-Configurations">Glue catalog sync based client Configurations<a href="#Glue-catalog-sync-based-client-Configurations" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Configs that control Glue catalog sync based client.</p>
<p><a href="#Glue-catalog-sync-based-client-Configurations-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiedatasourcemetasyncgluepartition_index_fields">hoodie.datasource.meta.sync.glue.partition_index_fields</a></td><td></td><td>Specify the partitions fields to index on aws glue. Separate the fields by semicolon. By default, when the feature is enabled, all the partition will be indexed. You can create up to three indexes, separate them by comma. Eg: col1;col2;col3,col2,col3<br><code>Config Param: META_SYNC_PARTITION_INDEX_FIELDS</code><br><code>Since Version: 0.15.0</code></td></tr><tr><td><a href="#hoodiedatasourcemetasyncgluepartition_index_fieldsenable">hoodie.datasource.meta.sync.glue.partition_index_fields.enable</a></td><td>false</td><td>Enable aws glue partition index feature, to speedup partition based query pattern<br><code>Config Param: META_SYNC_PARTITION_INDEX_FIELDS_ENABLE</code><br><code>Since Version: 0.15.0</code></td></tr></tbody></table>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="BigQuery-Sync-Configs">BigQuery Sync Configs<a href="#BigQuery-Sync-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Configurations used by the Hudi to sync metadata to Google BigQuery.</p>
<p><a href="#BigQuery-Sync-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiedatasourcemetasyncenable">hoodie.datasource.meta.sync.enable</a></td><td>false</td><td>Enable Syncing the Hudi Table with an external meta store or data catalog.<br><code>Config Param: META_SYNC_ENABLED</code></td></tr></tbody></table>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="Hive-Sync-Configs">Hive Sync Configs<a href="#Hive-Sync-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Configurations used by the Hudi to sync metadata to Hive Metastore.</p>
<p><a href="#Hive-Sync-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiedatasourcehive_syncmode">hoodie.datasource.hive_sync.mode</a></td><td>(N/A)</td><td>Mode to choose for Hive ops. Valid values are hms, jdbc and hiveql.<br><code>Config Param: HIVE_SYNC_MODE</code></td></tr><tr><td><a href="#hoodiedatasourcehive_syncenable">hoodie.datasource.hive_sync.enable</a></td><td>false</td><td>When set to true, register/sync the table to Apache Hive metastore.<br><code>Config Param: HIVE_SYNC_ENABLED</code></td></tr><tr><td><a href="#hoodiedatasourcehive_syncjdbcurl">hoodie.datasource.hive_sync.jdbcurl</a></td><td>jdbc:hive2://localhost:10000</td><td>Hive metastore url<br><code>Config Param: HIVE_URL</code></td></tr><tr><td><a href="#hoodiedatasourcehive_syncmetastoreuris">hoodie.datasource.hive_sync.metastore.uris</a></td><td>thrift://localhost:9083</td><td>Hive metastore url<br><code>Config Param: METASTORE_URIS</code></td></tr><tr><td><a href="#hoodiedatasourcemetasyncenable">hoodie.datasource.meta.sync.enable</a></td><td>false</td><td>Enable Syncing the Hudi Table with an external meta store or data catalog.<br><code>Config Param: META_SYNC_ENABLED</code></td></tr></tbody></table>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="Global-Hive-Sync-Configs">Global Hive Sync Configs<a href="#Global-Hive-Sync-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Global replication configurations used by the Hudi to sync metadata to Hive Metastore.</p>
<p><a href="#Global-Hive-Sync-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiedatasourcehive_syncmode">hoodie.datasource.hive_sync.mode</a></td><td>(N/A)</td><td>Mode to choose for Hive ops. Valid values are hms, jdbc and hiveql.<br><code>Config Param: HIVE_SYNC_MODE</code></td></tr><tr><td><a href="#hoodiedatasourcehive_syncenable">hoodie.datasource.hive_sync.enable</a></td><td>false</td><td>When set to true, register/sync the table to Apache Hive metastore.<br><code>Config Param: HIVE_SYNC_ENABLED</code></td></tr><tr><td><a href="#hoodiedatasourcehive_syncjdbcurl">hoodie.datasource.hive_sync.jdbcurl</a></td><td>jdbc:hive2://localhost:10000</td><td>Hive metastore url<br><code>Config Param: HIVE_URL</code></td></tr><tr><td><a href="#hoodiedatasourcehive_syncmetastoreuris">hoodie.datasource.hive_sync.metastore.uris</a></td><td>thrift://localhost:9083</td><td>Hive metastore url<br><code>Config Param: METASTORE_URIS</code></td></tr><tr><td><a href="#hoodiedatasourcemetasyncenable">hoodie.datasource.meta.sync.enable</a></td><td>false</td><td>Enable Syncing the Hudi Table with an external meta store or data catalog.<br><code>Config Param: META_SYNC_ENABLED</code></td></tr></tbody></table>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="DataHub-Sync-Configs">DataHub Sync Configs<a href="#DataHub-Sync-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Configurations used by the Hudi to sync metadata to DataHub.</p>
<p><a href="#DataHub-Sync-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiedatasourcemetasyncenable">hoodie.datasource.meta.sync.enable</a></td><td>false</td><td>Enable Syncing the Hudi Table with an external meta store or data catalog.<br><code>Config Param: META_SYNC_ENABLED</code></td></tr></tbody></table>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="METRICS">Metrics Configs<a href="#METRICS" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>These set of configs are used to enable monitoring and reporting of key Hudi stats and metrics.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="Metrics-Configurations">Metrics Configurations<a href="#Metrics-Configurations" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Enables reporting on Hudi metrics. Hudi publishes metrics on every commit, clean, rollback etc. The following sections list the supported reporters.</p>
<p><a href="#Metrics-Configurations-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiemetricson">hoodie.metrics.on</a></td><td>false</td><td>Turn on/off metrics reporting. off by default.<br><code>Config Param: TURN_METRICS_ON</code><br><code>Since Version: 0.5.0</code></td></tr><tr><td><a href="#hoodiemetricsreportertype">hoodie.metrics.reporter.type</a></td><td>GRAPHITE</td><td>Type of metrics reporter.<br><code>Config Param: METRICS_REPORTER_TYPE_VALUE</code><br><code>Since Version: 0.5.0</code></td></tr><tr><td><a href="#hoodiemetricscompactionlogblockson">hoodie.metricscompaction.log.blocks.on</a></td><td>false</td><td>Turn on/off metrics reporting for log blocks with compaction commit. off by default.<br><code>Config Param: TURN_METRICS_COMPACTION_LOG_BLOCKS_ON</code><br><code>Since Version: 0.14.0</code></td></tr></tbody></table>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="Metrics-Configurations-for-M3">Metrics Configurations for M3<a href="#Metrics-Configurations-for-M3" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Enables reporting on Hudi metrics using M3.  Hudi publishes metrics on every commit, clean, rollback etc.</p>
<p><a href="#Metrics-Configurations-for-M3-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiemetricsm3env">hoodie.metrics.m3.env</a></td><td>production</td><td>M3 tag to label the environment (defaults to &#x27;production&#x27;), applied to all metrics.<br><code>Config Param: M3_ENV</code><br><code>Since Version: 0.15.0</code></td></tr><tr><td><a href="#hoodiemetricsm3host">hoodie.metrics.m3.host</a></td><td>localhost</td><td>M3 host to connect to.<br><code>Config Param: M3_SERVER_HOST_NAME</code><br><code>Since Version: 0.15.0</code></td></tr><tr><td><a href="#hoodiemetricsm3port">hoodie.metrics.m3.port</a></td><td>9052</td><td>M3 port to connect to.<br><code>Config Param: M3_SERVER_PORT_NUM</code><br><code>Since Version: 0.15.0</code></td></tr><tr><td><a href="#hoodiemetricsm3service">hoodie.metrics.m3.service</a></td><td>hoodie</td><td>M3 tag to label the service name (defaults to &#x27;hoodie&#x27;), applied to all metrics.<br><code>Config Param: M3_SERVICE</code><br><code>Since Version: 0.15.0</code></td></tr><tr><td><a href="#hoodiemetricsm3tags">hoodie.metrics.m3.tags</a></td><td></td><td>Optional M3 tags applied to all metrics.<br><code>Config Param: M3_TAGS</code><br><code>Since Version: 0.15.0</code></td></tr></tbody></table>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="KAFKA_CONNECT">Kafka Connect Configs<a href="#KAFKA_CONNECT" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>These set of configs are used for Kafka Connect Sink Connector for writing Hudi Tables</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="Kafka-Sink-Connect-Configurations">Kafka Sink Connect Configurations<a href="#Kafka-Sink-Connect-Configurations" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Configurations for Kafka Connect Sink Connector for Hudi.</p>
<p><a href="#Kafka-Sink-Connect-Configurations-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#bootstrapservers">bootstrap.servers</a></td><td>localhost:9092</td><td>The bootstrap servers for the Kafka Cluster.<br><code>Config Param: KAFKA_BOOTSTRAP_SERVERS</code></td></tr></tbody></table>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="HUDI_STREAMER">Hudi Streamer Configs<a href="#HUDI_STREAMER" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h2>
<p>These set of configs are used for Hudi Streamer utility which provides the way to ingest from different sources such as DFS or Kafka.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="Hudi-Streamer-Configs">Hudi Streamer Configs<a href="#Hudi-Streamer-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p><a href="#Hudi-Streamer-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiestreamersourcekafkatopic">hoodie.streamer.source.kafka.topic</a></td><td>(N/A)</td><td>Kafka topic name. The config is specific to HoodieMultiTableStreamer<br><code>Config Param: KAFKA_TOPIC</code></td></tr></tbody></table>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="Hudi-Streamer-SQL-Transformer-Configs">Hudi Streamer SQL Transformer Configs<a href="#Hudi-Streamer-SQL-Transformer-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Configurations controlling the behavior of SQL transformer in Hudi Streamer.</p>
<p><a href="#Hudi-Streamer-SQL-Transformer-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiestreamertransformersql">hoodie.streamer.transformer.sql</a></td><td>(N/A)</td><td>SQL Query to be executed during write<br><code>Config Param: TRANSFORMER_SQL</code></td></tr><tr><td><a href="#hoodiestreamertransformersqlfile">hoodie.streamer.transformer.sql.file</a></td><td>(N/A)</td><td>File with a SQL script to be executed during write<br><code>Config Param: TRANSFORMER_SQL_FILE</code></td></tr></tbody></table>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="DELTA_STREAMER_SOURCE">Hudi Streamer Source Configs<a href="#DELTA_STREAMER_SOURCE" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Configurations controlling the behavior of reading source data.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="DFS-Path-Selector-Configs">DFS Path Selector Configs<a href="#DFS-Path-Selector-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>Configurations controlling the behavior of path selector for DFS source in Hudi Streamer.</p>
<p><a href="#DFS-Path-Selector-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiestreamersourcedfsroot">hoodie.streamer.source.dfs.root</a></td><td>(N/A)</td><td>Root path of the source on DFS<br><code>Config Param: ROOT_INPUT_PATH</code></td></tr></tbody></table>
<hr>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="Hudi-Incremental-Source-Configs">Hudi Incremental Source Configs<a href="#Hudi-Incremental-Source-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>Configurations controlling the behavior of incremental pulling from a Hudi table as a source in Hudi Streamer.</p>
<p><a href="#Hudi-Incremental-Source-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiestreamersourcehoodieincrpath">hoodie.streamer.source.hoodieincr.path</a></td><td>(N/A)</td><td>Base-path for the source Hudi table<br><code>Config Param: HOODIE_SRC_BASE_PATH</code></td></tr></tbody></table>
<hr>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="Kafka-Source-Configs">Kafka Source Configs<a href="#Kafka-Source-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>Configurations controlling the behavior of Kafka source in Hudi Streamer.</p>
<p><a href="#Kafka-Source-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiestreamersourcekafkatopic">hoodie.streamer.source.kafka.topic</a></td><td>(N/A)</td><td>Kafka topic name.<br><code>Config Param: KAFKA_TOPIC_NAME</code></td></tr><tr><td><a href="#hoodiestreamersourcekafkaprotovaluedeserializerclass">hoodie.streamer.source.kafka.proto.value.deserializer.class</a></td><td>org.apache.kafka.common.serialization.ByteArrayDeserializer</td><td>Kafka Proto Payload Deserializer Class<br><code>Config Param: KAFKA_PROTO_VALUE_DESERIALIZER_CLASS</code><br><code>Since Version: 0.15.0</code></td></tr></tbody></table>
<hr>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="Pulsar-Source-Configs">Pulsar Source Configs<a href="#Pulsar-Source-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>Configurations controlling the behavior of Pulsar source in Hudi Streamer.</p>
<p><a href="#Pulsar-Source-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiestreamersourcepulsartopic">hoodie.streamer.source.pulsar.topic</a></td><td>(N/A)</td><td>Name of the target Pulsar topic to source data from<br><code>Config Param: PULSAR_SOURCE_TOPIC_NAME</code></td></tr><tr><td><a href="#hoodiestreamersourcepulsarendpointadminurl">hoodie.streamer.source.pulsar.endpoint.admin.url</a></td><td><a href="http://localhost:8080" target="_blank" rel="noopener noreferrer">http://localhost:8080</a></td><td>URL of the target Pulsar endpoint (of the form &#x27;pulsar://host<!-- -->:port<!-- -->&#x27;<br><code>Config Param: PULSAR_SOURCE_ADMIN_ENDPOINT_URL</code></td></tr><tr><td><a href="#hoodiestreamersourcepulsarendpointserviceurl">hoodie.streamer.source.pulsar.endpoint.service.url</a></td><td>pulsar://localhost:6650</td><td>URL of the target Pulsar endpoint (of the form &#x27;pulsar://host<!-- -->:port<!-- -->&#x27;<br><code>Config Param: PULSAR_SOURCE_SERVICE_ENDPOINT_URL</code></td></tr></tbody></table>
<hr>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="S3-Source-Configs">S3 Source Configs<a href="#S3-Source-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>Configurations controlling the behavior of S3 source in Hudi Streamer.</p>
<p><a href="#S3-Source-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiestreamers3sourcequeueurl">hoodie.streamer.s3.source.queue.url</a></td><td>(N/A)</td><td>Queue url for cloud object events<br><code>Config Param: S3_SOURCE_QUEUE_URL</code></td></tr></tbody></table>
<hr>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="File-based-SQL-Source-Configs">File-based SQL Source Configs<a href="#File-based-SQL-Source-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>Configurations controlling the behavior of File-based SQL Source in Hudi Streamer.</p>
<p><a href="#File-based-SQL-Source-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiestreamersourcesqlfile">hoodie.streamer.source.sql.file</a></td><td>(N/A)</td><td>SQL file path containing the SQL query to read source data.<br><code>Config Param: SOURCE_SQL_FILE</code><br><code>Since Version: 0.14.0</code></td></tr></tbody></table>
<hr>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="SQL-Source-Configs">SQL Source Configs<a href="#SQL-Source-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>Configurations controlling the behavior of SQL source in Hudi Streamer.</p>
<p><a href="#SQL-Source-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiestreamersourcesqlsqlquery">hoodie.streamer.source.sql.sql.query</a></td><td>(N/A)</td><td>SQL query for fetching source data.<br><code>Config Param: SOURCE_SQL</code></td></tr></tbody></table>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="SCHEMA_PROVIDER">Hudi Streamer Schema Provider Configs<a href="#SCHEMA_PROVIDER" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h3>
<p>Configurations that control the schema provider for Hudi Streamer.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="Hudi-Streamer-Schema-Provider-Configs">Hudi Streamer Schema Provider Configs<a href="#Hudi-Streamer-Schema-Provider-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p><a href="#Hudi-Streamer-Schema-Provider-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiestreamerschemaproviderregistrytargetUrl">hoodie.streamer.schemaprovider.registry.targetUrl</a></td><td>(N/A)</td><td>The schema of the target you are writing to e.g. <a href="https://foo:bar@schemaregistry.org" target="_blank" rel="noopener noreferrer">https://foo:bar@schemaregistry.org</a><br><code>Config Param: TARGET_SCHEMA_REGISTRY_URL</code></td></tr><tr><td><a href="#hoodiestreamerschemaproviderregistryurl">hoodie.streamer.schemaprovider.registry.url</a></td><td>(N/A)</td><td>The schema of the source you are reading from e.g. <a href="https://foo:bar@schemaregistry.org" target="_blank" rel="noopener noreferrer">https://foo:bar@schemaregistry.org</a><br><code>Config Param: SRC_SCHEMA_REGISTRY_URL</code></td></tr></tbody></table>
<hr>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="File-based-Schema-Provider-Configs">File-based Schema Provider Configs<a href="#File-based-Schema-Provider-Configs" class="hash-link" aria-label="Direct link to heading" title="Direct link to heading">​</a></h4>
<p>Configurations for file-based schema provider.</p>
<p><a href="#File-based-Schema-Provider-Configs-basic-configs"><strong>Basic Configs</strong></a></p>
<table><thead><tr><th>Config Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><a href="#hoodiestreamerschemaprovidersourceschemafile">hoodie.streamer.schemaprovider.source.schema.file</a></td><td>(N/A)</td><td>The schema of the source you are reading from<br><code>Config Param: SOURCE_SCHEMA_FILE</code></td></tr><tr><td><a href="#hoodiestreamerschemaprovidertargetschemafile">hoodie.streamer.schemaprovider.target.schema.file</a></td><td>(N/A)</td><td>The schema of the target you are writing to<br><code>Config Param: TARGET_SCHEMA_FILE</code></td></tr></tbody></table>
<hr></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/apache/hudi/tree/asf-site/website/versioned_docs/version-1.0.2/basic_configurations.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/cn/docs/flink_tuning"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Flink Tuning Guide</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/cn/docs/configurations"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">配置</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#TABLE_CONFIG" class="table-of-contents__link toc-highlight">Hudi Table Config</a><ul><li><a href="#Hudi-Table-Basic-Configs" class="table-of-contents__link toc-highlight">Hudi Table Basic Configs</a></li></ul></li><li><a href="#SPARK_DATASOURCE" class="table-of-contents__link toc-highlight">Spark Datasource Configs</a><ul><li><a href="#Read-Options" class="table-of-contents__link toc-highlight">Read Options</a></li><li><a href="#Write-Options" class="table-of-contents__link toc-highlight">Write Options</a></li></ul></li><li><a href="#FLINK_SQL" class="table-of-contents__link toc-highlight">Flink Sql Configs</a><ul><li><a href="#Flink-Options" class="table-of-contents__link toc-highlight">Flink Options</a></li></ul></li><li><a href="#WRITE_CLIENT" class="table-of-contents__link toc-highlight">Write Client Configs</a><ul><li><a href="#Common-Configurations" class="table-of-contents__link toc-highlight">Common Configurations</a></li><li><a href="#Metadata-Configs" class="table-of-contents__link toc-highlight">Metadata Configs</a></li><li><a href="#Storage-Configs" class="table-of-contents__link toc-highlight">Storage Configs</a></li><li><a href="#Archival-Configs" class="table-of-contents__link toc-highlight">Archival Configs</a></li><li><a href="#Bootstrap-Configs" class="table-of-contents__link toc-highlight">Bootstrap Configs</a></li><li><a href="#Clean-Configs" class="table-of-contents__link toc-highlight">Clean Configs</a></li><li><a href="#Clustering-Configs" class="table-of-contents__link toc-highlight">Clustering Configs</a></li><li><a href="#Compaction-Configs" class="table-of-contents__link toc-highlight">Compaction Configs</a></li><li><a href="#Error-table-Configs" class="table-of-contents__link toc-highlight">Error table Configs</a></li><li><a href="#Write-Configurations" class="table-of-contents__link toc-highlight">Write Configurations</a></li><li><a href="#LOCK" class="table-of-contents__link toc-highlight">Lock Configs</a></li><li><a href="#KEY_GENERATOR" class="table-of-contents__link toc-highlight">Key Generator Configs</a></li><li><a href="#INDEX" class="table-of-contents__link toc-highlight">Index Configs</a></li></ul></li><li><a href="#META_SYNC" class="table-of-contents__link toc-highlight">Metastore and Catalog Sync Configs</a><ul><li><a href="#Common-Metadata-Sync-Configs" class="table-of-contents__link toc-highlight">Common Metadata Sync Configs</a></li><li><a href="#Glue-catalog-sync-based-client-Configurations" class="table-of-contents__link toc-highlight">Glue catalog sync based client Configurations</a></li><li><a href="#BigQuery-Sync-Configs" class="table-of-contents__link toc-highlight">BigQuery Sync Configs</a></li><li><a href="#Hive-Sync-Configs" class="table-of-contents__link toc-highlight">Hive Sync Configs</a></li><li><a href="#Global-Hive-Sync-Configs" class="table-of-contents__link toc-highlight">Global Hive Sync Configs</a></li><li><a href="#DataHub-Sync-Configs" class="table-of-contents__link toc-highlight">DataHub Sync Configs</a></li></ul></li><li><a href="#METRICS" class="table-of-contents__link toc-highlight">Metrics Configs</a><ul><li><a href="#Metrics-Configurations" class="table-of-contents__link toc-highlight">Metrics Configurations</a></li><li><a href="#Metrics-Configurations-for-M3" class="table-of-contents__link toc-highlight">Metrics Configurations for M3</a></li></ul></li><li><a href="#KAFKA_CONNECT" class="table-of-contents__link toc-highlight">Kafka Connect Configs</a><ul><li><a href="#Kafka-Sink-Connect-Configurations" class="table-of-contents__link toc-highlight">Kafka Sink Connect Configurations</a></li></ul></li><li><a href="#HUDI_STREAMER" class="table-of-contents__link toc-highlight">Hudi Streamer Configs</a><ul><li><a href="#Hudi-Streamer-Configs" class="table-of-contents__link toc-highlight">Hudi Streamer Configs</a></li><li><a href="#Hudi-Streamer-SQL-Transformer-Configs" class="table-of-contents__link toc-highlight">Hudi Streamer SQL Transformer Configs</a></li><li><a href="#DELTA_STREAMER_SOURCE" class="table-of-contents__link toc-highlight">Hudi Streamer Source Configs</a></li><li><a href="#SCHEMA_PROVIDER" class="table-of-contents__link toc-highlight">Hudi Streamer Schema Provider Configs</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">About</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/cn/blog/2021/07/21/streaming-data-lake-platform">Our Vision</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/concepts">Concepts</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/community/team">Team</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/releases/release-1.0.2">Releases</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/releases/download">Download</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/powered-by">Who&#x27;s Using</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Learn</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/cn/docs/quick-start-guide">Quick Start</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/learn/tutorial-series">Tutorial Series</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/blog">Blog</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/talks">Talks</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/videos">Video Guides</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/faq">FAQ</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Hudi On Cloud</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/cn/docs/s3_hoodie">AWS</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/gcs_hoodie">Google Cloud</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/oss_hoodie">Alibaba Cloud</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/azure_hoodie">Microsoft Azure</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/cos_hoodie">Tencent Cloud</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/ibm_cos_hoodie">IBM Cloud</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/cn/community/get-involved">Get Involved</a></li><li class="footer__item"><a href="https://join.slack.com/t/apache-hudi/shared_invite/zt-33fabmxb7-Q7QSUtNOHYCwUdYM8LbauA" target="_blank" rel="noopener noreferrer" class="footer__link-item">Slack<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://github.com/apache/hudi" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://x.com/ApacheHudi" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://www.youtube.com/channel/UCs7AhE0BWaEPZSChrBR-Muw" target="_blank" rel="noopener noreferrer" class="footer__link-item">YouTube<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://www.linkedin.com/company/apache-hudi/?viewAsMember=true" target="_blank" rel="noopener noreferrer" class="footer__link-item">Linkedin<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="mailto:dev-subscribe@hudi.apache.org?Subject=SubscribeToHudi" target="_blank" rel="noopener noreferrer" class="footer__link-item">Mailing List</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Apache</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.apache.org/events/current-event" target="_blank" rel="noopener noreferrer" class="footer__link-item">Events</a></li><li class="footer__item"><a href="https://www.apache.org/foundation/thanks.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Thanks</a></li><li class="footer__item"><a href="https://www.apache.org/licenses" target="_blank" rel="noopener noreferrer" class="footer__link-item">License</a></li><li class="footer__item"><a href="https://www.apache.org/security" target="_blank" rel="noopener noreferrer" class="footer__link-item">Security</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/asf/privacy">Privacy</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/asf/telemetry">Telemetry</a></li><li class="footer__item"><a href="https://www.apache.org/foundation/sponsorship.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Sponsorship</a></li><li class="footer__item"><a href="https://www.apache.org" target="_blank" rel="noopener noreferrer" class="footer__link-item">Foundation</a></li></ul></div></div><div class="footer__bottom text--center"><div class="margin-bottom--sm"><a href="https://hudi.apache.org/" rel="noopener noreferrer" class="footerLogoLink_BH7S"><img src="/cn/assets/images/logo-big.png" alt="Apache Hudi™" class="footer__logo themedComponent_mlkZ themedComponent--light_NVdE"><img src="/cn/assets/images/logo-big.png" alt="Apache Hudi™" class="footer__logo themedComponent_mlkZ themedComponent--dark_xIcU"></a></div><div class="footer__copyright">Copyright © 2021 <a href="https://apache.org">The Apache Software Foundation</a>, Licensed under the <a href="https://www.apache.org/licenses/LICENSE-2.0"> Apache License, Version 2.0</a>.
      Hudi, Apache and the Apache feather logo are trademarks of The Apache Software Foundation. <a href="/docs/privacy">Privacy Policy</a></div></div></div></footer></div>
</body>
</html>