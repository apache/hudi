<!doctype html>
<html class="docs-version-0.9.0" lang="cn" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.14">
<link rel="alternate" type="application/rss+xml" href="/cn/blog/rss.xml" title="Apache Hudi: User-Facing Analytics RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/cn/blog/atom.xml" title="Apache Hudi: User-Facing Analytics Atom Feed">
<link rel="alternate" type="application/json" href="/cn/blog/feed.json" title="Apache Hudi: User-Facing Analytics JSON Feed">
<link rel="search" type="application/opensearchdescription+xml" title="Apache Hudi" href="/cn/opensearch.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Comfortaa|Ubuntu|Roboto|Source+Code+Pro">
<link rel="stylesheet" href="https://at-ui.github.io/feather-font/css/iconfont.css"><title data-react-helmet="true">配置 | Apache Hudi</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://hudi.apache.org/cn/docs/0.9.0/configurations"><meta data-react-helmet="true" name="docsearch:language" content="cn"><meta data-react-helmet="true" name="docsearch:version" content="0.9.0"><meta data-react-helmet="true" name="docsearch:docusaurus_tag" content="docs-default-0.9.0"><meta data-react-helmet="true" property="og:title" content="配置 | Apache Hudi"><meta data-react-helmet="true" name="description" content="该页面介绍了几种配置写入或读取Hudi数据集的作业的方法。"><meta data-react-helmet="true" property="og:description" content="该页面介绍了几种配置写入或读取Hudi数据集的作业的方法。"><meta data-react-helmet="true" name="keywords" content="garbage collection,hudi,jvm,configs,tuning"><link data-react-helmet="true" rel="icon" href="/cn/assets/images/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://hudi.apache.org/cn/docs/0.9.0/configurations"><link data-react-helmet="true" rel="alternate" href="https://hudi.apache.org/docs/0.9.0/configurations" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://hudi.apache.org/cn/docs/0.9.0/configurations" hreflang="cn"><link data-react-helmet="true" rel="alternate" href="https://hudi.apache.org/docs/0.9.0/configurations" hreflang="x-default"><link data-react-helmet="true" rel="preconnect" href="https://BH4D9OD16A-dsn.algolia.net" crossorigin="anonymous"><link rel="stylesheet" href="/cn/assets/css/styles.286d56f1.css">
<link rel="preload" href="/cn/assets/js/runtime~main.2900c60c.js" as="script">
<link rel="preload" href="/cn/assets/js/main.c21a63e5.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><div class="announcementBar_axC9" role="banner"><div class="announcementBarPlaceholder_xYHE"></div><div class="announcementBarContent_6uhP">⭐️ If you like Apache Hudi, give it a star on <a target="_blank" rel="noopener noreferrer" href="https://github.com/apache/hudi">GitHub</a>! ⭐</div><button type="button" class="clean-btn close announcementBarClose_A3A1" aria-label="Close"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/cn/"><div class="navbar__logo"><img src="/cn/assets/images/hudi.png" alt="Apache Hudi" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/cn/assets/images/hudi.png" alt="Apache Hudi" class="themedImage_TMUO themedImage--dark_uzRr"></div></a><a class="navbar__item navbar__link" href="/cn/docs/overview">Docs</a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" class="navbar__link">Learn</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/cn/blog">Blog</a></li><li><a class="dropdown__link" href="/cn/talks-articles">Talks &amp; Articles</a></li><li><a class="dropdown__link" href="/cn/learn/faq">FAQ</a></li><li><a href="https://cwiki.apache.org/confluence/display/HUDI" target="_blank" rel="noopener noreferrer" class="dropdown__link"><span>Technical Wiki<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" class="navbar__link">Contribute</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/cn/contribute/how-to-contribute">How to Contribute</a></li><li><a class="dropdown__link" href="/cn/contribute/developer-setup">Developer Setup</a></li><li><a class="dropdown__link" href="/cn/contribute/rfc-process">RFC Process</a></li><li><a class="dropdown__link" href="/cn/contribute/report-security-issues">Report Security Issues</a></li><li><a href="https://issues.apache.org/jira/projects/HUDI/summary" target="_blank" rel="noopener noreferrer" class="dropdown__link"><span>Report Issues<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" class="navbar__link">Community</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/cn/community/get-involved">Get Involved</a></li><li><a class="dropdown__link" href="/cn/community/syncs">Community Syncs</a></li><li><a class="dropdown__link" href="/cn/community/team">Team</a></li></ul></div><a class="navbar__item navbar__link" href="/cn/powered-by">Who&#x27;s Using</a><a class="navbar__item navbar__link" href="/cn/roadmap">Roadmap</a><a class="navbar__item navbar__link" href="/cn/releases/download">Download</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a class="navbar__link" href="/cn/docs/0.9.0/overview">0.9.0</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/cn/docs/next/configurations">Next</a></li><li><a class="dropdown__link" href="/cn/docs/configurations">0.11.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.10.1/configurations">0.10.1</a></li><li><a class="dropdown__link" href="/cn/docs/0.10.0/configurations">0.10.0</a></li><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/cn/docs/0.9.0/configurations">0.9.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.8.0/configurations">0.8.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.7.0/configurations">0.7.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.6.0/configurations">0.6.0</a></li><li><a class="dropdown__link" href="/cn/docs/0.5.3/configurations">0.5.3</a></li><li><a class="dropdown__link" href="/cn/docs/0.5.2/configurations">0.5.2</a></li><li><a class="dropdown__link" href="/cn/docs/0.5.1/configurations">0.5.1</a></li><li><a class="dropdown__link" href="/cn/docs/0.5.0/configurations">0.5.0</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" class="navbar__link"><span><svg viewBox="0 0 20 20" width="20" height="20" aria-hidden="true" class="iconLanguage_EbrZ"><path fill="currentColor" d="M19.753 10.909c-.624-1.707-2.366-2.726-4.661-2.726-.09 0-.176.002-.262.006l-.016-2.063 3.525-.607c.115-.019.133-.119.109-.231-.023-.111-.167-.883-.188-.976-.027-.131-.102-.127-.207-.109-.104.018-3.25.461-3.25.461l-.013-2.078c-.001-.125-.069-.158-.194-.156l-1.025.016c-.105.002-.164.049-.162.148l.033 2.307s-3.061.527-3.144.543c-.084.014-.17.053-.151.143.019.09.19 1.094.208 1.172.018.08.072.129.188.107l2.924-.504.035 2.018c-1.077.281-1.801.824-2.256 1.303-.768.807-1.207 1.887-1.207 2.963 0 1.586.971 2.529 2.328 2.695 3.162.387 5.119-3.06 5.769-4.715 1.097 1.506.256 4.354-2.094 5.98-.043.029-.098.129-.033.207l.619.756c.08.096.206.059.256.023 2.51-1.73 3.661-4.515 2.869-6.683zm-7.386 3.188c-.966-.121-.944-.914-.944-1.453 0-.773.327-1.58.876-2.156a3.21 3.21 0 011.229-.799l.082 4.277a2.773 2.773 0 01-1.243.131zm2.427-.553l.046-4.109c.084-.004.166-.01.252-.01.773 0 1.494.145 1.885.361.391.217-1.023 2.713-2.183 3.758zm-8.95-7.668a.196.196 0 00-.196-.145h-1.95a.194.194 0 00-.194.144L.008 16.916c-.017.051-.011.076.062.076h1.733c.075 0 .099-.023.114-.072l1.008-3.318h3.496l1.008 3.318c.016.049.039.072.113.072h1.734c.072 0 .078-.025.062-.076-.014-.05-3.083-9.741-3.494-11.04zm-2.618 6.318l1.447-5.25 1.447 5.25H3.226z"></path></svg><span>Chinese</span></span></a><ul class="dropdown__menu"><li><a href="/docs/0.9.0/configurations" target="_self" rel="noopener noreferrer" class="dropdown__link">English</a></li><li><a href="/cn/docs/0.9.0/configurations" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active">Chinese</a></li></ul></div><a href="https://github.com/apache/hudi" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><a href="https://twitter.com/ApacheHudi" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-twitter-link" aria-label="Hudi Twitter Handle"></a><a href="https://join.slack.com/t/apache-hudi/shared_invite/enQtODYyNDAxNzc5MTg2LTE5OTBlYmVhYjM0N2ZhOTJjOWM4YzBmMWU2MjZjMGE4NDc5ZDFiOGQ2N2VkYTVkNzU3ZDQ4OTI1NmFmYWQ0NzE" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-slack-link" aria-label="Hudi Slack Channel"></a><div class="searchBox_Utm0"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_lDyR"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_i9tI" type="button"></button><aside class="docSidebarContainer_0YBq"><div class="sidebar_a3j0"><nav class="menu thin-scrollbar menu_cyFh menuWithAnnouncementBar_+O1J"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cn/docs/0.9.0/overview">概念</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_TwRn" href="/cn/docs/0.9.0/quick-start-guide">Quick Start</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cn/docs/0.9.0/quick-start-guide">Spark Guide</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cn/docs/0.9.0/flink-quick-start-guide">Flink 指南</a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cn/docs/0.9.0/use_cases">使用案例</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cn/docs/0.9.0/writing_data">写入 Hudi 数据集</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cn/docs/0.9.0/schema_evolution">Schema Evolution</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cn/docs/0.9.0/concurrency_control">Concurrency Control</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cn/docs/0.9.0/querying_data">查询 Hudi 数据集</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/cn/docs/0.9.0/configurations">配置</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cn/docs/0.9.0/performance">性能</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cn/docs/0.9.0/deployment">管理 Hudi Pipelines</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_TwRn" href="/cn/docs/0.9.0/cloud">Storage Configurations</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_TwRn" href="/cn/docs/0.9.0/docker_demo">Resources</a></div></li></ul></nav></div></aside><main class="docMainContainer_r8cw"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_zHA2"><div class="theme-doc-version-banner alert alert--warning margin-bottom--md" role="alert"><div>This is documentation for <!-- -->Apache Hudi<!-- --> <b>0.9.0</b>, which is no longer actively maintained.</div><div class="margin-top--md">For up-to-date documentation, see the <b><a href="/cn/docs/configurations">latest version</a></b> (<!-- -->0.11.0<!-- -->).</div></div><div class="docItemContainer_oiyr"><article><span class="theme-doc-version-badge badge badge--secondary">Version: <!-- -->0.9.0</span><div class="tocCollapsible_aw-L theme-doc-toc-mobile tocMobile_Tx6Y"><button type="button" class="clean-btn tocCollapsibleButton_zr6a">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>配置</h1></header><p>该页面介绍了几种配置写入或读取Hudi数据集的作业的方法。
简而言之，您可以在几个级别上控制行为。</p><ul><li><strong><a href="#spark-datasource">Spark数据源配置</a></strong> : 这些配置控制Hudi Spark数据源，提供如下功能：
定义键和分区、选择写操作、指定如何合并记录或选择要读取的视图类型。</li><li><strong><a href="#writeclient-configs">WriteClient 配置</a></strong> : 在内部，Hudi数据源使用基于RDD的<code>HoodieWriteClient</code> API
真正执行对存储的写入。 这些配置可对文件大小、压缩（compression）、并行度、压缩（compaction）、写入模式、清理等底层方面进行完全控制。
尽管Hudi提供了合理的默认设置，但在不同情形下，可能需要对这些配置进行调整以针对特定的工作负载进行优化。</li><li><strong><a href="#PAYLOAD_CLASS_OPT_KEY">RecordPayload 配置</a></strong> : 这是Hudi提供的最底层的定制。
RecordPayload定义了如何根据传入的新记录和存储的旧记录来产生新值以进行插入更新。
Hudi提供了诸如<code>OverwriteWithLatestAvroPayload</code>的默认实现，该实现仅使用最新或最后写入的记录来更新存储。
在数据源和WriteClient级别，都可以将其重写为扩展<code>HoodieRecordPayload</code>类的自定义类。</li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="spark数据源配置">Spark数据源配置<a class="hash-link" href="#spark数据源配置" title="Direct link to heading">​</a></h2><p>可以通过将以下选项传递到<code>option(k,v)</code>方法中来配置使用数据源的Spark作业。
实际的数据源级别配置在下面列出。</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="写选项">写选项<a class="hash-link" href="#写选项" title="Direct link to heading">​</a></h3><p>另外，您可以使用<code>options()</code>或<code>option(k,v)</code>方法直接传递任何WriteClient级别的配置。</p><div class="codeBlockContainer_J+bg language-java theme-code-block"><div class="codeBlockContent_csEI java"><pre tabindex="0" class="prism-code language-java codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">inputDF.write()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">.format(&quot;org.apache.hudi&quot;)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">.options(clientOpts) // 任何Hudi客户端选项都可以传入</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">.option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), &quot;_row_key&quot;)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">.option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), &quot;partition&quot;)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">.option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), &quot;timestamp&quot;)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">.option(HoodieWriteConfig.TABLE_NAME, tableName)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">.mode(SaveMode.Append)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">.save(basePath);</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>用于通过<code>write.format.option(...)</code>写入数据集的选项</p><h4 class="anchor anchorWithStickyNavbar_y2LR" id="table_name_opt_key">TABLE_NAME_OPT_KEY<a class="hash-link" href="#table_name_opt_key" title="Direct link to heading">​</a></h4><p>  属性：<code>hoodie.datasource.write.table.name</code> <!-- -->[必须]<br></p><span>Hive表名，用于将数据集注册到其中。</span>#### OPERATION_OPT_KEY 属性：`hoodie.datasource.write.operation`, 默认值：`upsert`<br><span>是否为写操作进行插入更新、插入或批量插入。使用`bulkinsert`将新数据加载到表中，之后使用`upsert`或`insert`。 批量插入使用基于磁盘的写入路径来扩展以加载大量输入，而无需对其进行缓存。</span>#### STORAGE_TYPE_OPT_KEY 属性：`hoodie.datasource.write.storage.type`, 默认值：`COPY_ON_WRITE` <br><span>此写入的基础数据的存储类型。两次写入之间不能改变。</span>#### PRECOMBINE_FIELD_OPT_KEY 属性：`hoodie.datasource.write.precombine.field`, 默认值：`ts` <br><span>实际写入之前在preCombining中使用的字段。 当两个记录具有相同的键值时，我们将使用Object.compareTo(..)从precombine字段中选择一个值最大的记录。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="payload_class_opt_key">PAYLOAD_CLASS_OPT_KEY<a class="hash-link" href="#payload_class_opt_key" title="Direct link to heading">​</a></h4><p>  属性：<code>hoodie.datasource.write.payload.class</code>, 默认值：<code>org.apache.hudi.OverwriteWithLatestAvroPayload</code> <br></p><span>使用的有效载荷类。如果您想在插入更新或插入时使用自己的合并逻辑，请重写此方法。 这将使得`PRECOMBINE_FIELD_OPT_VAL`设置的任何值无效</span>#### RECORDKEY_FIELD_OPT_KEY 属性：`hoodie.datasource.write.recordkey.field`, 默认值：`uuid` <br><span>记录键字段。用作`HoodieKey`中`recordKey`部分的值。 实际值将通过在字段值上调用.toString()来获得。可以使用点符号指定嵌套字段，例如：`a.b.c`</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="partitionpath_field_opt_key">PARTITIONPATH_FIELD_OPT_KEY<a class="hash-link" href="#partitionpath_field_opt_key" title="Direct link to heading">​</a></h4><p>  属性：<code>hoodie.datasource.write.partitionpath.field</code>, 默认值：<code>partitionpath</code> <br></p><span>分区路径字段。用作`HoodieKey`中`partitionPath`部分的值。 通过调用.toString()获得实际的值</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="hive_style_partitioning_opt_key">HIVE_STYLE_PARTITIONING_OPT_KEY<a class="hash-link" href="#hive_style_partitioning_opt_key" title="Direct link to heading">​</a></h4><p>  属性：<code>hoodie.datasource.write.hive_style_partitioning</code>, 默认值：<code>false</code> <br></p><span>如果设置为true，则生成基于Hive格式的partition目录：`partition_column_name=partition_value`</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="keygenerator_class_opt_key">KEYGENERATOR_CLASS_OPT_KEY<a class="hash-link" href="#keygenerator_class_opt_key" title="Direct link to heading">​</a></h4><p>  属性：<code>hoodie.datasource.write.keygenerator.class</code> <br></p><span>键生成器类，实现从输入的`Row`对象中提取键。该配置优先级大于 `hoodie.datasource.write.keygenerator.type`, 用于使用用户自定义键生成器</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="keygenerator_type_opt_key">KEYGENERATOR_TYPE_OPT_KEY<a class="hash-link" href="#keygenerator_type_opt_key" title="Direct link to heading">​</a></h4><p>  属性: <code>hoodie.datasource.write.keygenerator.type</code>, 默认值: <code>SIMPLE</code> <br></p><span>键生成器类型，默认 `SIMPLE` 类型，该配置优先级低于 `hoodie.datasource.write.keygenerator.class`, 是推荐使用的配置方式</span>#### COMMIT_METADATA_KEYPREFIX_OPT_KEY 属性：`hoodie.datasource.write.commitmeta.key.prefix`, 默认值：`_` <br><span>以该前缀开头的选项键会自动添加到提交/增量提交的元数据中。 这对于与hudi时间轴一致的方式存储检查点信息很有用</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="insert_drop_dups_opt_key">INSERT_DROP_DUPS_OPT_KEY<a class="hash-link" href="#insert_drop_dups_opt_key" title="Direct link to heading">​</a></h4><p>  属性：<code>hoodie.datasource.write.insert.drop.duplicates</code>, 默认值：<code>false</code> <br></p><span>如果设置为true，则在插入操作期间从传入DataFrame中过滤掉所有重复记录。</span>#### HIVE_SYNC_ENABLED_OPT_KEY 属性：`hoodie.datasource.hive_sync.enable`, 默认值：`false` <br><span>设置为true时，将数据集注册并同步到Apache Hive Metastore</span>#### HIVE_DATABASE_OPT_KEY 属性：`hoodie.datasource.hive_sync.database`, 默认值：`default` <br><span>要同步到的数据库</span>#### HIVE_TABLE_OPT_KEY 属性：`hoodie.datasource.hive_sync.table`, [Required] <br><span>要同步到的表</span>#### HIVE_USER_OPT_KEY 属性：`hoodie.datasource.hive_sync.username`, 默认值：`hive` <br><span>要使用的Hive用户名</span>#### HIVE_PASS_OPT_KEY 属性：`hoodie.datasource.hive_sync.password`, 默认值：`hive` <br><span>要使用的Hive密码</span>#### HIVE_URL_OPT_KEY 属性：`hoodie.datasource.hive_sync.jdbcurl`, 默认值：`jdbc:hive2://localhost:10000` <br><span>Hive metastore url</span>#### HIVE_PARTITION_FIELDS_OPT_KEY 属性：`hoodie.datasource.hive_sync.partition_fields`, 默认值：` ` <br><span>数据集中用于确定Hive分区的字段。</span>#### HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY 属性：`hoodie.datasource.hive_sync.partition_extractor_class`, 默认值：`org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor` <br><span>用于将分区字段值提取到Hive分区列中的类。</span>#### HIVE_ASSUME_DATE_PARTITION_OPT_KEY 属性：`hoodie.datasource.hive_sync.assume_date_partitioning`, 默认值：`false` <br><span>假设分区格式是yyyy/mm/dd</span><h3 class="anchor anchorWithStickyNavbar_y2LR" id="读选项">读选项<a class="hash-link" href="#读选项" title="Direct link to heading">​</a></h3><p>用于通过<code>read.format.option(...)</code>读取数据集的选项</p><h4 class="anchor anchorWithStickyNavbar_y2LR" id="view_type_opt_key">VIEW_TYPE_OPT_KEY<a class="hash-link" href="#view_type_opt_key" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.datasource.view.type</code>, 默认值：<code>read_optimized</code> <br></p><span>是否需要以某种模式读取数据，增量模式（自InstantTime以来的新数据） （或）读优化模式（基于列数据获取最新视图） （或）实时模式（基于行和列数据获取最新视图）</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="begin_instanttime_opt_key">BEGIN_INSTANTTIME_OPT_KEY<a class="hash-link" href="#begin_instanttime_opt_key" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.datasource.read.begin.instanttime</code>, <!-- -->[在增量模式下必须]<!-- --> <br></p><span>开始增量提取数据的即时时间。这里的instanttime不必一定与时间轴上的即时相对应。 取出以`instant_time &gt; BEGIN_INSTANTTIME`写入的新数据。 例如：&#x27;20170901080000&#x27;将获取2017年9月1日08:00 AM之后写入的所有新数据。</span>#### END_INSTANTTIME_OPT_KEY 属性：`hoodie.datasource.read.end.instanttime`, 默认值：最新即时（即从开始即时获取所有新数据） <br><span>限制增量提取的数据的即时时间。取出以`instant_time &lt;= END_INSTANTTIME`写入的新数据。</span><h2 class="anchor anchorWithStickyNavbar_y2LR" id="writeclient-配置">WriteClient 配置<a class="hash-link" href="#writeclient-配置" title="Direct link to heading">​</a></h2><p>直接使用RDD级别api进行编程的Jobs可以构建一个<code>HoodieWriteConfig</code>对象，并将其传递给<code>HoodieWriteClient</code>构造函数。
HoodieWriteConfig可以使用以下构建器模式构建。</p><div class="codeBlockContainer_J+bg language-java theme-code-block"><div class="codeBlockContent_csEI java"><pre tabindex="0" class="prism-code language-java codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        .withPath(basePath)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        .forTable(tableName)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        .withSchema(schemaStr)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        .withProps(props) // 从属性文件传递原始k、v对。</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        .withCompactionConfig(HoodieCompactionConfig.newBuilder().withXXX(...).build())</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        .withIndexConfig(HoodieIndexConfig.newBuilder().withXXX(...).build())</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        ...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        .build();</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>以下各节介绍了写配置的不同方面，并解释了最重要的配置及其属性名称和默认值。</p><h4 class="anchor anchorWithStickyNavbar_y2LR" id="withpathhoodie_base_path">withPath(hoodie_base_path)<a class="hash-link" href="#withpathhoodie_base_path" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.base.path</code> <!-- -->[必须]<!-- --> <br></p><span>创建所有数据分区所依据的基本DFS路径。 始终在前缀中明确指明存储方式（例如hdfs://，s3://等）。 Hudi将有关提交、保存点、清理审核日志等的所有主要元数据存储在基本目录下的.hoodie目录中。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="withschemaschema_str">withSchema(schema_str)<a class="hash-link" href="#withschemaschema_str" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.avro.schema</code> <!-- -->[必须]<br></p><span>这是数据集的当前读取器的avro模式（schema）。 这是整个模式的字符串。HoodieWriteClient使用此模式传递到HoodieRecordPayload的实现，以从源格式转换为avro记录。 在更新过程中重写记录时也使用此模式。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="fortabletable_name">forTable(table_name)<a class="hash-link" href="#fortabletable_name" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.table.name</code> <!-- -->[必须]<!-- --> <br></p><span>数据集的表名，将用于在Hive中注册。每次运行需要相同。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="withbulkinsertparallelismbulk_insert_parallelism--1500">withBulkInsertParallelism(bulk_insert_parallelism = 1500)<a class="hash-link" href="#withbulkinsertparallelismbulk_insert_parallelism--1500" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.bulkinsert.shuffle.parallelism</code><br></p><span>批量插入旨在用于较大的初始导入，而此处的并行度决定了数据集中文件的初始数量。 调整此值以达到在初始导入期间所需的最佳尺寸。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="withparallelisminsert_shuffle_parallelism--1500-upsert_shuffle_parallelism--1500">withParallelism(insert_shuffle_parallelism = 1500, upsert_shuffle_parallelism = 1500)<a class="hash-link" href="#withparallelisminsert_shuffle_parallelism--1500-upsert_shuffle_parallelism--1500" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.insert.shuffle.parallelism</code>, <code>hoodie.upsert.shuffle.parallelism</code><br></p><span>最初导入数据后，此并行度将控制用于读取输入记录的初始并行度。 确保此值足够高，例如：1个分区用于1 GB的输入数据</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="combineinputon_insert--false-on_updatetrue">combineInput(on_insert = false, on_update=true)<a class="hash-link" href="#combineinputon_insert--false-on_updatetrue" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.combine.before.insert</code>, <code>hoodie.combine.before.upsert</code><br></p><span>在DFS中插入或更新之前先组合输入RDD并将多个部分记录合并为单个记录的标志</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="withwritestatusstoragelevellevel--memory_and_disk_ser">withWriteStatusStorageLevel(level = MEMORY_AND_DISK_SER)<a class="hash-link" href="#withwritestatusstoragelevellevel--memory_and_disk_ser" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.write.status.storage.level</code><br></p><span>HoodieWriteClient.insert和HoodieWriteClient.upsert返回一个持久的RDD[WriteStatus]， 这是因为客户端可以选择检查WriteStatus并根据失败选择是否提交。这是此RDD的存储级别的配置</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="withautocommitautocommit--true">withAutoCommit(autoCommit = true)<a class="hash-link" href="#withautocommitautocommit--true" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.auto.commit</code><br></p><span>插入和插入更新后，HoodieWriteClient是否应该自动提交。 客户端可以选择关闭自动提交，并在&quot;定义的成功条件&quot;下提交</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="withassumedatepartitioningassumedatepartitioning--false">withAssumeDatePartitioning(assumeDatePartitioning = false)<a class="hash-link" href="#withassumedatepartitioningassumedatepartitioning--false" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.assume.date.partitioning</code><br></p><span>HoodieWriteClient是否应该假设数据按日期划分，即从基本路径划分为三个级别。 这是支持&lt;0.3.1版本创建的表的一个补丁。最终将被删除</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="withconsistencycheckenabledenabled--false">withConsistencyCheckEnabled(enabled = false)<a class="hash-link" href="#withconsistencycheckenabledenabled--false" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.consistency.check.enabled</code><br></p><span>HoodieWriteClient是否应该执行其他检查，以确保写入的文件在基础文件系统/存储上可列出。 将其设置为true可以解决S3的最终一致性模型，并确保作为提交的一部分写入的所有数据均能准确地用于查询。</span><h3 class="anchor anchorWithStickyNavbar_y2LR" id="索引配置">索引配置<a class="hash-link" href="#索引配置" title="Direct link to heading">​</a></h3><p>以下配置控制索引行为，该行为将传入记录标记为对较旧记录的插入或更新。</p><p><a href="#withIndexConfig">withIndexConfig</a> (HoodieIndexConfig) <br></p><span>可插入以具有外部索引（HBase）或使用存储在Parquet文件中的默认布隆过滤器（bloom filter）</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="withindexclassindexclass--xyzuserdefinedindex">withIndexClass(indexClass = &quot;x.y.z.UserDefinedIndex&quot;)<a class="hash-link" href="#withindexclassindexclass--xyzuserdefinedindex" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.index.class</code> <br></p><span>用户自定义索引的全路径名，索引类必须为HoodieIndex的子类，当指定该配置时，其会优先于`hoodie.index.type`配置</span>#### withIndexType(indexType = BLOOM) 属性：`hoodie.index.type` <br><span>要使用的索引类型。默认为布隆过滤器。可能的选项是[BLOOM | HBASE | INMEMORY]。 布隆过滤器消除了对外部系统的依赖，并存储在Parquet数据文件的页脚中</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="bloomfilternumentriesnumentries--60000">bloomFilterNumEntries(numEntries = 60000)<a class="hash-link" href="#bloomfilternumentriesnumentries--60000" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.index.bloom.num_entries</code> <br></p><span>仅在索引类型为BLOOM时适用。<br>这是要存储在布隆过滤器中的条目数。 我们假设maxParquetFileSize为128MB，averageRecordSize为1024B，因此，一个文件中的记录总数约为130K。 默认值（60000）大约是此近似值的一半。[HUDI-56](https://issues.apache.org/jira/browse/HUDI-56) 描述了如何动态地对此进行计算。 警告：将此值设置得太低，将产生很多误报，并且索引查找将必须扫描比其所需的更多的文件；如果将其设置得非常高，将线性增加每个数据文件的大小（每50000个条目大约4KB）。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="bloomfilterfppfpp--0000000001">bloomFilterFPP(fpp = 0.000000001)<a class="hash-link" href="#bloomfilterfppfpp--0000000001" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.index.bloom.fpp</code> <br></p><span>仅在索引类型为BLOOM时适用。<br>根据条目数允许的错误率。 这用于计算应为布隆过滤器分配多少位以及哈希函数的数量。通常将此值设置得很低（默认值：0.000000001），我们希望在磁盘空间上进行权衡以降低误报率</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="bloomindexprunebyrangespruneranges--true">bloomIndexPruneByRanges(pruneRanges = true)<a class="hash-link" href="#bloomindexprunebyrangespruneranges--true" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.bloom.index.prune.by.ranges</code> <br></p><span>仅在索引类型为BLOOM时适用。<br>为true时，从文件框定信息，可以加快索引查找的速度。 如果键具有单调递增的前缀，例如时间戳，则特别有用。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="bloomindexusecachingusecaching--true">bloomIndexUseCaching(useCaching = true)<a class="hash-link" href="#bloomindexusecachingusecaching--true" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.bloom.index.use.caching</code> <br></p><span>仅在索引类型为BLOOM时适用。<br>为true时，将通过减少用于计算并行度或受影响分区的IO来缓存输入的RDD以加快索引查找</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="bloomindextreebasedfilterusetreefilter--true">bloomIndexTreebasedFilter(useTreeFilter = true)<a class="hash-link" href="#bloomindextreebasedfilterusetreefilter--true" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.bloom.index.use.treebased.filter</code> <br></p><span>仅在索引类型为BLOOM时适用。<br>为true时，启用基于间隔树的文件过滤优化。与暴力模式相比，此模式可根据键范围加快文件过滤速度</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="bloomindexbucketizedcheckingbucketizedchecking--true">bloomIndexBucketizedChecking(bucketizedChecking = true)<a class="hash-link" href="#bloomindexbucketizedcheckingbucketizedchecking--true" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.bloom.index.bucketized.checking</code> <br></p><span>仅在索引类型为BLOOM时适用。<br>为true时，启用了桶式布隆过滤。这减少了在基于排序的布隆索引查找中看到的偏差</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="bloomindexkeysperbucketkeysperbucket--10000000">bloomIndexKeysPerBucket(keysPerBucket = 10000000)<a class="hash-link" href="#bloomindexkeysperbucketkeysperbucket--10000000" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.bloom.index.keys.per.bucket</code> <br></p><span>仅在启用bloomIndexBucketizedChecking并且索引类型为bloom的情况下适用。<br>此配置控制“存储桶”的大小，该大小可跟踪对单个文件进行的记录键检查的次数，并且是分配给执行布隆过滤器查找的每个分区的工作单位。 较高的值将分摊将布隆过滤器读取到内存的固定成本。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="bloomindexparallelism0">bloomIndexParallelism(0)<a class="hash-link" href="#bloomindexparallelism0" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.bloom.index.parallelism</code> <br></p><span>仅在索引类型为BLOOM时适用。<br>这是索引查找的并行度，其中涉及Spark Shuffle。 默认情况下，这是根据输入的工作负载特征自动计算的</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="hbasezkquorumzkstring-必须">hbaseZkQuorum(zkString) <!-- -->[必须]<a class="hash-link" href="#hbasezkquorumzkstring-必须" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.index.hbase.zkquorum</code> <br></p><span>仅在索引类型为HBASE时适用。要连接的HBase ZK Quorum URL。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="hbasezkportport-必须">hbaseZkPort(port) <!-- -->[必须]<a class="hash-link" href="#hbasezkportport-必须" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.index.hbase.zkport</code> <br></p><span>仅在索引类型为HBASE时适用。要连接的HBase ZK Quorum端口。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="hbasezkznodeparentzkznodeparent--必须">hbaseZkZnodeParent(zkZnodeParent)  <!-- -->[必须]<a class="hash-link" href="#hbasezkznodeparentzkznodeparent--必须" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.index.hbase.zknode.path</code> <br></p><span>仅在索引类型为HBASE时适用。这是根znode，它将包含HBase创建及使用的所有znode。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="hbasetablenametablename--必须">hbaseTableName(tableName)  <!-- -->[必须]<a class="hash-link" href="#hbasetablenametablename--必须" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.index.hbase.table</code> <br></p><span>仅在索引类型为HBASE时适用。HBase表名称，用作索引。Hudi将row_key和[partition_path, fileID, commitTime]映射存储在表中。</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="bloomindexupdatepartitionpathupdatepartitionpath--false">bloomIndexUpdatePartitionPath(updatePartitionPath = false)<a class="hash-link" href="#bloomindexupdatepartitionpathupdatepartitionpath--false" title="Direct link to heading">​</a></h5><p>属性：<code>hoodie.bloom.index.update.partition.path</code> <br></p><span>仅在索引类型为GLOBAL_BLOOM时适用。<br>为true时，当对一个已有记录执行包含分区路径的更新操作时，将会导致把新记录插入到新分区，而把原有记录从旧分区里删除。为false时，只对旧分区的原有记录进行更新。</span><h3 class="anchor anchorWithStickyNavbar_y2LR" id="存储选项">存储选项<a class="hash-link" href="#存储选项" title="Direct link to heading">​</a></h3><p>控制有关调整parquet和日志文件大小的方面。</p><p><a href="#withStorageConfig">withStorageConfig</a> (HoodieStorageConfig) <br></p><h4 class="anchor anchorWithStickyNavbar_y2LR" id="limitfilesize-size--120mb">limitFileSize (size = 120MB)<a class="hash-link" href="#limitfilesize-size--120mb" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.parquet.max.file.size</code> <br></p><span>Hudi写阶段生成的parquet文件的目标大小。对于DFS，这需要与基础文件系统块大小保持一致，以实现最佳性能。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="parquetblocksizerowgroupsize--120mb">parquetBlockSize(rowgroupsize = 120MB)<a class="hash-link" href="#parquetblocksizerowgroupsize--120mb" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.parquet.block.size</code> <br></p><span>Parquet行组大小。最好与文件大小相同，以便将文件中的单个列连续存储在磁盘上</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="parquetpagesizepagesize--1mb">parquetPageSize(pagesize = 1MB)<a class="hash-link" href="#parquetpagesizepagesize--1mb" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.parquet.page.size</code> <br></p><span>Parquet页面大小。页面是parquet文件中的读取单位。 在一个块内，页面被分别压缩。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="parquetcompressionratioparquetcompressionratio--01">parquetCompressionRatio(parquetCompressionRatio = 0.1)<a class="hash-link" href="#parquetcompressionratioparquetcompressionratio--01" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.parquet.compression.ratio</code> <br></p><span>当Hudi尝试调整新parquet文件的大小时，预期对parquet数据进行压缩的比例。 如果bulk_insert生成的文件小于预期大小，请增加此值</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="parquetcompressioncodecparquetcompressioncodec--gzip">parquetCompressionCodec(parquetCompressionCodec = gzip)<a class="hash-link" href="#parquetcompressioncodecparquetcompressioncodec--gzip" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.parquet.compression.codec</code> <br></p><span>Parquet压缩编解码方式名称。默认值为gzip。可能的选项是[gzip | snappy | uncompressed | lzo]</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="logfilemaxsizelogfilesize--1gb">logFileMaxSize(logFileSize = 1GB)<a class="hash-link" href="#logfilemaxsizelogfilesize--1gb" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.logfile.max.size</code> <br></p><span>LogFile的最大大小。这是在将日志文件移到下一个版本之前允许的最大大小。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="logfiledatablockmaxsizedatablocksize--256mb">logFileDataBlockMaxSize(dataBlockSize = 256MB)<a class="hash-link" href="#logfiledatablockmaxsizedatablocksize--256mb" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.logfile.data.block.max.size</code> <br></p><span>LogFile数据块的最大大小。这是允许将单个数据块附加到日志文件的最大大小。 这有助于确保附加到日志文件的数据被分解为可调整大小的块，以防止发生OOM错误。此大小应大于JVM内存。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="logfiletoparquetcompressionratiologfiletoparquetcompressionratio--035">logFileToParquetCompressionRatio(logFileToParquetCompressionRatio = 0.35)<a class="hash-link" href="#logfiletoparquetcompressionratiologfiletoparquetcompressionratio--035" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.logfile.to.parquet.compression.ratio</code> <br></p><span>随着记录从日志文件移动到parquet，预期会进行额外压缩的比例。 用于merge_on_read存储，以将插入内容发送到日志文件中并控制压缩parquet文件的大小。</span>#### parquetCompressionCodec(parquetCompressionCodec = gzip) 属性：`hoodie.parquet.compression.codec` <br><span>Parquet文件的压缩编解码方式</span><h3 class="anchor anchorWithStickyNavbar_y2LR" id="压缩配置">压缩配置<a class="hash-link" href="#压缩配置" title="Direct link to heading">​</a></h3><p>压缩配置用于控制压缩（将日志文件合并到新的parquet基本文件中）、清理（回收较旧及未使用的文件组）。
<a href="#withCompactionConfig">withCompactionConfig</a> (HoodieCompactionConfig) <br></p><h4 class="anchor anchorWithStickyNavbar_y2LR" id="withcleanerpolicypolicy--keep_latest_commits">withCleanerPolicy(policy = KEEP_LATEST_COMMITS)<a class="hash-link" href="#withcleanerpolicypolicy--keep_latest_commits" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.cleaner.policy</code> <br></p><span>要使用的清理政策。Hudi将删除旧版本的parquet文件以回收空间。 任何引用此版本文件的查询和计算都将失败。最好确保数据保留的时间超过最大查询执行时间。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="retaincommitsno_of_commits_to_retain--24">retainCommits(no_of_commits_to_retain = 24)<a class="hash-link" href="#retaincommitsno_of_commits_to_retain--24" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.cleaner.commits.retained</code> <br></p><span>保留的提交数。因此，数据将保留为num_of_commits * time_between_commits（计划的）。 这也直接转化为您可以逐步提取此数据集的数量</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="archivecommitswithmincommits--96-maxcommits--128">archiveCommitsWith(minCommits = 96, maxCommits = 128)<a class="hash-link" href="#archivecommitswithmincommits--96-maxcommits--128" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.keep.min.commits</code>, <code>hoodie.keep.max.commits</code> <br></p><span>每个提交都是`.hoodie`目录中的一个小文件。由于DFS通常不支持大量小文件，因此Hudi将较早的提交归档到顺序日志中。 提交通过重命名提交文件以原子方式发布。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="withcommitsarchivalbatchsizebatch--10">withCommitsArchivalBatchSize(batch = 10)<a class="hash-link" href="#withcommitsarchivalbatchsizebatch--10" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.commits.archival.batch</code> <br></p><span>这控制着批量读取并一起归档的提交即时的数量。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="compactionsmallfilesizesize--0">compactionSmallFileSize(size = 0)<a class="hash-link" href="#compactionsmallfilesizesize--0" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.parquet.small.file.limit</code> <br></p><span>该值应小于maxFileSize，如果将其设置为0，会关闭此功能。 由于批处理中分区中插入记录的数量众多，总会出现小文件。 Hudi提供了一个选项，可以通过将对该分区中的插入作为对现有小文件的更新来解决小文件的问题。 此处的大小是被视为“小文件大小”的最小文件大小。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="insertsplitsizesize--500000">insertSplitSize(size = 500000)<a class="hash-link" href="#insertsplitsizesize--500000" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.copyonwrite.insert.split.size</code> <br></p><span>插入写入并行度。为单个分区的总共插入次数。 写出100MB的文件，至少1kb大小的记录，意味着每个文件有100K记录。默认值是超额配置为500K。 为了改善插入延迟，请对其进行调整以匹配单个文件中的记录数。 将此值设置为较小的值将导致文件变小（尤其是当compactionSmallFileSize为0时）</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="autotuneinsertsplitstrue">autoTuneInsertSplits(true)<a class="hash-link" href="#autotuneinsertsplitstrue" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.copyonwrite.insert.auto.split</code> <br></p><span>Hudi是否应该基于最后24个提交的元数据动态计算insertSplitSize。默认关闭。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="approxrecordsizesize--1024">approxRecordSize(size = 1024)<a class="hash-link" href="#approxrecordsizesize--1024" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.copyonwrite.record.size.estimate</code> <br></p><span>平均记录大小。如果指定，hudi将使用它，并且不会基于最后24个提交的元数据动态地计算。 没有默认值设置。这对于计算插入并行度以及将插入打包到小文件中至关重要。如上所述。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="withinlinecompactioninlinecompaction--false">withInlineCompaction(inlineCompaction = false)<a class="hash-link" href="#withinlinecompactioninlinecompaction--false" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.compact.inline</code> <br></p><span>当设置为true时，紧接在插入或插入更新或批量插入的提交或增量提交操作之后由摄取本身触发压缩</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="withmaxnumdeltacommitsbeforecompactionmaxnumdeltacommitsbeforecompaction--10">withMaxNumDeltaCommitsBeforeCompaction(maxNumDeltaCommitsBeforeCompaction = 10)<a class="hash-link" href="#withmaxnumdeltacommitsbeforecompactionmaxnumdeltacommitsbeforecompaction--10" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.compact.inline.max.delta.commits</code> <br></p><span>触发内联压缩之前要保留的最大增量提交数</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="withcompactionlazyblockreadenabledtrue">withCompactionLazyBlockReadEnabled(true)<a class="hash-link" href="#withcompactionlazyblockreadenabledtrue" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.compaction.lazy.block.read</code> <br></p><span>当CompactedLogScanner合并所有日志文件时，此配置有助于选择是否应延迟读取日志块。 选择true以使用I/O密集型延迟块读取（低内存使用），或者为false来使用内存密集型立即块读取（高内存使用）</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="withcompactionreverselogreadenabledfalse">withCompactionReverseLogReadEnabled(false)<a class="hash-link" href="#withcompactionreverselogreadenabledfalse" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.compaction.reverse.log.read</code> <br></p><span>HoodieLogFormatReader会从pos=0到pos=file_length向前读取日志文件。 如果此配置设置为true，则Reader会从pos=file_length到pos=0反向读取日志文件</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="withcleanerparallelismcleanerparallelism--200">withCleanerParallelism(cleanerParallelism = 200)<a class="hash-link" href="#withcleanerparallelismcleanerparallelism--200" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.cleaner.parallelism</code> <br></p><span>如果清理变慢，请增加此值。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="withcompactionstrategycompactionstrategy--orgapachehudiiocompactstrategylogfilesizebasedcompactionstrategy">withCompactionStrategy(compactionStrategy = org.apache.hudi.io.compact.strategy.LogFileSizeBasedCompactionStrategy)<a class="hash-link" href="#withcompactionstrategycompactionstrategy--orgapachehudiiocompactstrategylogfilesizebasedcompactionstrategy" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.compaction.strategy</code> <br></p><span>用来决定在每次压缩运行期间选择要压缩的文件组的压缩策略。 默认情况下，Hudi选择具有累积最多未合并数据的日志文件</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="withtargetiopercompactioninmbtargetiopercompactioninmb--500000">withTargetIOPerCompactionInMB(targetIOPerCompactionInMB = 500000)<a class="hash-link" href="#withtargetiopercompactioninmbtargetiopercompactioninmb--500000" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.compaction.target.io</code> <br></p><span>LogFileSizeBasedCompactionStrategy的压缩运行期间要花费的MB量。当压缩以内联模式运行时，此值有助于限制摄取延迟。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="withtargetpartitionsperdaybasedcompactiontargetpartitionspercompaction--10">withTargetPartitionsPerDayBasedCompaction(targetPartitionsPerCompaction = 10)<a class="hash-link" href="#withtargetpartitionsperdaybasedcompactiontargetpartitionspercompaction--10" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.compaction.daybased.target</code> <br></p><span>由org.apache.hudi.io.compact.strategy.DayBasedCompactionStrategy使用，表示在压缩运行期间要压缩的最新分区数。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="withpayloadclasspayloadclassname--orgapachehudicommonmodelhoodieavropayload">withPayloadClass(payloadClassName = org.apache.hudi.common.model.HoodieAvroPayload)<a class="hash-link" href="#withpayloadclasspayloadclassname--orgapachehudicommonmodelhoodieavropayload" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.compaction.payload.class</code> <br></p><span>这需要与插入/插入更新过程中使用的类相同。 就像写入一样，压缩也使用记录有效负载类将日志中的记录彼此合并，再次与基本文件合并，并生成压缩后要写入的最终记录。</span><h3 class="anchor anchorWithStickyNavbar_y2LR" id="指标配置">指标配置<a class="hash-link" href="#指标配置" title="Direct link to heading">​</a></h3><p>配置Hudi指标报告。
<a href="#withMetricsConfig">withMetricsConfig</a> (HoodieMetricsConfig) <br></p><span>Hudi会发布有关每次提交、清理、回滚等的指标。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="graphite">GRAPHITE<a class="hash-link" href="#graphite" title="Direct link to heading">​</a></h4><h5 class="anchor anchorWithStickyNavbar_y2LR" id="onmetricson--false">on(metricsOn = false)<a class="hash-link" href="#onmetricson--false" title="Direct link to heading">​</a></h5><p>属性：<code>hoodie.metrics.on</code> <br></p><span>打开或关闭发送指标。默认情况下处于关闭状态。</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="withreportertypereportertype--graphite">withReporterType(reporterType = GRAPHITE)<a class="hash-link" href="#withreportertypereportertype--graphite" title="Direct link to heading">​</a></h5><p>属性：<code>hoodie.metrics.reporter.type</code> <br></p><span>指标报告者的类型。默认使用graphite。</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="tographitehosthost--localhost">toGraphiteHost(host = localhost)<a class="hash-link" href="#tographitehosthost--localhost" title="Direct link to heading">​</a></h5><p>属性：<code>hoodie.metrics.graphite.host</code> <br></p><span>要连接的graphite主机</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="ongraphiteportport--4756">onGraphitePort(port = 4756)<a class="hash-link" href="#ongraphiteportport--4756" title="Direct link to heading">​</a></h5><p>属性：<code>hoodie.metrics.graphite.port</code> <br></p><span>要连接的graphite端口</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="useprefixprefix--">usePrefix(prefix = &quot;&quot;)<a class="hash-link" href="#useprefixprefix--" title="Direct link to heading">​</a></h5><p>属性：<code>hoodie.metrics.graphite.metric.prefix</code> <br></p><span>适用于所有指标的标准前缀。这有助于添加如数据中心、环境等信息</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="jmx">JMX<a class="hash-link" href="#jmx" title="Direct link to heading">​</a></h4><h5 class="anchor anchorWithStickyNavbar_y2LR" id="onmetricson--false-1">on(metricsOn = false)<a class="hash-link" href="#onmetricson--false-1" title="Direct link to heading">​</a></h5><p>属性：<code>hoodie.metrics.on</code> <br></p><span>打开或关闭发送指标。默认情况下处于关闭状态。</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="withreportertypereportertype--jmx">withReporterType(reporterType = JMX)<a class="hash-link" href="#withreportertypereportertype--jmx" title="Direct link to heading">​</a></h5><p>属性：<code>hoodie.metrics.reporter.type</code> <br></p><span>指标报告者的类型。</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="tojmxhosthost--localhost">toJmxHost(host = localhost)<a class="hash-link" href="#tojmxhosthost--localhost" title="Direct link to heading">​</a></h5><p>属性：<code>hoodie.metrics.jmx.host</code> <br></p><span>要连接的Jmx主机</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="onjmxportport--1000-5000">onJmxPort(port = 1000-5000)<a class="hash-link" href="#onjmxportport--1000-5000" title="Direct link to heading">​</a></h5><p>属性：<code>hoodie.metrics.graphite.port</code> <br></p><span>要连接的Jmx端口</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="datadog">DATADOG<a class="hash-link" href="#datadog" title="Direct link to heading">​</a></h4><h5 class="anchor anchorWithStickyNavbar_y2LR" id="onmetricson--false-2">on(metricsOn = false)<a class="hash-link" href="#onmetricson--false-2" title="Direct link to heading">​</a></h5><p>属性：<code>hoodie.metrics.on</code> <br></p><span>打开或关闭发送指标。默认情况下处于关闭状态。</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="withreportertypereportertype--datadog">withReporterType(reporterType = DATADOG)<a class="hash-link" href="#withreportertypereportertype--datadog" title="Direct link to heading">​</a></h5><p>属性： <code>hoodie.metrics.reporter.type</code> <br></p><span>指标报告者的类型。</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="withdatadogreportperiodsecondsperiod--30">withDatadogReportPeriodSeconds(period = 30)<a class="hash-link" href="#withdatadogreportperiodsecondsperiod--30" title="Direct link to heading">​</a></h5><p>属性： <code>hoodie.metrics.datadog.report.period.seconds</code> <br></p><span>Datadog报告周期，单位为秒，默认30秒。</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="withdatadogapisiteapisite">withDatadogApiSite(apiSite)<a class="hash-link" href="#withdatadogapisiteapisite" title="Direct link to heading">​</a></h5><p>属性： <code>hoodie.metrics.datadog.api.site</code> <br></p><span>Datadog API站点：EU 或者 US</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="withdatadogapikeyapikey">withDatadogApiKey(apiKey)<a class="hash-link" href="#withdatadogapikeyapikey" title="Direct link to heading">​</a></h5><p>属性： <code>hoodie.metrics.datadog.api.key</code> <br></p><span>Datadog API密匙</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="withdatadogapikeyskipvalidationskip--false">withDatadogApiKeySkipValidation(skip = false)<a class="hash-link" href="#withdatadogapikeyskipvalidationskip--false" title="Direct link to heading">​</a></h5><p>属性： <code>hoodie.metrics.datadog.api.key.skip.validation</code> <br></p><span>在通过Datadog API发送指标前，选择是否跳过验证API密匙。默认不跳过。</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="withdatadogapikeysupplierapikeysupplier">withDatadogApiKeySupplier(apiKeySupplier)<a class="hash-link" href="#withdatadogapikeysupplierapikeysupplier" title="Direct link to heading">​</a></h5><p>属性： <code>hoodie.metrics.datadog.api.key.supplier</code> <br></p><span>Datadog API 密匙提供者，用来在运行时提供密匙。只有当`hoodie.metrics.datadog.api.key`未设定的情况下才有效。</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="withdatadogapitimeoutsecondstimeout--3">withDatadogApiTimeoutSeconds(timeout = 3)<a class="hash-link" href="#withdatadogapitimeoutsecondstimeout--3" title="Direct link to heading">​</a></h5><p>属性： <code>hoodie.metrics.datadog.metric.prefix</code> <br></p><span>Datadog API超时时长，单位为秒，默认3秒。</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="withdatadogprefixprefix">withDatadogPrefix(prefix)<a class="hash-link" href="#withdatadogprefixprefix" title="Direct link to heading">​</a></h5><p>属性： <code>hoodie.metrics.datadog.metric.prefix</code> <br></p><span>Datadog指标前缀。将被加在所有指标名称前，以点间隔。例如：如果设成`foo`，`foo.`将被用作实际前缀。</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="withdatadoghosthost">withDatadogHost(host)<a class="hash-link" href="#withdatadoghosthost" title="Direct link to heading">​</a></h5><p>属性： <code>hoodie.metrics.datadog.metric.host</code> <br></p><span>Datadog指标主机，将和指标数据一并发送。</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="withdatadogtagstags">withDatadogTags(tags)<a class="hash-link" href="#withdatadogtagstags" title="Direct link to heading">​</a></h5><p>属性： <code>hoodie.metrics.datadog.metric.tags</code> <br></p><span>Datadog指标标签（逗号分隔），将和指标数据一并发送。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="用户自定义发送器">用户自定义发送器<a class="hash-link" href="#用户自定义发送器" title="Direct link to heading">​</a></h4><h5 class="anchor anchorWithStickyNavbar_y2LR" id="onmetricson--false-3">on(metricsOn = false)<a class="hash-link" href="#onmetricson--false-3" title="Direct link to heading">​</a></h5><p>属性： <code>hoodie.metrics.on</code> <br></p><span>打开或关闭发送指标。默认情况下处于关闭状态。</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="withreporterclassclassname--">withReporterClass(className = &quot;&quot;)<a class="hash-link" href="#withreporterclassclassname--" title="Direct link to heading">​</a></h5><p>属性： <code>hoodie.metrics.reporter.class</code> <br></p><span>用于处理发送指标的用户自定义类，必须是AbstractUserDefinedMetricsReporter类的子类.</span><h3 class="anchor anchorWithStickyNavbar_y2LR" id="内存配置">内存配置<a class="hash-link" href="#内存配置" title="Direct link to heading">​</a></h3><p>控制由Hudi内部执行的压缩和合并的内存使用情况
<a href="#withMemoryConfig">withMemoryConfig</a> (HoodieMemoryConfig) <br></p><span>内存相关配置</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="withmaxmemoryfractionperpartitionmergemaxmemoryfractionperpartitionmerge--06">withMaxMemoryFractionPerPartitionMerge(maxMemoryFractionPerPartitionMerge = 0.6)<a class="hash-link" href="#withmaxmemoryfractionperpartitionmergemaxmemoryfractionperpartitionmerge--06" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.memory.merge.fraction</code> <br></p><span>该比例乘以用户内存比例（1-spark.memory.fraction）以获得合并期间要使用的堆空间的最终比例</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="withmaxmemorysizepercompactioninbytesmaxmemorysizepercompactioninbytes--1gb">withMaxMemorySizePerCompactionInBytes(maxMemorySizePerCompactionInBytes = 1GB)<a class="hash-link" href="#withmaxmemorysizepercompactioninbytesmaxmemorysizepercompactioninbytes--1gb" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.memory.compaction.fraction</code> <br></p><span>HoodieCompactedLogScanner读取日志块，将记录转换为HoodieRecords，然后合并这些日志块和记录。 在任何时候，日志块中的条目数可以小于或等于相应的parquet文件中的条目数。这可能导致Scanner出现OOM。 因此，可溢出的映射有助于减轻内存压力。使用此配置来设置可溢出映射的最大允许inMemory占用空间。</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="withwritestatusfailurefractionfailurefraction--01">withWriteStatusFailureFraction(failureFraction = 0.1)<a class="hash-link" href="#withwritestatusfailurefractionfailurefraction--01" title="Direct link to heading">​</a></h4><p>属性：<code>hoodie.memory.writestatus.failure.fraction</code> <br></p><span>此属性控制报告给驱动程序的失败记录和异常的比例</span><h3 class="anchor anchorWithStickyNavbar_y2LR" id="写提交回调配置">写提交回调配置<a class="hash-link" href="#写提交回调配置" title="Direct link to heading">​</a></h3><p>控制写提交的回调。 如果用户启用了回调并且回调过程发生了错误，则会抛出异常。 当前支持HTTP, Kafka 两种回调方式。
<a href="#withCallbackConfig">withCallbackConfig</a> (HoodieWriteCommitCallbackConfig) <br></p><span>写提交回调相关配置</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="writecommitcallbackoncallbackon--false">writeCommitCallbackOn(callbackOn = false)<a class="hash-link" href="#writecommitcallbackoncallbackon--false" title="Direct link to heading">​</a></h5><p>Property: <code>hoodie.write.commit.callback.on</code> <br></p><span>打开或关闭回调功能. 默认关闭.</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="withcallbackclasscallbackclass">withCallbackClass(callbackClass)<a class="hash-link" href="#withcallbackclasscallbackclass" title="Direct link to heading">​</a></h5><p>Property: <code>hoodie.write.commit.callback.class</code> <br></p><span>回调类的完全限定名，必须实现HoodieWriteCommitCallback接口。默认 org.apache.hudi.callback.impl.HoodieWriteCommitHttpCallback</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="http-callback">HTTP CALLBACK<a class="hash-link" href="#http-callback" title="Direct link to heading">​</a></h4><p>通过 HTTP 发送写提交回调信息. 这是默认的实现方式，用户不需要显式指定。</p><h5 class="anchor anchorWithStickyNavbar_y2LR" id="withcallbackhttpurlurl">withCallbackHttpUrl(url)<a class="hash-link" href="#withcallbackhttpurlurl" title="Direct link to heading">​</a></h5><p>Property: <code>hoodie.write.commit.callback.http.url</code> <br></p><span>Http回调主机，回调信息将会发送到该主机</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="withcallbackhttptimeoutsecondstimeoutseconds--3">withCallbackHttpTimeoutSeconds(timeoutSeconds = 3)<a class="hash-link" href="#withcallbackhttptimeoutsecondstimeoutseconds--3" title="Direct link to heading">​</a></h5><p>Property: <code>hoodie.write.commit.callback.http.timeout.seconds</code> <br></p><span>Http回调超时时间（单位秒），默认3秒</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="withcallbackhttpapikeyapikey">withCallbackHttpApiKey(apiKey)<a class="hash-link" href="#withcallbackhttpapikeyapikey" title="Direct link to heading">​</a></h5><p>Property: <code>hoodie.write.commit.callback.http.api.key</code> <br></p><span>Http 回调秘钥. 默认 hudi_write_commit_http_callback</span><h4 class="anchor anchorWithStickyNavbar_y2LR" id="kafka-callback">KAFKA CALLBACK<a class="hash-link" href="#kafka-callback" title="Direct link to heading">​</a></h4><p>使用Kafka发送写提交回调信息, 用户需要配置 <code>hoodie.write.commit.callback.class</code> = <code>org.apache.hudi.utilities.callback.kafka.HoodieWriteCommitKafkaCallback</code></p><h5 class="anchor anchorWithStickyNavbar_y2LR" id="callback_kafka_bootstrap_servers">CALLBACK_KAFKA_BOOTSTRAP_SERVERS<a class="hash-link" href="#callback_kafka_bootstrap_servers" title="Direct link to heading">​</a></h5><p>Property: <code>hoodie.write.commit.callback.kafka.bootstrap.servers</code> <br></p><span>Kafka 集群地址</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="callback_kafka_topic">CALLBACK_KAFKA_TOPIC<a class="hash-link" href="#callback_kafka_topic" title="Direct link to heading">​</a></h5><p>Property: <code>hoodie.write.commit.callback.kafka.topic</code> <br></p><span>发送回调信息的topic</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="callback_kafka_partition">CALLBACK_KAFKA_PARTITION<a class="hash-link" href="#callback_kafka_partition" title="Direct link to heading">​</a></h5><p>Property: <code>hoodie.write.commit.callback.kafka.partition</code> <br></p><span>指定发送的分区, 默认 0 </span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="callback_kafka_acks">CALLBACK_KAFKA_ACKS<a class="hash-link" href="#callback_kafka_acks" title="Direct link to heading">​</a></h5><p>Property: <code>hoodie.write.commit.callback.kafka.acks</code> <br></p><span>Acks 级别, 默认 `all`</span><h5 class="anchor anchorWithStickyNavbar_y2LR" id="callback_kafka_retries">CALLBACK_KAFKA_RETRIES<a class="hash-link" href="#callback_kafka_retries" title="Direct link to heading">​</a></h5><p>Property: <code>hoodie.write.commit.callback.kafka.retries</code> <br></p><span>Kafka 发送数据失败重试次数. 默认 3 次</span></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/apache/hudi/tree/asf-site/website/versioned_docs/version-0.9.0/configurations.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_mS5F" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_mt2f"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/cn/docs/0.9.0/querying_data"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">查询 Hudi 数据集</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/cn/docs/0.9.0/performance"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">性能</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_vrFS thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#spark数据源配置" class="table-of-contents__link toc-highlight">Spark数据源配置</a><ul><li><a href="#写选项" class="table-of-contents__link toc-highlight">写选项</a></li><li><a href="#读选项" class="table-of-contents__link toc-highlight">读选项</a></li></ul></li><li><a href="#writeclient-配置" class="table-of-contents__link toc-highlight">WriteClient 配置</a><ul><li><a href="#索引配置" class="table-of-contents__link toc-highlight">索引配置</a></li><li><a href="#存储选项" class="table-of-contents__link toc-highlight">存储选项</a></li><li><a href="#压缩配置" class="table-of-contents__link toc-highlight">压缩配置</a></li><li><a href="#指标配置" class="table-of-contents__link toc-highlight">指标配置</a></li><li><a href="#内存配置" class="table-of-contents__link toc-highlight">内存配置</a></li><li><a href="#写提交回调配置" class="table-of-contents__link toc-highlight">写提交回调配置</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">About</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/cn/blog/2021/07/21/streaming-data-lake-platform">Our Vision</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/concepts">Concepts</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/community/team">Team</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/releases/release-0.11.0">Releases</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/releases/download">Download</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/powered-by">Who&#x27;s Using</a></li></ul></div><div class="col footer__col"><div class="footer__title">Learn</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/cn/docs/quick-start-guide">Quick Start</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/docker_demo">Docker Demo</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/blog">Blog</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/talks-articles">Talks &amp; Articles</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/learn/faq">FAQ</a></li><li class="footer__item"><a href="https://cwiki.apache.org/confluence/display/HUDI" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Technical Wiki<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">Hudi On Cloud</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/cn/docs/s3_hoodie">AWS</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/gcs_hoodie">Google Cloud</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/oss_hoodie">Alibaba Cloud</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/azure_hoodie">Microsoft Azure</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/cos_hoodie">Tencent Cloud</a></li><li class="footer__item"><a class="footer__link-item" href="/cn/docs/ibm_cos_hoodie">IBM Cloud</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/cn/contribute/get-involved">Get Involved</a></li><li class="footer__item"><a href="https://join.slack.com/t/apache-hudi/shared_invite/enQtODYyNDAxNzc5MTg2LTE5OTBlYmVhYjM0N2ZhOTJjOWM4YzBmMWU2MjZjMGE4NDc5ZDFiOGQ2N2VkYTVkNzU3ZDQ4OTI1NmFmYWQ0NzE" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Slack<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://github.com/apache/hudi" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://twitter.com/ApacheHudi" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="mailto:dev-subscribe@hudi.apache.org?Subject=SubscribeToHudi" target="_blank" rel="noopener noreferrer" class="footer__link-item">Mailing List</a></li></ul></div><div class="col footer__col"><div class="footer__title">Apache</div><ul class="footer__items"><li class="footer__item"><a href="https://www.apache.org/events/current-event" target="_blank" rel="noopener noreferrer" class="footer__link-item">Events</a></li><li class="footer__item"><a href="https://www.apache.org/foundation/thanks.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Thanks</a></li><li class="footer__item"><a href="https://www.apache.org/licenses" target="_blank" rel="noopener noreferrer" class="footer__link-item">License</a></li><li class="footer__item"><a href="https://www.apache.org/security" target="_blank" rel="noopener noreferrer" class="footer__link-item">Security</a></li><li class="footer__item"><a href="https://www.apache.org/foundation/sponsorship.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Sponsorship</a></li><li class="footer__item"><a href="https://www.apache.org" target="_blank" rel="noopener noreferrer" class="footer__link-item">Foundation</a></li></ul></div></div><div class="footer__bottom text--center"><div class="margin-bottom--sm"><a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer" class="footerLogoLink_SRtH"><img src="/cn/assets/images/logo-big.png" alt="Apache Hudi™" class="themedImage_TMUO themedImage--light_4Vu1 footer__logo"><img src="/cn/assets/images/logo-big.png" alt="Apache Hudi™" class="themedImage_TMUO themedImage--dark_uzRr footer__logo"></a></div><div class="footer__copyright">Copyright © 2021 <a href="https://apache.org">The Apache Software Foundation</a>, Licensed under the <a href="https://www.apache.org/licenses/LICENSE-2.0"> Apache License, Version 2.0</a>.
      Hudi, Apache and the Apache feather logo are trademarks of The Apache Software Foundation. <a href="/docs/privacy">Privacy Policy</a></div></div></div></footer></div>
<script src="/cn/assets/js/runtime~main.2900c60c.js"></script>
<script src="/cn/assets/js/main.c21a63e5.js"></script>
</body>
</html>