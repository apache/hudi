"use strict";(globalThis.webpackChunkhudi=globalThis.webpackChunkhudi||[]).push([[46834],{28453(e,n,t){t.d(n,{R:()=>a,x:()=>r});var i=t(96540);const o={},s=i.createContext(o);function a(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),i.createElement(s.Provider,{value:n},e.children)}},66424(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"ingestion_kafka_connect","title":"Using Kafka Connect","description":"Kafka Connect is a popularly used framework for integrating and moving streaming data between various systems.","source":"@site/versioned_docs/version-1.0.2/ingestion_kafka_connect.md","sourceDirName":".","slug":"/ingestion_kafka_connect","permalink":"/docs/1.0.2/ingestion_kafka_connect","draft":false,"unlisted":false,"editUrl":"https://github.com/apache/hudi/tree/asf-site/website/versioned_docs/version-1.0.2/ingestion_kafka_connect.md","tags":[],"version":"1.0.2","frontMatter":{"title":"Using Kafka Connect","keywords":["hudi","kafka","connector","ingestion"]},"sidebar":"docs","previous":{"title":"Using Flink","permalink":"/docs/1.0.2/ingestion_flink"},"next":{"title":"SQL DDL","permalink":"/docs/1.0.2/sql_ddl"}}');var o=t(74848),s=t(28453);const a={title:"Using Kafka Connect",keywords:["hudi","kafka","connector","ingestion"]},r=void 0,c={},d=[{value:"Design",id:"design",level:2},{value:"Configs",id:"configs",level:2},{value:"Current Limitations",id:"current-limitations",level:2}];function l(e){const n={a:"a",code:"code",h2:"h2",img:"img",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.a,{href:"https://kafka.apache.org/documentation/#connect",children:"Kafka Connect"})," is a popularly used framework for integrating and moving streaming data between various systems.\nHudi provides a sink for Kafka Connect, that can ingest/stream records from Apache Kafka to Hudi Tables. To do so, while providing the same transactional features\nthe sink implements transaction co-ordination across the tasks and workers in the Kafka Connect framework."]}),"\n",(0,o.jsxs)(n.p,{children:["See ",(0,o.jsx)(n.a,{href:"https://github.com/apache/hudi/tree/master/hudi-kafka-connect",children:"readme"})," for a full demo, build instructions and configurations."]}),"\n",(0,o.jsx)(n.h2,{id:"design",children:"Design"}),"\n",(0,o.jsx)(n.p,{children:"At a high level, the sink treats the connect task/worker owning partition 0 of the source topic as the transaction coordinator.\nThe transaction coordinator implements a safe two-phase commit protocol that periodically commits data into the table. Transaction\nco-ordination between the coordinator and workers reading messages from source topic partitions and writing to Hudi file groups\nhappens via a special kafka control topic, that all processes are listening to."}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.img,{alt:"Txn Coordination",src:t(80638).A+"",width:"921",height:"862"}),"\n",(0,o.jsx)("p",{align:"center",children:"Figure: Transaction Coordinator State Machine"})]}),"\n",(0,o.jsx)(n.p,{children:"This distributed coordination helps the sink achieve high throughput, low-latency while still limiting the number of write actions\non the timeline to just 1 every commit interval. This helps scale table metadata even in the face large volume of writes, compared to\napproaches where each worker commits a separate action independently leading to 10s-100s of commits per interval."}),"\n",(0,o.jsxs)(n.p,{children:["The Hudi Kafka Connect sink uses ",(0,o.jsx)(n.code,{children:"Merge-On-Read"})," by default to reduce memory pressure of writing columnar/base files (typical scaling/operational problem with the\nKafka Connect parquet sink) and inserts/appends the kafka records directly to the log file(s). Asynchronously, compaction service can be executed to merge the log files\ninto base file (Parquet format). Alternatively, users have the option to reconfigure the table type to ",(0,o.jsx)(n.code,{children:"COPY_ON_WRITE"})," in config-sink.json if desired."]}),"\n",(0,o.jsx)(n.h2,{id:"configs",children:"Configs"}),"\n",(0,o.jsxs)(n.p,{children:["To use the Hudi sink, use ",(0,o.jsx)(n.code,{children:"connector.class=org.apache.hudi.connect.HudiSinkConnector"})," in Kafka Connect. Below lists additional configurations for the sink."]}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Config Name"}),(0,o.jsx)(n.th,{children:"Default"}),(0,o.jsx)(n.th,{children:"Description"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"target.base.path"}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Required"})}),(0,o.jsx)(n.td,{children:"base path of the Hudi table written."})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"target.table.name"}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Required"})}),(0,o.jsx)(n.td,{children:"name of the table"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"hoodie.kafka.control.topic"}),(0,o.jsx)(n.td,{children:"hudi-control-topic (optional)"}),(0,o.jsx)(n.td,{children:"topic used for transaction co-ordination"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"hoodie.kafka.commit.interval.secs"}),(0,o.jsx)(n.td,{children:"60 (optional)"}),(0,o.jsx)(n.td,{children:"The frequency at which the Sink will commit data into the table"})]})]})]}),"\n",(0,o.jsxs)(n.p,{children:["See ",(0,o.jsx)(n.a,{href:"https://cwiki.apache.org/confluence/display/HUDI/RFC-32+Kafka+Connect+Sink+for+Hudi",children:"RFC"})," for more details."]}),"\n",(0,o.jsx)(n.h2,{id:"current-limitations",children:"Current Limitations"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Only append-only or insert operations are supported at this time."}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Limited support for metadata table (file listings) with no support for advanced indexing during write operations."}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}},80638(e,n,t){t.d(n,{A:()=>i});const i=t.p+"assets/images/kafka-connect-txn-23990b5735697690a5d65c79f293bd03.png"}}]);