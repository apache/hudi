"use strict";(globalThis.webpackChunkhudi=globalThis.webpackChunkhudi||[]).push([[7629],{11470:(e,n,o)=>{o.d(n,{A:()=>S});var t=o(96540),s=o(34164),i=o(23104),a=o(56347),r=o(205),c=o(57485),l=o(31682),d=o(70679);function h(e){return t.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,t.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function m(e){const{values:n,children:o}=e;return(0,t.useMemo)(()=>{const e=n??function(e){return h(e).map(({props:{value:e,label:n,attributes:o,default:t}})=>({value:e,label:n,attributes:o,default:t}))}(o);return function(e){const n=(0,l.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,o])}function u({value:e,tabValues:n}){return n.some(n=>n.value===e)}function p({queryString:e=!1,groupId:n}){const o=(0,a.W6)(),s=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,c.aZ)(s),(0,t.useCallback)(e=>{if(!s)return;const n=new URLSearchParams(o.location.search);n.set(s,e),o.replace({...o.location,search:n.toString()})},[s,o])]}function _(e){const{defaultValue:n,queryString:o=!1,groupId:s}=e,i=m(e),[a,c]=(0,t.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!u({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const o=n.find(e=>e.default)??n[0];if(!o)throw new Error("Unexpected error: 0 tabValues");return o.value}({defaultValue:n,tabValues:i})),[l,h]=p({queryString:o,groupId:s}),[_,v]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[o,s]=(0,d.Dv)(n);return[o,(0,t.useCallback)(e=>{n&&s.set(e)},[n,s])]}({groupId:s}),b=(()=>{const e=l??_;return u({value:e,tabValues:i})?e:null})();(0,r.A)(()=>{b&&c(b)},[b]);return{selectedValue:a,selectValue:(0,t.useCallback)(e=>{if(!u({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);c(e),h(e),v(e)},[h,v,i]),tabValues:i}}var v=o(92303);const b={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var k=o(74848);function f({className:e,block:n,selectedValue:o,selectValue:t,tabValues:a}){const r=[],{blockElementScrollPositionUntilNextRender:c}=(0,i.a_)(),l=e=>{const n=e.currentTarget,s=r.indexOf(n),i=a[s].value;i!==o&&(c(n),t(i))},d=e=>{let n=null;switch(e.key){case"Enter":l(e);break;case"ArrowRight":{const o=r.indexOf(e.currentTarget)+1;n=r[o]??r[0];break}case"ArrowLeft":{const o=r.indexOf(e.currentTarget)-1;n=r[o]??r[r.length-1];break}}n?.focus()};return(0,k.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":n},e),children:a.map(({value:e,label:n,attributes:t})=>(0,k.jsx)("li",{role:"tab",tabIndex:o===e?0:-1,"aria-selected":o===e,ref:e=>{r.push(e)},onKeyDown:d,onClick:l,...t,className:(0,s.A)("tabs__item",b.tabItem,t?.className,{"tabs__item--active":o===e}),children:n??e},e))})}function g({lazy:e,children:n,selectedValue:o}){const i=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=i.find(e=>e.props.value===o);return e?(0,t.cloneElement)(e,{className:(0,s.A)("margin-top--md",e.props.className)}):null}return(0,k.jsx)("div",{className:"margin-top--md",children:i.map((e,n)=>(0,t.cloneElement)(e,{key:n,hidden:e.props.value!==o}))})}function y(e){const n=_(e);return(0,k.jsxs)("div",{className:(0,s.A)("tabs-container",b.tabList),children:[(0,k.jsx)(f,{...n,...e}),(0,k.jsx)(g,{...n,...e})]})}function S(e){const n=(0,v.A)();return(0,k.jsx)(y,{...e,children:h(e.children)},String(n))}},19365:(e,n,o)=>{o.d(n,{A:()=>a});o(96540);var t=o(34164);const s={tabItem:"tabItem_Ymn6"};var i=o(74848);function a({children:e,hidden:n,className:o}){return(0,i.jsx)("div",{role:"tabpanel",className:(0,t.A)(s.tabItem,o),hidden:n,children:e})}},28453:(e,n,o)=>{o.d(n,{R:()=>a,x:()=>r});var t=o(96540);const s={},i=t.createContext(s);function a(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(i.Provider,{value:n},e.children)}},61980:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>c,metadata:()=>t,toc:()=>h});const t=JSON.parse('{"id":"docker_demo","title":"Docker Demo","description":"A Demo using Docker containers","source":"@site/docs/docker_demo.md","sourceDirName":".","slug":"/docker_demo","permalink":"/docs/next/docker_demo","draft":false,"unlisted":false,"editUrl":"https://github.com/apache/hudi/tree/asf-site/website/docs/docker_demo.md","tags":[],"version":"current","frontMatter":{"title":"Docker Demo","keywords":["hudi","docker","demo"],"toc":true,"last_modified_at":"2025-09-26T21:59:57.000Z"},"sidebar":"docs","previous":{"title":"Python/Rust Quick Start","permalink":"/docs/next/python-rust-quick-start-guide"},"next":{"title":"Use Cases","permalink":"/docs/next/use_cases"}}');var s=o(74848),i=o(28453),a=o(11470),r=o(19365);const c={title:"Docker Demo",keywords:["hudi","docker","demo"],toc:!0,last_modified_at:new Date("2025-09-26T21:59:57.000Z")},l=void 0,d={},h=[{value:"A Demo using Docker containers",id:"a-demo-using-docker-containers",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Setting up Docker Cluster",id:"setting-up-docker-cluster",level:2},{value:"Build Hudi",id:"build-hudi",level:3},{value:"Bringing up Demo Cluster",id:"bringing-up-demo-cluster",level:3},{value:"Demo",id:"demo",level:2},{value:"Step 1 : Publish the first batch to Kafka",id:"step-1--publish-the-first-batch-to-kafka",level:3},{value:"Step 2: Incrementally ingest data from Kafka topic",id:"step-2-incrementally-ingest-data-from-kafka-topic",level:3},{value:"Step 3: Sync with Hive",id:"step-3-sync-with-hive",level:3},{value:"Step 4 (a): Run Hive Queries",id:"step-4-a-run-hive-queries",level:3},{value:"Step 4 (b): Run Spark-SQL Queries",id:"step-4-b-run-spark-sql-queries",level:3},{value:"Step 5: Upload second batch to Kafka and run Hudi Streamer to ingest",id:"step-5-upload-second-batch-to-kafka-and-run-hudi-streamer-to-ingest",level:3},{value:"Step 6 (a): Run Hive Queries",id:"step-6-a-run-hive-queries",level:3},{value:"Step 6 (b): Run Spark SQL Queries",id:"step-6-b-run-spark-sql-queries",level:3},{value:"Step 7 (a): Incremental Query for COPY-ON-WRITE Table",id:"step-7-a-incremental-query-for-copy-on-write-table",level:3},{value:"Step 7 (b): Incremental Query with Spark SQL:",id:"step-7-b-incremental-query-with-spark-sql",level:3},{value:"Step 8: Schedule and Run Compaction for Merge-On-Read table",id:"step-8-schedule-and-run-compaction-for-merge-on-read-table",level:3},{value:"Step 9: Run Hive Queries including incremental queries",id:"step-9-run-hive-queries-including-incremental-queries",level:3},{value:"Step 10: Read Optimized and Snapshot queries for MOR with Spark-SQL after compaction",id:"step-10-read-optimized-and-snapshot-queries-for-mor-with-spark-sql-after-compaction",level:3},{value:"Testing Hudi in Local Docker environment",id:"testing-hudi-in-local-docker-environment",level:2},{value:"Building Local Docker Containers:",id:"building-local-docker-containers",level:3}];function m(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"a-demo-using-docker-containers",children:"A Demo using Docker containers"}),"\n",(0,s.jsx)(n.p,{children:"Let's use a real world example to see how Hudi works end to end. For this purpose, a self contained\ndata infrastructure is brought up in a local Docker cluster within your computer. It requires the\nHudi repo to have been cloned locally."}),"\n",(0,s.jsx)(n.p,{children:"The steps have been tested on a Mac laptop"}),"\n",(0,s.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Clone the ",(0,s.jsx)(n.a,{href:"https://github.com/apache/hudi",children:"Hudi repository"})," to your local machine."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Docker Setup :  For Mac, Please follow the steps as defined in ",(0,s.jsx)(n.a,{href:"https://docs.docker.com/desktop/install/mac-install/",children:"Install Docker Desktop on Mac"}),". For running Spark-SQL queries, please ensure atleast 6 GB and 4 CPUs are allocated to Docker (See Docker -> Preferences -> Advanced). Otherwise, spark-SQL queries could be killed because of memory issues."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["kcat : A command-line utility to publish/consume from kafka topics. Use ",(0,s.jsx)(n.code,{children:"brew install kcat"})," to install kcat."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"/etc/hosts : The demo references many services running in container by the hostname. Add the following settings to /etc/hosts"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:"127.0.0.1 adhoc-1\n127.0.0.1 adhoc-2\n127.0.0.1 namenode\n127.0.0.1 datanode1\n127.0.0.1 hiveserver\n127.0.0.1 hivemetastore\n127.0.0.1 kafkabroker\n127.0.0.1 sparkmaster\n127.0.0.1 zookeeper\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Java : Java SE Development Kit 8."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Maven : A build automation tool for Java projects."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["jq : A lightweight and flexible command-line JSON processor. Use ",(0,s.jsx)(n.code,{children:"brew install jq"})," to install jq."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Also, this has not been tested on some environments like Docker on Windows."}),"\n",(0,s.jsx)(n.h2,{id:"setting-up-docker-cluster",children:"Setting up Docker Cluster"}),"\n",(0,s.jsx)(n.h3,{id:"build-hudi",children:"Build Hudi"}),"\n",(0,s.jsxs)(n.p,{children:["The first step is to build Hudi. ",(0,s.jsx)(n.strong,{children:"Note"})," This step builds Hudi on supported scala version - 2.12."]}),"\n",(0,s.jsxs)(n.p,{children:["NOTE: Make sure you've cloned the ",(0,s.jsx)(n.a,{href:"https://github.com/apache/hudi",children:"Hudi repository"})," first."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:"cd <HUDI_WORKSPACE>\nmvn clean package -Pintegration-tests -DskipTests -Dspark3.5 -Dscala-2.12\n"})}),"\n",(0,s.jsx)(n.h3,{id:"bringing-up-demo-cluster",children:"Bringing up Demo Cluster"}),"\n",(0,s.jsxs)(n.p,{children:["The next step is to run the Docker compose script and setup configs for bringing up the cluster. These files are in the ",(0,s.jsx)(n.a,{href:"https://github.com/apache/hudi",children:"Hudi repository"})," which you should already have locally on your machine from the previous steps."]}),"\n",(0,s.jsx)(a.A,{children:(0,s.jsxs)(r.A,{value:"Note",children:[(0,s.jsxs)("ul",{children:[(0,s.jsx)("li",{children:" The demo must be built and run using the master branch. "}),(0,s.jsx)("li",{children:" Presto and Trino are not supported in the current demo. "})]}),(0,s.jsx)(n.p,{children:"Build the required Docker images locally for this demo by running the following command."}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"cd docker\n./build_docker_images.sh\n"})}),(0,s.jsx)(n.p,{children:"This should setup the Docker cluster."}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:"cd docker\n./setup_demo.sh\n....\n....\n....\n[+] Running 10/13\n\u283f Container zookeeper             Removed                 8.6s\n\u283f Container datanode1             Removed                18.3s\n\u283f Container spark-worker-1        Removed                16.7s\n\u283f Container adhoc-2               Removed                16.9s\n\u283f Container graphite              Removed                16.9s\n\u283f Container kafkabroker           Removed                14.1s\n\u283f Container adhoc-1               Removed                14.1s\n.......\n......\n[+] Running 13/13\n\u283f adhoc-1 Pulled                                          2.9s\n\u283f graphite Pulled                                         2.8s\n\u283f spark-worker-1 Pulled                                   3.0s\n\u283f kafka Pulled                                            2.9s\n\u283f datanode1 Pulled                                        2.9s\n\u283f hivemetastore Pulled                                    2.9s\n\u283f hiveserver Pulled                                       3.0s\n\u283f hive-metastore-postgresql Pulled                        2.8s\n\u283f namenode Pulled                                         2.9s\n\u283f sparkmaster Pulled                                      2.9s\n\u283f zookeeper Pulled                                        2.8s\n\u283f adhoc-2 Pulled                                          2.9s\n\u283f historyserver Pulled                                    2.9s\n[+] Running 13/13\n\u283f Container zookeeper                  Started           41.0s\n\u283f Container kafkabroker                Started           41.7s\n\u283f Container graphite                   Started           41.5s\n\u283f Container hive-metastore-postgresql  Running            0.0s\n\u283f Container namenode                   Running            0.0s\n\u283f Container hivemetastore              Running            0.0s\n\u283f Container historyserver              Started           41.0s\n\u283f Container datanode1                  Started           49.9s\n\u283f Container hiveserver                 Running            0.0s\n\u283f Container sparkmaster                Started           41.9s\n\u283f Container spark-worker-1             Started           50.2s\n\u283f Container adhoc-2                    Started           38.5s\n\u283f Container adhoc-1                    Started           38.5s\nCopying spark default config and setting up configs\nCopying spark default config and setting up configs\n$ docker ps\n"})})]})}),"\n",(0,s.jsx)(n.p,{children:"At this point, the Docker cluster will be up and running. The demo cluster brings up the following services"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"HDFS Services (NameNode, DataNode)"}),"\n",(0,s.jsx)(n.li,{children:"Spark Master and Worker"}),"\n",(0,s.jsx)(n.li,{children:"Hive Services (Metastore, HiveServer2 along with PostgresDB)"}),"\n",(0,s.jsx)(n.li,{children:"Kafka Broker and a Zookeeper Node (Kafka will be used as upstream source for the demo)"}),"\n",(0,s.jsx)(n.li,{children:"Adhoc containers to run Hudi/Hive CLI commands"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"demo",children:"Demo"}),"\n",(0,s.jsx)(n.p,{children:"Stock Tracker data will be used to showcase different Hudi query types and the effects of Compaction."}),"\n",(0,s.jsxs)(n.p,{children:["Take a look at the directory ",(0,s.jsx)(n.code,{children:"docker/demo/data"}),". There are 2 batches of stock data - each at 1 minute granularity.\nThe first batch contains stocker tracker data for some stock symbols during the first hour of trading window\n(9:30 a.m to 10:30 a.m). The second batch contains tracker data for next 30 mins (10:30 - 11 a.m). Hudi will\nbe used to ingest these batches to a table which will contain the latest stock tracker data at hour level granularity.\nThe batches are windowed intentionally so that the second batch contains updates to some of the rows in the first batch."]}),"\n",(0,s.jsx)(n.h3,{id:"step-1--publish-the-first-batch-to-kafka",children:"Step 1 : Publish the first batch to Kafka"}),"\n",(0,s.jsx)(n.p,{children:"Upload the first batch to Kafka topic 'stock ticks'"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.code,{children:"cat demo/data/batch_1.json | kcat -b kafkabroker -t stock_ticks -P"})}),"\n",(0,s.jsx)(n.p,{children:"To check if the new topic shows up, use"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:'kcat -b kafkabroker -L -J | jq .\n{\n  "originating_broker": {\n    "id": 1001,\n    "name": "kafkabroker:9092/1001"\n  },\n  "query": {\n    "topic": "*"\n  },\n  "brokers": [\n    {\n      "id": 1001,\n      "name": "kafkabroker:9092"\n    }\n  ],\n  "topics": [\n    {\n      "topic": "stock_ticks",\n      "partitions": [\n        {\n          "partition": 0,\n          "leader": 1001,\n          "replicas": [\n            {\n              "id": 1001\n            }\n          ],\n          "isrs": [\n            {\n              "id": 1001\n            }\n          ]\n        }\n      ]\n    }\n  ]\n}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-2-incrementally-ingest-data-from-kafka-topic",children:"Step 2: Incrementally ingest data from Kafka topic"}),"\n",(0,s.jsx)(n.p,{children:"Hudi comes with a tool named Hudi Streamer. This tool can connect to variety of data sources (including Kafka) to\npull changes and apply to Hudi table using upsert/insert primitives. Here, we will use the tool to download\njson data from kafka topic and ingest to both COW and MOR tables we initialized in the previous step. This tool\nautomatically initializes the tables in the file-system if they do not exist yet."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:"docker exec -it adhoc-2 /bin/bash\n\n# Run the following spark-submit command to execute the Hudi Streamer and ingest to stock_ticks_cow table in HDFS\nspark-submit \\\n  --class org.apache.hudi.utilities.streamer.HoodieStreamer $HUDI_UTILITIES_BUNDLE \\\n  --table-type COPY_ON_WRITE \\\n  --source-class org.apache.hudi.utilities.sources.JsonKafkaSource \\\n  --source-ordering-field ts  \\\n  --target-base-path /user/hive/warehouse/stock_ticks_cow \\\n  --target-table stock_ticks_cow --props /var/demo/config/kafka-source.properties \\\n  --schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider\n\n# Run the following spark-submit command to execute the Hudi Streamer and ingest to stock_ticks_mor table in HDFS\nspark-submit \\\n  --class org.apache.hudi.utilities.streamer.HoodieStreamer $HUDI_UTILITIES_BUNDLE \\\n  --table-type MERGE_ON_READ \\\n  --source-class org.apache.hudi.utilities.sources.JsonKafkaSource \\\n  --source-ordering-field ts \\\n  --target-base-path /user/hive/warehouse/stock_ticks_mor \\\n  --target-table stock_ticks_mor \\\n  --props /var/demo/config/kafka-source.properties \\\n  --schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider \\\n  --disable-compaction\n\n# As part of the setup (Look at setup_demo.sh), the configs needed for Hudi Streamer is uploaded to HDFS. The configs\n# contain mostly Kafa connectivity settings, the avro-schema to be used for ingesting along with key and partitioning fields.\n\nexit\n"})}),"\n",(0,s.jsxs)(n.p,{children:["You can use HDFS web-browser to look at the tables\n",(0,s.jsx)(n.code,{children:"http://namenode:9870/explorer.html#/user/hive/warehouse/stock_ticks_cow"}),"."]}),"\n",(0,s.jsx)(n.p,{children:'You can explore the new partition folder created in the table along with a "commit" / "deltacommit"\nfile under .hoodie which signals a successful commit.'}),"\n",(0,s.jsxs)(n.p,{children:["There will be a similar setup when you browse the MOR table\n",(0,s.jsx)(n.code,{children:"http://namenode:9870/explorer.html#/user/hive/warehouse/stock_ticks_mor"})]}),"\n",(0,s.jsx)(n.h3,{id:"step-3-sync-with-hive",children:"Step 3: Sync with Hive"}),"\n",(0,s.jsx)(n.p,{children:"At this step, the tables are available in HDFS. We need to sync with Hive to create new Hive tables and add partitions\ninorder to run Hive queries against those tables."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:"docker exec -it adhoc-2 /bin/bash\n\n# This command takes in HiveServer URL and COW Hudi table location in HDFS and sync the HDFS state to Hive\n/var/hoodie/ws/hudi-sync/hudi-hive-sync/run_sync_tool.sh \\\n  --jdbc-url jdbc:hive2://hiveserver:10000 \\\n  --user hive \\\n  --pass hive \\\n  --partitioned-by dt \\\n  --base-path /user/hive/warehouse/stock_ticks_cow \\\n  --database default \\\n  --table stock_ticks_cow \\\n  --partition-value-extractor org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor\n.....\n2025-09-26 13:57:58,718 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(281)) - Sync complete for stock_ticks_cow\n.....\n\n# Now run hive-sync for the second data-set in HDFS using Merge-On-Read (MOR table type)\n/var/hoodie/ws/hudi-sync/hudi-hive-sync/run_sync_tool.sh \\\n  --jdbc-url jdbc:hive2://hiveserver:10000 \\\n  --user hive \\\n  --pass hive \\\n  --partitioned-by dt \\\n  --base-path /user/hive/warehouse/stock_ticks_mor \\\n  --database default \\\n  --table stock_ticks_mor \\\n  --partition-value-extractor org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor\n...\n2025-09-26 13:58:36,052 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(281)) - Sync complete for stock_ticks_mor_ro\n...\n2025-09-26 13:58:36,184 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(281)) - Sync complete for stock_ticks_mor_rt\n...\n2025-09-26 13:58:36,308 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(281)) - Sync complete for stock_ticks_mor\n....\n\nexit\n"})}),"\n",(0,s.jsx)(n.p,{children:"After executing the above command, you will notice"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["A hive table named ",(0,s.jsx)(n.code,{children:"stock_ticks_cow"})," created which supports Snapshot and Incremental queries on Copy On Write table."]}),"\n",(0,s.jsxs)(n.li,{children:["Two new tables ",(0,s.jsx)(n.code,{children:"stock_ticks_mor_rt"})," and ",(0,s.jsx)(n.code,{children:"stock_ticks_mor_ro"})," created for the Merge On Read table. The former\nsupports Snapshot and Incremental queries (providing near-real time data) while the later supports ReadOptimized queries."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"step-4-a-run-hive-queries",children:"Step 4 (a): Run Hive Queries"}),"\n",(0,s.jsx)(n.p,{children:"Run a hive query to find the latest timestamp ingested for stock symbol 'GOOG'. You will notice that both snapshot\n(for both COW and MOR _rt table) and read-optimized queries (for MOR _ro table) give the same value \"10:29 a.m\" as Hudi create a\nparquet file for the first batch of data."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:"docker exec -it adhoc-2 /bin/bash\n\nbeeline -u jdbc:hive2://hiveserver:10000 \\\n  --hiveconf hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat \\\n  --hiveconf hive.stats.autogather=false \\\n  --hiveconf hive.vectorized.input.format.excludes=org.apache.hudi.hadoop.HoodieParquetInputFormat \\\n  --hiveconf parquet.column.index.access=true\n\n# List Tables\n0: jdbc:hive2://hiveserver:10000> show tables;\n+---------------------+--+\n|      tab_name       |\n+---------------------+--+\n| stock_ticks_cow     |\n| stock_ticks_mor     |\n| stock_ticks_mor_ro  |\n| stock_ticks_mor_rt  |\n+---------------------+--+\n4 rows selected (1.099 seconds)\n0: jdbc:hive2://hiveserver:10000>\n\n\n# Look at partitions that were added\n0: jdbc:hive2://hiveserver:10000> show partitions stock_ticks_mor_rt;\n+----------------+--+\n|   partition    |\n+----------------+--+\n| dt=2018-08-31  |\n+----------------+--+\n1 row selected (0.24 seconds)\n\n\n# COPY-ON-WRITE Queries:\n=========================\n\n\n0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_cow group by symbol HAVING symbol = 'GOOG';\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:29:00  |\n+---------+----------------------+--+\n\nNow, run a projection query:\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_cow where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20250926135641514    | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20250926135641514    | GOOG    | 2018-08-31 10:29:00  | 3391    | 1230.1899  | 1230.085  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\n\n# Merge-On-Read Queries:\n==========================\n\nLets run similar queries against M-O-R table. Lets look at both \nReadOptimized and Snapshot(realtime data) queries supported by M-O-R table\n\n# Run ReadOptimized Query. Notice that the latest timestamp is 10:29\n0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_mor_ro group by symbol HAVING symbol = 'GOOG';\nWARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:29:00  |\n+---------+----------------------+--+\n1 row selected (6.326 seconds)\n\n\n# Run Snapshot Query. Notice that the latest timestamp is again 10:29\n\n0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_mor_rt group by symbol HAVING symbol = 'GOOG';\nWARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:29:00  |\n+---------+----------------------+--+\n1 row selected (1.606 seconds)\n\n\n# Run Read Optimized and Snapshot project queries\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_ro where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20250926135725397    | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20250926135725397    | GOOG    | 2018-08-31 10:29:00  | 3391    | 1230.1899  | 1230.085  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_rt where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20250926135725397    | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20250926135725397    | GOOG    | 2018-08-31 10:29:00  | 3391    | 1230.1899  | 1230.085  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\nexit\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-4-b-run-spark-sql-queries",children:"Step 4 (b): Run Spark-SQL Queries"}),"\n",(0,s.jsx)(n.p,{children:"Hudi support Spark as query processor just like Hive. Here are the same hive queries\nrunning in spark-sql"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:'docker exec -it adhoc-1 /bin/bash\n\n$SPARK_INSTALL/bin/spark-shell \\\n  --jars $HUDI_SPARK_BUNDLE \\\n  --master local[2] \\\n  --driver-class-path $HADOOP_CONF_DIR \\\n  --conf spark.sql.hive.convertMetastoreParquet=false \\\n  --deploy-mode client \\\n  --driver-memory 1G \\\n  --executor-memory 3G \\\n  --num-executors 1\n...\n\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  \'_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.3\n      /_/\n\nUsing Scala version 2.12.18 (OpenJDK 64-Bit Server VM, Java 1.8.0_342)\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> spark.sql("show tables").show(100, false)\n+--------+------------------+-----------+\n|database|tableName         |isTemporary|\n+--------+------------------+-----------+\n|default |stock_ticks_cow   |false      |\n|default |stock_ticks_mor   |false      |\n|default |stock_ticks_mor_ro|false      |\n|default |stock_ticks_mor_rt|false      |\n+--------+------------------+-----------+\n\n# Copy-On-Write Table\n\n## Run max timestamp query against COW table\n\nscala> spark.sql("select symbol, max(ts) from stock_ticks_cow group by symbol HAVING symbol = \'GOOG\'").show(100, false)\n[Stage 0:>                                                          (0 + 1) / 1]SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes#StaticLoggerBinder for further details.\n+------+-------------------+\n|symbol|max(ts)            |\n+------+-------------------+\n|GOOG  |2018-08-31 10:29:00|\n+------+-------------------+\n\n## Projection Query\n\nscala> spark.sql("select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_cow where  symbol = \'GOOG\'").show(100, false)\n+-------------------+------+-------------------+------+---------+--------+\n|_hoodie_commit_time|symbol|ts                 |volume|open     |close   |\n+-------------------+------+-------------------+------+---------+--------+\n|20250926135641514  |GOOG  |2018-08-31 09:59:00|6330  |1230.5   |1230.02 |\n|20250926135641514  |GOOG  |2018-08-31 10:29:00|3391  |1230.1899|1230.085|\n+-------------------+------+-------------------+------+---------+--------+\n\n# Merge-On-Read Queries:\n==========================\n\nLets run similar queries against M-O-R table. Lets look at both\nReadOptimized and Snapshot queries supported by M-O-R table\n\n# Run ReadOptimized Query. Notice that the latest timestamp is 10:29\nscala> spark.sql("select symbol, max(ts) from stock_ticks_mor_ro group by symbol HAVING symbol = \'GOOG\'").show(100, false)\n+------+-------------------+\n|symbol|max(ts)            |\n+------+-------------------+\n|GOOG  |2018-08-31 10:29:00|\n+------+-------------------+\n\n\n# Run Snapshot Query. Notice that the latest timestamp is again 10:29\n\nscala> spark.sql("select symbol, max(ts) from stock_ticks_mor_rt group by symbol HAVING symbol = \'GOOG\'").show(100, false)\n+------+-------------------+\n|symbol|max(ts)            |\n+------+-------------------+\n|GOOG  |2018-08-31 10:29:00|\n+------+-------------------+\n\n# Run Read Optimized and Snapshot project queries\n\nscala> spark.sql("select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_ro where  symbol = \'GOOG\'").show(100, false)\n+-------------------+------+-------------------+------+---------+--------+\n|_hoodie_commit_time|symbol|ts                 |volume|open     |close   |\n+-------------------+------+-------------------+------+---------+--------+\n|20250926135725397  |GOOG  |2018-08-31 09:59:00|6330  |1230.5   |1230.02 |\n|20250926135725397  |GOOG  |2018-08-31 10:29:00|3391  |1230.1899|1230.085|\n+-------------------+------+-------------------+------+---------+--------+\n\nscala> spark.sql("select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_rt where  symbol = \'GOOG\'").show(100, false)\n+-------------------+------+-------------------+------+---------+--------+\n|_hoodie_commit_time|symbol|ts                 |volume|open     |close   |\n+-------------------+------+-------------------+------+---------+--------+\n|20250926135725397  |GOOG  |2018-08-31 09:59:00|6330  |1230.5   |1230.02 |\n|20250926135725397  |GOOG  |2018-08-31 10:29:00|3391  |1230.1899|1230.085|\n+-------------------+------+-------------------+------+---------+--------+\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-5-upload-second-batch-to-kafka-and-run-hudi-streamer-to-ingest",children:"Step 5: Upload second batch to Kafka and run Hudi Streamer to ingest"}),"\n",(0,s.jsx)(n.p,{children:"Upload the second batch of data and ingest this batch using Hudi Streamer. As this batch does not bring in any new\npartitions, there is no need to run hive-sync"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:"cat demo/data/batch_2.json | kcat -b kafkabroker -t stock_ticks -P\n\n# Within Docker container, run the ingestion command\ndocker exec -it adhoc-2 /bin/bash\n\n# Run the following spark-submit command to execute the Hudi Streamer and ingest to stock_ticks_cow table in HDFS\nspark-submit \\\n  --class org.apache.hudi.utilities.streamer.HoodieStreamer $HUDI_UTILITIES_BUNDLE \\\n  --table-type COPY_ON_WRITE \\\n  --source-class org.apache.hudi.utilities.sources.JsonKafkaSource \\\n  --source-ordering-field ts \\\n  --target-base-path /user/hive/warehouse/stock_ticks_cow \\\n  --target-table stock_ticks_cow \\\n  --props /var/demo/config/kafka-source.properties \\\n  --schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider\n\n# Run the following spark-submit command to execute the Hudi Streamer and ingest to stock_ticks_mor table in HDFS\nspark-submit \\\n  --class org.apache.hudi.utilities.streamer.HoodieStreamer $HUDI_UTILITIES_BUNDLE \\\n  --table-type MERGE_ON_READ \\\n  --source-class org.apache.hudi.utilities.sources.JsonKafkaSource \\\n  --source-ordering-field ts \\\n  --target-base-path /user/hive/warehouse/stock_ticks_mor \\\n  --target-table stock_ticks_mor \\\n  --props /var/demo/config/kafka-source.properties \\\n  --schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider \\\n  --disable-compaction\n\nexit\n"})}),"\n",(0,s.jsxs)(n.p,{children:["With Copy-On-Write table, the second ingestion by Hudi Streamer resulted in a new version of Parquet file getting created.\nSee ",(0,s.jsx)(n.code,{children:"http://namenode:9870/explorer.html#/user/hive/warehouse/stock_ticks_cow/2018/08/31"})]}),"\n",(0,s.jsxs)(n.p,{children:["With Merge-On-Read table, the second ingestion merely appended the batch to an unmerged delta (log) file.\nTake a look at the HDFS filesystem to get an idea: ",(0,s.jsx)(n.code,{children:"http://namenode:9870/explorer.html#/user/hive/warehouse/stock_ticks_mor/2018/08/31"})]}),"\n",(0,s.jsx)(n.h3,{id:"step-6-a-run-hive-queries",children:"Step 6 (a): Run Hive Queries"}),"\n",(0,s.jsx)(n.p,{children:"With Copy-On-Write table, the Snapshot query immediately sees the changes as part of second batch once the batch\ngot committed as each ingestion creates newer versions of parquet files."}),"\n",(0,s.jsx)(n.p,{children:'With Merge-On-Read table, the second ingestion merely appended the batch to an unmerged delta (log) file.\nThis is the time, when ReadOptimized and Snapshot queries will provide different results. ReadOptimized query will still\nreturn "10:29 am" as it will only read from the Parquet file. Snapshot query will do on-the-fly merge and return\nlatest committed data which is "10:59 a.m".'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:"docker exec -it adhoc-2 /bin/bash\n\nbeeline -u jdbc:hive2://hiveserver:10000 \\\n  --hiveconf hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat \\\n  --hiveconf hive.stats.autogather=false \\\n  --hiveconf hive.vectorized.input.format.excludes=org.apache.hudi.hadoop.HoodieParquetInputFormat \\\n  --hiveconf parquet.column.index.access=true\n\n# Copy On Write Table:\n\n0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_cow group by symbol HAVING symbol = 'GOOG';\nWARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:59:00  |\n+---------+----------------------+--+\n1 row selected (1.932 seconds)\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_cow where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20250926135641514    | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20250926141521148    | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\nAs you can notice, the above queries now reflect the changes that came as part of ingesting second batch.\n\n\n# Merge On Read Table:\n\n# Read Optimized Query\n0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_mor_ro group by symbol HAVING symbol = 'GOOG';\nWARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:29:00  |\n+---------+----------------------+--+\n1 row selected (1.6 seconds)\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_ro where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20250926135725397    | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20250926135725397    | GOOG    | 2018-08-31 10:29:00  | 3391    | 1230.1899  | 1230.085  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\n# Snapshot Query\n0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_mor_rt group by symbol HAVING symbol = 'GOOG';\nWARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:59:00  |\n+---------+----------------------+--+\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_rt where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20250926135725397    | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20250926141535482    | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\nexit\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-6-b-run-spark-sql-queries",children:"Step 6 (b): Run Spark SQL Queries"}),"\n",(0,s.jsx)(n.p,{children:"Running the same queries in Spark-SQL:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:"docker exec -it adhoc-1 /bin/bash\n\n$SPARK_INSTALL/bin/spark-shell \\\n  --jars $HUDI_SPARK_BUNDLE \\\n  --driver-class-path $HADOOP_CONF_DIR \\\n  --conf spark.sql.hive.convertMetastoreParquet=false \\\n  --deploy-mode client \\\n  --driver-memory 1G \\\n  --master local[2] \\\n  --executor-memory 3G \\\n  --num-executors 1\n\n# Copy On Write Table:\n\nscala> spark.sql(\"select symbol, max(ts) from stock_ticks_cow group by symbol HAVING symbol = 'GOOG'\").show(100, false)\n+------+-------------------+\n|symbol|max(ts)            |\n+------+-------------------+\n|GOOG  |2018-08-31 10:59:00|\n+------+-------------------+\n\nscala> spark.sql(\"select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_cow where  symbol = 'GOOG'\").show(100, false)\n\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20250926135641514    | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20250926141521148    | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\nAs you can notice, the above queries now reflect the changes that came as part of ingesting second batch.\n\n\n# Merge On Read Table:\n\n# Read Optimized Query\nscala> spark.sql(\"select symbol, max(ts) from stock_ticks_mor_ro group by symbol HAVING symbol = 'GOOG'\").show(100, false)\n+---------+----------------------+\n| symbol  |         _c1          |\n+---------+----------------------+\n| GOOG    | 2018-08-31 10:29:00  |\n+---------+----------------------+\n1 row selected (1.6 seconds)\n\nscala> spark.sql(\"select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_ro where  symbol = 'GOOG'\").show(100, false)\n+----------------------+---------+----------------------+---------+------------+-----------+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+\n| 20250926135725397    | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20250926135725397    | GOOG    | 2018-08-31 10:29:00  | 3391    | 1230.1899  | 1230.085  |\n+----------------------+---------+----------------------+---------+------------+-----------+\n\n# Snapshot Query\nscala> spark.sql(\"select symbol, max(ts) from stock_ticks_mor_rt group by symbol HAVING symbol = 'GOOG'\").show(100, false)\n+---------+----------------------+\n| symbol  |         _c1          |\n+---------+----------------------+\n| GOOG    | 2018-08-31 10:59:00  |\n+---------+----------------------+\n\nscala> spark.sql(\"select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_rt where  symbol = 'GOOG'\").show(100, false)\n+----------------------+---------+----------------------+---------+------------+-----------+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+\n| 20250926135725397    | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20250926141535482    | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+\n\nexit\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-7-a-incremental-query-for-copy-on-write-table",children:"Step 7 (a): Incremental Query for COPY-ON-WRITE Table"}),"\n",(0,s.jsx)(n.p,{children:"With 2 batches of data ingested, lets showcase the support for incremental queries in Hudi Copy-On-Write tables"}),"\n",(0,s.jsx)(n.p,{children:"Lets take the same projection query example"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:"docker exec -it adhoc-2 /bin/bash\n\nbeeline -u jdbc:hive2://hiveserver:10000 \\\n  --hiveconf hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat \\\n  --hiveconf hive.stats.autogather=false \\\n  --hiveconf hive.vectorized.input.format.excludes=org.apache.hudi.hadoop.HoodieParquetInputFormat \\\n  --hiveconf parquet.column.index.access=true\n\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_cow where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20250926135641514    | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20250926141521148    | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n"})}),"\n",(0,s.jsx)(n.p,{children:"As you notice from the above queries, there are 2 commits - 20250926135641514 and 20250926141521148 in timeline order.\nWhen you follow the steps, you will be getting different timestamps for commits. Substitute them\nin place of the above timestamps."}),"\n",(0,s.jsx)(n.p,{children:"To show the effects of incremental-query, let us assume that a reader has already seen the changes as part of\ningesting first batch. Now, for the reader to see effect of the second batch, he/she has to keep the start timestamp to\nthe commit time of the first batch (20250926135641514) and run incremental query"}),"\n",(0,s.jsx)(n.p,{children:"Hudi incremental mode provides efficient scanning for incremental queries by filtering out files that do not have any\ncandidate rows using hudi-managed metadata."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:"docker exec -it adhoc-2 /bin/bash\n\nbeeline -u jdbc:hive2://hiveserver:10000 \\\n  --hiveconf hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat \\\n  --hiveconf hive.stats.autogather=false \\\n  --hiveconf hive.vectorized.input.format.excludes=org.apache.hudi.hadoop.HoodieParquetInputFormat \\\n  --hiveconf parquet.column.index.access=true\n\n0: jdbc:hive2://hiveserver:10000> set hoodie.stock_ticks_cow.consume.mode=INCREMENTAL;\nNo rows affected (0.009 seconds)\n0: jdbc:hive2://hiveserver:10000> set hoodie.stock_ticks_cow.consume.max.commits=3;\nNo rows affected (0.009 seconds)\n0: jdbc:hive2://hiveserver:10000> set hoodie.stock_ticks_cow.consume.start.timestamp=20250926135641514;\n"})}),"\n",(0,s.jsx)(n.p,{children:"With the above setting, file-ids that do not have any updates from the commit 20250926141521148 is filtered out without scanning.\nHere is the incremental query :"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:"0: jdbc:hive2://hiveserver:10000>\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_cow where  symbol = 'GOOG' and `_hoodie_commit_time` > '20250926135641514';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20250926141521148    | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n1 row selected (0.83 seconds)\n0: jdbc:hive2://hiveserver:10000>\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-7-b-incremental-query-with-spark-sql",children:"Step 7 (b): Incremental Query with Spark SQL:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:'docker exec -it adhoc-1 /bin/bash\n\n$SPARK_INSTALL/bin/spark-shell \\\n  --jars $HUDI_SPARK_BUNDLE \\\n  --driver-class-path $HADOOP_CONF_DIR \\\n  --conf spark.sql.hive.convertMetastoreParquet=false \\\n  --deploy-mode client \\\n  --driver-memory 1G \\\n  --master local[2] \\\n  --executor-memory 3G \\\n  --num-executors 1\n\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  \'_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.3\n      /_/\n\nUsing Scala version 2.12.18 (OpenJDK 64-Bit Server VM, Java 1.8.0_342)\nType in expressions to have them evaluated.\nType :help for more information.\n\n# In the below query, 20250926135641514 is the first commit\'s timestamp\nscala> val hoodieIncViewDF = spark.read.format("org.apache.hudi").option("hoodie.datasource.query.type", "incremental").option("hoodie.datasource.read.begin.instanttime", "20250926135641514").load("/user/hive/warehouse/stock_ticks_cow")\nSLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes#StaticLoggerBinder for further details.\nhoodieIncViewDF: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 15 more fields]\n\nscala> hoodieIncViewDF.registerTempTable("stock_ticks_cow_incr_tmp1")\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nscala> spark.sql("select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_cow_incr_tmp1 where  symbol = \'GOOG\'").show(100, false);\n+----------------------+---------+----------------------+---------+------------+-----------+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+\n| 20250926141521148    | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-8-schedule-and-run-compaction-for-merge-on-read-table",children:"Step 8: Schedule and Run Compaction for Merge-On-Read table"}),"\n",(0,s.jsx)(n.p,{children:"Lets schedule and run a compaction to create a new version of columnar  file so that read-optimized readers will see fresher data.\nAgain, You can use Hudi CLI to manually schedule and run compaction"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:'docker exec -it adhoc-1 /bin/bash\n\nroot@adhoc-1:/opt# /var/hoodie/ws/packaging/hudi-cli-bundle/hudi-cli-with-bundle.sh\n...\nTable command getting loaded\nHoodieSplashScreen loaded\n===================================================================\n*         ___                          ___                        *\n*        /\\__\\          ___           /\\  \\           ___         *\n*       / /  /         /\\__\\         /  \\  \\         /\\  \\        *\n*      / /__/         / /  /        / /\\ \\  \\        \\ \\  \\       *\n*     /  \\  \\ ___    / /  /        / /  \\ \\__\\       /  \\__\\      *\n*    / /\\ \\  /\\__\\  / /__/  ___   / /__/ \\ |__|     / /\\/__/      *\n*    \\/  \\ \\/ /  /  \\ \\  \\ /\\__\\  \\ \\  \\ / /  /  /\\/ /  /         *\n*         \\  /  /    \\ \\  / /  /   \\ \\  / /  /   \\  /__/          *\n*         / /  /      \\ \\/ /  /     \\ \\/ /  /     \\ \\__\\          *\n*        / /  /        \\  /  /       \\  /  /       \\/__/          *\n*        \\/__/          \\/__/         \\/__/    Apache Hudi CLI    *\n*                                                                 *\n===================================================================\n\nWelcome to Apache Hudi CLI. Please type help if you are looking for help.\nhudi->connect --path /user/hive/warehouse/stock_ticks_mor\n14512 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n14711 [main] INFO  org.apache.hudi.common.table.HoodieTableMetaClient [] - Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor\n14711 [main] INFO  org.apache.hudi.common.table.HoodieTableConfig [] - Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties\n14855 [main] INFO  org.apache.hudi.common.table.HoodieTableMetaClient [] - Finished Loading Table of type MERGE_ON_READ(version=2) from /user/hive/warehouse/stock_ticks_mor\nMetadata for table stock_ticks_mor loaded\nhoodie:stock_ticks_mor->compactions show all\n73614 [main] INFO  org.apache.hudi.common.table.timeline.versioning.v2.ActiveTimelineV2 [] - Loaded instants upto : Option{val=[20250926141535482__20250926141539083__deltacommit__COMPLETED]}\n\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 Compaction Instant Time \u2502 State \u2502 Total FileIds to be Compacted \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 (empty)                                                         \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\n# Schedule a compaction. This will use Spark Launcher to schedule compaction\nhoodie:stock_ticks_mor->compaction schedule --hoodieConfigs hoodie.compact.inline.max.delta.commits=1\n....\nAttempted to schedule compaction for stock_ticks_mor\n\n# Now refresh and check again. You will see that there is a new compaction requested\n\nhoodie:stock_ticks_mor->refresh\n185420 [main] INFO  org.apache.hudi.common.table.HoodieTableMetaClient [] - Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor\n185420 [main] INFO  org.apache.hudi.common.table.HoodieTableConfig [] - Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties\n185443 [main] INFO  org.apache.hudi.common.table.HoodieTableMetaClient [] - Finished Loading Table of type MERGE_ON_READ(version=2) from /user/hive/warehouse/stock_ticks_mor\nMetadata for table stock_ticks_mor refreshed.\n\nhoodie:stock_ticks_mor->compactions show all\n216313 [main] INFO  org.apache.hudi.common.table.timeline.versioning.v2.ActiveTimelineV2 [] - Loaded instants upto : Option{val=[==>20250926143925260__compaction__REQUESTED]}\n\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 Compaction Instant Time \u2502 State     \u2502 Total FileIds to be Compacted \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 20250926143925260       \u2502 REQUESTED \u2502 1                             \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\n# Execute the compaction. The compaction instant value passed below must be the one displayed in the above "compactions show all" query\nhoodie:stock_ticks_mor->compaction run --compactionInstant  20250926143925260 --parallelism 2 --sparkMemory 1G  --schemaFilePath /var/demo/config/schema.avsc --retry 1  \n....\nCompaction successfully completed for 20250926143925260\n\n## Now check if compaction is completed\n\nhoodie:stock_ticks_mor->refresh\n282367 [main] INFO  org.apache.hudi.common.table.HoodieTableMetaClient [] - Loading HoodieTableMetaClient from /user/hive/warehouse/stock_ticks_mor\n282367 [main] INFO  org.apache.hudi.common.table.HoodieTableConfig [] - Loading table properties from /user/hive/warehouse/stock_ticks_mor/.hoodie/hoodie.properties\n282383 [main] INFO  org.apache.hudi.common.table.HoodieTableMetaClient [] - Finished Loading Table of type MERGE_ON_READ(version=2) from /user/hive/warehouse/stock_ticks_mor\nMetadata for table stock_ticks_mor refreshed.\n\nhoodie:stock_ticks_mor->compactions show all\n298704 [main] INFO  org.apache.hudi.common.table.timeline.versioning.v2.ActiveTimelineV2 [] - Loaded instants upto : Option{val=[20250926143925260__20250926144127165__commit__COMPLETED]}\n\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 Compaction Instant Time \u2502 State     \u2502 Total FileIds to be Compacted \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 20250926143925260       \u2502 COMPLETED \u2502 1                             \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-9-run-hive-queries-including-incremental-queries",children:"Step 9: Run Hive Queries including incremental queries"}),"\n",(0,s.jsx)(n.p,{children:"You will see that both ReadOptimized and Snapshot queries will show the latest committed data.\nLets also run the incremental query for MOR table.\nFrom looking at the below query output, it will be clear that the fist commit time for the MOR table is 20250926135725397\nand the second commit time is 20250926141535482"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:"docker exec -it adhoc-2 /bin/bash\n\nbeeline -u jdbc:hive2://hiveserver:10000 \\\n  --hiveconf hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat \\\n  --hiveconf hive.stats.autogather=false \\\n  --hiveconf hive.vectorized.input.format.excludes=org.apache.hudi.hadoop.HoodieParquetInputFormat \\\n  --hiveconf parquet.column.index.access=true\n\n\n# Read Optimized Query\n0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_mor_ro group by symbol HAVING symbol = 'GOOG';\nWARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:59:00  |\n+---------+----------------------+--+\n1 row selected (1.6 seconds)\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_ro where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20250926135725397    | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20250926141535482    | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\n# Snapshot Query\n0: jdbc:hive2://hiveserver:10000> select symbol, max(ts) from stock_ticks_mor_rt group by symbol HAVING symbol = 'GOOG';\nWARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n+---------+----------------------+--+\n| symbol  |         _c1          |\n+---------+----------------------+--+\n| GOOG    | 2018-08-31 10:59:00  |\n+---------+----------------------+--+\n\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_rt where  symbol = 'GOOG';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20250926135725397    | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20250926141535482    | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\n# Incremental Query:\n\n0: jdbc:hive2://hiveserver:10000> set hoodie.stock_ticks_mor.consume.mode=INCREMENTAL;\nNo rows affected (0.008 seconds)\n# Max-Commits covers both second batch and compaction commit\n0: jdbc:hive2://hiveserver:10000> set hoodie.stock_ticks_mor.consume.max.commits=3;\nNo rows affected (0.007 seconds)\n0: jdbc:hive2://hiveserver:10000> set hoodie.stock_ticks_mor.consume.start.timestamp=20250926135725397;\nNo rows affected (0.013 seconds)\n# Query:\n0: jdbc:hive2://hiveserver:10000> select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_ro where  symbol = 'GOOG' and `_hoodie_commit_time` > '20250926135725397';\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n| 20250926141535482    | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+--+\n\nexit\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-10-read-optimized-and-snapshot-queries-for-mor-with-spark-sql-after-compaction",children:"Step 10: Read Optimized and Snapshot queries for MOR with Spark-SQL after compaction"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:"docker exec -it adhoc-1 /bin/bash\n\n$SPARK_INSTALL/bin/spark-shell \\\n  --jars $HUDI_SPARK_BUNDLE \\\n  --driver-class-path $HADOOP_CONF_DIR \\\n  --conf spark.sql.hive.convertMetastoreParquet=false \\\n  --deploy-mode client \\\n  --driver-memory 1G \\\n  --master local[2] \\\n  --executor-memory 3G \\\n  --num-executors 1\n\n# Read Optimized Query\nscala> spark.sql(\"select symbol, max(ts) from stock_ticks_mor_ro group by symbol HAVING symbol = 'GOOG'\").show(100, false)\n+---------+----------------------+\n| symbol  |        max(ts)       |\n+---------+----------------------+\n| GOOG    | 2018-08-31 10:59:00  |\n+---------+----------------------+\n\nscala> spark.sql(\"select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_ro where  symbol = 'GOOG'\").show(100, false)\n+----------------------+---------+----------------------+---------+------------+-----------+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+\n| 20250926135725397    | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20250926141535482    | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+\n\n# Snapshot Query\nscala> spark.sql(\"select symbol, max(ts) from stock_ticks_mor_rt group by symbol HAVING symbol = 'GOOG'\").show(100, false)\n+---------+----------------------+\n| symbol  |     max(ts)          |\n+---------+----------------------+\n| GOOG    | 2018-08-31 10:59:00  |\n+---------+----------------------+\n\nscala> spark.sql(\"select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_rt where  symbol = 'GOOG'\").show(100, false)\n+----------------------+---------+----------------------+---------+------------+-----------+\n| _hoodie_commit_time  | symbol  |          ts          | volume  |    open    |   close   |\n+----------------------+---------+----------------------+---------+------------+-----------+\n| 20250926135725397    | GOOG    | 2018-08-31 09:59:00  | 6330    | 1230.5     | 1230.02   |\n| 20250926141535482    | GOOG    | 2018-08-31 10:59:00  | 9021    | 1227.1993  | 1227.215  |\n+----------------------+---------+----------------------+---------+------------+-----------+\n"})}),"\n",(0,s.jsx)(n.p,{children:"This brings the demo to an end."}),"\n",(0,s.jsx)(n.h2,{id:"testing-hudi-in-local-docker-environment",children:"Testing Hudi in Local Docker environment"}),"\n",(0,s.jsx)(n.p,{children:"You can bring up a Hadoop Docker environment containing Hadoop, Hive and Spark services with support for Hudi."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:"$ mvn pre-integration-test -DskipTests\n"})}),"\n",(0,s.jsx)(n.p,{children:"The above command builds Docker images for all the services with\ncurrent Hudi source installed at /var/hoodie/ws and also brings up the services using a compose file. We\ncurrently use Hadoop (v3.3.4), Hive (v3.1.3) and Spark (v3.5.3) in Docker images."}),"\n",(0,s.jsx)(n.p,{children:"To bring down the containers"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:"$ cd hudi-integ-test\n$ mvn docker-compose:down\n"})}),"\n",(0,s.jsx)(n.p,{children:"If you want to bring up the Docker containers, use"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:"$ cd hudi-integ-test\n$ mvn docker-compose:up -DdetachedMode=true\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Hudi is a library that is operated in a broader data analytics/ingestion environment\ninvolving Hadoop, Hive and Spark. Interoperability with all these systems is a key objective for us. We are\nactively adding integration-tests under ",(0,s.jsx)(n.strong,{children:"hudi-integ-test/src/test/java"})," that makes use of this\ndocker environment (See ",(0,s.jsx)(n.strong,{children:"hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestHoodieSanity.java"})," )"]}),"\n",(0,s.jsx)(n.h3,{id:"building-local-docker-containers",children:"Building Local Docker Containers:"}),"\n",(0,s.jsx)(n.p,{children:"The Docker images required for demo and running integration test are already in docker-hub. The Docker images\nand compose scripts are carefully implemented so that they serve dual-purpose"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"The Docker images have inbuilt Hudi jar files with environment variable pointing to those jars (HUDI_HADOOP_BUNDLE, ...)"}),"\n",(0,s.jsxs)(n.li,{children:["For running integration-tests, we need the jars generated locally to be used for running services within docker. The\ndocker-compose scripts (see ",(0,s.jsx)(n.code,{children:"docker/compose/docker-compose_hadoop334_hive313_spark353_arm64.yml"}),") ensures local jars override\ninbuilt jars by mounting local Hudi workspace over the Docker location"]}),"\n",(0,s.jsx)(n.li,{children:"As these Docker containers have mounted local Hudi workspace, any changes that happen in the workspace would automatically\nreflect in the containers. This is a convenient way for developing and verifying Hudi for\ndevelopers who do not own a distributed environment. Note that this is how integration tests are run."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["This helps avoid maintaining separate Docker images and avoids the costly step of building Hudi Docker images locally.\nBut if users want to test Hudi from locations with lower network bandwidth, they can still build local images\nrun the script\n",(0,s.jsx)(n.code,{children:"docker/build_local_docker_images.sh"})," to build local Docker images before running ",(0,s.jsx)(n.code,{children:"docker/setup_demo.sh"})]}),"\n",(0,s.jsx)(n.p,{children:"Here are the commands:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:"cd docker\n./build_local_docker_images.sh\n.....\n\n[INFO] Reactor Summary:\n[INFO]\n[INFO] Hudi ............................................... SUCCESS [  2.507 s]\n[INFO] hudi-common ........................................ SUCCESS [ 15.181 s]\n[INFO] hudi-aws ........................................... SUCCESS [  2.621 s]\n[INFO] hudi-timeline-service .............................. SUCCESS [  1.811 s]\n[INFO] hudi-client ........................................ SUCCESS [  0.065 s]\n[INFO] hudi-client-common ................................. SUCCESS [  8.308 s]\n[INFO] hudi-hadoop-mr ..................................... SUCCESS [  3.733 s]\n[INFO] hudi-spark-client .................................. SUCCESS [ 18.567 s]\n[INFO] hudi-sync-common ................................... SUCCESS [  0.794 s]\n[INFO] hudi-hive-sync ..................................... SUCCESS [  3.691 s]\n[INFO] hudi-spark-datasource .............................. SUCCESS [  0.121 s]\n[INFO] hudi-spark-common_2.12 ............................. SUCCESS [ 12.979 s]\n[INFO] hudi-spark2_2.12 ................................... SUCCESS [ 12.516 s]\n[INFO] hudi-spark_2.12 .................................... SUCCESS [ 35.649 s]\n[INFO] hudi-utilities_2.12 ................................ SUCCESS [  5.881 s]\n[INFO] hudi-utilities-bundle_2.12 ......................... SUCCESS [ 12.661 s]\n[INFO] hudi-cli ........................................... SUCCESS [ 19.858 s]\n[INFO] hudi-java-client ................................... SUCCESS [  3.221 s]\n[INFO] hudi-flink-client .................................. SUCCESS [  5.731 s]\n[INFO] hudi-spark3_2.12 ................................... SUCCESS [  8.627 s]\n[INFO] hudi-dla-sync ...................................... SUCCESS [  1.459 s]\n[INFO] hudi-sync .......................................... SUCCESS [  0.053 s]\n[INFO] hudi-hadoop-mr-bundle .............................. SUCCESS [  5.652 s]\n[INFO] hudi-hive-sync-bundle .............................. SUCCESS [  1.623 s]\n[INFO] hudi-spark-bundle_2.12 ............................. SUCCESS [ 10.930 s]\n[INFO] hudi-presto-bundle ................................. SUCCESS [  3.652 s]\n[INFO] hudi-timeline-server-bundle ........................ SUCCESS [  4.804 s]\n[INFO] hudi-trino-bundle .................................. SUCCESS [  5.991 s]\n[INFO] hudi-hadoop-docker ................................. SUCCESS [  2.061 s]\n[INFO] hudi-hadoop-base-docker ............................ SUCCESS [ 53.372 s]\n[INFO] hudi-hadoop-base-java11-docker ..................... SUCCESS [ 48.545 s]\n[INFO] hudi-hadoop-namenode-docker ........................ SUCCESS [  6.098 s]\n[INFO] hudi-hadoop-datanode-docker ........................ SUCCESS [  4.825 s]\n[INFO] hudi-hadoop-history-docker ......................... SUCCESS [  3.829 s]\n[INFO] hudi-hadoop-hive-docker ............................ SUCCESS [ 52.660 s]\n[INFO] hudi-hadoop-sparkbase-docker ....................... SUCCESS [01:02 min]\n[INFO] hudi-hadoop-sparkmaster-docker ..................... SUCCESS [ 12.661 s]\n[INFO] hudi-hadoop-sparkworker-docker ..................... SUCCESS [  4.350 s]\n[INFO] hudi-hadoop-sparkadhoc-docker ...................... SUCCESS [ 59.083 s]\n[INFO] hudi-hadoop-presto-docker .......................... SUCCESS [01:31 min]\n[INFO] hudi-hadoop-trinobase-docker ....................... SUCCESS [02:40 min]\n[INFO] hudi-hadoop-trinocoordinator-docker ................ SUCCESS [ 14.003 s]\n[INFO] hudi-hadoop-trinoworker-docker ..................... SUCCESS [ 12.100 s]\n[INFO] hudi-integ-test .................................... SUCCESS [ 13.581 s]\n[INFO] hudi-integ-test-bundle ............................. SUCCESS [ 27.212 s]\n[INFO] hudi-examples ...................................... SUCCESS [  8.090 s]\n[INFO] hudi-flink_2.12 .................................... SUCCESS [  4.217 s]\n[INFO] hudi-kafka-connect ................................. SUCCESS [  2.966 s]\n[INFO] hudi-flink-bundle_2.12 ............................. SUCCESS [ 11.155 s]\n[INFO] hudi-kafka-connect-bundle .......................... SUCCESS [ 12.369 s]\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  14:35 min\n[INFO] Finished at: 2025-09-26T18:41:27-08:00\n[INFO] ------------------------------------------------------------------------\n"})})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(m,{...e})}):m(e)}}}]);