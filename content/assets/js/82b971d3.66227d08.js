"use strict";(self.webpackChunkhudi=self.webpackChunkhudi||[]).push([[90044],{3905:(e,t,i)=>{i.d(t,{Zo:()=>d,kt:()=>h});var n=i(67294);function r(e,t,i){return t in e?Object.defineProperty(e,t,{value:i,enumerable:!0,configurable:!0,writable:!0}):e[t]=i,e}function a(e,t){var i=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),i.push.apply(i,n)}return i}function o(e){for(var t=1;t<arguments.length;t++){var i=null!=arguments[t]?arguments[t]:{};t%2?a(Object(i),!0).forEach((function(t){r(e,t,i[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(i)):a(Object(i)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(i,t))}))}return e}function l(e,t){if(null==e)return{};var i,n,r=function(e,t){if(null==e)return{};var i,n,r={},a=Object.keys(e);for(n=0;n<a.length;n++)i=a[n],t.indexOf(i)>=0||(r[i]=e[i]);return r}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(n=0;n<a.length;n++)i=a[n],t.indexOf(i)>=0||Object.prototype.propertyIsEnumerable.call(e,i)&&(r[i]=e[i])}return r}var s=n.createContext({}),c=function(e){var t=n.useContext(s),i=t;return e&&(i="function"==typeof e?e(t):o(o({},t),e)),i},d=function(e){var t=c(e.components);return n.createElement(s.Provider,{value:t},e.children)},u="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var i=e.components,r=e.mdxType,a=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),u=c(i),m=r,h=u["".concat(s,".").concat(m)]||u[m]||p[m]||a;return i?n.createElement(h,o(o({ref:t},d),{},{components:i})):n.createElement(h,o({ref:t},d))}));function h(e,t){var i=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var a=i.length,o=new Array(a);o[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[u]="string"==typeof e?e:r,o[1]=l;for(var c=2;c<a;c++)o[c]=i[c];return n.createElement.apply(null,o)}return n.createElement.apply(null,i)}m.displayName="MDXCreateElement"},87583:(e,t,i)=>{i.r(t),i.d(t,{contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>l,toc:()=>s});var n=i(87462),r=(i(67294),i(3905));const a={title:"Concurrency Control",summary:"In this page, we will discuss how to perform concurrent writes to Hudi Tables.",toc:!0,toc_min_heading_level:2,toc_max_heading_level:4,last_modified_at:new Date("2021-03-19T19:59:57.000Z")},o=void 0,l={unversionedId:"concurrency_control",id:"version-0.14.0/concurrency_control",title:"Concurrency Control",description:"Concurrency control defines how different writers/readers coordinate access to the table. Hudi ensures atomic writes, by way of publishing commits atomically to the timeline, stamped with an instant time that denotes the time at which the action is deemed to have occurred. Unlike general purpose file version control, Hudi draws clear distinction between writer processes (that issue user\u2019s upserts/deletes), table services (that write data/metadata to optimize/perform bookkeeping) and readers (that execute queries and read data). Hudi provides snapshot isolation between all three types of processes, meaning they all operate on a consistent snapshot of the table. Hudi provides optimistic concurrency control (OCC) between writers, while providing lock-free, non-blocking Multiversion Concurrency Control (MVCC) based concurrency control between writers and table-services and between different table services.",source:"@site/versioned_docs/version-0.14.0/concurrency_control.md",sourceDirName:".",slug:"/concurrency_control",permalink:"/docs/concurrency_control",editUrl:"https://github.com/apache/hudi/tree/asf-site/website/versioned_docs/version-0.14.0/concurrency_control.md",tags:[],version:"0.14.0",frontMatter:{title:"Concurrency Control",summary:"In this page, we will discuss how to perform concurrent writes to Hudi Tables.",toc:!0,toc_min_heading_level:2,toc_max_heading_level:4,last_modified_at:"2021-03-19T19:59:57.000Z"},sidebar:"docs",previous:{title:"Key Generation",permalink:"/docs/key_generation"},next:{title:"Record Payload",permalink:"/docs/record_payload"}},s=[{value:"Deployment models with supported concurrency controls",id:"deployment-models-with-supported-concurrency-controls",children:[{value:"Model A: Single writer with inline table services",id:"model-a-single-writer-with-inline-table-services",children:[{value:"Single Writer Guarantees",id:"single-writer-guarantees",children:[],level:4}],level:3},{value:"Model B: Single writer with async table services",id:"model-b-single-writer-with-async-table-services",children:[],level:3},{value:"Model C: Multi-writer",id:"model-c-multi-writer",children:[{value:"Multi Writer Guarantees",id:"multi-writer-guarantees",children:[],level:4}],level:3}],level:2},{value:"Enabling Multi Writing",id:"enabling-multi-writing",children:[{value:"External Locking and lock providers",id:"external-locking-and-lock-providers",children:[{value:"Zookeeper based lock provider",id:"zookeeper-based-lock-provider",children:[],level:4},{value:"HiveMetastore based lock provider",id:"hivemetastore-based-lock-provider",children:[],level:4},{value:"Amazon DynamoDB based lock provider",id:"amazon-dynamodb-based-lock-provider",children:[],level:4},{value:"FileSystem based lock provider (Experimental)",id:"filesystem-based-lock-provider-experimental",children:[],level:4}],level:3}],level:2},{value:"Multi Writing via Spark Datasource Writer",id:"multi-writing-via-spark-datasource-writer",children:[],level:2},{value:"Multi Writing via Hudi Streamer",id:"multi-writing-via-hudi-streamer",children:[],level:2},{value:"Early conflict Detection",id:"early-conflict-detection",children:[],level:2},{value:"Best Practices when using Optimistic Concurrency Control",id:"best-practices-when-using-optimistic-concurrency-control",children:[],level:2},{value:"Disabling Multi Writing",id:"disabling-multi-writing",children:[],level:2},{value:"Caveats",id:"caveats",children:[],level:2}],c={toc:s},d="wrapper";function u(e){let{components:t,...i}=e;return(0,r.kt)(d,(0,n.Z)({},c,i,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"Concurrency control defines how different writers/readers coordinate access to the table. Hudi ensures atomic writes, by way of publishing commits atomically to the timeline, stamped with an instant time that denotes the time at which the action is deemed to have occurred. Unlike general purpose file version control, Hudi draws clear distinction between writer processes (that issue user\u2019s upserts/deletes), table services (that write data/metadata to optimize/perform bookkeeping) and readers (that execute queries and read data). Hudi provides snapshot isolation between all three types of processes, meaning they all operate on a consistent snapshot of the table. Hudi provides optimistic concurrency control (OCC) between writers, while providing lock-free, non-blocking Multiversion Concurrency Control (MVCC) based concurrency control between writers and table-services and between different table services."),(0,r.kt)("p",null,"In this section, we will discuss the different concurrency controls supported by Hudi and how they are leveraged to provide flexible deployment models; we will cover multi-writing, a  popular deployment model; finally, we\u2019ll describe ways to ingest data into a Hudi Table from multiple writers using different writers, like  Hudi Streamer, Hudi datasource, Spark Structured Streaming and Spark SQL."),(0,r.kt)("h2",{id:"deployment-models-with-supported-concurrency-controls"},"Deployment models with supported concurrency controls"),(0,r.kt)("h3",{id:"model-a-single-writer-with-inline-table-services"},"Model A: Single writer with inline table services"),(0,r.kt)("p",null,"This is the simplest form of concurrency, meaning there is no concurrency at all in the write processes. In this model, Hudi eliminates the need for concurrency control and maximizes throughput by supporting these table services out-of-box and running inline after every write to the table. Execution plans are idempotent, persisted to the timeline and auto-recover from failures. For most simple use-cases, this means just writing is sufficient to get a well-managed table that needs no concurrency control."),(0,r.kt)("p",null,"There is no actual concurrent writing in this model. ",(0,r.kt)("strong",{parentName:"p"},"MVCC")," is leveraged to provide snapshot isolation guarantees between ingestion writer and multiple readers and also between multiple table service writers and readers. Writes to the table either from ingestion or from table services produce versioned data that are available to readers only after the writes are committed. Until then, readers can access only the previous version of the data."),(0,r.kt)("p",null,"A single writer with all table services such as cleaning, clustering, compaction, etc can be configured to be inline (such as Hudi Streamer sync-once mode and Spark Datasource with default configs) without any additional configs."),(0,r.kt)("h4",{id:"single-writer-guarantees"},"Single Writer Guarantees"),(0,r.kt)("p",null,"In this model, the following are the guarantees on ",(0,r.kt)("a",{parentName:"p",href:"https://hudi.apache.org/docs/write_operations/"},"write operations")," to expect:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"UPSERT Guarantee"),": The target table will NEVER show duplicates."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"INSERT Guarantee"),": The target table wilL NEVER have duplicates if dedup: ",(0,r.kt)("a",{parentName:"li",href:"https://hudi.apache.org/docs/configurations#hoodiedatasourcewriteinsertdropduplicates"},(0,r.kt)("inlineCode",{parentName:"a"},"hoodie.datasource.write.insert.drop.duplicates"))," & ",(0,r.kt)("a",{parentName:"li",href:"https://hudi.apache.org/docs/configurations/#hoodiecombinebeforeinsert"},(0,r.kt)("inlineCode",{parentName:"a"},"hoodie.combine.before.insert")),", is enabled."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"BULK_INSERT Guarantee"),": The target table will NEVER have duplicates if dedup: ",(0,r.kt)("a",{parentName:"li",href:"https://hudi.apache.org/docs/configurations#hoodiedatasourcewriteinsertdropduplicates"},(0,r.kt)("inlineCode",{parentName:"a"},"hoodie.datasource.write.insert.drop.duplicates"))," & ",(0,r.kt)("a",{parentName:"li",href:"https://hudi.apache.org/docs/configurations/#hoodiecombinebeforeinsert"},(0,r.kt)("inlineCode",{parentName:"a"},"hoodie.combine.before.insert")),", is enabled."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"INCREMENTAL PULL Guarantee"),": Data consumption and checkpoints are NEVER out of order.")),(0,r.kt)("h3",{id:"model-b-single-writer-with-async-table-services"},"Model B: Single writer with async table services"),(0,r.kt)("p",null,"Hudi provides the option of running the table services in an async fashion, where most of the heavy lifting (e.g actually rewriting the columnar data by compaction service) is done asynchronously. In this model, the async deployment eliminates any repeated wasteful retries and optimizes the table using clustering techniques while a single writer consumes the writes to the table without having to be blocked by such table services. This model avoids the need for taking an ",(0,r.kt)("a",{parentName:"p",href:"#external-locking-and-lock-providers"},"external lock")," to control concurrency and avoids the need to separately orchestrate and monitor offline table services jobs.."),(0,r.kt)("p",null,"A single writer along with async table services runs in the same process. For example, you can have a  Hudi Streamer in continuous mode write to a MOR table using async compaction; you can use Spark Streaming (where ",(0,r.kt)("a",{parentName:"p",href:"https://hudi.apache.org/docs/compaction"},"compaction")," is async by default), and you can use Flink streaming or your own job setup and enable async table services inside the same writer."),(0,r.kt)("p",null,"Hudi leverages ",(0,r.kt)("strong",{parentName:"p"},"MVCC")," in this model to support running any number of table service jobs concurrently, without any concurrency conflict.  This is made possible by ensuring Hudi 's ingestion writer and async table services coordinate among themselves to ensure no conflicts and avoid race conditions. The same single write guarantees described in Model A above can be achieved in this model as well.\nWith this model users don't need to spin up different spark jobs and manage the orchestration among it. For larger deployments, this model can ease the operational burden significantly while getting the table services running without blocking the writers."),(0,r.kt)("h3",{id:"model-c-multi-writer"},"Model C: Multi-writer"),(0,r.kt)("p",null,"It is not always possible to serialize all write operations to a table (such as UPSERT, INSERT or DELETE) into the same write process and therefore, multi-writing capability may be required. In multi-writing, disparate distributed processes run in parallel or overlapping time windows to write to the same table. In such cases, an external locking mechanism becomes necessary to coordinate concurrent accesses. Here are few different scenarios that would all fall under multi-writing:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Multiple ingestion writers to the same table:For instance, two Spark Datasource writers working on different sets of partitions form a source kafka topic."),(0,r.kt)("li",{parentName:"ul"},"Multiple ingestion writers to the same table, including one writer with async table services: For example, a Hudi Streamer with async compaction for regular ingestion & a Spark Datasource writer for backfilling."),(0,r.kt)("li",{parentName:"ul"},"A single ingestion writer and a separate compaction (HoodieCompactor) or clustering (HoodieClusteringJob) job apart from the ingestion writer: This is considered as multi-writing as they are not running in the same process.")),(0,r.kt)("p",null,"Hudi's concurrency model intelligently differentiates actual writing to the table from table services that manage or optimize the table. Hudi offers similar ",(0,r.kt)("strong",{parentName:"p"},"optimistic concurrency control across multiple writers"),", but ",(0,r.kt)("strong",{parentName:"p"},"table services can still execute completely lock-free and async")," as long as they run in the same process as one of the writers.\nFor multi-writing, Hudi leverages file level optimistic concurrency control(OCC). For example, when two writers write to non overlapping files, both writes are allowed to succeed. However, when the writes from different writers overlap (touch the same set of files), only one of them will succeed. Please note that this feature is currently experimental and requires external lock providers to acquire locks briefly at critical sections during the write. More on lock providers below."),(0,r.kt)("h4",{id:"multi-writer-guarantees"},"Multi Writer Guarantees"),(0,r.kt)("p",null,"With multiple writers using OCC, these are the write guarantees to expect:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"UPSERT Guarantee"),": The target table will NEVER show duplicates."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"INSERT Guarantee"),": The target table MIGHT have duplicates even if dedup is enabled."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"BULK_INSERT Guarantee"),": The target table MIGHT have duplicates even if dedup is enabled."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"INCREMENTAL PULL Guarantee"),": Data consumption and checkpoints are NEVER out of order. If there are inflight commits\n(due to multi-writing), incremental queries will not expose the completed commits following the inflight commits. ")),(0,r.kt)("h2",{id:"enabling-multi-writing"},"Enabling Multi Writing"),(0,r.kt)("p",null,"The following properties are needed to be set appropriately to turn on optimistic concurrency control to achieve multi writing."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"hoodie.write.concurrency.mode=optimistic_concurrency_control\nhoodie.write.lock.provider=<lock-provider-classname>\nhoodie.cleaner.policy.failed.writes=LAZY\n")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Config Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Default"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hoodie.write.concurrency.mode"),(0,r.kt)("td",{parentName:"tr",align:null},"SINGLE_WRITER (Optional)"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("u",null,(0,r.kt)("a",{parentName:"td",href:"https://github.com/apache/hudi/blob/c387f2a6dd3dc9db2cd22ec550a289d3a122e487/hudi-common/src/main/java/org/apache/hudi/common/model/WriteConcurrencyMode.java"},"Concurrency modes"))," for write operations.",(0,r.kt)("br",null),"Possible values:",(0,r.kt)("br",null),(0,r.kt)("ul",null,(0,r.kt)("li",null,(0,r.kt)("inlineCode",{parentName:"td"},"SINGLE_WRITER"),": Only one active writer to the table. Maximizes throughput."),(0,r.kt)("li",null,(0,r.kt)("inlineCode",{parentName:"td"},"OPTIMISTIC_CONCURRENCY_CONTROL"),": Multiple writers can operate on the table with lazy conflict resolution using locks. This means that only one writer succeeds if multiple writers write to the same file group.")),(0,r.kt)("br",null),(0,r.kt)("inlineCode",{parentName:"td"},"Config Param: WRITE_CONCURRENCY_MODE"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hoodie.write.lock.provider"),(0,r.kt)("td",{parentName:"tr",align:null},"org.apache.hudi.client.transaction.lock.ZookeeperBasedLockProvider (Optional)"),(0,r.kt)("td",{parentName:"tr",align:null},"Lock provider class name, user can provide their own implementation of LockProvider which should be subclass of org.apache.hudi.common.lock.LockProvider",(0,r.kt)("br",null),(0,r.kt)("br",null),(0,r.kt)("inlineCode",{parentName:"td"},"Config Param: LOCK_PROVIDER_CLASS_NAME"),(0,r.kt)("br",null),(0,r.kt)("inlineCode",{parentName:"td"},"Since Version: 0.8.0"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hoodie.cleaner.policy.failed.writes"),(0,r.kt)("td",{parentName:"tr",align:null},"EAGER (Optional)"),(0,r.kt)("td",{parentName:"tr",align:null},"org.apache.hudi.common.model.HoodieFailedWritesCleaningPolicy: Policy that controls how to clean up failed writes. Hudi will delete any files written by failed writes to re-claim space.     EAGER(default): Clean failed writes inline after every write operation.     LAZY: Clean failed writes lazily after heartbeat timeout when the cleaning service runs. This policy is required when multi-writers are enabled.     NEVER: Never clean failed writes.",(0,r.kt)("br",null),(0,r.kt)("br",null),(0,r.kt)("inlineCode",{parentName:"td"},"Config Param: FAILED_WRITES_CLEANER_POLICY"))))),(0,r.kt)("h3",{id:"external-locking-and-lock-providers"},"External Locking and lock providers"),(0,r.kt)("p",null,"As can be seen above, a lock provider needs to be configured in muti-writing scenarios. External locking is typically used in conjunction with optimistic concurrency control because it provides a way to prevent conflicts that might occur when two or more transactions (commits in our case) attempt to modify the same resource concurrently. When a transaction attempts to modify a resource that is currently locked by another transaction, it must wait until the lock is released before proceeding."),(0,r.kt)("p",null,"In case of multi-writing in Hudi, the locks are acquired on the Hudi table for a very short duration during specific phases (such as just before committing the writes or before scheduling table services) instead of locking for the entire span of time. This approach allows multiple writers to work on the same table simultaneously, increasing concurrency and avoids conflicts."),(0,r.kt)("p",null,"There are 4 different lock providers that require different configurations to be set. Please refer to comprehensive locking configs ",(0,r.kt)("a",{parentName:"p",href:"https://hudi.apache.org/docs/configurations#LOCK"},"here"),"."),(0,r.kt)("h4",{id:"zookeeper-based-lock-provider"},"Zookeeper based lock provider"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"hoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.ZookeeperBasedLockProvider\n")),(0,r.kt)("p",null,"Following are the basic configs required to setup this lock provider:"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Config Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Default"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hoodie.write.lock.zookeeper.base_path"),(0,r.kt)("td",{parentName:"tr",align:null},"N/A ",(0,r.kt)("strong",{parentName:"td"},"(Required)")),(0,r.kt)("td",{parentName:"tr",align:null},"The base path on Zookeeper under which to create lock related ZNodes. This should be same for all concurrent writers to the same table",(0,r.kt)("br",null),(0,r.kt)("br",null),(0,r.kt)("inlineCode",{parentName:"td"},"Config Param: ZK_BASE_PATH"),(0,r.kt)("br",null),(0,r.kt)("inlineCode",{parentName:"td"},"Since Version: 0.8.0"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hoodie.write.lock.zookeeper.port"),(0,r.kt)("td",{parentName:"tr",align:null},"N/A ",(0,r.kt)("strong",{parentName:"td"},"(Required)")),(0,r.kt)("td",{parentName:"tr",align:null},"Zookeeper port to connect to.",(0,r.kt)("br",null),(0,r.kt)("br",null),(0,r.kt)("inlineCode",{parentName:"td"},"Config Param: ZK_PORT"),(0,r.kt)("br",null),(0,r.kt)("inlineCode",{parentName:"td"},"Since Version: 0.8.0"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hoodie.write.lock.zookeeper.url"),(0,r.kt)("td",{parentName:"tr",align:null},"N/A ",(0,r.kt)("strong",{parentName:"td"},"(Required)")),(0,r.kt)("td",{parentName:"tr",align:null},"Zookeeper URL to connect to.",(0,r.kt)("br",null),(0,r.kt)("br",null),(0,r.kt)("inlineCode",{parentName:"td"},"Config Param: ZK_CONNECT_URL"),(0,r.kt)("br",null),(0,r.kt)("inlineCode",{parentName:"td"},"Since Version: 0.8.0"))))),(0,r.kt)("h4",{id:"hivemetastore-based-lock-provider"},"HiveMetastore based lock provider"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"hoodie.write.lock.provider=org.apache.hudi.hive.transaction.lock.HiveMetastoreBasedLockProvider\n")),(0,r.kt)("p",null,"Following are the basic configs required to setup this lock provider:"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Config Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Default"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hoodie.write.lock.hivemetastore.database"),(0,r.kt)("td",{parentName:"tr",align:null},"N/A ",(0,r.kt)("strong",{parentName:"td"},"(Required)")),(0,r.kt)("td",{parentName:"tr",align:null},"For Hive based lock provider, the Hive database to acquire lock against",(0,r.kt)("br",null),(0,r.kt)("br",null),(0,r.kt)("inlineCode",{parentName:"td"},"Config Param: HIVE_DATABASE_NAME"),(0,r.kt)("br",null),(0,r.kt)("inlineCode",{parentName:"td"},"Since Version: 0.8.0"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hoodie.write.lock.hivemetastore.table"),(0,r.kt)("td",{parentName:"tr",align:null},"N/A ",(0,r.kt)("strong",{parentName:"td"},"(Required)")),(0,r.kt)("td",{parentName:"tr",align:null},"For Hive based lock provider, the Hive table to acquire lock against",(0,r.kt)("br",null),(0,r.kt)("br",null),(0,r.kt)("inlineCode",{parentName:"td"},"Config Param: HIVE_TABLE_NAME"),(0,r.kt)("br",null),(0,r.kt)("inlineCode",{parentName:"td"},"Since Version: 0.8.0"))))),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"The HiveMetastore URI's are picked up from the hadoop configuration file loaded during runtime.")),(0,r.kt)("h4",{id:"amazon-dynamodb-based-lock-provider"},"Amazon DynamoDB based lock provider"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"hoodie.write.lock.provider=org.apache.hudi.aws.transaction.lock.DynamoDBBasedLockProvider\n")),(0,r.kt)("p",null,"Amazon DynamoDB based lock provides a simple way to support multi writing across different clusters.  You can refer to the\n",(0,r.kt)("a",{parentName:"p",href:"https://hudi.apache.org/docs/configurations#DynamoDB-based-Locks-Configurations"},"DynamoDB based Locks Configurations"),"\nsection for the details of each related configuration knob. Following are the basic configs required to setup this lock provider:"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Config Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Default"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hoodie.write.lock.dynamodb.endpoint_url"),(0,r.kt)("td",{parentName:"tr",align:null},"N/A ",(0,r.kt)("strong",{parentName:"td"},"(Required)")),(0,r.kt)("td",{parentName:"tr",align:null},"For DynamoDB based lock provider, the url endpoint used for Amazon DynamoDB service. Useful for development with a local dynamodb instance.",(0,r.kt)("br",null),(0,r.kt)("br",null),(0,r.kt)("inlineCode",{parentName:"td"},"Config Param: DYNAMODB_ENDPOINT_URL"),(0,r.kt)("br",null),(0,r.kt)("inlineCode",{parentName:"td"},"Since Version: 0.10.1"))))),(0,r.kt)("p",null,"For advanced configs refer ",(0,r.kt)("a",{parentName:"p",href:"https://hudi.apache.org/docs/configurations#DynamoDB-based-Locks-Configurations"},"here")),(0,r.kt)("p",null,"When using the DynamoDB-based lock provider, the name of the DynamoDB table acting as the lock table for Hudi is\nspecified by the config ",(0,r.kt)("inlineCode",{parentName:"p"},"hoodie.write.lock.dynamodb.table"),". This DynamoDB table is automatically created by Hudi, so you\ndon't have to create the table yourself. If you want to use an existing DynamoDB table, make sure that an attribute with\nthe name ",(0,r.kt)("inlineCode",{parentName:"p"},"key")," is present in the table.  The ",(0,r.kt)("inlineCode",{parentName:"p"},"key")," attribute should be the partition key of the DynamoDB table. The\nconfig ",(0,r.kt)("inlineCode",{parentName:"p"},"hoodie.write.lock.dynamodb.partition_key")," specifies the value to put for the ",(0,r.kt)("inlineCode",{parentName:"p"},"key")," attribute (not the attribute\nname), which is used for the lock on the same table. By default, ",(0,r.kt)("inlineCode",{parentName:"p"},"hoodie.write.lock.dynamodb.partition_key")," is set to\nthe table name, so that multiple writers writing to the same table share the same lock. If you customize the name, make\nsure it's the same across multiple writers."),(0,r.kt)("p",null,"Also, to set up the credentials for accessing AWS resources, customers can pass the following props to Hudi jobs:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"hoodie.aws.access.key\nhoodie.aws.secret.key\nhoodie.aws.session.token\n")),(0,r.kt)("p",null,"If not configured, Hudi falls back to use ",(0,r.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/DefaultAWSCredentialsProviderChain.html"},"DefaultAWSCredentialsProviderChain"),"."),(0,r.kt)("p",null,"IAM policy for your service instance will need to add the following permissions:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "Sid":"DynamoDBLocksTable",\n  "Effect": "Allow",\n  "Action": [\n    "dynamodb:CreateTable",\n    "dynamodb:DeleteItem",\n    "dynamodb:DescribeTable",\n    "dynamodb:GetItem",\n    "dynamodb:PutItem",\n    "dynamodb:Scan",\n    "dynamodb:UpdateItem"\n  ],\n  "Resource": "arn:${Partition}:dynamodb:${Region}:${Account}:table/${TableName}"\n}\n')),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"TableName")," : same as ",(0,r.kt)("inlineCode",{parentName:"li"},"hoodie.write.lock.dynamodb.partition_key")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"Region"),": same as ",(0,r.kt)("inlineCode",{parentName:"li"},"hoodie.write.lock.dynamodb.region"))),(0,r.kt)("p",null,"AWS SDK dependencies are not bundled with Hudi from v0.10.x and will need to be added to your classpath.\nAdd the following Maven packages (check the latest versions at time of install):"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"com.amazonaws:dynamodb-lock-client\ncom.amazonaws:aws-java-sdk-dynamodb\ncom.amazonaws:aws-java-sdk-core\n")),(0,r.kt)("h4",{id:"filesystem-based-lock-provider-experimental"},"FileSystem based lock provider (Experimental)"),(0,r.kt)("p",null,"FileSystem based lock provider supports multiple writers cross different jobs/applications based on atomic create/delete operations of the underlying filesystem."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"hoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.FileSystemBasedLockProvider\n")),(0,r.kt)("p",null,"When using the FileSystem based lock provider, by default, the lock file will store into ",(0,r.kt)("inlineCode",{parentName:"p"},"hoodie.base.path"),"+",(0,r.kt)("inlineCode",{parentName:"p"},"/.hoodie/lock"),". You may use a custom folder to store the lock file by specifying ",(0,r.kt)("inlineCode",{parentName:"p"},"hoodie.write.lock.filesystem.path"),"."),(0,r.kt)("p",null,"In case the lock cannot release during job crash, you can set ",(0,r.kt)("inlineCode",{parentName:"p"},"hoodie.write.lock.filesystem.expire")," (lock will never expire by default) to a desired expire time in minutes. You may also delete lock file manually in such situation."),(0,r.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},"FileSystem based lock provider is not supported with cloud storage like S3 or GCS."))),(0,r.kt)("h2",{id:"multi-writing-via-spark-datasource-writer"},"Multi Writing via Spark Datasource Writer"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"hudi-spark")," module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table."),(0,r.kt)("p",null,"Following is an example of how to use optimistic_concurrency_control via spark datasource"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},'inputDF.write.format("hudi")\n       .options(getQuickstartWriteConfigs)\n       .option(PRECOMBINE_FIELD_OPT_KEY, "ts")\n       .option("hoodie.cleaner.policy.failed.writes", "LAZY")\n       .option("hoodie.write.concurrency.mode", "optimistic_concurrency_control")\n       .option("hoodie.write.lock.zookeeper.url", "zookeeper")\n       .option("hoodie.write.lock.zookeeper.port", "2181")\n       .option("hoodie.write.lock.zookeeper.base_path", "/test")\n       .option(RECORDKEY_FIELD_OPT_KEY, "uuid")\n       .option(PARTITIONPATH_FIELD_OPT_KEY, "partitionpath")\n       .option(TABLE_NAME, tableName)\n       .mode(Overwrite)\n       .save(basePath)\n')),(0,r.kt)("h2",{id:"multi-writing-via-hudi-streamer"},"Multi Writing via Hudi Streamer"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"HoodieStreamer")," utility (part of hudi-utilities-bundle) provides ways to ingest from different sources such as DFS or Kafka, with the following capabilities."),(0,r.kt)("p",null,"Using optimistic_concurrency_control via Hudi Streamer requires adding the above configs to the properties file that can be passed to the\njob. For example below, adding the configs to kafka-source.properties file and passing them to Hudi Streamer will enable optimistic concurrency.\nA Hudi Streamer job can then be triggered as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},"[hoodie]$ spark-submit --class org.apache.hudi.utilities.streamer.HoodieStreamer `ls packaging/hudi-utilities-bundle/target/hudi-utilities-bundle-*.jar` \\\n  --props file://${PWD}/hudi-utilities/src/test/resources/streamer-config/kafka-source.properties \\\n  --schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \\\n  --source-class org.apache.hudi.utilities.sources.AvroKafkaSource \\\n  --source-ordering-field impresssiontime \\\n  --target-base-path file:\\/\\/\\/tmp/hudi-streamer-op \\ \n  --target-table tableName \\\n  --op BULK_INSERT\n")),(0,r.kt)("h2",{id:"early-conflict-detection"},"Early conflict Detection"),(0,r.kt)("p",null,"Multi writing using OCC allows multiple writers to concurrently write and atomically commit to the Hudi table if there is no overlapping data file to be written, to guarantee data consistency, integrity and correctness. Prior to 0.13.0 release, as the OCC (optimistic concurrency control) name suggests, each writer will optimistically proceed with ingestion and towards the end, just before committing will go about conflict resolution flow to deduce overlapping writes and abort one if need be. But this could result in lot of compute waste, since the aborted commit will have to retry from beginning. With 0.13.0, Hudi introduced early conflict deduction leveraging markers in hudi to deduce the conflicts eagerly and abort early in the write lifecyle instead of doing it in the end. For large scale deployments, this might avoid wasting lot o compute resources if there could be overlapping concurrent writers."),(0,r.kt)("p",null,"To improve the concurrency control, the ",(0,r.kt)("a",{parentName:"p",href:"https://hudi.apache.org/releases/release-0.13.0#early-conflict-detection-for-multi-writer"},"0.13.0 release")," introduced a new feature, early conflict detection in OCC, to detect the conflict during the data writing phase and abort the writing early on once a conflict is detected, using Hudi's marker mechanism. Hudi can now stop a conflicting writer much earlier because of the early conflict detection and release computing resources necessary to cluster, improving resource utilization."),(0,r.kt)("p",null,"By default, this feature is turned off. To try this out, a user needs to set ",(0,r.kt)("inlineCode",{parentName:"p"},"hoodie.write.concurrency.early.conflict.detection.enable")," to true, when using OCC for concurrency control (Refer ",(0,r.kt)("a",{parentName:"p",href:"https://hudi.apache.org/docs/configurations#Write-Configurations-advanced-configs"},"configs")," page for all relevant configs)."),(0,r.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},"Early conflict Detection in OCC is an ",(0,r.kt)("strong",{parentName:"p"},"EXPERIMENTAL")," feature"))),(0,r.kt)("h2",{id:"best-practices-when-using-optimistic-concurrency-control"},"Best Practices when using Optimistic Concurrency Control"),(0,r.kt)("p",null,"Concurrent Writing to Hudi tables requires acquiring a lock with one of the lock providers mentioned above. Due to several reasons you might want to configure retries to allow your application to acquire the lock. "),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Network connectivity or excessive load on servers increasing time for lock acquisition resulting in timeouts"),(0,r.kt)("li",{parentName:"ol"},"Running a large number of concurrent jobs that are writing to the same hudi table can result in contention during lock acquisition can cause timeouts"),(0,r.kt)("li",{parentName:"ol"},"In some scenarios of conflict resolution, Hudi commit operations might take upto 10's of seconds while the lock is being held. This can result in timeouts for other jobs waiting to acquire a lock.")),(0,r.kt)("p",null,"Set the correct native lock provider client retries. "),(0,r.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},"Please note that sometimes these settings are set on the server once and all clients inherit the same configs. Please check your settings before enabling optimistic concurrency."))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"hoodie.write.lock.wait_time_ms\nhoodie.write.lock.num_retries\n")),(0,r.kt)("p",null,"Set the correct hudi client retries for Zookeeper & HiveMetastore. This is useful in cases when native client retry settings cannot be changed. Please note that these retries will happen in addition to any native client retries that you may have set. "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"hoodie.write.lock.client.wait_time_ms\nhoodie.write.lock.client.num_retries\n")),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"Setting the right values for these depends on a case by case basis; some defaults have been provided for general cases.")),(0,r.kt)("h2",{id:"disabling-multi-writing"},"Disabling Multi Writing"),(0,r.kt)("p",null,"Remove the following settings that were used to enable multi-writer or override with default values."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"hoodie.write.concurrency.mode=single_writer\nhoodie.cleaner.policy.failed.writes=EAGER\n")),(0,r.kt)("h2",{id:"caveats"},"Caveats"),(0,r.kt)("p",null,"If you are using the ",(0,r.kt)("inlineCode",{parentName:"p"},"WriteClient")," API, please note that multiple writes to the table need to be initiated from 2 different instances of the write client.\nIt is ",(0,r.kt)("strong",{parentName:"p"},"NOT")," recommended to use the same instance of the write client to perform multi writing."))}u.isMDXComponent=!0}}]);