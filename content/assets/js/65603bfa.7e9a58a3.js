"use strict";(self.webpackChunkhudi=self.webpackChunkhudi||[]).push([[25582],{15680:(e,t,a)=>{a.d(t,{xA:()=>p,yg:()=>m});var n=a(96540);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var s=n.createContext({}),d=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},p=function(e){var t=d(e.components);return n.createElement(s.Provider,{value:t},e.children)},h="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,r=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),h=d(a),c=o,m=h["".concat(s,".").concat(c)]||h[c]||u[c]||r;return a?n.createElement(m,i(i({ref:t},p),{},{components:a})):n.createElement(m,i({ref:t},p))}));function m(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=a.length,i=new Array(r);i[0]=c;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[h]="string"==typeof e?e:o,i[1]=l;for(var d=2;d<r;d++)i[d]=a[d];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}c.displayName="MDXCreateElement"},18410:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>d});var n=a(58168),o=(a(96540),a(15680));const r={title:"Registering sample dataset to Hive via beeline",excerpt:"How to manually register HUDI dataset into Hive using beeline",author:"vinoth",category:"blog",tags:["how-to","apache hudi"]},i=void 0,l={permalink:"/blog/2019/05/14/registering-dataset-to-hive",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2019-05-14-registering-dataset-to-hive.md",source:"@site/blog/2019-05-14-registering-dataset-to-hive.md",title:"Registering sample dataset to Hive via beeline",description:"Hudi hive sync tool typically handles registration of the dataset into Hive metastore. In case, there are issues with quickstart around this, following page shows commands that can be used to do this manually via beeline.",date:"2019-05-14T00:00:00.000Z",formattedDate:"May 14, 2019",tags:[{label:"how-to",permalink:"/blog/tags/how-to"},{label:"apache hudi",permalink:"/blog/tags/apache-hudi"}],readingTime:1.32,truncated:!0,authors:[{name:"vinoth"}],prevItem:{title:"Ingesting Database changes via Sqoop/Hudi",permalink:"/blog/2019/09/09/ingesting-database-changes"},nextItem:{title:"Big Batch vs Incremental Processing",permalink:"/blog/2019/03/07/batch-vs-incremental"}},s={authorsImageUrls:[void 0]},d=[],p={toc:d},h="wrapper";function u(e){let{components:t,...a}=e;return(0,o.yg)(h,(0,n.A)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Hudi hive sync tool typically handles registration of the dataset into Hive metastore. In case, there are issues with quickstart around this, following page shows commands that can be used to do this manually via beeline.  "),(0,o.yg)("p",null,"Add in the ",(0,o.yg)("em",{parentName:"p"},"packaging/hoodie-hive-bundle/target/hoodie-hive-bundle-0.4.6-SNAPSHOT.jar,")," so that Hive can read the Hudi dataset and answer the query."),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-java"},"hive> set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;\nhive> set hive.stats.autogather=false;\nhive> add jar file:///path/to/hoodie-hive-bundle-0.5.2-SNAPSHOT.jar;\nAdded [file:///path/to/hoodie-hive-bundle-0.5.2-SNAPSHOT.jar] to class path\nAdded resources: [file:///path/to/hoodie-hive-bundle-0.5.2-SNAPSHOT.jar]\n")),(0,o.yg)("p",null,"Then, you need to create a ",(0,o.yg)("em",{parentName:"p"},"ReadOptimized")," Hive table as below and register the sample partitions"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-java"},"DROP TABLE hoodie_test;\nCREATE EXTERNAL TABLE hoodie_test(`_row_key` string,\n    `_hoodie_commit_time` string,\n    `_hoodie_commit_seqno` string,\n    rider string,\n    driver string,\n    begin_lat double,\n    begin_lon double,\n    end_lat double,\n    end_lon double,\n    fare double)\n    PARTITIONED BY (`datestr` string)\n    ROW FORMAT SERDE\n    'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n    STORED AS INPUTFORMAT\n    'com.uber.hoodie.hadoop.HoodieInputFormat'\n    OUTPUTFORMAT\n    'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n    LOCATION\n    'hdfs:///tmp/hoodie/sample-table';\n     \nALTER TABLE `hoodie_test` ADD IF NOT EXISTS PARTITION (datestr='2016-03-15') LOCATION 'hdfs:///tmp/hoodie/sample-table/2016/03/15';\nALTER TABLE `hoodie_test` ADD IF NOT EXISTS PARTITION (datestr='2015-03-16') LOCATION 'hdfs:///tmp/hoodie/sample-table/2015/03/16';\nALTER TABLE `hoodie_test` ADD IF NOT EXISTS PARTITION (datestr='2015-03-17') LOCATION 'hdfs:///tmp/hoodie/sample-table/2015/03/17';\n     \nset mapreduce.framework.name=yarn;\n")),(0,o.yg)("p",null,"And you can add a ",(0,o.yg)("em",{parentName:"p"},"Realtime")," Hive table, as below"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-java"},"DROP TABLE hoodie_rt;\nCREATE EXTERNAL TABLE hoodie_rt(\n    `_hoodie_commit_time` string,\n    `_hoodie_commit_seqno` string,\n    `_hoodie_record_key` string,\n    `_hoodie_partition_path` string,\n    `_hoodie_file_name` string,\n    timestamp double,\n    `_row_key` string,\n    rider string,\n    driver string,\n    begin_lat double,\n    begin_lon double,\n    end_lat double,\n    end_lon double,\n    fare double)\n    PARTITIONED BY (`datestr` string)\n    ROW FORMAT SERDE\n    'com.uber.hoodie.hadoop.realtime.HoodieParquetSerde'\n    STORED AS INPUTFORMAT\n    'com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat'\n    OUTPUTFORMAT\n    'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n    LOCATION\n    'file:///tmp/hoodie/sample-table';\n     \nALTER TABLE `hoodie_rt` ADD IF NOT EXISTS PARTITION (datestr='2016-03-15') LOCATION 'file:///tmp/hoodie/sample-table/2016/03/15';\nALTER TABLE `hoodie_rt` ADD IF NOT EXISTS PARTITION (datestr='2015-03-16') LOCATION 'file:///tmp/hoodie/sample-table/2015/03/16';\nALTER TABLE `hoodie_rt` ADD IF NOT EXISTS PARTITION (datestr='2015-03-17') LOCATION 'file:///tmp/hoodie/sample-table/2015/03/17';\n")))}u.isMDXComponent=!0}}]);