"use strict";(self.webpackChunkhudi=self.webpackChunkhudi||[]).push([[50095],{70227:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>c,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"syncing_metastore","title":"Syncing to Metastore","description":"Spark and DeltaStreamer","source":"@site/versioned_docs/version-0.10.0/syncing_metastore.md","sourceDirName":".","slug":"/syncing_metastore","permalink":"/docs/0.10.0/syncing_metastore","draft":false,"unlisted":false,"editUrl":"https://github.com/apache/hudi/tree/asf-site/website/versioned_docs/version-0.10.0/syncing_metastore.md","tags":[],"version":"0.10.0","frontMatter":{"title":"Syncing to Metastore","keywords":["hudi","hive","sync"]},"sidebar":"docs","previous":{"title":"Flink Setup","permalink":"/docs/0.10.0/flink_configuration"},"next":{"title":"Bootstrapping","permalink":"/docs/0.10.0/migration_guide"}}');var o=i(74848),s=i(28453);const r={title:"Syncing to Metastore",keywords:["hudi","hive","sync"]},a=void 0,d={},l=[{value:"Spark and DeltaStreamer",id:"spark-and-deltastreamer",level:2},{value:"Flink Setup",id:"flink-setup",level:2},{value:"Install",id:"install",level:3},{value:"Hive Environment",id:"hive-environment",level:3},{value:"Sync Template",id:"sync-template",level:3},{value:"Query",id:"query",level:3}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h2,{id:"spark-and-deltastreamer",children:"Spark and DeltaStreamer"}),"\n",(0,o.jsxs)(n.p,{children:["Writing data with ",(0,o.jsx)(n.a,{href:"/docs/writing_data",children:"DataSource"})," writer or ",(0,o.jsx)(n.a,{href:"/docs/0.10.0/hoodie_deltastreamer",children:"HoodieDeltaStreamer"})," supports syncing of the table's latest schema to Hive metastore, such that queries can pick up new columns and partitions.\nIn case, it's preferable to run this from commandline or in an independent jvm, Hudi provides a ",(0,o.jsx)(n.code,{children:"HiveSyncTool"}),", which can be invoked as below,\nonce you have built the hudi-hive module. Following is how we sync the above Datasource Writer written table to Hive metastore."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-java",children:"cd hudi-hive\n./run_sync_tool.sh  --jdbc-url jdbc:hive2:\\/\\/hiveserver:10000 --user hive --pass hive --partitioned-by partition --base-path <basePath> --database default --table <tableName>\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Starting with Hudi 0.5.1 version read optimized version of merge-on-read tables are suffixed '_ro' by default. For backwards compatibility with older Hudi versions, an optional HiveSyncConfig - ",(0,o.jsx)(n.code,{children:"--skip-ro-suffix"}),", has been provided to turn off '_ro' suffixing if desired. Explore other hive sync options using the following command:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-java",children:"cd hudi-hive\n./run_sync_tool.sh\n [hudi-hive]$ ./run_sync_tool.sh --help\n"})}),"\n",(0,o.jsx)(n.h2,{id:"flink-setup",children:"Flink Setup"}),"\n",(0,o.jsx)(n.h3,{id:"install",children:"Install"}),"\n",(0,o.jsxs)(n.p,{children:["Now you can git clone Hudi master branch to test Flink hive sync. The first step is to install Hudi to get ",(0,o.jsx)(n.code,{children:"hudi-flink-bundle_2.11-0.x.jar"}),".\n",(0,o.jsx)(n.code,{children:"hudi-flink-bundle"})," module pom.xml sets the scope related to hive as ",(0,o.jsx)(n.code,{children:"provided"})," by default. If you want to use hive sync, you need to use the\nprofile ",(0,o.jsx)(n.code,{children:"flink-bundle-shade-hive"})," during packaging. Executing command below to install:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Maven install command\nmvn install -DskipTests -Drat.skip=true -Pflink-bundle-shade-hive2\n\n# For hive1, you need to use profile -Pflink-bundle-shade-hive1\n# For hive3, you need to use profile -Pflink-bundle-shade-hive3 \n"})}),"\n",(0,o.jsx)(n.admonition,{type:"note",children:(0,o.jsx)(n.p,{children:"Hive1.x can only synchronize metadata to hive, but cannot use hive query now. If you need to query, you can use spark to query hive table."})}),"\n",(0,o.jsx)(n.admonition,{type:"note",children:(0,o.jsxs)(n.p,{children:["If using hive profile, you need to modify the hive version in the profile to your hive cluster version (Only need to modify the hive version in this profile).\nThe location of this ",(0,o.jsx)(n.code,{children:"pom.xml"})," is ",(0,o.jsx)(n.code,{children:"packaging/hudi-flink-bundle/pom.xml"}),", and the corresponding profile is at the bottom of this file."]})}),"\n",(0,o.jsx)(n.h3,{id:"hive-environment",children:"Hive Environment"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Import ",(0,o.jsx)(n.code,{children:"hudi-hadoop-mr-bundle"})," into hive. Creating ",(0,o.jsx)(n.code,{children:"auxlib/"})," folder under the root directory of hive, and moving ",(0,o.jsx)(n.code,{children:"hudi-hadoop-mr-bundle-0.x.x-SNAPSHOT.jar"})," into ",(0,o.jsx)(n.code,{children:"auxlib"}),".\n",(0,o.jsx)(n.code,{children:"hudi-hadoop-mr-bundle-0.x.x-SNAPSHOT.jar"})," is at ",(0,o.jsx)(n.code,{children:"packaging/hudi-hadoop-mr-bundle/target"}),"."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["When Flink sql client connects hive metastore remotely, ",(0,o.jsx)(n.code,{children:"hive metastore"})," and ",(0,o.jsx)(n.code,{children:"hiveserver2"})," services need to be enabled, and the port number need to\nbe set correctly. Command to turn on the services:"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Enable hive metastore and hiveserver2\nnohup ./bin/hive --service metastore &\nnohup ./bin/hive --service hiveserver2 &\n\n# While modifying the jar package under auxlib, you need to restart the service.\n"})}),"\n",(0,o.jsx)(n.h3,{id:"sync-template",children:"Sync Template"}),"\n",(0,o.jsxs)(n.p,{children:["Flink hive sync now supports two hive sync mode, ",(0,o.jsx)(n.code,{children:"hms"})," and ",(0,o.jsx)(n.code,{children:"jdbc"}),". ",(0,o.jsx)(n.code,{children:"hms"})," mode only needs to configure metastore uris. For\nthe ",(0,o.jsx)(n.code,{children:"jdbc"})," mode, the JDBC attributes and metastore uris both need to be configured. The options template is as below:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-sql",children:"-- hms mode template\nCREATE TABLE t1(\n  uuid VARCHAR(20),\n  name VARCHAR(10),\n  age INT,\n  ts TIMESTAMP(3),\n  `partition` VARCHAR(20)\n)\nPARTITIONED BY (`partition`)\nWITH (\n  'connector' = 'hudi',\n  'path' = '${db_path}/t1',\n  'table.type' = 'COPY_ON_WRITE',  --If MERGE_ON_READ, hive query will not have output until the parquet file is generated\n  'hive_sync.enable' = 'true',     -- Required. To enable hive synchronization\n  'hive_sync.mode' = 'hms',        -- Required. Setting hive sync mode to hms, default jdbc\n  'hive_sync.metastore.uris' = 'thrift://${ip}:9083' -- Required. The port need set on hive-site.xml\n);\n\n\n-- jdbc mode template\nCREATE TABLE t1(\n  uuid VARCHAR(20),\n  name VARCHAR(10),\n  age INT,\n  ts TIMESTAMP(3),\n  `partition` VARCHAR(20)\n)\nPARTITIONED BY (`partition`)\nWITH (\n  'connector' = 'hudi',\n  'path' = '${db_path}/t1',\n  'table.type' = 'COPY_ON_WRITE',  -- If MERGE_ON_READ, hive query will not have output until the parquet file is generated\n  'hive_sync.enable' = 'true',     -- Required. To enable hive synchronization\n  'hive_sync.mode' = 'jdbc'        -- Required. Setting hive sync mode to hms, default jdbc\n  'hive_sync.metastore.uris' = 'thrift://${ip}:9083', -- Required. The port need set on hive-site.xml\n  'hive_sync.jdbc_url'='jdbc:hive2://${ip}:10000',    -- required, hiveServer port\n  'hive_sync.table'='${table_name}',                  -- required, hive table name\n  'hive_sync.db'='${db_name}',                        -- required, hive database name\n  'hive_sync.username'='${user_name}',                -- required, JDBC username\n  'hive_sync.password'='${password}'                  -- required, JDBC password\n);\n"})}),"\n",(0,o.jsx)(n.h3,{id:"query",children:"Query"}),"\n",(0,o.jsx)(n.p,{children:"While using hive beeline query, you need to enter settings:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"set hive.input.format = org.apache.hudi.hadoop.hive.HoodieCombineHiveInputFormat;\n"})})]})}function c(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var t=i(96540);const o={},s=t.createContext(o);function r(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);