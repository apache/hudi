"use strict";(globalThis.webpackChunkhudi=globalThis.webpackChunkhudi||[]).push([[52105],{23538:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>d,metadata:()=>t,toc:()=>o});const t=JSON.parse('{"id":"ingestion_flink","title":"Using Flink","description":"CDC Ingestion","source":"@site/versioned_docs/version-1.1.0/ingestion_flink.md","sourceDirName":".","slug":"/ingestion_flink","permalink":"/docs/ingestion_flink","draft":false,"unlisted":false,"editUrl":"https://github.com/apache/hudi/tree/asf-site/website/versioned_docs/version-1.1.0/ingestion_flink.md","tags":[],"version":"1.1.0","frontMatter":{"title":"Using Flink","keywords":["hudi","flink","streamer","ingestion"],"last_modified_at":"2025-11-22T04:53:57.000Z"},"sidebar":"docs","previous":{"title":"Using Spark","permalink":"/docs/hoodie_streaming_ingestion"},"next":{"title":"Using Kafka Connect","permalink":"/docs/ingestion_kafka_connect"}}');var s=i(74848),r=i(28453);const d={title:"Using Flink",keywords:["hudi","flink","streamer","ingestion"],last_modified_at:new Date("2025-11-22T04:53:57.000Z")},l=void 0,c={},o=[{value:"CDC Ingestion",id:"cdc-ingestion",level:2},{value:"Bulk Insert",id:"bulk-insert",level:2},{value:"Options",id:"options",level:3},{value:"Index Bootstrap",id:"index-bootstrap",level:2},{value:"Options",id:"options-1",level:3},{value:"How to use",id:"how-to-use",level:3},{value:"Changelog Mode",id:"changelog-mode",level:2},{value:"Options",id:"options-2",level:3},{value:"Append Mode",id:"append-mode",level:2},{value:"In-Memory Buffer Sort",id:"in-memory-buffer-sort",level:3},{value:"Disable Meta Fields",id:"disable-meta-fields",level:3},{value:"Inline Clustering",id:"inline-clustering",level:3},{value:"Async Clustering",id:"async-clustering",level:3},{value:"Clustering Plan Strategy",id:"clustering-plan-strategy",level:3},{value:"Using Bucket Index",id:"using-bucket-index",level:2},{value:"Comparison",id:"comparison",level:3},{value:"Bucket Index Examples",id:"bucket-index-examples",level:3},{value:"Simple Bucket Index",id:"simple-bucket-index",level:4},{value:"Partition-Level Bucket Index",id:"partition-level-bucket-index",level:4},{value:"Consistent Hashing Bucket Index",id:"consistent-hashing-bucket-index",level:4},{value:"Configuration Reference",id:"configuration-reference",level:3},{value:"Rate Limiting",id:"rate-limiting",level:2},{value:"Write Rate Limiting",id:"write-rate-limiting",level:3},{value:"Streaming Read Rate Limiting",id:"streaming-read-rate-limiting",level:3},{value:"Options",id:"options-3",level:3}];function a(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"cdc-ingestion",children:"CDC Ingestion"}),"\n",(0,s.jsx)(n.p,{children:"CDC (change data capture) keeps track of data changes evolving in a source system so a downstream process or system can act on those changes.\nWe recommend two ways for syncing CDC data into Hudi:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"slide1 title",src:i(32620).A+"",width:"1440",height:"626"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Use ",(0,s.jsx)(n.a,{href:"https://github.com/apache/flink-cdc",children:"Apache Flink-CDC"})," to directly connect to the database server and sync binlog data into Hudi.\nThe advantage is that it does not rely on message queues, but the disadvantage is that it puts pressure on the database server."]}),"\n",(0,s.jsx)(n.li,{children:"Consume data from a message queue (e.g., Kafka) using the Flink CDC format. The advantage is that it is highly scalable,\nbut the disadvantage is that it relies on message queues."}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsxs)(n.p,{children:["If the upstream data cannot guarantee ordering, you need to explicitly specify the ",(0,s.jsx)(n.code,{children:"ordering.fields"})," option."]})}),"\n",(0,s.jsx)(n.h2,{id:"bulk-insert",children:"Bulk Insert"}),"\n",(0,s.jsxs)(n.p,{children:["For snapshot data import requirements, if the snapshot data comes from other data sources, use the ",(0,s.jsx)(n.code,{children:"bulk_insert"})," mode to quickly\nimport the snapshot data into Hudi."]}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"bulk_insert"})," eliminates serialization and data merging. Data deduplication is skipped, so the user needs to guarantee data uniqueness."]})}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"bulk_insert"})," is more efficient in ",(0,s.jsx)(n.code,{children:"batch execution mode"}),". By default, ",(0,s.jsx)(n.code,{children:"batch execution mode"})," sorts the input records\nby partition path and writes these records to Hudi, which can avoid write\u2011performance degradation caused by\nfrequent file\u2011handle switching."]})}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsxs)(n.p,{children:["The parallelism of ",(0,s.jsx)(n.code,{children:"bulk_insert"})," is specified by ",(0,s.jsx)(n.code,{children:"write.tasks"}),". The parallelism affects the number of small files.\nIn theory, the parallelism of ",(0,s.jsx)(n.code,{children:"bulk_insert"})," equals the number of buckets. (In particular, when each bucket writes to the maximum file size, it\nrolls over to a new file handle.) The final number of files is greater than or equal to ",(0,s.jsx)(n.a,{href:"configurations#writebucket_assigntasks",children:(0,s.jsx)(n.code,{children:"write.bucket_assign.tasks"})}),"."]})}),"\n",(0,s.jsx)(n.h3,{id:"options",children:"Options"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Option Name"}),(0,s.jsx)(n.th,{children:"Required"}),(0,s.jsx)(n.th,{children:"Default"}),(0,s.jsx)(n.th,{children:"Remarks"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"write.operation"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"true"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"upsert"})}),(0,s.jsxs)(n.td,{children:["Set to ",(0,s.jsx)(n.code,{children:"bulk_insert"})," to enable this function"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"write.tasks"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"4"})}),(0,s.jsxs)(n.td,{children:["The parallelism of ",(0,s.jsx)(n.code,{children:"bulk_insert"}),"; the number of files \u2265 ",(0,s.jsx)(n.a,{href:"configurations#writebucket_assigntasks",children:(0,s.jsx)(n.code,{children:"write.bucket_assign.tasks"})})]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"write.bulk_insert.shuffle_input"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"true"})}),(0,s.jsx)(n.td,{children:"Whether to shuffle data by the input field before writing. Enabling this option reduces the number of small files but may introduce data\u2011skew risk"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"write.bulk_insert.sort_input"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"true"})}),(0,s.jsx)(n.td,{children:"Whether to sort data by the input field before writing. Enabling this option reduces the number of small files when a write task writes to multiple partitions"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"write.sort.memory"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"128"})}),(0,s.jsx)(n.td,{children:"Available managed memory for the sort operator; default is 128 MB"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"index-bootstrap",children:"Index Bootstrap"}),"\n",(0,s.jsxs)(n.p,{children:["For importing both snapshot data and incremental data: if the snapshot data has already been inserted into Hudi via ",(0,s.jsx)(n.a,{href:"#bulk-insert",children:"bulk insert"}),",\nusers can insert incremental data in real time and ensure the data is not duplicated by using the index bootstrap function."]}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsx)(n.p,{children:"If you find this process very time\u2011consuming, you can add resources to write in streaming mode while writing snapshot data,\nthen reduce the resources when writing incremental data (or enable the rate\u2011limit function)."})}),"\n",(0,s.jsx)(n.h3,{id:"options-1",children:"Options"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Option Name"}),(0,s.jsx)(n.th,{children:"Required"}),(0,s.jsx)(n.th,{children:"Default"}),(0,s.jsx)(n.th,{children:"Remarks"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"index.bootstrap.enabled"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"true"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:"When index bootstrap is enabled, the remaining records in the Hudi table are loaded into the Flink state at once"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"index.partition.regex"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"*"})}),(0,s.jsx)(n.td,{children:"Optimization option. Set a regular expression to filter partitions. By default, all partitions are loaded into Flink state"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"how-to-use",children:"How to use"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Use ",(0,s.jsx)(n.code,{children:"CREATE TABLE"})," to create a statement corresponding to the Hudi table. Note that ",(0,s.jsx)(n.code,{children:"table.type"})," must be correct."]}),"\n",(0,s.jsxs)(n.li,{children:["Set ",(0,s.jsx)(n.code,{children:"index.bootstrap.enabled"})," = ",(0,s.jsx)(n.code,{children:"true"})," to enable the index bootstrap function."]}),"\n",(0,s.jsxs)(n.li,{children:["Set the Flink checkpoint failure tolerance in ",(0,s.jsx)(n.code,{children:"flink-conf.yaml"}),": ",(0,s.jsx)(n.code,{children:"execution.checkpointing.tolerable-failed-checkpoints = n"})," (depending on Flink checkpoint scheduling times)."]}),"\n",(0,s.jsx)(n.li,{children:"Wait until the first checkpoint succeeds, indicating that the index bootstrap has completed."}),"\n",(0,s.jsx)(n.li,{children:"After the index bootstrap completes, users can exit and save the savepoint (or directly use the externalized checkpoint)."}),"\n",(0,s.jsxs)(n.li,{children:["Restart the job, setting ",(0,s.jsx)(n.code,{children:"index.bootstrap.enable"})," to ",(0,s.jsx)(n.code,{children:"false"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Index bootstrap is blocking, so checkpoints cannot complete during index bootstrap."}),"\n",(0,s.jsx)(n.li,{children:"Index bootstrap is triggered by the input data. Users need to ensure that there is at least one record in each partition."}),"\n",(0,s.jsxs)(n.li,{children:["Index bootstrap executes concurrently. Users can search logs for ",(0,s.jsx)(n.code,{children:"finish loading the index under partition"})," and ",(0,s.jsx)(n.code,{children:"Load record from file"})," to observe the index\u2011bootstrap progress."]}),"\n",(0,s.jsx)(n.li,{children:"The first successful checkpoint indicates that the index bootstrap has completed. There is no need to load the index again when recovering from the checkpoint."}),"\n"]})}),"\n",(0,s.jsx)(n.h2,{id:"changelog-mode",children:"Changelog Mode"}),"\n",(0,s.jsx)(n.p,{children:"Hudi can keep all the intermediate changes (I / -U / U / D) of messages, then consume them through stateful computing in Flink to build a near\u2011real\u2011time\ndata\u2011warehouse ETL pipeline (incremental computing). Hudi MOR tables store messages as rows, which supports the retention of all change logs (integration at the format level).\nAll changelog records can be consumed with the Flink streaming reader."}),"\n",(0,s.jsx)(n.h3,{id:"options-2",children:"Options"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Option Name"}),(0,s.jsx)(n.th,{children:"Required"}),(0,s.jsx)(n.th,{children:"Default"}),(0,s.jsx)(n.th,{children:"Remarks"})]})}),(0,s.jsx)(n.tbody,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"changelog.enabled"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsxs)(n.td,{children:["It is turned off by default, to have the ",(0,s.jsx)(n.code,{children:"upsert"})," semantics, only the merged messages are ensured to be kept, intermediate changes may be merged. Setting to true to support consumption of all changes"]})]})})]}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsx)(n.p,{children:"Batch (snapshot) reads still merge all the intermediate changes, regardless of whether the format has stored the intermediate changelog messages."})}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsxs)(n.p,{children:["After setting ",(0,s.jsx)(n.code,{children:"changelog.enable"})," to ",(0,s.jsx)(n.code,{children:"true"}),", the retention of changelog records is best\u2011effort only: the asynchronous compaction task will merge the changelog records into one record, so if the\nstream source does not consume in a timely manner, only the merged record for each key can be read after compaction. The solution is to reserve buffer time for the reader by adjusting the compaction strategy, such as\nthe compaction options ",(0,s.jsx)(n.code,{children:"compaction.delta_commits"})," and ",(0,s.jsx)(n.code,{children:"compaction.delta_seconds"}),"."]})}),"\n",(0,s.jsx)(n.h2,{id:"append-mode",children:"Append Mode"}),"\n",(0,s.jsxs)(n.p,{children:["For ",(0,s.jsx)(n.code,{children:"INSERT"})," mode write operations, new Parquet files are written directly, and the ",(0,s.jsx)(n.a,{href:"/docs/file_sizing",children:"auto\u2011file sizing"})," is not enabled."]}),"\n",(0,s.jsx)(n.h3,{id:"in-memory-buffer-sort",children:"In-Memory Buffer Sort"}),"\n",(0,s.jsx)(n.p,{children:"For append-only workloads, Hudi supports in-memory buffer sorting to improve Parquet compression ratio. When enabled, data is sorted within the write buffer before being flushed to disk. This improves columnar file compression efficiency by grouping similar values together."}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Option Name"}),(0,s.jsx)(n.th,{children:"Required"}),(0,s.jsx)(n.th,{children:"Default"}),(0,s.jsx)(n.th,{children:"Remarks"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"write.buffer.sort.enabled"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:"Whether to enable buffer sort within append write function. Improves Parquet compression ratio by sorting data before writing"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"write.buffer.sort.keys"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"N/A"})}),(0,s.jsxs)(n.td,{children:["Sort keys concatenated by comma (e.g., ",(0,s.jsx)(n.code,{children:"col1,col2"}),"). Required when ",(0,s.jsx)(n.code,{children:"write.buffer.sort.enabled"})," is ",(0,s.jsx)(n.code,{children:"true"})]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"write.buffer.size"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"1000"})}),(0,s.jsx)(n.td,{children:"Buffer size in number of records. When buffer reaches this size, data is sorted and flushed to disk"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"disable-meta-fields",children:"Disable Meta Fields"}),"\n",(0,s.jsxs)(n.p,{children:["For append-only workloads where Hudi metadata fields (e.g., ",(0,s.jsx)(n.code,{children:"_hoodie_commit_time"}),", ",(0,s.jsx)(n.code,{children:"_hoodie_record_key"}),") are not needed, you can disable them to reduce storage overhead. This is useful when integrating with external systems that don't require Hudi-specific metadata."]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Option Name"}),(0,s.jsx)(n.th,{children:"Required"}),(0,s.jsx)(n.th,{children:"Default"}),(0,s.jsx)(n.th,{children:"Remarks"})]})}),(0,s.jsx)(n.tbody,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"hoodie.populate.meta.fields"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"true"})}),(0,s.jsxs)(n.td,{children:["Whether to populate Hudi meta fields. Set to ",(0,s.jsx)(n.code,{children:"false"})," for append-only workloads to reduce storage overhead. Note: Some Hudi features may not work when disabled"]})]})})]}),"\n",(0,s.jsxs)(n.p,{children:["Hudi supports rich clustering strategies to optimize the files layout for ",(0,s.jsx)(n.code,{children:"INSERT"})," mode:"]}),"\n",(0,s.jsx)(n.h3,{id:"inline-clustering",children:"Inline Clustering"}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsx)(n.p,{children:"Only Copy\u2011on\u2011Write tables are supported."})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Option Name"}),(0,s.jsx)(n.th,{children:"Required"}),(0,s.jsx)(n.th,{children:"Default"}),(0,s.jsx)(n.th,{children:"Remarks"})]})}),(0,s.jsx)(n.tbody,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"write.insert.cluster"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:"Whether to merge small files while ingesting. For COW tables, enable this option to use the small\u2011file merging strategy (no deduplication for keys, but throughput will be affected)"})]})})]}),"\n",(0,s.jsx)(n.h3,{id:"async-clustering",children:"Async Clustering"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Option Name"}),(0,s.jsx)(n.th,{children:"Required"}),(0,s.jsx)(n.th,{children:"Default"}),(0,s.jsx)(n.th,{children:"Remarks"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"clustering.schedule.enabled"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:"Whether to schedule the clustering plan during the write process; default is false"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"clustering.delta_commits"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"4"})}),(0,s.jsxs)(n.td,{children:["Delta commits for scheduling the clustering plan; only valid when ",(0,s.jsx)(n.code,{children:"clustering.schedule.enabled"})," is true"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"clustering.async.enabled"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:"Whether to execute the clustering plan asynchronously; default is false"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"clustering.tasks"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"4"})}),(0,s.jsx)(n.td,{children:"Parallelism of the clustering tasks"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"clustering.plan.strategy.target.file.max.bytes"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"1024*1024*1024"})}),(0,s.jsx)(n.td,{children:"The target file size for the clustering group; default is 1 GB"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"clustering.plan.strategy.small.file.limit"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"600"})}),(0,s.jsx)(n.td,{children:"Files smaller than the threshold (in MB) are candidates for clustering"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"clustering.plan.strategy.sort.columns"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"N/A"})}),(0,s.jsx)(n.td,{children:"The columns to sort by when clustering"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"clustering-plan-strategy",children:"Clustering Plan Strategy"}),"\n",(0,s.jsx)(n.p,{children:"Custom clustering strategy is supported."}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Option Name"}),(0,s.jsx)(n.th,{children:"Required"}),(0,s.jsx)(n.th,{children:"Default"}),(0,s.jsx)(n.th,{children:"Remarks"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"clustering.plan.partition.filter.mode"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"NONE"})}),(0,s.jsxs)(n.td,{children:["Valid options 1) ",(0,s.jsx)(n.code,{children:"NONE"}),": no limit; 2) ",(0,s.jsx)(n.code,{children:"RECENT_DAYS"}),": choose partitions that represent recent days; 3) ",(0,s.jsx)(n.code,{children:"SELECTED_PARTITIONS"}),": specific partitions"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"clustering.plan.strategy.daybased.lookback.partitions"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"2"})}),(0,s.jsxs)(n.td,{children:["Number of partitions to look back; valid for ",(0,s.jsx)(n.code,{children:"RECENT_DAYS"})," mode"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"clustering.plan.strategy.cluster.begin.partition"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"N/A"})}),(0,s.jsxs)(n.td,{children:["Valid for ",(0,s.jsx)(n.code,{children:"SELECTED_PARTITIONS"})," mode; specify the partition to begin with (inclusive)"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"clustering.plan.strategy.cluster.end.partition"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"N/A"})}),(0,s.jsxs)(n.td,{children:["Valid for ",(0,s.jsx)(n.code,{children:"SELECTED_PARTITIONS"})," mode; specify the partition to end with (inclusive)"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"clustering.plan.strategy.partition.regex.pattern"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"N/A"})}),(0,s.jsx)(n.td,{children:"The regex to filter the partitions"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"clustering.plan.strategy.partition.selected"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"N/A"})}),(0,s.jsx)(n.td,{children:"Specific partitions, separated by commas"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"using-bucket-index",children:"Using Bucket Index"}),"\n",(0,s.jsx)(n.p,{children:"Hudi Flink writer supports two types of writer indexes:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Flink state (default)"}),"\n",(0,s.jsx)(n.li,{children:"Bucket index (3 variants: simple, partition-level, consistent hashing)"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"comparison",children:"Comparison"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Feature"}),(0,s.jsx)(n.th,{children:"Bucket Index"}),(0,s.jsx)(n.th,{children:"Flink State Index"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"How It Works"}),(0,s.jsx)(n.td,{children:"Uses a deterministic hash algorithm to shuffle records into buckets"}),(0,s.jsx)(n.td,{children:"Uses the Flink state backend to store index data: mappings of record keys to their residing file group's file IDs"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Computing/Storage Cost"}),(0,s.jsx)(n.td,{children:"No cost for state\u2011backend indexing"}),(0,s.jsx)(n.td,{children:"Has computing and storage cost for maintaining state; can become a bottleneck when working with large Hudi tables"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Performance"}),(0,s.jsx)(n.td,{children:"Better performance due to no state overhead"}),(0,s.jsx)(n.td,{children:"Performance depends on state backend efficiency"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"File Group Flexibility"}),(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.strong,{children:"Simple"}),": Fixed number of buckets (file groups) per partition, immutable once set",(0,s.jsx)("br",{}),(0,s.jsx)(n.strong,{children:"Partition-Level"}),": Different fixed buckets per partition via regex patterns (rescaling requires Spark procedure)",(0,s.jsx)("br",{}),(0,s.jsx)(n.strong,{children:"Consistent Hashing"}),": Auto-resizing buckets via clustering"]}),(0,s.jsx)(n.td,{children:"Dynamically assigns records to file groups based on current table layout; no pre-configured limits on file group count"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Cross\u2011Partition Changes"}),(0,s.jsx)(n.td,{children:"Cannot handle changes among partitions (unless input is a CDC stream)"}),(0,s.jsx)(n.td,{children:"No limit on handling cross\u2011partition changes"})]})]})]}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsxs)(n.p,{children:["Bucket index supports only the ",(0,s.jsx)(n.code,{children:"UPSERT"})," write operation and cannot be used with the ",(0,s.jsx)(n.a,{href:"#append-mode",children:"append mode"})," in Flink."]})}),"\n",(0,s.jsx)(n.h3,{id:"bucket-index-examples",children:"Bucket Index Examples"}),"\n",(0,s.jsx)(n.h4,{id:"simple-bucket-index",children:"Simple Bucket Index"}),"\n",(0,s.jsx)(n.p,{children:"Fixed number of buckets across all partitions:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE orders_simple_bucket (\n  order_id BIGINT,\n  customer_id BIGINT,\n  amount DOUBLE,\n  order_date STRING,\n  ts BIGINT,\n  PRIMARY KEY (order_id) NOT ENFORCED\n) PARTITIONED BY (order_date)\nWITH (\n  'connector' = 'hudi',\n  'path' = 'hdfs:///warehouse/orders_simple',\n  'table.type' = 'MERGE_ON_READ',\n  \n  -- Bucket Index Configuration\n  'index.type' = 'BUCKET',\n  'hoodie.bucket.index.engine' = 'SIMPLE',\n  'hoodie.bucket.index.hash.field' = 'order_id',\n  'hoodie.bucket.index.num.buckets' = '16'  -- Fixed 16 buckets for ALL partitions\n);\n\n-- Insert data\nINSERT INTO orders_simple_bucket VALUES\n  (1, 100, 99.99, '2024-01-15', 1000),\n  (2, 101, 49.99, '2024-02-20', 2000);\n"})}),"\n",(0,s.jsx)(n.h4,{id:"partition-level-bucket-index",children:"Partition-Level Bucket Index"}),"\n",(0,s.jsx)(n.p,{children:"Different bucket counts for different partitions based on regex patterns:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE orders_partition_bucket (\n  order_id BIGINT,\n  customer_id BIGINT,\n  amount DOUBLE,\n  order_date STRING,\n  ts BIGINT,\n  PRIMARY KEY (order_id) NOT ENFORCED\n) PARTITIONED BY (order_date)\nWITH (\n  'connector' = 'hudi',\n  'path' = 'hdfs:///warehouse/orders_partition',\n  'table.type' = 'MERGE_ON_READ',\n  \n  -- Bucket Index Configuration\n  'index.type' = 'BUCKET',\n  'hoodie.bucket.index.engine' = 'SIMPLE',\n  'hoodie.bucket.index.hash.field' = 'order_id',\n  \n  -- Partition-Level Configuration\n  'hoodie.bucket.index.num.buckets' = '8',  -- Default for non-matching partitions\n  'hoodie.bucket.index.partition.rule.type' = 'regex',\n  -- Black Friday (11-24), Cyber Monday (11-27), Christmas (12-25) get 128 buckets\n  -- All other dates get 8 buckets (default)\n  'hoodie.bucket.index.partition.expressions' = '\\\\d{4}-(11-(24|27)|12-25),128'\n);\n\n-- Insert data - bucket count varies by partition\nINSERT INTO orders_partition_bucket VALUES\n  (1, 100, 999.99, '2024-11-24', 1000),  -- Black Friday: 128 buckets\n  (2, 101, 499.99, '2024-11-27', 2000),  -- Cyber Monday: 128 buckets\n  (3, 102, 299.99, '2024-12-25', 3000),  -- Christmas: 128 buckets\n  (4, 103, 49.99, '2024-01-15', 4000);   -- Regular day: 8 buckets\n"})}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsxs)(n.p,{children:["For existing simple bucket index tables, use the Spark ",(0,s.jsx)(n.code,{children:"partition_bucket_index_manager"})," procedure to upgrade to partition-level bucket index. After upgrade, Flink writers automatically load the expressions from table metadata."]})}),"\n",(0,s.jsx)(n.h4,{id:"consistent-hashing-bucket-index",children:"Consistent Hashing Bucket Index"}),"\n",(0,s.jsx)(n.p,{children:"Auto-expanding buckets via clustering (requires Spark for execution):"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE orders_consistent_hashing (\n  order_id BIGINT,\n  customer_id BIGINT,\n  amount DOUBLE,\n  order_date STRING,\n  ts BIGINT,\n  PRIMARY KEY (order_id) NOT ENFORCED\n) PARTITIONED BY (order_date)\nWITH (\n  'connector' = 'hudi',\n  'path' = 'hdfs:///warehouse/orders_consistent',\n  'table.type' = 'MERGE_ON_READ',\n  \n  -- Consistent Hashing Bucket Index\n  'index.type' = 'BUCKET',\n  'hoodie.bucket.index.engine' = 'CONSISTENT_HASHING',\n  'hoodie.bucket.index.hash.field' = 'order_id',\n  \n  -- Initial and boundary configuration\n  'hoodie.bucket.index.num.buckets' = '4',      -- Initial bucket count\n  'hoodie.bucket.index.min.num.buckets' = '2',  -- Minimum allowed\n  'hoodie.bucket.index.max.num.buckets' = '128', -- Maximum allowed\n  \n  -- Clustering configuration (required for auto-resizing)\n  'clustering.schedule.enabled' = 'true',\n  'clustering.delta_commits' = '5',  -- Schedule clustering every 5 commits\n  'clustering.plan.strategy.class' = 'org.apache.hudi.client.clustering.plan.strategy.FlinkConsistentBucketClusteringPlanStrategy',\n  \n  -- File size thresholds for bucket resizing\n  'hoodie.clustering.plan.strategy.target.file.max.bytes' = '1073741824',  -- 1GB max\n  'hoodie.clustering.plan.strategy.small.file.limit' = '314572800'  -- 300MB min\n);\n\n-- Insert data - buckets auto-adjust based on file sizes\nINSERT INTO orders_consistent_hashing \nSELECT * FROM source_stream;\n"})}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Consistent hashing bucket index"})," automatically adjusts bucket counts via clustering. Flink can schedule clustering plans, but execution currently requires Spark. Start with ",(0,s.jsx)(n.code,{children:"hoodie.bucket.index.num.buckets"})," and the system dynamically resizes based on file sizes within the min/max bounds."]})}),"\n",(0,s.jsx)(n.h3,{id:"configuration-reference",children:"Configuration Reference"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Option"}),(0,s.jsx)(n.th,{children:"Applies To"}),(0,s.jsx)(n.th,{children:"Default"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"index.type"})}),(0,s.jsx)(n.td,{children:"All"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"FLINK_STATE"})}),(0,s.jsxs)(n.td,{children:["Set to ",(0,s.jsx)(n.code,{children:"BUCKET"})," to enable bucket index"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"hoodie.bucket.index.engine"})}),(0,s.jsx)(n.td,{children:"All"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"SIMPLE"})}),(0,s.jsxs)(n.td,{children:["Engine type: ",(0,s.jsx)(n.code,{children:"SIMPLE"})," or ",(0,s.jsx)(n.code,{children:"CONSISTENT_HASHING"})]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"hoodie.bucket.index.hash.field"})}),(0,s.jsx)(n.td,{children:"All"}),(0,s.jsx)(n.td,{children:"Record key"}),(0,s.jsx)(n.td,{children:"Fields to hash for bucketing; can be a subset of record key"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"hoodie.bucket.index.num.buckets"})}),(0,s.jsx)(n.td,{children:"All"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"4"})}),(0,s.jsx)(n.td,{children:"Default bucket count per partition (initial count for consistent hashing)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"hoodie.bucket.index.partition.expressions"})}),(0,s.jsx)(n.td,{children:"Partition-Level"}),(0,s.jsx)(n.td,{children:"N/A"}),(0,s.jsxs)(n.td,{children:["Regex patterns and bucket counts: ",(0,s.jsx)(n.code,{children:"pattern1,count1;pattern2,count2"})]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"hoodie.bucket.index.partition.rule.type"})}),(0,s.jsx)(n.td,{children:"Partition-Level"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"regex"})}),(0,s.jsx)(n.td,{children:"Rule parser type"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"hoodie.bucket.index.min.num.buckets"})}),(0,s.jsx)(n.td,{children:"Consistent Hashing"}),(0,s.jsx)(n.td,{children:"N/A"}),(0,s.jsx)(n.td,{children:"Minimum bucket count (prevents over-merging)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"hoodie.bucket.index.max.num.buckets"})}),(0,s.jsx)(n.td,{children:"Consistent Hashing"}),(0,s.jsx)(n.td,{children:"N/A"}),(0,s.jsx)(n.td,{children:"Maximum bucket count (prevents unlimited expansion)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"clustering.schedule.enabled"})}),(0,s.jsx)(n.td,{children:"Consistent Hashing"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsxs)(n.td,{children:["Must be ",(0,s.jsx)(n.code,{children:"true"})," for auto-resizing"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"clustering.plan.strategy.class"})}),(0,s.jsx)(n.td,{children:"Consistent Hashing"}),(0,s.jsx)(n.td,{children:"N/A"}),(0,s.jsxs)(n.td,{children:["Set to ",(0,s.jsx)(n.code,{children:"FlinkConsistentBucketClusteringPlanStrategy"})]})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"rate-limiting",children:"Rate Limiting"}),"\n",(0,s.jsx)(n.p,{children:"Hudi provides rate limiting capabilities for both writes and streaming reads to control data flow and prevent performance degradation."}),"\n",(0,s.jsx)(n.h3,{id:"write-rate-limiting",children:"Write Rate Limiting"}),"\n",(0,s.jsx)(n.p,{children:"In many scenarios, users publish both historical snapshot data and real\u2011time incremental updates to the same message queue, then consume from the earliest offset using Flink to ingest everything into Hudi. This backfill pattern can cause performance issues:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"High burst throughput"}),": The entire historical dataset arrives at once, overwhelming the writer with a massive volume of records"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scattered writes across table partitions"}),": Historical records arrive scattered across many different table partitions (e.g., records from many different dates if the table is partitioned by date). This forces the writer to constantly switch between partitions, keeping many file handles open simultaneously and causing memory pressure, which degrades write performance and causes throughput instability"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"write.rate.limit"})," option helps smooth out the ingestion flow, preventing traffic jitter and improving stability during backfill operations."]}),"\n",(0,s.jsx)(n.h3,{id:"streaming-read-rate-limiting",children:"Streaming Read Rate Limiting"}),"\n",(0,s.jsxs)(n.p,{children:["For Flink streaming reads, rate limiting helps avoid backpressure when processing large workloads. The ",(0,s.jsx)(n.code,{children:"read.splits.limit"})," option controls the maximum number of input splits allowed to be read in each check interval. This feature is particularly useful when:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Reading from tables with a large backlog of commits"}),"\n",(0,s.jsx)(n.li,{children:"Preventing downstream operators from being overwhelmed"}),"\n",(0,s.jsx)(n.li,{children:"Controlling resource consumption during catch-up scenarios"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["The average read rate can be calculated as: ",(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.code,{children:"read.splits.limit"})," / ",(0,s.jsx)(n.code,{children:"read.streaming.check-interval"})]})," splits per second."]}),"\n",(0,s.jsx)(n.h3,{id:"options-3",children:"Options"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Option Name"}),(0,s.jsx)(n.th,{children:"Required"}),(0,s.jsx)(n.th,{children:"Default"}),(0,s.jsx)(n.th,{children:"Remarks"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"write.rate.limit"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"0"})}),(0,s.jsx)(n.td,{children:"Write record rate limit per second to prevent traffic jitter and improve stability. Default is 0 (no limit)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"read.splits.limit"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"Integer.MAX_VALUE"})}),(0,s.jsxs)(n.td,{children:["Maximum number of splits allowed to read in each instant check for streaming reads. Average read rate = ",(0,s.jsx)(n.code,{children:"read.splits.limit"}),"/",(0,s.jsx)(n.code,{children:"read.streaming.check-interval"}),". Default is no limit"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"read.streaming.check-interval"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"false"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"60"})}),(0,s.jsx)(n.td,{children:"Check interval in seconds for streaming reads. Default is 60 seconds (1 minute)"})]})]})]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(a,{...e})}):a(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>d,x:()=>l});var t=i(96540);const s={},r=t.createContext(s);function d(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:d(e.components),t.createElement(r.Provider,{value:n},e.children)}},32620:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/cdc-2-hudi-d151389758f4ce3fd873c1258b0a8ce5.png"}}]);