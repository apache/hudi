"use strict";(globalThis.webpackChunkhudi=globalThis.webpackChunkhudi||[]).push([[17650],{11470(e,n,i){i.d(n,{A:()=>T});var r=i(96540),s=i(34164),a=i(17559),t=i(23104),l=i(56347),o=i(205),d=i(57485),c=i(31682),h=i(70679);function u(e){return r.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function p(e){const{values:n,children:i}=e;return(0,r.useMemo)(()=>{const e=n??function(e){return u(e).map(({props:{value:e,label:n,attributes:i,default:r}})=>({value:e,label:n,attributes:i,default:r}))}(i);return function(e){const n=(0,c.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,i])}function g({value:e,tabValues:n}){return n.some(n=>n.value===e)}function m({queryString:e=!1,groupId:n}){const i=(0,l.W6)(),s=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,d.aZ)(s),(0,r.useCallback)(e=>{if(!s)return;const n=new URLSearchParams(i.location.search);n.set(s,e),i.replace({...i.location,search:n.toString()})},[s,i])]}function f(e){const{defaultValue:n,queryString:i=!1,groupId:s}=e,a=p(e),[t,l]=(0,r.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!g({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const i=n.find(e=>e.default)??n[0];if(!i)throw new Error("Unexpected error: 0 tabValues");return i.value}({defaultValue:n,tabValues:a})),[d,c]=m({queryString:i,groupId:s}),[u,f]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[i,s]=(0,h.Dv)(n);return[i,(0,r.useCallback)(e=>{n&&s.set(e)},[n,s])]}({groupId:s}),x=(()=>{const e=d??u;return g({value:e,tabValues:a})?e:null})();(0,o.A)(()=>{x&&l(x)},[x]);return{selectedValue:t,selectValue:(0,r.useCallback)(e=>{if(!g({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);l(e),c(e),f(e)},[c,f,a]),tabValues:a}}var x=i(92303);const j="tabList__CuJ",b="tabItem_LNqP";var w=i(74848);function v({className:e,block:n,selectedValue:i,selectValue:r,tabValues:a}){const l=[],{blockElementScrollPositionUntilNextRender:o}=(0,t.a_)(),d=e=>{const n=e.currentTarget,s=l.indexOf(n),t=a[s].value;t!==i&&(o(n),r(t))},c=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const i=l.indexOf(e.currentTarget)+1;n=l[i]??l[0];break}case"ArrowLeft":{const i=l.indexOf(e.currentTarget)-1;n=l[i]??l[l.length-1];break}}n?.focus()};return(0,w.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":n},e),children:a.map(({value:e,label:n,attributes:r})=>(0,w.jsx)("li",{role:"tab",tabIndex:i===e?0:-1,"aria-selected":i===e,ref:e=>{l.push(e)},onKeyDown:c,onClick:d,...r,className:(0,s.A)("tabs__item",b,r?.className,{"tabs__item--active":i===e}),children:n??e},e))})}function y({lazy:e,children:n,selectedValue:i}){const a=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=a.find(e=>e.props.value===i);return e?(0,r.cloneElement)(e,{className:(0,s.A)("margin-top--md",e.props.className)}):null}return(0,w.jsx)("div",{className:"margin-top--md",children:a.map((e,n)=>(0,r.cloneElement)(e,{key:n,hidden:e.props.value!==i}))})}function k(e){const n=f(e);return(0,w.jsxs)("div",{className:(0,s.A)(a.G.tabs.container,"tabs-container",j),children:[(0,w.jsx)(v,{...n,...e}),(0,w.jsx)(y,{...n,...e})]})}function T(e){const n=(0,x.A)();return(0,w.jsx)(k,{...e,children:u(e.children)},String(n))}},19365(e,n,i){i.d(n,{A:()=>t});i(96540);var r=i(34164);const s="tabItem_Ymn6";var a=i(74848);function t({children:e,hidden:n,className:i}){return(0,a.jsx)("div",{role:"tabpanel",className:(0,r.A)(s,i),hidden:n,children:e})}},28453(e,n,i){i.d(n,{R:()=>t,x:()=>l});var r=i(96540);const s={},a=r.createContext(s);function t(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),r.createElement(a.Provider,{value:n},e.children)}},81748(e,n,i){i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"release-1.0","title":"Release 1.0","description":"This page contains release notes for all Apache Hudi 1.0.x releases, including:","source":"@site/releases/release-1.0.md","sourceDirName":".","slug":"/release-1.0","permalink":"/releases/release-1.0","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Release 1.0","layout":"releases","toc":true},"sidebar":"releases","previous":{"title":"Release 1.1","permalink":"/releases/release-1.1"},"next":{"title":"Release 0.15","permalink":"/releases/release-0.15"}}');var s=i(74848),a=i(28453);i(11470),i(19365);const t={title:"Release 1.0",layout:"releases",toc:!0},l=void 0,o={},d=[{value:"Release 1.0.0",id:"release-100",level:2},{value:"Migration Guide",id:"migration-guide",level:2},{value:"Bundle Updates",id:"bundle-updates",level:2},{value:"Highlights",id:"highlights",level:2},{value:"Format changes",id:"format-changes",level:3},{value:"Timeline",id:"timeline",level:4},{value:"Log File Format",id:"log-file-format",level:4},{value:"Compatibility with Old Formats",id:"compatibility-with-old-formats",level:3},{value:"Concurrency Control",id:"concurrency-control",level:3},{value:"New Indices",id:"new-indices",level:3},{value:"Secondary Index",id:"secondary-index",level:4},{value:"Partition Stats Index",id:"partition-stats-index",level:4},{value:"Expression Index",id:"expression-index",level:4},{value:"Partial Updates",id:"partial-updates",level:3},{value:"Multiple Base File Formats in a single table",id:"multiple-base-file-formats-in-a-single-table",level:3},{value:"API Changes",id:"api-changes",level:3},{value:"Record Merger API",id:"record-merger-api",level:4},{value:"Positional Merging with Filegroup Reader",id:"positional-merging-with-filegroup-reader",level:4},{value:"Flink Enhancements",id:"flink-enhancements",level:3},{value:"Call to Action",id:"call-to-action",level:2},{value:"Known Regressions",id:"known-regressions",level:2},{value:"Release 1.0.1",id:"release-101",level:2},{value:"Migration Guide",id:"migration-guide-1",level:2},{value:"Bug fixes",id:"bug-fixes",level:3},{value:"Known Regressions",id:"known-regressions-1",level:2},{value:"Raw Release Notes",id:"raw-release-notes",level:2},{value:"Release 1.0.2",id:"release-102",level:2},{value:"Migration Guide",id:"migration-guide-2",level:2},{value:"Bug fixes",id:"bug-fixes-1",level:3},{value:"Known Regressions",id:"known-regressions-2",level:2},{value:"Raw Release Notes",id:"raw-release-notes-1",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",h4:"h4",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"This page contains release notes for all Apache Hudi 1.0.x releases, including:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#release-100",children:"Release 1.0.0"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#release-101",children:"Release 1.0.1"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#release-102",children:"Release 1.0.2"})}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"release-100",children:(0,s.jsx)(n.a,{href:"https://github.com/apache/hudi/releases/tag/release-1.0.0",children:"Release 1.0.0"})}),"\n",(0,s.jsx)(n.p,{children:"Apache Hudi 1.0.0 is a major milestone release of Apache Hudi. This release contains significant format changes and new exciting features\nas we will see below."}),"\n",(0,s.jsx)(n.h2,{id:"migration-guide",children:"Migration Guide"}),"\n",(0,s.jsxs)(n.p,{children:["We encourage users to try the ",(0,s.jsx)(n.strong,{children:"1.0.0"})," features on new tables first. The 1.0 general availability (GA) release will\nsupport automatic table upgrades from 0.x versions while also ensuring full backward compatibility when reading 0.x\nHudi tables using 1.0, ensuring a seamless migration experience."]}),"\n",(0,s.jsxs)(n.p,{children:["This release comes with ",(0,s.jsx)(n.strong,{children:"backward compatible writes"})," i.e. 1.0.0 can write in both the table version 8 (latest) and older\ntable version 6 (corresponds to 0.14 & above) formats. Automatic upgrades for tables from 0.x versions are fully\nsupported, minimizing migration challenges. Until all the readers are upgraded, users can still deploy 1.0.0 binaries\nfor the writers and leverage backward compatible writes to continue writing the tables in the older format. Once the readers\nare fully upgraded, users can switch to the latest format through a config change. We recommend users to follow the upgrade\nsteps mentioned in the ",(0,s.jsx)(n.a,{href:"/docs/deployment#upgrading-to-100",children:"migration guide"})," to ensure a smooth transition."]}),"\n",(0,s.jsx)(n.admonition,{type:"caution",children:(0,s.jsxs)(n.p,{children:["Most things are seamlessly handled by the auto upgrade process, but there are some limitations. Please read through the\nlimitations of the upgrade downgrade process before proceeding to migrate. Please check the ",(0,s.jsx)(n.a,{href:"/docs/deployment#upgrading-to-100",children:"migration guide"}),"\nand ",(0,s.jsx)(n.a,{href:"https://github.com/apache/hudi/blob/master/rfc/rfc-78/rfc-78.md#support-matrix-for-different-readers-and-writers",children:"RFC-78"})," for more details."]})}),"\n",(0,s.jsx)(n.h2,{id:"bundle-updates",children:"Bundle Updates"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Same bundles supported in the ",(0,s.jsx)(n.a,{href:"release-0.15#new-spark-bundles",children:"0.15.0 release"})," are still supported."]}),"\n",(0,s.jsx)(n.li,{children:"New Flink Bundles to support Flink 1.19 and Flink 1.20."}),"\n",(0,s.jsx)(n.li,{children:"In this release, we have deprecated support for Spark 3.2 or lower version in Spark 3."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"highlights",children:"Highlights"}),"\n",(0,s.jsx)(n.h3,{id:"format-changes",children:"Format changes"}),"\n",(0,s.jsxs)(n.p,{children:["The main epic covering all the format changes is ",(0,s.jsx)(n.a,{href:"https://github.com/apache/hudi/issues/15964",children:"this GitHub issue"}),", which is also covered in the ",(0,s.jsx)(n.a,{href:"/learn/tech-specs-1point0",children:"Hudi 1.0 tech specification"}),". The following are the main highlights with respect to format changes:"]}),"\n",(0,s.jsx)(n.h4,{id:"timeline",children:"Timeline"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The active and archived timeline dichotomy has been done away with a more scalable LSM tree based\ntimeline. The timeline layout is now more organized and efficient for time-range queries and scaling to infinite history."}),"\n",(0,s.jsxs)(n.li,{children:["As a result, timeline layout has been changed, and it has been moved to ",(0,s.jsx)(n.code,{children:".hoodie/timeline"})," directory under the base\npath of the table."]}),"\n",(0,s.jsxs)(n.li,{children:["There are changes to the timeline instant files as well:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"All commit metadata is serialized to Avro, allowing for future compatibility and uniformity in metadata across all\nactions."}),"\n",(0,s.jsx)(n.li,{children:"Instant files for completed actions now include a completion time."}),"\n",(0,s.jsxs)(n.li,{children:["Action for the pending clustering instant is now renamed to ",(0,s.jsx)(n.code,{children:"clustering"})," to make it distinct from other\n",(0,s.jsx)(n.code,{children:"replacecommit"})," actions."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"log-file-format",children:"Log File Format"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["In addition to the keys in the log file header, we also store record positions. Refer to the\nlatest ",(0,s.jsx)(n.a,{href:"/learn/tech-specs-1point0#log-format",children:"spec"})," for more details. This allows us to do position-based merging (apart\nfrom key-based merging) and skip pages based on positions."]}),"\n",(0,s.jsx)(n.li,{children:"Log file name will now have the deltacommit instant time instead of base commit instant time."}),"\n",(0,s.jsx)(n.li,{children:"The new log file format also enables fast partial updates with low storage overhead."}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"compatibility-with-old-formats",children:"Compatibility with Old Formats"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Backward Compatible writes:"})," Hudi 1.0 writes now support writing in both the table version 8 (latest) and older table version 6 (corresponds to 0.14 & above) formats, ensuring seamless\nintegration with existing setups."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Automatic upgrades"}),": for tables from 0.x versions are fully supported, minimizing migration challenges. We also recommend users first try migrating to 0.14 &\nabove, if you have advanced setups with multiple readers/writers/table services."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"concurrency-control",children:"Concurrency Control"}),"\n",(0,s.jsxs)(n.p,{children:["1.0.0 introduces ",(0,s.jsx)(n.strong,{children:"Non-Blocking Concurrency Control (NBCC)"}),", enabling multi-stream concurrent ingestion without\nconflict. This is a general-purpose concurrency model aimed at the stream processing or high-contention/frequent writing\nscenarios. In contrast to Optimistic Concurrency Control, where writers abort the transaction if there is a hint of\ncontention, this innovation allows multiple streaming writes to the same Hudi table without any overhead of conflict\nresolution, while keeping the semantics of event-time ordering found in streaming systems, along with asynchronous table\nservice such as compaction, archiving and cleaning."]}),"\n",(0,s.jsxs)(n.p,{children:["To learn more about NBCC, refer to ",(0,s.jsx)(n.a,{href:"/blog/2024/12/06/non-blocking-concurrency-control",children:"this blog"})," which also includes a demo with Flink writers."]}),"\n",(0,s.jsx)(n.h3,{id:"new-indices",children:"New Indices"}),"\n",(0,s.jsx)(n.p,{children:"1.0.0 introduces new indices to the multi-modal indexing subsystem of Apache Hudi. These indices are designed to improve\nquery performance through partition pruning and further data skipping."}),"\n",(0,s.jsx)(n.h4,{id:"secondary-index",children:"Secondary Index"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:"secondary index"})," allows users to create indexes on columns that are not part of record key columns in Hudi\ntables. It can be used to speed up queries with predicates on columns other than record key columns."]}),"\n",(0,s.jsx)(n.h4,{id:"partition-stats-index",children:"Partition Stats Index"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:"partition stats index"})," aggregates statistics at the partition level for the columns for which it is enabled. This\nhelps in efficient partition pruning even for non-partition fields."]}),"\n",(0,s.jsx)(n.h4,{id:"expression-index",children:"Expression Index"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:"expression index"})," enables efficient queries on columns derived from expressions. It can collect stats on columns\nderived from expressions without materializing them, and can be used to speed up queries with filters containing such\nexpressions."]}),"\n",(0,s.jsxs)(n.p,{children:["To learn more about these indices, refer to the ",(0,s.jsx)(n.a,{href:"/docs/sql_queries#snapshot-query-with-index-acceleration",children:"SQL queries"})," docs."]}),"\n",(0,s.jsx)(n.h3,{id:"partial-updates",children:"Partial Updates"}),"\n",(0,s.jsx)(n.p,{children:"1.0.0 extends support for partial updates to Merge-on-Read tables, which allows users to update only a subset of columns\nin a record. This feature is useful when users want to update only a few columns in a record without rewriting the\nentire record."}),"\n",(0,s.jsxs)(n.p,{children:["To learn more about partial updates, refer to the ",(0,s.jsx)(n.a,{href:"/docs/sql_dml#merge-into-partial-update",children:"SQL DML"})," docs."]}),"\n",(0,s.jsx)(n.h3,{id:"multiple-base-file-formats-in-a-single-table",children:"Multiple Base File Formats in a single table"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Support for multiple base file formats (e.g., ",(0,s.jsx)(n.strong,{children:"Parquet"}),", ",(0,s.jsx)(n.strong,{children:"ORC"}),", ",(0,s.jsx)(n.strong,{children:"HFile"}),") within a single Hudi table, allowing\ntailored formats for specific use cases like indexing and ML applications."]}),"\n",(0,s.jsx)(n.li,{children:"It is also useful when users want to switch from one file\nformat to another, e.g. from ORC to Parquet, without rewriting the whole table."}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Configuration:"})," Enable with ",(0,s.jsx)(n.code,{children:"hoodie.table.multiple.base.file.formats.enable"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["To learn more about the format changes, refer to the ",(0,s.jsx)(n.a,{href:"/learn/tech-specs-1point0",children:"Hudi 1.0 tech specification"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"api-changes",children:"API Changes"}),"\n",(0,s.jsx)(n.p,{children:"1.0.0 introduces several API changes, including:"}),"\n",(0,s.jsx)(n.h4,{id:"record-merger-api",children:"Record Merger API"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"HoodieRecordPayload"})," interface is deprecated in favor of the new ",(0,s.jsx)(n.code,{children:"HoodieRecordMerger"})," interface. Record merger is a\ngeneric interface that allows users to define custom logic for merging base file and log file records. This release\ncomes with a few out-of-the-box merge modes, which define how the base and log files are ordered in a file slice and\nfurther how different records with the same record key within that file slice are merged consistently to produce the\nsame deterministic results for snapshot queries, writers and table services. Specifically, there are three merge modes\nsupported as a table-level configuration:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"COMMIT_TIME_ORDERING"}),": Merging simply picks the record belonging to the latest write (commit time) as the merged\nresult."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"EVENT_TIME_ORDERING"}),": Merging picks the record with the highest value on a user specified ordering or precombine\nfield as the merged result."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"CUSTOM"}),": Users can provide custom merger implementation to have better control over the merge logic."]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsx)(n.p,{children:"Going forward, we recommend users to migrate and use the record merger APIs and not write new payload implementations."})}),"\n",(0,s.jsx)(n.h4,{id:"positional-merging-with-filegroup-reader",children:"Positional Merging with Filegroup Reader"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Position-Based Merging:"})," Offers an alternative to key-based merging, allowing for page skipping based on record\npositions. Enabled by default for Spark and Hive."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Configuration:"})," Activate positional merging using ",(0,s.jsx)(n.code,{children:"hoodie.merge.use.record.positions=true"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["The new reader has shown impressive performance gains for ",(0,s.jsx)(n.strong,{children:"partial updates"})," with key-based merging. For a MOR table of\nsize 1TB with 100 partitions and 80% random updates in subsequent commits, the new reader is ",(0,s.jsx)(n.strong,{children:"5.7x faster"})," for\nsnapshot queries with ",(0,s.jsx)(n.strong,{children:"70x reduced write amplification"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"flink-enhancements",children:"Flink Enhancements"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Lookup Joins:"})," Flink now supports lookup joins, enabling table enrichment with external data sources."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Partition Stats Index Support:"})," As mentioned above, partition stats support is now available for Flink, bringing\nefficient partition pruning to streaming workloads."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Non-Blocking Concurrency Control:"})," NBCC is now available for Flink streaming writers, allowing for multi-stream\nconcurrent ingestion without conflict."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"call-to-action",children:"Call to Action"}),"\n",(0,s.jsx)(n.p,{children:"The 1.0.0 GA release is the culmination of extensive development, testing, and feedback. We invite you to upgrade and\nexperience the new features and enhancements."}),"\n",(0,s.jsx)(n.h2,{id:"known-regressions",children:"Known Regressions"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"We discovered a regression in Hudi 1.0.0 release for backwards compatible writer for MOR table.\nIt can silently deletes committed data after upgrade when new data is ingested to the table."}),"\n",(0,s.jsxs)(n.li,{children:["We also have a ComplexKeyGenerator related regression reported ",(0,s.jsx)(n.a,{href:"release-0.14#known-regressions",children:"here"}),". Please refrain from migrating, if you have single field as record key and multiple fields as partition fields."]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{type:"tip",children:(0,s.jsx)(n.p,{children:"Avoid upgrading any existing table to 1.0.0 if any of the above scenario matches your workload. Incase of backwards compatible writer for MOR table, you are good to upgrade to 1.0.2 release."})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"release-101",children:(0,s.jsx)(n.a,{href:"https://github.com/apache/hudi/releases/tag/release-1.0.1",children:"Release 1.0.1"})}),"\n",(0,s.jsx)(n.h2,{id:"migration-guide-1",children:"Migration Guide"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"This release (1.0.1) does not introduce any new table version, thus no migration is needed if you are on 1.0.0."}),"\n",(0,s.jsxs)(n.li,{children:["If migrating from an older release, please check the migration guide from the previous release notes, specifically\nthe upgrade instructions in ",(0,s.jsx)(n.a,{href:"/releases/release-0.6.0",children:"0.6.0"}),",\n",(0,s.jsx)(n.a,{href:"/releases/release-0.9.0",children:"0.9.0"}),", ",(0,s.jsx)(n.a,{href:"/releases/release-0.10.0",children:"0.10.0"}),",\n",(0,s.jsx)(n.a,{href:"/releases/release-0.11.0",children:"0.11.0"}),", ",(0,s.jsx)(n.a,{href:"/releases/release-0.12.0",children:"0.12.0"}),", ",(0,s.jsx)(n.a,{href:"/releases/release-0.13.0",children:"0.13.0"}),",\n",(0,s.jsx)(n.a,{href:"/releases/release-0.14#release-0140",children:"0.14.0"})," and ",(0,s.jsx)(n.a,{href:"/releases/release-1.0#release-100",children:"1.0.0"})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"bug-fixes",children:"Bug fixes"}),"\n",(0,s.jsx)(n.p,{children:"1.0.1 release is mainly intended for bug fixes and stability. The fixes span across many components, including"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Hudi Streamer"}),"\n",(0,s.jsx)(n.li,{children:"Spark SQL"}),"\n",(0,s.jsx)(n.li,{children:"Spark datasource writer"}),"\n",(0,s.jsx)(n.li,{children:"Table services"}),"\n",(0,s.jsx)(n.li,{children:"Backwards compatible writer"}),"\n",(0,s.jsx)(n.li,{children:"Flink engine"}),"\n",(0,s.jsx)(n.li,{children:"Unit, functional, integration tests and CI"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"known-regressions-1",children:"Known Regressions"}),"\n",(0,s.jsxs)(n.p,{children:["We have a ComplexKeyGenerator related regression reported ",(0,s.jsx)(n.a,{href:"release-0.14#known-regressions",children:"here"}),". Please refrain from migrating, if you have single field as record key and multiple partition fields."]}),"\n",(0,s.jsx)(n.admonition,{type:"tip",children:(0,s.jsx)(n.p,{children:"Avoid upgrading any existing table to 1.0.1 if you are using ComplexKeyGenerator with single record key configured."})}),"\n",(0,s.jsx)(n.h2,{id:"raw-release-notes",children:"Raw Release Notes"}),"\n",(0,s.jsxs)(n.p,{children:["The raw release notes are available ",(0,s.jsx)(n.a,{href:"https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12322822&version=12355195",children:"here"})]}),"\n",(0,s.jsx)(n.admonition,{type:"tip",children:(0,s.jsxs)(n.p,{children:["1.0.1 release also contains all the new features and bug fixes from 1.0.0, of which the release notes are ",(0,s.jsx)(n.a,{href:"/releases/release-1.0#release-100",children:"here"})]})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"release-102",children:(0,s.jsx)(n.a,{href:"https://github.com/apache/hudi/releases/tag/release-1.0.2",children:"Release 1.0.2"})}),"\n",(0,s.jsx)(n.h2,{id:"migration-guide-2",children:"Migration Guide"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"This release (1.0.2) does not introduce any new table version, thus no migration is needed if you are on 1.0.1."}),"\n",(0,s.jsxs)(n.li,{children:["If migrating from an older release, please check the migration guide from the previous release notes, specifically\nthe upgrade instructions in ",(0,s.jsx)(n.a,{href:"/releases/release-0.6.0",children:"0.6.0"}),",\n",(0,s.jsx)(n.a,{href:"/releases/release-0.9.0",children:"0.9.0"}),", ",(0,s.jsx)(n.a,{href:"/releases/release-0.10.0",children:"0.10.0"}),",\n",(0,s.jsx)(n.a,{href:"/releases/release-0.11.0",children:"0.11.0"}),", ",(0,s.jsx)(n.a,{href:"/releases/release-0.12.0",children:"0.12.0"}),", ",(0,s.jsx)(n.a,{href:"/releases/release-0.13.0",children:"0.13.0"}),",\n",(0,s.jsx)(n.a,{href:"/releases/release-0.14#release-0140",children:"0.14.0"})," and ",(0,s.jsx)(n.a,{href:"/releases/release-1.0#release-100",children:"1.0.0"})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"bug-fixes-1",children:"Bug fixes"}),"\n",(0,s.jsx)(n.p,{children:"The 1.0.2 release primarily focuses on bug fixes, stability enhancements, and critical improvements, particularly around migration and backwards compatibility. The changes span across various components, including:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Metadata Table (MDT):"})," Numerous fixes and improvements related to validation, writing, reading, compaction, indexing (column stats), and backwards compatibility (especially for table version 6)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Spark Integration:"})," Enhancements and fixes for Spark SQL (MERGE INTO, query behavior), datasource reader/writer, schema handling, performance, and backward compatibility."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Backwards Compatibility:"})," Significant effort ensuring compatibility with older table versions (specifically v6) and smoother upgrades from 0.x versions, including dedicated writers/readers."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"File Group Reader:"})," Validation, fixes, and feature completeness improvements, including making it default for table version 6."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Flink Engine:"})," Fixes and improvements related to streamer checkpoints and bundle validation."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Compaction and Table Services:"})," Fixes related to compaction scheduling, execution (especially with global index or RLI), archival, and cleanup."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Indexing:"})," Fixes and enhancements for Column Stats, Record Level Index (RLI), and Bloom Filters."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance:"})," Optimizations in areas like log file writing, schema reuse, and metadata initialization."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Testing, CI, and Dependencies:"})," Fixes for flaky tests, improved code coverage, bundle validation, dependency cleanup (HBase removal), and extensive release testing."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"known-regressions-2",children:"Known Regressions"}),"\n",(0,s.jsxs)(n.p,{children:["We have a ComplexKeyGenerator related regression reported ",(0,s.jsx)(n.a,{href:"release-0.14#known-regressions",children:"here"}),". Please refrain from migrating, if you have single field as record key and mutiple partition fields."]}),"\n",(0,s.jsx)(n.admonition,{type:"tip",children:(0,s.jsx)(n.p,{children:"Avoid upgrading any existing table to 1.0.2 if you are using ComplexKeyGenerator with single record key configured."})}),"\n",(0,s.jsx)(n.h2,{id:"raw-release-notes-1",children:"Raw Release Notes"}),"\n",(0,s.jsxs)(n.p,{children:["The raw release notes are available ",(0,s.jsx)(n.a,{href:"https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12322822&version=12355558",children:"here"})]}),"\n",(0,s.jsx)(n.admonition,{type:"tip",children:(0,s.jsxs)(n.p,{children:["1.0.2 release also contains all the new features and bug fixes from 1.0.1, of which the release notes are ",(0,s.jsx)(n.a,{href:"/releases/release-1.0#release-101",children:"here"})]})})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}}}]);