"use strict";(globalThis.webpackChunkhudi=globalThis.webpackChunkhudi||[]).push([[82595],{10039:(e,i,a)=>{a.d(i,{A:()=>s});const s=a.p+"assets/images/image2-b8e66c22453fcee2810fc23d8cb480d0.png"},28453:(e,i,a)=>{a.d(i,{R:()=>r,x:()=>l});var s=a(96540);const n={},t=s.createContext(n);function r(e){const i=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function l(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:r(e.components),s.createElement(t.Provider,{value:i},e.children)}},44541:(e,i,a)=>{a.d(i,{A:()=>s});const s=a.p+"assets/images/image8-20b16694604281c92df9a8a9079c22b5.png"},45536:(e,i,a)=>{a.d(i,{A:()=>s});const s=a.p+"assets/images/image6-c6d05709a20d20f096539322b86f933d.png"},61815:e=>{e.exports=JSON.parse('{"permalink":"/blog/2025/11/07/how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-11-07-how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse.mdx","source":"@site/blog/2025-11-07-how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse.mdx","title":"How FreeWheel Uses Apache Hudi to Power Its Data Lakehouse","description":"Talk title slide","date":"2025-11-07T00:00:00.000Z","tags":[{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"case-study","permalink":"/blog/tags/case-study"},{"inline":true,"label":"freewheel","permalink":"/blog/tags/freewheel"}],"readingTime":6.54,"hasTruncateMarker":false,"authors":[{"name":"The Hudi Community","key":null,"page":null}],"frontMatter":{"title":"How FreeWheel Uses Apache Hudi to Power Its Data Lakehouse","excerpt":"How FreeWheel unified batch and streaming with an Apache Hudi\u2013powered lakehouse to improve freshness, simplify operations, and scale analytics.","author":"The Hudi Community","category":"blog","image":"/assets/images/blog/2025-11-07-how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse/image1.png","tags":["hudi","lakehouse","case-study","freewheel"]},"unlisted":false,"prevItem":{"title":"Deep Dive Into Hudi\'s Indexing Subsystem (Part 2 of 2)","permalink":"/blog/2025/11/12/deep-dive-into-hudis-indexing-subsystem-part-2-of-2"},"nextItem":{"title":"Deep Dive Into Hudi\u2019s Indexing Subsystem (Part 1 of 2)","permalink":"/blog/2025/10/29/deep-dive-into-hudis-indexing-subsystem-part-1-of-2"}}')},67946:(e,i,a)=>{a.d(i,{A:()=>s});const s=a.p+"assets/images/image7-a67a3aa2b20416cf15c3229553c626ce.png"},68284:(e,i,a)=>{a.d(i,{A:()=>s});const s=a.p+"assets/images/image1-352adf99bd976cab9ea093daab842339.png"},77400:(e,i,a)=>{a.d(i,{A:()=>s});const s=a.p+"assets/images/image5-61c7ba9d9a0b31e28ce22ae67eb12d08.png"},80814:(e,i,a)=>{a.d(i,{A:()=>s});const s=a.p+"assets/images/image3-ecdbeac000b14fa4bf639c7e0daf8654.png"},97042:(e,i,a)=>{a.r(i),a.d(i,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>c});var s=a(61815),n=a(74848),t=a(28453);const r={title:"How FreeWheel Uses Apache Hudi to Power Its Data Lakehouse",excerpt:"How FreeWheel unified batch and streaming with an Apache Hudi\u2013powered lakehouse to improve freshness, simplify operations, and scale analytics.",author:"The Hudi Community",category:"blog",image:"/assets/images/blog/2025-11-07-how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse/image1.png",tags:["hudi","lakehouse","case-study","freewheel"]},l=void 0,o={authorsImageUrls:[void 0]},c=[{value:"Data freshness issues",id:"data-freshness-issues",level:2},{value:"Complex ingestion",id:"complex-ingestion",level:2},{value:"Query performance bottlenecks",id:"query-performance-bottlenecks",level:2},{value:"Use Case 1: Lambda Architecture and Its Drawbacks",id:"use-case-1-lambda-architecture-and-its-drawbacks",level:2},{value:"Use Case 2: Real-Time Inventory Management",id:"use-case-2-real-time-inventory-management",level:2},{value:"Use Case 3: Scalable Audience Data Processing",id:"use-case-3-scalable-audience-data-processing",level:2},{value:"Hudi in practice 1: Billion\u2011scale updates for audience\u2011segment ingestion",id:"hudi-in-practice-1-billionscale-updates-for-audiencesegment-ingestion",level:2},{value:"Use case overview",id:"use-case-overview",level:3},{value:"Key architecture and design principles",id:"key-architecture-and-design-principles",level:3},{value:"<strong>Partitioning and orchestration</strong>",id:"partitioning-and-orchestration",level:4},{value:"<strong>Decoupled ingestion pipeline</strong>",id:"decoupled-ingestion-pipeline",level:4},{value:"Challenges of input data at scale",id:"challenges-of-input-data-at-scale",level:3},{value:"Metrics and performance insights",id:"metrics-and-performance-insights",level:3},{value:"<strong>Operational optimizations</strong>",id:"operational-optimizations",level:3},{value:"Hudi in practice 2: Real\u2011time aggregated ingestion using Spark Streaming + clustering",id:"hudi-in-practice-2-realtime-aggregated-ingestion-using-spark-streaming--clustering",level:2},{value:"Pipeline overview",id:"pipeline-overview",level:3},{value:"Data ingestion flow",id:"data-ingestion-flow",level:3},{value:"Results at a glance",id:"results-at-a-glance",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const i={a:"a",blockquote:"blockquote",em:"em",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(i.p,{children:(0,n.jsx)(i.img,{alt:"Talk title slide",src:a(68284).A+"",width:"1257",height:"437"})}),"\n",(0,n.jsx)(i.p,{children:(0,n.jsxs)(i.em,{children:["This post summarizes a FreeWheel talk from the Apache Hudi Community Sync. Watch the recording on ",(0,n.jsx)(i.a,{href:"https://www.youtube.com/watch?v=hQNSf82o3Rk",children:"YouTube"}),"."]})}),"\n",(0,n.jsxs)(i.p,{children:[(0,n.jsx)(i.a,{href:"https://www.freewheel.com/",children:"FreeWheel"}),", a division of Comcast, provides advanced video advertising solutions across TV and digital platforms. As the business scaled, FreeWheel faced growing challenges maintaining consistency, freshness, and operational efficiency in its data systems. To address these challenges, the team began transitioning from a legacy Lambda architecture to a modern, ",(0,n.jsx)(i.a,{href:"https://hudi.apache.org/",children:"Apache Hudi"}),"-powered lakehouse approach."]}),"\n",(0,n.jsxs)(i.p,{children:["Their original stack, shown below, used multiple systems like ",(0,n.jsx)(i.strong,{children:"Presto"}),", ",(0,n.jsx)(i.strong,{children:"ClickHouse"}),", and ",(0,n.jsx)(i.strong,{children:"Druid"})," to serve analytical and real-time use cases. However, the architecture had some limitations:"]}),"\n",(0,n.jsx)(i.p,{children:(0,n.jsx)(i.img,{alt:"Original multi-engine architecture",src:a(10039).A+"",width:"1999",height:"848"})}),"\n",(0,n.jsx)(i.h2,{id:"data-freshness-issues",children:"Data freshness issues"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"Presto tables had a 3\u20134 hour delay, which was too slow for operational use cases."}),"\n",(0,n.jsx)(i.li,{children:"Only ClickHouse and Druid offered near\u2011real\u2011time access (~5 minutes) but added complexity."}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"complex-ingestion",children:"Complex ingestion"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"Data came from logs, CDC streams, files, and databases."}),"\n",(0,n.jsx)(i.li,{children:"Each system had its own ingestion pipeline and refresh logic."}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"query-performance-bottlenecks",children:"Query performance bottlenecks"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"With ~15 PB of data and 20M+ queries/day, scaling across three engines was costly and hard to operate."}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"use-case-1-lambda-architecture-and-its-drawbacks",children:"Use Case 1: Lambda Architecture and Its Drawbacks"}),"\n",(0,n.jsx)(i.p,{children:(0,n.jsx)(i.img,{alt:"Lambda architecture overview",src:a(80814).A+"",width:"1999",height:"857"})}),"\n",(0,n.jsx)(i.p,{children:"FreeWheel initially followed a traditional Lambda architecture, which separated the processing of batch and real\u2011time data. This approach created several problems: it required duplicate pipelines for batch and real\u2011time processing (leading to inefficient engineering workflows), and it struggled to scale ClickHouse for large aggregates."}),"\n",(0,n.jsxs)(i.p,{children:["By consolidating on Hudi as the table format for both streaming and historical data, FreeWheel unified the storage layer and eliminated duplicate pipelines. Hudi\u2019s ",(0,n.jsx)(i.a,{href:"https://hudi.apache.org/docs/write_operations/#upsert",children:"upserts"})," by key and ",(0,n.jsx)(i.a,{href:"https://hudi.apache.org/docs/table_types/#incremental-queries",children:"incremental processing"})," make it possible to serve near\u2013real\u2011time analytics. The result is simpler operations, consistent logic, and a platform that scales with data volume and query complexity."]}),"\n",(0,n.jsx)(i.h2,{id:"use-case-2-real-time-inventory-management",children:"Use Case 2: Real-Time Inventory Management"}),"\n",(0,n.jsx)(i.p,{children:(0,n.jsx)(i.img,{alt:"Real-time inventory with Hudi",src:a(98529).A+"",width:"1999",height:"1621"})}),"\n",(0,n.jsx)(i.p,{children:"Historically, daily ad inventory updates were a significant challenge. This method led to low forecasting accuracy and frequent delivery-performance mismatches."}),"\n",(0,n.jsx)(i.p,{children:"By modernizing the platform with Hudi, FreeWheel updates inventory within minutes. Order changes are applied as upserts to Hudi tables and become queryable shortly thereafter, dramatically improving forecast accuracy and reducing delivery\u2011vs\u2011forecast mismatches."}),"\n",(0,n.jsx)(i.h2,{id:"use-case-3-scalable-audience-data-processing",children:"Use Case 3: Scalable Audience Data Processing"}),"\n",(0,n.jsx)(i.p,{children:(0,n.jsx)(i.img,{alt:"Audience data architecture with Hudi snapshot",src:a(77400).A+"",width:"941",height:"746"})}),"\n",(0,n.jsx)(i.p,{children:"FreeWheel uses Aerospike to ingest audience segments for its online services, which involves handling high\u2011frequency, real\u2011time data. However, this setup brought a few key challenges\u2014chiefly, the need for analytical insights on top of real\u2011time data and the need to efficiently manage bulk loads alongside frequent updates."}),"\n",(0,n.jsxs)(i.p,{children:["To address these challenges, FreeWheel introduced Hudi into the data pipeline. Hudi maintains a snapshot table for all audience data, enabling more flexible and efficient data management. It supports ",(0,n.jsx)(i.a,{href:"https://hudi.apache.org/docs/write_operations/#bulk_insert",children:"bulk inserts"}),", ",(0,n.jsx)(i.a,{href:"https://hudi.apache.org/docs/write_operations/#upsert",children:"upserts"}),", and change data capture (CDC), enabling smoother handling of updates and large\u2011scale data loads. Using CDC, large batches of audience updates are applied incrementally to the snapshot. With Hudi in place, the back\u2011end analytics system became much stronger, while the responsiveness of the online systems was preserved. This setup also improved the stability of the online targeting system, as heavy analytical workloads were moved off the key\u2011value store, reducing pressure on Aerospike and enhancing overall performance."]}),"\n",(0,n.jsx)(i.h2,{id:"hudi-in-practice-1-billionscale-updates-for-audiencesegment-ingestion",children:"Hudi in practice 1: Billion\u2011scale updates for audience\u2011segment ingestion"}),"\n",(0,n.jsx)(i.h3,{id:"use-case-overview",children:"Use case overview"}),"\n",(0,n.jsx)(i.p,{children:"This implementation showcases how a large\u2011scale platform ingests and updates audience\u2011segmentation data at the billion\u2011record scale using Hudi tables.\nThe architecture efficiently handles high\u2011frequency updates across more than 63,000 partitions and a table over 600 TB, with performance optimizations at both the data and infrastructure levels."}),"\n",(0,n.jsx)(i.h3,{id:"key-architecture-and-design-principles",children:"Key architecture and design principles"}),"\n",(0,n.jsx)(i.p,{children:(0,n.jsx)(i.img,{alt:"Audience ingestion architecture and scheduler",src:a(45536).A+"",width:"1333",height:"1360"})}),"\n",(0,n.jsx)(i.h4,{id:"partitioning-and-orchestration",children:(0,n.jsx)(i.strong,{children:"Partitioning and orchestration"})}),"\n",(0,n.jsxs)(i.p,{children:["FreeWheel uses the audience\u2011segment ID as the ",(0,n.jsx)(i.a,{href:"https://hudi.apache.org/docs/key_generation",children:"partition key"}),". Each partition can be processed independently, allowing many Spark jobs to run in parallel. Each job upserts data to the Hudi lakehouse table."]}),"\n",(0,n.jsx)(i.p,{children:"A central scheduler allocates work based on input size, priority, and write concurrency limits. This enables dynamic scaling across more than 63,000 partitions, where per-partition input sizes range from 1 million to 100 billion records."}),"\n",(0,n.jsx)(i.h4,{id:"decoupled-ingestion-pipeline",children:(0,n.jsx)(i.strong,{children:"Decoupled ingestion pipeline"})}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsxs)(i.li,{children:["Scheduler: allocates resources based on input size and supports job priority, ",(0,n.jsx)(i.a,{href:"https://hudi.apache.org/docs/concurrency_control/",children:"multi-writer concurrency control"}),", and concurrency planning."]}),"\n",(0,n.jsx)(i.li,{children:"Ingestion job: Spark jobs process data and write it to the Hudi segment table in the lakehouse."}),"\n"]}),"\n",(0,n.jsx)(i.h3,{id:"challenges-of-input-data-at-scale",children:"Challenges of input data at scale"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"Table size: over 600 TB."}),"\n",(0,n.jsx)(i.li,{children:"Partition count: 63,000 audience\u2011segment partitions."}),"\n",(0,n.jsx)(i.li,{children:"Data skew: massive variation in partition sizes, ranging from 1 million to 100 billion records."}),"\n"]}),"\n",(0,n.jsx)(i.h3,{id:"metrics-and-performance-insights",children:"Metrics and performance insights"}),"\n",(0,n.jsx)(i.p,{children:(0,n.jsx)(i.img,{alt:"Ingestion metrics and throughput",src:a(67946).A+"",width:"1999",height:"979"})}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsxs)(i.li,{children:["Cost optimization","\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"Unit cost on AWS: ~$0.10 per million records updated."}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(i.li,{children:"Throughput: the pipeline supports up to 12 million upserts per second."}),"\n"]}),"\n",(0,n.jsx)(i.h3,{id:"operational-optimizations",children:(0,n.jsx)(i.strong,{children:"Operational optimizations"})}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"Handle S3 throttling by increasing partition parallelism. Hash partition prefixes and coordinate with AWS to raise per\u2011bucket request caps and remove I/O bottlenecks."}),"\n",(0,n.jsx)(i.li,{children:"Balance SLA and cost with adaptive resource provisioning through the scheduler; choose resources based on input size to keep jobs stable while controlling spend."}),"\n",(0,n.jsx)(i.li,{children:"Deduplicate before commit: group by record key, order by event timestamp, and write only the latest value to reduce churn and speed up writes."}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"hudi-in-practice-2-realtime-aggregated-ingestion-using-spark-streaming--clustering",children:"Hudi in practice 2: Real\u2011time aggregated ingestion using Spark Streaming + clustering"}),"\n",(0,n.jsx)(i.h3,{id:"pipeline-overview",children:"Pipeline overview"}),"\n",(0,n.jsxs)(i.p,{children:["This implementation showcases an efficient pipeline where Spark Streaming ingests aggregated data into a Hudi lakehouse using the ",(0,n.jsx)(i.a,{href:"https://hudi.apache.org/docs/write_operations/#bulk_insert",children:"bulk_insert"})," operation, followed by ",(0,n.jsx)(i.a,{href:"https://hudi.apache.org/blog/2021/08/23/async-clustering/",children:"asynchronous clustering"}),"."]}),"\n",(0,n.jsx)(i.p,{children:(0,n.jsx)(i.img,{alt:"Streaming ingestion and clustering flow",src:a(44541).A+"",width:"1003",height:"445"})}),"\n",(0,n.jsx)(i.h3,{id:"data-ingestion-flow",children:"Data ingestion flow"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"Kafka: raw events are streamed into Kafka."}),"\n",(0,n.jsx)(i.li,{children:"Spark SQL on Streaming: consumes Kafka messages and performs near\u2011real\u2011time aggregations."}),"\n",(0,n.jsxs)(i.li,{children:["bulk_insert into Hudi lakehouse: aggregated data is appended using ",(0,n.jsx)(i.a,{href:"https://hudi.apache.org/docs/write_operations/#bulk_insert",children:"bulk_insert"}),"."]}),"\n",(0,n.jsx)(i.li,{children:"Clustering plan generation: clustering plans are created asynchronously."}),"\n",(0,n.jsxs)(i.li,{children:["HoodieClusteringJob: a cron job runs hourly to execute ",(0,n.jsx)(i.a,{href:"https://hudi.apache.org/docs/clustering/",children:"clustering"})," and consolidate small files."]}),"\n"]}),"\n",(0,n.jsx)(i.h3,{id:"results-at-a-glance",children:"Results at a glance"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"Massive file reduction: clustering reduced total file count by nearly 90%, minimizing small\u2011file pressure and improving metadata performance."}),"\n",(0,n.jsx)(i.li,{children:"Write throughput boost: increased by about 114% due to optimized file layout."}),"\n",(0,n.jsx)(i.li,{children:"Faster queries: Presto query performance improved significantly after clustering."}),"\n"]}),"\n",(0,n.jsx)(i.p,{children:"However, Spark Streaming is a macro\u2011batch system, typically executing every one or two minutes. As a result, it does not trigger clustering jobs immediately but instead generates clustering plans for later execution. In production, clustering jobs are scheduled to run hourly and apply only to stable partitions, ensuring compaction and file optimization without impacting real\u2011time ingestion."}),"\n",(0,n.jsx)(i.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,n.jsx)(i.p,{children:"FreeWheel\u2019s journey with Hudi transformed its data architecture\u2014offering unified access, real\u2011time freshness, and scalable operations. The team credits Hudi\u2019s community and feature set as key to its success."}),"\n",(0,n.jsxs)(i.blockquote,{children:["\n",(0,n.jsx)(i.p,{children:"\u201cWe\u2019re lucky to choose Hudi as our Lakehouse. Thanks to the powerful Hudi community!\u201d \u2013 Bing Jiang"}),"\n"]})]})}function h(e={}){const{wrapper:i}={...(0,t.R)(),...e.components};return i?(0,n.jsx)(i,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},98529:(e,i,a)=>{a.d(i,{A:()=>s});const s=a.p+"assets/images/image4-d88e573a6dba93c4a1edd9e87a09fa5f.png"}}]);