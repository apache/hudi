"use strict";(self.webpackChunkhudi=self.webpackChunkhudi||[]).push([[29060],{47900:(e,a,r)=>{r.r(a),r.d(a,{assets:()=>c,contentTitle:()=>d,default:()=>u,frontMatter:()=>o,metadata:()=>n,toc:()=>h});const n=JSON.parse('{"id":"quick-start-guide","title":"Spark Guide","description":"This guide provides a quick peek at Hudi\'s capabilities using Spark. Using Spark Datasource APIs(both scala and python) and using Spark SQL,","source":"@site/versioned_docs/version-0.14.0/quick-start-guide.md","sourceDirName":".","slug":"/quick-start-guide","permalink":"/docs/0.14.0/quick-start-guide","draft":false,"unlisted":false,"editUrl":"https://github.com/apache/hudi/tree/asf-site/website/versioned_docs/version-0.14.0/quick-start-guide.md","tags":[],"version":"0.14.0","sidebarPosition":2,"frontMatter":{"title":"Spark Guide","sidebar_position":2,"toc":true,"last_modified_at":"2023-08-23T12:14:52.000Z"},"sidebar":"docs","previous":{"title":"Overview","permalink":"/docs/0.14.0/overview"},"next":{"title":"Flink Guide","permalink":"/docs/0.14.0/flink-quick-start-guide"}}');var t=r(74848),s=r(28453),i=r(11470),l=r(19365);const o={title:"Spark Guide",sidebar_position:2,toc:!0,last_modified_at:new Date("2023-08-23T12:14:52.000Z")},d=void 0,c={},h=[{value:"Setup",id:"setup",level:2},{value:"Spark 3 Support Matrix",id:"spark-3-support-matrix",level:3},{value:"Spark Shell/SQL",id:"spark-shellsql",level:3},{value:"Setup project",id:"setup-project",level:3},{value:"Create Table",id:"create-table",level:2},{value:"Insert data",id:"inserts",level:2},{value:"Query data",id:"querying",level:2},{value:"Update data",id:"upserts",level:2},{value:"Merging Data",id:"merge",level:2},{value:"Delete data",id:"deletes",level:2},{value:"Time Travel Query",id:"timetravel",level:2},{value:"Incremental query",id:"incremental-query",level:2},{value:"Change Data Capture Query",id:"cdc-query",level:2},{value:"Table Types",id:"table-types",level:2},{value:"Keys",id:"keys",level:2},{value:"Ordering Field",id:"ordering-field",level:2},{value:"Where to go from here?",id:"where-to-go-from-here",level:2},{value:"Spark SQL Reference",id:"spark-sql-reference",level:3},{value:"Streaming workloads",id:"streaming-workloads",level:3},{value:"Dockerized Demo",id:"dockerized-demo",level:3}];function p(e){const a={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(a.p,{children:"This guide provides a quick peek at Hudi's capabilities using Spark. Using Spark Datasource APIs(both scala and python) and using Spark SQL,\nwe will walk through code snippets that allows you to insert, update, delete and query a Hudi table."}),"\n",(0,t.jsx)(a.h2,{id:"setup",children:"Setup"}),"\n",(0,t.jsxs)(a.p,{children:["Hudi works with Spark-2.4.3+ & Spark 3.x versions. You can follow instructions ",(0,t.jsx)(a.a,{href:"https://spark.apache.org/downloads",children:"here"})," for setting up Spark."]}),"\n",(0,t.jsx)(a.h3,{id:"spark-3-support-matrix",children:"Spark 3 Support Matrix"}),"\n",(0,t.jsxs)(a.table,{children:[(0,t.jsx)(a.thead,{children:(0,t.jsxs)(a.tr,{children:[(0,t.jsx)(a.th,{style:{textAlign:"left"},children:"Hudi"}),(0,t.jsx)(a.th,{style:{textAlign:"left"},children:"Supported Spark 3 version"})]})}),(0,t.jsxs)(a.tbody,{children:[(0,t.jsxs)(a.tr,{children:[(0,t.jsx)(a.td,{style:{textAlign:"left"},children:"0.14.x"}),(0,t.jsx)(a.td,{style:{textAlign:"left"},children:"3.4.x (default build), 3.3.x, 3.2.x, 3.1.x, 3.0.x"})]}),(0,t.jsxs)(a.tr,{children:[(0,t.jsx)(a.td,{style:{textAlign:"left"},children:"0.13.x"}),(0,t.jsx)(a.td,{style:{textAlign:"left"},children:"3.3.x (default build), 3.2.x, 3.1.x"})]}),(0,t.jsxs)(a.tr,{children:[(0,t.jsx)(a.td,{style:{textAlign:"left"},children:"0.12.x"}),(0,t.jsx)(a.td,{style:{textAlign:"left"},children:"3.3.x (default build), 3.2.x, 3.1.x"})]}),(0,t.jsxs)(a.tr,{children:[(0,t.jsx)(a.td,{style:{textAlign:"left"},children:"0.11.x"}),(0,t.jsx)(a.td,{style:{textAlign:"left"},children:"3.2.x (default build, Spark bundle only), 3.1.x"})]}),(0,t.jsxs)(a.tr,{children:[(0,t.jsx)(a.td,{style:{textAlign:"left"},children:"0.10.x"}),(0,t.jsx)(a.td,{style:{textAlign:"left"},children:"3.1.x (default build), 3.0.x"})]}),(0,t.jsxs)(a.tr,{children:[(0,t.jsx)(a.td,{style:{textAlign:"left"},children:"0.7.0 - 0.9.0"}),(0,t.jsx)(a.td,{style:{textAlign:"left"},children:"3.0.x"})]}),(0,t.jsxs)(a.tr,{children:[(0,t.jsx)(a.td,{style:{textAlign:"left"},children:"0.6.0 and prior"}),(0,t.jsx)(a.td,{style:{textAlign:"left"},children:"not supported"})]})]})]}),"\n",(0,t.jsxs)(a.p,{children:["The ",(0,t.jsx)(a.em,{children:"default build"})," Spark version indicates how we build ",(0,t.jsx)(a.code,{children:"hudi-spark3-bundle"}),"."]}),"\n",(0,t.jsx)(a.admonition,{title:"Change summary",type:"note",children:(0,t.jsxs)(a.p,{children:["In 0.14.0, we introduced the support for Spark 3.4.x and bring back the support for Spark 3.0.x.\nIn 0.12.0, we introduced the experimental support for Spark 3.3.0.\nIn 0.11.0, there are changes on using Spark bundles, please refer to ",(0,t.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-0.11.0/#spark-versions-and-bundles",children:"0.11.0 release notes"})," for detailed instructions."]})}),"\n",(0,t.jsx)(a.h3,{id:"spark-shellsql",children:"Spark Shell/SQL"}),"\n",(0,t.jsxs)(i.A,{groupId:"programming-language",defaultValue:"scala",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"},{label:"Spark SQL",value:"sparksql"}],children:[(0,t.jsxs)(l.A,{value:"scala",children:[(0,t.jsx)(a.p,{children:"From the extracted directory run spark-shell with Hudi:"}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-shell",children:"# For Spark versions: 3.2 - 3.4\nexport SPARK_VERSION=3.4\nspark-shell --packages org.apache.hudi:hudi-spark$SPARK_VERSION-bundle_2.12:0.14.0 --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog' --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' --conf 'spark.kryo.registrator=org.apache.spark.HoodieSparkKryoRegistrar'\n"})}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-shell",children:"# For Spark versions: 3.0 - 3.1\nexport SPARK_VERSION=3.1\nspark-shell --packages org.apache.hudi:hudi-spark$SPARK_VERSION-bundle_2.12:0.14.0 --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' --conf 'spark.kryo.registrator=org.apache.spark.HoodieSparkKryoRegistrar'\n"})}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-shell",children:"# For Spark version: 2.4\nspark-shell --packages org.apache.hudi:hudi-spark2.4-bundle_2.11:0.14.0 --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' --conf 'spark.kryo.registrator=org.apache.spark.HoodieSparkKryoRegistrar'\n"})})]}),(0,t.jsxs)(l.A,{value:"python",children:[(0,t.jsx)(a.p,{children:"From the extracted directory run pyspark with Hudi:"}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-shell",children:"# For Spark versions: 3.2 - 3.4\nexport PYSPARK_PYTHON=$(which python3)\nexport SPARK_VERSION=3.4\npyspark --packages org.apache.hudi:hudi-spark$SPARK_VERSION-bundle_2.12:0.14.0 --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog' --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' --conf 'spark.kryo.registrator=org.apache.spark.HoodieSparkKryoRegistrar'\n"})}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-shell",children:"# For Spark versions: 3.0 - 3.1\nexport PYSPARK_PYTHON=$(which python3)\nexport SPARK_VERSION=3.1\npyspark --packages org.apache.hudi:hudi-spark$SPARK_VERSION-bundle_2.12:0.14.0 --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' --conf 'spark.kryo.registrator=org.apache.spark.HoodieSparkKryoRegistrar'\n"})}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-shell",children:"# For Spark version: 2.4\nexport PYSPARK_PYTHON=$(which python3)\npyspark --packages org.apache.hudi:hudi-spark2.4-bundle_2.11:0.14.0 --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' --conf 'spark.kryo.registrator=org.apache.spark.HoodieSparkKryoRegistrar'\n"})})]}),(0,t.jsxs)(l.A,{value:"sparksql",children:[(0,t.jsxs)(a.p,{children:["Hudi support using Spark SQL to write and read data with the ",(0,t.jsx)(a.strong,{children:"HoodieSparkSessionExtension"})," sql extension.\nFrom the extracted directory run Spark SQL with Hudi:"]}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-shell",children:"# For Spark versions: 3.2 - 3.4\nexport SPARK_VERSION=3.4\nspark-sql --packages org.apache.hudi:hudi-spark$SPARK_VERSION-bundle_2.12:0.14.0 --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog' --conf 'spark.kryo.registrator=org.apache.spark.HoodieSparkKryoRegistrar'\n"})}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-shell",children:"# For Spark versions: 3.0 - 3.1\nexport SPARK_VERSION=3.1\nspark-sql --packages org.apache.hudi:hudi-spark$SPARK_VERSION-bundle_2.12:0.14.0 --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' --conf 'spark.kryo.registrator=org.apache.spark.HoodieSparkKryoRegistrar'\n"})}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-shell",children:"# For Spark version: 2.4\nspark-sql --packages org.apache.hudi:hudi-spark2.4-bundle_2.11:0.14.0 --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension' --conf 'spark.kryo.registrator=org.apache.spark.HoodieSparkKryoRegistrar'\n"})})]})]}),"\n",(0,t.jsx)(a.admonition,{title:"for Spark 3.2 and higher versions",type:"note",children:(0,t.jsx)(a.p,{children:"Use scala 2.12 builds with an additional config: --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog'"})}),"\n",(0,t.jsx)(a.h3,{id:"setup-project",children:"Setup project"}),"\n",(0,t.jsx)(a.p,{children:"Below, we do imports and setup the table name and corresponding base path."}),"\n",(0,t.jsxs)(i.A,{groupId:"programming-language",defaultValue:"scala",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"},{label:"Spark SQL",value:"sparksql"}],children:[(0,t.jsx)(l.A,{value:"scala",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-scala",children:'// spark-shell\nimport scala.collection.JavaConversions._\nimport org.apache.spark.sql.SaveMode._\nimport org.apache.hudi.DataSourceReadOptions._\nimport org.apache.hudi.DataSourceWriteOptions._\nimport org.apache.hudi.common.table.HoodieTableConfig._\nimport org.apache.hudi.config.HoodieWriteConfig._\nimport org.apache.hudi.keygen.constant.KeyGeneratorOptions._\nimport org.apache.hudi.common.model.HoodieRecord\nimport spark.implicits._\n\nval tableName = "trips_table"\nval basePath = "file:///tmp/trips_table"\n'})})}),(0,t.jsx)(l.A,{value:"python",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'# pyspark\nfrom pyspark.sql.functions import lit, col\n\ntableName = "trips_table"\nbasePath = "file:///tmp/trips_table"\n'})})}),(0,t.jsx)(l.A,{value:"sparksql",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-sql",children:"// Next section will go over create table commands\n"})})})]}),"\n",(0,t.jsx)(a.h2,{id:"create-table",children:"Create Table"}),"\n",(0,t.jsx)(a.p,{children:"First, let's create a Hudi table. Here, we use a partitioned table for illustration, but Hudi also supports non-partitioned tables."}),"\n",(0,t.jsxs)(i.A,{groupId:"programming-language",defaultValue:"scala",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"},{label:"Spark SQL",value:"sparksql"}],children:[(0,t.jsx)(l.A,{value:"scala",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-scala",children:"// scala\n// First commit will auto-initialize the table, if it did not exist in the specified base path. \n"})})}),(0,t.jsx)(l.A,{value:"python",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:"# pyspark\n# First commit will auto-initialize the table, if it did not exist in the specified base path. \n"})})}),(0,t.jsxs)(l.A,{value:"sparksql",children:[(0,t.jsx)(a.admonition,{title:"NOTE:",type:"note",children:(0,t.jsx)(a.p,{children:"For users who have Spark-Hive integration in their environment, this guide assumes that you have the appropriate\nsettings configured to allow Spark to create tables and register in Hive Metastore."})}),(0,t.jsx)(a.p,{children:"Here is an example of creating a Hudi table."}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-sql",children:"-- create a Hudi table that is partitioned.\nCREATE TABLE hudi_table (\n    ts BIGINT,\n    uuid STRING,\n    rider STRING,\n    driver STRING,\n    fare DOUBLE,\n    city STRING\n) USING HUDI\nPARTITIONED BY (city);\n"})}),(0,t.jsxs)(a.p,{children:["For more options for creating Hudi tables or if you're running into any issues, please refer to ",(0,t.jsx)(a.a,{href:"/docs/sql_ddl",children:"SQL DDL"})," reference guide."]})]})]}),"\n",(0,t.jsx)(a.h2,{id:"inserts",children:"Insert data"}),"\n",(0,t.jsxs)(i.A,{groupId:"programming-language",defaultValue:"scala",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"},{label:"Spark SQL",value:"sparksql"}],children:[(0,t.jsxs)(l.A,{value:"scala",children:[(0,t.jsx)(a.p,{children:"Generate some new records as a DataFrame and write the DataFrame into a Hudi table.\nSince, this is the first write, it will also auto-create the table."}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-scala",children:'// spark-shell\nval columns = Seq("ts","uuid","rider","driver","fare","city")\nval data =\n  Seq((1695159649087L,"334e26e9-8355-45cc-97c6-c31daf0df330","rider-A","driver-K",19.10,"san_francisco"),\n    (1695091554788L,"e96c4396-3fad-413a-a942-4cb36106d721","rider-C","driver-M",27.70 ,"san_francisco"),\n    (1695046462179L,"9909a8b1-2d15-4d3d-8ec9-efc48c536a00","rider-D","driver-L",33.90 ,"san_francisco"),\n    (1695516137016L,"e3cf430c-889d-4015-bc98-59bdce1e530c","rider-F","driver-P",34.15,"sao_paulo"    ),\n    (1695115999911L,"c8abbe79-8d89-47ea-b4ce-4d224bae5bfa","rider-J","driver-T",17.85,"chennai"));\n\nvar inserts = spark.createDataFrame(data).toDF(columns:_*)\ninserts.write.format("hudi").\n  option(PARTITIONPATH_FIELD_NAME.key(), "city").\n  option(TABLE_NAME, tableName).\n  mode(Overwrite).\n  save(basePath)\n'})}),(0,t.jsx)(a.admonition,{title:"Mapping to Hudi write operations",type:"info",children:(0,t.jsxs)(a.p,{children:["Hudi provides a wide range of ",(0,t.jsx)(a.a,{href:"/docs/write_operations",children:"write operations"})," - both batch and incremental - to write data into Hudi tables,\nwith different semantics and performance. When record keys are not configured (see ",(0,t.jsx)(a.a,{href:"#keys",children:"keys"})," below), ",(0,t.jsx)(a.code,{children:"bulk_insert"})," will be chosen as\nthe write operation, matching the out-of-behavior of Spark's Parquet Datasource."]})})]}),(0,t.jsxs)(l.A,{value:"python",children:[(0,t.jsx)(a.p,{children:"Generate some new records as a DataFrame and write the DataFrame into a Hudi table.\nSince, this is the first write, it will also auto-create the table."}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'# pyspark\ncolumns = ["ts","uuid","rider","driver","fare","city"]\ndata =[(1695159649087,"334e26e9-8355-45cc-97c6-c31daf0df330","rider-A","driver-K",19.10,"san_francisco"),\n       (1695091554788,"e96c4396-3fad-413a-a942-4cb36106d721","rider-C","driver-M",27.70 ,"san_francisco"),\n       (1695046462179,"9909a8b1-2d15-4d3d-8ec9-efc48c536a00","rider-D","driver-L",33.90 ,"san_francisco"),\n       (1695516137016,"e3cf430c-889d-4015-bc98-59bdce1e530c","rider-F","driver-P",34.15,"sao_paulo"),\n       (1695115999911,"c8abbe79-8d89-47ea-b4ce-4d224bae5bfa","rider-J","driver-T",17.85,"chennai")]\ninserts = spark.createDataFrame(data).toDF(*columns)\n\nhudi_options = {\n    \'hoodie.table.name\': tableName,\n    \'hoodie.datasource.write.partitionpath.field\': \'city\'\n}\n\ninserts.write.format("hudi"). \\\n    options(**hudi_options). \\\n    mode("overwrite"). \\\n    save(basePath)\n'})}),(0,t.jsx)(a.admonition,{title:"Mapping to Hudi write operations",type:"info",children:(0,t.jsxs)(a.p,{children:["Hudi provides a wide range of ",(0,t.jsx)(a.a,{href:"/docs/write_operations",children:"write operations"})," - both batch and incremental - to write data into Hudi tables,\nwith different semantics and performance. When record keys are not configured (see ",(0,t.jsx)(a.a,{href:"#keys",children:"keys"})," below), ",(0,t.jsx)(a.code,{children:"bulk_insert"})," will be chosen as\nthe write operation, matching the out-of-behavior of Spark's Parquet Datasource."]})})]}),(0,t.jsxs)(l.A,{value:"sparksql",children:[(0,t.jsxs)(a.p,{children:["Users can use 'INSERT INTO' to insert data into a Hudi table. See ",(0,t.jsx)(a.a,{href:"/docs/sql_dml#insert-into",children:"Insert Into"})," for more advanced options."]}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-sql",children:"INSERT INTO hudi_table\nVALUES\n(1695159649087,'334e26e9-8355-45cc-97c6-c31daf0df330','rider-A','driver-K',19.10,'san_francisco'),\n(1695091554788,'e96c4396-3fad-413a-a942-4cb36106d721','rider-C','driver-M',27.70 ,'san_francisco'),\n(1695046462179,'9909a8b1-2d15-4d3d-8ec9-efc48c536a00','rider-D','driver-L',33.90 ,'san_francisco'),\n(1695332066204,'1dced545-862b-4ceb-8b43-d2a568f6616b','rider-E','driver-O',93.50,'san_francisco'),\n(1695516137016,'e3cf430c-889d-4015-bc98-59bdce1e530c','rider-F','driver-P',34.15,'sao_paulo'    ),\n(1695376420876,'7a84095f-737f-40bc-b62f-6b69664712d2','rider-G','driver-Q',43.40 ,'sao_paulo'    ),\n(1695173887231,'3eeb61f7-c2b0-4636-99bd-5d7a5a1d2c04','rider-I','driver-S',41.06 ,'chennai'      ),\n(1695115999911,'c8abbe79-8d89-47ea-b4ce-4d224bae5bfa','rider-J','driver-T',17.85,'chennai');\n"})}),(0,t.jsx)(a.p,{children:"If you want to control the Hudi write operation used for the INSERT statement, you can set the following config before issuing\nthe INSERT statement:"}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-sql",children:"-- bulk_insert using INSERT_INTO \nSET hoodie.spark.sql.insert.into.operation = 'bulk_insert' \n"})})]})]}),"\n",(0,t.jsx)(a.h2,{id:"querying",children:"Query data"}),"\n",(0,t.jsx)(a.p,{children:"Hudi tables can be queried back into a DataFrame or Spark SQL."}),"\n",(0,t.jsxs)(i.A,{groupId:"programming-language",defaultValue:"scala",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"},{label:"Spark SQL",value:"sparksql"}],children:[(0,t.jsx)(l.A,{value:"scala",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-scala",children:'// spark-shell\nval tripsDF = spark.read.format("hudi").load(basePath)\ntripsDF.createOrReplaceTempView("trips_table")\n\nspark.sql("SELECT uuid, fare, ts, rider, driver, city FROM  trips_table WHERE fare > 20.0").show()\nspark.sql("SELECT _hoodie_commit_time, _hoodie_record_key, _hoodie_partition_path, rider, driver, fare FROM  trips_table").show()\n'})})}),(0,t.jsx)(l.A,{value:"python",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'# pyspark\ntripsDF = spark.read.format("hudi").load(basePath)\ntripsDF.createOrReplaceTempView("trips_table")\n\nspark.sql("SELECT uuid, fare, ts, rider, driver, city FROM  trips_table WHERE fare > 20.0").show()\nspark.sql("SELECT _hoodie_commit_time, _hoodie_record_key, _hoodie_partition_path, rider, driver, fare FROM trips_table").show()\n'})})}),(0,t.jsx)(l.A,{value:"sparksql",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-sql",children:" SELECT ts, fare, rider, driver, city FROM  hudi_table WHERE fare > 20.0;\n"})})})]}),"\n",(0,t.jsx)(a.h2,{id:"upserts",children:"Update data"}),"\n",(0,t.jsx)(a.p,{children:"Hudi tables can be updated by streaming in a DataFrame or using a standard UPDATE statement."}),"\n",(0,t.jsxs)(i.A,{groupId:"programming-language",defaultValue:"scala",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"},{label:"Spark SQL",value:"sparksql"}],children:[(0,t.jsxs)(l.A,{value:"scala",children:[(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-scala",children:'// Lets read data from target Hudi table, modify fare column for rider-D and update it. \nval updatesDf = spark.read.format("hudi").load(basePath).filter($"rider" === "rider-D").withColumn("fare", col("fare") * 10)\n\nupdatesDf.write.format("hudi").\n  option(OPERATION_OPT_KEY, "upsert").\n  option(PARTITIONPATH_FIELD_NAME.key(), "city").\n  option(TABLE_NAME, tableName).\n  mode(Append).\n  save(basePath)\n'})}),(0,t.jsx)(a.admonition,{title:"Key requirements",type:"info",children:(0,t.jsxs)(a.p,{children:["Updates with spark-datasource is feasible only when the source dataframe contains Hudi's meta fields or a ",(0,t.jsx)(a.a,{href:"#keys",children:"key field"})," is configured.\nNotice that the save mode is now ",(0,t.jsx)(a.code,{children:"Append"}),". In general, always use append mode unless you are trying to create the table for the first time."]})})]}),(0,t.jsxs)(l.A,{value:"sparksql",children:[(0,t.jsxs)(a.p,{children:["Hudi table can be update using a regular UPDATE statement. See ",(0,t.jsx)(a.a,{href:"/docs/sql_dml#update",children:"Update"})," for more advanced options."]}),(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-sql",children:"UPDATE hudi_table SET fare = 25.0 WHERE rider = 'rider-D';\n"})})]}),(0,t.jsxs)(l.A,{value:"python",children:[(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'# pyspark\n# Lets read data from target Hudi table, modify fare column for rider-D and update it.\nupdatesDf = spark.read.format("hudi").load(basePath).filter("rider == \'rider-D\'").withColumn("fare",col("fare")*10)\n\nupdatesDf.write.format("hudi"). \\\n  options(**hudi_options). \\\n  mode("append"). \\\n  save(basePath)\n'})}),(0,t.jsx)(a.admonition,{title:"Key requirements",type:"info",children:(0,t.jsxs)(a.p,{children:["Updates with spark-datasource is feasible only when the source dataframe contains Hudi's meta fields or a ",(0,t.jsx)(a.a,{href:"#keys",children:"key field"})," is configured.\nNotice that the save mode is now ",(0,t.jsx)(a.code,{children:"Append"}),". In general, always use append mode unless you are trying to create the table for the first time."]})})]})]}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.a,{href:"#querying",children:"Querying"})," the data again will now show updated records. Each write operation generates a new ",(0,t.jsx)(a.a,{href:"/docs/concepts",children:"commit"}),".\nLook for changes in ",(0,t.jsx)(a.code,{children:"_hoodie_commit_time"}),", ",(0,t.jsx)(a.code,{children:"fare"})," fields for the given ",(0,t.jsx)(a.code,{children:"_hoodie_record_key"})," value from a previous commit."]}),"\n",(0,t.jsx)(a.h2,{id:"merge",children:"Merging Data"}),"\n",(0,t.jsxs)(i.A,{groupId:"programming-language",defaultValue:"scala",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"},{label:"Spark SQL",value:"sparksql"}],children:[(0,t.jsxs)(l.A,{value:"scala",children:[(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-scala",children:'// spark-shell\nval adjustedFareDF = spark.read.format("hudi").\n  load(basePath).limit(2).\n  withColumn("fare", col("fare") * 10)\nadjustedFareDF.write.format("hudi").\noption("hoodie.datasource.write.payload.class","com.payloads.CustomMergeIntoConnector").\nmode(Append).\nsave(basePath)\n// Notice Fare column has been updated but all other columns remain intact.\nspark.read.format("hudi").load(basePath).show()\n'})}),(0,t.jsxs)(a.p,{children:["The ",(0,t.jsx)(a.code,{children:"com.payloads.CustomMergeIntoConnector"})," adds adjusted fare values to the original table and preserves all other fields.\nRefer ",(0,t.jsx)(a.a,{href:"https://gist.github.com/bhasudha/7ea07f2bb9abc5c6eb86dbd914eec4c6",children:"here"})," for sample implementation of ",(0,t.jsx)(a.code,{children:"com.payloads.CustomMergeIntoConnector"}),"."]})]}),(0,t.jsxs)(l.A,{value:"python",children:[(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'# pyspark\nadjustedFareDF = spark.read.format("hudi").load(basePath). \\\n    limit(2).withColumn("fare", col("fare") * 100)\nadjustedFareDF.write.format("hudi"). \\\noption("hoodie.datasource.write.payload.class","com.payloads.CustomMergeIntoConnector"). \\\nmode("append"). \\\nsave(basePath)\n# Notice Fare column has been updated but all other columns remain intact.\nspark.read.format("hudi").load(basePath).show()\n'})}),(0,t.jsxs)(a.p,{children:["The ",(0,t.jsx)(a.code,{children:"com.payloads.CustomMergeIntoConnector"})," adds adjusted fare values to the original table and preserves all other fields.\nRefer ",(0,t.jsx)(a.a,{href:"https://gist.github.com/bhasudha/7ea07f2bb9abc5c6eb86dbd914eec4c6",children:"here"})," for sample implementation of ",(0,t.jsx)(a.code,{children:"com.payloads.CustomMergeIntoConnector"}),"."]})]}),(0,t.jsxs)(l.A,{value:"sparksql",children:[(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-sql",children:"-- source table using Hudi for testing merging into target Hudi table\nCREATE TABLE fare_adjustment (ts BIGINT, uuid STRING, rider STRING, driver STRING, fare DOUBLE, city STRING) \nUSING HUDI;\nINSERT INTO fare_adjustment VALUES \n(1695091554788,'e96c4396-3fad-413a-a942-4cb36106d721','rider-C','driver-M',-2.70 ,'san_francisco'),\n(1695530237068,'3f3d9565-7261-40e6-9b39-b8aa784f95e2','rider-K','driver-U',64.20 ,'san_francisco'),\n(1695241330902,'ea4c36ff-2069-4148-9927-ef8c1a5abd24','rider-H','driver-R',66.60 ,'sao_paulo'    ),\n(1695115999911,'c8abbe79-8d89-47ea-b4ce-4d224bae5bfa','rider-J','driver-T',1.85,'chennai'      );\n\n\nMERGE INTO hudi_table AS target\nUSING fare_adjustment AS source\nON target.uuid = source.uuid\nWHEN MATCHED THEN UPDATE SET target.fare = target.fare + source.fare\nWHEN NOT MATCHED THEN INSERT *\n;\n\n"})}),(0,t.jsx)(a.admonition,{title:"Key requirements",type:"info",children:(0,t.jsxs)(a.ol,{children:["\n",(0,t.jsxs)(a.li,{children:["For a Hudi table with user defined primary record ",(0,t.jsx)(a.a,{href:"#keys",children:"keys"}),", the join condition is expected to contain the primary keys of the table.\nFor a Hudi table with Hudi generated primary keys, the join condition can be on any arbitrary data columns."]}),"\n",(0,t.jsxs)(a.li,{children:["For Merge-On-Read tables, partial column updates are not yet supported, i.e. ",(0,t.jsx)(a.strong,{children:"all columns"})," need to be SET from a\nMERGE statement either using ",(0,t.jsx)(a.code,{children:"SET *"})," or using ",(0,t.jsx)(a.code,{children:"SET column1 = expression1 [, column2 = expression2 ...]"}),"."]}),"\n"]})})]})]}),"\n",(0,t.jsx)(a.h2,{id:"deletes",children:"Delete data"}),"\n",(0,t.jsxs)(a.p,{children:["Delete operation removes the records specified from the table. For example, this code snippet deletes records\nfor the HoodieKeys passed in. Check out the ",(0,t.jsx)(a.a,{href:"/docs/writing_data#deletes",children:"deletion section"})," for more details."]}),"\n",(0,t.jsxs)(i.A,{groupId:"programming-language",defaultValue:"scala",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"},{label:"Spark SQL",value:"sparksql"}],children:[(0,t.jsxs)(l.A,{value:"scala",children:[(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-scala",children:'// spark-shell\n// Lets  delete rider: rider-D\nval deletesDF = spark.read.format("hudi").load(basePath).filter($"rider" === "rider-F")\n\ndeletesDF.write.format("hudi").\n  option(OPERATION_OPT_KEY, "delete").\n  option(PARTITIONPATH_FIELD_NAME.key(), "city").\n  option(TABLE_NAME, tableName).\n  mode(Append).\n  save(basePath)\n\n'})}),(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.a,{href:"#querying",children:"Querying"})," the data again will not show the deleted record."]}),(0,t.jsx)(a.admonition,{title:"Key requirements",type:"info",children:(0,t.jsxs)(a.p,{children:["Deletes with spark-datasource is supported only when the source dataframe contains Hudi's meta fields or a ",(0,t.jsx)(a.a,{href:"#keys",children:"key field"})," is configured.\nNotice that the save mode is again ",(0,t.jsx)(a.code,{children:"Append"}),"."]})})]}),(0,t.jsx)(l.A,{value:"sparksql",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-sql",children:"DELETE FROM hudi_table WHERE uuid = '3f3d9565-7261-40e6-9b39-b8aa784f95e2';\n"})})}),(0,t.jsxs)(l.A,{value:"python",children:[(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:"# pyspark\n# Lets  delete rider: rider-D\ndeletesDF = spark.read.format(\"hudi\").load(basePath).filter(\"rider == 'rider-F'\")\n\n# issue deletes\nhudi_hard_delete_options = {\n  'hoodie.table.name': tableName,\n  'hoodie.datasource.write.partitionpath.field': 'city',\n  'hoodie.datasource.write.operation': 'delete',\n}\n\ndeletesDF.write.format(\"hudi\"). \\\noptions(**hudi_hard_delete_options). \\\nmode(\"append\"). \\\nsave(basePath)\n"})}),(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.a,{href:"#querying",children:"Querying"})," the data again will not show the deleted record."]}),(0,t.jsx)(a.admonition,{title:"Key requirements",type:"info",children:(0,t.jsxs)(a.p,{children:["Deletes with spark-datasource is supported only when the source dataframe contains Hudi's meta fields or a ",(0,t.jsx)(a.a,{href:"#keys",children:"key field"})," is configured.\nNotice that the save mode is again ",(0,t.jsx)(a.code,{children:"Append"}),"."]})})]})]}),"\n",(0,t.jsx)(a.h2,{id:"timetravel",children:"Time Travel Query"}),"\n",(0,t.jsx)(a.p,{children:"Hudi supports time travel query to query the table as of a point-in-time in history. Three timestamp formats are supported as illustrated below."}),"\n",(0,t.jsxs)(i.A,{groupId:"programming-language",defaultValue:"scala",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"},{label:"Spark SQL",value:"sparksql"}],children:[(0,t.jsx)(l.A,{value:"scala",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-scala",children:'spark.read.format("hudi").\n  option("as.of.instant", "20210728141108100").\n  load(basePath)\n\nspark.read.format("hudi").\n  option("as.of.instant", "2021-07-28 14:11:08.200").\n  load(basePath)\n\n// It is equal to "as.of.instant = 2021-07-28 00:00:00"\nspark.read.format("hudi").\n  option("as.of.instant", "2021-07-28").\n  load(basePath)\n\n'})})}),(0,t.jsx)(l.A,{value:"python",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'# pyspark\nspark.read.format("hudi"). \\\n  option("as.of.instant", "20210728141108100"). \\\n  load(basePath)\n\nspark.read.format("hudi"). \\\n  option("as.of.instant", "2021-07-28 14:11:08.000"). \\\n  load(basePath)\n\n# It is equal to "as.of.instant = 2021-07-28 00:00:00"\nspark.read.format("hudi"). \\\n  option("as.of.instant", "2021-07-28"). \\\n  load(basePath)\n'})})}),(0,t.jsxs)(l.A,{value:"sparksql",children:[(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-sql",children:"\n-- time travel based on commit time, for eg: `20220307091628793`\nSELECT * FROM hudi_table TIMESTAMP AS OF '20220307091628793' WHERE id = 1;\n-- time travel based on different timestamp formats\nSELECT * FROM hudi_table TIMESTAMP AS OF '2022-03-07 09:16:28.100' WHERE id = 1;\nSELECT * FROM hudi_table TIMESTAMP AS OF '2022-03-08' WHERE id = 1;\n"})}),(0,t.jsx)(a.admonition,{type:"note",children:(0,t.jsx)(a.p,{children:"Requires Spark 3.2+"})})]})]}),"\n",(0,t.jsx)(a.h2,{id:"incremental-query",children:"Incremental query"}),"\n",(0,t.jsxs)(a.p,{children:['Hudi provides the unique capability to obtain a set of records that changed between a start and end commit time, providing you with the\n"latest state" for each such record as of the end commit time. By default, Hudi tables are configured to support incremental queries, using\nrecord level ',(0,t.jsx)(a.a,{href:"https://hudi.apache.org/blog/2023/05/19/hudi-metafields-demystified",children:"metadata tracking"}),"."]}),"\n",(0,t.jsxs)(a.p,{children:["Below, we fetch changes since a given begin time while the end time defaults to the latest commit on the table. Users can also specify an\nend time using ",(0,t.jsx)(a.code,{children:"END_INSTANTTIME.key()"})," option."]}),"\n",(0,t.jsxs)(i.A,{groupId:"programming-language",defaultValue:"scala",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"},{label:"Spark SQL",value:"sparksql"}],children:[(0,t.jsx)(l.A,{value:"scala",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-scala",children:'// spark-shell\nspark.read.format("hudi").load(basePath).createOrReplaceTempView("trips_table")\n\nval commits = spark.sql("SELECT DISTINCT(_hoodie_commit_time) AS commitTime FROM  trips_table ORDER BY commitTime").map(k => k.getString(0)).take(50)\nval beginTime = commits(commits.length - 2) // commit time we are interested in\n\n// incrementally query data\nval tripsIncrementalDF = spark.read.format("hudi").\n  option(QUERY_TYPE.key(), QUERY_TYPE_INCREMENTAL_OPT_VAL).\n  option(BEGIN_INSTANTTIME.key(), 0).\n  load(basePath)\ntripsIncrementalDF.createOrReplaceTempView("trips_incremental")\n\nspark.sql("SELECT `_hoodie_commit_time`, fare, rider, driver, uuid, ts FROM  trips_incremental WHERE fare > 20.0").show()\n'})})}),(0,t.jsx)(l.A,{value:"python",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'# pyspark\n# reload data\nspark.read.format("hudi").load(basePath).createOrReplaceTempView("trips_table")\n\ncommits = list(map(lambda row: row[0], spark.sql("SELECT DISTINCT(_hoodie_commit_time) AS commitTime FROM  trips_table ORDER BY commitTime").limit(50).collect()))\nbeginTime = commits[len(commits) - 2] # commit time we are interested in\n\n# incrementally query data\nincremental_read_options = {\n  \'hoodie.datasource.query.type\': \'incremental\',\n  \'hoodie.datasource.read.begin.instanttime\': beginTime,\n}\n\ntripsIncrementalDF = spark.read.format("hudi"). \\\n  options(**incremental_read_options). \\\n  load(basePath)\ntripsIncrementalDF.createOrReplaceTempView("trips_incremental")\n\nspark.sql("SELECT `_hoodie_commit_time`, fare, rider, driver, uuid, ts FROM trips_incremental WHERE fare > 20.0").show()\n'})})}),(0,t.jsx)(l.A,{value:"sparksql",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-sql",children:"-- syntax\nhudi_table_changes(table or path, queryType, beginTime [, endTime]);  \n-- table or path: table identifier, example: db.tableName, tableName, \n--                or path for of your table, example: path/to/hudiTable  \n--                in this case table does not need to exist in the metastore,\n-- queryType: incremental query mode, example: latest_state, cdc  \n--            (for cdc query, first enable cdc for your table by setting cdc.enabled=true),\n-- beginTime: instantTime to begin query from, example: earliest, 202305150000, \n-- endTime: optional instantTime to end query at, example: 202305160000, \n\n-- incrementally query data by table name\n-- start from earliest available commit, end at latest available commit.  \nSELECT * FROM hudi_table_changes('db.table', 'latest_state', 'earliest');\n\n-- start from earliest, end at 202305160000.  \nSELECT * FROM hudi_table_changes('table', 'latest_state', 'earliest', '202305160000');  \n\n-- start from 202305150000, end at 202305160000.\nSELECT * FROM hudi_table_changes('table', 'latest_state', '202305150000', '202305160000');\n"})})})]}),"\n",(0,t.jsx)(a.h2,{id:"cdc-query",children:"Change Data Capture Query"}),"\n",(0,t.jsx)(a.p,{children:"Hudi also exposes first-class support for Change Data Capture (CDC) queries. CDC queries are useful for applications that need to\nobtain all the changes, along with before/after images of records, given a commit time range."}),"\n",(0,t.jsxs)(i.A,{groupId:"programming-language",defaultValue:"scala",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"},{label:"Spark SQL",value:"sparksql"}],children:[(0,t.jsx)(l.A,{value:"scala",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-scala",children:'// spark-shell\n// Lets first insert data to a new table with cdc enabled.\nval columns = Seq("ts","uuid","rider","driver","fare","city")\nval data =\n  Seq((1695158649187L,"334e26e9-8355-45cc-97c6-c31daf0df330","rider-A","driver-K",19.10,"san_francisco"),\n    (1695091544288L,"e96c4396-3fad-413a-a942-4cb36106d721","rider-B","driver-L",27.70 ,"san_paulo"),\n    (1695046452379L,"9909a8b1-2d15-4d3d-8ec9-efc48c536a00","rider-C","driver-M",33.90 ,"san_francisco"),\n    (1695332056404L,"1dced545-862b-4ceb-8b43-d2a568f6616b","rider-D","driver-N",93.50,"chennai"));\nvar df = spark.createDataFrame(data).toDF(columns:_*)\n\n// Insert data\ndf.write.format("hudi").\n  option(PARTITIONPATH_FIELD_NAME.key(), "city").\n  option(CDC_ENABLED.key(), "true").\n  option(TABLE_NAME, tableName).\n  mode(Overwrite).\n  save(basePath)\n\n// Update fare for riders: rider-A and rider-B \nval updatesDf = spark.read.format("hudi").load(basePath).filter($"rider" === "rider-A" || $"rider" === "rider-B").withColumn("fare", col("fare") * 10)\n\nupdatesDf.write.format("hudi").\n  option(OPERATION_OPT_KEY, "upsert").\n  option(PARTITIONPATH_FIELD_NAME.key(), "city").\n  option(CDC_ENABLED.key(), "true").\n  option(TABLE_NAME, tableName).\n  mode(Append).\n  save(basePath)\n\n\n// Query CDC data\nspark.read.option(BEGIN_INSTANTTIME.key(), 0).\n  option(QUERY_TYPE.key(), QUERY_TYPE_INCREMENTAL_OPT_VAL).\n  option(INCREMENTAL_FORMAT.key(), "cdc").\n  format("hudi").load(basePath).show(false)\n'})})}),(0,t.jsx)(l.A,{value:"python",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'# pyspark\n# Lets first insert data to a new table with cdc enabled.\ncolumns = ["ts","uuid","rider","driver","fare","city"]\ndata =[(1695159649087,"334e26e9-8355-45cc-97c6-c31daf0df330","rider-A","driver-K",19.10,"san_francisco"),\n       (1695091554788,"e96c4396-3fad-413a-a942-4cb36106d721","rider-B","driver-L",27.70 ,"san_francisco"),\n       (1695046462179,"9909a8b1-2d15-4d3d-8ec9-efc48c536a00","rider-C","driver-M",33.90 ,"san_francisco"),\n       (1695516137016,"e3cf430c-889d-4015-bc98-59bdce1e530c","rider-C","driver-N",34.15,"sao_paulo")]\n       \n\ninserts = spark.createDataFrame(data).toDF(*columns)\n\nhudi_options = {\n    \'hoodie.table.name\': tableName,\n    \'hoodie.datasource.write.partitionpath.field\': \'city\',\n    \'hoodie.table.cdc.enabled\': \'true\'\n}\n# Insert data\ninserts.write.format("hudi"). \\\n    options(**hudi_options). \\\n    mode("overwrite"). \\\n    save(basePath)\n\n\n#  Update fare for riders: rider-A and rider-B \nupdatesDf = spark.read.format("hudi").load(basePath).filter("rider == \'rider-A\' or rider == \'rider-B\'").withColumn("fare",col("fare")*10)\n\nupdatesDf.write.format("hudi"). \\\n  mode("append"). \\\n  save(basePath)\n\n# Query CDC data\ncdc_read_options = {\n    \'hoodie.datasource.query.incremental.format\': \'cdc\',\n    \'hoodie.datasource.query.type\': \'incremental\',\n    \'hoodie.datasource.read.begin.instanttime\': 0\n}\nspark.read.format("hudi"). \\\n    options(**cdc_read_options). \\\n    load(basePath).show(10, False)\n'})})}),(0,t.jsx)(l.A,{value:"sparksql",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-sql",children:"-- incrementally query data by path\n-- start from earliest available commit, end at latest available commit.\nSELECT * FROM hudi_table_changes('path/to/table', 'cdc', 'earliest');\n\n-- start from earliest, end at 202305160000.\nSELECT * FROM hudi_table_changes('path/to/table', 'cdc', 'earliest', '202305160000');\n\n-- start from 202305150000, end at 202305160000.\nSELECT * FROM hudi_table_changes('path/to/table', 'cdc', '202305150000', '202305160000');\n"})})})]}),"\n",(0,t.jsx)(a.admonition,{title:"Key requirements",type:"info",children:(0,t.jsx)(a.p,{children:"Note that CDC queries are currently only supported on Copy-on-Write tables."})}),"\n",(0,t.jsx)(a.h2,{id:"table-types",children:"Table Types"}),"\n",(0,t.jsxs)(a.p,{children:["The examples thus far have showcased one of the two table types, that Hudi supports - Copy-on-Write (COW) tables. Hudi also supports\na more advanced write-optimized table type called Merge-on-Read (MOR) tables, that can balance read and write performance in a more\nflexible manner. See ",(0,t.jsx)(a.a,{href:"/docs/table_types",children:"table types"})," for more details."]}),"\n",(0,t.jsx)(a.p,{children:"Any of these examples can be run on a Merge-on-Read table by simply changing the table type to MOR, while creating the table, as below."}),"\n",(0,t.jsxs)(i.A,{groupId:"programming-language",defaultValue:"scala",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"},{label:"Spark SQL",value:"sparksql"}],children:[(0,t.jsx)(l.A,{value:"scala",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-scala",children:'// spark-shell\ninserts.write.format("hudi").\n  ...\n  option(TABLE_TYPE.key(), "MERGE_ON_READ").\n  ...\n'})})}),(0,t.jsx)(l.A,{value:"python",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:"# pyspark\nhudi_options = {\n  ...\n  'hoodie.datasource.write.table.type': 'MERGE_ON_READ'\n}\n\ninserts.write.format(\"hudi\"). \\\noptions(**hudi_options). \\\nmode(\"overwrite\"). \\\nsave(basePath)\n"})})}),(0,t.jsx)(l.A,{value:"sparksql",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-sql",children:"CREATE TABLE hudi_table (\n    uuid STRING,\n    rider STRING,\n    driver STRING,\n    fare DOUBLE,\n    city STRING\n) USING HUDI TBLPROPERTIES (type = 'mor')\nPARTITIONED BY (city);\n"})})})]}),"\n",(0,t.jsx)(a.h2,{id:"keys",children:"Keys"}),"\n",(0,t.jsxs)(a.p,{children:["Hudi also allows users to specify a record key, which will be used to uniquely identify a record within a Hudi table. This is useful and\ncritical to support features like indexing and clustering, which speed up upserts and queries respectively, in a consistent manner. Some of the other\nbenefits of keys are explained in detail ",(0,t.jsx)(a.a,{href:"https://hudi.apache.org/blog/2023/05/19/hudi-metafields-demystified",children:"here"}),". To this end, Hudi supports a\nwide range of built-in ",(0,t.jsx)(a.a,{href:"https://hudi.apache.org/blog/2021/02/13/hudi-key-generators",children:"key generators"}),", that make it easy to generate record\nkeys for a given table. In the absence of a user configured key, Hudi will auto generate record keys, which are highly compressible."]}),"\n",(0,t.jsxs)(i.A,{groupId:"programming-language",defaultValue:"scala",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"},{label:"Spark SQL",value:"sparksql"}],children:[(0,t.jsx)(l.A,{value:"scala",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-scala",children:'// spark-shell\ninserts.write.format("hudi").\n...\noption(RECORDKEY_FIELD.key(), "uuid").\n...\n'})})}),(0,t.jsx)(l.A,{value:"python",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:"# pyspark\nhudi_options = {\n  ...\n  'hoodie.datasource.write.recordkey.field': 'uuid'\n}\n\ninserts.write.format(\"hudi\"). \\\noptions(**hudi_options). \\\nmode(\"overwrite\"). \\\nsave(basePath)\n"})})}),(0,t.jsx)(l.A,{value:"sparksql",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-sql",children:"CREATE TABLE hudi_table (\n    ts BIGINT,\n    uuid STRING,\n    rider STRING,\n    driver STRING,\n    fare DOUBLE,\n    city STRING\n) USING HUDI TBLPROPERTIES (primaryKey = 'uuid')\nPARTITIONED BY (city);\n"})})})]}),"\n",(0,t.jsx)(a.admonition,{title:"Implications of defining record keys",type:"note",children:(0,t.jsxs)(a.p,{children:["Configuring keys for a Hudi table, has a new implications on the table. If record key is set by the user, ",(0,t.jsx)(a.code,{children:"upsert"})," is chosen as the ",(0,t.jsx)(a.a,{href:"/docs/write_operations",children:"write operation"}),".\nAlso if a record key is configured, then it's also advisable to specify a precombine or ordering field, to correctly handle cases where the source data has\nmultiple records with the same key. See section below."]})}),"\n",(0,t.jsx)(a.h2,{id:"ordering-field",children:"Ordering Field"}),"\n",(0,t.jsxs)(a.p,{children:["Hudi also allows users to specify a ",(0,t.jsx)(a.em,{children:"precombine"})," field, which will be used to order and resolve conflicts between multiple versions of the same record. This is very important for\nuse-cases like applying database CDC logs to a Hudi table, where a given record may be appear multiple times in the source data due to repeated upstream updates.\nHudi also uses this mechanism to support out-of-order data arrival into a table, where records may need to be resolved in a different order than their commit time.\nFor e.g using a ",(0,t.jsx)(a.em,{children:"created_at"})," timestamp field as the precombine field will prevent older versions of a record from overwriting newer ones or being exposed to queries, even\nif they are written at a later commit time to the table. This is one of the key features, that makes Hudi, best suited for dealing with streaming data."]}),"\n",(0,t.jsxs)(i.A,{groupId:"programming-language",defaultValue:"scala",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"},{label:"Spark SQL",value:"sparksql"}],children:[(0,t.jsx)(l.A,{value:"scala",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-scala",children:'// spark-shell \nupdatesDf.write.format("hudi").\n  ...\n  option(PRECOMBINE_FIELD_NAME.key(), "ts").\n  ...\n'})})}),(0,t.jsx)(l.A,{value:"python",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:"# pyspark\nhudi_options = {\n...\n'hoodie.datasource.write.precombine.field': 'ts'\n}\n\nupsert.write.format(\"hudi\").\n    options(**hudi_options).\n    mode(\"append\").\n    save(basePath)\n"})})}),(0,t.jsx)(l.A,{value:"sparksql",children:(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-sql",children:"CREATE TABLE hudi_table (\n    ts BIGINT,\n    uuid STRING,\n    rider STRING,\n    driver STRING,\n    fare DOUBLE,\n    city STRING\n) USING HUDI TBLPROPERTIES (preCombineField = 'ts')\nPARTITIONED BY (city);\n"})})})]}),"\n",(0,t.jsx)(a.h2,{id:"where-to-go-from-here",children:"Where to go from here?"}),"\n",(0,t.jsxs)(a.p,{children:["You can also ",(0,t.jsx)(a.a,{href:"https://github.com/apache/hudi#building-apache-hudi-from-source",children:"build hudi yourself"})," and try this quickstart using ",(0,t.jsx)(a.code,{children:"--jars <path to spark bundle jar>"}),"(see also ",(0,t.jsx)(a.a,{href:"https://github.com/apache/hudi#build-with-different-spark-versions",children:"build with scala 2.12"}),")\nfor more info. If you are looking for ways to migrate your existing data to Hudi, refer to ",(0,t.jsx)(a.a,{href:"/docs/migration_guide",children:"migration guide"}),"."]}),"\n",(0,t.jsx)(a.h3,{id:"spark-sql-reference",children:"Spark SQL Reference"}),"\n",(0,t.jsxs)(a.p,{children:["For advanced usage of spark SQL, please refer to ",(0,t.jsx)(a.a,{href:"/docs/sql_ddl",children:"Spark SQL DDL"})," and ",(0,t.jsx)(a.a,{href:"/docs/sql_dml",children:"Spark SQL DML"})," reference guides.\nFor alter table commands, check out ",(0,t.jsx)(a.a,{href:"/docs/sql_ddl#spark-alter-table",children:"this"}),". Stored procedures provide a lot of powerful capabilities using Hudi SparkSQL to assist with monitoring, managing and operating Hudi tables, please check ",(0,t.jsx)(a.a,{href:"/docs/procedures",children:"this"})," out."]}),"\n",(0,t.jsx)(a.h3,{id:"streaming-workloads",children:"Streaming workloads"}),"\n",(0,t.jsx)(a.p,{children:"Hudi provides industry-leading performance and functionality for streaming data."}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.strong,{children:"Hudi Streamer"})," - Hudi provides an incremental ingestion/ETL tool - ",(0,t.jsx)(a.a,{href:"/docs/hoodie_streaming_ingestion#hudi-streamer",children:"HoodieStreamer"}),", to assist with ingesting data into Hudi\nfrom various different sources in a streaming manner, with powerful built-in capabilities like auto checkpointing, schema enforcement via schema provider,\ntransformation support, automatic table services and so on."]}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.strong,{children:"Structured Streaming"})," - Hudi supports Spark Structured Streaming reads and writes as well. Please see ",(0,t.jsx)(a.a,{href:"/docs/hoodie_streaming_ingestion#structured-streaming",children:"here"})," for more."]}),"\n",(0,t.jsxs)(a.p,{children:["Check out more information on ",(0,t.jsx)(a.a,{href:"/docs/faq#how-do-i-model-the-data-stored-in-hudi",children:"modeling data in Hudi"})," and different ways to ",(0,t.jsx)(a.a,{href:"/docs/writing_data",children:"writing Hudi Tables"}),"."]}),"\n",(0,t.jsx)(a.h3,{id:"dockerized-demo",children:"Dockerized Demo"}),"\n",(0,t.jsxs)(a.p,{children:["Even as we showcased the core capabilities, Hudi supports a lot more advanced functionality that can make it easy\nto get your transactional data lakes up and running quickly, across a variety query engines like Hive, Flink, Spark, Presto, Trino and much more.\nWe have put together a ",(0,t.jsx)(a.a,{href:"https://www.youtube.com/watch?v=VhNgUsxdrD0",children:"demo video"})," that showcases all of this on a docker based setup with all\ndependent systems running locally. We recommend you replicate the same setup and run the demo yourself, by following\nsteps ",(0,t.jsx)(a.a,{href:"/docs/docker_demo",children:"here"})," to get a taste for it."]})]})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,t.jsx)(a,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}},19365:(e,a,r)=>{r.d(a,{A:()=>i});r(96540);var n=r(34164);const t={tabItem:"tabItem_Ymn6"};var s=r(74848);function i(e){let{children:a,hidden:r,className:i}=e;return(0,s.jsx)("div",{role:"tabpanel",className:(0,n.A)(t.tabItem,i),hidden:r,children:a})}},11470:(e,a,r)=>{r.d(a,{A:()=>j});var n=r(96540),t=r(34164),s=r(23104),i=r(56347),l=r(205),o=r(57485),d=r(31682),c=r(70679);function h(e){return n.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,n.isValidElement)(e)&&function(e){const{props:a}=e;return!!a&&"object"==typeof a&&"value"in a}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function p(e){const{values:a,children:r}=e;return(0,n.useMemo)((()=>{const e=a??function(e){return h(e).map((e=>{let{props:{value:a,label:r,attributes:n,default:t}}=e;return{value:a,label:r,attributes:n,default:t}}))}(r);return function(e){const a=(0,d.XI)(e,((e,a)=>e.value===a.value));if(a.length>0)throw new Error(`Docusaurus error: Duplicate values "${a.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[a,r])}function u(e){let{value:a,tabValues:r}=e;return r.some((e=>e.value===a))}function m(e){let{queryString:a=!1,groupId:r}=e;const t=(0,i.W6)(),s=function(e){let{queryString:a=!1,groupId:r}=e;if("string"==typeof a)return a;if(!1===a)return null;if(!0===a&&!r)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return r??null}({queryString:a,groupId:r});return[(0,o.aZ)(s),(0,n.useCallback)((e=>{if(!s)return;const a=new URLSearchParams(t.location.search);a.set(s,e),t.replace({...t.location,search:a.toString()})}),[s,t])]}function f(e){const{defaultValue:a,queryString:r=!1,groupId:t}=e,s=p(e),[i,o]=(0,n.useState)((()=>function(e){let{defaultValue:a,tabValues:r}=e;if(0===r.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(a){if(!u({value:a,tabValues:r}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${a}" but none of its children has the corresponding value. Available values are: ${r.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return a}const n=r.find((e=>e.default))??r[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:a,tabValues:s}))),[d,h]=m({queryString:r,groupId:t}),[f,g]=function(e){let{groupId:a}=e;const r=function(e){return e?`docusaurus.tab.${e}`:null}(a),[t,s]=(0,c.Dv)(r);return[t,(0,n.useCallback)((e=>{r&&s.set(e)}),[r,s])]}({groupId:t}),b=(()=>{const e=d??f;return u({value:e,tabValues:s})?e:null})();(0,l.A)((()=>{b&&o(b)}),[b]);return{selectedValue:i,selectValue:(0,n.useCallback)((e=>{if(!u({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);o(e),h(e),g(e)}),[h,g,s]),tabValues:s}}var g=r(92303);const b={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var x=r(74848);function k(e){let{className:a,block:r,selectedValue:n,selectValue:i,tabValues:l}=e;const o=[],{blockElementScrollPositionUntilNextRender:d}=(0,s.a_)(),c=e=>{const a=e.currentTarget,r=o.indexOf(a),t=l[r].value;t!==n&&(d(a),i(t))},h=e=>{let a=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const r=o.indexOf(e.currentTarget)+1;a=o[r]??o[0];break}case"ArrowLeft":{const r=o.indexOf(e.currentTarget)-1;a=o[r]??o[o.length-1];break}}a?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,t.A)("tabs",{"tabs--block":r},a),children:l.map((e=>{let{value:a,label:r,attributes:s}=e;return(0,x.jsx)("li",{role:"tab",tabIndex:n===a?0:-1,"aria-selected":n===a,ref:e=>o.push(e),onKeyDown:h,onClick:c,...s,className:(0,t.A)("tabs__item",b.tabItem,s?.className,{"tabs__item--active":n===a}),children:r??a},a)}))})}function y(e){let{lazy:a,children:r,selectedValue:s}=e;const i=(Array.isArray(r)?r:[r]).filter(Boolean);if(a){const e=i.find((e=>e.props.value===s));return e?(0,n.cloneElement)(e,{className:(0,t.A)("margin-top--md",e.props.className)}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:i.map(((e,a)=>(0,n.cloneElement)(e,{key:a,hidden:e.props.value!==s})))})}function v(e){const a=f(e);return(0,x.jsxs)("div",{className:(0,t.A)("tabs-container",b.tabList),children:[(0,x.jsx)(k,{...a,...e}),(0,x.jsx)(y,{...a,...e})]})}function j(e){const a=(0,g.A)();return(0,x.jsx)(v,{...e,children:h(e.children)},String(a))}},28453:(e,a,r)=>{r.d(a,{R:()=>i,x:()=>l});var n=r(96540);const t={},s=n.createContext(t);function i(e){const a=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function l(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),n.createElement(s.Provider,{value:a},e.children)}}}]);