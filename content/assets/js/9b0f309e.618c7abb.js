"use strict";(globalThis.webpackChunkhudi=globalThis.webpackChunkhudi||[]).push([[49595],{8792(e){e.exports=JSON.parse('{"permalink":"/blog/2025/11/25/apache-hudi-release-1-1-announcement","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-11-25-apache-hudi-release-1-1-announcement.md","source":"@site/blog/2025-11-25-apache-hudi-release-1-1-announcement.md","title":"Apache Hudi 1.1 is Here\u2014Building the Foundation for the Next Generation of Lakehouse","description":"The Hudi community is excited to announce the release of Hudi 1.1, a major milestone that sets the stage for the next generation of data lakehouse capabilities. This release represents months of focused engineering on foundational improvements, engine-specific optimizations, and key architectural enhancements, laying the foundation for ambitious features coming in future releases.","date":"2025-11-25T00:00:00.000Z","tags":[{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"release","permalink":"/blog/tags/release"},{"inline":true,"label":"feature","permalink":"/blog/tags/feature"},{"inline":true,"label":"performance","permalink":"/blog/tags/performance"}],"readingTime":14.47,"hasTruncateMarker":false,"authors":[{"name":"Shiyan Xu","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi 1.1 is Here\u2014Building the Foundation for the Next Generation of Lakehouse","excerpt":"","author":"Shiyan Xu","category":"blog","image":"/assets/images/blog/2025-11-25-apache-hudi-release-1-1-announcement/1-pluggable-TF.png","tags":["hudi","release","feature","performance"]},"unlisted":false,"prevItem":{"title":"Apache Hudi Dynamic Bloom Filter\\"","permalink":"/blog/2025/11/28/Apache-Hudi-Dynamic-Bloom-Filter"},"nextItem":{"title":"Deep Dive Into Hudi\'s Indexing Subsystem (Part 2 of 2)","permalink":"/blog/2025/11/12/deep-dive-into-hudis-indexing-subsystem-part-2-of-2"}}')},28453(e,i,t){t.d(i,{R:()=>o,x:()=>s});var a=t(96540);const n={},r=a.createContext(n);function o(e){const i=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function s(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:o(e.components),a.createElement(r.Provider,{value:i},e.children)}},38326(e,i,t){t.d(i,{A:()=>a});const a=t.p+"assets/images/7-flink-write-throughput-chart-040f5478de7a2bf3fa8ddb03ffa499c0.png"},38746(e,i,t){t.d(i,{A:()=>a});const a=t.p+"assets/images/1-pluggable-TF-36f7e26bf8dc4a479bcaff713a24debd.png"},38846(e,i,t){t.d(i,{A:()=>a});const a=t.p+"assets/images/2-metadata-table-lookup-451d218e2a4fc0aac40b6d3c37522572.png"},48840(e,i,t){t.r(i),t.d(i,{assets:()=>d,contentTitle:()=>s,default:()=>h,frontMatter:()=>o,metadata:()=>a,toc:()=>l});var a=t(8792),n=t(74848),r=t(28453);const o={title:"Apache Hudi 1.1 is Here\u2014Building the Foundation for the Next Generation of Lakehouse",excerpt:"",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2025-11-25-apache-hudi-release-1-1-announcement/1-pluggable-TF.png",tags:["hudi","release","feature","performance"]},s=void 0,d={authorsImageUrls:[void 0]},l=[{value:"Pluggable Table Format\u2014The Foundation for Multi-Format Support",id:"pluggable-table-formatthe-foundation-for-multi-format-support",level:2},{value:"Vision and Design",id:"vision-and-design",level:3},{value:"Key Architectural Components",id:"key-architectural-components",level:3},{value:"Indexing Improvements\u2014Faster and Smarter Lookups",id:"indexing-improvementsfaster-and-smarter-lookups",level:2},{value:"Partitioned Record Index",id:"partitioned-record-index",level:3},{value:"Partition-level Bucket Index",id:"partition-level-bucket-index",level:3},{value:"Indexing Performance Optimizations",id:"indexing-performance-optimizations",level:3},{value:"Faster Clustering with Parquet File Binary Copy",id:"faster-clustering-with-parquet-file-binary-copy",level:2},{value:"Storage-Based Lock Provider\u2014Eliminating External Dependencies for Concurrent Writers",id:"storage-based-lock-providereliminating-external-dependencies-for-concurrent-writers",level:2},{value:"Use Merge Modes and Custom Mergers\u2014Say Goodbye to Payload Classes",id:"use-merge-modes-and-custom-mergerssay-goodbye-to-payload-classes",level:2},{value:"Merge Modes\u2014Declarative Record Merging",id:"merge-modesdeclarative-record-merging",level:3},{value:"Custom Mergers\u2014The Flexible Approach",id:"custom-mergersthe-flexible-approach",level:3},{value:"Apache Spark Integration Improvements",id:"apache-spark-integration-improvements",level:2},{value:"Spark 4.0 Support",id:"spark-40-support",level:3},{value:"Metadata Table Streaming Writes",id:"metadata-table-streaming-writes",level:3},{value:"New and Enhanced SQL Procedures",id:"new-and-enhanced-sql-procedures",level:3},{value:"Apache Flink Integration Improvements",id:"apache-flink-integration-improvements",level:2},{value:"Flink 2.0 Support",id:"flink-20-support",level:3},{value:"Engine-Native Record Support",id:"engine-native-record-support",level:3},{value:"Buffer Sort",id:"buffer-sort",level:3},{value:"New Integration: Apache Polaris (Incubating)",id:"new-integration-apache-polaris-incubating",level:2},{value:"What\u2019s Next\u2014Join Us in Building the Future",id:"whats-nextjoin-us-in-building-the-future",level:2}];function c(e){const i={a:"a",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(i.p,{children:["The Hudi community is excited to announce the ",(0,n.jsx)(i.a,{href:"https://hudi.apache.org/releases/release-1.1#release-111",children:"release of Hudi 1.1"}),", a major milestone that sets the stage for the next generation of data lakehouse capabilities. This release represents months of focused engineering on foundational improvements, engine-specific optimizations, and key architectural enhancements, laying the foundation for ambitious features coming in future releases."]}),"\n",(0,n.jsx)(i.p,{children:"Hudi continues to evolve rapidly, with contributions from a vibrant community of developers and users. The 1.1 release brings over 800 commits addressing performance bottlenecks, expanding engine support, and introducing new capabilities that make Hudi tables more reliable, faster, and easier to operate. Let\u2019s dive into the highlights."}),"\n",(0,n.jsx)(i.h2,{id:"pluggable-table-formatthe-foundation-for-multi-format-support",children:"Pluggable Table Format\u2014The Foundation for Multi-Format Support"}),"\n",(0,n.jsxs)(i.p,{children:["Hudi 1.1 introduces a ",(0,n.jsx)(i.a,{href:"https://hudi.apache.org/docs/hudi_stack#pluggable-table-format",children:"pluggable table format"})," framework that opens up the powerful storage engine capabilities beyond Hudi\u2019s native storage format to other table formats like Apache Iceberg and Delta Lake. This framework represents a fundamental shift in how Hudi approaches table format support, enabling native integration of multiple formats and giving you a unified system with total read-write compatibility across formats."]}),"\n",(0,n.jsx)(i.h3,{id:"vision-and-design",children:"Vision and Design"}),"\n",(0,n.jsx)(i.p,{children:"The table format landscape in the modern lakehouse ecosystem is diverse and evolving. Like a game of rock-paper-scissors, different formats\u2014Hudi, Iceberg, Delta Lake\u2014each have unique strengths for specific use cases. Rather than forcing a one-size-fits-all approach, Hudi 1.1 introduces a pluggable table format framework that embraces the open lakehouse ecosystem and prevents vendor lock-in."}),"\n",(0,n.jsxs)(i.p,{children:["The framework is built on a clean abstraction layer that decouples Hudi\u2019s core capabilities\u2014transaction management, indexing, concurrency control, and table services\u2014from the specific storage format used for data files. At the heart of this design is the ",(0,n.jsx)(i.code,{children:"HoodieTableFormat"})," interface, which different format implementations can extend."]}),"\n",(0,n.jsx)(i.p,{children:(0,n.jsx)(i.img,{alt:"pluggable table format",src:t(38746).A+"",width:"894",height:"665"})}),"\n",(0,n.jsx)(i.h3,{id:"key-architectural-components",children:"Key Architectural Components"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"Storage engine: Hudi\u2019s storage engine capabilities, such as timeline management, concurrency control mechanisms, indexes, and table services, can work across multiple table formats"}),"\n",(0,n.jsx)(i.li,{children:"Pluggable adapters: Format-specific implementations handle the generation of conforming metadata upon writes"}),"\n"]}),"\n",(0,n.jsxs)(i.p,{children:["Hudi\u2019s artifact provides support for the native Hudi format, while ",(0,n.jsx)(i.a,{href:"https://xtable.apache.org/",children:"Apache XTable (incubating)"})," supplies pluggable format adapters. For example, ",(0,n.jsx)(i.a,{href:"https://github.com/apache/incubator-xtable/pull/723",children:"this XTable PR"})," implements the Iceberg adapter to allow you to add dependencies to your running pipelines as needed. This architecture enables organizations to choose the right format for each use case while maintaining a unified operational experience and leveraging Hudi\u2019s sophisticated storage engine across all of them."]}),"\n",(0,n.jsxs)(i.p,{children:["In the 1.1 release, the framework comes with native Hudi format support (configured via ",(0,n.jsx)(i.code,{children:"hoodie.table.format=native"})," by default). Existing users don't need to change anything\u2014tables continue to work exactly as before. The real excitement lies ahead: the framework paves the way for supporting additional formats like Iceberg and Delta Lake. Imagine writing high-frequency updates to a Hudi table efficiently with Hudi's record-level indexing capability while maintaining Iceberg metadata through the Iceberg adapter, which supports a wide range of catalogs for reads. The pluggable table format framework in 1.1 makes such usage patterns possible\u2014a game-changer for organizations that need flexibility and openness in their data architecture."]}),"\n",(0,n.jsx)(i.h2,{id:"indexing-improvementsfaster-and-smarter-lookups",children:"Indexing Improvements\u2014Faster and Smarter Lookups"}),"\n",(0,n.jsx)(i.p,{children:"Hudi\u2019s indexing subsystem is one of its most powerful features, enabling fast record lookups during writes and efficient data skipping during reads."}),"\n",(0,n.jsx)(i.h3,{id:"partitioned-record-index",children:"Partitioned Record Index"}),"\n",(0,n.jsxs)(i.p,{children:["Since version 0.14.0, Hudi has supported a global record index in the indexing subsystem\u2014a breakthrough that enables blazing-fast lookups on large datasets. While this is ideal for globally unique identifiers like order IDs or SSNs, many scenarios only require uniqueness within a partition\u2014for example, user events partitioned by date. Hudi 1.1 introduces the ",(0,n.jsx)(i.a,{href:"https://hudi.apache.org/docs/indexes#record-index",children:"partitioned record index"}),", a non-global variant of the record index that works with the combination of partition path and record key, leveraging partition information to prune irrelevant partitions during lookups and dramatically reducing the search space, and thus achieving efficient lookups even on very large datasets."]}),"\n",(0,n.jsx)(i.pre,{children:(0,n.jsx)(i.code,{className:"language-sql",children:"-- Spark SQL: Create table with partitioned record index\nCREATE TABLE user_activity (\n  user_id STRING,\n  activity_type STRING,\n  timestamp BIGINT,\n  event_date DATE\n) USING hudi\nTBLPROPERTIES (\n  'primaryKey' = 'user_id',\n  'preCombineField' = 'timestamp',\n  -- Enable partitioned record index\n  'hoodie.metadata.record.level.index.enable' = 'true',\n  'hoodie.index.type' = 'RECORD_LEVEL_INDEX'\n)\nPARTITIONED BY (event_date);\n"})}),"\n",(0,n.jsx)(i.p,{children:"The partitioned record index enables index lookups that scale proportionally with partition size\u2014file group accesses correlate directly to the data partition size, optimizing performance across heterogeneous data distributions. The design also supports future clustering operations that can dynamically expand file groups within partitions as they grow."}),"\n",(0,n.jsx)(i.h3,{id:"partition-level-bucket-index",children:"Partition-level Bucket Index"}),"\n",(0,n.jsx)(i.p,{children:"The bucket index is a popular choice for high-throughput write workloads because it eliminates expensive record lookups by deterministically mapping keys to file groups. However, the existing bucket index has a key limitation: once you set the number of buckets, changing it requires rewriting the entire table."}),"\n",(0,n.jsx)(i.p,{children:"The 1.1 release introduces partition-level bucket index, which enables different bucket counts for different partitions using regex-based rules. This design allows tables to adapt as data volumes change over time\u2014for example, older, smaller partitions can use fewer buckets while newer, larger partitions can have more."}),"\n",(0,n.jsx)(i.pre,{children:(0,n.jsx)(i.code,{className:"language-sql",children:"-- Spark SQL: Create table with partition-level bucket index\nCREATE TABLE sales_transactions (\n  transaction_id BIGINT,\n  user_id BIGINT,\n  amount DOUBLE,\n  transaction_date DATE\n) USING hudi\nTBLPROPERTIES (\n  'primaryKey' = 'transaction_id',\n  -- Partition-level bucket index\n  'hoodie.index.type' = 'BUCKET',\n  'hoodie.bucket.index.hash.field' = 'transaction_id',\n  'hoodie.bucket.index.partition.rule.type' = 'regex',\n  'hoodie.bucket.index.partition.expressions' = '2023-.*,16;2024-.*,32;2025-.*,64',\n  'hoodie.bucket.index.num.buckets' = '8'\n)\nPARTITIONED BY (transaction_date);\n"})}),"\n",(0,n.jsxs)(i.p,{children:["The partition-level bucket index is ideal for time-series data where partition sizes vary significantly over time. The adaptive bucket sizing helps you maintain optimal write performance as your data volume changes. See the ",(0,n.jsx)(i.a,{href:"https://hudi.apache.org/docs/indexes#additional-writer-side-indexes",children:"docs"})," and ",(0,n.jsx)(i.a,{href:"https://github.com/apache/hudi/blob/master/rfc/rfc-89/rfc-89.md",children:"RFC 89"})," for more information."]}),"\n",(0,n.jsx)(i.h3,{id:"indexing-performance-optimizations",children:"Indexing Performance Optimizations"}),"\n",(0,n.jsx)(i.p,{children:"Beyond new indexes, Hudi 1.1 delivers substantial performance improvements for metadata table operations:"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"HFile block cache and prefetching: The new block cache stores recently accessed data blocks in memory, avoiding repeated reads from storage. For smaller HFiles, Hudi prefetches the entire file upfront rather than making multiple read requests. Benchmarks show approximately 4x speedup for repeated lookups, enabled by default."}),"\n"]}),"\n",(0,n.jsx)(i.p,{children:(0,n.jsx)(i.img,{alt:"metadata table key lookup",src:t(38846).A+"",width:"960",height:"540"})}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsxs)(i.li,{children:["HFile Bloom filter: Adding Bloom filters to HFiles enables Hudi to quickly determine whether a key might exist in a file before fetching data blocks, avoiding unnecessary I/O and dramatically speeding up point lookups. You can enable it with ",(0,n.jsx)(i.code,{children:"hoodie.metadata.bloom.filter.enable=true"}),"."]}),"\n"]}),"\n",(0,n.jsx)(i.p,{children:"These optimizations compound to make the metadata table significantly faster, directly improving both write and read performance across your Hudi tables. Additionally, Hudi 1.1 adds its own native HFile writer implementation, eliminating the dependency on HBase libraries. This refactoring significantly reduces the Hudi bundle size and provides the foundation for future HFile performance optimizations."}),"\n",(0,n.jsx)(i.h2,{id:"faster-clustering-with-parquet-file-binary-copy",children:"Faster Clustering with Parquet File Binary Copy"}),"\n",(0,n.jsx)(i.p,{children:"Clustering reorganizes data to improve query performance, but traditional approaches are expensive\u2014decompressing, decoding, transforming, re-encoding, and re-compressing data even when no transformation is needed."}),"\n",(0,n.jsx)(i.p,{children:"Hudi 1.1 implements Parquet file binary copy for clustering operations. Instead of processing records, this optimization directly copies Parquet RowGroups from source to destination files when schema-compatible, eliminating redundant transformations entirely."}),"\n",(0,n.jsx)(i.p,{children:(0,n.jsx)(i.img,{alt:"parquet binary copy",src:t(59731).A+"",width:"739",height:"407"})}),"\n",(0,n.jsx)(i.p,{children:"On 100GB test data, using Parquet file binary copy achieved 15x faster execution (18 minutes \u2192 1.2 minutes) and 95% reduction in compute (28.7 task-hours \u2192 1.3 task-hours) compared to the normal rewriting of Parquet files. Real-world validation with 1.7TB datasets (300 columns) showed approximately 5x performance improvement (35 min \u2192 7.7 min) with CPU usage dropping from 90% to 60%."}),"\n",(0,n.jsx)(i.p,{children:(0,n.jsx)(i.img,{alt:"parquet binary copy chart",src:t(73239).A+"",width:"960",height:"540"})}),"\n",(0,n.jsxs)(i.p,{children:["The optimization is currently supported for Copy-on-Write tables and enabled automatically when safe, with Hudi intelligently falling back to traditional clustering when schema reconciliation is required. You may refer to ",(0,n.jsx)(i.a,{href:"https://github.com/apache/hudi/pull/13365",children:"this PR"})," for more detail."]}),"\n",(0,n.jsx)(i.h2,{id:"storage-based-lock-providereliminating-external-dependencies-for-concurrent-writers",children:"Storage-Based Lock Provider\u2014Eliminating External Dependencies for Concurrent Writers"}),"\n",(0,n.jsx)(i.p,{children:"Multi-writer concurrency is critical for production data lakehouses, where multiple jobs need to write to the same table simultaneously. Historically, enabling multi-writer support in Hudi required setting up external lock providers like AWS DynamoDB, Apache Zookeeper, or Hive Metastore. While these work well, they add operational complexity\u2014you need to provision, maintain, and monitor additional infrastructure."}),"\n",(0,n.jsxs)(i.p,{children:["Hudi 1.1 introduces a storage-based lock provider that eliminates this dependency entirely by managing concurrency directly using the ",(0,n.jsx)(i.code,{children:".hoodie/"})," directory in your table's storage layer."]}),"\n",(0,n.jsx)(i.p,{children:(0,n.jsx)(i.img,{alt:"storage based lock provider",src:t(72806).A+"",width:"708",height:"373"})}),"\n",(0,n.jsxs)(i.p,{children:["The implementation uses conditional writes on a single lock file under ",(0,n.jsx)(i.code,{children:".hoodie/.locks/"})," to ensure only one writer holds the lock at a time, with heartbeat-based renewal and automatic expiration for fault tolerance. To use the storage-based lock provider, you need to add the corresponding Hudi cloud bundle (",(0,n.jsx)(i.code,{children:"hudi-aws-bundle"})," for S3 and ",(0,n.jsx)(i.code,{children:"hudi-gcp-bundle"})," for GCS) and set the following configuration:"]}),"\n",(0,n.jsx)(i.pre,{children:(0,n.jsx)(i.code,{className:"language-properties",children:"hoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.StorageBasedLockProvider\n"})}),"\n",(0,n.jsxs)(i.p,{children:["This approach eliminates the need for DynamoDB, ZooKeeper, or Hive Metastore dependencies, reducing operational costs and infrastructure complexity. The cloud-native design works directly with S3 or GCS storage features, with support for additional storage systems planned, making Hudi easier to operate at scale in cloud-native environments. Check out the ",(0,n.jsx)(i.a,{href:"https://hudi.apache.org/docs/concurrency_control#storage-based-lock-provider",children:"docs"})," and ",(0,n.jsx)(i.a,{href:"https://github.com/apache/hudi/blob/master/rfc/rfc-91/rfc-91.md",children:"RFC 91"})," for more detail."]}),"\n",(0,n.jsx)(i.h2,{id:"use-merge-modes-and-custom-mergerssay-goodbye-to-payload-classes",children:"Use Merge Modes and Custom Mergers\u2014Say Goodbye to Payload Classes"}),"\n",(0,n.jsx)(i.p,{children:"A core design principle of Hudi is enabling the storage layer to understand how to merge updates to the same record key, even when changes arrive out of order\u2014a common scenario with mobile apps, IoT devices, and distributed systems. Prior to Hudi 1.1, record merging logic was primarily implemented through payload classes, which were fragmented and lacked standardized semantics."}),"\n",(0,n.jsxs)(i.p,{children:["Hudi 1.1 deprecates payload classes and encourages users to adopt the new APIs introduced since 1.0 for record merging: merge modes and the ",(0,n.jsx)(i.code,{children:"HoodieRecordMerger"})," interface."]}),"\n",(0,n.jsx)(i.h3,{id:"merge-modesdeclarative-record-merging",children:"Merge Modes\u2014Declarative Record Merging"}),"\n",(0,n.jsxs)(i.p,{children:["For common use cases, the ",(0,n.jsx)(i.code,{children:"COMMIT_TIME_ORDERING"})," and ",(0,n.jsx)(i.code,{children:"EVENT_TIME_ORDERING"})," merge modes provide a declarative way to specify merge behavior:"]}),"\n",(0,n.jsxs)(i.table,{children:[(0,n.jsx)(i.thead,{children:(0,n.jsxs)(i.tr,{children:[(0,n.jsx)(i.th,{style:{textAlign:"left"},children:"Merge mode"}),(0,n.jsx)(i.th,{style:{textAlign:"left"},children:"What does it do?"})]})}),(0,n.jsxs)(i.tbody,{children:[(0,n.jsxs)(i.tr,{children:[(0,n.jsx)(i.td,{style:{textAlign:"left"},children:(0,n.jsx)(i.code,{children:"COMMIT_TIME_ORDERING"})}),(0,n.jsx)(i.td,{style:{textAlign:"left"},children:"Picks the record with the highest completion time/instant as the final merge result (standard relational semantics or arrival time processing)"})]}),(0,n.jsxs)(i.tr,{children:[(0,n.jsx)(i.td,{style:{textAlign:"left"},children:(0,n.jsx)(i.code,{children:"EVENT_TIME_ORDERING"})}),(0,n.jsx)(i.td,{style:{textAlign:"left"},children:"Picks the record with the highest value on a user-specified ordering field as the final merge result. Enables event time processing semantics for handling late-arriving data without corrupting record state."})]})]})]}),"\n",(0,n.jsxs)(i.p,{children:["The default behavior is adaptive: if no ordering field (",(0,n.jsx)(i.code,{children:"hoodie.table.ordering.fields"}),") is configured, Hudi defaults to ",(0,n.jsx)(i.code,{children:"COMMIT_TIME_ORDERING"}),"; if one or more ordering fields are set, it uses ",(0,n.jsx)(i.code,{children:"EVENT_TIME_ORDERING"}),". This makes Hudi work out-of-the-box for simple use cases while still supporting event-time ordering when needed."]}),"\n",(0,n.jsx)(i.h3,{id:"custom-mergersthe-flexible-approach",children:"Custom Mergers\u2014The Flexible Approach"}),"\n",(0,n.jsxs)(i.p,{children:["For complex merging logic\u2014such as field-level reconciliation, aggregating counters, or preserving audit fields\u2014the ",(0,n.jsx)(i.code,{children:"HoodieRecordMerger"})," interface provides a modern, engine-native alternative to payload classes. You need to set the merge mode to ",(0,n.jsx)(i.code,{children:"CUSTOM"})," and provide your own implementation of ",(0,n.jsx)(i.code,{children:"HoodieRecordMerger"}),". By using the new API, you can achieve consistent merging across all code paths: precombine, updating writes, compaction, and snapshot reads\u2014you are strongly encouraged to migrate to the new APIs. See ",(0,n.jsx)(i.a,{href:"https://hudi.apache.org/docs/record_merger",children:"the docs"})," for more details. For migration guidance, see the ",(0,n.jsx)(i.a,{href:"https://hudi.apache.org/releases/release-1.1#release-111",children:"release notes"})," and ",(0,n.jsx)(i.a,{href:"https://github.com/apache/hudi/pull/13499",children:"RFC-97"}),"."]}),"\n",(0,n.jsx)(i.h2,{id:"apache-spark-integration-improvements",children:"Apache Spark Integration Improvements"}),"\n",(0,n.jsx)(i.p,{children:"Spark remains one of the most popular engines for working with Hudi tables, and the 1.1 release brings several important enhancements."}),"\n",(0,n.jsx)(i.h3,{id:"spark-40-support",children:"Spark 4.0 Support"}),"\n",(0,n.jsxs)(i.p,{children:["Spark 4.0 brought significant performance gains for ML/AI workloads, smarter query optimization with automatic join strategy switching, dynamic partition skew mitigation, and enhanced streaming capabilities. Hudi 1.1 adds Spark 4.0 support to unlock these improvements for working with Hudi tables. To get started, use the new ",(0,n.jsx)(i.code,{children:"hudi-spark4.0-bundle_2.13:1.1.1"})," artifact in your dependency list."]}),"\n",(0,n.jsx)(i.h3,{id:"metadata-table-streaming-writes",children:"Metadata Table Streaming Writes"}),"\n",(0,n.jsx)(i.p,{children:"Hudi 1.1 introduces streaming writes to the metadata table, unifying data and metadata writes into a single RDD execution chain. The key design generates metadata records directly during data writes in parallel across executors, eliminating redundant file lookups that previously created bottlenecks and enhancing reliability when performing stage retries in Spark."}),"\n",(0,n.jsx)(i.p,{children:(0,n.jsx)(i.img,{alt:"spark upsert time chart",src:t(85337).A+"",width:"960",height:"540"})}),"\n",(0,n.jsx)(i.p,{children:"A benchmark with update-intensive workloads showed that this 1.1 feature delivered about 18% faster write times for tables with record index, compared to Hudi 1.0. The feature is enabled by default for Spark writers."}),"\n",(0,n.jsx)(i.h3,{id:"new-and-enhanced-sql-procedures",children:"New and Enhanced SQL Procedures"}),"\n",(0,n.jsxs)(i.p,{children:["Hudi 1.1 expands the ",(0,n.jsx)(i.a,{href:"https://hudi.apache.org/docs/procedures",children:"SQL procedure"})," library with useful additions and enhanced capabilities for table management and observability, bringing operational capabilities directly into Spark SQL."]}),"\n",(0,n.jsxs)(i.p,{children:["The new procedures, ",(0,n.jsx)(i.code,{children:"show_cleans"}),", ",(0,n.jsx)(i.code,{children:"show_clean_plans"}),", and ",(0,n.jsx)(i.code,{children:"show_cleans_metadata"}),", provide visibility into cleaning operations:"]}),"\n",(0,n.jsx)(i.pre,{children:(0,n.jsx)(i.code,{className:"language-sql",children:"CALL show_cleans(table => 'hudi_table', limit => 10);\nCALL show_clean_plans(table => 'hudi_table', limit => 10);\nCALL show_cleans_metadata(table => 'hudi_table', limit => 10);\n"})}),"\n",(0,n.jsxs)(i.p,{children:["The enhanced ",(0,n.jsx)(i.code,{children:"run_clustering"})," procedure supports partition filtering with regex patterns:"]}),"\n",(0,n.jsx)(i.pre,{children:(0,n.jsx)(i.code,{className:"language-sql",children:"-- Cluster all 2025 partitions matching a pattern\nCALL run_clustering(\n  table => 'hudi_table',\n  partition_regex_pattern => '2025-.*',\n);\n"})}),"\n",(0,n.jsxs)(i.p,{children:["All ",(0,n.jsx)(i.code,{children:"show"})," procedures, where applicable, were enhanced with ",(0,n.jsx)(i.code,{children:"path"})," and ",(0,n.jsx)(i.code,{children:"filter"})," parameters. ",(0,n.jsx)(i.code,{children:"path"})," helps when ",(0,n.jsx)(i.code,{children:"table_name"})," is not able to identify a table properly. ",(0,n.jsx)(i.code,{children:"filter"})," can support advanced predicate expressions. For example:"]}),"\n",(0,n.jsx)(i.pre,{children:(0,n.jsx)(i.code,{className:"language-sql",children:"-- Find large files in recent partitions\nCALL show_file_status(\n  path => '/data/warehouse/transactions',\n  filter => \"partition LIKE '2025-11%' AND file_size > 524288000\"\n);\n"})}),"\n",(0,n.jsx)(i.p,{children:"The new and enhanced SQL procedures bring table management directly into Spark SQL, streamlining operations for SQL-focused workflows."}),"\n",(0,n.jsx)(i.h2,{id:"apache-flink-integration-improvements",children:"Apache Flink Integration Improvements"}),"\n",(0,n.jsx)(i.p,{children:"Flink is a popular choice for real-time data pipelines, and Hudi 1.1 brings substantial improvements to the Flink integration."}),"\n",(0,n.jsx)(i.h3,{id:"flink-20-support",children:"Flink 2.0 Support"}),"\n",(0,n.jsxs)(i.p,{children:["Hudi 1.1 brings support for Flink 2.0, the first major Flink release in nine years. Flink 2.0 introduced disaggregated state storage (ForSt) that decouples state from compute for unlimited scalability, asynchronous state execution for improved resource utilization, adaptive broadcast join for efficient query processing, and materialized tables for simplified stream-batch unification. Use the new ",(0,n.jsx)(i.code,{children:"hudi-flink2.0-bundle:1.1.1"})," artifact to get started."]}),"\n",(0,n.jsx)(i.h3,{id:"engine-native-record-support",children:"Engine-Native Record Support"}),"\n",(0,n.jsxs)(i.p,{children:["Hudi 1.1 eliminates expensive Avro conversions by processing Flink's native ",(0,n.jsx)(i.code,{children:"RowData"})," format directly, enabling zero-copy operations throughout the pipeline. This automatic change (no configuration required) delivers 2-3x improvement in write and read performance on average compared to Hudi 1.0."]}),"\n",(0,n.jsx)(i.p,{children:(0,n.jsx)(i.img,{alt:"flink throughput chart",src:t(38326).A+"",width:"960",height:"540"})}),"\n",(0,n.jsx)(i.p,{children:"The above shows a benchmark that inserted 500 million records with a schema of 1 STRING and 10 BIGINT fields: Hudi 1.1 achieved 235.3k records per second and Hudi 1.0 67k records per second\u2014over 3 times higher throughput."}),"\n",(0,n.jsx)(i.h3,{id:"buffer-sort",children:"Buffer Sort"}),"\n",(0,n.jsxs)(i.p,{children:["For append-only tables, Hudi 1.1 introduces in-memory buffer sorting that pre-sorts records before flushing to Parquet. This delivers 15-30% better compression and faster queries through better min/max filtering. You can enable this feature with ",(0,n.jsx)(i.code,{children:"write.buffer.sort.enabled=true"})," and specify sort keys via ",(0,n.jsx)(i.code,{children:"write.buffer.sort.keys"}),' (e.g., "timestamp,event_type"). You may also adjust the buffer size for sorting via ',(0,n.jsx)(i.code,{children:"write.buffer.size"})," (default 1000 records)."]}),"\n",(0,n.jsx)(i.h2,{id:"new-integration-apache-polaris-incubating",children:"New Integration: Apache Polaris (Incubating)"}),"\n",(0,n.jsxs)(i.p,{children:[(0,n.jsx)(i.a,{href:"https://polaris.apache.org/",children:"Polaris (incubating)"})," is an open-source catalog for lakehouse platforms that provides multi-engine interoperability and unified governance across diverse table formats and query engines. Its key feature is enabling data teams to use multiple engines\u2014Spark, Trino, Dremio, Flink, Presto\u2014on a single copy of data with consistent metadata, governed openly by a diverse committee including Snowflake, AWS, Google Cloud, Azure, and others to prevent vendor lock-in."]}),"\n",(0,n.jsxs)(i.p,{children:["Hudi 1.1 introduces ",(0,n.jsx)(i.a,{href:"https://hudi.apache.org/docs/catalog_polaris",children:"native integration with Polaris"})," (pending a Polaris release that includes ",(0,n.jsx)(i.a,{href:"https://github.com/apache/polaris/pull/1862",children:"this PR"}),"), allowing users to register Hudi tables in the Polaris catalog and query them from any Polaris-compatible engine, simplifying multi-engine workflows and providing centralized role-based access control that works uniformly across S3, Azure Blob Storage, and Google Cloud Storage."]}),"\n",(0,n.jsx)(i.h2,{id:"whats-nextjoin-us-in-building-the-future",children:"What\u2019s Next\u2014Join Us in Building the Future"}),"\n",(0,n.jsx)(i.p,{children:"The future of Hudi is incredibly exciting, and we're building it together with a vibrant, global community of contributors. Building on the strong foundation of 1.1, we're actively developing transformative AI/ML-focused capabilities for Hudi 1.2 and beyond\u2014unstructured data types and column groups for efficient storage of embeddings and documents, Lance, Vortex, blob-optimized Parquet support, and vector search capabilities for lakehouse tables. This is just the beginning\u2014we're reimagining what's possible in the lakehouse, from multi-format interoperability to next-generation AI/ML workloads, and we need your ideas, code, and creativity to make it happen."}),"\n",(0,n.jsxs)(i.p,{children:["Join us in building the future. Check out the ",(0,n.jsx)(i.a,{href:"https://hudi.apache.org/releases/release-1.1#release-111",children:"1.1 release notes"})," to get started, join our ",(0,n.jsx)(i.a,{href:"https://hudi.apache.org/slack/",children:"Slack space"}),", follow us on ",(0,n.jsx)(i.a,{href:"https://www.linkedin.com/company/apache-hudi",children:"LinkedIn"})," and ",(0,n.jsx)(i.a,{href:"http://x.com/apachehudi",children:"X (twitter)"}),", and subscribe (send an empty email) to the ",(0,n.jsx)(i.a,{href:"mailto:dev@hudi.apache.org",children:"mailing list"}),"\u2014let's build the next generation of Hudi together."]})]})}function h(e={}){const{wrapper:i}={...(0,r.R)(),...e.components};return i?(0,n.jsx)(i,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},59731(e,i,t){t.d(i,{A:()=>a});const a=t.p+"assets/images/3-binary-copy-d58175c8f9cbd5817d1ace637a617a18.png"},72806(e,i,t){t.d(i,{A:()=>a});const a=t.p+"assets/images/5-storage-based-lp-8528a28ca24084f9c01e550a1c312a36.png"},73239(e,i,t){t.d(i,{A:()=>a});const a=t.p+"assets/images/4-binary-copy-chart-f7366c95b72eb12b3ce39cc1b83bfcff.png"},85337(e,i,t){t.d(i,{A:()=>a});const a=t.p+"assets/images/6-spark-upsert-write-time-chart-734ca6975e38a28de049809099a99caa.png"}}]);