"use strict";(self.webpackChunkhudi=self.webpackChunkhudi||[]).push([[49655],{28453:(e,t,n)=>{n.d(t,{R:()=>d,x:()=>o});var a=n(96540);const i={},s=a.createContext(i);function d(e){const t=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:d(e.components),a.createElement(s.Provider,{value:t},e.children)}},57463:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>d,metadata:()=>a,toc:()=>r});const a=JSON.parse('{"id":"syncing_datahub","title":"DataHub","description":"DataHub is a rich metadata platform that supports features like data discovery, data","source":"@site/docs/syncing_datahub.md","sourceDirName":".","slug":"/syncing_datahub","permalink":"/docs/next/syncing_datahub","draft":false,"unlisted":false,"editUrl":"https://github.com/apache/hudi/tree/asf-site/website/docs/syncing_datahub.md","tags":[],"version":"current","frontMatter":{"title":"DataHub","keywords":["hudi","datahub","sync"]},"sidebar":"docs","previous":{"title":"AWS Glue Data Catalog","permalink":"/docs/next/syncing_aws_glue_data_catalog"},"next":{"title":"Hive Metastore","permalink":"/docs/next/syncing_metastore"}}');var i=n(74848),s=n(28453);const d={title:"DataHub",keywords:["hudi","datahub","sync"]},o=void 0,c={},r=[{value:"Configurations",id:"configurations",level:3},{value:"Example",id:"example",level:3}];function l(e){const t={a:"a",code:"code",em:"em",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.a,{href:"https://datahubproject.io/",children:"DataHub"})," is a rich metadata platform that supports features like data discovery, data\nobeservability, federated governance, etc."]}),"\n",(0,i.jsxs)(t.p,{children:["Since Hudi 0.11.0, you can now sync to a DataHub instance by setting ",(0,i.jsx)(t.code,{children:"DataHubSyncTool"})," as one of the sync tool classes\nfor ",(0,i.jsx)(t.code,{children:"HoodieStreamer"}),"."]}),"\n",(0,i.jsxs)(t.p,{children:["The target Hudi table will be sync'ed to DataHub as a ",(0,i.jsx)(t.code,{children:"Dataset"}),", which will be created with the following properties:"]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["Hudi table properties and partitioning information","\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["This includes: ",(0,i.jsx)(t.code,{children:"hudi.table.type"}),", ",(0,i.jsx)(t.code,{children:"hudi.table.version"})," and ",(0,i.jsx)(t.code,{children:"hudi.base.path"})]}),"\n",(0,i.jsxs)(t.li,{children:["As well as: ",(0,i.jsx)(t.code,{children:"hudi.partition.fields"})," (if and only if ",(0,i.jsx)(t.code,{children:"hoodie.datasource.hive_sync.partition_fields"})," is properly set\nin the Hudi Config)"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(t.li,{children:"Spark-related properties"}),"\n",(0,i.jsxs)(t.li,{children:["User-defined properties (see ",(0,i.jsx)(t.code,{children:"hoodie.meta.sync.datahub.table.properties"}),' in the "Configurations" section)']}),"\n",(0,i.jsx)(t.li,{children:"The last commit and the last commit completion timestamps"}),"\n"]}),"\n",(0,i.jsxs)(t.p,{children:["Additionally, the ",(0,i.jsx)(t.code,{children:"Dataset"})," object will include the following metadata:"]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["sub-type as ",(0,i.jsx)(t.code,{children:"Table"})]}),"\n",(0,i.jsx)(t.li,{children:"browse path"}),"\n",(0,i.jsx)(t.li,{children:"parent container"}),"\n",(0,i.jsx)(t.li,{children:"Avro schema"}),"\n",(0,i.jsxs)(t.li,{children:["optionally, attached with a ",(0,i.jsx)(t.code,{children:"Domain"})," object"]}),"\n"]}),"\n",(0,i.jsxs)(t.p,{children:["Also, the parent database will be sync'ed to DataHub as a ",(0,i.jsx)(t.code,{children:"Container"}),", including the following metadata:"]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["sub-type as ",(0,i.jsx)(t.code,{children:"Database"})]}),"\n",(0,i.jsx)(t.li,{children:"browse paths"}),"\n",(0,i.jsxs)(t.li,{children:["optionally, attached with a ",(0,i.jsx)(t.code,{children:"Domain"})," object"]}),"\n"]}),"\n",(0,i.jsx)(t.h3,{id:"configurations",children:"Configurations"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.code,{children:"DataHubSyncTool"})," makes use of DataHub's Java Emitter to send the metadata via HTTP REST APIs. It is required to\nset ",(0,i.jsx)(t.code,{children:"hoodie.meta.sync.datahub.emitter.server"})," to the URL of the DataHub instance for sync."]}),"\n",(0,i.jsxs)(t.p,{children:["If needs auth token, set ",(0,i.jsx)(t.code,{children:"hoodie.meta.sync.datahub.emitter.token"}),"."]}),"\n",(0,i.jsxs)(t.p,{children:["If needs customized creation of the emitter object,\nimplement ",(0,i.jsx)(t.code,{children:"org.apache.hudi.sync.datahub.config.DataHubEmitterSupplier"})," and supply the implementation's FQCN\nto ",(0,i.jsx)(t.code,{children:"hoodie.meta.sync.datahub.emitter.supplier.class"}),"."]}),"\n",(0,i.jsxs)(t.p,{children:["By default, the sync config's database name and table name will be used to make the target ",(0,i.jsx)(t.code,{children:"Dataset"}),"'s URN.\nSubclass ",(0,i.jsx)(t.code,{children:"HoodieDataHubDatasetIdentifier"})," and set it to ",(0,i.jsx)(t.code,{children:"hoodie.meta.sync.datahub.dataset.identifier.class"})," to customize\nthe URN creation."]}),"\n",(0,i.jsxs)(t.p,{children:["Optionally, sync'ed ",(0,i.jsx)(t.code,{children:"Dataset"})," and ",(0,i.jsx)(t.code,{children:"Container"})," objects can be attached with a ",(0,i.jsx)(t.code,{children:"Domain"})," object. To do this, set\n",(0,i.jsx)(t.code,{children:"hoodie.meta.sync.datahub.domain.name"})," to a valid ",(0,i.jsx)(t.code,{children:"Domain"})," URN. Also, sync'ed ",(0,i.jsx)(t.code,{children:"Dataset"})," can be attached with\nuser defined properties. To do this, set ",(0,i.jsx)(t.code,{children:"hoodie.meta.sync.datahub.table.properties"})," to a comma-separated key-value\nstring (",(0,i.jsx)(t.em,{children:"eg"})," ",(0,i.jsx)(t.code,{children:"key1=val1,key2=val2"}),")."]}),"\n",(0,i.jsx)(t.h3,{id:"example",children:"Example"}),"\n",(0,i.jsxs)(t.p,{children:["The following shows an example configuration to run ",(0,i.jsx)(t.code,{children:"HoodieStreamer"})," with ",(0,i.jsx)(t.code,{children:"DataHubSyncTool"}),"."]}),"\n",(0,i.jsxs)(t.p,{children:["In addition to ",(0,i.jsx)(t.code,{children:"hudi-utilities-slim-bundle"})," that contains ",(0,i.jsx)(t.code,{children:"HoodieStreamer"}),", you also add ",(0,i.jsx)(t.code,{children:"hudi-datahub-sync-bundle"})," to\nthe classpath."]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-shell",children:"spark-submit --master yarn \\\n--packages org.apache.hudi:hudi-utilities-slim-bundle_2.12:1.0.1,org.apache.hudi:hudi-spark3.5-bundle_2.12:1.0.1 \\\n--jars /opt/hudi-datahub-sync-bundle-1.0.1.jar \\\n--class org.apache.hudi.utilities.streamer.HoodieStreamer \\\n/opt/hudi-utilities-slim-bundle_2.12-1.0.1.jar \\\n--target-table mytable \\\n# ... other HoodieStreamer's configs\n--enable-sync \\\n--sync-tool-classes org.apache.hudi.sync.datahub.DataHubSyncTool \\\n--hoodie-conf hoodie.meta.sync.datahub.emitter.server=http://url-to-datahub-instance:8080 \\\n--hoodie-conf hoodie.datasource.hive_sync.database=mydb \\\n--hoodie-conf hoodie.datasource.hive_sync.table=mytable \\\n"})})]})}function h(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}}}]);