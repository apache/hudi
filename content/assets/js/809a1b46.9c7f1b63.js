"use strict";(self.webpackChunkhudi=self.webpackChunkhudi||[]).push([[15186],{73809:(e,i,t)=>{t.r(i),t.d(i,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>n,toc:()=>d});const n=JSON.parse('{"id":"release-0.14.0","title":"Release 0.14.0","description":"Release 0.14.0 (docs)","source":"@site/releases/release-0.14.0.md","sourceDirName":".","slug":"/release-0.14.0","permalink":"/releases/release-0.14.0","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Release 0.14.0","sidebar_position":5,"layout":"releases","toc":true},"sidebar":"releases","previous":{"title":"Release 1.0.0-beta1","permalink":"/releases/release-1.0.0-beta1"},"next":{"title":"Release 0.13.1","permalink":"/releases/release-0.13.1"}}');var r=t(74848),a=t(28453);t(11470),t(19365);const s={title:"Release 0.14.0",sidebar_position:5,layout:"releases",toc:!0},o=void 0,l={},d=[{value:"Release 0.14.0 (docs)",id:"release-0140-docs",level:2},{value:"Migration Guide",id:"migration-guide",level:2},{value:"Bundle Updates",id:"bundle-updates",level:3},{value:"New Spark Bundles",id:"new-spark-bundles",level:4},{value:"Breaking Changes",id:"breaking-changes",level:3},{value:"INSERT INTO behavior with Spark SQL",id:"insert-into-behavior-with-spark-sql",level:4},{value:"Behavior changes",id:"behavior-changes",level:3},{value:"Simplified duplicates handling with Inserts in Spark SQL",id:"simplified-duplicates-handling-with-inserts-in-spark-sql",level:4},{value:"Compaction with MOR table",id:"compaction-with-mor-table",level:4},{value:"HoodieDeltaStreamer renamed to HoodieStreamer",id:"hoodiedeltastreamer-renamed-to-hoodiestreamer",level:4},{value:"MERGE INTO JOIN condition",id:"merge-into-join-condition",level:4},{value:"Release Highlights",id:"release-highlights",level:2},{value:"Record Level Index",id:"record-level-index",level:3},{value:"Support for Hudi tables with Autogenerated keys",id:"support-for-hudi-tables-with-autogenerated-keys",level:3},{value:"Spark 3.4 version support",id:"spark-34-version-support",level:3},{value:"Query side improvements:",id:"query-side-improvements",level:3},{value:"Metadata table support with Athena",id:"metadata-table-support-with-athena",level:4},{value:"Leverage Parquet bloom filters w/ read queries",id:"leverage-parquet-bloom-filters-w-read-queries",level:4},{value:"Incremental queries with multi-writers",id:"incremental-queries-with-multi-writers",level:4},{value:"Timestamp support with Hive 3.x",id:"timestamp-support-with-hive-3x",level:4},{value:"Google BigQuery sync enhancements",id:"google-bigquery-sync-enhancements",level:4},{value:"Spark read side improvements",id:"spark-read-side-improvements",level:3},{value:"Snapshot read support for MOR Bootstrap tables",id:"snapshot-read-support-for-mor-bootstrap-tables",level:4},{value:"Table-valued function named hudi_table_changes designed for incremental reading through Spark SQL",id:"table-valued-function-named-hudi_table_changes-designed-for-incremental-reading-through-spark-sql",level:4},{value:"New MOR file format reader in Spark:",id:"new-mor-file-format-reader-in-spark",level:4},{value:"Spark write side improvements",id:"spark-write-side-improvements",level:3},{value:"Bulk_Insert and row writer enhancements",id:"bulk_insert-and-row-writer-enhancements",level:4},{value:"Hoodie Streamer enhancements",id:"hoodie-streamer-enhancements",level:3},{value:"Dynamic configuration updates",id:"dynamic-configuration-updates",level:4},{value:"SQL File based source for HoodieStreamer",id:"sql-file-based-source-for-hoodiestreamer",level:4},{value:"Flink Enhancements",id:"flink-enhancements",level:3},{value:"Consistent hashing index support",id:"consistent-hashing-index-support",level:4},{value:"Dynamic partition pruning for streaming read",id:"dynamic-partition-pruning-for-streaming-read",level:4},{value:"Simple bucket index table query speed up (with index fields)",id:"simple-bucket-index-table-query-speed-up-with-index-fields",level:4},{value:"Flink 1.17 support",id:"flink-117-support",level:4},{value:"Update deletes statement for Flink",id:"update-deletes-statement-for-flink",level:4},{value:"Java Enhancements",id:"java-enhancements",level:3},{value:"Known Regressions",id:"known-regressions",level:2},{value:"Raw Release Notes",id:"raw-release-notes",level:2}];function c(e){const i={a:"a",admonition:"admonition",br:"br",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(i.h2,{id:"release-0140-docs",children:[(0,r.jsx)(i.a,{href:"https://github.com/apache/hudi/releases/tag/release-0.14.0",children:"Release 0.14.0"})," (",(0,r.jsx)(i.a,{href:"/docs/quick-start-guide",children:"docs"}),")"]}),"\n",(0,r.jsxs)(i.p,{children:["Apache Hudi 0.14.0 marks a significant milestone with a range of new functionalities and enhancements.\nThese include the introduction of Record Level Index, automatic generation of record keys, the ",(0,r.jsx)(i.code,{children:"hudi_table_changes"}),"\nfunction for incremental reads, and more. Notably, this release also incorporates support for Spark 3.4. On the Flink\nfront, version 0.14.0 brings several exciting features such as consistent hashing index support, Flink 1.17 support, and\nUpdate and Delete statement support. Additionally, this release upgrades the Hudi table version, prompting users to consult\nthe Migration Guide provided below. We encourage users to review the ",(0,r.jsx)(i.a,{href:"#release-highlights",children:"release highlights"}),",\n",(0,r.jsx)(i.a,{href:"#breaking-changes",children:"breaking changes"}),", and ",(0,r.jsx)(i.a,{href:"#behavior-changes",children:"behavior changes"})," before\nadopting the 0.14.0 release."]}),"\n",(0,r.jsx)(i.h2,{id:"migration-guide",children:"Migration Guide"}),"\n",(0,r.jsxs)(i.p,{children:['In version 0.14.0, we\'ve made changes such as the removal of compaction plans from the ".aux" folder and the introduction\nof a new log block version. As part of this release, the table version is updated to version ',(0,r.jsx)(i.code,{children:"6"}),". When running a Hudi job\nwith version 0.14.0 on a table with an older table version, an automatic upgrade process is triggered to bring the table\nup to version ",(0,r.jsx)(i.code,{children:"6"}),". This upgrade is a one-time occurrence for each Hudi table, as the ",(0,r.jsx)(i.code,{children:"hoodie.table.version"})," is updated in\nthe property file upon completion of the upgrade. Additionally, a command-line tool for downgrading has been included,\nallowing users to move from table version ",(0,r.jsx)(i.code,{children:"6"})," to ",(0,r.jsx)(i.code,{children:"5"}),", or revert from Hudi 0.14.0 to a version prior to 0.14.0. To use this\ntool, execute it from a 0.14.0 environment. For more details, refer to the\n",(0,r.jsx)(i.a,{href:"/docs/cli/#upgrade-and-downgrade-table",children:"hudi-cli"}),"."]}),"\n",(0,r.jsx)(i.admonition,{type:"caution",children:(0,r.jsx)(i.p,{children:"If migrating from an older release (pre 0.14.0), please also check the upgrade instructions from each older release in\nsequence."})}),"\n",(0,r.jsx)(i.h3,{id:"bundle-updates",children:"Bundle Updates"}),"\n",(0,r.jsx)(i.h4,{id:"new-spark-bundles",children:"New Spark Bundles"}),"\n",(0,r.jsxs)(i.p,{children:["In this release, we've expanded our support to include bundles for both Spark 3.4\n(",(0,r.jsx)(i.a,{href:"https://mvnrepository.com/artifact/org.apache.hudi/hudi-spark3.4-bundle_2.12",children:"hudi-spark3.4-bundle_2.12"}),")\nand Spark 3.0 (",(0,r.jsx)(i.a,{href:"https://mvnrepository.com/artifact/org.apache.hudi/hudi-spark3.0-bundle_2.12",children:"hudi-spark3.0-bundle_2.12"}),").\nPlease note that, the support for Spark 3.0 had been discontinued after Hudi version 0.10.1, but due to strong community\ninterest, it has been reinstated in this release."]}),"\n",(0,r.jsx)(i.h3,{id:"breaking-changes",children:"Breaking Changes"}),"\n",(0,r.jsx)(i.h4,{id:"insert-into-behavior-with-spark-sql",children:"INSERT INTO behavior with Spark SQL"}),"\n",(0,r.jsxs)(i.p,{children:["Before version 0.14.0, data ingested through ",(0,r.jsx)(i.code,{children:"INSERT INTO"})," in Spark SQL followed the upsert flow, where multiple versions\nof records would be merged into one version. However, starting from 0.14.0, we've altered the default behavior of\n",(0,r.jsx)(i.code,{children:"INSERT INTO"})," to utilize the ",(0,r.jsx)(i.code,{children:"insert"})," flow internally. This change significantly enhances write performance as it\nbypasses index lookups."]}),"\n",(0,r.jsxs)(i.p,{children:["If a table is created with a ",(0,r.jsx)(i.em,{children:"preCombine"})," key, the default operation for ",(0,r.jsx)(i.code,{children:"INSERT INTO"})," remains as ",(0,r.jsx)(i.code,{children:"upsert"}),". Conversely,\nif no ",(0,r.jsx)(i.em,{children:"preCombine"})," key is set, the underlying write operation for ",(0,r.jsx)(i.code,{children:"INSERT INTO"})," defaults to ",(0,r.jsx)(i.code,{children:"insert"}),". Users have the\nflexibility to override this behavior by explicitly setting values for the config\n",(0,r.jsx)(i.a,{href:"https://hudi.apache.org/docs/configurations#hoodiesparksqlinsertintooperation",children:(0,r.jsx)(i.code,{children:"hoodie.spark.sql.insert.into.operation"})}),"\nas per their requirements. Possible values for this config include ",(0,r.jsx)(i.code,{children:"insert"}),", ",(0,r.jsx)(i.code,{children:"bulk_insert"}),", and ",(0,r.jsx)(i.code,{children:"upsert"}),"."]}),"\n",(0,r.jsxs)(i.p,{children:["Additionally, in version 0.14.0, we have ",(0,r.jsx)(i.strong,{children:"deprecated"})," two related older configs:"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:(0,r.jsx)(i.code,{children:"hoodie.sql.insert.mode"})}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"hoodie.sql.bulk.insert.enable"}),"."]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"behavior-changes",children:"Behavior changes"}),"\n",(0,r.jsx)(i.h4,{id:"simplified-duplicates-handling-with-inserts-in-spark-sql",children:"Simplified duplicates handling with Inserts in Spark SQL"}),"\n",(0,r.jsxs)(i.p,{children:["In cases where the operation type is configured as ",(0,r.jsx)(i.code,{children:"insert"})," for the Spark SQL ",(0,r.jsx)(i.code,{children:"INSERT INTO"})," flow, users now have the\noption to enforce a duplicate policy using the configuration setting\n",(0,r.jsx)(i.a,{href:"https://hudi.apache.org/docs/configurations#hoodiedatasourceinsertduppolicy",children:(0,r.jsx)(i.code,{children:"hoodie.datasource.insert.dup.policy"})}),".\nThis policy determines the action taken when incoming records being ingested already exist in storage. The available\nvalues for this configuration are as follows:"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"none"}),": No specific action is taken, allowing duplicates to exist in the Hudi table if the incoming records contain duplicates."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"drop"}),": Matching records from the incoming writes will be dropped, and the remaining ones will be ingested."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"fail"}),": The write operation will fail if the same records are re-ingested. In essence, a given record, as determined\nby the key generation policy, can only be ingested once into the target table."]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:["With this addition, an older related configuration setting,\n",(0,r.jsx)(i.a,{href:"https://hudi.apache.org/docs/configurations#hoodiedatasourcewriteinsertdropduplicates",children:(0,r.jsx)(i.code,{children:"hoodie.datasource.write.insert.drop.duplicates"})}),",\nwill be deprecated. The newer configuration will take precedence over the old one when both are specified. If no specific\nconfigurations are provided, the default value for the newer configuration will be assumed. Users are strongly encouraged\nto migrate to the use of these newer configurations when using Spark SQL."]}),"\n",(0,r.jsx)(i.admonition,{type:"caution",children:(0,r.jsx)(i.p,{children:"This is only applicable to Spark SQL writing."})}),"\n",(0,r.jsx)(i.h4,{id:"compaction-with-mor-table",children:"Compaction with MOR table"}),"\n",(0,r.jsxs)(i.p,{children:["For Spark batch writers (both the Spark datasource and Spark SQL), compaction is automatically enabled by default for\nMOR (Merge On Read) tables, unless users explicitly override this behavior. Users have the option to disable compaction\nexplicitly by setting ",(0,r.jsx)(i.a,{href:"https://hudi.apache.org/docs/configurations#hoodiecompactinline",children:(0,r.jsx)(i.code,{children:"hoodie.compact.inline"})})," to false.\nIn case users do not override this configuration, compaction may be triggered for MOR tables approximately once every\n5 delta commits (the default value for\n",(0,r.jsx)(i.a,{href:"https://hudi.apache.org/docs/configurations#hoodiecompactinlinemaxdeltacommits",children:(0,r.jsx)(i.code,{children:"hoodie.compact.inline.max.delta.commits"})}),")."]}),"\n",(0,r.jsx)(i.h4,{id:"hoodiedeltastreamer-renamed-to-hoodiestreamer",children:"HoodieDeltaStreamer renamed to HoodieStreamer"}),"\n",(0,r.jsxs)(i.p,{children:["Starting from version 0.14.0, we have renamed ",(0,r.jsx)(i.a,{href:"https://github.com/apache/hudi/blob/84a80e21b5f0cdc1f4a33957293272431b221aa9/hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java",children:"HoodieDeltaStreamer"}),"\nto ",(0,r.jsx)(i.a,{href:"https://github.com/apache/hudi/blob/84a80e21b5f0cdc1f4a33957293272431b221aa9/hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/HoodieStreamer.java",children:"HoodieStreamer"}),".\nWe have ensured backward compatibility so that existing user jobs remain unaffected. However, in upcoming\nreleases, support for Deltastreamer might be discontinued. Hence, we strongly advise users to transition to using\nHoodieStreamer instead."]}),"\n",(0,r.jsx)(i.h4,{id:"merge-into-join-condition",children:"MERGE INTO JOIN condition"}),"\n",(0,r.jsxs)(i.p,{children:["Starting from version 0.14.0, Hudi has the capability to automatically generate primary record keys when users do not\nprovide explicit specifications. This enhancement enables the ",(0,r.jsx)(i.code,{children:"MERGE INTO JOIN"})," clause to reference any data column for\nthe join condition in Hudi tables where the primary keys are generated by Hudi itself. However, in cases where users\nconfigure the primary record key, the join condition still expects the primary key fields as specified by the user."]}),"\n",(0,r.jsx)(i.h2,{id:"release-highlights",children:"Release Highlights"}),"\n",(0,r.jsx)(i.h3,{id:"record-level-index",children:"Record Level Index"}),"\n",(0,r.jsxs)(i.p,{children:["Hudi version 0.14.0, introduces a new index implementation -",(0,r.jsx)(i.br,{}),"\n",(0,r.jsx)(i.a,{href:"https://github.com/apache/hudi/blob/master/rfc/rfc-8/rfc-8.md#rfc-8-metadata-based-record-index",children:"Record Level Index"}),".\nThe Record level Index significantly enhances write performance for large tables by efficiently storing per-record\nlocations and enabling swift retrieval during index lookup operations. It can effectively replace other\n",(0,r.jsx)(i.a,{href:"https://hudi.apache.org/docs/next/indexing#global-and-non-global-indexes",children:"Global indices"})," like Global_bloom,\nGlobal_Simple, or Hbase, commonly used in Hudi."]}),"\n",(0,r.jsx)(i.p,{children:"Bloom and Simple Indexes exhibit slower performance for large datasets due to the high costs associated with gathering\nindex data from various data files during lookup. Moreover, these indexes do not preserve a one-to-one record-key to\nrecord file path mapping; instead, they deduce the mapping through an optimized search at lookup time. The per-file\noverhead required by these indexes makes them less effective for datasets with a larger number of files or records."}),"\n",(0,r.jsx)(i.p,{children:"On the other hand, the Hbase Index saves a one-to-one mapping for each record key, resulting in fast performance that\nscales with the dataset size. However, it necessitates a separate HBase cluster for maintenance, which is operationally\nchallenging and resource-intensive, requiring specialized expertise."}),"\n",(0,r.jsx)(i.p,{children:"The Record Index combines the speed and scalability of the HBase Index without its limitations and overhead. Being a\npart of the HUDI Metadata Table, any future performance enhancements in writes and queries will automatically translate\ninto improved performance for the Record Index. Adopting the Record Level Index has the potential to boost index lookup\nperformance by 4 to 10 times, depending on the workload, even for extremely large-scale datasets (e.g., 1TB)."}),"\n",(0,r.jsx)(i.p,{children:"With the Record Level Index, significant performance improvements can be observed for large datasets, as latency is\ndirectly proportional to the amount of data being ingested. This is in contrast to other Global indices where index\nlookup time increases linearly with the table size. The Record Level Index is specifically designed to efficiently\nhandle lookups for such large-scale data without a linear increase in lookup times as the table size grows."}),"\n",(0,r.jsx)(i.p,{children:"To harness the benefits of this lightning-fast index, users need to enable two configurations:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.a,{href:"https://hudi.apache.org/docs/next/configurations#hoodiemetadatarecordindexenable",children:(0,r.jsx)(i.code,{children:"hoodie.metadata.record.index.enable"})})," must be enabled to write the Record Level Index to the metadata table."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"hoodie.index.type"})," needs to be set to ",(0,r.jsx)(i.code,{children:"RECORD_INDEX"})," for the index lookup to utilize the Record Level Index."]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"support-for-hudi-tables-with-autogenerated-keys",children:"Support for Hudi tables with Autogenerated keys"}),"\n",(0,r.jsxs)(i.p,{children:["Since the initial official version of Hudi, the primary key was a mandatory field that users needed to configure for any\nHudi table. Starting 0.14.0, we are relaxing this constraint. This enhancement addresses a longstanding need within the\ncommunity, where certain use-cases didn't naturally possess an intrinsic primary key. Version 0.14.0 now offers the\nflexibility for users to create a Hudi table without the need to explicitly configure a primary key (by omitting the\nconfiguration setting -\n",(0,r.jsx)(i.a,{href:"https://hudi.apache.org/docs/configurations#hoodiedatasourcewriterecordkeyfield",children:(0,r.jsx)(i.code,{children:"hoodie.datasource.write.recordkey.field"})}),").\nHudi will ",(0,r.jsx)(i.strong,{children:"automatically generate the primary keys"})," in such cases. This feature is applicable only for new tables and\ncannot be altered for existing ones."]}),"\n",(0,r.jsxs)(i.p,{children:["This functionality is available in all spark writers with certain limitations. For append only type of use cases, Inserts and\nbulk_inserts are allowed with all four writers - Spark Datasource, Spark SQL, Spark Streaming, Hoodie Streamer. Updates and\nDeletes are supported only using spark-sql ",(0,r.jsx)(i.code,{children:"MERGE INTO"})," , ",(0,r.jsx)(i.code,{children:"UPDATE"})," and ",(0,r.jsx)(i.code,{children:"DELETE"})," statements. With Spark Datasource, ",(0,r.jsx)(i.code,{children:"UPDATE"}),"\nand ",(0,r.jsx)(i.code,{children:"DELETE"})," are supported only when the source dataframe contains Hudi's meta fields. Please check out our\n",(0,r.jsx)(i.a,{href:"https://hudi.apache.org/docs/quick-start-guide",children:"quick start guide"})," for code snippets on Hudi table CRUD operations where\nkeys are autogenerated."]}),"\n",(0,r.jsx)(i.h3,{id:"spark-34-version-support",children:"Spark 3.4 version support"}),"\n",(0,r.jsxs)(i.p,{children:["Spark 3.4 support is added; users who are on Spark 3.4 can use\n",(0,r.jsx)(i.a,{href:"https://mvnrepository.com/artifact/org.apache.hudi/hudi-spark3.4-bundle",children:"hudi-spark3.4-bundle"}),". Spark 3.2, Spark 3.1,\nSpark3.0 and Spark 2.4 will continue to be supported. Please check the migration guide for bundle updates. To quickly get\nstarted with Hudi and Spark 3.4, you can explore our ",(0,r.jsx)(i.a,{href:"https://hudi.apache.org/docs/quick-start-guide",children:"quick start guide"}),"."]}),"\n",(0,r.jsx)(i.h3,{id:"query-side-improvements",children:"Query side improvements:"}),"\n",(0,r.jsx)(i.h4,{id:"metadata-table-support-with-athena",children:"Metadata table support with Athena"}),"\n",(0,r.jsxs)(i.p,{children:["Users now have the ability to utilize Hudi\u2019s ",(0,r.jsx)(i.a,{href:"https://hudi.apache.org/docs/metadata/",children:"Metadata table"}),' seamlessly with Athena.\nThe file listing index removes the need for recursive file system calls like "list files" by retrieving information\nfrom an index that maintains a mapping of partitions to files. This approach proves to be highly efficient, particularly\nwhen dealing with extensive datasets. With Hudi 0.14.0, users can activate file listing based on the metadata table when\nperforming Glue catalog synchronization for their Hudi tables. To enable this functionality, users can configure\n',(0,r.jsx)(i.code,{children:"hoodie.datasource.meta.sync.glue.metadata_file_listing"})," and set it to true during the Glue sync process."]}),"\n",(0,r.jsx)(i.h4,{id:"leverage-parquet-bloom-filters-w-read-queries",children:"Leverage Parquet bloom filters w/ read queries"}),"\n",(0,r.jsxs)(i.p,{children:["In Hudi 0.14.0, users can now utilize the native\n",(0,r.jsx)(i.a,{href:"https://github.com/apache/parquet-format/blob/1603152f8991809e8ad29659dffa224b4284f31b/BloomFilter.md",children:"Parquet bloom filters"}),",\nprovided their compute engine supports Apache Parquet 1.12.0 or higher. This support covers both the writing and reading\nof datasets. Hudi facilitates the use of native Parquet bloom filters through Hadoop configuration. Users are required\nto set a Hadoop configuration with a specific key representing the column for which the bloom filter is to be applied.\nFor example, ",(0,r.jsx)(i.code,{children:"parquet.bloom.filter.enabled#rider=true"})," creates a bloom filter for the rider column. Whenever a query\ninvolves a predicate on the rider column, the bloom filter comes into play, enhancing read performance."]}),"\n",(0,r.jsx)(i.h4,{id:"incremental-queries-with-multi-writers",children:"Incremental queries with multi-writers"}),"\n",(0,r.jsxs)(i.p,{children:["In multi-writer scenarios, there can be instances of gaps in the timeline (requests or inflight instants that are not\nthe latest instant) due to concurrent writing activities. These gaps may result in inconsistent outcomes when\nperforming incremental queries. To address this issue, Hudi 0.14.0 introduces a new configuration setting,\n",(0,r.jsx)(i.a,{href:"https://hudi.apache.org/docs/configurations#hoodiereadtimelineholesresolutionpolicy",children:(0,r.jsx)(i.code,{children:"hoodie.read.timeline.holes.resolution.policy"})}),",\nspecifically designed for handling these inconsistencies in incremental queries. The configuration provides three possible policies:"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"FAIL"}),": This serves as the default policy and throws an exception when such timeline gaps are identified during an incremental query."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"BLOCK"}),": In this policy, the results of an incremental query are limited to the time range between the holes in the\ntimeline. For instance, if a gap is detected at instant t1 within the incremental query range from t0 to t2, the\nquery will only display results between t0 and t1 without failing."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"USE_TRANSITION_TIME"}),": This policy is experimental and involves using the state transition time, which is based on the\nfile modification time of commit metadata files in the timeline, during the incremental query."]}),"\n"]}),"\n",(0,r.jsx)(i.h4,{id:"timestamp-support-with-hive-3x",children:"Timestamp support with Hive 3.x"}),"\n",(0,r.jsxs)(i.p,{children:["For quite some time, Hudi users encountered ",(0,r.jsx)(i.a,{href:"https://issues.apache.org/jira/browse/HUDI-83",children:"challenges"})," regarding reading Timestamp type columns written by Spark and\nsubsequently attempting to read them with Hive 3.x. While in Hudi 0.13.x, we introduced a\n",(0,r.jsx)(i.a,{href:"https://github.com/apache/hudi/commit/cd314b8cfa58c32f731f7da2aa6377a09df4c6f9#diff-cff4dfc264f7abcac63a5ba5db55b38115177fe279ab35807d345c2b8872475e",children:"workaround"}),"\nto mitigate this issue, version 0.14.0 now ensures full compatibility of HiveAvroSerializer with Hive 3.x to resolve this."]}),"\n",(0,r.jsx)(i.h4,{id:"google-bigquery-sync-enhancements",children:"Google BigQuery sync enhancements"}),"\n",(0,r.jsxs)(i.p,{children:["With 0.14.0, the ",(0,r.jsx)(i.a,{href:"https://hudi.apache.org/docs/gcp_bigquery",children:"BigQuerySyncTool"})," supports syncing table to BigQuery\nusing ",(0,r.jsx)(i.a,{href:"https://cloud.google.com/blog/products/data-analytics/bigquery-manifest-file-support-for-open-table-format-queries",children:"manifests"}),".\nThis is expected to have better query performance compared to legacy way. Schema evolution is supported with the manifest approach.\nPartition column no longer needs to be dropped from the files due to new schema handling improvements. To enable this\nfeature, users can set\n",(0,r.jsx)(i.a,{href:"https://hudi.apache.org/docs/configurations#hoodiegcpbigquerysyncuse_bq_manifest_file",children:(0,r.jsx)(i.code,{children:"hoodie.gcp.bigquery.sync.use_bq_manifest_file"})}),"\nto true."]}),"\n",(0,r.jsx)(i.h3,{id:"spark-read-side-improvements",children:"Spark read side improvements"}),"\n",(0,r.jsx)(i.h4,{id:"snapshot-read-support-for-mor-bootstrap-tables",children:"Snapshot read support for MOR Bootstrap tables"}),"\n",(0,r.jsxs)(i.p,{children:["With 0.14.0, MOR snapshot read support is added for Bootstrapped tables. The default behavior has been changed in several\nways to match the behavior of non-bootstrapped MOR tables. Snapshot reads will now be the default reading mode. Use\n",(0,r.jsx)(i.code,{children:"hoodie.datasource.query.type=read_optimized"})," for read optimized queries which was previously the default behavior.\nHive sync for such tables will result in both _ro and _rt suffixed to the table name to signify read optimized and snapshot\nreading respectively."]}),"\n",(0,r.jsx)(i.h4,{id:"table-valued-function-named-hudi_table_changes-designed-for-incremental-reading-through-spark-sql",children:"Table-valued function named hudi_table_changes designed for incremental reading through Spark SQL"}),"\n",(0,r.jsxs)(i.p,{children:["Hudi offers the functionality to fetch a stream of records changed since a specified commit timestamp through the ",(0,r.jsx)(i.a,{href:"https://hudi.apache.org/docs/quick-start-guide#incremental-query",children:"incremental query"})," type. With the release of Hudi 0.14.0, we've introduced a more straightforward method to access the most recent state or change streams of Hudi datasets. This is achieved using a table-valued function named ",(0,r.jsx)(i.code,{children:"hudi_table_changes"})," in Spark SQL. Here's the syntax and several examples of how to utilize this function:"]}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-text",children:"SYNTAX\nhudi_table_changes(table, queryType, beginTime [, endTime]);\n-- table: table identifier, example: db.tableName, tableName, or path for the table, example: hdfs://path/to/hudiTable.\n-- queryType: incremental query mode, valid values: latest_state, cdc\n(for cdc query, first enable cdc for the table by setting cdc.enabled=true),\n-- beginTime: instantTime to begin query from, example: earliest, 202305150000,\n-- endTime: optional instantTime to end query at, example: 202305160000,\n\nEXAMPLES\n-- incrementally query data by table name\n-- start from earliest available commit, end at latest available commit.\nSELECT * FROM hudi_table_changes('db.table', 'latest_state', 'earliest');\n\n-- start from earliest, end at 202305160000.\nSELECT * FROM hudi_table_changes('table', 'latest_state', 'earliest', '202305160000');\n\n-- incrementally query data by path\n-- start from earliest available commit, end at latest available commit.\nSELECT * FROM hudi_table_changes('path/to/table', 'cdc', 'earliest');\n"})}),"\n",(0,r.jsxs)(i.p,{children:["Checkout the ",(0,r.jsx)(i.a,{href:"/docs/quick-start-guide#incremental-query",children:"quickstart"})," for more examples."]}),"\n",(0,r.jsx)(i.h4,{id:"new-mor-file-format-reader-in-spark",children:"New MOR file format reader in Spark:"}),"\n",(0,r.jsxs)(i.p,{children:["Based on a proposal from ",(0,r.jsx)(i.a,{href:"https://github.com/apache/hudi/pull/9235",children:"RFC-72"})," aimed at redesigning Hudi-Spark integration,\nwe are introducing an experimental file format reader for MOR (Merge On Read) tables. This reader is expected to\nsignificantly reduce read latencies by 20 to 40% when compared to the older file format, particularly for snapshot and\nbootstrap queries. The goal is to bring the latencies closer to those of the COW (Copy On Write) file format. To utilize\nthis new file format, users need to set ",(0,r.jsx)(i.code,{children:"hoodie.datasource.read.use.new.parquet.file.format=true"}),". It's important to note\nthat this feature is still experimental and comes with a few limitations. For more details and if you're interested in\ncontributing, please refer to ",(0,r.jsx)(i.a,{href:"https://issues.apache.org/jira/browse/HUDI-6568",children:"HUDI-6568"}),"."]}),"\n",(0,r.jsx)(i.h3,{id:"spark-write-side-improvements",children:"Spark write side improvements"}),"\n",(0,r.jsx)(i.h4,{id:"bulk_insert-and-row-writer-enhancements",children:"Bulk_Insert and row writer enhancements"}),"\n",(0,r.jsxs)(i.p,{children:["The 0.14.0 release provides support for using bulk insert operation while performing SQL operations like ",(0,r.jsx)(i.code,{children:"INSERT OVERWRITE TABLE"}),"\nand ",(0,r.jsx)(i.code,{children:"INSERT OVERWRITE PARTITION"}),". To enable bulk insert, set config\n",(0,r.jsx)(i.a,{href:"https://hudi.apache.org/docs/configurations#hoodiesparksqlinsertintooperation",children:(0,r.jsx)(i.code,{children:"hoodie.spark.sql.insert.into.operation"})}),"\nto value ",(0,r.jsx)(i.code,{children:"bulk_insert"}),". Bulk insert has better write performance compared to insert operation. Row writer support is\nalso added for Simple bucket index."]}),"\n",(0,r.jsx)(i.h3,{id:"hoodie-streamer-enhancements",children:"Hoodie Streamer enhancements"}),"\n",(0,r.jsx)(i.h4,{id:"dynamic-configuration-updates",children:"Dynamic configuration updates"}),"\n",(0,r.jsxs)(i.p,{children:["When Hoodie Streamer is run in continuous mode, the properties can be refreshed/updated before each sync calls.\nInterested users can implement ",(0,r.jsx)(i.code,{children:"org.apache.hudi.utilities.deltastreamer.ConfigurationHotUpdateStrategy"})," to leverage this."]}),"\n",(0,r.jsx)(i.h4,{id:"sql-file-based-source-for-hoodiestreamer",children:"SQL File based source for HoodieStreamer"}),"\n",(0,r.jsxs)(i.p,{children:["A new source - ",(0,r.jsx)(i.a,{href:"https://github.com/apache/hudi/blob/30146d61f5544f06e2100234b9bf9c5e4bc2a97f/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/SqlFileBasedSource.java",children:"SqlFileBasedSource"}),",\nhas been added to HoodieStreamer designed to facilitate one-time backfill scenarios."]}),"\n",(0,r.jsx)(i.h3,{id:"flink-enhancements",children:"Flink Enhancements"}),"\n",(0,r.jsx)(i.p,{children:"Below are the Flink Engine based enhancements in the 0.14.0 release."}),"\n",(0,r.jsx)(i.h4,{id:"consistent-hashing-index-support",children:"Consistent hashing index support"}),"\n",(0,r.jsxs)(i.p,{children:["In comparison to the static hashing index (BUCKET index), the consistent hashing index offers dynamic scalability of\ndata buckets for the writer. To utilize this feature, configure the option ",(0,r.jsx)(i.code,{children:"index.type"})," as ",(0,r.jsx)(i.code,{children:"BUCKET"})," and set\n",(0,r.jsx)(i.code,{children:"hoodie.index.bucket.engine"})," to ",(0,r.jsx)(i.code,{children:"CONSISTENT_HASHING"}),"."]}),"\n",(0,r.jsx)(i.p,{children:"When enabling the consistent hashing index, it's important to activate clustering scheduling within the writer.\nThe clustering plan should be executed through an offline Spark job. During this process, the writer will perform dual writes\nfor both the old and new data buckets while the clustering is pending. Although the dual write does not impact correctness,\nit is strongly recommended to execute clustering as quickly as possible."}),"\n",(0,r.jsx)(i.h4,{id:"dynamic-partition-pruning-for-streaming-read",children:"Dynamic partition pruning for streaming read"}),"\n",(0,r.jsx)(i.p,{children:"Before 0.14.0, the Flink streaming reader can not prune the datetime partitions correctly when the queries have\npredicates with constant datetime filtering. Since this release, the Flink streaming queries have been fixed to support\nany pattern of filtering predicates, including but not limited to the datetime filtering."}),"\n",(0,r.jsx)(i.h4,{id:"simple-bucket-index-table-query-speed-up-with-index-fields",children:"Simple bucket index table query speed up (with index fields)"}),"\n",(0,r.jsxs)(i.p,{children:["For a simple bucket index table, if the query takes equality filtering predicates on index key fields, Flink engine\nwould optimize the planning to only include the source data files from a very specific data bucket; such queries expect\nto have nearly ",(0,r.jsx)(i.code,{children:"hoodie.bucket.index.num.buckets"})," times performance improvement in average."]}),"\n",(0,r.jsx)(i.h4,{id:"flink-117-support",children:"Flink 1.17 support"}),"\n",(0,r.jsxs)(i.p,{children:["Flink 1.17 is supported with a new compile maven profile ",(0,r.jsx)(i.code,{children:"flink1.17"}),", adding profile ",(0,r.jsx)(i.code,{children:"-Pflink1.17"})," in the compile cmd of\nFlink Hudi bundle jar to enable the integration with Flink 1.17."]}),"\n",(0,r.jsx)(i.h4,{id:"update-deletes-statement-for-flink",children:"Update deletes statement for Flink"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.code,{children:"UPDATE"})," and ",(0,r.jsx)(i.code,{children:"DELETE"})," statements have been integrated since this release for batch queries. Current only table that\ndefines primary keys can handle the statement correctly."]}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{children:"UPDATE hudi_table SET ... WHERE ...\nDELETE FROM hudi_table WHERE ...\n\nEXAMPLES\n-- update the specific records with constant age\nUPDATE hudi_table SET age=19 WHERE UUID in ('id1', 'id2');\n-- delete all the records that with age greater than 23\nDELETE FROM hudi_table WHERE age > 23;\n"})}),"\n",(0,r.jsx)(i.h3,{id:"java-enhancements",children:"Java Enhancements"}),"\n",(0,r.jsx)(i.p,{children:"Lot of write operations have been extended to support Java engine to bring it to parity with other engines. For eg,\ncompaction, clustering, and metadata table support has been added to Java Engine with 0.14.0."}),"\n",(0,r.jsx)(i.h2,{id:"known-regressions",children:"Known Regressions"}),"\n",(0,r.jsxs)(i.p,{children:["In Hudi 0.14.0, when querying a table that uses ComplexKeyGenerator or CustomKeyGenerator, partition values are returned\nas string. Note that there is no type change on the storage i.e. partition fields are written in the user-defined type\non storage. However, this is a breaking change for the aforementioned key generators and will be fixed in 0.14.1 -\n",(0,r.jsx)(i.a,{href:"https://issues.apache.org/jira/browse/HUDI-6914",children:"HUDI-6914"})]}),"\n",(0,r.jsx)(i.h2,{id:"raw-release-notes",children:"Raw Release Notes"}),"\n",(0,r.jsxs)(i.p,{children:["The raw release notes are available ",(0,r.jsx)(i.a,{href:"https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12322822&version=12352700",children:"here"}),"."]})]})}function h(e={}){const{wrapper:i}={...(0,a.R)(),...e.components};return i?(0,r.jsx)(i,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},19365:(e,i,t)=>{t.d(i,{A:()=>s});t(96540);var n=t(34164);const r={tabItem:"tabItem_Ymn6"};var a=t(74848);function s(e){let{children:i,hidden:t,className:s}=e;return(0,a.jsx)("div",{role:"tabpanel",className:(0,n.A)(r.tabItem,s),hidden:t,children:i})}},11470:(e,i,t)=>{t.d(i,{A:()=>j});var n=t(96540),r=t(34164),a=t(23104),s=t(56347),o=t(205),l=t(57485),d=t(31682),c=t(70679);function h(e){return n.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,n.isValidElement)(e)&&function(e){const{props:i}=e;return!!i&&"object"==typeof i&&"value"in i}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function u(e){const{values:i,children:t}=e;return(0,n.useMemo)((()=>{const e=i??function(e){return h(e).map((e=>{let{props:{value:i,label:t,attributes:n,default:r}}=e;return{value:i,label:t,attributes:n,default:r}}))}(t);return function(e){const i=(0,d.XI)(e,((e,i)=>e.value===i.value));if(i.length>0)throw new Error(`Docusaurus error: Duplicate values "${i.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[i,t])}function p(e){let{value:i,tabValues:t}=e;return t.some((e=>e.value===i))}function m(e){let{queryString:i=!1,groupId:t}=e;const r=(0,s.W6)(),a=function(e){let{queryString:i=!1,groupId:t}=e;if("string"==typeof i)return i;if(!1===i)return null;if(!0===i&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:i,groupId:t});return[(0,l.aZ)(a),(0,n.useCallback)((e=>{if(!a)return;const i=new URLSearchParams(r.location.search);i.set(a,e),r.replace({...r.location,search:i.toString()})}),[a,r])]}function f(e){const{defaultValue:i,queryString:t=!1,groupId:r}=e,a=u(e),[s,l]=(0,n.useState)((()=>function(e){let{defaultValue:i,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(i){if(!p({value:i,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${i}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return i}const n=t.find((e=>e.default))??t[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:i,tabValues:a}))),[d,h]=m({queryString:t,groupId:r}),[f,g]=function(e){let{groupId:i}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(i),[r,a]=(0,c.Dv)(t);return[r,(0,n.useCallback)((e=>{t&&a.set(e)}),[t,a])]}({groupId:r}),b=(()=>{const e=d??f;return p({value:e,tabValues:a})?e:null})();(0,o.A)((()=>{b&&l(b)}),[b]);return{selectedValue:s,selectValue:(0,n.useCallback)((e=>{if(!p({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);l(e),h(e),g(e)}),[h,g,a]),tabValues:a}}var g=t(92303);const b={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var x=t(74848);function v(e){let{className:i,block:t,selectedValue:n,selectValue:s,tabValues:o}=e;const l=[],{blockElementScrollPositionUntilNextRender:d}=(0,a.a_)(),c=e=>{const i=e.currentTarget,t=l.indexOf(i),r=o[t].value;r!==n&&(d(i),s(r))},h=e=>{let i=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=l.indexOf(e.currentTarget)+1;i=l[t]??l[0];break}case"ArrowLeft":{const t=l.indexOf(e.currentTarget)-1;i=l[t]??l[l.length-1];break}}i?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":t},i),children:o.map((e=>{let{value:i,label:t,attributes:a}=e;return(0,x.jsx)("li",{role:"tab",tabIndex:n===i?0:-1,"aria-selected":n===i,ref:e=>l.push(e),onKeyDown:h,onClick:c,...a,className:(0,r.A)("tabs__item",b.tabItem,a?.className,{"tabs__item--active":n===i}),children:t??i},i)}))})}function y(e){let{lazy:i,children:t,selectedValue:a}=e;const s=(Array.isArray(t)?t:[t]).filter(Boolean);if(i){const e=s.find((e=>e.props.value===a));return e?(0,n.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:s.map(((e,i)=>(0,n.cloneElement)(e,{key:i,hidden:e.props.value!==a})))})}function w(e){const i=f(e);return(0,x.jsxs)("div",{className:(0,r.A)("tabs-container",b.tabList),children:[(0,x.jsx)(v,{...i,...e}),(0,x.jsx)(y,{...i,...e})]})}function j(e){const i=(0,g.A)();return(0,x.jsx)(w,{...e,children:h(e.children)},String(i))}},28453:(e,i,t)=>{t.d(i,{R:()=>s,x:()=>o});var n=t(96540);const r={},a=n.createContext(r);function s(e){const i=n.useContext(a);return n.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),n.createElement(a.Provider,{value:i},e.children)}}}]);