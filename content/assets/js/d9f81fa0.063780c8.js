"use strict";(globalThis.webpackChunkhudi=globalThis.webpackChunkhudi||[]).push([[81466],{28453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var s=i(96540);const r={},a=s.createContext(r);function t(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),s.createElement(a.Provider,{value:n},e.children)}},41858:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"faq_integrations","title":"Integrations","description":"Does AWS GLUE support Hudi ?","source":"@site/versioned_docs/version-1.0.2/faq_integrations.md","sourceDirName":".","slug":"/faq_integrations","permalink":"/docs/faq_integrations","draft":false,"unlisted":false,"editUrl":"https://github.com/apache/hudi/tree/asf-site/website/versioned_docs/version-1.0.2/faq_integrations.md","tags":[],"version":"1.0.2","frontMatter":{"title":"Integrations","keywords":["hudi","writing","reading"]}}');var r=i(74848),a=i(28453);const t={title:"Integrations",keywords:["hudi","writing","reading"]},o="Integrations FAQ",d={},l=[{value:"Does AWS GLUE support Hudi ?",id:"does-aws-glue-support-hudi-",level:3},{value:"How to override Hudi jars in EMR?",id:"how-to-override-hudi-jars-in-emr",level:3}];function u(e){const n={a:"a",code:"code",h1:"h1",h3:"h3",header:"header",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"integrations-faq",children:"Integrations FAQ"})}),"\n",(0,r.jsx)(n.h3,{id:"does-aws-glue-support-hudi-",children:"Does AWS GLUE support Hudi ?"}),"\n",(0,r.jsxs)(n.p,{children:['AWS Glue jobs can write, read and update Glue Data Catalog for hudi tables. In order to successfully integrate with Glue Data Catalog, you need to subscribe to one of the AWS provided Glue connectors named "AWS Glue Connector for Apache Hudi". Glue job needs to have "Use Glue data catalog as the Hive metastore" option ticked. Detailed steps with a sample scripts is available on this article provided by AWS - ',(0,r.jsx)(n.a,{href:"https://aws.amazon.com/blogs/big-data/writing-to-apache-hudi-tables-using-aws-glue-connector/",children:"https://aws.amazon.com/blogs/big-data/writing-to-apache-hudi-tables-using-aws-glue-connector/"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"In case if your using either notebooks or Zeppelin through Glue dev-endpoints, your script might not be able to integrate with Glue DataCatalog when writing to hudi tables."}),"\n",(0,r.jsx)(n.h3,{id:"how-to-override-hudi-jars-in-emr",children:"How to override Hudi jars in EMR?"}),"\n",(0,r.jsx)(n.p,{children:"If you are looking to override Hudi jars in your EMR clusters one way to achieve this is by providing the Hudi jars through a bootstrap script."}),"\n",(0,r.jsx)(n.p,{children:"Here are the example steps for overriding Hudi version 0.7.0 in EMR 0.6.2."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Build Hudi Jars:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Git clone\ngit clone https://github.com/apache/hudi.git && cd hudi   \n\n# Get version 0.7.0\ngit checkout --track origin/release-0.7.0\n\n# Build jars with spark 3.0.0 and scala 2.12 (since emr 6.2.0 uses spark 3 which requires scala 2.12):\nmvn clean package -DskipTests -Dspark3  -Dscala-2.12 -T 30 \n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Copy jars to s3:"})}),"\n",(0,r.jsx)(n.p,{children:"These are the jars we are interested in after build completes. Copy them to a temp location first."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"mkdir -p ~/Downloads/hudi-jars\ncp packaging/hudi-hadoop-mr-bundle/target/hudi-hadoop-mr-bundle-0.7.0.jar ~/Downloads/hudi-jars/\ncp packaging/hudi-hive-sync-bundle/target/hudi-hive-sync-bundle-0.7.0.jar ~/Downloads/hudi-jars/\ncp packaging/hudi-spark-bundle/target/hudi-spark-bundle_2.12-0.7.0.jar ~/Downloads/hudi-jars/\ncp packaging/hudi-timeline-server-bundle/target/hudi-timeline-server-bundle-0.7.0.jar ~/Downloads/hudi-jars/\ncp packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.12-0.7.0.jar ~/Downloads/hudi-jars/\n"})}),"\n",(0,r.jsx)(n.p,{children:"Upload all jars from ~/Downloads/hudi-jars/ to the s3 location s3://xxx/yyy/hudi-jars"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Include Hudi jars as part of the emr bootstrap script:"})}),"\n",(0,r.jsxs)(n.p,{children:["Below script downloads Hudi jars from above s3 location. Use this script as part ",(0,r.jsx)(n.code,{children:"bootstrap-actions"})," when launching the EMR cluster to install the jars in each node."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"#!/bin/bash\nsudo mkdir -p /mnt1/hudi-jars\n\nsudo aws s3 cp s3://xxx/yyy/hudi-jars /mnt1/hudi-jars --recursive\n\n# create symlinks\ncd /mnt1/hudi-jars\nsudo ln -sf hudi-hadoop-mr-bundle-0.7.0.jar hudi-hadoop-mr-bundle.jar\nsudo ln -sf hudi-hive-sync-bundle-0.7.0.jar hudi-hive-sync-bundle.jar\nsudo ln -sf hudi-spark-bundle_2.12-0.7.0.jar hudi-spark-bundle.jar\nsudo ln -sf hudi-timeline-server-bundle-0.7.0.jar hudi-timeline-server-bundle.jar\nsudo ln -sf hudi-utilities-bundle_2.12-0.7.0.jar hudi-utilities-bundle.jar\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Using the overriden jar in Deltastreamer:"})}),"\n",(0,r.jsx)(n.p,{children:"When invoking DeltaStreamer specify the above jar location as part of spark-submit command."})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}}}]);