"use strict";(globalThis.webpackChunkhudi=globalThis.webpackChunkhudi||[]).push([[58301],{4219(t){t.exports=JSON.parse('{"permalink":"/blog/2020/03/22/exporting-hudi-datasets","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-03-22-exporting-hudi-datasets.md","source":"@site/blog/2020-03-22-exporting-hudi-datasets.md","title":"Export Hudi datasets as a copy or as different formats","description":"Copy to Hudi dataset","date":"2020-03-22T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"snapshot exporter","permalink":"/blog/tags/snapshot-exporter"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":2.12,"hasTruncateMarker":true,"authors":[{"name":"rxu","key":null,"page":null}],"frontMatter":{"title":"Export Hudi datasets as a copy or as different formats","excerpt":"Learn how to copy or export HUDI dataset in various formats.","author":"rxu","category":"blog","tags":["how-to","snapshot exporter","apache hudi"]},"unlisted":false,"prevItem":{"title":"Apache Hudi Support on Apache Zeppelin","permalink":"/blog/2020/04/27/apache-hudi-apache-zepplin"},"nextItem":{"title":"Change Capture Using AWS Database Migration Service and Hudi","permalink":"/blog/2020/01/20/change-capture-using-aws"}}')},17721(t,e,a){a.r(e),a.d(e,{assets:()=>p,contentTitle:()=>s,default:()=>l,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=a(4219),o=a(74848),n=a(28453);const r={title:"Export Hudi datasets as a copy or as different formats",excerpt:"Learn how to copy or export HUDI dataset in various formats.",author:"rxu",category:"blog",tags:["how-to","snapshot exporter","apache hudi"]},s=void 0,p={authorsImageUrls:[void 0]},d=[{value:"Copy to Hudi dataset",id:"copy-to-hudi-dataset",level:3},{value:"Export to json or parquet dataset",id:"export-to-json-or-parquet-dataset",level:3},{value:"Re-partitioning",id:"re-partitioning",level:3},{value:"<code>--output-partition-field</code>",id:"--output-partition-field",level:4},{value:"<code>--output-partitioner</code>",id:"--output-partitioner",level:4}];function u(t){const e={code:"code",h3:"h3",h4:"h4",p:"p",pre:"pre",strong:"strong",...(0,n.R)(),...t.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.h3,{id:"copy-to-hudi-dataset",children:"Copy to Hudi dataset"}),"\n",(0,o.jsxs)(e.p,{children:["Similar to the existing  ",(0,o.jsx)(e.code,{children:"HoodieSnapshotCopier"}),", the Exporter scans the source dataset and then makes a copy of it to the target output path."]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:'spark-submit \\\n  --jars "packaging/hudi-spark-bundle/target/hudi-spark-bundle_2.11-0.6.0-SNAPSHOT.jar" \\\n  --deploy-mode "client" \\\n  --class "org.apache.hudi.utilities.HoodieSnapshotExporter" \\\n      packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.6.0-SNAPSHOT.jar \\\n  --source-base-path "/tmp/" \\\n  --target-output-path "/tmp/exported/hudi/" \\\n  --output-format "hudi"\n'})}),"\n",(0,o.jsx)(e.h3,{id:"export-to-json-or-parquet-dataset",children:"Export to json or parquet dataset"}),"\n",(0,o.jsx)(e.p,{children:'The Exporter can also convert the source dataset into other formats. Currently only "json" and "parquet" are supported.'}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:'spark-submit \\\n  --jars "packaging/hudi-spark-bundle/target/hudi-spark-bundle_2.11-0.6.0-SNAPSHOT.jar" \\\n  --deploy-mode "client" \\\n  --class "org.apache.hudi.utilities.HoodieSnapshotExporter" \\\n      packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.6.0-SNAPSHOT.jar \\\n  --source-base-path "/tmp/" \\\n  --target-output-path "/tmp/exported/json/" \\\n  --output-format "json"  # or "parquet"\n'})}),"\n",(0,o.jsx)(e.h3,{id:"re-partitioning",children:"Re-partitioning"}),"\n",(0,o.jsx)(e.p,{children:"When export to a different format, the Exporter takes parameters to do some custom re-partitioning. By default, if neither of the 2 parameters below is given, the output dataset will have no partition."}),"\n",(0,o.jsx)(e.h4,{id:"--output-partition-field",children:(0,o.jsx)(e.code,{children:"--output-partition-field"})}),"\n",(0,o.jsxs)(e.p,{children:["This parameter uses an existing non-metadata field as the output partitions. All  ",(0,o.jsx)(e.code,{children:"_hoodie_*"}),"  metadata field will be stripped during export."]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:'spark-submit \\\n  --jars "packaging/hudi-spark-bundle/target/hudi-spark-bundle_2.11-0.6.0-SNAPSHOT.jar" \\\n  --deploy-mode "client" \\\n  --class "org.apache.hudi.utilities.HoodieSnapshotExporter" \\\n      packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.6.0-SNAPSHOT.jar \\  \n  --source-base-path "/tmp/" \\\n  --target-output-path "/tmp/exported/json/" \\\n  --output-format "json" \\\n  --output-partition-field "symbol"  # assume the source dataset contains a field `symbol`\n'})}),"\n",(0,o.jsx)(e.p,{children:"The output directory will look like this"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"`_SUCCESS symbol=AMRS symbol=AYX symbol=CDMO symbol=CRC symbol=DRNA ...`\n"})}),"\n",(0,o.jsx)(e.h4,{id:"--output-partitioner",children:(0,o.jsx)(e.code,{children:"--output-partitioner"})}),"\n",(0,o.jsxs)(e.p,{children:["This parameter takes in a fully-qualified name of a class that implements  ",(0,o.jsx)(e.code,{children:"HoodieSnapshotExporter.Partitioner"}),". This parameter takes higher precedence than  ",(0,o.jsx)(e.code,{children:"--output-partition-field"}),", which will be ignored if this is provided."]}),"\n",(0,o.jsx)(e.p,{children:"An example implementation is shown below:"}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"MyPartitioner.java"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-java",children:'package com.foo.bar;\npublic class MyPartitioner implements HoodieSnapshotExporter.Partitioner {\n\n  private static final String PARTITION_NAME = "date";\n \n  @Override\n  public DataFrameWriter<Row> partition(Dataset<Row> source) {\n    // use the current hoodie partition path as the output partition\n    return source\n        .withColumnRenamed(HoodieRecord.PARTITION_PATH_METADATA_FIELD, PARTITION_NAME)\n        .repartition(new Column(PARTITION_NAME))\n        .write()\n        .partitionBy(PARTITION_NAME);\n  }\n}\n'})}),"\n",(0,o.jsxs)(e.p,{children:["After putting this class in ",(0,o.jsx)(e.code,{children:"my-custom.jar"}),", which is then placed on the job classpath, the submit command will look like this:"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:'spark-submit \\\n  --jars "packaging/hudi-spark-bundle/target/hudi-spark-bundle_2.11-0.6.0-SNAPSHOT.jar,my-custom.jar" \\\n  --deploy-mode "client" \\\n  --class "org.apache.hudi.utilities.HoodieSnapshotExporter" \\\n      packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.6.0-SNAPSHOT.jar \\\n  --source-base-path "/tmp/" \\\n  --target-output-path "/tmp/exported/json/" \\\n  --output-format "json" \\\n  --output-partitioner "com.foo.bar.MyPartitioner"\n'})})]})}function l(t={}){const{wrapper:e}={...(0,n.R)(),...t.components};return e?(0,o.jsx)(e,{...t,children:(0,o.jsx)(u,{...t})}):u(t)}},28453(t,e,a){a.d(e,{R:()=>r,x:()=>s});var i=a(96540);const o={},n=i.createContext(o);function r(t){const e=i.useContext(n);return i.useMemo(function(){return"function"==typeof t?t(e):{...e,...t}},[e,t])}function s(t){let e;return e=t.disableParentContext?"function"==typeof t.components?t.components(o):t.components||o:r(t.components),i.createElement(n.Provider,{value:e},t.children)}}}]);