"use strict";(globalThis.webpackChunkhudi=globalThis.webpackChunkhudi||[]).push([[11745],{7320(e,t,i){i.r(t),i.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>n,toc:()=>d});var n=i(64341),s=i(74848),a=i(28453);const r={title:"Announcing Apache Hudi 1.0 and the Next Generation of Data Lakehouses",excerpt:"game-changing major release, that reimagines Hudi and Data Lakehouses.",author:"Vinoth Chandar",category:"blog",image:"/assets/images/blog/dlms-hierarchy.png",tags:["timeline","design","release","streaming ingestion","multi-writer","concurrency-control","blog"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Overview",id:"overview",level:2},{value:"Evolution of the Data Lakehouse",id:"evolution-of-the-data-lakehouse",level:2},{value:"Key Features in Hudi 1.0",id:"key-features-in-hudi-10",level:2},{value:"New Time and Timeline",id:"new-time-and-timeline",level:3},{value:"Secondary Indexing for Faster Lookups",id:"secondary-indexing-for-faster-lookups",level:3},{value:"Bloom Filter indexes",id:"bloom-filter-indexes",level:3},{value:"Partitioning replaced by Expression Indexes",id:"partitioning-replaced-by-expression-indexes",level:3},{value:"Efficient Partial Updates",id:"efficient-partial-updates",level:3},{value:"Merge Modes and Custom Mergers",id:"merge-modes-and-custom-mergers",level:3},{value:"Non-Blocking Concurrency Control for Streaming Writes",id:"non-blocking-concurrency-control-for-streaming-writes",level:3},{value:"Backwards Compatible Writing",id:"backwards-compatible-writing",level:3},{value:"What\u2019s Next?",id:"whats-next",level:2},{value:"Get Started with Apache Hudi 1.0",id:"get-started-with-apache-hudi-10",level:2}];function c(e){const t={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsxs)(t.p,{children:["We are thrilled to announce the release of Apache Hudi 1.0, a landmark achievement for our vibrant community that defines what the next generation of data lakehouses should achieve. Hudi pioneered ",(0,s.jsx)(t.em,{children:(0,s.jsx)(t.strong,{children:"transactional data lakes"})})," in 2017, and today, we live in a world where this technology category is mainstream as the \u201c",(0,s.jsx)(t.em,{children:(0,s.jsx)(t.strong,{children:"Data Lakehouse\u201d"})}),". The Hudi community has made several key, original, and first-of-its-kind contributions to this category, as shown below, compared to when other OSS alternatives emerged. This is an incredibly rare feat for a relatively small OSS community to sustain in a fiercely competitive commercial data ecosystem. On the other hand, it also demonstrates the value of deeply understanding the technology category within a focused open-source community. So, I first want to thank/congratulate the Hudi community and the ",(0,s.jsx)(t.strong,{children:"60+ contributors"})," for making 1.0 happen."]}),"\n",(0,s.jsx)("div",{style:{textAlign:"center"},children:(0,s.jsx)("img",{src:"/assets/images/blog/hudi-innovation-timeline.jpg",alt:"innovation timeline"})}),"\n",(0,s.jsxs)(t.p,{children:["This ",(0,s.jsx)(t.a,{href:"/releases/release-1.0.0",children:"release"})," is more than just a version increment\u2014it advances the breadth of Hudi\u2019s feature set and its architecture's robustness while bringing fresh innovation to shape the future. This post reflects on how technology and the surrounding ecosystem have evolved, making a case for a holistic \u201c",(0,s.jsx)(t.em,{children:(0,s.jsx)(t.strong,{children:"Data Lakehouse Management System"})}),"\u201d (",(0,s.jsx)(t.em,{children:(0,s.jsx)(t.strong,{children:"DLMS"})}),") as the new Northstar. For most of this post, we will deep dive into the latest capabilities of Hudi 1.0 that make this evolution possible."]}),"\n",(0,s.jsx)(t.h2,{id:"evolution-of-the-data-lakehouse",children:"Evolution of the Data Lakehouse"}),"\n",(0,s.jsxs)(t.p,{children:["Technologies must constantly evolve\u2014",(0,s.jsx)(t.a,{href:"https://en.wikipedia.org/wiki/Web3",children:"Web 3.0"}),", ",(0,s.jsx)(t.a,{href:"https://en.wikipedia.org/wiki/List_of_wireless_network_technologies",children:"cellular tech"}),", ",(0,s.jsx)(t.a,{href:"https://en.wikipedia.org/wiki/Programming_language_generations",children:"programming language generations"}),"\u2014based on emerging needs. Data lakehouses are no exception. This section explores the hierarchy of such needs for data lakehouse users. The most basic need is the \u201c",(0,s.jsx)(t.strong,{children:"table format"}),"\u201d functionality, the foundation for data lakehouses. Table format organizes the collection of files/objects into tables with snapshots, schema, and statistics tracking, enabling higher abstraction. Furthermore, table format dictates the organization of files within each snapshot, encoding deletes/updates and metadata about how the table changes over time. Table format also provides protocols for various readers and writers and table management processes to handle concurrent access and provide ACID transactions safely. In the last five years, leading data warehouse and cloud vendors have integrated their proprietary SQL warehouse stack with open table formats. While they mostly default to their closed table formats and the compute engines remain closed, this welcome move provides users an open alternative for their data."]}),"\n",(0,s.jsxs)(t.p,{children:["However, the benefits of a format end there, and now a table format is just the tip of the iceberg. Users require an ",(0,s.jsx)(t.a,{href:"https://www.onehouse.ai/blog/open-table-formats-and-the-open-data-lakehouse-in-perspective",children:"end-to-end open data lakehouse"}),", and modern data lakehouse features need a sophisticated layer of ",(0,s.jsx)(t.em,{children:(0,s.jsx)(t.strong,{children:"open-source software"})})," operating on data stored in open table formats. For example, Optimized writers can balance cost and performance by carefully managing file sizes using the statistics maintained in the table format or catalog syncing service that can make data in Hudi readily available to half a dozen catalogs open and closed out there. Hudi shines by providing a high-performance open table format as well as a comprehensive open-source software stack that can ingest, store, optimize and effectively self-manage a data lakehouse. This distinction between open formats and open software is often lost in translation inside the large vendor ecosystem in which Hudi operates. Still, it has been and remains a key consideration for Hudi\u2019s ",(0,s.jsx)(t.a,{href:"/powered-by",children:"users"})," to avoid compute-lockin to any given data vendor. The Hudi streamer tool, e.g., powers hundreds of data lakes by ingesting data seamlessly from various sources at the convenience of a single command in a terminal."]}),"\n",(0,s.jsx)("div",{style:{textAlign:"center",width:"90%",height:"auto"},children:(0,s.jsx)("img",{src:"/assets/images/blog/dlms-hierarchy.png",alt:"dlms hierarchy"})}),"\n",(0,s.jsxs)(t.p,{children:["Moving forward with 1.0, the community has ",(0,s.jsx)(t.a,{href:"https://github.com/apache/hudi/pull/8679",children:"debated"})," these key points and concluded that we need more open-source \u201c",(0,s.jsx)(t.strong,{children:"software capabilities"}),"\u201d that are directly comparable with DBMSes for two main reasons."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Significantly expand the technical capabilities of a data lakehouse"}),": Many design decisions in Hudi have been inspired by databases (see ",(0,s.jsx)(t.a,{href:"https://github.com/apache/hudi/blob/master/rfc/rfc-69/rfc-69.md#hudi-1x",children:"here"})," for a layer-by-layer mapping) and have delivered significant benefits to the community. For example, Hudi\u2019s indexing mechanisms deliver the fast update performance the project has come to be known for.  We want to generalize such features across writers and queries and introduce new capabilities like fast metastores for query planning, support for unstructured/multimodal data and caching mechanisms that can be deeply integrated into (at least) open-source query engines in the ecosystem. We also need concurrency control that works for lakehouse workloads instead of employing techniques applicable to OLTP databases at the surface level."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"We also need a database-like experience"}),": We originally designed Hudi as a software library that can be embedded into different query/processing engines for reading/writing/managing tables. This model has been a great success within the existing data ecosystem, which is familiar with scheduling jobs and employing multiple engines for ETL and interactive queries. However, for a new user wanting to explore data lakehouses, there is no piece of software to easily install and explore all functionality packaged coherently. Such data lakehouse functionality packaged and delivered like a typical database system unlocks new use cases. For example, with such a system, we could bring HTAP capabilities to the data lakehouses on faster cloud storage/row-oriented formats, finally making it a low-latency data serving layer."]}),"\n",(0,s.jsxs)(t.p,{children:["If combined, we would gain a powerful database built on top of the data lake(house) architecture\u2014a ",(0,s.jsx)(t.em,{children:(0,s.jsx)(t.strong,{children:"data"})})," ",(0,s.jsx)(t.em,{children:(0,s.jsx)(t.strong,{children:"lakehouse"})})," ",(0,s.jsx)(t.em,{children:(0,s.jsx)(t.strong,{children:"management"})})," ",(0,s.jsx)(t.em,{children:(0,s.jsx)(t.strong,{children:"system"})})," ",(0,s.jsx)(t.em,{children:(0,s.jsx)(t.strong,{children:"(DLMS)"})}),"\u2014that we believe the industry needs."]}),"\n",(0,s.jsx)(t.h2,{id:"key-features-in-hudi-10",children:"Key Features in Hudi 1.0"}),"\n",(0,s.jsxs)(t.p,{children:["In Hudi 1.0, we\u2019ve delivered a significant expansion of data lakehouse technical capabilities discussed above inside Hudi\u2019s ",(0,s.jsx)(t.a,{href:"https://en.wikipedia.org/wiki/Database_engine",children:"storage engine"})," layer.  Storage engines (a.k.a database engines) are standard database components that sit on top of the storage/file/table format and are wrapped by the DBMS layer above, handling the core read/write/management functionality. In the figure below, we map the Hudi components with the seminal ",(0,s.jsx)(t.a,{href:"https://dsf.berkeley.edu/papers/fntdb07-architecture.pdf",children:"Architecture of a Database System"})," paper (see page 4) to illustrate the standard layering discussed. If the layering is implemented correctly, we can deliver the benefits of the storage engine to even other table formats, which may lack such fully-developed open-source software for table management or achieving high performance, via interop standards defined in projects like ",(0,s.jsx)(t.a,{href:"https://xtable.apache.org/",children:"Apache XTable (Incubating)"}),"."]}),"\n",(0,s.jsxs)("div",{style:{textAlign:"center",width:"80%",height:"auto"},children:[(0,s.jsx)("img",{src:"/assets/images/hudi-stack-1-x.png",alt:"Hudi DB Architecture"}),(0,s.jsx)("p",{align:"center",children:"Figure: Apache Hudi Database Architecture"})]}),"\n",(0,s.jsx)(t.p,{children:"Regarding full-fledged DLMS functionality, the closest experience Hudi 1.0 offers is through Apache Spark. Users can deploy a Spark server (or Spark Connect) with Hudi 1.0 installed, submit SQL/jobs, orchestrate table services via SQL commands, and enjoy new secondary index functionality to speed up queries like a DBMS. Subsequent releases in the 1.x release line and beyond will continuously add new features and improve this experience."}),"\n",(0,s.jsx)(t.p,{children:"In the following sections, let\u2019s dive into what makes Hudi 1.0 a standout release."}),"\n",(0,s.jsx)(t.h3,{id:"new-time-and-timeline",children:"New Time and Timeline"}),"\n",(0,s.jsxs)(t.p,{children:["For the familiar user, time is a key concept in Hudi. Hudi\u2019s original notion of time was instantaneous, i.e., actions that modify the table appear to take effect at a given instant. This was limiting when designing features like non-blocking concurrency control across writers, which needs to reason about actions more as an \u201cinterval\u201d to detect other conflicting actions. Every action on the Hudi timeline now gets a ",(0,s.jsx)(t.em,{children:"requested"})," and a ",(0,s.jsx)(t.em,{children:"completion"})," time; Thus, the timeline layout version has bumped up in the 1.0 release. Furthermore, to ease the understanding and bring consistency around time generation for users and implementors, we have formalized the adoption of ",(0,s.jsx)(t.a,{href:"/docs/timeline#truetime-generation",children:"TrueTime"})," semantics. The default implementation assures forward-moving clocks even with distributed processes, assuming a maximum tolerable clock skew similar to ",(0,s.jsx)(t.a,{href:"https://cockroachlabs.com/blog/living-without-atomic-clocks/",children:"OLTP/NoSQL"})," stores adopting TrueTime."]}),"\n",(0,s.jsxs)("div",{style:{textAlign:"center"},children:[(0,s.jsx)("img",{src:"/assets/images/hudi-timeline-actions.png",alt:"Timeline actions"}),(0,s.jsx)("p",{align:"center",children:"Figure: Showing actions in Hudi 1.0 modeled as an interval of two instants: requested and completed"})]}),"\n",(0,s.jsxs)(t.p,{children:["Hudi tables are frequently updated, and users also want to retain a more extended action history associated with the table. Before Hudi 1.0, the older action history in a table was archived for audit access. But, due to the lack of support for cloud storage appends, access might become cumbersome due to tons of small files. In Hudi 1.0, we have redesigned the timeline as an ",(0,s.jsx)(t.a,{href:"https://en.wikipedia.org/wiki/Log-structured_merge-tree",children:"LSM tree"}),", which is widely adopted for cases where good write performance on temporal data is desired."]}),"\n",(0,s.jsxs)(t.p,{children:["In the Hudi 1.0 release, the ",(0,s.jsx)(t.a,{href:"/docs/timeline#lsm-timeline-history",children:"LSM timeline"})," is heavily used in the query planning to map requested and completion times across Apache Spark, Apache Flink and Apache Hive. Future releases plan to leverage this to unify the timeline's active and history components, providing infinite retention of table history. Micro benchmarks show that the LSM timeline can be pretty efficient, even committing every ",(0,s.jsx)(t.em,{children:(0,s.jsx)(t.strong,{children:"30 seconds for 10 years with about 10M instants"})}),", further cementing Hudi\u2019s table format as the most suited for frequently written tables."]}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"Number of actions"}),(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"Instant Batch Size"}),(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"Read cost (just times)"}),(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"Read cost \v(along with action metadata)"}),(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"Total file size"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"10000"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"10"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"32ms"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"150ms"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"8.39MB"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"20000"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"10"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"51ms"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"188ms"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"16.8MB"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"10000000"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"1000"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"3400ms"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"162s"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"8.4GB"})]})]})]}),"\n",(0,s.jsx)(t.h3,{id:"secondary-indexing-for-faster-lookups",children:"Secondary Indexing for Faster Lookups"}),"\n",(0,s.jsxs)(t.p,{children:["Indexes are core to Hudi\u2019s design, so much so that even the first pre-open-source version of Hudi shipped with ",(0,s.jsx)(t.a,{href:"/docs/indexes#additional-writer-side-indexes",children:"indexes"})," to speed up writes. However, these indexes were limited to the writer's side, except for record indexes in 0.14+ above, which were also integrated with Spark SQL queries. Hudi 1.0 generalizes indexes closer to the indexing functionality found in relational databases, supporting indexes on any secondary column across both writer and readers. Hudi 1.0 also supports near-standard ",(0,s.jsx)(t.a,{href:"/docs/sql_ddl#create-index",children:"SQL syntax"})," for creating/dropping indexes on different columns via Spark SQL, along with an asynchronous indexing table service to build indexes without interrupting the writers."]}),"\n",(0,s.jsxs)("div",{style:{textAlign:"center",paddingLeft:"10%",width:"70%",height:"auto"},children:[(0,s.jsx)("img",{src:"/assets/images/hudi-stack-indexes.png",alt:"Indexes"}),(0,s.jsx)("p",{align:"center",children:"Figure: the indexing subsystem in Hudi 1.0, showing different types of indexes"})]}),"\n",(0,s.jsxs)(t.p,{children:["With secondary indexes, queries and DMLs scan a much-reduced amount of files from cloud storage, dramatically reducing costs (e.g., on engines like AWS Athena, which price by data scanned) and improving query performance for queries with low to even moderate amount of selectivity. On a benchmark of a query on ",(0,s.jsx)(t.em,{children:"web_sales"})," table (from ",(0,s.jsx)(t.em,{children:(0,s.jsx)(t.strong,{children:"10 TB tpc-ds dataset"})}),"), with file groups - 286,603, total records - 7,198,162,544 and cardinality of secondary index column in the ~ 1:150 ranges, we see a remarkable ",(0,s.jsx)(t.em,{children:(0,s.jsx)(t.strong,{children:"~95% decrease in latency"})}),"."]}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"Run 1"}),(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"Total Query Latency w/o indexing skipping (secs)"}),(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"Total Query Latency with secondary index skipping (secs)"}),(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"% decrease"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"1"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"252"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"31"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"~88%"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"2"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"214"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"10"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"~95%"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"3"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"204"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"9"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"~95%"})]})]})]}),"\n",(0,s.jsx)(t.p,{children:"In Hudi 1.0, secondary indexes are only supported for Apache Spark, with planned support for other engines in Hudi 1.1, starting with Flink, Presto and Trino."}),"\n",(0,s.jsx)(t.h3,{id:"bloom-filter-indexes",children:"Bloom Filter indexes"}),"\n",(0,s.jsxs)(t.p,{children:["Bloom filter indexes have existed on the Hudi writers for a long time. It is one of the most performant and versatile indexes users prefer for \u201cneedle-in-a-haystack\u201d deletes/updates or de-duplication. The index works by storing special footers in base files around min/max key ranges and a dynamic bloom filter that adapts to the file size and can automatically handle partitioning/skew on the writer's path. Hudi 1.0 introduces a newer kind of bloom filter index for Spark SQL while retaining the writer-side index as-is. The new index stores bloom filters in the Hudi metadata table and other secondary/record indexes for scalable access, even for huge tables, since the index is stored in fewer files compared to being stored alongside data files. It can be created using standard ",(0,s.jsx)(t.a,{href:"/docs/sql_ddl#create-bloom-filter-index",children:"SQL syntax"}),", as shown below. Subsequent queries on the indexed columns will use the bloom filters to speed up queries."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:"-- Create a bloom filter index on the driver column of the table `hudi_table`\nCREATE INDEX idx_bloom_driver ON hudi_indexed_table USING bloom_filters(driver)\v\n-- Create a bloom filter index on the column derived from expression `lower(rider)` of the table `hudi_table`\nCREATE INDEX idx_bloom_rider ON hudi_indexed_table USING bloom_filters(rider) OPTIONS(expr='lower');\n"})}),"\n",(0,s.jsx)(t.p,{children:"In future releases of Hudi, we aim to fully integrate the benefits of the older writer-side index into the new bloom index. Nonetheless, this demonstrates the adaptability of Hudi\u2019s indexing system to handle different types of indexes on the table."}),"\n",(0,s.jsx)(t.h3,{id:"partitioning-replaced-by-expression-indexes",children:"Partitioning replaced by Expression Indexes"}),"\n",(0,s.jsxs)(t.p,{children:["An astute reader may have noticed above that the indexing is supported on a function/expression on a column. Hudi 1.0 introduces expression indexes similar to ",(0,s.jsx)(t.a,{href:"https://www.postgresql.org/docs/current/indexes-expressional.html",children:"Postgres"})," to generalize a two-decade-old relic in the data lake ecosystem - partitioning! At a high level, partitioning on the data lake divides the table into folders based on a column or a mapping function (partitioning function). When queries or operations are performed against the table, they can efficiently skip entire partitions (folders), reducing the amount of metadata and data involved. This is very effective since data lake tables span 100s of thousands of files. But, as simple as it sounds, this is one of the ",(0,s.jsx)(t.a,{href:"https://www.onehouse.ai/blog/knowing-your-data-partitioning-vices-on-the-data-lakehouse",children:"most common pitfalls"})," around performance on the data lake, where new users use it like an index by partitioning based on a high cardinality column, resulting in lots of storage partitions/tiny files and abysmal write/query performance for no good reason. Further, tying storage organization to partitioning makes it inflexible to changes."]}),"\n",(0,s.jsxs)("div",{style:{textAlign:"center"},children:[(0,s.jsx)("img",{src:"/assets/images/expression-index-date-partitioning.png",alt:"Timeline actions"}),(0,s.jsx)("p",{align:"center",children:"Figure: Shows index on a date expression when a different column physically partitions data"})]}),"\n",(0,s.jsxs)(t.p,{children:["Hudi 1.0 treats partitions as a ",(0,s.jsx)(t.a,{href:"/docs/sql_queries#query-using-column-stats-expression-index",children:"coarse-grained index"})," on a column value or an expression of a column, as they should have been. To support the efficiency of skipping entire storage paths/folders, Hudi 1.0 introduces partition stats indexes that aggregate these statistics on the storage partition path level, in addition to doing so at the file level. Now, users can create different types of indexes on columns to achieve the effects of partitioning in a streamlined fashion using fewer concepts to achieve the same results. Along with support for other 1.x features, partition stats and expression indexes support will be extended to other engines like Presto, Trino, Apache Doris, and Starrocks with the 1.1 release."]}),"\n",(0,s.jsx)(t.h3,{id:"efficient-partial-updates",children:"Efficient Partial Updates"}),"\n",(0,s.jsxs)(t.p,{children:["Managing large-scale datasets often involves making fine-grained changes to records. Hudi has long supported ",(0,s.jsx)(t.a,{href:"/docs/0.15.0/record_payload#partialupdateavropayload",children:"partial updates"})," to records via the record payload interface. However, this usually comes at the cost of sacrificing engine-native performance by moving away from specific objects used by engines to represent rows. As users have embraced Hudi for incremental SQL pipelines on top of dbt/Spark or Flink Dynamic Tables, there was a rise in interest in making this much more straightforward and mainstream. Hudi 1.0 introduces first-class support for ",(0,s.jsx)(t.strong,{children:"partial updates"})," at the log format level, enabling ",(0,s.jsx)(t.em,{children:"MERGE INTO"})," SQL statements to modify only the changed fields of a record instead of rewriting/reprocessing the entire row."]}),"\n",(0,s.jsxs)(t.p,{children:["Partial updates improve query and write performance simultaneously by reducing write amplification for writes and the amount of data read by Merge-on-Read snapshot queries. It also achieves much better storage utilization due to fewer bytes stored and improved compute efficiency over existing partial update support by retaining vectorized engine-native processing. Using the 1TB Brooklyn benchmark for write performance, we observe about ",(0,s.jsx)(t.strong,{children:"2.6x"})," improvement in Merge-on-Read query performance due to an ",(0,s.jsx)(t.strong,{children:"85%"})," reduction in write amplification. For random write workloads, the gains can be much more pronounced. Below shows a second benchmark for partial updates, 1TB MOR table, 1000 partitions, 80% random updates. 3/100 columns randomly updated."]}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{style:{textAlign:"left"}}),(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"Full Record Update"}),(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"Partial Update"}),(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"Gains"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:(0,s.jsx)(t.strong,{children:"Update latency (s)"})}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"2072"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"1429"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"1.4x"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:(0,s.jsx)(t.strong,{children:"Bytes written (GB)"})}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"891.7"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"12.7"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"70.2x"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:(0,s.jsx)(t.strong,{children:"Query latency (s)"})}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"164"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"29"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"5.7x"})]})]})]}),"\n",(0,s.jsxs)(t.p,{children:["This also lays the foundation for managing unstructured and multimodal data inside a Hudi table and supporting ",(0,s.jsx)(t.a,{href:"https://github.com/apache/hudi/pull/11733",children:"wide tables"})," efficiently for machine learning use cases."]}),"\n",(0,s.jsx)(t.h3,{id:"merge-modes-and-custom-mergers",children:"Merge Modes and Custom Mergers"}),"\n",(0,s.jsxs)(t.p,{children:["One of the most unique capabilities Hudi provides is how it helps process streaming data. Specifically, Hudi has, since the very beginning, supported merging records pre-write (to reduce write amplification), during write (against an existing record in storage with the same record key) and reads (for MoR snapshot queries), using a ",(0,s.jsx)(t.em,{children:"precombine"})," or ",(0,s.jsx)(t.em,{children:"ordering"})," field. This helps implement ",(0,s.jsx)(t.a,{href:"https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/",children:"event time processing"})," semantics, widely supported by stream processing systems, on data lakehouse storage. This helps integrate late-arriving data into Hudi tables without causing weird movement of record state back in time. For example, if an older database CDC record arrives late and gets committed as the new value, the state of the record would be incorrect even though the writes to the table themselves were serialized in some order."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.img,{alt:"event time ordering",src:i(90534).A+"",width:"1360",height:"490"}),"\n",(0,s.jsx)("p",{align:"center",children:"Figure: Shows EVENT_TIME_ORDERING where merging reconciles state based on the highest event_time"})]}),"\n",(0,s.jsxs)(t.p,{children:["Prior Hudi versions supported this functionality through the record payload interface with built-in support for a pre-combine field on the default payloads. Hudi 1.0 makes these two styles of processing and merging changes first class by introducing ",(0,s.jsx)(t.a,{href:"/docs/record_merger",children:"merge modes"})," within Hudi."]}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"Merge Mode"}),(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"What does it do?"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"COMMIT_TIME_ORDERING"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"Picks record with highest completion time/instant as final merge result  i.e., standard relational semantics or arrival time processing"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"EVENT_TIME_ORDERING"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"Default (for now, to ease migration).\vPicks record with the highest value for a user-specified ordering/precombine field as the final merge result."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"CUSTOM"}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"Uses a user-provided RecordMerger implementation to produce final merge result (similar to stream processing processor APIs)"})]})]})]}),"\n",(0,s.jsxs)(t.p,{children:["Like partial update support, the new ",(0,s.jsx)(t.em,{children:"RecordMerger"})," API provides a more efficient engine-native alternative to the older RecordPayload interface through native objects and vectorized processing on EVENT_TIME_ORDERING merge modes. In future versions, we intend to change the default to COMMIT_TIME_ORDERING to provide simple, out-of-the-box relational table semantics."]}),"\n",(0,s.jsx)(t.h3,{id:"non-blocking-concurrency-control-for-streaming-writes",children:"Non-Blocking Concurrency Control for Streaming Writes"}),"\n",(0,s.jsx)(t.p,{children:"We have expressed dissatisfaction with the optimistic concurrency control approaches employed on the data lakehouse since they appear to paint the problem with a broad brush without paying attention to the nuances of the lakehouse workloads. Specifically, contention is much more common in data lakehouses, even for Hudi, the only data lakehouse storage project capable of asynchronously compacting delta updates without failing or causing retries on the writer. Ultimately, data lakehouses are high-throughput systems, and failing concurrent writers to handle contention can waste expensive compute clusters. Streaming and high-frequency writes often require fine-grained concurrency control to prevent bottlenecks."}),"\n",(0,s.jsxs)(t.p,{children:["Hudi 1.0 introduces a new ",(0,s.jsx)(t.strong,{children:"non-blocking concurrency control (NBCC)"})," designed explicitly for data lakehouse workloads, using years of experience gained supporting some of the largest data lakes on the planet in the Hudi community. NBCC enables simultaneous writing from multiple writers and compaction of the same record without blocking any involved processes. This is achieved by simply lightweight distributed locks and TrueTime semantics discussed above. (see ",(0,s.jsx)(t.a,{href:"https://github.com/apache/hudi/blob/master/rfc/rfc-66/rfc-66.md",children:"RFC-66"})," for more)"]}),"\n",(0,s.jsxs)("div",{style:{textAlign:"center"},children:[(0,s.jsx)("img",{src:"/assets/images/nbcc_partial_updates.gif",alt:"NBCC"}),(0,s.jsx)("p",{align:"center",children:"Figure: Two streaming jobs in action writing to the same records concurrently on different columns."})]}),"\n",(0,s.jsxs)(t.p,{children:["NBCC operates with streaming semantics, tying together concepts from previous sections. Data necessary to compute table updates are emitted from an upstream source, and changes and partial updates can be merged in any of the merge modes above. For example, in the figure above, two independent Flink jobs enrich different table columns in parallel, a pervasive pattern seen in stream processing use cases. Check out this ",(0,s.jsx)(t.a,{href:"https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control",children:"blog"})," for a full demo. We also expect to support NBCC across other compute engines in future releases."]}),"\n",(0,s.jsx)(t.h3,{id:"backwards-compatible-writing",children:"Backwards Compatible Writing"}),"\n",(0,s.jsx)(t.p,{children:"If you are wondering: \u201cAll of this sounds cool, but how do I upgrade?\u201d we have put a lot of thought into making that seamless. Hudi has always supported backward-compatible reads to older table versions. Table versions are stored in table properties unrelated to the software binary version. The supported way of upgrading has been to first migrate readers/query engines to new software binary versions and then upgrade the writers, which will auto-upgrade the table if there is a table version change between the old and new software binary versions. Upon community feedback, users expressed the need to be able to do upgrades on the writers without waiting on the reader side upgrades and reduce any additional coordination necessary within different teams."}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.img,{alt:"Indexes",src:i(45960).A+"",width:"1481",height:"825"}),"\n",(0,s.jsx)("p",{align:"center",children:"Figure: 4-step process for painless rolling upgrades to Hudi 1.0"})]}),"\n",(0,s.jsxs)(t.p,{children:["Hudi 1.0 introduces backward-compatible writing to achieve this in 4 steps, as described above. Hudi 1.0 also automatically handles any checkpoint translation necessary as we switch to completion time-based processing semantics for incremental and CDC queries. The Hudi metadata table has to be temporarily disabled during this upgrade process but can be turned on once the upgrade is completed successfully. Please read the ",(0,s.jsx)(t.a,{href:"/releases/release-1.0.0",children:"release notes"})," carefully to plan your migration."]}),"\n",(0,s.jsx)(t.h2,{id:"whats-next",children:"What\u2019s Next?"}),"\n",(0,s.jsx)(t.p,{children:"Hudi 1.0 is a testament to the power of open-source collaboration. This release embodies the contributions of 60+ developers, maintainers, and users who have actively shaped its roadmap. We sincerely thank the Apache Hudi community for their passion, feedback, and unwavering support."}),"\n",(0,s.jsxs)(t.p,{children:["The release of Hudi 1.0 is just the beginning. Our current ",(0,s.jsx)(t.a,{href:"/roadmap",children:"roadmap"})," includes exciting developments across the following planned releases:"]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"1.0.1"}),": First bug fix, patch release on top of 1.0, which hardens the functionality above and makes it easier. We intend to publish additional patch releases to aid migration to 1.0 as the bridge release for the community from 0.x."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"1.1"}),":  Faster writer code path rewrite, new indexes like bitmap/vector search, granular record-level change encoding, Hudi storage engine APIs, abstractions for cross-format interop."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"1.2"}),": Multi-table transactions, platform services for reverse streaming from Hudi etc., Multi-modal data + indexing, NBCC clustering"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"2.0"}),": Server components for DLMS, caching and metaserver functionality."]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"Hudi releases are drafted collaboratively by the community. If you don\u2019t see something you like here, please help shape the roadmap together."}),"\n",(0,s.jsx)(t.h2,{id:"get-started-with-apache-hudi-10",children:"Get Started with Apache Hudi 1.0"}),"\n",(0,s.jsx)(t.p,{children:"Are you ready to experience the future of data lakehouses? Here\u2019s how you can dive into Hudi 1.0:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["Documentation: Explore Hudi\u2019s ",(0,s.jsx)(t.a,{href:"/docs/overview",children:"Documentation"})," and learn the ",(0,s.jsx)(t.a,{href:"/docs/hudi_stack",children:"concepts"}),"."]}),"\n",(0,s.jsxs)(t.li,{children:["Quickstart Guide: Follow the ",(0,s.jsx)(t.a,{href:"/docs/quick-start-guide",children:"Quickstart Guide"})," to set up your first Hudi project."]}),"\n",(0,s.jsxs)(t.li,{children:["Upgrading from a previous version?  Follow the ",(0,s.jsx)(t.a,{href:"/releases/release-1.0.0#migration-guide",children:"migration guide"})," and contact the Hudi OSS community for help."]}),"\n",(0,s.jsxs)(t.li,{children:["Join the Community: Participate in discussions on the ",(0,s.jsx)(t.a,{href:"https://hudi.apache.org/community/get-involved/",children:"Hudi Mailing List"}),", ",(0,s.jsx)(t.a,{href:"https://join.slack.com/t/apache-hudi/shared_invite/zt-2ggm1fub8-_yt4Reu9djwqqVRFC7X49g",children:"Slack"})," and ",(0,s.jsx)(t.a,{href:"https://github.com/apache/hudi/issues",children:"GitHub"}),"."]}),"\n",(0,s.jsxs)(t.li,{children:["Follow us on social media: ",(0,s.jsx)(t.a,{href:"https://www.linkedin.com/company/apache-hudi/?viewAsMember=true",children:"Linkedin"}),", ",(0,s.jsx)(t.a,{href:"https://twitter.com/ApacheHudi",children:"X/Twitter"}),"."]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"We can\u2019t wait to see what you build with Apache Hudi 1.0. Let\u2019s work together to shape the future of data lakehouses!"}),"\n",(0,s.jsx)(t.p,{children:"Crafted with passion for the Apache Hudi community."})]})}function h(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},28453(e,t,i){i.d(t,{R:()=>r,x:()=>o});var n=i(96540);const s={},a=n.createContext(s);function r(e){const t=n.useContext(a);return n.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),n.createElement(a.Provider,{value:t},e.children)}},45960(e,t,i){i.d(t,{A:()=>n});const n=i.p+"assets/images/backwards-compat-writing-6299b055646e2577964069b755ee1f3d.png"},64341(e){e.exports=JSON.parse('{"permalink":"/blog/2024/12/16/announcing-hudi-1-0-0","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-12-16-announcing-hudi-1-0-0.mdx","source":"@site/blog/2024-12-16-announcing-hudi-1-0-0.mdx","title":"Announcing Apache Hudi 1.0 and the Next Generation of Data Lakehouses","description":"Overview","date":"2024-12-16T00:00:00.000Z","tags":[{"inline":true,"label":"timeline","permalink":"/blog/tags/timeline"},{"inline":true,"label":"design","permalink":"/blog/tags/design"},{"inline":true,"label":"release","permalink":"/blog/tags/release"},{"inline":true,"label":"streaming ingestion","permalink":"/blog/tags/streaming-ingestion"},{"inline":true,"label":"multi-writer","permalink":"/blog/tags/multi-writer"},{"inline":true,"label":"concurrency-control","permalink":"/blog/tags/concurrency-control"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"}],"readingTime":61.49,"hasTruncateMarker":false,"authors":[{"name":"Vinoth Chandar","key":null,"page":null}],"frontMatter":{"title":"Announcing Apache Hudi 1.0 and the Next Generation of Data Lakehouses","excerpt":"game-changing major release, that reimagines Hudi and Data Lakehouses.","author":"Vinoth Chandar","category":"blog","image":"/assets/images/blog/dlms-hierarchy.png","tags":["timeline","design","release","streaming ingestion","multi-writer","concurrency-control","blog"]},"unlisted":false,"prevItem":{"title":"How lakehouse handles concurrent Read and Writes","permalink":"/blog/2024/12/28/how-lakehouse-handles-concurrent-read-and-writes"},"nextItem":{"title":"Introducing Hudi\'s Non-blocking Concurrency Control for streaming, high-frequency writes","permalink":"/blog/2024/12/06/non-blocking-concurrency-control"}}')},90534(e,t,i){i.d(t,{A:()=>n});const n=i.p+"assets/images/event-time-ordering-merge-mode-c8164e035840388bf4290fa81ac6262a.png"}}]);