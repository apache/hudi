"use strict";(globalThis.webpackChunkhudi=globalThis.webpackChunkhudi||[]).push([[36858],{11470(e,a,i){i.d(a,{A:()=>S});var n=i(96540),r=i(34164),o=i(17559),s=i(23104),t=i(56347),d=i(205),c=i(57485),l=i(31682),h=i(70679);function u(e){return n.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,n.isValidElement)(e)&&function(e){const{props:a}=e;return!!a&&"object"==typeof a&&"value"in a}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function p(e){const{values:a,children:i}=e;return(0,n.useMemo)(()=>{const e=a??function(e){return u(e).map(({props:{value:e,label:a,attributes:i,default:n}})=>({value:e,label:a,attributes:i,default:n}))}(i);return function(e){const a=(0,l.XI)(e,(e,a)=>e.value===a.value);if(a.length>0)throw new Error(`Docusaurus error: Duplicate values "${a.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[a,i])}function m({value:e,tabValues:a}){return a.some(a=>a.value===e)}function f({queryString:e=!1,groupId:a}){const i=(0,t.W6)(),r=function({queryString:e=!1,groupId:a}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:e,groupId:a});return[(0,c.aZ)(r),(0,n.useCallback)(e=>{if(!r)return;const a=new URLSearchParams(i.location.search);a.set(r,e),i.replace({...i.location,search:a.toString()})},[r,i])]}function g(e){const{defaultValue:a,queryString:i=!1,groupId:r}=e,o=p(e),[s,t]=(0,n.useState)(()=>function({defaultValue:e,tabValues:a}){if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${a.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const i=a.find(e=>e.default)??a[0];if(!i)throw new Error("Unexpected error: 0 tabValues");return i.value}({defaultValue:a,tabValues:o})),[c,l]=f({queryString:i,groupId:r}),[u,g]=function({groupId:e}){const a=function(e){return e?`docusaurus.tab.${e}`:null}(e),[i,r]=(0,h.Dv)(a);return[i,(0,n.useCallback)(e=>{a&&r.set(e)},[a,r])]}({groupId:r}),b=(()=>{const e=c??u;return m({value:e,tabValues:o})?e:null})();(0,d.A)(()=>{b&&t(b)},[b]);return{selectedValue:s,selectValue:(0,n.useCallback)(e=>{if(!m({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);t(e),l(e),g(e)},[l,g,o]),tabValues:o}}var b=i(92303);const j="tabList__CuJ",x="tabItem_LNqP";var v=i(74848);function y({className:e,block:a,selectedValue:i,selectValue:n,tabValues:o}){const t=[],{blockElementScrollPositionUntilNextRender:d}=(0,s.a_)(),c=e=>{const a=e.currentTarget,r=t.indexOf(a),s=o[r].value;s!==i&&(d(a),n(s))},l=e=>{let a=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const i=t.indexOf(e.currentTarget)+1;a=t[i]??t[0];break}case"ArrowLeft":{const i=t.indexOf(e.currentTarget)-1;a=t[i]??t[t.length-1];break}}a?.focus()};return(0,v.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":a},e),children:o.map(({value:e,label:a,attributes:n})=>(0,v.jsx)("li",{role:"tab",tabIndex:i===e?0:-1,"aria-selected":i===e,ref:e=>{t.push(e)},onKeyDown:l,onClick:c,...n,className:(0,r.A)("tabs__item",x,n?.className,{"tabs__item--active":i===e}),children:a??e},e))})}function w({lazy:e,children:a,selectedValue:i}){const o=(Array.isArray(a)?a:[a]).filter(Boolean);if(e){const e=o.find(e=>e.props.value===i);return e?(0,n.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,v.jsx)("div",{className:"margin-top--md",children:o.map((e,a)=>(0,n.cloneElement)(e,{key:a,hidden:e.props.value!==i}))})}function k(e){const a=g(e);return(0,v.jsxs)("div",{className:(0,r.A)(o.G.tabs.container,"tabs-container",j),children:[(0,v.jsx)(y,{...a,...e}),(0,v.jsx)(w,{...a,...e})]})}function S(e){const a=(0,b.A)();return(0,v.jsx)(k,{...e,children:u(e.children)},String(a))}},19365(e,a,i){i.d(a,{A:()=>s});i(96540);var n=i(34164);const r="tabItem_Ymn6";var o=i(74848);function s({children:e,hidden:a,className:i}){return(0,o.jsx)("div",{role:"tabpanel",className:(0,n.A)(r,i),hidden:a,children:e})}},28453(e,a,i){i.d(a,{R:()=>s,x:()=>t});var n=i(96540);const r={},o=n.createContext(r);function s(e){const a=n.useContext(o);return n.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function t(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),n.createElement(o.Provider,{value:a},e.children)}},65878(e,a,i){i.r(a),i.d(a,{assets:()=>d,contentTitle:()=>t,default:()=>h,frontMatter:()=>s,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"release-0.15","title":"Release 0.15","description":"This page contains release notes for all Apache Hudi 0.15.x releases, including:","source":"@site/releases/release-0.15.md","sourceDirName":".","slug":"/release-0.15","permalink":"/releases/release-0.15","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Release 0.15","layout":"releases","toc":true},"sidebar":"releases","previous":{"title":"Release 1.0","permalink":"/releases/release-1.0"},"next":{"title":"Release 0.14","permalink":"/releases/release-0.14"}}');var r=i(74848),o=i(28453);i(11470),i(19365);const s={title:"Release 0.15",layout:"releases",toc:!0},t=void 0,d={},c=[{value:"Release 0.15.0",id:"release-0150",level:2},{value:"Migration Guide",id:"migration-guide",level:2},{value:"Bundle Updates",id:"bundle-updates",level:3},{value:"New Spark Bundles",id:"new-spark-bundles",level:4},{value:"New Utilities Bundles for Scala 2.13",id:"new-utilities-bundles-for-scala-213",level:4},{value:"New and Deprecated Flink Bundles",id:"new-and-deprecated-flink-bundles",level:4},{value:"Module and API Changes",id:"module-and-api-changes",level:3},{value:"Hudi Storage and I/O Abstractions",id:"hudi-storage-and-io-abstractions",level:4},{value:"Module Changes",id:"module-changes",level:4},{value:"Lock Provider API Change",id:"lock-provider-api-change",level:4},{value:"Behavior Changes",id:"behavior-changes",level:3},{value:"Improving Cleaning Table Service",id:"improving-cleaning-table-service",level:4},{value:"Allowing Duplicates on Inserts",id:"allowing-duplicates-on-inserts",level:4},{value:"Sync MOR Snapshot to The Metastore",id:"sync-mor-snapshot-to-the-metastore",level:4},{value:"Flink Option Default Flips",id:"flink-option-default-flips",level:4},{value:"Release Highlights",id:"release-highlights",level:2},{value:"Hudi Storage and I/O Abstractions",id:"hudi-storage-and-io-abstractions-1",level:3},{value:"Engine Support",id:"engine-support",level:3},{value:"Spark 3.5 and Scala 2.13 Support",id:"spark-35-and-scala-213-support",level:4},{value:"Flink 1.18 Support",id:"flink-118-support",level:4},{value:"Hudi-Native HFile Reader",id:"hudi-native-hfile-reader",level:3},{value:"New Features in Hudi Utilities",id:"new-features-in-hudi-utilities",level:3},{value:"StreamContext and SourceProfile Interfaces",id:"streamcontext-and-sourceprofile-interfaces",level:4},{value:"Enhanced Proto Kafka Source Support",id:"enhanced-proto-kafka-source-support",level:4},{value:"Ignoring Checkpoint in Hudi Streamer",id:"ignoring-checkpoint-in-hudi-streamer",level:4},{value:"Meta Sync Improvements",id:"meta-sync-improvements",level:3},{value:"Paralleled Listing in Glue Catalog Sync",id:"paralleled-listing-in-glue-catalog-sync",level:4},{value:"BigQuery Sync Optimization with Metadata Table",id:"bigquery-sync-optimization-with-metadata-table",level:4},{value:"Metrics Reporting to M3",id:"metrics-reporting-to-m3",level:3},{value:"Other Features and Improvements",id:"other-features-and-improvements",level:3},{value:"Schema Exception Classification",id:"schema-exception-classification",level:4},{value:"Record Size Estimation Improvement",id:"record-size-estimation-improvement",level:4},{value:"Using <code>s3</code> Scheme for Athena",id:"using-s3-scheme-for-athena",level:4},{value:"Known Regressions",id:"known-regressions",level:2},{value:"Raw Release Notes",id:"raw-release-notes",level:2}];function l(e){const a={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",h4:"h4",hr:"hr",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(a.p,{children:"This page contains release notes for all Apache Hudi 0.15.x releases, including:"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"#release-0150",children:"Release 0.15.0"})}),"\n"]}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.h2,{id:"release-0150",children:(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/releases/tag/release-0.15.0",children:"Release 0.15.0"})}),"\n",(0,r.jsxs)(a.p,{children:["Apache Hudi 0.15.0 release brings enhanced engine integration, new features, and improvements in several areas. These\ninclude Spark 3.5 and Scala 2.13 support, Flink 1.18 support, better Trino Hudi native connector support with newly\nintroduced Hadoop-agnostic storage and I/O abstractions. We encourage users to review\nthe ",(0,r.jsx)(a.a,{href:"#release-highlights",children:"Release Highlights"})," and ",(0,r.jsx)(a.a,{href:"#migration-guide-overview",children:"Migration Guide"})," down below on\nrelevant ",(0,r.jsx)(a.a,{href:"#module-and-api-changes",children:"module and API changes"})," and\n",(0,r.jsx)(a.a,{href:"#behavior-changes",children:"behavior changes"})," before using the 0.15.0 release."]}),"\n",(0,r.jsx)(a.h2,{id:"migration-guide",children:"Migration Guide"}),"\n",(0,r.jsxs)(a.p,{children:["This release keeps the same table version (",(0,r.jsx)(a.code,{children:"6"}),") as ",(0,r.jsx)(a.a,{href:"/releases/release-0.14#release-0140",children:"0.14.0 release"}),", and there is no need for\na table version upgrade if you are upgrading from 0.14.0. There are a\nfew ",(0,r.jsx)(a.a,{href:"#module-and-api-changes",children:"module and API changes"}),"\nand ",(0,r.jsx)(a.a,{href:"#behavior-changes",children:"behavior changes"})," as\ndescribed below, and users are expected to take action accordingly before using 0.15.0 release."]}),"\n",(0,r.jsx)(a.admonition,{type:"caution",children:(0,r.jsx)(a.p,{children:"If migrating from an older release (pre-0.14.0), please also check the upgrade instructions from each older release in\nsequence."})}),"\n",(0,r.jsx)(a.h3,{id:"bundle-updates",children:"Bundle Updates"}),"\n",(0,r.jsx)(a.h4,{id:"new-spark-bundles",children:"New Spark Bundles"}),"\n",(0,r.jsx)(a.p,{children:"We have expanded Hudi support to Spark 3.5 with two new bundles:"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:["Spark 3.5 and Scala\n2.12: ",(0,r.jsx)(a.a,{href:"https://mvnrepository.com/artifact/org.apache.hudi/hudi-spark3.5-bundle_2.12",children:"hudi-spark3.5-bundle_2.12"})]}),"\n",(0,r.jsxs)(a.li,{children:["Spark 3.5 and Scala\n2.13: ",(0,r.jsx)(a.a,{href:"https://mvnrepository.com/artifact/org.apache.hudi/hudi-spark3.5-bundle_2.13",children:"hudi-spark3.5-bundle_2.13"})]}),"\n"]}),"\n",(0,r.jsx)(a.h4,{id:"new-utilities-bundles-for-scala-213",children:"New Utilities Bundles for Scala 2.13"}),"\n",(0,r.jsxs)(a.p,{children:["Besides adding a bundle for Spark 3.5 and Scala 2.13, we have added new utilities bundles to use with Scala\n2.13, ",(0,r.jsx)(a.a,{href:"https://mvnrepository.com/artifact/org.apache.hudi/hudi-utilities-bundle_2.13",children:"hudi-utilities-bundle_2.13"}),"\nand ",(0,r.jsx)(a.a,{href:"https://mvnrepository.com/artifact/org.apache.hudi/hudi-utilities-slim-bundle_2.13",children:"hudi-utilities-slim-bundle_2.13"}),"."]}),"\n",(0,r.jsx)(a.h4,{id:"new-and-deprecated-flink-bundles",children:"New and Deprecated Flink Bundles"}),"\n",(0,r.jsxs)(a.p,{children:["We have expanded Hudi support to Flink 1.18 with a new\nbundle, ",(0,r.jsx)(a.a,{href:"https://mvnrepository.com/artifact/org.apache.hudi/hudi-flink1.18-bundle",children:"hudi-flink1.18-bundle"}),". This release\nremoves Hudi support on Flink 1.13."]}),"\n",(0,r.jsx)(a.h3,{id:"module-and-api-changes",children:"Module and API Changes"}),"\n",(0,r.jsx)(a.h4,{id:"hudi-storage-and-io-abstractions",children:"Hudi Storage and I/O Abstractions"}),"\n",(0,r.jsxs)(a.p,{children:["This release introduces new storage and I/O abstractions that are Hadoop-agnostic to improve integration with query\nengines, including ",(0,r.jsx)(a.a,{href:"https://trino.io/",children:"Trino"}),", which uses its own native File System APIs. Core Hudi classes\nincluding ",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableMetaClient.java",children:(0,r.jsx)(a.code,{children:"HoodieTableMetaClient"})}),",\n",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieBaseFile.java",children:(0,r.jsx)(a.code,{children:"HoodieBaseFile"})}),",\n",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieLogFile.java",children:(0,r.jsx)(a.code,{children:"HoodieLogFile"})}),",\n",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-common/src/main/java/org/apache/hudi/common/engine/HoodieEngineContext.java",children:(0,r.jsx)(a.code,{children:"HoodieEngineContext"})}),",\netc., now depend on new\nstorage and I/O classes. If you\u2019re using these classes directly in your applications, you need to change your\nintegration code and usage. For more details, check out ",(0,r.jsx)(a.a,{href:"#hudi-storage-and-io-abstractions-1",children:"this section"}),"."]}),"\n",(0,r.jsx)(a.h4,{id:"module-changes",children:"Module Changes"}),"\n",(0,r.jsx)(a.p,{children:"As part of introducing new storage and I/O abstractions and making core reader logic Hadoop-agnostic, this release\nrestructures the Hudi modules to clearly reflect the layering. Specifically,"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsxs)(a.a,{href:"https://github.com/apache/hudi/tree/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-io",children:[(0,r.jsx)(a.code,{children:"hudi-io"})," module"]})," is added\nfor I/O related functionality, and the Hudi-native HFile reader implementation sits inside this new module;"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsxs)(a.a,{href:"https://github.com/apache/hudi/tree/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-common",children:[(0,r.jsx)(a.code,{children:"hudi-common"})," module"]}),"\ncontains the core\nimplementation of the ",(0,r.jsx)(a.a,{href:"https://hudi.apache.org/tech-specs",children:"Apache Hudi Technical Specification"})," and is now\nHadoop-independent;"]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsxs)(a.a,{href:"https://github.com/apache/hudi/tree/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-hadoop-common",children:[(0,r.jsx)(a.code,{children:"hudi-hadoop-common"})," module"]}),"\ncontains\nimplementation based on Hadoop file system APIs to be used\nwith ",(0,r.jsxs)(a.a,{href:"https://github.com/apache/hudi/tree/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-common",children:[(0,r.jsx)(a.code,{children:"hudi-common"})," module"]}),"\non engines\nincluding Spark,\nFlink, Hive, and Presto."]}),"\n"]}),"\n",(0,r.jsxs)(a.p,{children:["If you\nuse ",(0,r.jsxs)(a.a,{href:"https://github.com/apache/hudi/tree/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-common",children:[(0,r.jsx)(a.code,{children:"hudi-common"})," module"]})," as\nthe dependency before and Hadoop file system APIs and implementations, you should include all three\nmodules, ",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/tree/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-io",children:(0,r.jsx)(a.code,{children:"hudi-io"})}),", ",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/tree/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-common",children:(0,r.jsx)(a.code,{children:"hudi-common"})}),",\nand ",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/tree/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-hadoop-common",children:(0,r.jsx)(a.code,{children:"hudi-hadoop-common"})}),",\nas the dependency now. Note that, Presto and Trino will be released based on Hudi 0.15.0 release with such changes."]}),"\n",(0,r.jsx)(a.h4,{id:"lock-provider-api-change",children:"Lock Provider API Change"}),"\n",(0,r.jsxs)(a.p,{children:["The ",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/LockManager.java#L125",children:(0,r.jsx)(a.code,{children:"LockProvider"})}),"\ninstantiation now expects\nthe ",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-io/src/main/java/org/apache/hudi/storage/StorageConfiguration.java",children:(0,r.jsx)(a.code,{children:"StorageConfiguration"})}),"\ninstance as the second argument of the constructor. If you\nextend ",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-common/src/main/java/org/apache/hudi/common/lock/LockProvider.java",children:(0,r.jsx)(a.code,{children:"LockProvider"})}),"\nto implement a custom lock provider\nbefore, you need to change the constructor to match the aforementioned constructor signature. Here's an example:"]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-java",children:"public class XYZLockProvider implements LockProvider<String>, Serializable {\n  ...\n\n    public XYZLockProvider(final LockConfiguration lockConfiguration, final StorageConfiguration<?> conf) {\n      ...\n    }\n  \n  ...\n}\n"})}),"\n",(0,r.jsx)(a.h3,{id:"behavior-changes",children:"Behavior Changes"}),"\n",(0,r.jsx)(a.h4,{id:"improving-cleaning-table-service",children:"Improving Cleaning Table Service"}),"\n",(0,r.jsxs)(a.p,{children:["We have improved the default cleaner behavior to only schedule a new cleaner plan if there is no inflight plan, by\nflipping the default of ",(0,r.jsx)(a.a,{href:"/docs/configurations#hoodiecleanallowmultiple",children:(0,r.jsx)(a.code,{children:"hoodie.clean.allow.multiple"})})," from ",(0,r.jsx)(a.code,{children:"true"}),"\nto ",(0,r.jsx)(a.code,{children:"false"}),". This simplifies cleaning table service when the metadata table is enabled. The config is deprecated now and\nwill be removed after the next release."]}),"\n",(0,r.jsx)(a.h4,{id:"allowing-duplicates-on-inserts",children:"Allowing Duplicates on Inserts"}),"\n",(0,r.jsxs)(a.p,{children:["We now allow duplicate keys on ",(0,r.jsx)(a.code,{children:"INSERT"})," operation by default, even if inserts are routed to merge with an existing\nfile (for ensuring file sizing), by flipping the default\nof ",(0,r.jsx)(a.a,{href:"/docs/configurations#hoodiemergeallowduplicateoninserts",children:(0,r.jsx)(a.code,{children:"hoodie.merge.allow.duplicate.on.inserts"})})," from ",(0,r.jsx)(a.code,{children:"false"}),"\nto ",(0,r.jsx)(a.code,{children:"true"}),". This is only relevant for ",(0,r.jsx)(a.code,{children:"INSERT"})," operation, since ",(0,r.jsx)(a.code,{children:"UPSERT"}),", and ",(0,r.jsx)(a.code,{children:"DELETE"})," operations always ensure unique\nkey constraint."]}),"\n",(0,r.jsx)(a.h4,{id:"sync-mor-snapshot-to-the-metastore",children:"Sync MOR Snapshot to The Metastore"}),"\n",(0,r.jsxs)(a.p,{children:["To better support snapshot queries on MOR tables on OLAP engines, the MOR snapshot or RT is synced to the metastore with\nthe table name by default, by flipping the default\nof ",(0,r.jsx)(a.a,{href:"/docs/configurations#hoodiemetasyncsync_snapshot_with_table_name",children:(0,r.jsx)(a.code,{children:"hoodie.meta.sync.sync_snapshot_with_table_name"})}),"\nfrom ",(0,r.jsx)(a.code,{children:"false"}),"\nto ",(0,r.jsx)(a.code,{children:"true"}),"."]}),"\n",(0,r.jsx)(a.h4,{id:"flink-option-default-flips",children:"Flink Option Default Flips"}),"\n",(0,r.jsxs)(a.p,{children:["The default value of ",(0,r.jsx)(a.a,{href:"/docs/configurations#readstreamingskip_clustering",children:(0,r.jsx)(a.code,{children:"read.streaming.skip_clustering"})})," is ",(0,r.jsx)(a.code,{children:"false"}),"\nbefore this release, which could cause the situation that Flink streaming reading reads the replaced file slices of\nclustering and duplicated data (same concern\nfor ",(0,r.jsx)(a.a,{href:"/docs/configurations#readstreamingskip_compaction",children:(0,r.jsx)(a.code,{children:"read.streaming.skip_compaction"})}),"). The\n0.15.0 release makes Flink streaming read to skip clustering and compaction instants for all cases to avoid reading the\nrelevant file slices, by flipping the default\nof ",(0,r.jsx)(a.a,{href:"/docs/configurations#readstreamingskip_clustering",children:(0,r.jsx)(a.code,{children:"read.streaming.skip_clustering"})}),"\nand ",(0,r.jsx)(a.a,{href:"/docs/configurations#readstreamingskip_compaction",children:(0,r.jsx)(a.code,{children:"read.streaming.skip_compaction"})}),"\nfrom ",(0,r.jsx)(a.code,{children:"false"})," to ",(0,r.jsx)(a.code,{children:"true"}),"."]}),"\n",(0,r.jsx)(a.h2,{id:"release-highlights",children:"Release Highlights"}),"\n",(0,r.jsx)(a.h3,{id:"hudi-storage-and-io-abstractions-1",children:"Hudi Storage and I/O Abstractions"}),"\n",(0,r.jsxs)(a.p,{children:["To provide better integration experience with query engines including ",(0,r.jsx)(a.a,{href:"https://trino.io/",children:"Trino"})," which uses its own\nnative File System APIs, this release introduces new storage and I/O abstractions that are Hadoop-agnostic."]}),"\n",(0,r.jsxs)(a.p,{children:["To be specific, the release introduces Hudi storage\nabstraction ",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-io/src/main/java/org/apache/hudi/storage/HoodieStorage.java",children:(0,r.jsx)(a.code,{children:"HoodieStorage"})}),"\nwhich provides all I/O APIs to read and write files and directories on storage, such as ",(0,r.jsx)(a.code,{children:"open"}),", ",(0,r.jsx)(a.code,{children:"read"}),", etc. This class\ncan be extended to implement storage layer optimizations like caching, federated storage layout, hot/cold storage\nseparation, etc. This class needs to be implemented based on particular systems, such as\nHadoop\u2019s ",(0,r.jsx)(a.a,{href:"https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/FileSystem.html",children:(0,r.jsx)(a.code,{children:"FileSystem"})})," and\nTrino\u2019s ",(0,r.jsx)(a.a,{href:"https://github.com/trinodb/trino/blob/450/lib/trino-filesystem/src/main/java/io/trino/filesystem/TrinoFileSystem.java",children:(0,r.jsx)(a.code,{children:"TrinoFileSystem"})}),".\nCore classes are introduced for accessing file systems:"]}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-io/src/main/java/org/apache/hudi/storage/StoragePath.java",children:(0,r.jsx)(a.code,{children:"StoragePath"})}),":\nrepresents a path of a file or directory on storage, which replaces\nHadoop's ",(0,r.jsx)(a.a,{href:"https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/Path.html",children:(0,r.jsx)(a.code,{children:"Path"})}),"."]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-io/src/main/java/org/apache/hudi/storage/StoragePathInfo.java",children:(0,r.jsx)(a.code,{children:"StoragePathInfo"})}),":\nkeeps the path, length, isDirectory, modification time, and other information which are used by Hudi, which replaces\nHadoop's ",(0,r.jsx)(a.a,{href:"https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/FileStatus.html",children:(0,r.jsx)(a.code,{children:"FileStatus"})}),"."]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-io/src/main/java/org/apache/hudi/storage/StorageConfiguration.java",children:(0,r.jsx)(a.code,{children:"StorageConfiguration"})}),":\nprovides the storage configuration by wrapping the particular configuration class object used by the corresponding\nfile system."]}),"\n"]}),"\n",(0,r.jsxs)(a.p,{children:["The ",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieIOFactory.java",children:(0,r.jsx)(a.code,{children:"HoodieIOFactory"})}),"\nabstraction is introduced to provide APIs to create readers and writers for I/O without depending on Hadoop classes."]}),"\n",(0,r.jsxs)(a.p,{children:["By using the new storage and I/O abstractions, we make\nthe ",(0,r.jsxs)(a.a,{href:"https://github.com/apache/hudi/tree/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-common",children:[(0,r.jsx)(a.code,{children:"hudi-common"})," module"]})," and\ncore reader logic in Hudi\nHadoop-independent in this release. We have introduced a\nnew ",(0,r.jsxs)(a.a,{href:"https://github.com/apache/hudi/tree/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-hadoop-common",children:[(0,r.jsx)(a.code,{children:"hudi-hadoop-common"})," module"]}),"\nwhich contains\nthe implementation\nof ",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-io/src/main/java/org/apache/hudi/storage/HoodieStorage.java",children:(0,r.jsx)(a.code,{children:"HoodieStorage"})}),"\nand ",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieIOFactory.java",children:(0,r.jsx)(a.code,{children:"HoodieIOFactory"})}),"\nbased on Hadoop\u2019s file system APIs and implementation, and\nexisting reader and writer logic that depends on Hadoop-dependent APIs.\nThe ",(0,r.jsxs)(a.a,{href:"https://github.com/apache/hudi/tree/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-hadoop-common",children:[(0,r.jsx)(a.code,{children:"hudi-hadoop-common"})," module"]}),"\nis used by\nSpark, Flink, Hive, and Presto integration where the logic remains unchanged."]}),"\n",(0,r.jsxs)(a.p,{children:["For the engine that is independent of Hadoop, the integration should use\nthe ",(0,r.jsxs)(a.a,{href:"https://github.com/apache/hudi/tree/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-common",children:[(0,r.jsx)(a.code,{children:"hudi-common"})," module"]})," and\nplug in its own\nimplementation\nof ",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-io/src/main/java/org/apache/hudi/storage/HoodieStorage.java",children:(0,r.jsx)(a.code,{children:"HoodieStorage"})}),"\nand ",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieIOFactory.java",children:(0,r.jsx)(a.code,{children:"HoodieIOFactory"})}),"\nby setting the new\nconfigs ",(0,r.jsx)(a.a,{href:"/docs/configurations#hoodiestorageclass",children:(0,r.jsx)(a.code,{children:"hoodie.storage.class"})}),"\nand ",(0,r.jsx)(a.a,{href:"/docs/configurations#hoodieiofactoryclass",children:(0,r.jsx)(a.code,{children:"hoodie.io.factory.class"})})," through the storage configuration."]}),"\n",(0,r.jsx)(a.h3,{id:"engine-support",children:"Engine Support"}),"\n",(0,r.jsx)(a.h4,{id:"spark-35-and-scala-213-support",children:"Spark 3.5 and Scala 2.13 Support"}),"\n",(0,r.jsxs)(a.p,{children:["This release has added the Spark 3.5 support and Scala 2.13 support; users who are on Spark 3.5 can use the new Spark\nbundle based on the Scala\nversion: ",(0,r.jsx)(a.a,{href:"https://mvnrepository.com/artifact/org.apache.hudi/hudi-spark3.5-bundle_2.12",children:"hudi-spark3.5-bundle_2.12"})," and\n",(0,r.jsx)(a.a,{href:"https://mvnrepository.com/artifact/org.apache.hudi/hudi-spark3.5-bundle_2.13",children:"hudi-spark3.5-bundle_2.13"}),". Spark 3.4,\n3.3, 3.2, 3.1, 3.0 and 2.4 continue to be supported in this release. To quickly get started with Hudi and Spark 3.5, you\ncan explore our ",(0,r.jsx)(a.a,{href:"/docs/quick-start-guide",children:"quick start guide"}),"."]}),"\n",(0,r.jsx)(a.h4,{id:"flink-118-support",children:"Flink 1.18 Support"}),"\n",(0,r.jsxs)(a.p,{children:["This release has added the Flink 1.18 support with a new compile maven profile ",(0,r.jsx)(a.code,{children:"flink1.18"})," and new Flink\nbundle ",(0,r.jsx)(a.a,{href:"https://mvnrepository.com/artifact/org.apache.hudi/hudi-flink1.18-bundle",children:"hudi-flink1.18-bundle"}),"."]}),"\n",(0,r.jsx)(a.h3,{id:"hudi-native-hfile-reader",children:"Hudi-Native HFile Reader"}),"\n",(0,r.jsxs)(a.p,{children:["Hudi uses ",(0,r.jsx)(a.a,{href:"https://hbase.apache.org/book.html#_hfile_format_2",children:"HFile format"})," as the base file format for storing various\nmetadata, e.g., file listing, column stats, and bloom filters, in the metadata table (MDT), as HFile format is optimized\nfor range scans and point lookups.  ",(0,r.jsx)(a.a,{href:"https://hbase.apache.org/book.html#_hfile_format_2",children:"HFile format"})," is originally\ndesigned and implemented by ",(0,r.jsx)(a.a,{href:"https://hbase.apache.org/",children:"HBase"}),". To avoid HBase dependency conflict and make engine\nintegration easy with Hadoop-independent implementation, we have implemented a\nnew ",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-io/src/main/java/org/apache/hudi/io/hfile/HFileReaderImpl.java",children:"HFile reader in Java"}),"\nwhich is independent of HBase or Hadoop dependencies. This HFile reader is backwards compatible with existing Hudi\nreleases and storage format."]}),"\n",(0,r.jsxs)(a.p,{children:["We have also written\na ",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-io/hfile_format.md",children:"HFile Format Specification"}),",\nthat defines\nthe HFile Format required by Hudi. This makes HFile reader and writer implementation possible in any language, for\nexample, C++ or Rust, by following this Spec."]}),"\n",(0,r.jsx)(a.h3,{id:"new-features-in-hudi-utilities",children:"New Features in Hudi Utilities"}),"\n",(0,r.jsx)(a.h4,{id:"streamcontext-and-sourceprofile-interfaces",children:"StreamContext and SourceProfile Interfaces"}),"\n",(0,r.jsxs)(a.p,{children:["For the Hudi streamer, we have introduced the\nnew ",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/StreamContext.java",children:(0,r.jsx)(a.code,{children:"StreamContext"})}),"\nand ",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/SourceProfile.java",children:(0,r.jsx)(a.code,{children:"SourceProfile"})}),"\nInterfaces. These are\ndesigned to contain details about how the data should be consumed from the source and written (e.g., parallelism) in the\nnext sync round in StreamSync. This allows users to control the behavior and performance of source reading and data\nwriting to the target Hudi table."]}),"\n",(0,r.jsx)(a.h4,{id:"enhanced-proto-kafka-source-support",children:"Enhanced Proto Kafka Source Support"}),"\n",(0,r.jsxs)(a.p,{children:["We have added the support for deserializing with the Confluent proto deserializer, through a new\nconfig ",(0,r.jsx)(a.a,{href:"/docs/configurations#hoodiestreamersourcekafkaprotovaluedeserializerclass",children:(0,r.jsx)(a.code,{children:"hoodie.streamer.source.kafka.proto.value.deserializer.class"})}),"\nto specify the Kafka Proto payload deserializer\nclass."]}),"\n",(0,r.jsx)(a.h4,{id:"ignoring-checkpoint-in-hudi-streamer",children:"Ignoring Checkpoint in Hudi Streamer"}),"\n",(0,r.jsxs)(a.p,{children:["Hudi streamer has a new option, ",(0,r.jsx)(a.code,{children:"--ignore-checkpoint"}),", to ignore the last committed checkpoint for the source. This\noptions should be set with a unique value, a timestamp value or UUID as recommended. Setting this config indicates that\nthe subsequent sync should ignore the last committed checkpoint for the source. The config value is stored in the commit\nhistory, so setting the config with same values would not have any affect. This config can be used in scenarios like\nkafka topic change where we would want to start ingesting from the latest or earliest offset after switching the topic\n(in this case we would want to ignore the previously committed checkpoint, and rely on other configs to pick the\nstarting offset)."]}),"\n",(0,r.jsx)(a.h3,{id:"meta-sync-improvements",children:"Meta Sync Improvements"}),"\n",(0,r.jsx)(a.h4,{id:"paralleled-listing-in-glue-catalog-sync",children:"Paralleled Listing in Glue Catalog Sync"}),"\n",(0,r.jsx)(a.p,{children:"AWS Glue Catalog sync now supports paralleled listing of partitions to improve the listing performance and reduce meta\nsync latency. Three new configs are added to control the listing parallelism:"}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.a,{href:"/docs/configurations#hoodiedatasourcemetasyncglueall_partitions_read_parallelism",children:(0,r.jsx)(a.code,{children:"hoodie.datasource.meta.sync.glue.all_partitions_read_parallelism"})}),":\nparallelism for listing all partitions (first time sync)."]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.a,{href:"/docs/configurations#hoodiedatasourcemetasyncgluechanged_partitions_read_parallelism",children:(0,r.jsx)(a.code,{children:"hoodie.datasource.meta.sync.glue.changed_partitions_read_parallelism"})}),":\nparallelism for listing changed partitions (second and subsequent syncs)."]}),"\n",(0,r.jsxs)(a.li,{children:[(0,r.jsx)(a.a,{href:"/docs/configurations#hoodiedatasourcemetasyncgluepartition_change_parallelism",children:(0,r.jsx)(a.code,{children:"hoodie.datasource.meta.sync.glue.partition_change_parallelism"})}),":\nparallelism for change operations such as create, update, and delete."]}),"\n"]}),"\n",(0,r.jsx)(a.h4,{id:"bigquery-sync-optimization-with-metadata-table",children:"BigQuery Sync Optimization with Metadata Table"}),"\n",(0,r.jsx)(a.p,{children:"The BigQuery Sync now loads all partitions from the metadata table once if the metadata table is enabled, to improve the\nfile listing performance."}),"\n",(0,r.jsx)(a.h3,{id:"metrics-reporting-to-m3",children:"Metrics Reporting to M3"}),"\n",(0,r.jsxs)(a.p,{children:["A new MetricsReporter\nimplementation ",(0,r.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/38832854be37cb78ad1edd87f515f01ca5ea6a8a/hudi-common/src/main/java/org/apache/hudi/metrics/m3/M3MetricsReporter.java",children:(0,r.jsx)(a.code,{children:"M3MetricsReporter"})}),"\nis added to support emitting metrics to ",(0,r.jsx)(a.a,{href:"https://m3db.io/",children:"M3"}),". Users can now enable reporting metrics to M3 by\nsetting ",(0,r.jsx)(a.a,{href:"/docs/basic_configurations#hoodiemetricsreportertype",children:(0,r.jsx)(a.code,{children:"hoodie.metrics.reporter.type"})})," as ",(0,r.jsx)(a.code,{children:"M3"})," and their\ncorresponding\nhost address and port in ",(0,r.jsx)(a.a,{href:"/docs/basic_configurations#hoodiemetricsreportertype",children:(0,r.jsx)(a.code,{children:"hoodie.metrics.m3.host"})}),"\nand ",(0,r.jsx)(a.a,{href:"/docs/basic_configurations#hoodiemetricsm3port",children:(0,r.jsx)(a.code,{children:"hoodie.metrics.m3.port"})}),"."]}),"\n",(0,r.jsx)(a.h3,{id:"other-features-and-improvements",children:"Other Features and Improvements"}),"\n",(0,r.jsx)(a.h4,{id:"schema-exception-classification",children:"Schema Exception Classification"}),"\n",(0,r.jsxs)(a.p,{children:["This release introduces the classification of schema-related\nexceptions (",(0,r.jsx)(a.a,{href:"https://issues.apache.org/jira/browse/HUDI-7486",children:"HUDI-7486"}),") to make it easy for users to\nunderstand the root cause, including the errors during converting the records from Avro to Spark Row due to illegal\nschema, or the records are incompatible with the provided schema."]}),"\n",(0,r.jsx)(a.h4,{id:"record-size-estimation-improvement",children:"Record Size Estimation Improvement"}),"\n",(0,r.jsxs)(a.p,{children:["The record size estimation in Hudi is improved by considering replace commits and delta commits additionally\n(",(0,r.jsx)(a.a,{href:"https://issues.apache.org/jira/browse/HUDI-7429",children:"HUDI-7429"}),")."]}),"\n",(0,r.jsxs)(a.h4,{id:"using-s3-scheme-for-athena",children:["Using ",(0,r.jsx)(a.code,{children:"s3"})," Scheme for Athena"]}),"\n",(0,r.jsxs)(a.p,{children:["Recent Athena version silently drops Hudi data when the partition location has a ",(0,r.jsx)(a.code,{children:"s3a"})," scheme. Recreating the table with\npartition ",(0,r.jsx)(a.code,{children:"s3"})," scheme fixes the issue. We have added a fix to use ",(0,r.jsx)(a.code,{children:"s3"})," scheme for the Hudi table partitions in AWS Glue\nCatalog sync (",(0,r.jsx)(a.a,{href:"https://issues.apache.org/jira/browse/HUDI-7362",children:"HUDI-7362"}),")."]}),"\n",(0,r.jsx)(a.h2,{id:"known-regressions",children:"Known Regressions"}),"\n",(0,r.jsx)(a.p,{children:"The Hudi 0.15.0 release introduces a regression related to Complex Key generation when the record key consists of a\nsingle field. This issue was also present in version 0.14.1. When upgrading a table from previous versions,\nit may silently ingest duplicate records."}),"\n",(0,r.jsx)(a.admonition,{type:"tip",children:(0,r.jsx)(a.p,{children:"Avoid upgrading any existing table to 0.14.1 and 0.15.0 from any prior version if you are using ComplexKeyGenerator with single field as record key and multiple partition fields."})}),"\n",(0,r.jsx)(a.h2,{id:"raw-release-notes",children:"Raw Release Notes"}),"\n",(0,r.jsxs)(a.p,{children:["The raw release notes are\navailable ",(0,r.jsx)(a.a,{href:"https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12322822&version=12353381",children:"here"}),"."]})]})}function h(e={}){const{wrapper:a}={...(0,o.R)(),...e.components};return a?(0,r.jsx)(a,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}}}]);