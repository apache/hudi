"use strict";(globalThis.webpackChunkhudi=globalThis.webpackChunkhudi||[]).push([[40959],{28453(e,n,o){o.d(n,{R:()=>s,x:()=>r});var t=o(96540);const i={},a=t.createContext(i);function s(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(a.Provider,{value:n},e.children)}},37343(e){e.exports=JSON.parse('{"permalink":"/blog/2020/08/21/async-compaction-deployment-model","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-08-21-async-compaction-deployment-model.md","source":"@site/blog/2020-08-21-async-compaction-deployment-model.md","title":"Async Compaction Deployment Models","description":"We will look at different deployment models for executing compactions asynchronously.","date":"2020-08-21T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"compaction","permalink":"/blog/tags/compaction"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":2.02,"hasTruncateMarker":true,"authors":[{"name":"vbalaji","key":null,"page":null}],"frontMatter":{"title":"Async Compaction Deployment Models","excerpt":"Mechanisms for executing compaction jobs in Hudi asynchronously","author":"vbalaji","category":"blog","tags":["how-to","compaction","apache hudi"]},"unlisted":false,"prevItem":{"title":"Ingest multiple tables using Hudi","permalink":"/blog/2020/08/22/ingest-multiple-tables-using-hudi"},"nextItem":{"title":"Efficient Migration of Large Parquet Tables to Apache Hudi","permalink":"/blog/2020/08/20/efficient-migration-of-large-parquet-tables"}}')},37728(e,n,o){o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>d,frontMatter:()=>s,metadata:()=>t,toc:()=>l});var t=o(37343),i=o(74848),a=o(28453);const s={title:"Async Compaction Deployment Models",excerpt:"Mechanisms for executing compaction jobs in Hudi asynchronously",author:"vbalaji",category:"blog",tags:["how-to","compaction","apache hudi"]},r=void 0,c={authorsImageUrls:[void 0]},l=[{value:"Compaction",id:"compaction",level:2},{value:"Async Compaction",id:"async-compaction",level:2},{value:"Deployment Models",id:"deployment-models",level:2},{value:"Spark Structured Streaming",id:"spark-structured-streaming",level:3},{value:"DeltaStreamer Continuous Mode",id:"deltastreamer-continuous-mode",level:3},{value:"Hudi CLI",id:"hudi-cli",level:3},{value:"Hudi Compactor Script",id:"hudi-compactor-script",level:3}];function p(e){const n={br:"br",code:"code",em:"em",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:"We will look at different deployment models for executing compactions asynchronously."}),"\n",(0,i.jsx)(n.h2,{id:"compaction",children:"Compaction"}),"\n",(0,i.jsx)(n.p,{children:"For Merge-On-Read table, data is stored using a combination of columnar (e.g parquet) + row based (e.g avro) file formats.\nUpdates are logged to delta files & later compacted to produce new versions of columnar files synchronously or\nasynchronously. One of th main motivations behind Merge-On-Read is to reduce data latency when ingesting records.\nHence, it makes sense to run compaction asynchronously without blocking ingestion."}),"\n",(0,i.jsx)(n.h2,{id:"async-compaction",children:"Async Compaction"}),"\n",(0,i.jsx)(n.p,{children:"Async Compaction is performed in 2 steps:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:(0,i.jsx)(n.strong,{children:"Compaction Scheduling"})}),": This is done by the ingestion job. In this step, Hudi scans the partitions and selects ",(0,i.jsx)(n.strong,{children:"file\nslices"})," to be compacted. A compaction plan is finally written to Hudi timeline."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:(0,i.jsx)(n.strong,{children:"Compaction Execution"})}),": A separate process reads the compaction plan and performs compaction of file slices."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"deployment-models",children:"Deployment Models"}),"\n",(0,i.jsx)(n.p,{children:"There are few ways by which we can execute compactions asynchronously."}),"\n",(0,i.jsx)(n.h3,{id:"spark-structured-streaming",children:"Spark Structured Streaming"}),"\n",(0,i.jsx)(n.p,{children:"With 0.6.0, we now have support for running async compactions in Spark\nStructured Streaming jobs. Compactions are scheduled and executed asynchronously inside the\nstreaming job.  Async Compactions are enabled by default for structured streaming jobs\non Merge-On-Read table."}),"\n",(0,i.jsx)(n.p,{children:"Here is an example snippet in java"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-properties",children:'import org.apache.hudi.DataSourceWriteOptions;\nimport org.apache.hudi.HoodieDataSourceHelpers;\nimport org.apache.hudi.config.HoodieCompactionConfig;\nimport org.apache.hudi.config.HoodieWriteConfig;\n\nimport org.apache.spark.sql.streaming.OutputMode;\nimport org.apache.spark.sql.streaming.ProcessingTime;\n\n\n DataStreamWriter<Row> writer = streamingInput.writeStream().format("org.apache.hudi")\n        .option(DataSourceWriteOptions.OPERATION_OPT_KEY(), operationType)\n        .option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY(), tableType)\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), "_row_key")\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), "partition")\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), "timestamp")\n        .option(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP, "10")\n        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE_OPT_KEY(), "true")\n        .option(HoodieWriteConfig.TABLE_NAME, tableName).option("checkpointLocation", checkpointLocation)\n        .outputMode(OutputMode.Append());\n writer.trigger(new ProcessingTime(30000)).start(tablePath);\n'})}),"\n",(0,i.jsx)(n.h3,{id:"deltastreamer-continuous-mode",children:"DeltaStreamer Continuous Mode"}),"\n",(0,i.jsxs)(n.p,{children:["Hudi DeltaStreamer provides continuous ingestion mode where a single long running spark application",(0,i.jsx)(n.br,{}),"\n","ingests data to Hudi table continuously from upstream sources. In this mode, Hudi supports managing asynchronous\ncompactions. Here is an example snippet for running in continuous mode with async compactions"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-properties",children:"spark-submit --packages org.apache.hudi:hudi-utilities-bundle_2.11:0.6.0 \\\n--class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer \\\n--table-type MERGE_ON_READ \\\n--target-base-path <hudi_base_path> \\\n--target-table <hudi_table> \\\n--source-class org.apache.hudi.utilities.sources.JsonDFSSource \\\n--source-ordering-field ts \\\n--schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider \\\n--props /path/to/source.properties \\\n--continous\n"})}),"\n",(0,i.jsx)(n.h3,{id:"hudi-cli",children:"Hudi CLI"}),"\n",(0,i.jsx)(n.p,{children:"Hudi CLI is yet another way to execute specific compactions asynchronously. Here is an example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-properties",children:"hudi:trips->compaction run --tableName <table_name> --parallelism <parallelism> --compactionInstant <InstantTime>\n...\n"})}),"\n",(0,i.jsx)(n.h3,{id:"hudi-compactor-script",children:"Hudi Compactor Script"}),"\n",(0,i.jsx)(n.p,{children:"Hudi provides a standalone tool to also execute specific compactions asynchronously. Here is an example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-properties",children:"spark-submit --packages org.apache.hudi:hudi-utilities-bundle_2.11:0.6.0 \\\n--class org.apache.hudi.utilities.HoodieCompactor \\\n--base-path <base_path> \\\n--table-name <table_name> \\\n--instant-time <compaction_instant> \\\n--schema-file <schema_file>\n"})})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}}}]);