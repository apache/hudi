"use strict";(self.webpackChunkhudi=self.webpackChunkhudi||[]).push([[29243],{62867:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"release-0.13.0","title":"Release 0.13.0","description":"Release 0.13.0 (docs)","source":"@site/releases/release-0.13.0.md","sourceDirName":".","slug":"/release-0.13.0","permalink":"/releases/release-0.13.0","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"title":"Release 0.13.0","sidebar_position":8,"layout":"releases","toc":true},"sidebar":"releases","previous":{"title":"Release 0.12.3","permalink":"/releases/release-0.12.3"},"next":{"title":"Release 0.12.2","permalink":"/releases/release-0.12.2"}}');var t=i(74848),a=i(28453);i(11470),i(19365);const s={title:"Release 0.13.0",sidebar_position:8,layout:"releases",toc:!0},o=void 0,l={},d=[{value:"Release 0.13.0 (docs)",id:"release-0130-docs",level:2},{value:"Migration Guide: Overview",id:"migration-guide-overview",level:2},{value:"Migration Guide: Breaking Changes",id:"migration-guide-breaking-changes",level:2},{value:"Bundle Updates",id:"bundle-updates",level:3},{value:"Spark bundle Support",id:"spark-bundle-support",level:4},{value:"Utilities Bundle Change",id:"utilities-bundle-change",level:4},{value:"New Flink Bundle",id:"new-flink-bundle",level:4},{value:"Lazy File Index in Spark",id:"lazy-file-index-in-spark",level:3},{value:"Checkpoint Management in Spark Structured Streaming",id:"checkpoint-management-in-spark-structured-streaming",level:3},{value:"ORC Support in Spark",id:"orc-support-in-spark",level:3},{value:"Mandatory Record Key Field",id:"mandatory-record-key-field",level:3},{value:"Migration Guide: Behavior Changes",id:"migration-guide-behavior-changes",level:2},{value:"Schema Handling in Write Path",id:"schema-handling-in-write-path",level:3},{value:"Removal of Default Shuffle Parallelism",id:"removal-of-default-shuffle-parallelism",level:3},{value:"Simple Write Executor as Default",id:"simple-write-executor-as-default",level:3},{value:"<code>NONE</code> Sort Mode for Bulk Insert to Match Parquet Writes",id:"none-sort-mode-for-bulk-insert-to-match-parquet-writes",level:3},{value:"Meta Sync Failure in Deltastreamer",id:"meta-sync-failure-in-deltastreamer",level:3},{value:"No Override of Internal Metadata Table Configs",id:"no-override-of-internal-metadata-table-configs",level:3},{value:"Spark SQL CTAS Performance Fix",id:"spark-sql-ctas-performance-fix",level:3},{value:"Flink CkpMetadata",id:"flink-ckpmetadata",level:3},{value:"Release Highlights",id:"release-highlights",level:2},{value:"Metaserver",id:"metaserver",level:3},{value:"Change Data Capture",id:"change-data-capture",level:3},{value:"Optimizing Record Payload handling",id:"optimizing-record-payload-handling",level:3},{value:"New Source Support in Deltastreamer",id:"new-source-support-in-deltastreamer",level:3},{value:"Proto Kafka Source",id:"proto-kafka-source",level:4},{value:"GCS Incremental Source",id:"gcs-incremental-source",level:4},{value:"Pulsar Source",id:"pulsar-source",level:4},{value:"Support for Partial Payload Update",id:"support-for-partial-payload-update",level:3},{value:"Consistent Hashing Index",id:"consistent-hashing-index",level:3},{value:"Early Conflict Detection for Multi-Writer",id:"early-conflict-detection-for-multi-writer",level:3},{value:"Lock-Free Message Queue in Writing Data",id:"lock-free-message-queue-in-writing-data",level:3},{value:"Hudi CLI Bundle",id:"hudi-cli-bundle",level:3},{value:"Support for Flink 1.16",id:"support-for-flink-116",level:3},{value:"Json Schema Converter",id:"json-schema-converter",level:3},{value:"Providing Hudi Configs via Spark SQL Config",id:"providing-hudi-configs-via-spark-sql-config",level:3},{value:"Known Regressions",id:"known-regressions",level:2},{value:"Raw Release Notes",id:"raw-release-notes",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(n.h2,{id:"release-0130-docs",children:[(0,t.jsx)(n.a,{href:"https://github.com/apache/hudi/releases/tag/release-0.13.0",children:"Release 0.13.0"})," (",(0,t.jsx)(n.a,{href:"/docs/quick-start-guide",children:"docs"}),")"]}),"\n",(0,t.jsxs)(n.p,{children:["Apache Hudi 0.13.0 release introduces a number of new features including ",(0,t.jsx)(n.a,{href:"#metaserver",children:"Metaserver"}),",\n",(0,t.jsx)(n.a,{href:"#change-data-capture",children:"Change Data Capture"}),", ",(0,t.jsx)(n.a,{href:"#optimizing-record-payload-handling",children:"new Record Merge API"}),",\n",(0,t.jsx)(n.a,{href:"#new-source-support-in-deltastreamer",children:"new sources for Deltastreamer"})," and more.  While there is no table version upgrade\nrequired for this release, users are expected to take actions by following the ",(0,t.jsx)(n.a,{href:"#migration-guide-overview",children:"Migration Guide"}),"\ndown below on relevant ",(0,t.jsx)(n.a,{href:"#migration-guide-breaking-changes",children:"breaking changes"})," and\n",(0,t.jsx)(n.a,{href:"#migration-guide-behavior-changes",children:"behavior changes"})," before using 0.13.0 release."]}),"\n",(0,t.jsx)(n.h2,{id:"migration-guide-overview",children:"Migration Guide: Overview"}),"\n",(0,t.jsxs)(n.p,{children:["This release keeps the same table version (",(0,t.jsx)(n.code,{children:"5"}),") as ",(0,t.jsx)(n.a,{href:"/releases/release-0.12.0",children:"0.12.0 release"}),", and there is no need for\na table version upgrade if you are upgrading from 0.12.0.  There are a few\n",(0,t.jsx)(n.a,{href:"#migration-guide-breaking-changes",children:"breaking changes"})," and ",(0,t.jsx)(n.a,{href:"#migration-guide-behavior-changes",children:"behavior changes"})," as\ndescribed below, and users are expected to take action accordingly before using 0.13.0 release."]}),"\n",(0,t.jsx)(n.admonition,{type:"caution",children:(0,t.jsx)(n.p,{children:"If migrating from an older release (pre 0.12.0), please also check the upgrade instructions from each older release in\nsequence."})}),"\n",(0,t.jsx)(n.h2,{id:"migration-guide-breaking-changes",children:"Migration Guide: Breaking Changes"}),"\n",(0,t.jsx)(n.h3,{id:"bundle-updates",children:"Bundle Updates"}),"\n",(0,t.jsx)(n.h4,{id:"spark-bundle-support",children:"Spark bundle Support"}),"\n",(0,t.jsxs)(n.p,{children:["From now on, ",(0,t.jsx)(n.a,{href:"https://mvnrepository.com/artifact/org.apache.hudi/hudi-spark3.2-bundle",children:(0,t.jsx)(n.code,{children:"hudi-spark3.2-bundle"})})," works\nwith Apache Spark 3.2.1 and newer versions for Spark 3.2.x.  The support for Spark 3.2.0 with\n",(0,t.jsx)(n.a,{href:"https://mvnrepository.com/artifact/org.apache.hudi/hudi-spark3.2-bundle",children:(0,t.jsx)(n.code,{children:"hudi-spark3.2-bundle"})})," is\ndropped because of the Spark implementation change of ",(0,t.jsx)(n.code,{children:"getHive"})," method of ",(0,t.jsx)(n.code,{children:"HiveClientImpl"})," which is incompatible between\nSpark version 3.2.0 and 3.2.1."]}),"\n",(0,t.jsx)(n.h4,{id:"utilities-bundle-change",children:"Utilities Bundle Change"}),"\n",(0,t.jsxs)(n.p,{children:["The AWS and GCP bundle jars are separated from\n",(0,t.jsx)(n.a,{href:"https://mvnrepository.com/artifact/org.apache.hudi/hudi-utilities-bundle",children:(0,t.jsx)(n.code,{children:"hudi-utilities-bundle"})}),". The user would need\nto use ",(0,t.jsx)(n.a,{href:"https://mvnrepository.com/artifact/org.apache.hudi/hudi-aws-bundle",children:(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"hudi-aws-bundle"})})})," or\n",(0,t.jsx)(n.a,{href:"https://mvnrepository.com/artifact/org.apache.hudi/hudi-gcp-bundle",children:(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"hudi-gcp-bundle"})})})," along with\n",(0,t.jsx)(n.a,{href:"https://mvnrepository.com/artifact/org.apache.hudi/hudi-utilities-bundle",children:(0,t.jsx)(n.code,{children:"hudi-utilities-bundle"})})," while using the\ncloud services."]}),"\n",(0,t.jsx)(n.h4,{id:"new-flink-bundle",children:"New Flink Bundle"}),"\n",(0,t.jsxs)(n.p,{children:["Hudi is now supported on Flink 1.16.x with the new\n",(0,t.jsx)(n.a,{href:"https://mvnrepository.com/artifact/org.apache.hudi/hudi-flink1.16-bundle",children:(0,t.jsx)(n.code,{children:"hudi-flink1.16-bundle"})}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"lazy-file-index-in-spark",children:"Lazy File Index in Spark"}),"\n",(0,t.jsxs)(n.p,{children:["Hudi's File Index in Spark is switched to be listed lazily ",(0,t.jsx)(n.em,{children:(0,t.jsx)(n.strong,{children:"by default"})}),": this entails that it would ",(0,t.jsx)(n.strong,{children:"only"})," be listing\npartitions that are requested by the query (i.e., after partition-pruning) as opposed to always listing the whole table\nbefore this release. This is expected to bring considerable performance improvement for large tables."]}),"\n",(0,t.jsxs)(n.p,{children:["A new configuration property is added if the user wants to change the listing behavior:\n",(0,t.jsx)(n.code,{children:"hoodie.datasource.read.file.index.listing.mode"})," (now default to ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"lazy"})}),"). There are two possible values that you can\nset:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"eager"})}),": This lists all partition paths and corresponding file slices within them eagerly, during initialization.\nThis is the default behavior prior 0.13.0."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"If a Hudi table has 1000 partitions, the eager mode lists the files under all of them when constructing the file index."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"lazy"})}),": The partitions and file-slices within them will be listed lazily, allowing partition pruning predicates to\nbe pushed down appropriately, therefore only listing partitions after these have already been pruned."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The files are not listed under the partitions when the File Index is initialized. The files are listed only under\ntargeted partition(s) after partition pruning using predicates (e.g., ",(0,t.jsx)(n.code,{children:"datestr=2023-02-19"}),") in queries."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["To preserve the behavior pre 0.13.0, the user needs to set ",(0,t.jsx)(n.code,{children:"hoodie.datasource.read.file.index.listing.mode=eager"}),"."]})}),"\n",(0,t.jsx)(n.admonition,{title:"Breaking Change",type:"danger",children:(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.strong,{children:"breaking change"})," occurs only in cases when the table has ",(0,t.jsx)(n.strong,{children:"BOTH"}),": multiple partition columns AND partition\nvalues contain slashes that are not URL-encoded."]})}),"\n",(0,t.jsxs)(n.p,{children:["For example let's assume we want to parse two partition columns - ",(0,t.jsx)(n.code,{children:"month"})," (",(0,t.jsx)(n.code,{children:"2022/01"}),") and ",(0,t.jsx)(n.code,{children:"day"})," (",(0,t.jsx)(n.code,{children:"03"}),"), from the\npartition path ",(0,t.jsx)(n.code,{children:"2022/01/03"}),". Since there is a mismatch between the number of partition columns (2 here - ",(0,t.jsx)(n.code,{children:"month"})," and\n",(0,t.jsx)(n.code,{children:"day"}),") and the number of components in the partition path delimited by ",(0,t.jsx)(n.code,{children:"/"})," (3 in this case - month, year and day) it\ncauses ambiguity. In such cases, it is not possible to recover the partition value corresponding to each partition column."]}),"\n",(0,t.jsxs)(n.p,{children:["There are two ways to ",(0,t.jsx)(n.strong,{children:"avoid"})," the breaking changes:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["The first option is to change how partition values are constructed. A user can switch the partition value of column\n",(0,t.jsx)(n.code,{children:"month"})," to avoid slashes in any partition column values, such as ",(0,t.jsx)(n.code,{children:"202201"}),", then there is no problem parsing the\npartition path (",(0,t.jsx)(n.code,{children:"202201/03"}),")."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["The second option is to switch the listing mode to ",(0,t.jsx)(n.code,{children:"eager"}),'.  The File Index would "gracefully regress" to assume the\ntable is non-partitioned and just sacrifice partition-pruning, but would be able to process the query as if the\ntable was non-partitioned (therefore potentially incurring performance penalty), instead of failing the queries.']}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"checkpoint-management-in-spark-structured-streaming",children:"Checkpoint Management in Spark Structured Streaming"}),"\n",(0,t.jsxs)(n.p,{children:["If you are using ",(0,t.jsx)(n.a,{href:"https://spark.apache.org/docs/3.3.2/structured-streaming-programming-guide.html",children:"Spark streaming"})," to\ningest into Hudi, Hudi self-manages the checkpoint internally. We are now adding support for multiple writers, each\ningesting into the same Hudi table via streaming ingest. In older versions of hudi, you can't have multiple streaming\ningestion writers ingesting into the same hudi table (one streaming ingestion writer with a concurrent Spark datasource\nwriter works with lock provider; however, two Spark streaming ingestion writers are not supported). With 0.13.0, we are\nadding support where multiple streaming ingestions can be done to the same table. In case of a single streaming ingestion,\nusers don't have to do anything; the old pipeline will work without needing any additional changes. But, if you are\nhaving multiple streaming writers to same Hudi table, each table has to set a unique value for the config,\n",(0,t.jsx)(n.code,{children:"hoodie.datasource.write.streaming.checkpoint.identifier"}),". Also, users are expected to set the usual multi-writer\nconfigs. More details can be found ",(0,t.jsx)(n.a,{href:"/docs/concurrency_control",children:"here"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"orc-support-in-spark",children:"ORC Support in Spark"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.a,{href:"https://orc.apache.org/",children:"ORC"})," support for Spark 2.x is removed in this release, as the dependency of\n",(0,t.jsx)(n.code,{children:"orc-core:nohive"})," in Hudi is now replaced by  ",(0,t.jsx)(n.code,{children:"orc-core"}),", to be compatible with Spark 3.  ",(0,t.jsx)(n.a,{href:"https://orc.apache.org/",children:"ORC"}),"\nsupport is now available for Spark 3.x, which was broken in previous releases."]}),"\n",(0,t.jsx)(n.h3,{id:"mandatory-record-key-field",children:"Mandatory Record Key Field"}),"\n",(0,t.jsxs)(n.p,{children:["The configuration for setting the record key field, ",(0,t.jsx)(n.code,{children:"hoodie.datasource.write.recordkey.field"}),", is now required to be set\nand has no default value. Previously, the default value is ",(0,t.jsx)(n.code,{children:"uuid"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"migration-guide-behavior-changes",children:"Migration Guide: Behavior Changes"}),"\n",(0,t.jsx)(n.h3,{id:"schema-handling-in-write-path",children:"Schema Handling in Write Path"}),"\n",(0,t.jsx)(n.p,{children:"Many users have requested using Hudi for CDC use cases that they want to have schema auto-evolution where existing\ncolumns might be dropped in a new schema. As of 0.13.0 release, Hudi now has this functionality. You can permit schema\nauto-evolution where existing columns can be dropped in a new schema."}),"\n",(0,t.jsxs)(n.p,{children:["Since dropping columns in the target table based on the source schema constitutes a considerable behavior change, this\nis disabled by default and is guarded by the following config: ",(0,t.jsx)(n.code,{children:"hoodie.datasource.write.schema.allow.auto.evolution.column.drop"}),".\nTo enable automatic dropping of the columns along with new evolved schema of the incoming batch, set this to ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"true"})}),"."]}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["This config is ",(0,t.jsx)(n.strong,{children:"NOT"})," required to evolve schema manually by using, for example, ",(0,t.jsx)(n.code,{children:"ALTER TABLE \u2026 DROP COLUMN"})," in Spark."]})}),"\n",(0,t.jsx)(n.h3,{id:"removal-of-default-shuffle-parallelism",children:"Removal of Default Shuffle Parallelism"}),"\n",(0,t.jsxs)(n.p,{children:["This release changes how Hudi decides the shuffle parallelism of ",(0,t.jsx)(n.a,{href:"/docs/write_operations",children:"write operations"})," including\n",(0,t.jsx)(n.code,{children:"INSERT"}),", ",(0,t.jsx)(n.code,{children:"BULK_INSERT"}),", ",(0,t.jsx)(n.code,{children:"UPSERT"})," and ",(0,t.jsx)(n.code,{children:"DELETE"})," (",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"hoodie.insert|bulkinsert|upsert|delete.shuffle.parallelism"})}),"), which\ncan ultimately affect the write performance."]}),"\n",(0,t.jsxs)(n.p,{children:["Previously, if users did not configure it, Hudi would use ",(0,t.jsx)(n.code,{children:"200"})," as the default shuffle parallelism. From 0.13.0 onwards\nHudi by default automatically deduces the shuffle parallelism by either using the number of output RDD partitions as\ndetermined by Spark when available or by using the ",(0,t.jsx)(n.code,{children:"spark.default.parallelism"})," value.  If the above Hudi shuffle\nparallelisms are explicitly configured by the user, then the user-configured parallelism is still used in defining the\nactual parallelism.  Such behavior change improves the out-of-the-box performance by 20% for workloads with reasonably\nsized input."]}),"\n",(0,t.jsx)(n.admonition,{type:"caution",children:(0,t.jsxs)(n.p,{children:["If the input data files are small, e.g., smaller than 10MB, we suggest configuring the Hudi shuffle parallelism\n(",(0,t.jsx)(n.code,{children:"hoodie.insert|bulkinsert|upsert|delete.shuffle.parallelism"}),") explicitly, such that the parallelism is at least\ntotal_input_data_size/500MB, to avoid potential performance regression (see ",(0,t.jsx)(n.a,{href:"/docs/tuning-guide",children:"Tuning Guide"})," for more\ninformation)."]})}),"\n",(0,t.jsx)(n.h3,{id:"simple-write-executor-as-default",children:"Simple Write Executor as Default"}),"\n",(0,t.jsxs)(n.p,{children:["For the execution of insert/upsert operations, Hudi historically used the notion of an executor, relying on in-memory\nqueue to decouple ingestion operations (that were previously often bound by I/O operations fetching shuffled blocks)\nfrom writing operations. Since then, Spark architectures have evolved considerably making such writing architecture\nredundant. Towards evolving this writing pattern and leveraging the changes in Spark, in 0.13.0 we introduce a new,\nsimplified version of the executor named (creatively) as ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"SimpleExecutor"})})," and also make it out-of-the-box default."]}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"SimpleExecutor"})})," does not have any internal buffering (i.e., does not hold records in memory), which internally\nimplements simple iteration over provided iterator (similar to default Spark behavior).  It provides ",(0,t.jsx)(n.strong,{children:"~10%"}),"\nout-of-the-box performance improvement on modern Spark versions (3.x) and even more when used with Spark's native\n",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"SparkRecordMerger"})}),"."]}),"\n",(0,t.jsxs)(n.h3,{id:"none-sort-mode-for-bulk-insert-to-match-parquet-writes",children:[(0,t.jsx)(n.code,{children:"NONE"})," Sort Mode for Bulk Insert to Match Parquet Writes"]}),"\n",(0,t.jsxs)(n.p,{children:["This release adjusts the parallelism for ",(0,t.jsx)(n.code,{children:"NONE"})," sort mode (default sort mode) for ",(0,t.jsx)(n.code,{children:"BULK_INSERT"})," write operation. From\nnow on, by default, the input parallelism is used instead of the shuffle parallelism (",(0,t.jsx)(n.code,{children:"hoodie.bulkinsert.shuffle.parallelism"}),")\nfor writing data, to match the default parquet write behavior. This does not change the behavior of clustering using the\n",(0,t.jsx)(n.code,{children:"NONE"})," sort mode."]}),"\n",(0,t.jsxs)(n.p,{children:["Such behavior change on ",(0,t.jsx)(n.code,{children:"BULK_INSERT"})," write operation improves the write performance out of the box."]}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["If you still observe small file issues with the default ",(0,t.jsx)(n.code,{children:"NONE"})," sort mode, we suggest sorting the input data based on the\npartition path and record key before writing to the Hudi table. You can also use ",(0,t.jsx)(n.code,{children:"GLOBAL_SORT"})," to ensure the best file\nsize."]})}),"\n",(0,t.jsx)(n.h3,{id:"meta-sync-failure-in-deltastreamer",children:"Meta Sync Failure in Deltastreamer"}),"\n",(0,t.jsxs)(n.p,{children:["In earlier versions, we used a fail-fast approach where syncing to remaining catalogs is not attempted if any\n",(0,t.jsx)(n.a,{href:"/docs/syncing_metastore",children:"catalog sync"})," fails. In 0.13.0, syncing to all configured catalogs is attempted before failing\nthe operation on any catalog sync failure.  In the case of sync failure for one catalog, the sync to other catalogs can\nstill succeed, so the user only needs to retry the failed one now."]}),"\n",(0,t.jsx)(n.h3,{id:"no-override-of-internal-metadata-table-configs",children:"No Override of Internal Metadata Table Configs"}),"\n",(0,t.jsx)(n.p,{children:"Since misconfiguration could lead to possible data integrity issues, in 0.13.0, we have put in efforts to make the\nmetadata table configuration much simpler for the users. Internally, Hudi determines the best choices for these\nconfigurations for optimal performance and stability of the system."}),"\n",(0,t.jsx)(n.p,{children:"The following metadata-table-related configurations are made internal; you can no longer configure these configs\nexplicitly:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"hoodie.metadata.clean.async\nhoodie.metadata.cleaner.commits.retained\nhoodie.metadata.enable.full.scan.log.files\nhoodie.metadata.populate.meta.fields\n"})}),"\n",(0,t.jsx)(n.h3,{id:"spark-sql-ctas-performance-fix",children:"Spark SQL CTAS Performance Fix"}),"\n",(0,t.jsxs)(n.p,{children:["Previously, CTAS write operation was incorrectly set to use ",(0,t.jsx)(n.code,{children:"UPSERT"})," due to misconfiguration. In 0.13.0 release, we fix\nthis to make sure CTAS uses ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"BULK_INSERT"})})," operation to boost the write performance of the first batch to a Hudi table\n(there's no real need to use ",(0,t.jsx)(n.code,{children:"UPSERT"})," for it, as the table is being created)."]}),"\n",(0,t.jsx)(n.h3,{id:"flink-ckpmetadata",children:"Flink CkpMetadata"}),"\n",(0,t.jsx)(n.p,{children:"Before 0.13.0, we bootstrapped the ckp metadata (checkpoint related metadata) by cleaning all the messages. Some corner\ncases were not handled correctly. For example:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"The write task can not fetch the pending instant correctly when restarting a job."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"If a checkpoint succeeds and the job crashes suddenly, the instant hasn't had time to commit. The data is lost because\nthe last pending instant was rolled back; however, the Flink engine still thinks the checkpoint/instant is successful."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Q: Why did we clean the messages prior to the 0.13.0 release?"}),"\n",(0,t.jsx)(n.p,{children:"A: To prevent inconsistencies between timeline and the messages."}),"\n",(0,t.jsx)(n.p,{children:"Q: Why are we retaining the messages in the 0.13.0 release?"}),"\n",(0,t.jsx)(n.p,{children:"A: There are two cases for the inconsistency:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"The timeline instant is complete but the ckp message is inflight (for committing instant)."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"The timeline instant is pending while the ckp message does not start (for starting a new instant)."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"For case 1, there is no need to re-commit the instant, and it is fine if the write task does not get any pending instant\nwhen recovering."}),"\n",(0,t.jsx)(n.p,{children:"For case 2, the instant is basically pending. The instant would be rolled back (as expected). Thus, keeping the ckp\nmessages as is can actually maintain correctness."}),"\n",(0,t.jsx)(n.h2,{id:"release-highlights",children:"Release Highlights"}),"\n",(0,t.jsx)(n.h3,{id:"metaserver",children:"Metaserver"}),"\n",(0,t.jsx)(n.p,{children:"In 0.13.0, we introduce Metaserver, a centralized metadata management service. This is one of the first platform service\ncomponents we introduce from many more to come. Metaserver helps users to easily manage a large number of tables in a\ndata lake platform."}),"\n",(0,t.jsx)(n.admonition,{type:"caution",children:(0,t.jsxs)(n.p,{children:["This is an ",(0,t.jsx)(n.em,{children:(0,t.jsx)(n.strong,{children:"EXPERIMENTAL"})})," feature."]})}),"\n",(0,t.jsxs)(n.p,{children:["To set up the metaserver in your environment, use\n",(0,t.jsx)(n.a,{href:"https://mvnrepository.com/artifact/org.apache.hudi/hudi-metaserver-server-bundle",children:(0,t.jsx)(n.code,{children:"hudi-metaserver-server-bundle"})})," and\nrun it as a java server application, like ",(0,t.jsx)(n.code,{children:"java -jar hudi-metaserver-server-bundle-<HUDI_VERSION>.jar"}),". On the client\nside, add the following options to integrate with the metaserver:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"hoodie.metaserver.enabled=true\nhoodie.metaserver.uris=thrift://<server url>:9090\n"})}),"\n",(0,t.jsx)(n.p,{children:"The Metaserver stores Hudi tables' metadata like table name, database, owner; and the timeline's metadata like commit\ninstants, actions, states, etc. In addition, the Metaserver supports the Spark writer and reader through Hudi Spark\nbundles."}),"\n",(0,t.jsx)(n.h3,{id:"change-data-capture",children:"Change Data Capture"}),"\n",(0,t.jsx)(n.p,{children:"In cases where Hudi tables are used as streaming sources, we want to be aware of all changes for the records that belong\nto a single commit.  For instance, we want to know which records were inserted, deleted and updated. For updated records,\nthe subsequent pipeline may want to get the old values before the update and the new ones after. Prior to 0.13.0, the\nincremental query does not contain the hard-delete records, and users need to use soft deletes to stream deletes, which\nmay not meet the GDPR requirements."}),"\n",(0,t.jsx)(n.p,{children:"The Change-Data-Capture (CDC) feature enables Hudi to show how records are changed by producing the changes and\ntherefore to handle CDC query use cases."}),"\n",(0,t.jsx)(n.admonition,{type:"caution",children:(0,t.jsxs)(n.p,{children:["CDC is an ",(0,t.jsx)(n.em,{children:(0,t.jsx)(n.strong,{children:"EXPERIMENTAL"})})," feature and is supported to work for COW tables with Spark and Flink engines. MOR tables\nare not supported by CDC query yet."]})}),"\n",(0,t.jsx)(n.p,{children:"To use CDC, users need to enable it first while writing to a table to log extra data, which are returned by CDC\nincremental queries."}),"\n",(0,t.jsxs)(n.p,{children:["For writing, set ",(0,t.jsx)(n.code,{children:"hoodie.table.cdc.enabled=true"})," and specify CDC logging mode through ",(0,t.jsx)(n.code,{children:"hoodie.datasource.query.incremental.format"}),",\nto control the data being logged. There are 3 modes to choose from:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"data_before_after"})}),": This logs the changed records' operations and the whole record before and after the change. This\nmode incurs the most CDC data on storage and has the least computing efforts for querying CDC results."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"data_before"})}),": This logs the changed records' operations and the whole record before the change."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"op_key_only"})}),": This only logs the changed records' operations and key. This mode incurs the least CDC data on\nstorage, and requires most computing efforts for querying CDC results."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["The default value is ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"data_before_after"})}),"."]}),"\n",(0,t.jsx)(n.p,{children:"For reading, set:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"hoodie.datasource.query.type=incremental\nhoodie.datasource.query.incremental.format=cdc\n"})}),"\n",(0,t.jsxs)(n.p,{children:["and other usual ",(0,t.jsx)(n.a,{href:"/docs/quick-start-guide#incremental-query",children:"incremental query"}),"'s options like begin and end instant\ntimes, and CDC results are returned."]}),"\n",(0,t.jsx)(n.admonition,{type:"caution",children:(0,t.jsxs)(n.p,{children:["Note that ",(0,t.jsx)(n.code,{children:"hoodie.table.cdc.enabled"})," is a table configuration. Once it is enabled, it is not allowed to be turned off\nfor that table. Similarly, you cannot change ",(0,t.jsx)(n.code,{children:"hoodie.table.cdc.supplemental.logging.mode"}),", once it's saved as a table\nconfiguration."]})}),"\n",(0,t.jsx)(n.h3,{id:"optimizing-record-payload-handling",children:"Optimizing Record Payload handling"}),"\n",(0,t.jsx)(n.p,{children:"This release introduces the long-awaited support for handling records as their engine-native representations, therefore\navoiding the need to convert them to an intermediate one (Avro)."}),"\n",(0,t.jsx)(n.admonition,{type:"caution",children:(0,t.jsxs)(n.p,{children:["This feature is in an ",(0,t.jsx)(n.em,{children:(0,t.jsx)(n.strong,{children:"EXPERIMENTAL"})})," mode and is currently only supported for Spark."]})}),"\n",(0,t.jsxs)(n.p,{children:["This was made possible through RFC-46 by introducing a new\n",(0,t.jsx)(n.a,{href:"https://github.com/apache/hudi/blob/release-0.13.0/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordMerger.java",children:(0,t.jsx)(n.code,{children:"HoodieRecordMerger"})}),"\nabstraction. The ",(0,t.jsx)(n.code,{children:"HoodieRecordMerger"})," is the core\nand a source of truth for implementing any merging semantics in Hudi going forward. In this capacity, it replaces the\n",(0,t.jsx)(n.a,{href:"https://github.com/apache/hudi/blob/release-0.13.0/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordPayload.java",children:(0,t.jsx)(n.code,{children:"HoodieRecordPayload"})}),"\nhierarchy previously used for implementing custom merging semantics. By relying on the unified\ncomponent in the form of a ",(0,t.jsx)(n.code,{children:"HoodieRecordMerger"})," allows us to handle records throughout the lifecycle of the write operation\nin a uniform manner. This substantially reduces latency because the records are now held in the engine-native\nrepresentation, avoiding unnecessary copying, deserialization and conversion to the intermediate representation (Avro).\nIn our benchmarks, upsert performance improves in the ballpark of 10% against 0.13.0 default state and 20% when compared\nto 0.12.2."]}),"\n",(0,t.jsx)(n.p,{children:"To try it today, you'd need to specify the configs differently for each Hudi table:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["For COW, specify ",(0,t.jsx)(n.code,{children:"hoodie.datasource.write.record.merger.impls=org.apache.hudi.HoodieSparkRecordMerger"})]}),"\n",(0,t.jsxs)(n.li,{children:["For MOR, specify ",(0,t.jsx)(n.code,{children:"hoodie.datasource.write.record.merger.impls=org.apache.hudi.HoodieSparkRecordMerger"})," and\n",(0,t.jsx)(n.code,{children:"hoodie.logfile.data.block.format=parquet"})]}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{type:"caution",children:(0,t.jsxs)(n.p,{children:["Please note, that the current\n",(0,t.jsx)(n.a,{href:"https://github.com/apache/hudi/blob/release-0.13.0/hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/HoodieSparkRecordMerger.java",children:(0,t.jsx)(n.code,{children:"HoodieSparkRecordMerger"})}),"\nimplementation only supports merging semantic equivalent to the\n",(0,t.jsx)(n.a,{href:"https://github.com/apache/hudi/blob/release-0.13.0/hudi-common/src/main/java/org/apache/hudi/common/model/OverwriteWithLatestAvroPayload.java",children:(0,t.jsx)(n.code,{children:"OverwriteWithLatestAvroPayload"})}),"\nclass, which is the default ",(0,t.jsx)(n.code,{children:"HoodieRecordPayload"})," implementation used for merging records\ncurrently (set as \u201choodie.compaction.payload.class\u201d). Therefore, if you're using any other ",(0,t.jsx)(n.code,{children:"HoodieRecordPayload"}),"\nimplementation, unfortunately, you'd need to wait until it is replaced by the corresponding ",(0,t.jsx)(n.code,{children:"HoodieRecordMerger"})," implementation."]})}),"\n",(0,t.jsx)(n.h3,{id:"new-source-support-in-deltastreamer",children:"New Source Support in Deltastreamer"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"/docs/0.13.0/hoodie_deltastreamer",children:"Deltastreamer"})," is a fully-managed incremental ETL utility that supports a wide variety of\nsources. In this release, we have added three new sources to its repertoire."]}),"\n",(0,t.jsx)(n.h4,{id:"proto-kafka-source",children:"Proto Kafka Source"}),"\n",(0,t.jsxs)(n.p,{children:["Deltastreamer already supports exactly-once ingestion of new events from Kafka using JSON and Avro formats.\n",(0,t.jsx)(n.a,{href:"https://github.com/apache/hudi/blob/release-0.13.0/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/ProtoKafkaSource.java",children:(0,t.jsx)(n.code,{children:"ProtoKafkaSource"})}),"\nextends this support to Protobuf class-based schemas as well. With just one additional config, one\ncan easily set up this source. Check out the ",(0,t.jsx)(n.a,{href:"/docs/0.13.0/hoodie_deltastreamer",children:"docs"})," for more details."]}),"\n",(0,t.jsx)(n.h4,{id:"gcs-incremental-source",children:"GCS Incremental Source"}),"\n",(0,t.jsxs)(n.p,{children:["Along the lines of ",(0,t.jsx)(n.a,{href:"https://hudi.apache.org/blog/2021/08/23/s3-events-source",children:"S3 events source"}),", we now have a reliable\nand fast way of ingesting from objects in Google Cloud Storage (GCS) through\n",(0,t.jsx)(n.a,{href:"https://github.com/apache/hudi/blob/release-0.13.0/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/GcsEventsHoodieIncrSource.java",children:(0,t.jsx)(n.code,{children:"GcsEventsHoodieIncrSource"})}),".\nCheck out the docs on how to set up this source."]}),"\n",(0,t.jsx)(n.h4,{id:"pulsar-source",children:"Pulsar Source"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"https://pulsar.apache.org/",children:"Apache Pulsar"})," is an open-source, distributed messaging and streaming platform built for\nthe cloud. ",(0,t.jsx)(n.a,{href:"https://github.com/apache/hudi/blob/release-0.13.0/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/PulsarSource.java",children:(0,t.jsx)(n.code,{children:"PulsarSource"})}),"\nsupports ingesting from Apache Pulsar through the Deltastreamer. Check out the ",(0,t.jsx)(n.a,{href:"/docs/0.13.0/hoodie_deltastreamer",children:"docs"})," on how\nto set up this source."]}),"\n",(0,t.jsx)(n.h3,{id:"support-for-partial-payload-update",children:"Support for Partial Payload Update"}),"\n",(0,t.jsxs)(n.p,{children:["Partial update is a frequently asked use case from the community that requires the ability to update only certain fields\nand not replace the whole record. Previously, we recommended users satisfy this use case by bringing in their own custom\nrecord payload implementation.  With the popularity of this, in the 0.13.0 release, we added a new record payload\nimplementation,\n",(0,t.jsx)(n.a,{href:"https://github.com/apache/hudi/blob/release-0.13.0/hudi-common/src/main/java/org/apache/hudi/common/model/PartialUpdateAvroPayload.java",children:(0,t.jsx)(n.code,{children:"PartialUpdateAvroPayload"})}),",\nto support this out-of-the-box so users can use this implementation instead of having to write their own custom implementation."]}),"\n",(0,t.jsx)(n.h3,{id:"consistent-hashing-index",children:"Consistent Hashing Index"}),"\n",(0,t.jsxs)(n.p,{children:["We introduce the Consistent Hashing Index as yet another indexing option for your writes with Hudi. This is an\nenhancement to ",(0,t.jsx)(n.a,{href:"/releases/release-0.11.0#bucket-index",children:"Bucket Index"})," which is added in the 0.11.0 release. With Bucket\nIndex, buckets/file groups per partition are statically allocated, whereas with Consistent Hashing Index, buckets can\ngrow dynamically and so users don't need to sweat about data skews. Buckets will expand and shrink depending on the load\nfactor for each partition. You can find the ",(0,t.jsx)(n.a,{href:"https://github.com/apache/hudi/blob/master/rfc/rfc-42/rfc-42.md",children:"RFC"})," for\nthe design of this feature."]}),"\n",(0,t.jsx)(n.p,{children:"Here are the configs of interest if you wish to give it a try."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"hoodie.index.type=bucket\nhoodie.index.bucket.engine=CONSISTENT_HASHING\nhoodie.bucket.index.max.num.buckets=128\nhoodie.bucket.index.min.num.buckets=32\nhoodie.bucket.index.num.buckets=4\n## do split if the bucket size reach 1.5 * max_file_size\nhoodie.bucket.index.split.threshold=1.5\n## do merge if the bucket size smaller than 0.2 * max_file_size\nhoodie.bucket.index.merge.threshold=0.1 \n"})}),"\n",(0,t.jsx)(n.p,{children:"To enforce shrinking or scaling up of buckets, you need to enable clustering using the following configs"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"## check resize for every 4 commit\nhoodie.clustering.inline=true\nhoodie.clustering.inline.max.commits=4\nhoodie.clustering.plan.strategy.class=org.apache.hudi.client.clustering.plan.strategy.SparkConsistentBucketClusteringPlanStrategy\nhoodie.clustering.execution.strategy.class=org.apache.hudi.client.clustering.run.strategy.SparkConsistentBucketClusteringExecutionStrategy\n## for supporting concurrent write & resizing\nhoodie.clustering.updates.strategy=org.apache.hudi.client.clustering.update.strategy.SparkConsistentBucketDuplicateUpdateStrategy\n"})}),"\n",(0,t.jsxs)(n.admonition,{type:"caution",children:[(0,t.jsx)(n.p,{children:"Consistent Hashing Index is still an evolving feature and currently there are some limitations to use it as of 0.13.0:"}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"This index is supported only for Spark engine using a MOR table."}),"\n",(0,t.jsx)(n.li,{children:"It does not work with metadata table enabled."}),"\n",(0,t.jsx)(n.li,{children:"To scale up or shrink the buckets, users have to manually trigger clustering using above configs (at some cadence), but\nthey cannot have compaction concurrently running."}),"\n",(0,t.jsx)(n.li,{children:"So, if compaction is enabled with your regular write pipeline, please follow this recommendation: You can choose to\ntrigger the scale/shrink once every 12 hours. In such cases, once every 12 hours, you might need to disable compaction,\nstop your write pipeline and enable clustering. You should take extreme care to not run both concurrently because it\nmight result in conflicts and a failed pipeline. Once clustering is complete, you can resume your regular write pipeline,\nwhich will have compaction enabled."}),"\n"]}),(0,t.jsxs)(n.p,{children:["We are working towards automating these and making it easier for users to leverage the Consistent Hashing Index. You can\nfollow the ongoing work on the Consistent Hashing Index ",(0,t.jsx)(n.a,{href:"https://issues.apache.org/jira/browse/HUDI-3000",children:"here"}),"."]}),(0,t.jsx)(n.h3,{id:"early-conflict-detection-for-multi-writer",children:"Early Conflict Detection for Multi-Writer"}),(0,t.jsx)(n.p,{children:"Hudi provides Optimistic Concurrency Control (OCC) to allow multiple writers to concurrently write and atomically commit\nto the Hudi table if there is no overlapping data file to be written, to guarantee data consistency, integrity and\ncorrectness. Prior to the 0.13.0 release, such conflict detection of overlapping data files is performed before commit\nmetadata and after the data writing is completed. If any conflict is detected in the final stage, it could have wasted\ncompute resources because the data writing is finished already."}),(0,t.jsx)(n.p,{children:"To improve the concurrency control, the 0.13.0 release introduces a new feature, early conflict detection in OCC, to\ndetect the conflict during the data writing phase and abort the writing early on once a conflict is detected, using\nHudi's marker mechanism.  Hudi can now stop a conflicting writer much earlier because of the early conflict detection\nand release computing resources necessary to cluster, improving resource utilization."})]}),"\n",(0,t.jsx)(n.admonition,{type:"caution",children:(0,t.jsxs)(n.p,{children:["The early conflict detection in OCC is ",(0,t.jsx)(n.em,{children:(0,t.jsx)(n.strong,{children:"EXPERIMENTAL"})})," in 0.13.0 release."]})}),"\n",(0,t.jsxs)(n.p,{children:["By default, this feature is turned off. To try this out, a user needs to set\n",(0,t.jsx)(n.code,{children:"hoodie.write.concurrency.early.conflict.detection.enable"})," to ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"true"})}),", when using OCC for concurrency control\n(see the ",(0,t.jsx)(n.a,{href:"/docs/concurrency_control",children:"Concurrency Control"})," page for more details)."]}),"\n",(0,t.jsx)(n.h3,{id:"lock-free-message-queue-in-writing-data",children:"Lock-Free Message Queue in Writing Data"}),"\n",(0,t.jsxs)(n.p,{children:["In previous versions, Hudi writes incoming data into a table via a bounded in-memory queue using a producer-consumer\nmodel. In this release, we added a new type of queue, leveraging ",(0,t.jsx)(n.a,{href:"https://lmax-exchange.github.io/disruptor/user-guide/index.html",children:"Disruptor"}),",\nwhich is lock-free. This increases the write throughput when data volume is large.\n",(0,t.jsx)(n.a,{href:"https://github.com/apache/hudi/pull/5416",children:"The benchmark"})," writing 100 million records to 1000 partitions in a Hudi table\non cloud storage shows ",(0,t.jsx)(n.strong,{children:"20%"})," performance improvement compared to the existing executor type of bounded in-memory queue."]}),"\n",(0,t.jsx)(n.admonition,{type:"caution",children:(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"DisruptorExecutor"})," is supported for Spark insert and Spark bulk insert operations as an ",(0,t.jsx)(n.em,{children:(0,t.jsx)(n.strong,{children:"EXPERIMENTAL"})})," feature"]})}),"\n",(0,t.jsxs)(n.p,{children:["Users can set ",(0,t.jsx)(n.code,{children:"hoodie.write.executor.type=DISRUPTOR_EXECUTOR"})," to enable this feature. There are other configurations\nlike ",(0,t.jsx)(n.code,{children:"hoodie.write.wait.strategy"})," and ",(0,t.jsx)(n.code,{children:"hoodie.write.buffer.size"})," to tune the performance further."]}),"\n",(0,t.jsx)(n.h3,{id:"hudi-cli-bundle",children:"Hudi CLI Bundle"}),"\n",(0,t.jsxs)(n.p,{children:["We introduce a new Hudi CLI Bundle, ",(0,t.jsx)(n.a,{href:"https://mvnrepository.com/artifact/org.apache.hudi/hudi-cli-bundle",children:(0,t.jsx)(n.code,{children:"hudi-cli-bundle_2.12"})}),",\nfor Spark 3.x to make Hudi CLI easier and more usable.  A user can now use this single bundle jar\n(published to Maven repository) along with Hudi Spark bundle to start the script\nto launch the Hudi-CLI shell with Spark. This brings ease of deployment for the Hudi-CLI as the user does not need to\ncompile the Hudi CLI module locally, upload jars and address any dependency conflict if there is any, which was the\ncase before this release. Detailed instructions can be found on the ",(0,t.jsx)(n.a,{href:"/docs/cli",children:"Hudi CLI"})," page."]}),"\n",(0,t.jsx)(n.h3,{id:"support-for-flink-116",children:"Support for Flink 1.16"}),"\n",(0,t.jsxs)(n.p,{children:["Flink 1.16.x is integrated with Hudi, using profile param ",(0,t.jsx)(n.code,{children:"-Pflink1.16"})," when compiling the source code to activate the\nversion. Alternatively, use ",(0,t.jsx)(n.a,{href:"https://mvnrepository.com/artifact/org.apache.hudi/hudi-flink1.16-bundle",children:(0,t.jsx)(n.code,{children:"hudi-flink1.16-bundle"})}),".\nFlink 1.15, Flink 1.14 and Flink 1.13 will continue to be supported. Please check the ",(0,t.jsx)(n.a,{href:"#new-flink-bundle",children:"migration guide"}),"\nfor bundle updates."]}),"\n",(0,t.jsx)(n.h3,{id:"json-schema-converter",children:"Json Schema Converter"}),"\n",(0,t.jsxs)(n.p,{children:["For DeltaStreamer users who configure schema registry, a JSON schema converter is added to help convert JSON schema into\nAVRO for the target Hudi table. Set ",(0,t.jsx)(n.code,{children:"hoodie.deltastreamer.schemaprovider.registry.schemaconverter"})," to\n",(0,t.jsx)(n.code,{children:"org.apache.hudi.utilities.schema.converter.JsonToAvroSchemaConverter"})," to use this feature. Users can also implement\nthis interface ",(0,t.jsx)(n.code,{children:"org.apache.hudi.utilities.schema.SchemaRegistryProvider.SchemaConverter"})," to provide custom conversion\nfrom the original schema to AVRO."]}),"\n",(0,t.jsx)(n.h3,{id:"providing-hudi-configs-via-spark-sql-config",children:"Providing Hudi Configs via Spark SQL Config"}),"\n",(0,t.jsx)(n.p,{children:"Users can now provide Hudi configs via Spark SQL conf, for example, setting"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'spark.sql("set hoodie.sql.bulk.insert.enable = true")\n'})}),"\n",(0,t.jsxs)(n.p,{children:["to make sure Hudi is able to use ",(0,t.jsx)(n.code,{children:"BULK_INSERT"})," operation when executing ",(0,t.jsx)(n.code,{children:"INSERT INTO"})," statement."]}),"\n",(0,t.jsx)(n.h2,{id:"known-regressions",children:"Known Regressions"}),"\n",(0,t.jsx)(n.p,{children:"We discovered a regression in Hudi 0.13 release related to metadata table and timeline server interplay with streaming ingestion pipelines."}),"\n",(0,t.jsx)(n.p,{children:"The FileSystemView that Hudi maintains internally could go out of sync due to a occasional race conditions when table services are involved\n(compaction, clustering) and could result in updates and deletes routed to older file versions and hence resulting in missed updates and deletes."}),"\n",(0,t.jsx)(n.p,{children:"Here are the user-flows that could potentially be impacted with this."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["This impacts pipelines using Deltastreamer in ",(0,t.jsx)(n.strong,{children:"continuous mode"})," (sync once is not impacted), Spark streaming, or if you have been directly\nusing write client across batches/commits instead of the standard ways to write to Hudi. In other words, batch writes should not be impacted."]}),"\n",(0,t.jsxs)(n.li,{children:["Among these write models, this could have an impact only when table services are enabled.","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"COW: clustering enabled (inline or async)"}),"\n",(0,t.jsx)(n.li,{children:"MOR: compaction enabled (by default, inline or async)"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"Also, the impact is applicable only when metadata table is enabled, and timeline server is enabled (which are defaults as of 0.13.0)"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Based on some production data, we expect this issue might impact roughly < 1% of updates to be missed, since its a race condition\nand table services are generally scheduled once every N commits. The percentage of update misses could be even less if the\nfrequency of table services is less."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"https://issues.apache.org/jira/browse/HUDI-5863",children:"Here"})," is the jira for the issue of interest and the fix has already been landed in master.\nNext minor release(0.13.1) should have the ",(0,t.jsx)(n.a,{href:"https://github.com/apache/hudi/pull/8079",children:"fix"}),". Until we have a next minor release with the fix, we recommend you to disable metadata table\n(",(0,t.jsx)(n.code,{children:"hoodie.metadata.enable=false"}),") to mitigate the issue."]}),"\n",(0,t.jsxs)(n.p,{children:["We also discovered a regression for Flink streaming writer with the hive meta sync which is introduced by HUDI-3730, the refactoring to ",(0,t.jsx)(n.code,{children:"HiveSyncConfig"}),"\ncauses the Hive ",(0,t.jsx)(n.code,{children:"Resources"})," config objects leaking, which finally leads to an OOM exception for the JobManager if the streaming job runs continuously for weeks.\nNext minor release(0.13.1) should have the ",(0,t.jsx)(n.a,{href:"https://github.com/apache/hudi/pull/8050",children:"fix"}),". Until we have a 0.13.1 release, we recommend you to cherry-pick the fix to local\nif hive meta sync is required."]}),"\n",(0,t.jsx)(n.p,{children:"Sorry about the inconvenience caused."}),"\n",(0,t.jsx)(n.h2,{id:"raw-release-notes",children:"Raw Release Notes"}),"\n",(0,t.jsxs)(n.p,{children:["The raw release notes are available ",(0,t.jsx)(n.a,{href:"https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12322822&version=12352101",children:"here"}),"."]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},19365:(e,n,i)=>{i.d(n,{A:()=>s});i(96540);var r=i(34164);const t={tabItem:"tabItem_Ymn6"};var a=i(74848);function s(e){let{children:n,hidden:i,className:s}=e;return(0,a.jsx)("div",{role:"tabpanel",className:(0,r.A)(t.tabItem,s),hidden:i,children:n})}},11470:(e,n,i)=>{i.d(n,{A:()=>w});var r=i(96540),t=i(34164),a=i(23104),s=i(56347),o=i(205),l=i(57485),d=i(31682),c=i(70679);function h(e){return r.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function u(e){const{values:n,children:i}=e;return(0,r.useMemo)((()=>{const e=n??function(e){return h(e).map((e=>{let{props:{value:n,label:i,attributes:r,default:t}}=e;return{value:n,label:i,attributes:r,default:t}}))}(i);return function(e){const n=(0,d.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,i])}function p(e){let{value:n,tabValues:i}=e;return i.some((e=>e.value===n))}function m(e){let{queryString:n=!1,groupId:i}=e;const t=(0,s.W6)(),a=function(e){let{queryString:n=!1,groupId:i}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!i)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return i??null}({queryString:n,groupId:i});return[(0,l.aZ)(a),(0,r.useCallback)((e=>{if(!a)return;const n=new URLSearchParams(t.location.search);n.set(a,e),t.replace({...t.location,search:n.toString()})}),[a,t])]}function g(e){const{defaultValue:n,queryString:i=!1,groupId:t}=e,a=u(e),[s,l]=(0,r.useState)((()=>function(e){let{defaultValue:n,tabValues:i}=e;if(0===i.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!p({value:n,tabValues:i}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${i.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const r=i.find((e=>e.default))??i[0];if(!r)throw new Error("Unexpected error: 0 tabValues");return r.value}({defaultValue:n,tabValues:a}))),[d,h]=m({queryString:i,groupId:t}),[g,f]=function(e){let{groupId:n}=e;const i=function(e){return e?`docusaurus.tab.${e}`:null}(n),[t,a]=(0,c.Dv)(i);return[t,(0,r.useCallback)((e=>{i&&a.set(e)}),[i,a])]}({groupId:t}),x=(()=>{const e=d??g;return p({value:e,tabValues:a})?e:null})();(0,o.A)((()=>{x&&l(x)}),[x]);return{selectedValue:s,selectValue:(0,r.useCallback)((e=>{if(!p({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);l(e),h(e),f(e)}),[h,f,a]),tabValues:a}}var f=i(92303);const x={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var j=i(74848);function b(e){let{className:n,block:i,selectedValue:r,selectValue:s,tabValues:o}=e;const l=[],{blockElementScrollPositionUntilNextRender:d}=(0,a.a_)(),c=e=>{const n=e.currentTarget,i=l.indexOf(n),t=o[i].value;t!==r&&(d(n),s(t))},h=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const i=l.indexOf(e.currentTarget)+1;n=l[i]??l[0];break}case"ArrowLeft":{const i=l.indexOf(e.currentTarget)-1;n=l[i]??l[l.length-1];break}}n?.focus()};return(0,j.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,t.A)("tabs",{"tabs--block":i},n),children:o.map((e=>{let{value:n,label:i,attributes:a}=e;return(0,j.jsx)("li",{role:"tab",tabIndex:r===n?0:-1,"aria-selected":r===n,ref:e=>l.push(e),onKeyDown:h,onClick:c,...a,className:(0,t.A)("tabs__item",x.tabItem,a?.className,{"tabs__item--active":r===n}),children:i??n},n)}))})}function v(e){let{lazy:n,children:i,selectedValue:a}=e;const s=(Array.isArray(i)?i:[i]).filter(Boolean);if(n){const e=s.find((e=>e.props.value===a));return e?(0,r.cloneElement)(e,{className:(0,t.A)("margin-top--md",e.props.className)}):null}return(0,j.jsx)("div",{className:"margin-top--md",children:s.map(((e,n)=>(0,r.cloneElement)(e,{key:n,hidden:e.props.value!==a})))})}function y(e){const n=g(e);return(0,j.jsxs)("div",{className:(0,t.A)("tabs-container",x.tabList),children:[(0,j.jsx)(b,{...n,...e}),(0,j.jsx)(v,{...n,...e})]})}function w(e){const n=(0,f.A)();return(0,j.jsx)(y,{...e,children:h(e.children)},String(n))}},28453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>o});var r=i(96540);const t={},a=r.createContext(t);function s(e){const n=r.useContext(a);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);