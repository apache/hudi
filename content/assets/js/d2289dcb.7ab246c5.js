"use strict";(self.webpackChunkhudi=self.webpackChunkhudi||[]).push([[47989],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>c});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),p=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},d=function(e){var t=p(e.components);return n.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),m=p(a),c=r,k=m["".concat(s,".").concat(c)]||m[c]||u[c]||i;return a?n.createElement(k,l(l({ref:t},d),{},{components:a})):n.createElement(k,l({ref:t},d))}));function c(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,l=new Array(i);l[0]=m;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o.mdxType="string"==typeof e?e:r,l[1]=o;for(var p=2;p<i;p++)l[p]=a[p];return n.createElement.apply(null,l)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},58215:(e,t,a)=>{a.d(t,{Z:()=>r});var n=a(67294);const r=function(e){let{children:t,hidden:a,className:r}=e;return n.createElement("div",{role:"tabpanel",hidden:a,className:r},t)}},26396:(e,t,a)=>{a.d(t,{Z:()=>m});var n=a(87462),r=a(67294),i=a(72389),l=a(79443);const o=function(){const e=(0,r.useContext)(l.Z);if(null==e)throw new Error('"useUserPreferencesContext" is used outside of "Layout" component.');return e};var s=a(53810),p=a(86010);const d="tabItem_vU9c";function u(e){var t,a,i;const{lazy:l,block:u,defaultValue:m,values:c,groupId:k,className:h}=e,g=r.Children.map(e.children,(e=>{if((0,r.isValidElement)(e)&&void 0!==e.props.value)return e;throw new Error("Docusaurus error: Bad <Tabs> child <"+("string"==typeof e.type?e.type:e.type.name)+'>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.')})),N=null!=c?c:g.map((e=>{let{props:{value:t,label:a,attributes:n}}=e;return{value:t,label:a,attributes:n}})),b=(0,s.lx)(N,((e,t)=>e.value===t.value));if(b.length>0)throw new Error('Docusaurus error: Duplicate values "'+b.map((e=>e.value)).join(", ")+'" found in <Tabs>. Every value needs to be unique.');const f=null===m?m:null!=(t=null!=m?m:null==(a=g.find((e=>e.props.default)))?void 0:a.props.value)?t:null==(i=g[0])?void 0:i.props.value;if(null!==f&&!N.some((e=>e.value===f)))throw new Error('Docusaurus error: The <Tabs> has a defaultValue "'+f+'" but none of its children has the corresponding value. Available values are: '+N.map((e=>e.value)).join(", ")+". If you intend to show no default tab, use defaultValue={null} instead.");const{tabGroupChoices:y,setTabGroupChoices:v}=o(),[_,w]=(0,r.useState)(f),T=[],{blockElementScrollPositionUntilNextRender:E}=(0,s.o5)();if(null!=k){const e=y[k];null!=e&&e!==_&&N.some((t=>t.value===e))&&w(e)}const O=e=>{const t=e.currentTarget,a=T.indexOf(t),n=N[a].value;n!==_&&(E(t),w(n),null!=k&&v(k,n))},C=e=>{var t;let a=null;switch(e.key){case"ArrowRight":{const t=T.indexOf(e.currentTarget)+1;a=T[t]||T[0];break}case"ArrowLeft":{const t=T.indexOf(e.currentTarget)-1;a=T[t]||T[T.length-1];break}}null==(t=a)||t.focus()};return r.createElement("div",{className:"tabs-container"},r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,p.Z)("tabs",{"tabs--block":u},h)},N.map((e=>{let{value:t,label:a,attributes:i}=e;return r.createElement("li",(0,n.Z)({role:"tab",tabIndex:_===t?0:-1,"aria-selected":_===t,key:t,ref:e=>T.push(e),onKeyDown:C,onFocus:O,onClick:O},i,{className:(0,p.Z)("tabs__item",d,null==i?void 0:i.className,{"tabs__item--active":_===t})}),null!=a?a:t)}))),l?(0,r.cloneElement)(g.filter((e=>e.props.value===_))[0],{className:"margin-vert--md"}):r.createElement("div",{className:"margin-vert--md"},g.map(((e,t)=>(0,r.cloneElement)(e,{key:t,hidden:e.props.value!==_})))))}function m(e){const t=(0,i.Z)();return r.createElement(u,(0,n.Z)({key:String(t)},e))}},26340:(e,t,a)=>{a.r(t),a.d(t,{contentTitle:()=>s,default:()=>m,frontMatter:()=>o,metadata:()=>p,toc:()=>d});var n=a(87462),r=(a(67294),a(3905)),i=a(26396),l=a(58215);const o={title:"Writing Data",keywords:["hudi","incremental","batch","stream","processing","Hive","ETL","Spark SQL"],summary:"In this page, we will discuss some available tools for incrementally ingesting & storing data.",toc:!0,last_modified_at:new Date("2019-12-30T19:59:57.000Z")},s=void 0,p={unversionedId:"writing_data",id:"version-0.12.0/writing_data",title:"Writing Data",description:"In this section, we will cover ways to ingest new changes from external sources or even other Hudi tables.",source:"@site/versioned_docs/version-0.12.0/writing_data.md",sourceDirName:".",slug:"/writing_data",permalink:"/docs/writing_data",editUrl:"https://github.com/apache/hudi/tree/asf-site/website/versioned_docs/version-0.12.0/writing_data.md",tags:[],version:"0.12.0",frontMatter:{title:"Writing Data",keywords:["hudi","incremental","batch","stream","processing","Hive","ETL","Spark SQL"],summary:"In this page, we will discuss some available tools for incrementally ingesting & storing data.",toc:!0,last_modified_at:"2019-12-30T19:59:57.000Z"},sidebar:"docs",previous:{title:"Procedures",permalink:"/docs/procedures"},next:{title:"Streaming Ingestion",permalink:"/docs/hoodie_deltastreamer"}},d=[{value:"Spark Datasource Writer",id:"spark-datasource-writer",children:[{value:"Insert Overwrite Table",id:"insert-overwrite-table",children:[],level:3},{value:"Insert Overwrite",id:"insert-overwrite",children:[],level:3},{value:"Deletes",id:"deletes",children:[],level:3},{value:"Concurrency Control",id:"concurrency-control",children:[],level:3},{value:"Commit Notifications",id:"commit-notifications",children:[{value:"HTTP Endpoints",id:"http-endpoints",children:[],level:4},{value:"Kafka Endpoints",id:"kafka-endpoints",children:[],level:4},{value:"Pulsar Endpoints",id:"pulsar-endpoints",children:[],level:4},{value:"Bring your own implementation",id:"bring-your-own-implementation",children:[],level:4}],level:3}],level:2},{value:"Flink SQL Writer",id:"flink-sql-writer",children:[],level:2}],u={toc:d};function m(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"In this section, we will cover ways to ingest new changes from external sources or even other Hudi tables.\nThe two main tools available are the ",(0,r.kt)("a",{parentName:"p",href:"/docs/hoodie_deltastreamer#deltastreamer"},"DeltaStreamer")," tool, as well as the ",(0,r.kt)("a",{parentName:"p",href:"#spark-datasource-writer"},"Spark Hudi datasource"),"."),(0,r.kt)("h2",{id:"spark-datasource-writer"},"Spark Datasource Writer"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"hudi-spark")," module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("inlineCode",{parentName:"strong"},"HoodieWriteConfig")),":"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"TABLE_NAME")," (Required)",(0,r.kt)("br",null)),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("inlineCode",{parentName:"strong"},"DataSourceWriteOptions")),":"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"RECORDKEY_FIELD_OPT_KEY")," (Required): Primary key field(s). Record keys uniquely identify a record/row within each partition. If one wants to have a global uniqueness, there are two options. You could either make the dataset non-partitioned, or, you can leverage Global indexes to ensure record keys are unique irrespective of the partition path. Record keys can either be a single column or refer to multiple columns. ",(0,r.kt)("inlineCode",{parentName:"p"},"KEYGENERATOR_CLASS_OPT_KEY")," property should be set accordingly based on whether it is a simple or complex key. For eg: ",(0,r.kt)("inlineCode",{parentName:"p"},'"col1"')," for simple field, ",(0,r.kt)("inlineCode",{parentName:"p"},'"col1,col2,col3,etc"')," for complex field. Nested fields can be specified using the dot notation eg: ",(0,r.kt)("inlineCode",{parentName:"p"},"a.b.c"),". ",(0,r.kt)("br",null),"\nDefault value: ",(0,r.kt)("inlineCode",{parentName:"p"},'"uuid"'),(0,r.kt)("br",null)),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"PARTITIONPATH_FIELD_OPT_KEY")," (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: ",(0,r.kt)("inlineCode",{parentName:"p"},'""'),". Specify partitioning/no partitioning using ",(0,r.kt)("inlineCode",{parentName:"p"},"KEYGENERATOR_CLASS_OPT_KEY"),". If partition path needs to be url encoded, you can set ",(0,r.kt)("inlineCode",{parentName:"p"},"URL_ENCODE_PARTITIONING_OPT_KEY"),". If synchronizing to hive, also specify using ",(0,r.kt)("inlineCode",{parentName:"p"},"HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY."),(0,r.kt)("br",null),"\nDefault value: ",(0,r.kt)("inlineCode",{parentName:"p"},'"partitionpath"'),(0,r.kt)("br",null)),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"PRECOMBINE_FIELD_OPT_KEY")," (Required): When two records within the same batch have the same key value, the record with the largest value from the field specified will be choosen. If you are using default payload of OverwriteWithLatestAvroPayload for HoodieRecordPayload (",(0,r.kt)("inlineCode",{parentName:"p"},"WRITE_PAYLOAD_CLASS"),"), an incoming record will always takes precendence compared to the one in storage ignoring this ",(0,r.kt)("inlineCode",{parentName:"p"},"PRECOMBINE_FIELD_OPT_KEY"),". ",(0,r.kt)("br",null),"\nDefault value: ",(0,r.kt)("inlineCode",{parentName:"p"},'"ts"'),(0,r.kt)("br",null)),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"OPERATION_OPT_KEY"),": The ",(0,r.kt)("a",{parentName:"p",href:"/docs/write_operations"},"write operations")," to use.",(0,r.kt)("br",null),"\nAvailable values:",(0,r.kt)("br",null),"\n",(0,r.kt)("inlineCode",{parentName:"p"},"UPSERT_OPERATION_OPT_VAL")," (default), ",(0,r.kt)("inlineCode",{parentName:"p"},"BULK_INSERT_OPERATION_OPT_VAL"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"INSERT_OPERATION_OPT_VAL"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"DELETE_OPERATION_OPT_VAL")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"TABLE_TYPE_OPT_KEY"),": The ",(0,r.kt)("a",{parentName:"p",href:"/docs/concepts#table-types"},"type of table")," to write to. Note: After the initial creation of a table, this value must stay consistent when writing to (updating) the table using the Spark ",(0,r.kt)("inlineCode",{parentName:"p"},"SaveMode.Append")," mode.",(0,r.kt)("br",null),"\nAvailable values:",(0,r.kt)("br",null),"\n",(0,r.kt)("a",{parentName:"p",href:"/docs/concepts#copy-on-write-table"},(0,r.kt)("inlineCode",{parentName:"a"},"COW_TABLE_TYPE_OPT_VAL"))," (default), ",(0,r.kt)("a",{parentName:"p",href:"/docs/concepts#merge-on-read-table"},(0,r.kt)("inlineCode",{parentName:"a"},"MOR_TABLE_TYPE_OPT_VAL"))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"KEYGENERATOR_CLASS_OPT_KEY"),": Refer to ",(0,r.kt)("a",{parentName:"p",href:"/docs/key_generation"},"Key Generation")," section below."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY"),": If using hive, specify if the table should or should not be partitioned.",(0,r.kt)("br",null),"\nAvailable values:",(0,r.kt)("br",null),"\n",(0,r.kt)("inlineCode",{parentName:"p"},"classOf[MultiPartKeysValueExtractor].getCanonicalName")," (default), ",(0,r.kt)("inlineCode",{parentName:"p"},"classOf[SlashEncodedDayPartitionValueExtractor].getCanonicalName"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"classOf[TimestampBasedKeyGenerator].getCanonicalName"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"classOf[NonPartitionedExtractor].getCanonicalName"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"classOf[GlobalDeleteKeyGenerator].getCanonicalName")," (to be used when ",(0,r.kt)("inlineCode",{parentName:"p"},"OPERATION_OPT_KEY")," is set to ",(0,r.kt)("inlineCode",{parentName:"p"},"DELETE_OPERATION_OPT_VAL"),")"),(0,r.kt)("p",null,"Example:\nUpsert a DataFrame, specifying the necessary field names for ",(0,r.kt)("inlineCode",{parentName:"p"},"recordKey => _row_key"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"partitionPath => partition"),", and ",(0,r.kt)("inlineCode",{parentName:"p"},"precombineKey => timestamp")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},'inputDF.write()\n       .format("org.apache.hudi")\n       .options(clientOpts) //Where clientOpts is of type Map[String, String]. clientOpts can include any other options necessary.\n       .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), "_row_key")\n       .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), "partition")\n       .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), "timestamp")\n       .option(HoodieWriteConfig.TABLE_NAME, tableName)\n       .mode(SaveMode.Append)\n       .save(basePath);\n')),(0,r.kt)(i.Z,{defaultValue:"scala",values:[{label:"Scala",value:"scala"},{label:"Python",value:"python"},{label:"SparkSQL",value:"sparksql"}],mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"scala",mdxType:"TabItem"},(0,r.kt)("p",null,"Generate some new trips, load them into a DataFrame and write the DataFrame into the Hudi table as below."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'// spark-shell\nval inserts = convertToStringList(dataGen.generateInserts(10))\nval df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\ndf.write.format("hudi").\n  options(getQuickstartWriteConfigs).\n  option(PRECOMBINE_FIELD_OPT_KEY, "ts").\n  option(RECORDKEY_FIELD_OPT_KEY, "uuid").\n  option(PARTITIONPATH_FIELD_OPT_KEY, "partitionpath").\n  option(TABLE_NAME, tableName).\n  mode(Overwrite).\n  save(basePath)\n')),(0,r.kt)("div",{className:"admonition admonition-info alert alert--info"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"info")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},(0,r.kt)("inlineCode",{parentName:"p"},"mode(Overwrite)")," overwrites and recreates the table if it already exists.\nYou can check the data generated under ",(0,r.kt)("inlineCode",{parentName:"p"},"/tmp/hudi_trips_cow/<region>/<country>/<city>/"),". We provided a record key\n(",(0,r.kt)("inlineCode",{parentName:"p"},"uuid")," in ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/6f9b02decb5bb2b83709b1b6ec04a97e4d102c11/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L60"},"schema"),"), partition field (",(0,r.kt)("inlineCode",{parentName:"p"},"region/country/city"),") and combine logic (",(0,r.kt)("inlineCode",{parentName:"p"},"ts")," in\n",(0,r.kt)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/6f9b02decb5bb2b83709b1b6ec04a97e4d102c11/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L60"},"schema"),") to ensure trip records are unique within each partition. For more info, refer to\n",(0,r.kt)("a",{parentName:"p",href:"https://hudi.apache.org/learn/faq/#how-do-i-model-the-data-stored-in-hudi"},"Modeling data stored in Hudi"),"\nand for info on ways to ingest data into Hudi, refer to ",(0,r.kt)("a",{parentName:"p",href:"/docs/writing_data"},"Writing Hudi Tables"),".\nHere we are using the default write operation : ",(0,r.kt)("inlineCode",{parentName:"p"},"upsert"),". If you have a workload without updates, you can also issue\n",(0,r.kt)("inlineCode",{parentName:"p"},"insert")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"bulk_insert")," operations which could be faster. To know more, refer to ",(0,r.kt)("a",{parentName:"p",href:"/docs/write_operations"},"Write operations"))))),(0,r.kt)(l.Z,{value:"python",mdxType:"TabItem"},"Generate some new trips, load them into a DataFrame and write the DataFrame into the Hudi table as below.",(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# pyspark\ninserts = sc._jvm.org.apache.hudi.QuickstartUtils.convertToStringList(dataGen.generateInserts(10))\ndf = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\n\nhudi_options = {\n    'hoodie.table.name': tableName,\n    'hoodie.datasource.write.recordkey.field': 'uuid',\n    'hoodie.datasource.write.partitionpath.field': 'partitionpath',\n    'hoodie.datasource.write.table.name': tableName,\n    'hoodie.datasource.write.operation': 'upsert',\n    'hoodie.datasource.write.precombine.field': 'ts',\n    'hoodie.upsert.shuffle.parallelism': 2,\n    'hoodie.insert.shuffle.parallelism': 2\n}\n\ndf.write.format(\"hudi\").\n    options(**hudi_options).\n    mode(\"overwrite\").\n    save(basePath)\n")),(0,r.kt)("div",{className:"admonition admonition-info alert alert--info"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"info")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},(0,r.kt)("inlineCode",{parentName:"p"},"mode(Overwrite)")," overwrites and recreates the table if it already exists.\nYou can check the data generated under ",(0,r.kt)("inlineCode",{parentName:"p"},"/tmp/hudi_trips_cow/<region>/<country>/<city>/"),". We provided a record key\n(",(0,r.kt)("inlineCode",{parentName:"p"},"uuid")," in ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/2e6e302efec2fa848ded4f88a95540ad2adb7798/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L60"},"schema"),"), partition field (",(0,r.kt)("inlineCode",{parentName:"p"},"region/country/city"),") and combine logic (",(0,r.kt)("inlineCode",{parentName:"p"},"ts")," in\n",(0,r.kt)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/2e6e302efec2fa848ded4f88a95540ad2adb7798/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L60"},"schema"),") to ensure trip records are unique within each partition. For more info, refer to\n",(0,r.kt)("a",{parentName:"p",href:"https://hudi.apache.org/learn/faq/#how-do-i-model-the-data-stored-in-hudi"},"Modeling data stored in Hudi"),"\nand for info on ways to ingest data into Hudi, refer to ",(0,r.kt)("a",{parentName:"p",href:"/docs/writing_data"},"Writing Hudi Tables"),".\nHere we are using the default write operation : ",(0,r.kt)("inlineCode",{parentName:"p"},"upsert"),". If you have a workload without updates, you can also issue\n",(0,r.kt)("inlineCode",{parentName:"p"},"insert")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"bulk_insert")," operations which could be faster. To know more, refer to ",(0,r.kt)("a",{parentName:"p",href:"/docs/write_operations"},"Write operations"))))),(0,r.kt)(l.Z,{value:"sparksql",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"insert into h0 select 1, 'a1', 20;\n\n-- insert static partition\ninsert into h_p0 partition(dt = '2021-01-02') select 1, 'a1';\n\n-- insert dynamic partition\ninsert into h_p0 select 1, 'a1', dt;\n\n-- insert dynamic partition\ninsert into h_p1 select 1 as id, 'a1', '2021-01-03' as dt, '19' as hh;\n\n-- insert overwrite table\ninsert overwrite table h0 select 1, 'a1', 20;\n\n-- insert overwrite table with static partition\ninsert overwrite h_p0 partition(dt = '2021-01-02') select 1, 'a1';\n\n-- insert overwrite table with dynamic partition\n  insert overwrite table h_p1 select 2 as id, 'a2', '2021-01-03' as dt, '19' as hh;\n")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"NOTICE")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Insert mode : Hudi supports two insert modes when inserting data to a table with primary key(we call it pk-table as followed):",(0,r.kt)("br",null),"\nUsing ",(0,r.kt)("inlineCode",{parentName:"p"},"strict")," mode, insert statement will keep the primary key uniqueness constraint for COW table which do not allow\nduplicate records. If a record already exists during insert, a HoodieDuplicateKeyException will be thrown\nfor COW table. For MOR table, updates are allowed to existing record.",(0,r.kt)("br",null),"\nUsing ",(0,r.kt)("inlineCode",{parentName:"p"},"non-strict")," mode, hudi uses the same code path used by ",(0,r.kt)("inlineCode",{parentName:"p"},"insert")," operation in spark data source for the pk-table. ",(0,r.kt)("br",null),"\nOne can set the insert mode by using the config: ",(0,r.kt)("strong",{parentName:"p"},"hoodie.sql.insert.mode"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Bulk Insert : By default, hudi uses the normal insert operation for insert statements. Users can set ",(0,r.kt)("strong",{parentName:"p"},"hoodie.sql.bulk.insert.enable"),"\nto true to enable the bulk insert for insert statement."))))),(0,r.kt)("p",null,"Checkout ",(0,r.kt)("a",{parentName:"p",href:"https://hudi.apache.org/blog/2021/02/13/hudi-key-generators"},"https://hudi.apache.org/blog/2021/02/13/hudi-key-generators")," for various key generator options, like Timestamp based,\ncomplex, custom, NonPartitioned Key gen, etc."),(0,r.kt)("h3",{id:"insert-overwrite-table"},"Insert Overwrite Table"),(0,r.kt)("p",null,"Generate some new trips, overwrite the table logically at the Hudi metadata level. The Hudi cleaner will eventually\nclean up the previous table snapshot's file groups. This can be faster than deleting the older table and recreating\nin ",(0,r.kt)("inlineCode",{parentName:"p"},"Overwrite")," mode."),(0,r.kt)(i.Z,{defaultValue:"scala",values:[{label:"Scala",value:"scala"},{label:"SparkSQL",value:"sparksql"}],mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'// spark-shell\nspark.\n  read.format("hudi").\n  load(basePath).\n  select("uuid","partitionpath").\n  show(10, false)\n\nval inserts = convertToStringList(dataGen.generateInserts(10))\nval df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\ndf.write.format("hudi").\n  options(getQuickstartWriteConfigs).\n  option(OPERATION_OPT_KEY,"insert_overwrite_table").\n  option(PRECOMBINE_FIELD_OPT_KEY, "ts").\n  option(RECORDKEY_FIELD_OPT_KEY, "uuid").\n  option(PARTITIONPATH_FIELD_OPT_KEY, "partitionpath").\n  option(TABLE_NAME, tableName).\n  mode(Append).\n  save(basePath)\n\n// Should have different keys now, from query before.\nspark.\n  read.format("hudi").\n  load(basePath).\n  select("uuid","partitionpath").\n  show(10, false)\n\n'))),(0,r.kt)(l.Z,{value:"sparksql",mdxType:"TabItem"},(0,r.kt)("p",null,"The insert overwrite non-partitioned table sql statement will convert to the ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"insert_overwrite_table"))," operation.\ne.g."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"insert overwrite table h0 select 1, 'a1', 20;\n")))),(0,r.kt)("h3",{id:"insert-overwrite"},"Insert Overwrite"),(0,r.kt)("p",null,"Generate some new trips, overwrite the all the partitions that are present in the input. This operation can be faster\nthan ",(0,r.kt)("inlineCode",{parentName:"p"},"upsert")," for batch ETL jobs, that are recomputing entire target partitions at once (as opposed to incrementally\nupdating the target tables). This is because, we are able to bypass indexing, precombining and other repartitioning\nsteps in the upsert write path completely."),(0,r.kt)(i.Z,{defaultValue:"scala",values:[{label:"Scala",value:"scala"},{label:"SparkSQL",value:"sparksql"}],mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'// spark-shell\nspark.\n  read.format("hudi").\n  load(basePath).\n  select("uuid","partitionpath").\n  sort("partitionpath","uuid").\n  show(100, false)\n\nval inserts = convertToStringList(dataGen.generateInserts(10))\nval df = spark.\n  read.json(spark.sparkContext.parallelize(inserts, 2)).\n  filter("partitionpath = \'americas/united_states/san_francisco\'")\ndf.write.format("hudi").\n  options(getQuickstartWriteConfigs).\n  option(OPERATION_OPT_KEY,"insert_overwrite").\n  option(PRECOMBINE_FIELD_OPT_KEY, "ts").\n  option(RECORDKEY_FIELD_OPT_KEY, "uuid").\n  option(PARTITIONPATH_FIELD_OPT_KEY, "partitionpath").\n  option(TABLE_NAME, tableName).\n  mode(Append).\n  save(basePath)\n\n// Should have different keys now for San Francisco alone, from query before.\nspark.\n  read.format("hudi").\n  load(basePath).\n  select("uuid","partitionpath").\n  sort("partitionpath","uuid").\n  show(100, false)\n'))),(0,r.kt)(l.Z,{value:"sparksql",mdxType:"TabItem"},(0,r.kt)("p",null,"The insert overwrite partitioned table sql statement will convert to the ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"insert_overwrite"))," operation.\ne.g."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"insert overwrite table h_p1 select 2 as id, 'a2', '2021-01-03' as dt, '19' as hh;\n")))),(0,r.kt)("h3",{id:"deletes"},"Deletes"),(0,r.kt)("p",null,"Hudi supports implementing two types of deletes on data stored in Hudi tables, by enabling the user to specify a different record payload implementation.\nFor more info refer to ",(0,r.kt)("a",{parentName:"p",href:"https://cwiki.apache.org/confluence/x/6IqvC"},"Delete support in Hudi"),"."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Soft Deletes")," : Retain the record key and just null out the values for all the other fields.\nThis can be achieved by ensuring the appropriate fields are nullable in the table schema and simply upserting the table after setting these fields to null.\nNote that soft deletes are always persisted in storage and never removed, but all values are set to nulls.\nSo for GDPR or other compliance reasons, users should consider doing hard deletes if record key and partition path\ncontain PII.")),(0,r.kt)("p",null,"For example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'// fetch two records for soft deletes\nval softDeleteDs = spark.sql("select * from hudi_trips_snapshot").limit(2)\n\n// prepare the soft deletes by ensuring the appropriate fields are nullified\nval nullifyColumns = softDeleteDs.schema.fields.\n  map(field => (field.name, field.dataType.typeName)).\n  filter(pair => (!HoodieRecord.HOODIE_META_COLUMNS.contains(pair._1)\n    && !Array("ts", "uuid", "partitionpath").contains(pair._1)))\n\nval softDeleteDf = nullifyColumns.\n  foldLeft(softDeleteDs.drop(HoodieRecord.HOODIE_META_COLUMNS: _*))(\n    (ds, col) => ds.withColumn(col._1, lit(null).cast(col._2)))\n\n// simply upsert the table after setting these fields to null\nsoftDeleteDf.write.format("hudi").\n  options(getQuickstartWriteConfigs).\n  option(OPERATION_OPT_KEY, "upsert").\n  option(PRECOMBINE_FIELD_OPT_KEY, "ts").\n  option(RECORDKEY_FIELD_OPT_KEY, "uuid").\n  option(PARTITIONPATH_FIELD_OPT_KEY, "partitionpath").\n  option(TABLE_NAME, tableName).\n  mode(Append).\n  save(basePath)\n')),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Hard Deletes")," : A stronger form of deletion is to physically remove any trace of the record from the table. This can be achieved in 3 different ways. ")),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Using Datasource, set ",(0,r.kt)("inlineCode",{parentName:"li"},"OPERATION_OPT_KEY")," to ",(0,r.kt)("inlineCode",{parentName:"li"},"DELETE_OPERATION_OPT_VAL"),". This will remove all the records in the DataSet being submitted.")),(0,r.kt)("p",null,"Example, first read in a dataset:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'val roViewDF = spark.\n        read.\n        format("org.apache.hudi").\n        load(basePath + "/*/*/*/*")\nroViewDF.createOrReplaceTempView("hudi_ro_table")\nspark.sql("select count(*) from hudi_ro_table").show() // should return 10 (number of records inserted above)\nval riderValue = spark.sql("select distinct rider from hudi_ro_table").show()\n// copy the value displayed to be used in next step\n')),(0,r.kt)("p",null,"Now write a query of which records you would like to delete:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},"val df = spark.sql(\"select uuid, partitionPath from hudi_ro_table where rider = 'rider-213'\")\n")),(0,r.kt)("p",null,"Lastly, execute the deletion of these records:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'val deletes = dataGen.generateDeletes(df.collectAsList())\nval df = spark.read.json(spark.sparkContext.parallelize(deletes, 2));\ndf.write.format("org.apache.hudi").\noptions(getQuickstartWriteConfigs).\noption(OPERATION_OPT_KEY,"delete").\noption(PRECOMBINE_FIELD_OPT_KEY, "ts").\noption(RECORDKEY_FIELD_OPT_KEY, "uuid").\noption(PARTITIONPATH_FIELD_OPT_KEY, "partitionpath").\noption(TABLE_NAME, tableName).\nmode(Append).\nsave(basePath);\n')),(0,r.kt)("ol",{start:2},(0,r.kt)("li",{parentName:"ol"},"Using DataSource, set ",(0,r.kt)("inlineCode",{parentName:"li"},"PAYLOAD_CLASS_OPT_KEY")," to ",(0,r.kt)("inlineCode",{parentName:"li"},'"org.apache.hudi.EmptyHoodieRecordPayload"'),". This will remove all the records in the DataSet being submitted. ")),(0,r.kt)("p",null,"This example will remove all the records from the table that exist in the DataSet ",(0,r.kt)("inlineCode",{parentName:"p"},"deleteDF"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},' deleteDF // dataframe containing just records to be deleted\n   .write().format("org.apache.hudi")\n   .option(...) // Add HUDI options like record-key, partition-path and others as needed for your setup\n   // specify record_key, partition_key, precombine_fieldkey & usual params\n   .option(DataSourceWriteOptions.PAYLOAD_CLASS_OPT_KEY, "org.apache.hudi.EmptyHoodieRecordPayload")\n')),(0,r.kt)("ol",{start:3},(0,r.kt)("li",{parentName:"ol"},"Using DataSource or DeltaStreamer, add a column named ",(0,r.kt)("inlineCode",{parentName:"li"},"_hoodie_is_deleted")," to DataSet. The value of this column must be set to ",(0,r.kt)("inlineCode",{parentName:"li"},"true")," for all the records to be deleted and either ",(0,r.kt)("inlineCode",{parentName:"li"},"false")," or left null for any records which are to be upserted.")),(0,r.kt)("p",null,"Let's say the original schema is:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "type":"record",\n  "name":"example_tbl",\n  "fields":[{\n     "name": "uuid",\n     "type": "String"\n  }, {\n     "name": "ts",\n     "type": "string"\n  },  {\n     "name": "partitionPath",\n     "type": "string"\n  }, {\n     "name": "rank",\n     "type": "long"\n  }\n]}\n')),(0,r.kt)("p",null,"Make sure you add ",(0,r.kt)("inlineCode",{parentName:"p"},"_hoodie_is_deleted")," column:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "type":"record",\n  "name":"example_tbl",\n  "fields":[{\n     "name": "uuid",\n     "type": "String"\n  }, {\n     "name": "ts",\n     "type": "string"\n  },  {\n     "name": "partitionPath",\n     "type": "string"\n  }, {\n     "name": "rank",\n     "type": "long"\n  }, {\n    "name" : "_hoodie_is_deleted",\n    "type" : "boolean",\n    "default" : false\n  }\n]}\n')),(0,r.kt)("p",null,"Then any record you want to delete you can mark ",(0,r.kt)("inlineCode",{parentName:"p"},"_hoodie_is_deleted")," as true:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{"ts": 0.0, "uuid": "19tdb048-c93e-4532-adf9-f61ce6afe10", "rank": 1045, "partitionpath": "americas/brazil/sao_paulo", "_hoodie_is_deleted" : true}\n')),(0,r.kt)("h3",{id:"concurrency-control"},"Concurrency Control"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"hudi-spark")," module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table."),(0,r.kt)("p",null,"Following is an example of how to use optimistic_concurrency_control via spark datasource. Read more in depth about concurrency control in the ",(0,r.kt)("a",{parentName:"p",href:"https://hudi.apache.org/docs/concurrency_control"},"concurrency control concepts")," section  "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},'inputDF.write.format("hudi")\n       .options(getQuickstartWriteConfigs)\n       .option(PRECOMBINE_FIELD_OPT_KEY, "ts")\n       .option("hoodie.cleaner.policy.failed.writes", "LAZY")\n       .option("hoodie.write.concurrency.mode", "optimistic_concurrency_control")\n       .option("hoodie.write.lock.zookeeper.url", "zookeeper")\n       .option("hoodie.write.lock.zookeeper.port", "2181")\n       .option("hoodie.write.lock.zookeeper.lock_key", "test_table")\n       .option("hoodie.write.lock.zookeeper.base_path", "/test")\n       .option(RECORDKEY_FIELD_OPT_KEY, "uuid")\n       .option(PARTITIONPATH_FIELD_OPT_KEY, "partitionpath")\n       .option(TABLE_NAME, tableName)\n       .mode(Overwrite)\n       .save(basePath)\n')),(0,r.kt)("h3",{id:"commit-notifications"},"Commit Notifications"),(0,r.kt)("p",null,"Apache Hudi provides the ability to post a callback notification about a write commit. This may be valuable if you need\nan event notification stream to take actions with other services after a Hudi write commit.\nYou can push a write commit callback notification into HTTP endpoints or to a Kafka server."),(0,r.kt)("h4",{id:"http-endpoints"},"HTTP Endpoints"),(0,r.kt)("p",null,"You can push a commit notification to an HTTP URL and can specify custom values by extending a callback class defined below."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Config"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"),(0,r.kt)("th",{parentName:"tr",align:null},"Required"),(0,r.kt)("th",{parentName:"tr",align:null},"Default"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"TURN_CALLBACK_ON"),(0,r.kt)("td",{parentName:"tr",align:null},"Turn commit callback on/off"),(0,r.kt)("td",{parentName:"tr",align:null},"optional"),(0,r.kt)("td",{parentName:"tr",align:null},"false (",(0,r.kt)("em",{parentName:"td"},"callbacks off"),")")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"CALLBACK_HTTP_URL"),(0,r.kt)("td",{parentName:"tr",align:null},"Callback host to be sent along with callback messages"),(0,r.kt)("td",{parentName:"tr",align:null},"required"),(0,r.kt)("td",{parentName:"tr",align:null},"N/A")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"CALLBACK_HTTP_TIMEOUT_IN_SECONDS"),(0,r.kt)("td",{parentName:"tr",align:null},"Callback timeout in seconds"),(0,r.kt)("td",{parentName:"tr",align:null},"optional"),(0,r.kt)("td",{parentName:"tr",align:null},"3")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"CALLBACK_CLASS_NAME"),(0,r.kt)("td",{parentName:"tr",align:null},"Full path of callback class and must be a subclass of HoodieWriteCommitCallback class, org.apache.hudi.callback.impl.HoodieWriteCommitHttpCallback by default"),(0,r.kt)("td",{parentName:"tr",align:null},"optional"),(0,r.kt)("td",{parentName:"tr",align:null},"org.apache.hudi.callback.impl.HoodieWriteCommitHttpCallback")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"CALLBACK_HTTP_API_KEY_VALUE"),(0,r.kt)("td",{parentName:"tr",align:null},"Http callback API key"),(0,r.kt)("td",{parentName:"tr",align:null},"optional"),(0,r.kt)("td",{parentName:"tr",align:null},"hudi_write_commit_http_callback")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null})))),(0,r.kt)("h4",{id:"kafka-endpoints"},"Kafka Endpoints"),(0,r.kt)("p",null,"You can push a commit notification to a Kafka topic so it can be used by other real time systems."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Config"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"),(0,r.kt)("th",{parentName:"tr",align:null},"Required"),(0,r.kt)("th",{parentName:"tr",align:null},"Default"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"TOPIC"),(0,r.kt)("td",{parentName:"tr",align:null},"Kafka topic name to publish timeline activity into."),(0,r.kt)("td",{parentName:"tr",align:null},"required"),(0,r.kt)("td",{parentName:"tr",align:null},"N/A")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"PARTITION"),(0,r.kt)("td",{parentName:"tr",align:null},"It may be desirable to serialize all changes into a single Kafka partition for providing strict ordering. By default, Kafka messages are keyed by table name, which guarantees ordering at the table level, but not globally (or when new partitions are added)"),(0,r.kt)("td",{parentName:"tr",align:null},"required"),(0,r.kt)("td",{parentName:"tr",align:null},"N/A")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"RETRIES"),(0,r.kt)("td",{parentName:"tr",align:null},"Times to retry the produce"),(0,r.kt)("td",{parentName:"tr",align:null},"optional"),(0,r.kt)("td",{parentName:"tr",align:null},"3")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"ACKS"),(0,r.kt)("td",{parentName:"tr",align:null},"kafka acks level, all by default to ensure strong durability"),(0,r.kt)("td",{parentName:"tr",align:null},"optional"),(0,r.kt)("td",{parentName:"tr",align:null},"all")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"BOOTSTRAP_SERVERS"),(0,r.kt)("td",{parentName:"tr",align:null},"Bootstrap servers of kafka cluster, to be used for publishing commit metadata"),(0,r.kt)("td",{parentName:"tr",align:null},"required"),(0,r.kt)("td",{parentName:"tr",align:null},"N/A")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null})))),(0,r.kt)("h4",{id:"pulsar-endpoints"},"Pulsar Endpoints"),(0,r.kt)("p",null,"You can push a commit notification to a Pulsar topic so it can be used by other real time systems. "),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Config"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"),(0,r.kt)("th",{parentName:"tr",align:null},"Required"),(0,r.kt)("th",{parentName:"tr",align:null},"Default"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hoodie.write.commit.callback.pulsar.broker.service.url"),(0,r.kt)("td",{parentName:"tr",align:null},"Server's Url of pulsar cluster to use to publish commit metadata."),(0,r.kt)("td",{parentName:"tr",align:null},"required"),(0,r.kt)("td",{parentName:"tr",align:null},"N/A")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hoodie.write.commit.callback.pulsar.topic"),(0,r.kt)("td",{parentName:"tr",align:null},"Pulsar topic name to publish timeline activity into"),(0,r.kt)("td",{parentName:"tr",align:null},"required"),(0,r.kt)("td",{parentName:"tr",align:null},"N/A")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hoodie.write.commit.callback.pulsar.producer.route-mode"),(0,r.kt)("td",{parentName:"tr",align:null},"Message routing logic for producers on partitioned topics."),(0,r.kt)("td",{parentName:"tr",align:null},"optional"),(0,r.kt)("td",{parentName:"tr",align:null},"RoundRobinPartition")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hoodie.write.commit.callback.pulsar.producer.pending-queue-size"),(0,r.kt)("td",{parentName:"tr",align:null},"The maximum size of a queue holding pending messages."),(0,r.kt)("td",{parentName:"tr",align:null},"optional"),(0,r.kt)("td",{parentName:"tr",align:null},"1000")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hoodie.write.commit.callback.pulsar.producer.pending-total-size"),(0,r.kt)("td",{parentName:"tr",align:null},"The maximum number of pending messages across partitions."),(0,r.kt)("td",{parentName:"tr",align:null},"required"),(0,r.kt)("td",{parentName:"tr",align:null},"50000")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hoodie.write.commit.callback.pulsar.producer.block-if-queue-full"),(0,r.kt)("td",{parentName:"tr",align:null},"When the queue is full, the method is blocked instead of an exception is thrown."),(0,r.kt)("td",{parentName:"tr",align:null},"optional"),(0,r.kt)("td",{parentName:"tr",align:null},"true")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hoodie.write.commit.callback.pulsar.producer.send-timeout"),(0,r.kt)("td",{parentName:"tr",align:null},"The timeout in each sending to pulsar."),(0,r.kt)("td",{parentName:"tr",align:null},"optional"),(0,r.kt)("td",{parentName:"tr",align:null},"30s")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hoodie.write.commit.callback.pulsar.operation-timeout"),(0,r.kt)("td",{parentName:"tr",align:null},"Duration of waiting for completing an operation."),(0,r.kt)("td",{parentName:"tr",align:null},"optional"),(0,r.kt)("td",{parentName:"tr",align:null},"30s")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hoodie.write.commit.callback.pulsar.connection-timeout"),(0,r.kt)("td",{parentName:"tr",align:null},"Duration of waiting for a connection to a broker to be established."),(0,r.kt)("td",{parentName:"tr",align:null},"optional"),(0,r.kt)("td",{parentName:"tr",align:null},"10s")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hoodie.write.commit.callback.pulsar.request-timeout"),(0,r.kt)("td",{parentName:"tr",align:null},"Duration of waiting for completing a request."),(0,r.kt)("td",{parentName:"tr",align:null},"optional"),(0,r.kt)("td",{parentName:"tr",align:null},"60s")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hoodie.write.commit.callback.pulsar.keepalive-interval"),(0,r.kt)("td",{parentName:"tr",align:null},"Duration of keeping alive interval for each client broker connection."),(0,r.kt)("td",{parentName:"tr",align:null},"optional"),(0,r.kt)("td",{parentName:"tr",align:null},"30s")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null})))),(0,r.kt)("h4",{id:"bring-your-own-implementation"},"Bring your own implementation"),(0,r.kt)("p",null,"You can extend the HoodieWriteCommitCallback class to implement your own way to asynchronously handle the callback\nof a successful write. Use this public API:"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/master/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/callback/HoodieWriteCommitCallback.java"},"https://github.com/apache/hudi/blob/master/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/callback/HoodieWriteCommitCallback.java")),(0,r.kt)("h2",{id:"flink-sql-writer"},"Flink SQL Writer"),(0,r.kt)("p",null,"The hudi-flink module defines the Flink SQL connector for both hudi source and sink.\nThere are a number of options available for the sink table:"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Option Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Required"),(0,r.kt)("th",{parentName:"tr",align:null},"Default"),(0,r.kt)("th",{parentName:"tr",align:null},"Remarks"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"path"),(0,r.kt)("td",{parentName:"tr",align:null},"Y"),(0,r.kt)("td",{parentName:"tr",align:null},"N/A"),(0,r.kt)("td",{parentName:"tr",align:null},"Base path for the target hoodie table. The path would be created if it does not exist, otherwise a hudi table expects to be initialized successfully")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"table.type"),(0,r.kt)("td",{parentName:"tr",align:null},"N"),(0,r.kt)("td",{parentName:"tr",align:null},"COPY_ON_WRITE"),(0,r.kt)("td",{parentName:"tr",align:null},"Type of table to write. COPY_ON_WRITE (or) MERGE_ON_READ")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"write.operation"),(0,r.kt)("td",{parentName:"tr",align:null},"N"),(0,r.kt)("td",{parentName:"tr",align:null},"upsert"),(0,r.kt)("td",{parentName:"tr",align:null},"The write operation, that this write should do (insert or upsert is supported)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"write.precombine.field"),(0,r.kt)("td",{parentName:"tr",align:null},"N"),(0,r.kt)("td",{parentName:"tr",align:null},"ts"),(0,r.kt)("td",{parentName:"tr",align:null},"Field used in preCombining before actual write. When two records have the same key value, we will pick the one with the largest value for the precombine field, determined by Object.compareTo(..)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"write.payload.class"),(0,r.kt)("td",{parentName:"tr",align:null},"N"),(0,r.kt)("td",{parentName:"tr",align:null},"OverwriteWithLatestAvroPayload.class"),(0,r.kt)("td",{parentName:"tr",align:null},"Payload class used. Override this, if you like to roll your own merge logic, when upserting/inserting. This will render any value set for the option in-effective")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"write.insert.drop.duplicates"),(0,r.kt)("td",{parentName:"tr",align:null},"N"),(0,r.kt)("td",{parentName:"tr",align:null},"false"),(0,r.kt)("td",{parentName:"tr",align:null},"Flag to indicate whether to drop duplicates upon insert. By default insert will accept duplicates, to gain extra performance")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"write.ignore.failed"),(0,r.kt)("td",{parentName:"tr",align:null},"N"),(0,r.kt)("td",{parentName:"tr",align:null},"true"),(0,r.kt)("td",{parentName:"tr",align:null},"Flag to indicate whether to ignore any non exception error (e.g. writestatus error). within a checkpoint batch. By default true (in favor of streaming progressing over data integrity)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hoodie.datasource.write.recordkey.field"),(0,r.kt)("td",{parentName:"tr",align:null},"N"),(0,r.kt)("td",{parentName:"tr",align:null},"uuid"),(0,r.kt)("td",{parentName:"tr",align:null},"Record key field. Value to be used as the ",(0,r.kt)("inlineCode",{parentName:"td"},"recordKey")," component of ",(0,r.kt)("inlineCode",{parentName:"td"},"HoodieKey"),". Actual value will be obtained by invoking .toString() on the field value. Nested fields can be specified using the dot notation eg: ",(0,r.kt)("inlineCode",{parentName:"td"},"a.b.c"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"hoodie.datasource.write.keygenerator.class"),(0,r.kt)("td",{parentName:"tr",align:null},"N"),(0,r.kt)("td",{parentName:"tr",align:null},"SimpleAvroKeyGenerator.class"),(0,r.kt)("td",{parentName:"tr",align:null},"Key generator class, that implements will extract the key out of incoming record")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"write.tasks"),(0,r.kt)("td",{parentName:"tr",align:null},"N"),(0,r.kt)("td",{parentName:"tr",align:null},"4"),(0,r.kt)("td",{parentName:"tr",align:null},"Parallelism of tasks that do actual write, default is 4")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"write.batch.size.MB"),(0,r.kt)("td",{parentName:"tr",align:null},"N"),(0,r.kt)("td",{parentName:"tr",align:null},"128"),(0,r.kt)("td",{parentName:"tr",align:null},"Batch buffer size in MB to flush data into the underneath filesystem")))),(0,r.kt)("p",null,"If the table type is MERGE_ON_READ, you can also specify the asynchronous compaction strategy through options:"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Option Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Required"),(0,r.kt)("th",{parentName:"tr",align:null},"Default"),(0,r.kt)("th",{parentName:"tr",align:null},"Remarks"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"compaction.async.enabled"),(0,r.kt)("td",{parentName:"tr",align:null},"N"),(0,r.kt)("td",{parentName:"tr",align:null},"true"),(0,r.kt)("td",{parentName:"tr",align:null},"Async Compaction, enabled by default for MOR")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"compaction.trigger.strategy"),(0,r.kt)("td",{parentName:"tr",align:null},"N"),(0,r.kt)("td",{parentName:"tr",align:null},"num_commits"),(0,r.kt)("td",{parentName:"tr",align:null},"Strategy to trigger compaction, options are 'num_commits': trigger compaction when reach N delta commits; 'time_elapsed': trigger compaction when time elapsed > N seconds since last compaction; 'num_and_time': trigger compaction when both NUM_COMMITS and TIME_ELAPSED are satisfied; 'num_or_time': trigger compaction when NUM_COMMITS or TIME_ELAPSED is satisfied. Default is 'num_commits'")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"compaction.delta_commits"),(0,r.kt)("td",{parentName:"tr",align:null},"N"),(0,r.kt)("td",{parentName:"tr",align:null},"5"),(0,r.kt)("td",{parentName:"tr",align:null},"Max delta commits needed to trigger compaction, default 5 commits")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"compaction.delta_seconds"),(0,r.kt)("td",{parentName:"tr",align:null},"N"),(0,r.kt)("td",{parentName:"tr",align:null},"3600"),(0,r.kt)("td",{parentName:"tr",align:null},"Max delta seconds time needed to trigger compaction, default 1 hour")))),(0,r.kt)("p",null,"You can write the data using the SQL ",(0,r.kt)("inlineCode",{parentName:"p"},"INSERT INTO")," statements:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO hudi_table select ... from ...; \n")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Note"),": INSERT OVERWRITE is not supported yet but already on the roadmap."))}m.isMDXComponent=!0}}]);