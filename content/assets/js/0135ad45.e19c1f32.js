(globalThis.webpackChunkhudi=globalThis.webpackChunkhudi||[]).push([[323,354,437,1119,1190,1293,1550,2053,2256,2739,2759,2932,3202,3610,4060,4530,5270,5622,5717,5888,6552,6739,6791,6847,7773,8348,8363,9152,9189,9650,10181,10472,10687,10978,11364,11745,11905,12869,13525,13580,13803,13896,14134,14340,14388,15144,15805,15917,16942,17411,17474,17524,17626,17702,18445,18940,18950,19865,20535,20713,20809,21805,22385,22504,23591,24912,25290,25397,25452,25582,25781,26003,26183,27388,27398,27586,28627,28679,28701,28902,29419,30432,31016,31137,31536,32183,32391,32420,32691,32746,32875,33290,33659,33825,34331,34837,35270,35402,35752,35766,36055,36619,37147,37438,37839,38001,38238,38600,38605,38974,39260,39405,39487,39973,40555,40690,40959,41159,41594,41640,42566,42900,44040,44601,45339,46472,46547,46662,46850,47803,48361,48592,48661,49595,49783,49981,50012,50178,50584,50839,51378,51490,51636,52042,52132,52266,52572,52731,52920,53132,53638,53771,53786,54399,54749,55401,55483,55929,55955,56028,56408,56447,56493,56560,56916,57547,57760,58e3,58301,58428,58594,58692,59020,59349,60167,60884,61114,61740,61786,61959,62046,62182,62254,62388,62549,62641,62845,64694,64700,65091,65223,65497,65533,65926,65987,67438,67545,68210,68685,68930,69384,69875,70418,70452,71021,71473,73693,74312,74588,74657,75062,75378,76123,76554,77123,78001,78139,78522,78540,78764,78820,78915,79039,79521,80102,80362,80442,80557,81666,82019,82354,83027,83089,83385,83700,84013,84015,84563,85353,85537,85588,85896,86078,86264,86659,86664,86846,86992,87205,87485,87905,88298,88394,88547,88622,88775,89210,89437,89455,89901,90438,90513,90562,90749,91109,91280,91333,91787,92031,92196,92273,92360,92410,92475,92955,93636,94863,95020,95159,96771,97006,97107,97246,97342,97895,97913,98021,98392,98425,98713,99204,99330,99335,99678,99701,99709,99746],{193:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(8184),n=t(74848),s=t(28453),r=t(9230);const o={title:"Architecting Data Lakes for the Modern Enterprise at Data Summit Connect Fall 2020",authors:[{name:"Stephanie Simone"}],category:"blog",image:"/assets/images/blog/data-summit-connect.jpeg",tags:["blog","dbta"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.dbta.com/Editorial/News-Flashes/Architecting-Data-Lakes-for-the-Modern-Enterprise-at-Data-Summit-Connect-Fall-2020-143512.aspx",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},327:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(41911),n=t(74848),s=t(28453),r=t(9230);const o={title:"Build your Apache Hudi data lake on AWS using Amazon EMR \u2013 Part 1",authors:[{name:"Suthan Phillips"},{name:"Dylan Qu"}],category:"blog",image:"/assets/images/blog/2022-11-22-aws_hudi_best_practices_part1.png",tags:["how-to","best practices","amazon"]},l=void 0,d={authorsImageUrls:[void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/part-1-build-your-apache-hudi-data-lake-on-aws-using-amazon-emr//",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},346:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/files/bytedance-hudi-slides-chinese-697ab94acf090a0dd627bec988d9cc74.pdf"},380:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/12/01/apache-hudi-JD-meetup-asia-2025-recap","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-12-01-apache-hudi-JD-meetup-asia-2025-recap.md","source":"@site/blog/2025-12-01-apache-hudi-JD-meetup-asia-2025-recap.md","title":"Next Generation Lakehouse: New Engine for the Intelligent Future | Apache Hudi Meetup Asia Recap","description":"---","date":"2025-12-01T00:00:00.000Z","tags":[{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"meetup","permalink":"/blog/tags/meetup"},{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"community","permalink":"/blog/tags/community"}],"readingTime":10.59,"hasTruncateMarker":false,"authors":[{"name":"Team at JD.com","key":null,"page":null}],"frontMatter":{"title":"Next Generation Lakehouse: New Engine for the Intelligent Future | Apache Hudi Meetup Asia Recap","excerpt":"A comprehensive recap of the Apache Hudi Meetup Asia held at JD.com headquarters, featuring insights from Onehouse, JD.com, Kuaishou, and Huawei on Hudi 1.1, AI-native architectures, and production optimizations.","author":"Team at JD.com","category":"blog","image":"/assets/images/blog/2025-12-01-apache-hudi-JD-meetup-asia-2025-recap/jdpost-image7.jpg","tags":["hudi","meetup","lakehouse","community"]},"unlisted":false,"nextItem":{"title":"Apache Hudi 1.1 is Here\u2014Building the Foundation for the Next Generation of Lakehouse","permalink":"/blog/2025/11/25/apache-hudi-release-1-1-announcement"}}')},403:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/06/10/employing-right-configurations-for-hudi-cleaner","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-06-10-employing-right-configurations-for-hudi-cleaner.md","source":"@site/blog/2021-06-10-employing-right-configurations-for-hudi-cleaner.md","title":"Employing correct configurations for Hudi\'s cleaner table service","description":"Apache Hudi provides snapshot isolation between writers and readers. This is made possible by Hudi\u2019s MVCC concurrency model. In this blog, we will explain how to employ the right configurations to manage multiple file versions. Furthermore, we will discuss mechanisms available to users on how to maintain just the required number of old file versions so that long running readers do not fail.","date":"2021-06-10T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"cleaner","permalink":"/blog/tags/cleaner"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":6.86,"hasTruncateMarker":true,"authors":[{"name":"pratyakshsharma","key":null,"page":null}],"frontMatter":{"title":"Employing correct configurations for Hudi\'s cleaner table service","excerpt":"Ensuring isolation between Hudi writers and readers using `HoodieCleaner.java`","author":"pratyakshsharma","category":"blog","image":"/assets/images/blog/hoodie-cleaner/Initial_timeline.png","tags":["how-to","cleaner","apache hudi"]},"unlisted":false,"prevItem":{"title":"Part1: Query apache hudi dataset in an amazon S3 data lake with amazon athena : Read optimized queries","permalink":"/blog/2021/07/16/Query-apache-hudi-dataset-in-an-amazon-S3-data-lake-with-amazon-athena-Read-optimized-queries"},"nextItem":{"title":"Apache Hudi: How Uber gets data a ride to its destination","permalink":"/blog/2021/06/04/Apache-Hudi-How-Uber-gets-data-a-ride-to-its-destination"}}')},547:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(4956),n=t(74848),s=t(28453),r=t(9230);const o={title:"Understanding COW and MOR in Apache Hudi: Choosing the Right Storage Strategy",author:"Deepak Nishad",category:"blog",image:"/assets/images/blog/2024-11-12-understanding-cow-and-mor-in-apache-hudi.jpeg",tags:["blog","apache hudi","cow","mor","opstree"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://opstree.com/blog/2024/11/12/understanding-cow-and-mor-in-apache-hudi-choosing-the-right-storage-strategy/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},698:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(68445),n=t(74848),s=t(28453),r=t(82915);const o={title:"Change Data Capture with Debezium and Apache Hudi",excerpt:"A review of new Debezium source connector for Apache Hudi",author:"Rajesh Mahindra",category:"blog",image:"/assets/images/blog/debezium.png",tags:["design","deltastreamer","cdc","change data capture","apache hudi"]},l=void 0,d={authorsImageUrls:[void 0]},c=[{value:"Background",id:"background",level:2},{value:"Design Overview",id:"design-overview",level:2},{value:"Apache Hudi Configurations",id:"apache-hudi-configurations",level:2},{value:"Bootstrapping Existing tables",id:"bootstrapping-existing-tables",level:3},{value:"Example Implementation",id:"example-implementation",level:3},{value:"Database",id:"database",level:3},{value:"Debezium Connector",id:"debezium-connector",level:3},{value:"Hudi Deltastreamer",id:"hudi-deltastreamer",level:3},{value:"Conclusion",id:"conclusion",level:2}];function h(e){const a={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(a.p,{children:["As of Hudi v0.10.0, we are excited to announce the availability of ",(0,n.jsx)(a.a,{href:"https://debezium.io/",children:"Debezium"})," sources for ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/hoodie_streaming_ingestion",children:"Deltastreamer"})," that provide the ingestion of change capture data (CDC) from Postgres and Mysql databases to your data lake. For more details, please refer to the original ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/rfc/rfc-39/rfc-39.md",children:"RFC"}),"."]}),"\n",(0,n.jsx)(a.h2,{id:"background",children:"Background"}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/data-network.png",alt:"drawing",width:"600"}),"\n",(0,n.jsxs)(a.p,{children:["When you want to perform analytics on data from transactional databases like Postgres or Mysql you typically need to bring this data into an OLAP system such as a data warehouse or a data lake through a process called ",(0,n.jsx)(a.a,{href:"https://debezium.io/documentation/faq/#what_is_change_data_capture",children:"Change Data Capture"})," (CDC). Debezium is a popular tool that makes CDC easy. It provides a way to capture row-level changes in your databases by ",(0,n.jsx)(a.a,{href:"https://debezium.io/blog/2018/07/19/advantages-of-log-based-change-data-capture/",children:"reading changelogs"}),". By doing so, Debezium avoids increased CPU load on your database and ensures you capture all changes including deletes."]}),"\n",(0,n.jsxs)(a.p,{children:["Now that ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/overview/",children:"Apache Hudi"})," offers a Debezium source connector, CDC ingestion into a data lake is easier than ever with some ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/use_cases",children:"unique differentiated capabilities"}),". Hudi enables efficient update, merge, and delete transactions on a data lake. Hudi uniquely provides ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/table_types#merge-on-read-table",children:"Merge-On-Read"})," writers which unlock ",(0,n.jsx)(a.a,{href:"https://aws.amazon.com/blogs/big-data/how-amazon-transportation-service-enabled-near-real-time-event-analytics-at-petabyte-scale-using-aws-glue-with-apache-hudi/",children:"significantly lower latency"})," ingestion than typical data lake writers with Spark or Flink. Last but not least, Apache Hudi offers ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/querying_data#spark-incr-query",children:"incremental queries"})," so after capturing changes from your database, you can incrementally process these changes downstream throughout all of your subsequent ETL pipelines."]}),"\n",(0,n.jsx)(a.h2,{id:"design-overview",children:"Design Overview"}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/debezium.png",alt:"drawing",width:"600"}),"\n",(0,n.jsx)(a.p,{children:"The architecture for an end-to-end CDC ingestion flow with Apache Hudi is shown above. The first component is the Debezium deployment, which consists of a Kafka cluster, schema registry (Confluent or Apicurio), and the Debezium connector. The Debezium connector continuously polls the changelogs from the database and writes an AVRO message with the changes for each database row to a dedicated Kafka topic per table."}),"\n",(0,n.jsxs)(a.p,{children:["The second component is ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/hoodie_streaming_ingestion",children:"Hudi Deltastreamer"})," that reads and processes the incoming Debezium records from Kafka for each table and writes (updates) the corresponding rows in a Hudi table on your cloud storage."]}),"\n",(0,n.jsxs)(a.p,{children:["To ingest the data from the database table into a Hudi table in near real-time, we implement two classes that can be plugged into the Deltastreamer. Firstly, we implemented a ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/83f8ed2ae3ba7fb20813cbb8768deae6244b020c/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/debezium/DebeziumSource.java",children:"Debezium source"}),". With Deltastreamer running in continuous mode, the source continuously reads and processes the Debezium change records in Avro format from the Kafka topic for a given table, and writes the updated record to the destination Hudi table. In addition to the columns from the database table, we also ingest some meta fields that are added by Debezium in the target Hudi table. The meta fields help us correctly merge updates and delete records. The records are read using the latest schema from the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/hoodie_streaming_ingestion#schema-providers",children:"Schema Registry"}),"."]}),"\n",(0,n.jsxs)(a.p,{children:["Secondly, we implement a custom ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/83f8ed2ae3ba7fb20813cbb8768deae6244b020c/hudi-common/src/main/java/org/apache/hudi/common/model/debezium/AbstractDebeziumAvroPayload.java",children:"Debezium Payload"})," that essentially governs how Hudi records are merged when the same row is updated or deleted. When a new Hudi record is received for an existing row, the payload picks the latest record using the higher value of the appropriate column (FILEID and POS fields in MySql and LSN fields in Postgres). In the case that the latter event is a delete record, the payload implementation ensures that the record is hard deleted from the storage. Delete records are identified using the op field, which has a value of ",(0,n.jsx)(a.strong,{children:"d"})," for deletes."]}),"\n",(0,n.jsx)(a.h2,{id:"apache-hudi-configurations",children:"Apache Hudi Configurations"}),"\n",(0,n.jsx)(a.p,{children:"It is important to consider the following configurations of your Hudi deployments when using the Debezium source connector for CDC ingestion."}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Record Keys -"})," The Hudi ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/next/indexing",children:"record key(s)"})," for a table should be set as the Primary keys of the table in the upstream database. This ensures that updates are applied correctly as record key(s) uniquely identify a row in the Hudi table."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Source Ordering Fields"})," -\xa0 For de-duplication of changelog records the source ordering field should be set to the actual position of the change event as it happened on the database. For instance, we use the FILEID and POS fields in MySql and LSN fields in Postgres databases respectively to ensure records are processed in the correct order of occurrence in the original database."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Partition Fields"})," - Don\u2019t feel restricted to matching the partitioning of your Hudi tables with the same partition fields as the upstream database. You can set partition fields independently for the Hudi table as needed."]}),"\n"]}),"\n",(0,n.jsx)(a.h3,{id:"bootstrapping-existing-tables",children:"Bootstrapping Existing tables"}),"\n",(0,n.jsx)(a.p,{children:"One important use case might be when CDC ingestion has to be done for existing database tables. There are two ways we can ingest existing database data prior to streaming the changes:"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsx)(a.li,{children:"By default on initialization, Debezium performs an initial consistent snapshot of the database (controlled by config snapshot.mode). After the initial snapshot, it continues streaming updates from the correct position to avoid loss of data."}),"\n",(0,n.jsxs)(a.li,{children:["While the first approach is simple, for large tables it may take a long time for Debezium to bootstrap the initial snapshot. Alternatively, we could run a Deltastreamer job to bootstrap the table directly from the database using the ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/JdbcSource.java",children:"JDBC source"}),". This provides more flexibility to the users in defining and executing more optimized SQL queries required to bootstrap the database table. Once the bootstrap job finishes successfully, another Deltastreamer job is executed that processes the database changelogs from Debezium. Users will have to use ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/hoodie_streaming_ingestion/#checkpointing",children:"checkpointing"})," in Deltastreamer to ensure the second job starts processing the changelogs from the correct position to avoid data loss."]}),"\n"]}),"\n",(0,n.jsx)(a.h3,{id:"example-implementation",children:"Example Implementation"}),"\n",(0,n.jsx)(a.p,{children:"The following describes steps to implement an end-to-end CDC pipeline using an AWS RDS instance of Postgres, Kubernetes-based Debezium deployment, and Hudi Deltastreamer running on a spark cluster."}),"\n",(0,n.jsx)(a.h3,{id:"database",children:"Database"}),"\n",(0,n.jsx)(a.p,{children:"A few configuration changes are required for the RDS instance to enable logical replication."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-roomsql",children:"SET rds.logical_replication to 1 (instead of 0)\n\npsql --host=<aws_rds_instance> --port=5432 --username=postgres --password -d <database_name>;\n\nCREATE PUBLICATION <publication_name> FOR TABLE schema1.table1, schema1.table2;\n\nALTER TABLE schema1.table1 REPLICA IDENTITY FULL;\n"})}),"\n",(0,n.jsx)(a.h3,{id:"debezium-connector",children:"Debezium Connector"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.a,{href:"https://strimzi.io/blog/2020/01/27/deploying-debezium-with-kafkaconnector-resource/",children:"Strimzi"})," is the recommended option to deploy and manage Kafka connectors on Kubernetes clusters. Alternatively, you have the option to use the Confluent managed ",(0,n.jsx)(a.a,{href:"https://docs.confluent.io/debezium-connect-postgres-source/current/overview.html",children:"Debezium connector"}),"."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:"kubectl create namespace kafka\nkubectl create -f https://strimzi.io/install/latest?namespace=kafka -n kafka\nkubectl -n kafka apply -f kafka-connector.yaml\n"})}),"\n",(0,n.jsx)(a.p,{children:"An example for kafka-connector.yaml is shown below:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-yaml",children:'apiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaConnect\nmetadata:\nname: debezium-kafka-connect\nannotations:\nstrimzi.io/use-connector-resources: "false"\nspec:\nimage: debezium-kafka-connect:latest\nreplicas: 1\nbootstrapServers: localhost:9092\nconfig:\nconfig.storage.replication.factor: 1\noffset.storage.replication.factor: 1\nstatus.storage.replication.factor: 1\n'})}),"\n",(0,n.jsx)(a.p,{children:"The docker image debezium-kafka-connect can be built using the following Dockerfile that includes the Postgres Debezium Connector."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-yaml",children:"FROM confluentinc/cp-kafka-connect:6.2.0 as cp\nRUN confluent-hub install --no-prompt confluentinc/kafka-connect-avro-converter:6.2.0\nFROM strimzi/kafka:0.18.0-kafka-2.5.0\nUSER root:root\nRUN yum -y update\nRUN yum -y install git\nRUN yum -y install wget\n\nRUN wget https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/1.6.1.Final/debezium-connector-postgres-1.6.1.Final-plugin.tar.gz\nRUN tar xzf debezium-connector-postgres-1.6.1.Final-plugin.tar.gz\n\nRUN mkdir -p /opt/kafka/plugins/debezium && mkdir -p /opt/kafka/plugins/avro/\nRUN mv debezium-connector-postgres /opt/kafka/plugins/debezium/\nCOPY --from=cp /usr/share/confluent-hub-components/confluentinc-kafka-connect-avro-converter/lib /opt/kafka/plugins/avro/\nUSER 1001\n"})}),"\n",(0,n.jsx)(a.p,{children:"Once the Strimzi operator and the Kafka connect are deployed, we can start the Debezium connector."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:'curl -X POST -H "Content-Type:application/json" -d @connect-source.json http://localhost:8083/connectors/\n'})}),"\n",(0,n.jsx)(a.p,{children:"The following is an example of a configuration to setup Debezium connector for generating the changelogs for two tables, table1, and table2."}),"\n",(0,n.jsx)(a.p,{children:"Contents of connect-source.json:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-json",children:'{\n  "name": "postgres-debezium-connector",\n  "config": {\n    "connector.class": "io.debezium.connector.postgresql.PostgresConnector",\n    "database.hostname": "localhost",\n    "database.port": "5432",\n    "database.user": "postgres",\n    "database.password": "postgres",\n    "database.dbname": "database",\n    "plugin.name": "pgoutput",\n    "database.server.name": "postgres",\n    "table.include.list": "schema1.table1,schema1.table2",\n    "publication.autocreate.mode": "filtered",\n    "tombstones.on.delete":"false",\n    "key.converter": "io.confluent.connect.avro.AvroConverter",\n    "key.converter.schema.registry.url": "<schema_registry_host>",\n    "value.converter": "io.confluent.connect.avro.AvroConverter",\n    "value.converter.schema.registry.url": "<schema_registry_host>",\n    "slot.name": "pgslot"\n  }\n}\n'})}),"\n",(0,n.jsx)(a.h3,{id:"hudi-deltastreamer",children:"Hudi Deltastreamer"}),"\n",(0,n.jsx)(a.p,{children:"Next, we run the Hudi Deltastreamer using spark that will ingest the Debezium changelogs from kafka and write them as a Hudi table. One such instance of the command is shown below that works for Postgres database.\xa0 A few key configurations are as follows:"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsx)(a.li,{children:"Set the source class to PostgresDebeziumSource."}),"\n",(0,n.jsx)(a.li,{children:"Set the payload class to PostgresDebeziumAvroPayload."}),"\n",(0,n.jsx)(a.li,{children:"Configure the schema registry URLs for Debezium Source and Kafka Source."}),"\n",(0,n.jsx)(a.li,{children:"Set the record key(s) as the primary key(s) of the database table."}),"\n",(0,n.jsx)(a.li,{children:"Set the source ordering field (dedup) to _event_lsn"}),"\n"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-scala",children:'spark-submit \\\\\n  --jars "/home/hadoop/hudi-utilities-bundle_2.12-0.10.0.jar,/usr/lib/spark/external/lib/spark-avro.jar" \\\\\n  --master yarn --deploy-mode client \\\\\n  --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer /home/hadoop/hudi-packages/hudi-utilities-bundle_2.12-0.10.0-SNAPSHOT.jar \\\\\n  --table-type COPY_ON_WRITE --op UPSERT \\\\\n  --target-base-path s3://bucket_name/path/for/hudi_table1 \\\\\n  --target-table hudi_table1\xa0 --continuous \\\\\n  --min-sync-interval-seconds 60 \\\\\n  --source-class org.apache.hudi.utilities.sources.debezium.PostgresDebeziumSource \\\\\n  --source-ordering-field _event_lsn \\\\\n  --payload-class org.apache.hudi.common.model.debezium.PostgresDebeziumAvroPayload \\\\\n  --hoodie-conf schema.registry.url=https://localhost:8081 \\\\\n  --hoodie-conf hoodie.deltastreamer.schemaprovider.registry.url=https://localhost:8081/subjects/postgres.schema1.table1-value/versions/latest \\\\\n  --hoodie-conf hoodie.deltastreamer.source.kafka.value.deserializer.class=io.confluent.kafka.serializers.KafkaAvroDeserializer \\\\\n  --hoodie-conf hoodie.deltastreamer.source.kafka.topic=postgres.schema1.table1 \\\\\n  --hoodie-conf auto.offset.reset=earliest \\\\\n  --hoodie-conf hoodie.datasource.write.recordkey.field=\u201ddatabase_primary_key\u201d \\\\\n  --hoodie-conf hoodie.datasource.write.partitionpath.field=partition_key \\\\\n  --enable-hive-sync \\\\\n  --hoodie-conf hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor \\\\\n  --hoodie-conf hoodie.datasource.write.hive_style_partitioning=true \\\\\n  --hoodie-conf hoodie.datasource.hive_sync.database=default \\\\\n  --hoodie-conf hoodie.datasource.hive_sync.table=hudi_table1 \\\\\n  --hoodie-conf hoodie.datasource.hive_sync.partition_fields=partition_key\n'})}),"\n",(0,n.jsx)(a.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,n.jsx)(a.p,{children:"This post introduced the Debezium Source for Hudi Deltastreamer to ingest Debezium changelogs into Hudi tables. Database data can now be ingested into data lakes to provide a cost-effective way to store and analyze database data."}),"\n",(0,n.jsxs)(a.p,{children:["Please follow this ",(0,n.jsx)(a.a,{href:"https://issues.apache.org/jira/browse/HUDI-1290",children:"JIRA"})," to learn more about active development on this new feature. I look forward to more contributions and feedback from the community. Come join our ",(0,n.jsx)(r.A,{title:"Hudi Slack"})," channel or attend one of our ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/community/syncs",children:"community events"})," to learn more."]})]})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h(e)}},1050:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/06/11/cleaner-and-archival-in-apache-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-06-11-cleaner-and-archival-in-apache-hudi.mdx","source":"@site/blog/2023-06-11-cleaner-and-archival-in-apache-hudi.mdx","title":"Cleaner and Archival in Apache Hudi","description":"Redirecting... please wait!!","date":"2023-06-11T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"cleaner","permalink":"/blog/tags/cleaner"},{"inline":true,"label":"timeline","permalink":"/blog/tags/timeline"},{"inline":true,"label":"active timeline","permalink":"/blog/tags/active-timeline"},{"inline":true,"label":"archival timeline","permalink":"/blog/tags/archival-timeline"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Sivabalan Narayanan","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Cleaner and Archival in Apache Hudi","authors":[{"name":"Sivabalan Narayanan"}],"category":"blog","image":"/assets/images/blog/2023-06-11-cleaner-and-archival-in-apache-hudi.jpg","tags":["blog","cleaner","timeline","active timeline","archival timeline","medium"]},"unlisted":false,"prevItem":{"title":"Exploring New Frontiers: How Apache Flink, Apache Hudi and Presto Power New Insights at Scale","permalink":"/blog/2023/06/16/Exploring-New-Frontiers-How-Apache-Flink-Apache-Hudi-and-Presto-Power-New-Insights-at-Scale"},"nextItem":{"title":"Text-Based Search: From Elastic Search to Vector Search","permalink":"/blog/2023/06/03/text-based-search-from-elastic-search-to-vector-search"}}')},1352:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi-design-diagrams_-_Page_4-163995bcecff993234f40d17228ecd6b.png"},1365:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(53697),n=t(74848),s=t(28453),r=t(9230);const o={title:"Create an Apache Hudi-based-near-real-time transactional data lake using AWS DMS, Amazon Kinesis, AWS Glue streaming ETL, and data visualization using Amazon QuickSight",authors:[{name:"Raj Ramasubbu"},{name:"Sundeep Kumar"},{name:"Rahul Sonawane"}],category:"blog",image:"/assets/images/blog/2023-08-03-near-realtime-trans-datalake-aws-dms-kinesis.png",tags:["how-to","cdc","change data capture","upserts","amazon"]},l=void 0,d={authorsImageUrls:[void 0,void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/create-an-apache-hudi-based-near-real-time-transactional-data-lake-using-aws-dms-amazon-kinesis-aws-glue-streaming-etl-and-data-visualization-using-amazon-quicksight/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},1579:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/03/01/Data-Lakehouse-Building-the-Next-Generation-of-Data-Lakes-using-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-03-01-Data-Lakehouse-Building-the-Next-Generation-of-Data-Lakes-using-Apache-Hudi.mdx","source":"@site/blog/2021-03-01-Data-Lakehouse-Building-the-Next-Generation-of-Data-Lakes-using-Apache-Hudi.mdx","title":"Data Lakehouse: Building the Next Generation of Data Lakes using Apache Hudi","description":"Redirecting... please wait!!","date":"2021-03-01T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"data-lakehouse","permalink":"/blog/tags/data-lakehouse"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.15,"hasTruncateMarker":false,"authors":[{"name":"Ryan D\'Souza","socials":{},"key":null,"page":null},{"name":"Brandon Stanley","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Data Lakehouse: Building the Next Generation of Data Lakes using Apache Hudi","authors":[{"name":"Ryan D\'Souza"},{"name":"Brandon Stanley"}],"category":"blog","image":"/assets/images/blog/2021-03-01-Data-Lakehouse-Building-the-Next-Generation-of-Data-Lakes-using-Apache-Hudi.png","tags":["blog","data-lakehouse","medium"]},"unlisted":false,"prevItem":{"title":"Streaming Responsibly - How Apache Hudi maintains optimum sized files","permalink":"/blog/2021/03/01/hudi-file-sizing"},"nextItem":{"title":"Time travel operations in Hopsworks Feature Store","permalink":"/blog/2021/02/24/Time-travel-operations-in-Hopsworks-Feature-Store"}}')},2016:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide7-5941e55407477f2a843749121fd90709.png"},2074:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/12/29/Apache-Hudi-2022-A-Year-In-Review","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-12-29-Apache-Hudi-2022-A-Year-In-Review.mdx","source":"@site/blog/2022-12-29-Apache-Hudi-2022-A-Year-In-Review.mdx","title":"Apache Hudi 2022 - A year in Review","description":"Apache Hudi Momentum","date":"2022-12-29T00:00:00.000Z","tags":[{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"community","permalink":"/blog/tags/community"}],"readingTime":10.37,"hasTruncateMarker":false,"authors":[{"name":"Sivabalan Narayanan","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi 2022 - A year in Review","excerpt":"2022 was the best year for Apache Hudi yet! Huge thank you to everyone who contributed!","author":"Sivabalan Narayanan","category":"blog","image":"/assets/images/blog/Apache-Hudi-2022-Review.png","tags":["apache hudi","community"]},"unlisted":false,"prevItem":{"title":"Apache Hudi vs Delta Lake vs Apache Iceberg - Lakehouse Feature Comparison","permalink":"/blog/2023/01/11/Apache-Hudi-vs-Delta-Lake-vs-Apache-Iceberg-Lakehouse-Feature-Comparison"},"nextItem":{"title":"Build Your First Hudi Lakehouse with AWS S3 and AWS Glue","permalink":"/blog/2022/12/19/Build-Your-First-Hudi-Lakehouse-with-AWS-Glue-and-AWS-S3"}}')},2089:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(43793),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi Architecture Tools and Best Practices",authors:[{name:"Chandan Gaur"}],category:"blog",image:"/assets/images/blog/2021-11-22-hudi-architecture-tools-best-practices.png",tags:["blog","xenonstack"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.xenonstack.com/insights/what-is-hudi",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},2090:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/10/29/deep-dive-into-hudis-indexing-subsystem-part-1-of-2","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-10-29-deep-dive-into-hudis-indexing-subsystem-part-1-of-2.md","source":"@site/blog/2025-10-29-deep-dive-into-hudis-indexing-subsystem-part-1-of-2.md","title":"Deep Dive Into Hudi\u2019s Indexing Subsystem (Part 1 of 2)","description":"For decades, databases have relied on indexes\u2014specialized data structures\u2014to dramatically improve read and write performance by quickly locating specific records. Apache Hudi extends this fundamental principle to the data lakehouse with a unique and powerful approach. Every Hudi table contains a self-managed metadata table that functions as an indexing subsystem, enabling efficient data skipping and fast record lookups across a wide range of read and write scenarios.","date":"2025-10-29T00:00:00.000Z","tags":[{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"indexing","permalink":"/blog/tags/indexing"},{"inline":true,"label":"data lakehouse","permalink":"/blog/tags/data-lakehouse"},{"inline":true,"label":"data skipping","permalink":"/blog/tags/data-skipping"}],"readingTime":11.99,"hasTruncateMarker":false,"authors":[{"name":"Shiyan Xu","key":null,"page":null}],"frontMatter":{"title":"Deep Dive Into Hudi\u2019s Indexing Subsystem (Part 1 of 2)","excerpt":"","author":"Shiyan Xu","category":"blog","image":"/assets/images/blog/2025-10-29-deep-dive-into-hudis-indexing-subsystem-part-1-of-2/fig1.png","tags":["hudi","indexing","data lakehouse","data skipping"]},"unlisted":false,"prevItem":{"title":"How FreeWheel Uses Apache Hudi to Power Its Data Lakehouse","permalink":"/blog/2025/11/07/how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse"},"nextItem":{"title":"Partition Stats: Enhancing Column Stats in Hudi 1.0","permalink":"/blog/2025/10/22/Partition_Stats_Enhancing_Column_Stats_in_Hudi_1.0"}}')},2128:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/12/06/Apache-Hudi-From-Zero-To-One-blog-7","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-12-06-Apache-Hudi-From-Zero-To-One-blog-7.mdx","source":"@site/blog/2023-12-06-Apache-Hudi-From-Zero-To-One-blog-7.mdx","title":"Apache Hudi: From Zero To One (7/10)","description":"Redirecting... please wait!!","date":"2023-12-06T00:00:00.000Z","tags":[{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"spark","permalink":"/blog/tags/spark"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"course","permalink":"/blog/tags/course"},{"inline":true,"label":"tutorial","permalink":"/blog/tags/tutorial"},{"inline":true,"label":"datumagic","permalink":"/blog/tags/datumagic"},{"inline":true,"label":"data lake","permalink":"/blog/tags/data-lake"},{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"apache spark","permalink":"/blog/tags/apache-spark"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Shiyan Xu","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi: From Zero To One (7/10)","excerpt":"Concurrently run writers and table services","author":"Shiyan Xu","category":"blog","image":"/assets/images/blog/2023-12-06-Apache-Hudi-From-Zero-To-One-blog-7.png","tags":["hudi","spark","blog","course","tutorial","datumagic","data lake","lakehouse","apache hudi","apache spark"]},"unlisted":false,"prevItem":{"title":"Getting started with Apache Hudi","permalink":"/blog/2023/12/09/Getting-started-with-Apache-Hudi"},"nextItem":{"title":"Getting started with Apache Hudi","permalink":"/blog/2023/12/01/Getting-started-with-Apache-Hudi"}}')},2270:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/01/20/Hudi-powering-data-lake-efforts-at-Walmart-and-Disney-Hotstar","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-01-20-Hudi-powering-data-lake-efforts-at-Walmart-and-Disney-Hotstar.mdx","source":"@site/blog/2022-01-20-Hudi-powering-data-lake-efforts-at-Walmart-and-Disney-Hotstar.mdx","title":"Hudi powering data lake efforts at Walmart and Disney+ Hotstar","description":"Redirecting... please wait!!","date":"2022-01-20T00:00:00.000Z","tags":[{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"techtarget","permalink":"/blog/tags/techtarget"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Sean Michael Kerner","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Hudi powering data lake efforts at Walmart and Disney+ Hotstar","authors":[{"name":"Sean Michael Kerner"}],"category":"blog","image":"/assets/images/blog/2022-01-20-hudi-powering-datalake-efforts.png","tags":["use-case","techtarget"]},"unlisted":false,"prevItem":{"title":"Cost Efficiency @ Scale in Big Data File Format","permalink":"/blog/2022/01/25/Cost-Efficiency-Scale-in-Big-Data-File-Format"},"nextItem":{"title":"Why and How I Integrated Airbyte and Apache Hudi","permalink":"/blog/2022/01/18/Why-and-How-I-Integrated-Airbyte-and-Apache-Hudi"}}')},2350:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(69127),n=t(74848),s=t(28453),r=t(9230);const o={title:" From Swamp to Stream: How Apache Hudi Transforms the Modern Data Lake",author:"Everton Gomede",category:"blog",tags:["blog","Apache Hudi","real-time datalake","incremental processing","upserts","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/aimonks/from-swamp-to-stream-how-apache-hudi-transforms-the-modern-data-lake-8a938f517ea1",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},2634:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image1-3639ab8c9d27f6e461866dc83e8346c0.png"},2722:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(75525),n=t(74848),s=t(28453),r=t(9230);const o={title:"How Stifel built a modern data platform using AWS Glue and an event-driven domain architecture",author:"Amit Maindola and Srinivas Kandi, Hossein Johari, Ahmad Rawashdeh, Lei Meng",category:"blog",image:"/assets/images/blog/2025-07-07-how-stifel-built-a-modern-data-platform-using-aws-glue-and-an-event-driven-domain-architecture.png",tags:["blog","Apache Hudi","aws","AWS Glue","AWS Blogs","Amazon EMR","AWS Lake Formation","Data Governance","Lakehouse","use-case","det"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/how-stifel-built-a-modern-data-platform-using-aws-glue-and-an-event-driven-domain-architecture/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},2847:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(53492),n=t(74848),s=t(28453),r=t(9230);const o={title:"Build Real Time Streaming Pipeline with Kinesis, Apache Flink and Apache Hudi with Hands-on",author:"Md Shahid Afridi P",category:"blog",image:"/assets/images/blog/2024-04-21-build-real-time-streaming-pipeline-with-kinesis-apache-flink-and-apache-hudi.png",tags:["blog","apache hudi","apache flink","amazon kinesis","amazon s3","streaming ingestion","real-time datalake","incremental processing","devgenius"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blog.devgenius.io/build-real-time-streaming-pipeline-with-kinesis-apache-flink-and-apache-hudi-35d8501855b4",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},2979:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi-design-diagrams_-_Page_7-03e378cf49a27e58e544a3eca59905f0.png"},3239:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/03/13/hudi-on-dbr","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-03-13-hudi-on-dbr.mdx","source":"@site/blog/2025-03-13-hudi-on-dbr.mdx","title":"Building an Amazon Sales Analytics Pipeline with Apache Hudi on Databricks","description":"Redirecting... please wait!!","date":"2025-03-13T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"aws","permalink":"/blog/tags/aws"},{"inline":true,"label":"databricks","permalink":"/blog/tags/databricks"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Sameer Shaik","key":null,"page":null}],"frontMatter":{"title":"Building an Amazon Sales Analytics Pipeline with Apache Hudi on Databricks","author":"Sameer Shaik","category":"blog","image":"/assets/images/blog/hudi-aws-dbr.png","tags":["blog","apache hudi","aws","databricks"]},"unlisted":false,"prevItem":{"title":"From Transactional Bottlenecks to Lightning-Fast Analytics","permalink":"/blog/2025/03/26/uptycs"},"nextItem":{"title":"From Transactional Bottlenecks to Lightning-Fast Analytics","permalink":"/blog/2025/03/13/lightning-fast-analytics"}}')},3319:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/jdpost-image1-98f8473e19ed6ff3c1f91a0a47c779f8.png"},3664:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(86672),n=t(74848),s=t(28453),r=t(9230);const o={title:"Building Streaming Data Lakes with Hudi and MinIO",authors:[{name:"Matt Sarrel"}],category:"blog",image:"/assets/images/blog/2022-09-20_streaming_data_lakes_with_hudi_and_minio.png",tags:["how-to","datalake","datalake platform","streaming ingestion","minio"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blog.min.io/streaming-data-lakes-hudi-minio/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},3695:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/04/18/getting-started-incrementally-process-data-with-apache-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-04-18-getting-started-incrementally-process-data-with-apache-hudi.mdx","source":"@site/blog/2023-04-18-getting-started-incrementally-process-data-with-apache-hudi.mdx","title":"Getting Started: Incrementally process data with Apache Hudi","description":"Redirecting... please wait!!","date":"2023-04-18T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"incremental processing","permalink":"/blog/tags/incremental-processing"},{"inline":true,"label":"onehouse","permalink":"/blog/tags/onehouse"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Raymond Xu","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Getting Started: Incrementally process data with Apache Hudi","authors":[{"name":"Raymond Xu"}],"category":"blog","image":"/assets/images/blog/2023-04-18-getting-started-incrementally-process-data-with-apache-hudi.png","tags":["how-to","incremental processing","onehouse"]},"unlisted":false,"prevItem":{"title":"Delta, Hudi, and Iceberg: The Data Lakehouse Trifecta","permalink":"/blog/2023/04/26/the-lakehouse-trifecta"},"nextItem":{"title":"Speed up your write latencies using Bucket Index in Apache Hudi","permalink":"/blog/2023/04/07/Speed-up-your-write-latencies-using-Bucket-Index-in-Apache-Hudi"}}')},3798:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi-data-lake-platform_-_Page_2_4-a088663c0cb4f7e97b5b74da634975ef.png"},3865:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/11/12/storing-200-billion-entities-notions","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-11-12-storing-200-billion-entities-notions.mdx","source":"@site/blog/2024-11-12-storing-200-billion-entities-notions.mdx","title":"Storing 200 Billion Entities: Notion\u2019s Data Lake Project","description":"Redirecting... please wait!!","date":"2024-11-12T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"bytebytego","permalink":"/blog/tags/bytebytego"}],"readingTime":0.11,"hasTruncateMarker":false,"authors":[{"name":"ByteByteGo","key":null,"page":null}],"frontMatter":{"title":"Storing 200 Billion Entities: Notion\u2019s Data Lake Project","author":"ByteByteGo","category":"blog","image":"/assets/images/blog/2024-11-12-storing-200-billion-entities-notions.jpeg","tags":["blog","apache hudi","use-case","bytebytego"]},"unlisted":false,"prevItem":{"title":"Record Level Indexing in Apache Hudi","permalink":"/blog/2024/11/12/record-level-indexing-in-apache-hudi"},"nextItem":{"title":"Understanding COW and MOR in Apache Hudi: Choosing the Right Storage Strategy","permalink":"/blog/2024/11/12/understanding-cow-and-mor-in-apache-hudi"}}')},3874:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/05/22/use-aws-data-exchange-to-seamlessly-share-apache-hudi-datasets","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-05-22-use-aws-data-exchange-to-seamlessly-share-apache-hudi-datasets.mdx","source":"@site/blog/2024-05-22-use-aws-data-exchange-to-seamlessly-share-apache-hudi-datasets.mdx","title":"Use AWS Data Exchange to seamlessly share Apache Hudi datasets","description":"Redirecting... please wait!!","date":"2024-05-22T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"aws data exchange","permalink":"/blog/tags/aws-data-exchange"},{"inline":true,"label":"amazon emr","permalink":"/blog/tags/amazon-emr"},{"inline":true,"label":"amazon s3","permalink":"/blog/tags/amazon-s-3"},{"inline":true,"label":"amazon athena","permalink":"/blog/tags/amazon-athena"},{"inline":true,"label":"data sahring","permalink":"/blog/tags/data-sahring"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Saurabh Bhutyani, Ankith Ede, and Chandra Krishnan","key":null,"page":null}],"frontMatter":{"title":"Use AWS Data Exchange to seamlessly share Apache Hudi datasets","author":"Saurabh Bhutyani, Ankith Ede, and Chandra Krishnan","category":"blog","image":"/assets/images/blog/2024-05-22-use-aws-data-exchange-to-seamlessly-share-apache-hudi-datasets.png","tags":["blog","apache hudi","aws data exchange","amazon emr","amazon s3","amazon athena","data sahring","amazon"]},"unlisted":false,"prevItem":{"title":"Apache Hudi vs. Delta Lake: Choosing the Right Tool for Your Data Lake on AWS","permalink":"/blog/2024/05/27/apache-hudi-vs-delta-lake-choosing-the-right-tool-for-your-data-lake-on-aws"},"nextItem":{"title":"Apache Hudi on AWS Glue","permalink":"/blog/2024/05/19/apache-hudi-on-aws-glue"}}')},3952:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(87190),n=t(74848),s=t(28453);const r={title:"A Deep Dive on Merge-on-Read (MoR) in Lakehouse Table Formats",excerpt:"How is MoR implemented in Hudi, Iceberg, Delta and how it impacts workloads",author:"Dipankar Mazumdar",category:"blog",image:"/assets/images/blog/2025-07-21-mor-comparison/mor-1200x600.jpg",tags:["Apache Hudi","Merge-on-Read (MoR)","Streaming"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"How Merge-on-Read Works Across Table Formats",id:"how-merge-on-read-works-across-table-formats",level:2},{value:"Apache Hudi",id:"apache-hudi",level:3},{value:"Storage Layout",id:"storage-layout",level:4},{value:"Write Path",id:"write-path",level:4},{value:"Read Path",id:"read-path",level:4},{value:"Compaction",id:"compaction",level:4},{value:"Apache Iceberg",id:"apache-iceberg",level:3},{value:"Storage Layout",id:"storage-layout-1",level:4},{value:"Write Path",id:"write-path-1",level:4},{value:"Read Path",id:"read-path-1",level:4},{value:"Delta Lake",id:"delta-lake",level:3},{value:"Storage Layout",id:"storage-layout-2",level:4},{value:"Write Path",id:"write-path-2",level:4},{value:"Read Path",id:"read-path-2",level:4},{value:"Comparative Design Analysis",id:"comparative-design-analysis",level:2},{value:"Streaming Data Support &amp; Event-Time Ordering",id:"streaming-data-support--event-time-ordering",level:3},{value:"Scalable Incremental Write Costs",id:"scalable-incremental-write-costs",level:3},{value:"Asynchronous Compaction during \u2018Merge\u2019",id:"asynchronous-compaction-during-merge",level:3},{value:"Non-Blocking Concurrency Control (NBCC) for Real-time applications",id:"non-blocking-concurrency-control-nbcc-for-real-time-applications",level:3},{value:"Minimizing Read Costs",id:"minimizing-read-costs",level:3},{value:"Performant Read-Side Merge",id:"performant-read-side-merge",level:3},{value:"Efficient Compaction Planning",id:"efficient-compaction-planning",level:3},{value:"Temporal and Spatial Locality for Event-Time Filters",id:"temporal-and-spatial-locality-for-event-time-filters",level:3},{value:"Partial Updates for Performant Merge",id:"partial-updates-for-performant-merge",level:3},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const a={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",hr:"hr",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(a.admonition,{type:"tip",children:[(0,n.jsx)(a.mdxAdmonitionTitle,{}),(0,n.jsx)(a.p,{children:"TL;DR"}),(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Merge-on-Read tables help manage updates on immutable files without constant rewrites."}),"\n",(0,n.jsx)(a.li,{children:"Apache Hudi\u2019s MoR tables, with delta logs, file groups, asynchronous compaction, and event-time merging, are well-suited for update-heavy, low-latency streaming and CDC workloads."}),"\n",(0,n.jsx)(a.li,{children:"Iceberg and Delta Lake also support MoR, but with design differences around delete files and deletion vectors."}),"\n"]})]}),"\n",(0,n.jsxs)(a.p,{children:["As ",(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/blog/open-table-formats-and-the-open-data-lakehouse-in-perspective",children:"open table formats"})," like Apache Hudi, Apache Iceberg, and Delta Lake become foundational to modern data lakes, understanding how data is written and read becomes critical for designing high-performance pipelines. One such key dimension is the table's write mechanism, specifically, what happens when ",(0,n.jsx)(a.em,{children:"updates or deletes"})," are made to these lakehouse tables."]}),"\n",(0,n.jsxs)(a.p,{children:["This is where ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/table_types#copy-on-write-table",children:"Copy-on-Write (CoW)"})," and ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/table_types#merge-on-read-table",children:"Merge-on-Read (MoR)"})," table types come into play. These terms were popularized by ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org",children:"Apache Hudi"}),", in the ",(0,n.jsx)(a.a,{href:"https://www.uber.com/blog/hoodie/",children:"original blog"})," from Uber Engineering, when the project was open-sourced in 2017. These strategies exist to overcome a fundamental limitation: data file formats like Parquet and ORC are immutable in nature. Therefore, any update or delete operation that is executed on these files (managed by a lakehouse table format) requires a specific way to deal with it - either by merging changes right away during writes, rewriting entire files (CoW) or maintaining a differential log or delete index that can  be merged at read time (MoR)."]}),"\n",(0,n.jsxs)(a.p,{children:["Viewed through the lens of the ",(0,n.jsx)(a.a,{href:"https://substack.com/home/post/p-159031300?utm_campaign=post&utm_medium=web",children:"RUM Conjecture"})," - which states that optimizing for two of Read, Update, and Memory inevitably requires trading off the third. CoW and MoR emerge as two natural design responses to the trade-offs in lakehouse table formats:"]}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"Copy-on-Write tables optimize for read performance. They rewrite Parquet files entirely when a change is made, ensuring clean, columnar files with no extra merge logic at query time. This suits batch-style, read-optimized analytics workloads where write frequency is low."}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"Merge-on-Read, in contrast, introduces flexibility for write-intensive and latency-sensitive workloads by avoiding expensive writes. Instead of rewriting files for every change, MoR tables store updates in delta logs (Hudi), delete files (Iceberg V2), or deletion vectors (Delta Lake). Reads then stitch together the base data files with these changes to present an up-to-date view. This tradeoff favors streaming or near real-time workloads where low write latency is critical."}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Here is a generic comparison table between CoW and MoR tables."}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{children:"Trade-Off"}),(0,n.jsx)(a.th,{children:"CoW"}),(0,n.jsx)(a.th,{children:"MoR"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"Write latency"}),(0,n.jsx)(a.td,{children:"Higher"}),(0,n.jsx)(a.td,{children:"Lower"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"Query latency"}),(0,n.jsx)(a.td,{children:"Lower"}),(0,n.jsx)(a.td,{children:"Higher"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"Update cost"}),(0,n.jsx)(a.td,{children:"High"}),(0,n.jsx)(a.td,{children:"Low"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"File Size Guidance"}),(0,n.jsx)(a.td,{children:"Base files should be smaller to keep rewrites manageable"}),(0,n.jsx)(a.td,{children:"Base files can be larger, as updates don\u2019t rewrite them directly"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"Read Amplification"}),(0,n.jsx)(a.td,{children:"Minimal - all changes are already materialized into base files"}),(0,n.jsx)(a.td,{children:"Higher - readers must combine base files with change logs or metadata (e.g., delete files or vectors)"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"Write Amplification"}),(0,n.jsx)(a.td,{children:"Higher - changes often rewrite full files, even for small updates"}),(0,n.jsx)(a.td,{children:"Lower - only incremental data (e.g., updates/deletes) is written as separate files or metadata"})]})]})]}),"\n",(0,n.jsxs)(a.p,{children:["In this blog, we will understand how various lakehouse table formats implement ",(0,n.jsx)(a.strong,{children:"MoR"})," strategy and how the design influences performance and other related factors."]}),"\n",(0,n.jsx)(a.h2,{id:"how-merge-on-read-works-across-table-formats",children:"How Merge-on-Read Works Across Table Formats"}),"\n",(0,n.jsx)(a.p,{children:"Although Merge-on-Read is a shared concept across open table formats, each system implements it using different techniques, influenced by their internal design philosophy and read-write optimization goals. Here\u2019s a breakdown of how Apache Hudi, Apache Iceberg, and Delta Lake enable Merge-on-Read behavior."}),"\n",(0,n.jsx)(a.h3,{id:"apache-hudi",children:"Apache Hudi"}),"\n",(0,n.jsxs)(a.p,{children:["Hudi implements Merge-on-Read as one of its two core table types (along with Copy-on-Write), offering a trade-off between read and write costs by maintaining base files alongside delta log files. Instead of rewriting columnar files for every update or delete, MoR tables maintain a combination of base files and log files that encode delta updates/deletes to the base file, enabling fast ingestion and deferred file merging via asynchronous ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/compaction",children:"compaction"}),". This design is particularly suited for streaming ingestion and update-heavy workloads, where minimizing write amplification and achieving high throughput are critical, without any downtime whatsoever for the writers."]}),"\n",(0,n.jsx)(a.h4,{id:"storage-layout",children:"Storage Layout"}),"\n",(0,n.jsxs)(a.p,{children:["At the physical level, a Hudi MoR table stores data in ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/tech-specs/#file-layout-hierarchy",children:(0,n.jsx)(a.strong,{children:"File Groups"})}),", each uniquely identified by a ",(0,n.jsx)(a.code,{children:"fileId"}),". A file group consists of:"]}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["Base File (",(0,n.jsx)(a.code,{children:".parquet, .orc"}),"): Stores the base snapshot of records in columnar format."]}),"\n",(0,n.jsxs)(a.li,{children:["Delta Log Files (",(0,n.jsx)(a.code,{children:".log"}),"): Append-only files that capture incremental updates, inserts, and deletes since the last compaction, in either row-oriented data formats like Apache Avro, Hudi\u2019s native SSTable format or columnar-formats like Apache Parquet"]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"This hybrid design enables fast writes and defers expensive columnar file writing to asynchronous compaction."}),"\n",(0,n.jsx)(a.h4,{id:"write-path",children:"Write Path"}),"\n",(0,n.jsx)(a.p,{children:"In a Merge-on-Read table, insert and update operations are handled differently to strike a balance between write efficiency and read performance."}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:["Insert operations behave similarly to those in Copy-on-Write tables. New records are written to freshly created ",(0,n.jsx)(a.em,{children:"base files"}),", aligned to a configured block size. In some cases, these inserts may be merged into the smallest existing base file in the partition to control file counts and sizes."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:["Update operations, however, are written to ",(0,n.jsx)(a.em,{children:"log files"})," associated with the corresponding file group. These updates in the log files are written using Hudi\u2019s ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/45312d437a51ccd1d8c75ba0bd8af21a47dbb9e0/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkMergeOnReadTable.java#L205",children:(0,n.jsx)(a.code,{children:"HoodieAppendHandle"})})," class. At runtime, a new instance of ",(0,n.jsx)(a.code,{children:"HoodieAppendHandle"})," is created with the target ",(0,n.jsx)(a.em,{children:"partition"})," and ",(0,n.jsx)(a.em,{children:"file ID"}),". The update records are passed to its ",(0,n.jsx)(a.code,{children:"write()"})," method, which processes and appends them to the active ",(0,n.jsx)(a.em,{children:"log file"})," associated with that file group. This mechanism avoids rewriting large Parquet base files and instead accumulates changes in a rolling log structure associated with each base file."]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"HoodieAppendHandle appendHandle = new HoodieAppendHandle(config, instantTime, this,\n    partitionPath, fileId, recordMap.values().iterator(), taskContextSupplier, header);\nappendHandle.write(recordMap);\nList<WriteStatus> writeStatuses = appendHandle.close();\nreturn Collections.singletonList(writeStatuses).iterator();\n"})}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Delete operations are also appended to log files as either delete keys or deleted vector positions, to refer to the base file records that were deleted. These delete entries are not applied to the base files immediately. Instead, they are taken into account during snapshot reads, which merge the base and log files to produce the latest view, and during compaction, which merges the accumulated log files (including deletes) into new base files."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"This design ensures that write operations remain lightweight and fast, regardless of the size of the base files. Writers are not blocked by background compaction or cleanup operations, making the system well-suited for streaming and CDC workloads."}),"\n",(0,n.jsx)(a.h4,{id:"read-path",children:"Read Path"}),"\n",(0,n.jsxs)(a.p,{children:["Hudi MoR tables offer flexible read semantics by supporting both ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/sql_queries/#snapshot-query",children:"snapshot queries"})," and ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/table_types#query-types",children:"read-optimized queries"}),", depending on the user's performance and freshness requirements."]}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"Snapshot queries provide the most current view of the dataset by dynamically merging base files with their corresponding log files at read time. The system selects between different reader types based on the nature of the query and the presence of log files:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["A ",(0,n.jsx)(a.strong,{children:"full-schema"})," reader reads the complete row data to ensure correct application of updates and deletes."]}),"\n",(0,n.jsxs)(a.li,{children:["A ",(0,n.jsx)(a.strong,{children:"required-schema"})," reader projects only the needed columns to reduce I/O, while still applying log file merges."]}),"\n",(0,n.jsxs)(a.li,{children:["A ",(0,n.jsx)(a.strong,{children:"skip-merging"})," reader is used when log files are absent for a file group, allowing the query engine to read directly from base files without incurring merge costs."]}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"Read-optimized queries, in contrast, skip reading the delta log files altogether. These queries only scan the base Parquet files, providing faster response times at the cost of not reflecting the latest un-compacted changes. This mode is suitable for applications where slightly stale data is acceptable or where performance is critical."}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Together, these two read strategies allow Hudi MoR tables to serve both real-time and interactive queries from the same dataset, adjusting behavior depending on the workload and latency constraints."}),"\n",(0,n.jsx)(a.h4,{id:"compaction",children:"Compaction"}),"\n",(0,n.jsxs)(a.p,{children:["As log files accumulate new updates and deletes, Hudi triggers a compaction operation to merge these log files back into columnar base files. This process is ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/compaction#async--offline-compaction-models",children:"configurable and asynchronous"}),", and plays a key role in balancing write and read performance."]}),"\n",(0,n.jsxs)(a.p,{children:["Compaction in Hudi is triggered based on thresholds that can be configured by the user, such as the ",(0,n.jsx)(a.em,{children:"number of commits (NUM_COMMITS)"}),". During compaction, all log files associated with a file group are read and merged with the existing base file to produce a new compacted base file."]}),"\n",(0,n.jsx)(a.h3,{id:"apache-iceberg",children:"Apache Iceberg"}),"\n",(0,n.jsxs)(a.p,{children:["Apache Iceberg supports Merge-on-Read (MoR) semantics by maintaining immutable base data files and tracking updates and deletions through separate ",(0,n.jsx)(a.a,{href:"https://iceberg.apache.org/spec/#delete-formats",children:(0,n.jsx)(a.em,{children:"delete files"})}),". This design avoids rewriting data files for every update or delete operation. Instead, these changes are applied at query time by merging delete files with the base files to produce an up-to-date view."]}),"\n",(0,n.jsx)(a.h4,{id:"storage-layout-1",children:"Storage Layout"}),"\n",(0,n.jsx)(a.p,{children:"An Iceberg table consists of:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Base Data Files: Immutable Parquet, ORC, or Avro files that contain the primary data."}),"\n",(0,n.jsx)(a.li,{children:"Delete Files: Auxiliary files that record row-level deletions."}),"\n"]}),"\n",(0,n.jsx)(a.h4,{id:"write-path-1",children:"Write Path"}),"\n",(0,n.jsx)(a.p,{children:"In Iceberg\u2019s MoR tables, write operations implement row-level updates by encoding them as a delete of the old record and an insert of the new one. Rather than modifying existing Parquet base files directly, Iceberg maintains a clear separation between new data and logical deletes by introducing delete files alongside new data files."}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Inserts behave in the same way as CoW tables. The new data is appended to the table as part of a new snapshot."}),"\n",(0,n.jsxs)(a.li,{children:["For delete operations, Iceberg writes a delete file containing rows to be logically removed across multiple base files. Delete files are of two types:","\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Position Deletes: Reference row positions in a specific data file."}),"\n",(0,n.jsx)(a.li,{children:"Equality Deletes: Encode a predicate that matches rows based on one or more column values."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Equality deletes are typically not favored in performance sensitive data platforms, since it forces predicate evaluation against every single base file during snapshot reads."}),"\n",(0,n.jsxs)(a.p,{children:["The ",(0,n.jsx)(a.code,{children:"DeleteFile"})," interface captures these semantics:"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"public interface DeleteFile extends ContentFile<StructLike> {\n  enum DeleteType {\n    EQUALITY, POSITION\n  }\n}\n"})}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Note:"})," Iceberg v3 introduces Deletion Vectors as a more efficient alternative to positional deletes. Deletion vectors attach a ",(0,n.jsx)(a.em,{children:"bitmap"})," to a data file to indicate deleted rows, allowing query engines to skip over deleted rows at read time. Deletion Vectors are already supported by Delta Lake and Hudi and this is now borrowed into the Iceberg spec as well."]}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["For update operations, Iceberg uses a two-step process. An update is implemented as a delete + insert pattern. First, a delete file is created to logically remove the old record, using either a position or equality delete. Then, a new data file is written that contains the full image of the updated record. Both the delete file and the new data file are added in a single atomic commit, creating a new snapshot of the table. This behavior is implemented via the ",(0,n.jsx)(a.code,{children:"RowDelta"})," interface:"]}),"\n"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"RowDelta rowDelta = table.newRowDelta()\n    .addDeletes(deleteFile)\n    .addRows(dataFile);\nrowDelta.commit();\n"})}),"\n",(0,n.jsx)(a.p,{children:"All write operations, whether adding new data files or new delete files, produce a new snapshot in Iceberg\u2019s timeline. This guarantees consistent isolation across readers and writers while avoiding any rewriting of immutable data files."}),"\n",(0,n.jsx)(a.h4,{id:"read-path-1",children:"Read Path"}),"\n",(0,n.jsx)(a.p,{children:"During query execution, Iceberg performs a Merge-on-Read query by combining the immutable base data files with any relevant delete files to present a consistent and up-to-date view. Before reading, the scan planning logic identifies which delete files apply to each data file, ensuring that deletes are correctly associated with their targets."}),"\n",(0,n.jsx)(a.p,{children:"This planning step guarantees that any row marked for deletion through either position deletes or equality deletes is filtered out of the final results, while the original base files remain unchanged. The merging of base data with delete files is applied dynamically by the query engine, allowing Iceberg to preserve the immutable file structure and still deliver row-level updates."}),"\n",(0,n.jsx)(a.h3,{id:"delta-lake",children:"Delta Lake"}),"\n",(0,n.jsxs)(a.p,{children:["Delta Lake supports Merge-on-Read semantics using ",(0,n.jsx)(a.a,{href:"https://docs.delta.io/latest/delta-deletion-vectors.html",children:(0,n.jsx)(a.em,{children:"Deletion Vectors (DVs)"})}),", a feature that allows rows to be logically removed from a dataset without rewriting the base Parquet files. This enables efficient row-level delete   while preserving immutability of data files. For updates, Delta Lake encodes changes as a combination of DELETE and INSERT operations, i.e. the old row is marked as deleted, and a new row with updated values is appended."]}),"\n",(0,n.jsx)(a.h4,{id:"storage-layout-2",children:"Storage Layout"}),"\n",(0,n.jsx)(a.p,{children:"In Delta Lake, the storage layout consists of:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Base Data Files: Immutable Parquet files that hold the core data"}),"\n",(0,n.jsx)(a.li,{children:"Deletion Vectors: Structures that track rows that should be considered deleted during reads, instead of physically removing them from Parquet"}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"A deletion vector is described by a descriptor which captures its storage type (inline, on-disk, or UUID-based), its physical location or inline data, an offset if stored on disk, its size in bytes, and the cardinality (number of rows it marks as deleted)."}),"\n",(0,n.jsx)(a.p,{children:"Small deletion vectors can be embedded directly into the Delta transaction log (inline), while larger ones are stored as separate files, with the UUIDs referencing them by a unique identifier."}),"\n",(0,n.jsx)(a.h4,{id:"write-path-2",children:"Write Path"}),"\n",(0,n.jsx)(a.p,{children:"When a DELETE, UPDATE, or MERGE operation is performed on a Delta table, Delta Lake does not rewrite the affected base Parquet files. Instead, it generates a deletion vector that identifies which rows are logically removed. These deletion vectors are built as compressed bitmap structures (using Roaring Bitmaps), which efficiently encode the positions of the deleted rows."}),"\n",(0,n.jsx)(a.p,{children:"Smaller deletion vectors are kept inline in the transaction log for quick lookup, while larger ones are persisted as separate deletion vector files. All write operations that affect rows in this way update the metadata to track the associated deletion vectors, maintaining a consistent and atomic snapshot view for downstream reads."}),"\n",(0,n.jsx)(a.h4,{id:"read-path-2",children:"Read Path"}),"\n",(0,n.jsx)(a.p,{children:"During query execution, Delta Lake consults any deletion vectors attached to the current snapshot. The query execution loads these deletion vectors and applies them dynamically, filtering out rows marked as deleted before returning results to the user. This happens without rewriting or modifying the base Parquet files, preserving their immutability while still providing correct row-level semantics."}),"\n",(0,n.jsx)(a.p,{children:"This Merge-on-Read approach allows Delta Lake to combine efficient write operations with the ability to serve up-to-date views, ensuring that queries see a consistent, deletion-aware representation of the dataset."}),"\n",(0,n.jsx)(a.h2,{id:"comparative-design-analysis",children:"Comparative Design Analysis"}),"\n",(0,n.jsx)(a.p,{children:"Merge-on-Read semantics are implemented differently across open table formats, with each approach reflecting distinct trade-offs that influence workload performance, complexity, and operational flexibility. MoR is generally well-suited for high-throughput, low-latency streaming ingestion scenarios in a lakehouse, where frequent updates and late-arriving data are expected. In contrast, Copy-on-Write (CoW) tables often work best for simpler, batch-oriented workloads where updates are infrequent and read-optimized behavior is a priority."}),"\n",(0,n.jsx)(a.p,{children:"In this section, we focus on Apache Hudi and Apache Iceberg table formats and explore how their MoR designs influence real-world workloads."}),"\n",(0,n.jsx)(a.h3,{id:"streaming-data-support--event-time-ordering",children:"Streaming Data Support & Event-Time Ordering"}),"\n",(0,n.jsxs)(a.p,{children:["Hudi\u2019s Merge-on-Read design supports event-time ordering and late-arriving data for streaming workloads by providing ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/record_merger#record-payloads",children:(0,n.jsx)(a.code,{children:"RecordPayload"})})," and ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/record_merger",children:(0,n.jsx)(a.code,{children:"RecordMerger"})})," APIs. These allow updates to be merged based on database sequence numbers or event timestamps, so that if data arrives out of order or has late arriving data, the final state is still correct from a temporal perspective."]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2025-07-21-mor-comparison/mor_fig1.png",alt:"index",width:"1000",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"Iceberg uses a last-writer-wins approach, where the most recent commit determines record values regardless of event time. This design may be tricky to deal with late-arriving data  in streaming workloads or CDC ingestion. For e.g. if the source stream is ever repositioned to an earlier time, it will cause the table to move backwards in time where older record values from the replayed stream overwrite newer record images in the table."}),"\n",(0,n.jsx)(a.h3,{id:"scalable-incremental-write-costs",children:"Scalable Incremental Write Costs"}),"\n",(0,n.jsxs)(a.p,{children:["One of the main goals of MoR is to reduce write costs and latencies by avoiding full file rewrites. Hudi achieves this by appending changes to ",(0,n.jsx)(a.em,{children:"delta logs"})," and using ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/indexes",children:(0,n.jsx)(a.strong,{children:"indexing"})})," to quickly identify which file group an incoming update belongs to. Hudi supports different index types to accelerate this lookup process, so it does not need to scan the entire table on every update. This ensures that even if you are updating a relatively small amount of data - for example, 1GB of changes into a 1TB table every five to ten minutes, the system can efficiently target only the affected files."]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2025-07-21-mor-comparison/mor_fig2.png",alt:"index",width:"1000",align:"middle"}),"\n",(0,n.jsxs)(a.p,{children:["Iceberg handles row-level updates and deletes by recording them as ",(0,n.jsx)(a.a,{href:"https://iceberg.apache.org/spec/#delete-formats",children:(0,n.jsx)(a.em,{children:"delete files"})}),". To identify which records to update or delete, Iceberg relies on scanning table metadata, and in some cases file-level data, to locate affected rows. This design uses a simple metadata approach but if partitioning is not highly selective, this lookup step can become a bottleneck for write performance on large tables with frequent small updates."]}),"\n",(0,n.jsx)(a.h3,{id:"asynchronous-compaction-during-merge",children:"Asynchronous Compaction during \u2018Merge\u2019"}),"\n",(0,n.jsxs)(a.p,{children:["Hudi employs ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2025/01/28/concurrency-control#occ-multi-writers",children:"optimistic concurrency control"})," (OCC) between writers and maintains blocking-free ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2025/01/28/concurrency-control#mvcc-writer-table-service-and-table-service-table-service",children:"multi-version concurrency control"})," (MVCC) between writers and its asynchronous compaction process. This means writers can continue appending updates to the same records while earlier versions are being compacted in the background. Compaction operates ",(0,n.jsx)(a.em,{children:"asynchronously"}),", creating new base files from accumulated log files, without interfering with active writers. This ensures great data freshness as well as better compression ratio and thus excellent query performance for columnar files longer term."]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2025-07-21-mor-comparison/mor_fig3.png",alt:"index",width:"1000",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"Iceberg maintains consistent snapshots across all operations, but it does not separate a dedicated compaction action from other write operations. As a result, if both a writer and a maintenance process try to modify overlapping data, standard snapshot conflict resolution ensures only one succeeds and might require retries in some concurrent write scenarios, but there is no asynchronous way to run compaction services. This could lead to livelocking between the writer and table maintenance, where one of them continuously causes the other to fail."}),"\n",(0,n.jsx)(a.h3,{id:"non-blocking-concurrency-control-nbcc-for-real-time-applications",children:"Non-Blocking Concurrency Control (NBCC) for Real-time applications"}),"\n",(0,n.jsxs)(a.p,{children:["Hudi 1.0 further extends its concurrency model to allow multiple writers to safely update the same record at the same time with ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2025/01/28/concurrency-control#non-blocking-concurrency-control-multi-writers",children:"non-blocking conflict resolution"}),". It supports serializability guarantees based on write completion timestamps (arrival-time processing), while also allowing record merging according to event-time order if required. This flexible concurrency strategy enables concurrent writes to proceed, without the need to wait, making it ideal for real-time applications that demand faster ingestion."]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2025-07-21-mor-comparison/mor_fig4.png",alt:"index",width:"700",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"Iceberg applies OCC through its snapshot approach, where writers commit updates against the latest known snapshot, and if conflicts are detected, retries are required. There is no explicit distinction between arrival-time and event-time semantics for concurrent record updates."}),"\n",(0,n.jsx)(a.h3,{id:"minimizing-read-costs",children:"Minimizing Read Costs"}),"\n",(0,n.jsxs)(a.p,{children:["Hudi organizes records into ",(0,n.jsx)(a.em,{children:"file groups,"})," ensuring that updates are consistently routed back to the same group where the original records were stored. This approach means that when a query is executed, it only needs to scan the base file and any delta log files within that specific file group, reducing the data that must be read and merged at query time. By tying updates and inserts to a consistent file group, Hudi preserves locality and limits merge complexity."]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2025-07-21-mor-comparison/mor_fig5.png",alt:"index",width:"1000",align:"middle"}),"\n",(0,n.jsxs)(a.p,{children:["Iceberg applies updates and deletes using ",(0,n.jsx)(a.em,{children:"delete files"}),", and these can reference any row in any base file. As a result, readers must examine all relevant delete files along with all associated base data files during scan planning and execution, which can increase I/O and metadata processing requirements for large tables."]}),"\n",(0,n.jsx)(a.h3,{id:"performant-read-side-merge",children:"Performant Read-Side Merge"}),"\n",(0,n.jsxs)(a.p,{children:["Hudi\u2019s MoR implementation uses ",(0,n.jsx)(a.em,{children:"key-based"})," merging to reconcile delta log records with base files, which allows query engines to push down filters and still correctly merge updates based on record keys. This selective merging reduces unnecessary I/O and improves performance for queries that only need a subset of columns or rows."]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2025-07-21-mor-comparison/mor_fig6.png",alt:"index",width:"800",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"Iceberg historically required readers (particularly Spark readers) to load entire base files when applying positional deletes. This was because pushing down filters could change the order or number of rows returned by the Parquet reader, making positional delete applications incorrect. As a result, filter pushdowns could not be safely applied, forcing a full file scan to maintain correctness. There has been ongoing work in the Iceberg community to address this limitation by improving how positional information is tracked through filtered reads."}),"\n",(0,n.jsx)(a.h3,{id:"efficient-compaction-planning",children:"Efficient Compaction Planning"}),"\n",(0,n.jsxs)(a.p,{children:["Hudi\u2019s compaction strategy operates at the level of individual file groups, which means it can plan and execute small, predictable units of compaction work. This fine-grained approach allows compaction to proceed ",(0,n.jsx)(a.em,{children:"incrementally"})," and avoids large, unpredictable workloads."]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2025-07-21-mor-comparison/mor_fig7.png",alt:"index",width:"800",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"In Iceberg, compaction must consider all base files and their related delete files together, because delete files reference rows in the base data files. This creates a dependency graph where all related files must be handled in a coordinated way. As delete files accumulate over time, these compaction operations can become increasingly large and complex to plan, making it harder to schedule resources efficiently. If compaction falls behind, the amount of data that must be compacted in future operations continues to grow, potentially making the problem worse."}),"\n",(0,n.jsx)(a.h3,{id:"temporal-and-spatial-locality-for-event-time-filters",children:"Temporal and Spatial Locality for Event-Time Filters"}),"\n",(0,n.jsx)(a.p,{children:"Hudi maintains temporal and spatial locality by ensuring that updates and deletes are routed back to the same file group where the original record was first stored. This preserves the time-based clustering or ordering of records, which is especially beneficial for queries filtering by event time or operating within specific time windows. By keeping related records together, Hudi enables efficient pruning of file groups along with partition pruning, during time-based queries."}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2025-07-21-mor-comparison/mor_fig8.png",alt:"index",width:"1000",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"Iceberg handles updates by deleting the existing record and inserting a new one, which may place the updated record in a different data file. Over time, this can scatter records that belong to the same logical or temporal group across multiple files, reducing the effectiveness of partition pruning and requiring periodic clustering or optimization to restore temporal locality."}),"\n",(0,n.jsx)(a.h3,{id:"partial-updates-for-performant-merge",children:"Partial Updates for Performant Merge"}),"\n",(0,n.jsx)(a.p,{children:"Hudi supports partial updates by encoding only the columns that have changed into its delta log files. This means the cost of merging updates is proportional to the number of columns actually modified, rather than the total width of the record. For columnar datasets with wide schemas, this can significantly reduce write amplification and improve merge performance."}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2025-07-21-mor-comparison/mor_fig9.png",alt:"index",width:"800",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"In Iceberg, updates are implemented as a delete plus a full-row insert, which requires rewriting the entire record even if only a single column has changed. As a result, update costs in Iceberg scale with the total number of columns in the record, increasing I/O and storage requirements for wide tables with frequent column-level updates."}),"\n",(0,n.jsx)(a.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,n.jsx)(a.p,{children:"Merge-on-Read (MoR) table type provides an alternative approach to managing updates and deletes on immutable columnar files in a lakehouse. While multiple open table formats support MoR semantics, their design choices significantly affect suitability for real-time and change-data driven workloads."}),"\n",(0,n.jsx)(a.p,{children:"Apache Hudi\u2019s MoR implementation specifically addresses the needs of high-ingestion, update-heavy pipelines. By appending changes to delta logs, preserving file-group-based data locality, supporting event-time ordering, and enabling asynchronous, non-blocking compaction, Hudi minimizes write amplification and supports low-latency data availability. These design primitives directly align with streaming and CDC patterns, where data arrives frequently and potentially out of order. Iceberg and Delta Lake also implement MoR semantics in their own ways to address transactional consistency and immutable storage goals."}),"\n",(0,n.jsx)(a.hr,{})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},3987:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/01/15/outofbox-key-generators-in-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-01-15-outofbox-key-generators-in-hudi.mdx","source":"@site/blog/2025-01-15-outofbox-key-generators-in-hudi.mdx","title":"Out of the box Key Generators in Apache Hudi","description":"Introduction","date":"2025-01-15T00:00:00.000Z","tags":[{"inline":true,"label":"Data Lake","permalink":"/blog/tags/data-lake"},{"inline":true,"label":"Data Lakehouse","permalink":"/blog/tags/data-lakehouse"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"Key Generators","permalink":"/blog/tags/key-generators"},{"inline":true,"label":"partition","permalink":"/blog/tags/partition"}],"readingTime":9.48,"hasTruncateMarker":false,"authors":[{"name":"Aditya Goenka","key":null,"page":null}],"frontMatter":{"title":"Out of the box Key Generators in Apache Hudi","excerpt":"Explain need for key gerators and out of box key generators in Apache Hudi","author":"Aditya Goenka","category":"blog","image":"/assets/images/blog/2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.png","tags":["Data Lake","Data Lakehouse","Apache Hudi","Key Generators","partition"]},"unlisted":false,"prevItem":{"title":"Apache Hudi 1.0 Now Generally Available","permalink":"/blog/2025/01/18/apache-hudi-1-0-now-generally-available"},"nextItem":{"title":"Apache Iceberg vs Delta Lake vs Apache Hudi","permalink":"/blog/2025/01/09/apache-iceberg-vs-delta-lake-vs-apache-hudi"}}')},4054:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(66515),n=t(74848),s=t(28453),r=t(9230);const o={title:"PrestoDB and Apache Hudi",authors:[{name:"Bhavani Sudha Saktheeswaran"},{name:"Brandon Scheller"}],category:"blog",image:"/assets/images/blog/2020-08-04-PrestoDB-and-Apache-Hudi.png",tags:["blog","prestodb"]},l=void 0,d={authorsImageUrls:[void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://prestodb.io/blog/2020/08/04/prestodb-and-hudi",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},4171:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/10/07/mastering-slowly-changing-dimensions-with-apache-hudi-and-spark-sql","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-10-07-mastering-slowly-changing-dimensions-with-apache-hudi-and-spark-sql.mdx","source":"@site/blog/2024-10-07-mastering-slowly-changing-dimensions-with-apache-hudi-and-spark-sql.mdx","title":"Mastering Slowly Changing Dimensions with Apache Hudi & Spark SQL","description":"Redirecting... please wait!!","date":"2024-10-07T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"scd1","permalink":"/blog/tags/scd-1"},{"inline":true,"label":"scd2","permalink":"/blog/tags/scd-2"},{"inline":true,"label":"scd3","permalink":"/blog/tags/scd-3"},{"inline":true,"label":"spark-sql","permalink":"/blog/tags/spark-sql"},{"inline":true,"label":"linkedin","permalink":"/blog/tags/linkedin"}],"readingTime":0.15,"hasTruncateMarker":false,"authors":[{"name":"Sameer Shaik","key":null,"page":null}],"frontMatter":{"title":"Mastering Slowly Changing Dimensions with Apache Hudi & Spark SQL","author":"Sameer Shaik","category":"blog","image":"/assets/images/blog/2024-10-07-mastering-slowly-changing-dimensions-with-apache-hudi-and-spark-sql.png","tags":["blog","Apache Hudi","scd1","scd2","scd3","spark-sql","linkedin"]},"unlisted":false,"prevItem":{"title":"Iceberg vs. Delta Lake vs. Hudi: A Comparative Look at Lakehouse Architectures","permalink":"/blog/2024/10/07/iceberg-vs-delta-lake-vs-hudi-a-comparative-look-at-lakehouse-architectures"},"nextItem":{"title":"Apache Hudi, Spark and Minio: Hands-on Lab in Docker","permalink":"/blog/2024/10/02/apache-hudi-spark-and-minio-hands-on-lab-in-docker"}}')},4191:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/s3-endpoint-configuration-2-b275c182ed2fa52e4c7a33bffba394d5.png"},4219:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2020/03/22/exporting-hudi-datasets","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-03-22-exporting-hudi-datasets.md","source":"@site/blog/2020-03-22-exporting-hudi-datasets.md","title":"Export Hudi datasets as a copy or as different formats","description":"Copy to Hudi dataset","date":"2020-03-22T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"snapshot exporter","permalink":"/blog/tags/snapshot-exporter"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":2.12,"hasTruncateMarker":true,"authors":[{"name":"rxu","key":null,"page":null}],"frontMatter":{"title":"Export Hudi datasets as a copy or as different formats","excerpt":"Learn how to copy or export HUDI dataset in various formats.","author":"rxu","category":"blog","tags":["how-to","snapshot exporter","apache hudi"]},"unlisted":false,"prevItem":{"title":"Apache Hudi Support on Apache Zeppelin","permalink":"/blog/2020/04/27/apache-hudi-apache-zepplin"},"nextItem":{"title":"Change Capture Using AWS Database Migration Service and Hudi","permalink":"/blog/2020/01/20/change-capture-using-aws"}}')},4226:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig-10-PuppyGraph-Query-3-c324707053b02056cc824229fd1d1ac0.png"},4374:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide12-fee0df68abb2b276bfc5c14f3733f5fc.png"},4564:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(53936),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi: From Zero To One (10/10)",excerpt:"Becoming 'One' - the upcoming 1.0 highlights",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2024-04-13-Apache-Hudi-From-Zero-To-One-blog-10.jpg",tags:["hudi","spark","blog","course","tutorial","datumagic","data lake","lakehouse","apache hudi","apache spark"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blog.datumagic.ai/p/apache-hudi-from-zero-to-one-1010",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},4671:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(45134),n=t(74848),s=t(28453),r=t(9230);const o={title:"Simplify operational data processing in data lakes using AWS Glue and Apache Hudi",excerpt:"Use AWS Glue and Apache Hudi for data processing",authors:[{name:"Srinivas Kandi"},{name:"Ravi Itha"}],category:"blog",image:"/assets/images/blog/2023-09-13-Simplify-operational-data-processing-in-data-lakes-using-AWS-Glue-and-Apache-Hudi.png",tags:["aws glue","amazon","how-to","data processing","apache hudi"]},l=void 0,d={authorsImageUrls:[void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/simplify-operational-data-processing-in-data-lakes-using-aws-glue-and-apache-hudi/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},4956:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/11/12/understanding-cow-and-mor-in-apache-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-11-12-understanding-cow-and-mor-in-apache-hudi.mdx","source":"@site/blog/2024-11-12-understanding-cow-and-mor-in-apache-hudi.mdx","title":"Understanding COW and MOR in Apache Hudi: Choosing the Right Storage Strategy","description":"Redirecting... please wait!!","date":"2024-11-12T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"cow","permalink":"/blog/tags/cow"},{"inline":true,"label":"mor","permalink":"/blog/tags/mor"},{"inline":true,"label":"opstree","permalink":"/blog/tags/opstree"}],"readingTime":0.16,"hasTruncateMarker":false,"authors":[{"name":"Deepak Nishad","key":null,"page":null}],"frontMatter":{"title":"Understanding COW and MOR in Apache Hudi: Choosing the Right Storage Strategy","author":"Deepak Nishad","category":"blog","image":"/assets/images/blog/2024-11-12-understanding-cow-and-mor-in-apache-hudi.jpeg","tags":["blog","apache hudi","cow","mor","opstree"]},"unlisted":false,"prevItem":{"title":"Storing 200 Billion Entities: Notion\u2019s Data Lake Project","permalink":"/blog/2024/11/12/storing-200-billion-entities-notions"},"nextItem":{"title":"I spent 5 hours exploring the story behind Apache Hudi.","permalink":"/blog/2024/10/27/I-spent-5-hours-exploring-the-story-behind-Apache-Hudi"}}')},5375:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(9017),n=t(74848),s=t(28453),r=t(9230);const o={title:"Empowering data-driven excellence: How the Bluestone Data Platform embraced data mesh for success",author:"Toney Thomas, Ben Vengerovsky and Rada Stanic",category:"blog",image:"/assets/images/blog/2024-02-27-empowering-data-driven-excellence-how-the-bluestone-data-platform-embraced-data-mesh-for-success.png",tags:["blog","apache hudi","use-case","data mesh","amazon"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/empowering-data-driven-excellence-how-the-bluestone-data-platform-embraced-data-mesh-for-success/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},5478:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(92548),n=t(74848),s=t(28453),r=t(9230);const o={title:"Exploring Time Travel Queries in Apache Hudi",author:"Ramneek Kaur",category:"blog",image:"/assets/images/blog/2024-10-22-exploring-time-travel-queries-in-apache-hudi.jpeg",tags:["blog","Apache Hudi","time travel query","opstree"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://opstree.com/blog/2024/10/22/time-travel-queries-in-apache-hudi/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},5564:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(37647),n=t(74848),s=t(28453),r=t(9230);const o={title:"Hudi, Iceberg and Delta Lake: Data Lake Table Formats Compared",author:"Oz Katz",category:"blog",image:"/assets/images/blog/2024-09-24-hudi-iceberg-and-delta-lake-data-lake-table-formats-compared.png",tags:["blog","apache hudi","apache iceberg","delta lake","comparison","lakefs"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://lakefs.io/blog/hudi-iceberg-and-delta-lake-data-lake-table-formats-compared/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},5647:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/10/11/starrocks-query-performance-with-apache-hudi-and-onehouse","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-10-11-starrocks-query-performance-with-apache-hudi-and-onehouse.mdx","source":"@site/blog/2023-10-11-starrocks-query-performance-with-apache-hudi-and-onehouse.mdx","title":"StarRocks query performance with Apache Hudi and Onehouse","description":"Redirecting... please wait!!","date":"2023-10-11T00:00:00.000Z","tags":[{"inline":true,"label":"starrocks","permalink":"/blog/tags/starrocks"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"query performance","permalink":"/blog/tags/query-performance"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Albert Wong","socials":{},"key":null,"page":null}],"frontMatter":{"title":"StarRocks query performance with Apache Hudi and Onehouse","excerpt":"StarRocks Query Performance with Apache Hudi","authors":[{"name":"Albert Wong"}],"category":"blog","image":"/assets/images/blog/2023-10-11-starrocks-query-performance-with-apache-hudi-and-onehouse.png","tags":["starrocks","medium","blog","query performance","apache hudi"]},"unlisted":false,"prevItem":{"title":"Get started with Apache Hudi using AWS Glue by implementing key design concepts \u2013 Part 1","permalink":"/blog/2023/10/17/Get-started-with-Apache-Hudi-using-AWS-Glue-by-implementing-key-design-concepts-Part-1"},"nextItem":{"title":"Apache Hudi: Copy on Write(CoW) Table","permalink":"/blog/2023/10/06/Apache-Hudi-Copy-on-Write-CoW-Table"}}')},5709:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/04/24/understanding-apache-hudi-consistency-model-part-2","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-04-24-understanding-apache-hudi-consistency-model-part-2.mdx","source":"@site/blog/2024-04-24-understanding-apache-hudi-consistency-model-part-2.mdx","title":"Understanding Apache Hudi\'s Consistency Model Part 2","description":"Redirecting... please wait!!","date":"2024-04-24T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"consistency","permalink":"/blog/tags/consistency"},{"inline":true,"label":"concurrency control","permalink":"/blog/tags/concurrency-control"},{"inline":true,"label":"multi writer","permalink":"/blog/tags/multi-writer"},{"inline":true,"label":"monotonic timestamp","permalink":"/blog/tags/monotonic-timestamp"},{"inline":true,"label":"timestamp collision","permalink":"/blog/tags/timestamp-collision"},{"inline":true,"label":"jack-vanlightly","permalink":"/blog/tags/jack-vanlightly"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Jack Vanlightly","key":null,"page":null}],"frontMatter":{"title":"Understanding Apache Hudi\'s Consistency Model Part 2","author":"Jack Vanlightly","category":"blog","image":"/assets/images/blog/2024-04-24-understanding-apache-hudi-consistency-model-part-2.png","tags":["blog","consistency","concurrency control","multi writer","monotonic timestamp","timestamp collision","jack-vanlightly"]},"unlisted":false,"prevItem":{"title":"Understanding Apache Hudi\'s Consistency Model Part 1","permalink":"/blog/2024/04/24/understanding-apache-hudi-consistency-model-part-1"},"nextItem":{"title":"Understanding Apache Hudi\'s Consistency Model Part 3","permalink":"/blog/2024/04/24/understanding-apache-hudi-consistency-model-part-3"}}')},6056:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/04/04/New-features-from-Apache-Hudi-0.9.0-on-Amazon-EMR","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-04-04-New-features-from-Apache-Hudi-0.9.0-on-Amazon-EMR.mdx","source":"@site/blog/2022-04-04-New-features-from-Apache-Hudi-0.9.0-on-Amazon-EMR.mdx","title":"New features from Apache Hudi 0.9.0 on Amazon EMR","description":"Redirecting... please wait!!","date":"2022-04-04T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Kunal Gautam","socials":{},"key":null,"page":null},{"name":"Gabriele Cacciola","socials":{},"key":null,"page":null},{"name":"Udit Mehrotra","socials":{},"key":null,"page":null}],"frontMatter":{"title":"New features from Apache Hudi 0.9.0 on Amazon EMR","authors":[{"name":"Kunal Gautam"},{"name":"Gabriele Cacciola"},{"name":"Udit Mehrotra"}],"category":"blog","image":"/assets/images/blog/aws.jpg","tags":["blog","amazon"]},"unlisted":false,"prevItem":{"title":"Key Learnings on Using Apache HUDI in building Lakehouse Architecture @ Halodoc","permalink":"/blog/2022/04/04/Key-Learnings-on-Using-Apache-HUDI-in-building-Lakehouse-Architecture-at-Halodoc"},"nextItem":{"title":"Zendesk - Insights for CTOs: Part 3 \u2013 Growing your business with modern data capabilities","permalink":"/blog/2022/03/24/Zendesk-Insights-for-CTOs-Part-3-Growing-your-business-with-modern-data-capabilities"}}')},6218:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi-design-diagrams-table-format-3ba591d07f846d8a739366efdf6071ce.png"},6895:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/08/09/Lakehouse-Trifecta-Delta-Lake-Apache-Iceberg-and-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-08-09-Lakehouse-Trifecta-Delta-Lake-Apache-Iceberg-and-Apache-Hudi.mdx","source":"@site/blog/2023-08-09-Lakehouse-Trifecta-Delta-Lake-Apache-Iceberg-and-Apache-Hudi.mdx","title":"Lakehouse Trifecta \u2014 Delta Lake, Apache Iceberg & Apache Hudi","description":"Redirecting... please wait!!","date":"2023-08-09T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"delta lake","permalink":"/blog/tags/delta-lake"},{"inline":true,"label":"iceberg","permalink":"/blog/tags/iceberg"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Sandip Roy","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Lakehouse Trifecta \u2014 Delta Lake, Apache Iceberg & Apache Hudi","authors":[{"name":"Sandip Roy"}],"category":"blog","image":"/assets/images/blog/2023-08-09-Lakehouse-Trifecta-Delta-Lake-Apache-Iceberg-and-Apache-Hudi.png","tags":["blog","hudi","delta lake","iceberg","medium"]},"unlisted":false,"prevItem":{"title":"Exploring various storage types in Apache Hudi","permalink":"/blog/2023/08/22/Exploring-various-storage-types-in-Apache-Hudi"},"nextItem":{"title":"Data Lakehouse Architecture for Big Data with Apache Hudi","permalink":"/blog/2023/08/05/Data-Lakehouse-Architecture-for-Big-Data-with-Apache-Hudi"}}')},7128:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(79373),n=t(74848),s=t(28453),r=t(82915);const o={title:"Apache Hudi - 2021 a Year in Review",excerpt:"A reflection on the growth and momentum of Apache Hudi in 2021",author:"vinoth",category:"blog",image:"/assets/images/Hudi_community.png",tags:["blog","community","apache hudi"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){const a={a:"a",em:"em",p:"p",strong:"strong",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.p,{children:"As the year came to end, I took some time to reflect on where we are and what we accomplished in 2021. I am humbled by how strong our community is and how regardless of it being another tough pandemic year, that people from around the globe leaned in together and made this the best year yet for Apache Hudi. In this blog I want to recap some of the 2021 highlights."}),"\n",(0,n.jsx)("img",{src:"/assets/images/Hudi_community.png",alt:"drawing",width:"600"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:(0,n.jsx)(a.em,{children:"Community"})})}),"\n",(0,n.jsxs)(a.p,{children:["I want to call out how amazing it is to see such a diverse group of people step up and contribute to this project. There were over 30,000 interactions with the ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/",children:"project on github"}),", up 2x from last year. Over the last year 300 people have contributed to the project, with over 3,000 PRs over 5 releases. We moved Apache Hudi from release 0.5.X all the way to our feature packed 0.10.0 release. Come and join us on our ",(0,n.jsx)(r.A,{title:"active slack channel"}),"! Over 850 community members engaged on our slack, up about 100% from the year before. I want to add a special shout out to our top slack participants who have helped answer so many questions and drive rich discussions on our channel. Sivabalan Narayanan, Nishith Agarwal, Bhavani Sudha Saktheeswaran, Vinay Patil, Rubens Soto, Dave Hagman, Raghav Tandon, Sagar Sumit, Joyan Sil, Jake D, Felix Jose, Nick Vintila, KimL, Andrew Sukhan, Danny Chan, Biswajit Mohapatra, and Pratyaksh Sharma! I know I am missing plenty of other important callouts, every PR that landed this year has helped shape Hudi into what it is today. Thank you!"]}),"\n",(0,n.jsx)("img",{src:"/assets/images/powers/hudi-logo-page.png",alt:"drawing",width:"600"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:(0,n.jsx)(a.em,{children:"Impact"})})}),"\n",(0,n.jsxs)(a.p,{children:["In 2021, I personally developed a deeper gratitude and understanding of the magnitude of the impact we are making in the industry. Throughout the year I met more and more people that told me about how Hudi transformed their business and I was impressed by the large variety of use cases and applications that Hudi was able to serve. Some from the community who publicly shared their story include: ",(0,n.jsx)(a.a,{href:"https://aws.amazon.com/blogs/big-data/how-amazon-transportation-service-enabled-near-real-time-event-analytics-at-petabyte-scale-using-aws-glue-with-apache-hudi/",children:"Amazon"}),", ",(0,n.jsx)(a.a,{href:"https://aws.amazon.com/blogs/big-data/how-ge-aviation-built-cloud-native-data-pipelines-at-enterprise-scale-using-the-aws-platform/",children:"GE"}),", ",(0,n.jsx)(a.a,{href:"https://s.apache.org/hudi-robinhood-talk",children:"Robinhood"}),", ",(0,n.jsx)(a.a,{href:"http://hudi.apache.org/blog/2021/09/01/building-eb-level-data-lake-using-hudi-at-bytedance",children:"ByteDance"}),", ",(0,n.jsx)(a.a,{href:"https://blogs.halodoc.io/data-platform-2-0-part-1/",children:"Halodoc"}),", ",(0,n.jsx)(a.a,{href:"https://developpaper.com/baixin-banks-real-time-data-lake-evolution-scheme-based-on-apache-hudi/",children:"Baixin Bank"}),", ",(0,n.jsx)(a.a,{href:"https://developpaper.com/practice-of-apache-hudi-in-building-real-time-data-lake-at-station-b/",children:"BiliBili"}),", and so many more that haven\u2019t even shared yet. One particular highlight from 2021 was attending ",(0,n.jsxs)(a.a,{href:"https://youtu.be/lGm8qe4tBrg?t=2115",children:["AWS Re",":Invent"]})," and meeting an overwhelmingly large number of users who expressed joy with using Apache Hudi. This raises my sense of responsibility even more to be aware of just how many people depend on Apache Hudi."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:(0,n.jsx)(a.em,{children:"New Features"})})}),"\n",(0,n.jsxs)(a.p,{children:["Apache Hudi has come a long way in 2021 from v0.5.X to 0.10.0. Throughout this year we have developed innovative and leading edge features that make it easier and easier to build streaming data lakes. Some of these features include ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/sql_ddl",children:"Spark SQL DML Support"}),", ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/clustering",children:"Clustering"}),", ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2021/12/29/hudi-zorder-and-hilbert-space-filling-curves",children:"Z-Order/Hilbert curves"}),", ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/metadata",children:"Metadata Table file listing elimination"}),", ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/markers",children:"Timeline Server Markers"}),", ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/precommit_validator",children:"Precommit Validators"}),", ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/writing_data#flink-sql-writer",children:"Flink MOR write/read"}),", ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/concurrency_control",children:"Parallel Write support with OCC"}),", ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/clustering",children:"Clustering"}),", ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/querying_data#spark-incr-query",children:"Incremental Queries for MOR"}),", ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/tree/master/hudi-kafka-connect",children:"Kafka Connect Sink"}),", Delta Streamer sources for ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/hoodie_streaming_ingestion/#s3-events",children:"S3"})," and ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-0.10.0/#debezium-deltastreamer-sources",children:"Debezium"}),", ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-0.10.0/#dbt-support",children:"DBT Support"})," all of which are were added in 2021. To top it all, we put together ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2021/07/21/streaming-data-lake-platform",children:"a manifesto"})," to realize our vision for streaming data lakes."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:(0,n.jsx)(a.em,{children:"The Road Ahead"})})}),"\n",(0,n.jsxs)(a.p,{children:["2021 may have been our best year so far, but it still feels like we are just getting started when we look at our new year's resolutions for 2022. In the year ahead we have bold plans to realize the first cut of our entire vision and take Hudi 1.0, that includes full-featured multi-modal indexing for faster writes/queries, pathbreaking lock free concurrency, new server components for caching/metadata and finally Flink based incremental materialized views! \xa0",(0,n.jsx)(a.em,{children:"You can find our"})," ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/roadmap",children:(0,n.jsx)(a.em,{children:"detailed roadmap here"})}),(0,n.jsx)(a.em,{children:"."})]}),"\n",(0,n.jsxs)(a.p,{children:["I look forward to continued collaboration with the growing Hudi community! Come join our ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/community/syncs",children:(0,n.jsx)(a.em,{children:"community events"})})," ",(0,n.jsx)(a.em,{children:"and discussions in our"})," ",(0,n.jsx)(r.A,{title:"slack channel",isItalic:!0}),"! Happy new year 2022!_"]})]})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h(e)}},7320:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(64341),n=t(74848),s=t(28453);const r={title:"Announcing Apache Hudi 1.0 and the Next Generation of Data Lakehouses",excerpt:"game-changing major release, that reimagines Hudi and Data Lakehouses.",author:"Vinoth Chandar",category:"blog",image:"/assets/images/blog/dlms-hierarchy.png",tags:["timeline","design","release","streaming ingestion","multi-writer","concurrency-control","blog"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Overview",id:"overview",level:2},{value:"Evolution of the Data Lakehouse",id:"evolution-of-the-data-lakehouse",level:2},{value:"Key Features in Hudi 1.0",id:"key-features-in-hudi-10",level:2},{value:"New Time and Timeline",id:"new-time-and-timeline",level:3},{value:"Secondary Indexing for Faster Lookups",id:"secondary-indexing-for-faster-lookups",level:3},{value:"Bloom Filter indexes",id:"bloom-filter-indexes",level:3},{value:"Partitioning replaced by Expression Indexes",id:"partitioning-replaced-by-expression-indexes",level:3},{value:"Efficient Partial Updates",id:"efficient-partial-updates",level:3},{value:"Merge Modes and Custom Mergers",id:"merge-modes-and-custom-mergers",level:3},{value:"Non-Blocking Concurrency Control for Streaming Writes",id:"non-blocking-concurrency-control-for-streaming-writes",level:3},{value:"Backwards Compatible Writing",id:"backwards-compatible-writing",level:3},{value:"What\u2019s Next?",id:"whats-next",level:2},{value:"Get Started with Apache Hudi 1.0",id:"get-started-with-apache-hudi-10",level:2}];function c(e){const a={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.h2,{id:"overview",children:"Overview"}),"\n",(0,n.jsxs)(a.p,{children:["We are thrilled to announce the release of Apache Hudi 1.0, a landmark achievement for our vibrant community that defines what the next generation of data lakehouses should achieve. Hudi pioneered ",(0,n.jsx)(a.em,{children:(0,n.jsx)(a.strong,{children:"transactional data lakes"})})," in 2017, and today, we live in a world where this technology category is mainstream as the \u201c",(0,n.jsx)(a.em,{children:(0,n.jsx)(a.strong,{children:"Data Lakehouse\u201d"})}),". The Hudi community has made several key, original, and first-of-its-kind contributions to this category, as shown below, compared to when other OSS alternatives emerged. This is an incredibly rare feat for a relatively small OSS community to sustain in a fiercely competitive commercial data ecosystem. On the other hand, it also demonstrates the value of deeply understanding the technology category within a focused open-source community. So, I first want to thank/congratulate the Hudi community and the ",(0,n.jsx)(a.strong,{children:"60+ contributors"})," for making 1.0 happen."]}),"\n",(0,n.jsx)("div",{style:{textAlign:"center"},children:(0,n.jsx)("img",{src:"/assets/images/blog/hudi-innovation-timeline.jpg",alt:"innovation timeline"})}),"\n",(0,n.jsxs)(a.p,{children:["This ",(0,n.jsx)(a.a,{href:"/releases/release-1.0.0",children:"release"})," is more than just a version increment\u2014it advances the breadth of Hudi\u2019s feature set and its architecture's robustness while bringing fresh innovation to shape the future. This post reflects on how technology and the surrounding ecosystem have evolved, making a case for a holistic \u201c",(0,n.jsx)(a.em,{children:(0,n.jsx)(a.strong,{children:"Data Lakehouse Management System"})}),"\u201d (",(0,n.jsx)(a.em,{children:(0,n.jsx)(a.strong,{children:"DLMS"})}),") as the new Northstar. For most of this post, we will deep dive into the latest capabilities of Hudi 1.0 that make this evolution possible."]}),"\n",(0,n.jsx)(a.h2,{id:"evolution-of-the-data-lakehouse",children:"Evolution of the Data Lakehouse"}),"\n",(0,n.jsxs)(a.p,{children:["Technologies must constantly evolve\u2014",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Web3",children:"Web 3.0"}),", ",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/List_of_wireless_network_technologies",children:"cellular tech"}),", ",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Programming_language_generations",children:"programming language generations"}),"\u2014based on emerging needs. Data lakehouses are no exception. This section explores the hierarchy of such needs for data lakehouse users. The most basic need is the \u201c",(0,n.jsx)(a.strong,{children:"table format"}),"\u201d functionality, the foundation for data lakehouses. Table format organizes the collection of files/objects into tables with snapshots, schema, and statistics tracking, enabling higher abstraction. Furthermore, table format dictates the organization of files within each snapshot, encoding deletes/updates and metadata about how the table changes over time. Table format also provides protocols for various readers and writers and table management processes to handle concurrent access and provide ACID transactions safely. In the last five years, leading data warehouse and cloud vendors have integrated their proprietary SQL warehouse stack with open table formats. While they mostly default to their closed table formats and the compute engines remain closed, this welcome move provides users an open alternative for their data."]}),"\n",(0,n.jsxs)(a.p,{children:["However, the benefits of a format end there, and now a table format is just the tip of the iceberg. Users require an ",(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/blog/open-table-formats-and-the-open-data-lakehouse-in-perspective",children:"end-to-end open data lakehouse"}),", and modern data lakehouse features need a sophisticated layer of ",(0,n.jsx)(a.em,{children:(0,n.jsx)(a.strong,{children:"open-source software"})})," operating on data stored in open table formats. For example, Optimized writers can balance cost and performance by carefully managing file sizes using the statistics maintained in the table format or catalog syncing service that can make data in Hudi readily available to half a dozen catalogs open and closed out there. Hudi shines by providing a high-performance open table format as well as a comprehensive open-source software stack that can ingest, store, optimize and effectively self-manage a data lakehouse. This distinction between open formats and open software is often lost in translation inside the large vendor ecosystem in which Hudi operates. Still, it has been and remains a key consideration for Hudi\u2019s ",(0,n.jsx)(a.a,{href:"/powered-by",children:"users"})," to avoid compute-lockin to any given data vendor. The Hudi streamer tool, e.g., powers hundreds of data lakes by ingesting data seamlessly from various sources at the convenience of a single command in a terminal."]}),"\n",(0,n.jsx)("div",{style:{textAlign:"center",width:"90%",height:"auto"},children:(0,n.jsx)("img",{src:"/assets/images/blog/dlms-hierarchy.png",alt:"dlms hierarchy"})}),"\n",(0,n.jsxs)(a.p,{children:["Moving forward with 1.0, the community has ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/pull/8679",children:"debated"})," these key points and concluded that we need more open-source \u201c",(0,n.jsx)(a.strong,{children:"software capabilities"}),"\u201d that are directly comparable with DBMSes for two main reasons."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Significantly expand the technical capabilities of a data lakehouse"}),": Many design decisions in Hudi have been inspired by databases (see ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/rfc/rfc-69/rfc-69.md#hudi-1x",children:"here"})," for a layer-by-layer mapping) and have delivered significant benefits to the community. For example, Hudi\u2019s indexing mechanisms deliver the fast update performance the project has come to be known for.  We want to generalize such features across writers and queries and introduce new capabilities like fast metastores for query planning, support for unstructured/multimodal data and caching mechanisms that can be deeply integrated into (at least) open-source query engines in the ecosystem. We also need concurrency control that works for lakehouse workloads instead of employing techniques applicable to OLTP databases at the surface level."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"We also need a database-like experience"}),": We originally designed Hudi as a software library that can be embedded into different query/processing engines for reading/writing/managing tables. This model has been a great success within the existing data ecosystem, which is familiar with scheduling jobs and employing multiple engines for ETL and interactive queries. However, for a new user wanting to explore data lakehouses, there is no piece of software to easily install and explore all functionality packaged coherently. Such data lakehouse functionality packaged and delivered like a typical database system unlocks new use cases. For example, with such a system, we could bring HTAP capabilities to the data lakehouses on faster cloud storage/row-oriented formats, finally making it a low-latency data serving layer."]}),"\n",(0,n.jsxs)(a.p,{children:["If combined, we would gain a powerful database built on top of the data lake(house) architecture\u2014a ",(0,n.jsx)(a.em,{children:(0,n.jsx)(a.strong,{children:"data"})})," ",(0,n.jsx)(a.em,{children:(0,n.jsx)(a.strong,{children:"lakehouse"})})," ",(0,n.jsx)(a.em,{children:(0,n.jsx)(a.strong,{children:"management"})})," ",(0,n.jsx)(a.em,{children:(0,n.jsx)(a.strong,{children:"system"})})," ",(0,n.jsx)(a.em,{children:(0,n.jsx)(a.strong,{children:"(DLMS)"})}),"\u2014that we believe the industry needs."]}),"\n",(0,n.jsx)(a.h2,{id:"key-features-in-hudi-10",children:"Key Features in Hudi 1.0"}),"\n",(0,n.jsxs)(a.p,{children:["In Hudi 1.0, we\u2019ve delivered a significant expansion of data lakehouse technical capabilities discussed above inside Hudi\u2019s ",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Database_engine",children:"storage engine"})," layer.  Storage engines (a.k.a database engines) are standard database components that sit on top of the storage/file/table format and are wrapped by the DBMS layer above, handling the core read/write/management functionality. In the figure below, we map the Hudi components with the seminal ",(0,n.jsx)(a.a,{href:"https://dsf.berkeley.edu/papers/fntdb07-architecture.pdf",children:"Architecture of a Database System"})," paper (see page 4) to illustrate the standard layering discussed. If the layering is implemented correctly, we can deliver the benefits of the storage engine to even other table formats, which may lack such fully-developed open-source software for table management or achieving high performance, via interop standards defined in projects like ",(0,n.jsx)(a.a,{href:"https://xtable.apache.org/",children:"Apache XTable (Incubating)"}),"."]}),"\n",(0,n.jsxs)("div",{style:{textAlign:"center",width:"80%",height:"auto"},children:[(0,n.jsx)("img",{src:"/assets/images/hudi-stack-1-x.png",alt:"Hudi DB Architecture"}),(0,n.jsx)("p",{align:"center",children:"Figure: Apache Hudi Database Architecture"})]}),"\n",(0,n.jsx)(a.p,{children:"Regarding full-fledged DLMS functionality, the closest experience Hudi 1.0 offers is through Apache Spark. Users can deploy a Spark server (or Spark Connect) with Hudi 1.0 installed, submit SQL/jobs, orchestrate table services via SQL commands, and enjoy new secondary index functionality to speed up queries like a DBMS. Subsequent releases in the 1.x release line and beyond will continuously add new features and improve this experience."}),"\n",(0,n.jsx)(a.p,{children:"In the following sections, let\u2019s dive into what makes Hudi 1.0 a standout release."}),"\n",(0,n.jsx)(a.h3,{id:"new-time-and-timeline",children:"New Time and Timeline"}),"\n",(0,n.jsxs)(a.p,{children:["For the familiar user, time is a key concept in Hudi. Hudi\u2019s original notion of time was instantaneous, i.e., actions that modify the table appear to take effect at a given instant. This was limiting when designing features like non-blocking concurrency control across writers, which needs to reason about actions more as an \u201cinterval\u201d to detect other conflicting actions. Every action on the Hudi timeline now gets a ",(0,n.jsx)(a.em,{children:"requested"})," and a ",(0,n.jsx)(a.em,{children:"completion"})," time; Thus, the timeline layout version has bumped up in the 1.0 release. Furthermore, to ease the understanding and bring consistency around time generation for users and implementors, we have formalized the adoption of ",(0,n.jsx)(a.a,{href:"/docs/timeline#truetime-generation",children:"TrueTime"})," semantics. The default implementation assures forward-moving clocks even with distributed processes, assuming a maximum tolerable clock skew similar to ",(0,n.jsx)(a.a,{href:"https://cockroachlabs.com/blog/living-without-atomic-clocks/",children:"OLTP/NoSQL"})," stores adopting TrueTime."]}),"\n",(0,n.jsxs)("div",{style:{textAlign:"center"},children:[(0,n.jsx)("img",{src:"/assets/images/hudi-timeline-actions.png",alt:"Timeline actions"}),(0,n.jsx)("p",{align:"center",children:"Figure: Showing actions in Hudi 1.0 modeled as an interval of two instants: requested and completed"})]}),"\n",(0,n.jsxs)(a.p,{children:["Hudi tables are frequently updated, and users also want to retain a more extended action history associated with the table. Before Hudi 1.0, the older action history in a table was archived for audit access. But, due to the lack of support for cloud storage appends, access might become cumbersome due to tons of small files. In Hudi 1.0, we have redesigned the timeline as an ",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Log-structured_merge-tree",children:"LSM tree"}),", which is widely adopted for cases where good write performance on temporal data is desired."]}),"\n",(0,n.jsxs)(a.p,{children:["In the Hudi 1.0 release, the ",(0,n.jsx)(a.a,{href:"/docs/timeline#lsm-timeline-history",children:"LSM timeline"})," is heavily used in the query planning to map requested and completion times across Apache Spark, Apache Flink and Apache Hive. Future releases plan to leverage this to unify the timeline's active and history components, providing infinite retention of table history. Micro benchmarks show that the LSM timeline can be pretty efficient, even committing every ",(0,n.jsx)(a.em,{children:(0,n.jsx)(a.strong,{children:"30 seconds for 10 years with about 10M instants"})}),", further cementing Hudi\u2019s table format as the most suited for frequently written tables."]}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Number of actions"}),(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Instant Batch Size"}),(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Read cost (just times)"}),(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Read cost \v(along with action metadata)"}),(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Total file size"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"10000"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"10"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"32ms"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"150ms"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"8.39MB"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"20000"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"10"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"51ms"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"188ms"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"16.8MB"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"10000000"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"1000"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"3400ms"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"162s"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"8.4GB"})]})]})]}),"\n",(0,n.jsx)(a.h3,{id:"secondary-indexing-for-faster-lookups",children:"Secondary Indexing for Faster Lookups"}),"\n",(0,n.jsxs)(a.p,{children:["Indexes are core to Hudi\u2019s design, so much so that even the first pre-open-source version of Hudi shipped with ",(0,n.jsx)(a.a,{href:"/docs/indexes#additional-writer-side-indexes",children:"indexes"})," to speed up writes. However, these indexes were limited to the writer's side, except for record indexes in 0.14+ above, which were also integrated with Spark SQL queries. Hudi 1.0 generalizes indexes closer to the indexing functionality found in relational databases, supporting indexes on any secondary column across both writer and readers. Hudi 1.0 also supports near-standard ",(0,n.jsx)(a.a,{href:"/docs/sql_ddl#create-index",children:"SQL syntax"})," for creating/dropping indexes on different columns via Spark SQL, along with an asynchronous indexing table service to build indexes without interrupting the writers."]}),"\n",(0,n.jsxs)("div",{style:{textAlign:"center",paddingLeft:"10%",width:"70%",height:"auto"},children:[(0,n.jsx)("img",{src:"/assets/images/hudi-stack-indexes.png",alt:"Indexes"}),(0,n.jsx)("p",{align:"center",children:"Figure: the indexing subsystem in Hudi 1.0, showing different types of indexes"})]}),"\n",(0,n.jsxs)(a.p,{children:["With secondary indexes, queries and DMLs scan a much-reduced amount of files from cloud storage, dramatically reducing costs (e.g., on engines like AWS Athena, which price by data scanned) and improving query performance for queries with low to even moderate amount of selectivity. On a benchmark of a query on ",(0,n.jsx)(a.em,{children:"web_sales"})," table (from ",(0,n.jsx)(a.em,{children:(0,n.jsx)(a.strong,{children:"10 TB tpc-ds dataset"})}),"), with file groups - 286,603, total records - 7,198,162,544 and cardinality of secondary index column in the ~ 1:150 ranges, we see a remarkable ",(0,n.jsx)(a.em,{children:(0,n.jsx)(a.strong,{children:"~95% decrease in latency"})}),"."]}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Run 1"}),(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Total Query Latency w/o indexing skipping (secs)"}),(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Total Query Latency with secondary index skipping (secs)"}),(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"% decrease"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"1"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"252"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"31"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"~88%"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"2"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"214"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"10"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"~95%"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"3"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"204"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"9"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"~95%"})]})]})]}),"\n",(0,n.jsx)(a.p,{children:"In Hudi 1.0, secondary indexes are only supported for Apache Spark, with planned support for other engines in Hudi 1.1, starting with Flink, Presto and Trino."}),"\n",(0,n.jsx)(a.h3,{id:"bloom-filter-indexes",children:"Bloom Filter indexes"}),"\n",(0,n.jsxs)(a.p,{children:["Bloom filter indexes have existed on the Hudi writers for a long time. It is one of the most performant and versatile indexes users prefer for \u201cneedle-in-a-haystack\u201d deletes/updates or de-duplication. The index works by storing special footers in base files around min/max key ranges and a dynamic bloom filter that adapts to the file size and can automatically handle partitioning/skew on the writer's path. Hudi 1.0 introduces a newer kind of bloom filter index for Spark SQL while retaining the writer-side index as-is. The new index stores bloom filters in the Hudi metadata table and other secondary/record indexes for scalable access, even for huge tables, since the index is stored in fewer files compared to being stored alongside data files. It can be created using standard ",(0,n.jsx)(a.a,{href:"/docs/sql_ddl#create-bloom-filter-index",children:"SQL syntax"}),", as shown below. Subsequent queries on the indexed columns will use the bloom filters to speed up queries."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"-- Create a bloom filter index on the driver column of the table `hudi_table`\nCREATE INDEX idx_bloom_driver ON hudi_indexed_table USING bloom_filters(driver)\v\n-- Create a bloom filter index on the column derived from expression `lower(rider)` of the table `hudi_table`\nCREATE INDEX idx_bloom_rider ON hudi_indexed_table USING bloom_filters(rider) OPTIONS(expr='lower');\n"})}),"\n",(0,n.jsx)(a.p,{children:"In future releases of Hudi, we aim to fully integrate the benefits of the older writer-side index into the new bloom index. Nonetheless, this demonstrates the adaptability of Hudi\u2019s indexing system to handle different types of indexes on the table."}),"\n",(0,n.jsx)(a.h3,{id:"partitioning-replaced-by-expression-indexes",children:"Partitioning replaced by Expression Indexes"}),"\n",(0,n.jsxs)(a.p,{children:["An astute reader may have noticed above that the indexing is supported on a function/expression on a column. Hudi 1.0 introduces expression indexes similar to ",(0,n.jsx)(a.a,{href:"https://www.postgresql.org/docs/current/indexes-expressional.html",children:"Postgres"})," to generalize a two-decade-old relic in the data lake ecosystem - partitioning! At a high level, partitioning on the data lake divides the table into folders based on a column or a mapping function (partitioning function). When queries or operations are performed against the table, they can efficiently skip entire partitions (folders), reducing the amount of metadata and data involved. This is very effective since data lake tables span 100s of thousands of files. But, as simple as it sounds, this is one of the ",(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/blog/knowing-your-data-partitioning-vices-on-the-data-lakehouse",children:"most common pitfalls"})," around performance on the data lake, where new users use it like an index by partitioning based on a high cardinality column, resulting in lots of storage partitions/tiny files and abysmal write/query performance for no good reason. Further, tying storage organization to partitioning makes it inflexible to changes."]}),"\n",(0,n.jsxs)("div",{style:{textAlign:"center"},children:[(0,n.jsx)("img",{src:"/assets/images/expression-index-date-partitioning.png",alt:"Timeline actions"}),(0,n.jsx)("p",{align:"center",children:"Figure: Shows index on a date expression when a different column physically partitions data"})]}),"\n",(0,n.jsxs)(a.p,{children:["Hudi 1.0 treats partitions as a ",(0,n.jsx)(a.a,{href:"/docs/sql_queries#query-using-column-stats-expression-index",children:"coarse-grained index"})," on a column value or an expression of a column, as they should have been. To support the efficiency of skipping entire storage paths/folders, Hudi 1.0 introduces partition stats indexes that aggregate these statistics on the storage partition path level, in addition to doing so at the file level. Now, users can create different types of indexes on columns to achieve the effects of partitioning in a streamlined fashion using fewer concepts to achieve the same results. Along with support for other 1.x features, partition stats and expression indexes support will be extended to other engines like Presto, Trino, Apache Doris, and Starrocks with the 1.1 release."]}),"\n",(0,n.jsx)(a.h3,{id:"efficient-partial-updates",children:"Efficient Partial Updates"}),"\n",(0,n.jsxs)(a.p,{children:["Managing large-scale datasets often involves making fine-grained changes to records. Hudi has long supported ",(0,n.jsx)(a.a,{href:"/docs/0.15.0/record_payload#partialupdateavropayload",children:"partial updates"})," to records via the record payload interface. However, this usually comes at the cost of sacrificing engine-native performance by moving away from specific objects used by engines to represent rows. As users have embraced Hudi for incremental SQL pipelines on top of dbt/Spark or Flink Dynamic Tables, there was a rise in interest in making this much more straightforward and mainstream. Hudi 1.0 introduces first-class support for ",(0,n.jsx)(a.strong,{children:"partial updates"})," at the log format level, enabling ",(0,n.jsx)(a.em,{children:"MERGE INTO"})," SQL statements to modify only the changed fields of a record instead of rewriting/reprocessing the entire row."]}),"\n",(0,n.jsxs)(a.p,{children:["Partial updates improve query and write performance simultaneously by reducing write amplification for writes and the amount of data read by Merge-on-Read snapshot queries. It also achieves much better storage utilization due to fewer bytes stored and improved compute efficiency over existing partial update support by retaining vectorized engine-native processing. Using the 1TB Brooklyn benchmark for write performance, we observe about ",(0,n.jsx)(a.strong,{children:"2.6x"})," improvement in Merge-on-Read query performance due to an ",(0,n.jsx)(a.strong,{children:"85%"})," reduction in write amplification. For random write workloads, the gains can be much more pronounced. Below shows a second benchmark for partial updates, 1TB MOR table, 1000 partitions, 80% random updates. 3/100 columns randomly updated."]}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{style:{textAlign:"left"}}),(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Full Record Update"}),(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Partial Update"}),(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Gains"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:(0,n.jsx)(a.strong,{children:"Update latency (s)"})}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"2072"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"1429"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"1.4x"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:(0,n.jsx)(a.strong,{children:"Bytes written (GB)"})}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"891.7"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"12.7"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"70.2x"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:(0,n.jsx)(a.strong,{children:"Query latency (s)"})}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"164"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"29"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"5.7x"})]})]})]}),"\n",(0,n.jsxs)(a.p,{children:["This also lays the foundation for managing unstructured and multimodal data inside a Hudi table and supporting ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/pull/11733",children:"wide tables"})," efficiently for machine learning use cases."]}),"\n",(0,n.jsx)(a.h3,{id:"merge-modes-and-custom-mergers",children:"Merge Modes and Custom Mergers"}),"\n",(0,n.jsxs)(a.p,{children:["One of the most unique capabilities Hudi provides is how it helps process streaming data. Specifically, Hudi has, since the very beginning, supported merging records pre-write (to reduce write amplification), during write (against an existing record in storage with the same record key) and reads (for MoR snapshot queries), using a ",(0,n.jsx)(a.em,{children:"precombine"})," or ",(0,n.jsx)(a.em,{children:"ordering"})," field. This helps implement ",(0,n.jsx)(a.a,{href:"https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/",children:"event time processing"})," semantics, widely supported by stream processing systems, on data lakehouse storage. This helps integrate late-arriving data into Hudi tables without causing weird movement of record state back in time. For example, if an older database CDC record arrives late and gets committed as the new value, the state of the record would be incorrect even though the writes to the table themselves were serialized in some order."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{alt:"event time ordering",src:t(90534).A+"",width:"1360",height:"490"}),"\n",(0,n.jsx)("p",{align:"center",children:"Figure: Shows EVENT_TIME_ORDERING where merging reconciles state based on the highest event_time"})]}),"\n",(0,n.jsxs)(a.p,{children:["Prior Hudi versions supported this functionality through the record payload interface with built-in support for a pre-combine field on the default payloads. Hudi 1.0 makes these two styles of processing and merging changes first class by introducing ",(0,n.jsx)(a.a,{href:"/docs/record_merger",children:"merge modes"})," within Hudi."]}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Merge Mode"}),(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"What does it do?"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"COMMIT_TIME_ORDERING"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"Picks record with highest completion time/instant as final merge result  i.e., standard relational semantics or arrival time processing"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"EVENT_TIME_ORDERING"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"Default (for now, to ease migration).\vPicks record with the highest value for a user-specified ordering/precombine field as the final merge result."})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"CUSTOM"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"Uses a user-provided RecordMerger implementation to produce final merge result (similar to stream processing processor APIs)"})]})]})]}),"\n",(0,n.jsxs)(a.p,{children:["Like partial update support, the new ",(0,n.jsx)(a.em,{children:"RecordMerger"})," API provides a more efficient engine-native alternative to the older RecordPayload interface through native objects and vectorized processing on EVENT_TIME_ORDERING merge modes. In future versions, we intend to change the default to COMMIT_TIME_ORDERING to provide simple, out-of-the-box relational table semantics."]}),"\n",(0,n.jsx)(a.h3,{id:"non-blocking-concurrency-control-for-streaming-writes",children:"Non-Blocking Concurrency Control for Streaming Writes"}),"\n",(0,n.jsx)(a.p,{children:"We have expressed dissatisfaction with the optimistic concurrency control approaches employed on the data lakehouse since they appear to paint the problem with a broad brush without paying attention to the nuances of the lakehouse workloads. Specifically, contention is much more common in data lakehouses, even for Hudi, the only data lakehouse storage project capable of asynchronously compacting delta updates without failing or causing retries on the writer. Ultimately, data lakehouses are high-throughput systems, and failing concurrent writers to handle contention can waste expensive compute clusters. Streaming and high-frequency writes often require fine-grained concurrency control to prevent bottlenecks."}),"\n",(0,n.jsxs)(a.p,{children:["Hudi 1.0 introduces a new ",(0,n.jsx)(a.strong,{children:"non-blocking concurrency control (NBCC)"})," designed explicitly for data lakehouse workloads, using years of experience gained supporting some of the largest data lakes on the planet in the Hudi community. NBCC enables simultaneous writing from multiple writers and compaction of the same record without blocking any involved processes. This is achieved by simply lightweight distributed locks and TrueTime semantics discussed above. (see ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/rfc/rfc-66/rfc-66.md",children:"RFC-66"})," for more)"]}),"\n",(0,n.jsxs)("div",{style:{textAlign:"center"},children:[(0,n.jsx)("img",{src:"/assets/images/nbcc_partial_updates.gif",alt:"NBCC"}),(0,n.jsx)("p",{align:"center",children:"Figure: Two streaming jobs in action writing to the same records concurrently on different columns."})]}),"\n",(0,n.jsxs)(a.p,{children:["NBCC operates with streaming semantics, tying together concepts from previous sections. Data necessary to compute table updates are emitted from an upstream source, and changes and partial updates can be merged in any of the merge modes above. For example, in the figure above, two independent Flink jobs enrich different table columns in parallel, a pervasive pattern seen in stream processing use cases. Check out this ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control",children:"blog"})," for a full demo. We also expect to support NBCC across other compute engines in future releases."]}),"\n",(0,n.jsx)(a.h3,{id:"backwards-compatible-writing",children:"Backwards Compatible Writing"}),"\n",(0,n.jsx)(a.p,{children:"If you are wondering: \u201cAll of this sounds cool, but how do I upgrade?\u201d we have put a lot of thought into making that seamless. Hudi has always supported backward-compatible reads to older table versions. Table versions are stored in table properties unrelated to the software binary version. The supported way of upgrading has been to first migrate readers/query engines to new software binary versions and then upgrade the writers, which will auto-upgrade the table if there is a table version change between the old and new software binary versions. Upon community feedback, users expressed the need to be able to do upgrades on the writers without waiting on the reader side upgrades and reduce any additional coordination necessary within different teams."}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{alt:"Indexes",src:t(45960).A+"",width:"1481",height:"825"}),"\n",(0,n.jsx)("p",{align:"center",children:"Figure: 4-step process for painless rolling upgrades to Hudi 1.0"})]}),"\n",(0,n.jsxs)(a.p,{children:["Hudi 1.0 introduces backward-compatible writing to achieve this in 4 steps, as described above. Hudi 1.0 also automatically handles any checkpoint translation necessary as we switch to completion time-based processing semantics for incremental and CDC queries. The Hudi metadata table has to be temporarily disabled during this upgrade process but can be turned on once the upgrade is completed successfully. Please read the ",(0,n.jsx)(a.a,{href:"/releases/release-1.0.0",children:"release notes"})," carefully to plan your migration."]}),"\n",(0,n.jsx)(a.h2,{id:"whats-next",children:"What\u2019s Next?"}),"\n",(0,n.jsx)(a.p,{children:"Hudi 1.0 is a testament to the power of open-source collaboration. This release embodies the contributions of 60+ developers, maintainers, and users who have actively shaped its roadmap. We sincerely thank the Apache Hudi community for their passion, feedback, and unwavering support."}),"\n",(0,n.jsxs)(a.p,{children:["The release of Hudi 1.0 is just the beginning. Our current ",(0,n.jsx)(a.a,{href:"/roadmap",children:"roadmap"})," includes exciting developments across the following planned releases:"]}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"1.0.1"}),": First bug fix, patch release on top of 1.0, which hardens the functionality above and makes it easier. We intend to publish additional patch releases to aid migration to 1.0 as the bridge release for the community from 0.x."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"1.1"}),":  Faster writer code path rewrite, new indexes like bitmap/vector search, granular record-level change encoding, Hudi storage engine APIs, abstractions for cross-format interop."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"1.2"}),": Multi-table transactions, platform services for reverse streaming from Hudi etc., Multi-modal data + indexing, NBCC clustering"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"2.0"}),": Server components for DLMS, caching and metaserver functionality."]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Hudi releases are drafted collaboratively by the community. If you don\u2019t see something you like here, please help shape the roadmap together."}),"\n",(0,n.jsx)(a.h2,{id:"get-started-with-apache-hudi-10",children:"Get Started with Apache Hudi 1.0"}),"\n",(0,n.jsx)(a.p,{children:"Are you ready to experience the future of data lakehouses? Here\u2019s how you can dive into Hudi 1.0:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["Documentation: Explore Hudi\u2019s ",(0,n.jsx)(a.a,{href:"/docs/overview",children:"Documentation"})," and learn the ",(0,n.jsx)(a.a,{href:"/docs/hudi_stack",children:"concepts"}),"."]}),"\n",(0,n.jsxs)(a.li,{children:["Quickstart Guide: Follow the ",(0,n.jsx)(a.a,{href:"/docs/quick-start-guide",children:"Quickstart Guide"})," to set up your first Hudi project."]}),"\n",(0,n.jsxs)(a.li,{children:["Upgrading from a previous version?  Follow the ",(0,n.jsx)(a.a,{href:"/releases/release-1.0.0#migration-guide",children:"migration guide"})," and contact the Hudi OSS community for help."]}),"\n",(0,n.jsxs)(a.li,{children:["Join the Community: Participate in discussions on the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/community/get-involved/",children:"Hudi Mailing List"}),", ",(0,n.jsx)(a.a,{href:"https://join.slack.com/t/apache-hudi/shared_invite/zt-2ggm1fub8-_yt4Reu9djwqqVRFC7X49g",children:"Slack"})," and ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/issues",children:"GitHub"}),"."]}),"\n",(0,n.jsxs)(a.li,{children:["Follow us on social media: ",(0,n.jsx)(a.a,{href:"https://www.linkedin.com/company/apache-hudi/?viewAsMember=true",children:"Linkedin"}),", ",(0,n.jsx)(a.a,{href:"https://twitter.com/ApacheHudi",children:"X/Twitter"}),"."]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"We can\u2019t wait to see what you build with Apache Hudi 1.0. Let\u2019s work together to shape the future of data lakehouses!"}),"\n",(0,n.jsx)(a.p,{children:"Crafted with passion for the Apache Hudi community."})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},7480:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(83846),n=t(74848),s=t(28453);t(82915);const r={title:"Apache Hudi 2024: A Year In Review",excerpt:"Reflect on and celebrate the myriad of exciting developments and accomplishments that have defined the year 2024 for the Hudi community.",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2024-12-29-a-year-in-review-2024/cover.jpg",tags:["apache hudi","community"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Community Growth and Engagement",id:"community-growth-and-engagement",level:2},{value:"Major Milestones",id:"major-milestones",level:2},{value:"Apache Hudi 1.0 Release",id:"apache-hudi-10-release",level:3},{value:"Launch of Hudi-rs",id:"launch-of-hudi-rs",level:3},{value:"Published Books and Educational Content",id:"published-books-and-educational-content",level:3},{value:"Community Events and Sharing",id:"community-events-and-sharing",level:2},{value:"Lakehouse Chronicles with Apache Hudi",id:"lakehouse-chronicles-with-apache-hudi",level:3},{value:"Hudi Newsletter",id:"hudi-newsletter",level:3},{value:"Community Syncs",id:"community-syncs",level:3},{value:"Notable User Stories and Technical Content",id:"notable-user-stories-and-technical-content",level:2},{value:"Looking Ahead to 2025",id:"looking-ahead-to-2025",level:2},{value:"Get Involved",id:"get-involved",level:2}];function c(e){const a={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)("img",{src:"/assets/images/blog/2024-12-29-a-year-in-review-2024/cover.jpg",alt:"drawing",style:{width:"80%",display:"block",marginLeft:"auto",marginRight:"auto",marginTop:"18pt",marginBottom:"18pt"}}),"\n",(0,n.jsx)(a.p,{children:"As we wrap up another remarkable year for Apache Hudi, I am thrilled to reflect on the tremendous achievements and milestones that have defined 2024. This year has been particularly special as we achieved several significant milestones, including the landmark release of Hudi 1.0, the publication of comprehensive books, and the introduction of new tools that expand Hudi's ecosystem."}),"\n",(0,n.jsx)(a.h2,{id:"community-growth-and-engagement",children:"Community Growth and Engagement"}),"\n",(0,n.jsx)(a.p,{children:"The Apache Hudi community continued its impressive growth trajectory in 2024. The number of new PRs has remained stable, indicating a consistent level of development activities:"}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2024-12-29-a-year-in-review-2024/pr-history.svg",alt:"drawing",style:{width:"80%",display:"block",marginLeft:"auto",marginRight:"auto",marginTop:"18pt",marginBottom:"18pt"}}),"\n",(0,n.jsx)(a.p,{children:"Our community presence expanded significantly across various platforms:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"The community grew to over 10,500 followers on LinkedIn"}),"\n",(0,n.jsx)(a.li,{children:"Added 8,755 new followers in the last 365 days"}),"\n",(0,n.jsx)(a.li,{children:"Generated 441,402 content impressions"}),"\n",(0,n.jsx)(a.li,{children:"Received 6,555 reactions and 493 comments across platforms"}),"\n",(0,n.jsx)(a.li,{children:"Our Slack community remained vibrant with rich technical discussions and knowledge sharing"}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"major-milestones",children:"Major Milestones"}),"\n",(0,n.jsx)(a.h3,{id:"apache-hudi-10-release",children:"Apache Hudi 1.0 Release"}),"\n",(0,n.jsxs)(a.p,{children:["2024 marked a historic moment with the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-1.0.0",children:"release of Apache Hudi 1.0"}),", representing a major evolution in data lakehouse technology. This release brought several groundbreaking features:"]}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Secondary Indexing"}),": First of its kind in lakehouses, enabling database-like query acceleration with demonstrated 95% latency reduction on 10TB TPC-DS for low-moderate selectivity queries"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Logical Partitioning via Expression Indexes"}),": Introducing PostgreSQL-style expression indexes for more efficient partition management"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Partial Updates"}),": Achieving 2.6x performance improvement and 85% reduction in bytes written for update-heavy workloads"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Non-blocking Concurrency Control (NBCC)"}),": An industry-first feature allowing simultaneous writing from multiple writers"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Merge Modes"}),": First-class support for both ",(0,n.jsx)(a.code,{children:"commit_time_ordering"})," and ",(0,n.jsx)(a.code,{children:"event_time_ordering"})]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"LSM Timeline"}),": Revamped timeline storage as a scalable LSM tree for extended table history retention"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"TrueTime"}),": Strengthened time semantics ensuring forward-moving clocks in distributed processes"]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["Please check out the ",(0,n.jsx)(a.a,{href:"/blog/2024/12/16/announcing-hudi-1-0-0",children:"announcement blog"}),"."]}),"\n",(0,n.jsx)(a.h3,{id:"launch-of-hudi-rs",children:"Launch of Hudi-rs"}),"\n",(0,n.jsxs)(a.p,{children:["A significant expansion of the Hudi ecosystem occurred with the ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi-rs",children:"release of Hudi-rs"}),", the native Rust implementation for Apache Hudi with Python API bindings. This new project enables:"]}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Reading Hudi Tables without Spark or JVM dependencies"}),"\n",(0,n.jsx)(a.li,{children:"Integration with Apache Arrow for enhanced compatibility"}),"\n",(0,n.jsx)(a.li,{children:"Support for Copy-on-Write (CoW) table snapshots and time-travel reads"}),"\n",(0,n.jsx)(a.li,{children:"Cloud storage support across AWS, Azure, and GCP"}),"\n",(0,n.jsx)(a.li,{children:"Native integration with Apache DataFusion, Ray, Daft, etc"}),"\n"]}),"\n",(0,n.jsx)(a.h3,{id:"published-books-and-educational-content",children:"Published Books and Educational Content"}),"\n",(0,n.jsx)(a.p,{children:"2024 saw the release of two comprehensive guides to Apache Hudi:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://learning.oreilly.com/library/view/apache-hudi-the/9781098173821/",children:(0,n.jsx)(a.strong,{children:'"Apache Hudi: The Definitive Guide"'})})," (O'Reilly) - Released in early access, ",(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/whitepaper/apache-hudi-the-definitive-guide",children:"free copy available"}),", providing comprehensive coverage of:","\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Distributed query engines"}),"\n",(0,n.jsx)(a.li,{children:"Snapshot and time travel queries"}),"\n",(0,n.jsx)(a.li,{children:"Incremental queries"}),"\n",(0,n.jsx)(a.li,{children:"Change-data-capture modes"}),"\n",(0,n.jsx)(a.li,{children:"End-to-end ingestion with Hudi Streamer"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2024-12-29-a-year-in-review-2024/hudi-tdg.jpg",alt:"drawing",style:{width:"80%",display:"block",marginLeft:"auto",marginRight:"auto",marginTop:"18pt",marginBottom:"18pt"}}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://blog.datumagic.com/p/apache-hudi-from-zero-to-one-110",children:(0,n.jsx)(a.strong,{children:'"Apache Hudi: From Zero to One"'})})," - A 10-part blog series turned into ",(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/whitepaper/ebook-apache-hudi---zero-to-one",children:"an ebook"}),", offering deep technical insights into Hudi's architecture and capabilities, covering:","\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Storage format and operations"}),"\n",(0,n.jsx)(a.li,{children:"Read and write flows"}),"\n",(0,n.jsx)(a.li,{children:"Table services and indexing"}),"\n",(0,n.jsx)(a.li,{children:"Incremental processing"}),"\n",(0,n.jsx)(a.li,{children:"Hudi 1.0 features"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2024-12-29-a-year-in-review-2024/hudi0to1.png",alt:"drawing",style:{width:"80%",display:"block",marginLeft:"auto",marginRight:"auto",marginTop:"18pt",marginBottom:"18pt"}}),"\n",(0,n.jsx)(a.h2,{id:"community-events-and-sharing",children:"Community Events and Sharing"}),"\n",(0,n.jsx)(a.p,{children:"The Apache Hudi community maintained a strong presence at major industry events throughout 2024:"}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2024-12-29-a-year-in-review-2024/community-events.png",alt:"drawing",style:{width:"80%",display:"block",marginLeft:"auto",marginRight:"auto",marginTop:"18pt",marginBottom:"18pt"}}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Databricks' Data+AI Summit - Presenting Apache Hudi's role in the lakehouse ecosystem and its interoperability with other table formats through XTable, an open-source project enabling seamless conversion between Hudi, Delta Lake, and Iceberg"}),"\n",(0,n.jsx)(a.li,{children:"Confluent's Current 2024 - Demonstrating Hudi's powerful CDC capabilities with Apache Flink, showcasing real-time data pipelines and the innovative Non-Blocking Concurrency Control (NBCC) for high-volume streaming workloads"}),"\n",(0,n.jsx)(a.li,{children:"Trino Fest 2024 - Showcasing Hudi connector's evolution and innovations in Trino, including multi-modal indexing capabilities and the roadmap for enhanced query performance through Alluxio-powered caching and expanded DDL/DML support"}),"\n",(0,n.jsx)(a.li,{children:"Bangalore Lakehouse Days - Deep dive into Apache Hudi 1.0's groundbreaking features including LSM-based timeline, functional indexes, and non-blocking concurrency control, demonstrating Hudi's continued innovation in the lakehouse space"}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Additionally, the community launched several new initiatives to foster learning and knowledge sharing:"}),"\n",(0,n.jsx)(a.h3,{id:"lakehouse-chronicles-with-apache-hudi",children:(0,n.jsx)(a.a,{href:"https://www.youtube.com/playlist?list=PLxSSOLH2WRMNQetyPU98B2dHnYv91R6Y8",children:"Lakehouse Chronicles with Apache Hudi"})}),"\n",(0,n.jsx)(a.p,{children:"A new community series with 4 episodes released."}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2024-12-29-a-year-in-review-2024/lakehouse-chronicles.png",alt:"drawing",style:{width:"80%",display:"block",marginLeft:"auto",marginRight:"auto",marginTop:"18pt",marginBottom:"18pt"}}),"\n",(0,n.jsx)(a.h3,{id:"hudi-newsletter",children:(0,n.jsx)(a.a,{href:"https://hudinewsletter.substack.com/",children:"Hudi Newsletter"})}),"\n",(0,n.jsx)(a.p,{children:"9 editions published, keeping the community informed about latest developments."}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2024-12-29-a-year-in-review-2024/newsletter.png",alt:"drawing",style:{width:"80%",display:"block",marginLeft:"auto",marginRight:"auto",marginTop:"18pt",marginBottom:"18pt"}}),"\n",(0,n.jsx)(a.h3,{id:"community-syncs",children:(0,n.jsx)(a.a,{href:"https://www.youtube.com/@apachehudi",children:"Community Syncs"})}),"\n",(0,n.jsx)(a.p,{children:"Featured 8 user stories from major organizations including Amazon, Peloton, Shopee and Uber."}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2024-12-29-a-year-in-review-2024/community-syncs.png",alt:"drawing",style:{width:"80%",display:"block",marginLeft:"auto",marginRight:"auto",marginTop:"18pt",marginBottom:"18pt"}}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.a,{href:"https://www.youtube.com/watch?v=rMXhlb7Uci8",children:"Powering Amazon Unit Economics with Configurations and Hudi"})}),"\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.a,{href:"https://www.youtube.com/watch?v=-Pyid5K9dyU",children:"Modernizing Data Infrastructure at Peleton using Apache Hudi"})}),"\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.a,{href:"https://www.youtube.com/watch?v=fqhr-4jXi6I",children:"Innovative Solution for Real-time Analytics at Scale using Apache Hudi (Shopee)"})}),"\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.a,{href:"https://www.youtube.com/watch?v=VpdimpH_nsI",children:"Scaling Complex Data Workflows using Apache Hudi (Uber)"})}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"notable-user-stories-and-technical-content",children:"Notable User Stories and Technical Content"}),"\n",(0,n.jsx)(a.p,{children:"Throughout 2024, several organizations shared their Hudi implementation experiences:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.a,{href:"https://www.notion.com/blog/building-and-scaling-notions-data-lake",children:"Notion's transition from Snowflake to Hudi"})}),"\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.a,{href:"https://engineering.grab.com/enabling-near-realtime-data-analytics",children:"Grab's implementation of near-realtime data analytics"})}),"\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.a,{href:"https://aws.amazon.com/blogs/big-data/use-aws-data-exchange-to-seamlessly-share-apache-hudi-datasets/",children:"AWS's data sharing capabilities with AWS Data Exchange"})}),"\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.a,{href:"https://www.y.uno/post/how-apache-hudi-transformed-yunos-data-lake",children:"Yuno's data lake transformation"})}),"\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.a,{href:"https://blogs.halodoc.io/data-lake-cost-optimisation-strategies/",children:"Halodoc's cost optimization strategies"})}),"\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.a,{href:"https://medium.com/upstox-engineering/navigating-the-future-the-evolutionary-journey-of-upstoxs-data-platform-92dc10ff22ae",children:"Upstox's data platform evolution"})}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"looking-ahead-to-2025",children:"Looking Ahead to 2025"}),"\n",(0,n.jsx)(a.p,{children:"As we look forward to 2025, Apache Hudi's roadmap includes several exciting developments:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Enhanced core engine with modernized write paths and advanced indexing (bitmap, vector search)"}),"\n",(0,n.jsx)(a.li,{children:"Multi-modal data support with improved storage engine APIs and cross-format interoperability"}),"\n",(0,n.jsx)(a.li,{children:"Enterprise-grade features including multi-table transactions and advanced caching"}),"\n",(0,n.jsx)(a.li,{children:"Robust platform services with Data Lakehouse Management System (DLMS) components"}),"\n",(0,n.jsx)(a.li,{children:"Broader adoption of Hudi-rs across the ecosystem"}),"\n",(0,n.jsx)(a.li,{children:"Continued focus on stability and seamless migration path for the community"}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"These initiatives reflect our commitment to advancing data lakehouse technology while ensuring reliability and user experience."}),"\n",(0,n.jsx)(a.h2,{id:"get-involved",children:"Get Involved"}),"\n",(0,n.jsx)(a.p,{children:"Join our thriving community:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["Contribute to the project on GitHub: ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi",children:"Hudi"})," & ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi-rs",children:"Hudi-rs"})]}),"\n",(0,n.jsxs)(a.li,{children:["Join our ",(0,n.jsx)(a.a,{href:"https://apache-hudi.slack.com/join/shared_invite/zt-2ggm1fub8-_yt4Reu9djwqqVRFC7X49g",children:"Slack community"})]}),"\n",(0,n.jsxs)(a.li,{children:["Follow us on ",(0,n.jsx)(a.a,{href:"https://www.linkedin.com/company/apache-hudi/",children:"LinkedIn"})," and ",(0,n.jsx)(a.a,{href:"https://x.com/apachehudi",children:"X (Twitter)"})]}),"\n",(0,n.jsxs)(a.li,{children:["Subscribe to our ",(0,n.jsx)(a.a,{href:"https://www.youtube.com/@apachehudi",children:"YouTube channel"})]}),"\n",(0,n.jsxs)(a.li,{children:["Participate in our ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/community/syncs",children:"community syncs"})," and ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/community/office_hours",children:"office hours"}),"."]}),"\n",(0,n.jsxs)(a.li,{children:["Subscribe to the dev mailing list by sending an empty email to ",(0,n.jsx)(a.code,{children:"dev-subscribe@hudi.apache.org"})]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"The success of Apache Hudi in 2024 wouldn't have been possible without our dedicated community of contributors, users, and supporters. As we celebrate these achievements, we look forward to another year of innovation and growth in 2025."})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},7550:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(69003),n=t(74848),s=t(28453),r=t(9230);const o={title:"Amazon Athena expands Apache Hudi support",category:"blog",image:"/assets/images/blog/aws.jpg",tags:["blog","amazon"]},l=void 0,d={authorsImageUrls:[]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/about-aws/whats-new/2021/07/amazon-athena-expands-apache-hudi-support/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},7604:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/bin_packing_existing_data_files-021f5b531f048bfd9cc1230f93c22a71.png"},7718:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(46380),n=t(74848),s=t(28453);const r={title:"Build Your First Hudi Lakehouse with AWS S3 and AWS Glue",excerpt:"Follow this tutorial on building your first hudi lakehouse with AWS S3 & AWS Glue",author:"Nadine Farah",category:"blog",image:"/assets/images/blog/DataCouncil.jpg",tags:["how-to","use-case","apache hudi","aws s3","aws glue"]},o="Build Your First Hudi Lakehouse with AWS S3 and AWS Glue",l={authorsImageUrls:[void 0]},d=[{value:"Getting Started",id:"getting-started",level:2},{value:"Questions",id:"questions",level:2}];function c(e){const a={a:"a",h2:"h2",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"/assets/images/blog/DataCouncil.jpg",src:t(74358).A+"",width:"1200",height:"602"})}),"\n",(0,n.jsxs)(a.p,{children:["Soumil Shah is a Hudi community champion building ",(0,n.jsx)(a.a,{href:"https://www.youtube.com/@SoumilShah/playlists",children:"YouTube content"})," so developers can easily get started incorporating a ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2024/07/11/what-is-a-data-lakehouse/",children:"lakehouse"})," into their data infrastructure. In this ",(0,n.jsx)(a.a,{href:"https://www.youtube.com/watch?v=5zF4jc_3rFs&list=PLL2hlSFBmWwwbMpcyMjYuRn8cN99gFSY6",children:"video"}),", Soumil shows you how to get started with AWS Glue, AWS S3, Hudi and Athena."]}),"\n",(0,n.jsx)(a.p,{children:"In this tutorial, you\u2019ll learn how to:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Create and configure AWS Glue"}),"\n",(0,n.jsx)(a.li,{children:"Create a Hudi Table"}),"\n",(0,n.jsx)(a.li,{children:"Create a Spark Data Frame"}),"\n",(0,n.jsx)(a.li,{children:"Add data to the Hudi Table"}),"\n",(0,n.jsx)(a.li,{children:"Query data via Athena"}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"/assets/images/blog/build-your-first-hudi-lakehouse-12-19-diagram.jpg",src:t(13738).A+"",width:"5185",height:"3085"})}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Step 1"}),": Users in this architecture purchase things from online retailers and generate an order transaction that is kept in DynamoDB."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Step 2"}),": The raw data layer stores the order transaction data that is fed into the data lake. To accomplish this, enable Kinesis Data Streams for DynamoDB, and we will stream real-time transactions from DynamoDB into kinesis data streams, process the streaming data with lambda, and insert the data into the next kinesis stream, where a glue streaming job will process and insert the data into Apache Hudi Transaction data lake."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Step 3"}),": Users can build dashboards and derive insights using QuickSight."]}),"\n",(0,n.jsx)(a.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,n.jsxs)(a.p,{children:["To get started on building this data app, follow the YouTube video on\n",(0,n.jsx)(a.a,{href:"https://www.youtube.com/watch?v=5zF4jc_3rFs&list=PLL2hlSFBmWwwbMpcyMjYuRn8cN99gFSY6&",children:"Build Datalakes on S3 and Glue with Apache HUDI"}),"."]}),"\n",(0,n.jsxs)(a.p,{children:["Follow the the ",(0,n.jsx)(a.a,{href:"https://drive.google.com/file/d/1W-E_SupsoI8VZWGtq5d7doxdWdNDPEoj/view",children:"step-by-step instructions"}),"."]}),"\n",(0,n.jsxs)(a.p,{children:["Apply the ",(0,n.jsx)(a.a,{href:"https://github.com/soumilshah1995/dynamodb-hudi-stream-project",children:"code source"}),"."]}),"\n",(0,n.jsx)(a.h2,{id:"questions",children:"Questions"}),"\n",(0,n.jsxs)(a.p,{children:["If you run into blockers doing this tutorial, please reach out on the Apache Hudi community and tag ",(0,n.jsx)(a.strong,{children:"soumilshah1995"})," to help debug."]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},7805:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/07/20/Backfilling-Apache-Hudi-Tables-in-Production-Techniques-and-Approaches-Using-AWS-Glue-by-Job-Target-LLC","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-07-20-Backfilling-Apache-Hudi-Tables-in-Production-Techniques-and-Approaches-Using-AWS-Glue-by-Job-Target-LLC.mdx","source":"@site/blog/2023-07-20-Backfilling-Apache-Hudi-Tables-in-Production-Techniques-and-Approaches-Using-AWS-Glue-by-Job-Target-LLC.mdx","title":"Backfilling Apache Hudi Tables in Production: Techniques & Approaches Using AWS Glue by Job Target LLC","description":"Redirecting... please wait!!","date":"2023-07-20T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"backfilling","permalink":"/blog/tags/backfilling"},{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"aws glue","permalink":"/blog/tags/aws-glue"},{"inline":true,"label":"code sample","permalink":"/blog/tags/code-sample"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Soumil Shah","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Backfilling Apache Hudi Tables in Production: Techniques & Approaches Using AWS Glue by Job Target LLC","authors":[{"name":"Soumil Shah"}],"category":"blog","image":"/assets/images/blog/2023-07-20-Backfilling-Apache-Hudi-Tables-in-Production-Techniques-and-Approaches-Using-AWS-Glue-by-Job-Target-LLC.png","tags":["blog","backfilling","hudi","aws glue","code sample"]},"unlisted":false,"prevItem":{"title":"AWS Glue Crawlers now supports Apache Hudi Tables","permalink":"/blog/2023/07/21/AWS-Glue-Crawlers-now-supports-Apache-Hudi-Tables"},"nextItem":{"title":"Hoodie Timeline: Foundational pillar for ACID transactions","permalink":"/blog/2023/07/09/Hoodie-Timeline-Foundational-pillar-for-ACID-transactions"}}')},7913:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(9620),n=t(74848),s=t(28453),r=t(9230);const o={title:"From Transactional Bottlenecks to Lightning-Fast Analytics",author:"Akash, Anudeep, Rohan",category:"blog",image:"/assets/images/blog/uptycs.png",tags:["blog","Apache Hudi","CDC","Debezium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/allthatscales/from-transactional-bottlenecks-to-lightning-fast-analytics-74e0d3fff1c0",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},8184:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2020/10/21/Architecting-Data-Lakes-for-the-Modern-Enterprise-at-Data-Summit-Connect-Fall-2020","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-10-21-Architecting-Data-Lakes-for-the-Modern-Enterprise-at-Data-Summit-Connect-Fall-2020.mdx","source":"@site/blog/2020-10-21-Architecting-Data-Lakes-for-the-Modern-Enterprise-at-Data-Summit-Connect-Fall-2020.mdx","title":"Architecting Data Lakes for the Modern Enterprise at Data Summit Connect Fall 2020","description":"Redirecting... please wait!!","date":"2020-10-21T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"dbta","permalink":"/blog/tags/dbta"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"Stephanie Simone","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Architecting Data Lakes for the Modern Enterprise at Data Summit Connect Fall 2020","authors":[{"name":"Stephanie Simone"}],"category":"blog","image":"/assets/images/blog/data-summit-connect.jpeg","tags":["blog","dbta"]},"unlisted":false,"prevItem":{"title":"Employing the right indexes for fast updates, deletes in Apache Hudi","permalink":"/blog/2020/11/11/hudi-indexing-mechanisms"},"nextItem":{"title":"Data Lake Change Capture using Apache Hudi & Amazon AMS/EMR","permalink":"/blog/2020/10/21/Data-Lake-Change-Capture-using-Apache-Hudi-and-Amazon-AMS-EMR"}}')},8193:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(78038),n=t(74848),s=t(28453),r=t(9230);const o={title:"An intro to Hudi with MinIO",author:"Simbu",category:"blog",image:"/assets/images/blog/2025-01-30-an-intro-to-hudi-with-minio.jpeg",tags:["blog","apache hudi","minio","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://dataxplorer.medium.com/an-intro-to-hudi-with-minio-i-75536fe75b4c",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},8367:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(80857),n=t(74848),s=t(28453),r=t(9230);const o={title:"From Transactional Bottlenecks to Lightning-Fast Analytics",author:"Akash Sankritya",category:"blog",image:"/assets/images/blog/lightning-fast-analytics.jpeg",tags:["blog","apache hudi","kafka","debezium","S3"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aakashsankritya.medium.com/from-transactional-bottlenecks-to-lightning-fast-analytics-74e0d3fff1c0",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},8375:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/10/14/streaming-dynamodb-data-into-a-hudi-table-aws-glue-in-action","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-10-14-streaming-dynamodb-data-into-a-hudi-table-aws-glue-in-action.mdx","source":"@site/blog/2024-10-14-streaming-dynamodb-data-into-a-hudi-table-aws-glue-in-action.mdx","title":"Streaming DynamoDB Data into a Hudi Table: AWS Glue in Action","description":"Redirecting... please wait!!","date":"2024-10-14T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"amazon s3","permalink":"/blog/tags/amazon-s-3"},{"inline":true,"label":"aws glue","permalink":"/blog/tags/aws-glue"},{"inline":true,"label":"amazon kinesis","permalink":"/blog/tags/amazon-kinesis"},{"inline":true,"label":"amazon dynamodb","permalink":"/blog/tags/amazon-dynamodb"},{"inline":true,"label":"antstack","permalink":"/blog/tags/antstack"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Rahul Kumar","key":null,"page":null}],"frontMatter":{"title":"Streaming DynamoDB Data into a Hudi Table: AWS Glue in Action","author":"Rahul Kumar","category":"blog","image":"/assets/images/blog/2024-10-14-streaming-dynamodb-data-into-a-hudi-table-aws-glue-in-action.png","tags":["how-to","Apache Hudi","amazon s3","aws glue","amazon kinesis","amazon dynamodb","antstack"]},"unlisted":false,"prevItem":{"title":"Exploring Time Travel Queries in Apache Hudi","permalink":"/blog/2024/10/22/exploring-time-travel-queries-in-apache-hudi"},"nextItem":{"title":"Iceberg vs. Delta Lake vs. Hudi: A Comparative Look at Lakehouse Architectures","permalink":"/blog/2024/10/07/iceberg-vs-delta-lake-vs-hudi-a-comparative-look-at-lakehouse-architectures"}}')},8662:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2020/01/20/change-capture-using-aws","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-01-20-change-capture-using-aws.md","source":"@site/blog/2020-01-20-change-capture-using-aws.md","title":"Change Capture Using AWS Database Migration Service and Hudi","description":"One of the core use-cases for Apache Hudi is enabling seamless, efficient database ingestion to your data lake. Even though a lot has been talked about and even users already adopting this model, content on how to go about this is sparse.","date":"2020-01-20T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"change data capture","permalink":"/blog/tags/change-data-capture"},{"inline":true,"label":"cdc","permalink":"/blog/tags/cdc"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":9.16,"hasTruncateMarker":true,"authors":[{"name":"vinoth","key":null,"page":null}],"frontMatter":{"title":"Change Capture Using AWS Database Migration Service and Hudi","excerpt":"In this blog, we will build an end-end solution for capturing changes from a MySQL instance running on AWS RDS to a Hudi table on S3, using capabilities in the Hudi 0.5.1 release.","author":"vinoth","category":"blog","image":"/assets/images/blog/change-capture-architecture.png","tags":["how-to","change data capture","cdc","apache hudi"]},"unlisted":false,"prevItem":{"title":"Export Hudi datasets as a copy or as different formats","permalink":"/blog/2020/03/22/exporting-hudi-datasets"},"nextItem":{"title":"Delete support in Hudi","permalink":"/blog/2020/01/15/delete-support-in-hudi"}}')},8751:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(73081),n=t(74848),s=t(28453),r=t(9230);const o={title:"Onehouse Commitment to Openness",authors:[{name:"Vinoth Chandar"}],category:"blog",image:"/assets/images/blog/2022-02-02-onehouse-commitment-to-openness.jpeg",tags:["blog","community","onehouse"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.onehouse.ai/blog/onehouse-commitment-to-openness",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},8792:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/11/25/apache-hudi-release-1-1-announcement","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-11-25-apache-hudi-release-1-1-announcement.md","source":"@site/blog/2025-11-25-apache-hudi-release-1-1-announcement.md","title":"Apache Hudi 1.1 is Here\u2014Building the Foundation for the Next Generation of Lakehouse","description":"The Hudi community is excited to announce the release of Hudi 1.1, a major milestone that sets the stage for the next generation of data lakehouse capabilities. This release represents months of focused engineering on foundational improvements, engine-specific optimizations, and key architectural enhancements, laying the foundation for ambitious features coming in future releases.","date":"2025-11-25T00:00:00.000Z","tags":[{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"release","permalink":"/blog/tags/release"},{"inline":true,"label":"feature","permalink":"/blog/tags/feature"},{"inline":true,"label":"performance","permalink":"/blog/tags/performance"}],"readingTime":14.44,"hasTruncateMarker":false,"authors":[{"name":"Shiyan Xu","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi 1.1 is Here\u2014Building the Foundation for the Next Generation of Lakehouse","excerpt":"","author":"Shiyan Xu","category":"blog","image":"/assets/images/blog/2025-11-25-apache-hudi-release-1-1-announcement/1-pluggable-TF.png","tags":["hudi","release","feature","performance"]},"unlisted":false,"prevItem":{"title":"Next Generation Lakehouse: New Engine for the Intelligent Future | Apache Hudi Meetup Asia Recap","permalink":"/blog/2025/12/01/apache-hudi-JD-meetup-asia-2025-recap"},"nextItem":{"title":"Deep Dive Into Hudi\'s Indexing Subsystem (Part 2 of 2)","permalink":"/blog/2025/11/12/deep-dive-into-hudis-indexing-subsystem-part-2-of-2"}}')},8903:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/files/bytedance-hudi-slides-english-878d79120cd12b838418e1815b12d8d6.pdf"},8969:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/06/18/how-to-use-apache-hudi-with-databricks","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-06-18-how-to-use-apache-hudi-with-databricks.mdx","source":"@site/blog/2024-06-18-how-to-use-apache-hudi-with-databricks.mdx","title":"How to use Apache Hudi with Databricks","description":"Redirecting... please wait!!","date":"2024-06-18T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"databricks","permalink":"/blog/tags/databricks"},{"inline":true,"label":"onehouse","permalink":"/blog/tags/onehouse"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Sagar Lakshmipathy","key":null,"page":null}],"frontMatter":{"title":"How to use Apache Hudi with Databricks","author":"Sagar Lakshmipathy","category":"blog","image":"/assets/images/blog/2024-06-18-how-to-use-apache-hudi-with-databricks.jpeg","tags":["blog","apache hudi","databricks","onehouse"]},"unlisted":false,"prevItem":{"title":"What is a Data Lakehouse & How does it Work?","permalink":"/blog/2024/07/11/what-is-a-data-lakehouse"},"nextItem":{"title":"Apache Hudi: A Deep Dive with Python Code Examples","permalink":"/blog/2024/06/07/apache-hudi-a-deep-dive-with-python-code-examples"}}')},9017:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/02/27/empowering-data-driven-excellence-how-the-bluestone-data-platform-embraced-data-mesh-for-success","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-02-27-empowering-data-driven-excellence-how-the-bluestone-data-platform-embraced-data-mesh-for-success.mdx","source":"@site/blog/2024-02-27-empowering-data-driven-excellence-how-the-bluestone-data-platform-embraced-data-mesh-for-success.mdx","title":"Empowering data-driven excellence: How the Bluestone Data Platform embraced data mesh for success","description":"Redirecting... please wait!!","date":"2024-02-27T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"data mesh","permalink":"/blog/tags/data-mesh"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.16,"hasTruncateMarker":false,"authors":[{"name":"Toney Thomas, Ben Vengerovsky and Rada Stanic","key":null,"page":null}],"frontMatter":{"title":"Empowering data-driven excellence: How the Bluestone Data Platform embraced data mesh for success","author":"Toney Thomas, Ben Vengerovsky and Rada Stanic","category":"blog","image":"/assets/images/blog/2024-02-27-empowering-data-driven-excellence-how-the-bluestone-data-platform-embraced-data-mesh-for-success.png","tags":["blog","apache hudi","use-case","data mesh","amazon"]},"unlisted":false,"prevItem":{"title":"Building Data Lakes on AWS with Kafka Connect, Debezium, Apicurio Registry, and Apache Hudi","permalink":"/blog/2024/02/27/Building-Data-Lakes-on-AWS-with-Kafka-Connect-Debezium-Apicurio-Registry-and-Apache-Hudi"},"nextItem":{"title":"Enabling near real-time data analytics on the data lake","permalink":"/blog/2024/02/23/Enabling-near-real-time-data-analytics-on-the-data-lake"}}')},9077:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/database-cdc-e9ee525e81a47e7744ae4f408c4e1d8f.png"},9230:(e,a,t)=>{"use strict";t.d(a,{A:()=>n});t(96540);var i=t(74848);function n({children:e,url:a}){return globalThis?.window?.location?.href&&(globalThis.window.location.href=a),(0,i.jsxs)("span",{children:[e,"or click ",(0,i.jsx)("a",{href:a,children:"here"})]})}},9296:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/s3-migration-task-1-61e22d0e163cf67bb9a9dd0879222177.png"},9318:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(49683),n=t(74848),s=t(28453),r=t(9230);const o={title:"Get started with Apache Hudi using AWS Glue by implementing key design concepts \u2013 Part 1",authors:[{name:"Amit Maindola"},{name:"Srinivas Kandi"},{name:"Mitesh Patel"}],category:"blog",image:"/assets/images/blog/2022-10-17-Get_started_with_apache_hudi_using_glue.jpeg",tags:["how-to","bulk-insert","amazon"]},l=void 0,d={authorsImageUrls:[void 0,void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/part-1-get-started-with-apache-hudi-using-aws-glue-by-implementing-key-design-concepts/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},9368:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2020/08/22/ingest-multiple-tables-using-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-08-22-ingest-multiple-tables-using-hudi.md","source":"@site/blog/2020-08-22-ingest-multiple-tables-using-hudi.md","title":"Ingest multiple tables using Hudi","description":"When building a change data capture pipeline for already existing or newly created relational databases, one of the most common problems that one faces is simplifying the onboarding process for multiple tables. Ingesting multiple tables to Hudi dataset at a single go is now possible using HoodieMultiTableDeltaStreamer class which is a wrapper on top of the more popular HoodieDeltaStreamer class. Currently HoodieMultiTableDeltaStreamer supports COPY_ON_WRITE storage type only and the ingestion is done in a sequential way.","date":"2020-08-22T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"multi deltastreamer","permalink":"/blog/tags/multi-deltastreamer"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":4.31,"hasTruncateMarker":true,"authors":[{"name":"pratyakshsharma","key":null,"page":null}],"frontMatter":{"title":"Ingest multiple tables using Hudi","excerpt":"Ingesting multiple tables using Hudi at a single go is now possible. This blog gives a detailed explanation of how to achieve the same using `HoodieMultiTableDeltaStreamer.java`","author":"pratyakshsharma","category":"blog","tags":["how-to","multi deltastreamer","apache hudi"]},"unlisted":false,"prevItem":{"title":"How nClouds Helps Accelerate Data Delivery with Apache Hudi on Amazon EMR","permalink":"/blog/2020/10/06/cdc-solution-using-hudi-by-nclouds"},"nextItem":{"title":"Async Compaction Deployment Models","permalink":"/blog/2020/08/21/async-compaction-deployment-model"}}')},9620:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/03/26/uptycs","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-03-26-uptycs.mdx","source":"@site/blog/2025-03-26-uptycs.mdx","title":"From Transactional Bottlenecks to Lightning-Fast Analytics","description":"Redirecting... please wait!!","date":"2025-03-26T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"CDC","permalink":"/blog/tags/cdc"},{"inline":true,"label":"Debezium","permalink":"/blog/tags/debezium"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Akash, Anudeep, Rohan","key":null,"page":null}],"frontMatter":{"title":"From Transactional Bottlenecks to Lightning-Fast Analytics","author":"Akash, Anudeep, Rohan","category":"blog","image":"/assets/images/blog/uptycs.png","tags":["blog","Apache Hudi","CDC","Debezium"]},"unlisted":false,"prevItem":{"title":"Data Deduplication Strategies in an Open Lakehouse Architecture","permalink":"/blog/2025/03/26/dedupe"},"nextItem":{"title":"Building an Amazon Sales Analytics Pipeline with Apache Hudi on Databricks","permalink":"/blog/2025/03/13/hudi-on-dbr"}}')},9757:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/06/03/text-based-search-from-elastic-search-to-vector-search","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-06-03-text-based-search-from-elastic-search-to-vector-search.mdx","source":"@site/blog/2023-06-03-text-based-search-from-elastic-search-to-vector-search.mdx","title":"Text-Based Search: From Elastic Search to Vector Search","description":"Redirecting... please wait!!","date":"2023-06-03T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"vector search","permalink":"/blog/tags/vector-search"},{"inline":true,"label":"indexing","permalink":"/blog/tags/indexing"},{"inline":true,"label":"bloom","permalink":"/blog/tags/bloom"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Kaushik Muniandi","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Text-Based Search: From Elastic Search to Vector Search","authors":[{"name":"Kaushik Muniandi"}],"category":"blog","image":"/assets/images/blog/2023-06-03-text-based-search-from-elastic-search-to-vector-search.png","tags":["blog","vector search","indexing","bloom","medium"]},"unlisted":false,"prevItem":{"title":"Cleaner and Archival in Apache Hudi","permalink":"/blog/2023/06/11/cleaner-and-archival-in-apache-hudi"},"nextItem":{"title":"Different Query types with Apache Hudi","permalink":"/blog/2023/05/29/different-query-types-with-apache-hudi"}}')},9933:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(65417),n=t(74848),s=t(28453),r=t(9230);const o={title:"The Art of Building Open Data Lakes with Apache Hudi, Kafka, Hive, and Debezium",authors:[{name:"Gary Stafford"}],category:"blog",image:"/assets/images/blog/2021-12-31-open-source-data-lakes-on-aws.png",tags:["how-to","datalake","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://garystafford.medium.com/the-art-of-building-open-data-lakes-with-apache-hudi-kafka-hive-and-debezium-3d2f71c5981f",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},10039:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image2-b8e66c22453fcee2810fc23d8cb480d0.png"},10234:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(59025),n=t(74848),s=t(28453);const r={title:"Understanding Data Lake Change Data Capture",excerpt:"Explains the concept of CDC in data lakes",author:"Sagar Lakshmipathy",category:"blog",image:"/assets/images/blog/data-lake-cdc/hudi-cdc.jpg",tags:["Data Lake","Apache Hudi","Change Data Capture","CDC"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Introduction",id:"introduction",level:2},{value:"Data Lake",id:"data-lake",level:3},{value:"Change Data Capture",id:"change-data-capture",level:3},{value:"CDC architecture pattern",id:"cdc-architecture-pattern",level:2},{value:"Common CDC Components",id:"common-cdc-components",level:3},{value:"Change Detection",id:"change-detection",level:4},{value:"Timestamp-based / Query-based:",id:"timestamp-based--query-based",level:5},{value:"Pros:",id:"pros",level:5},{value:"Cons:",id:"cons",level:5},{value:"Trigger-based:",id:"trigger-based",level:5},{value:"Pros:",id:"pros-1",level:5},{value:"Cons:",id:"cons-1",level:5},{value:"Log-based:",id:"log-based",level:5},{value:"Pros:",id:"pros-2",level:5},{value:"Cons:",id:"cons-2",level:5},{value:"Data Extraction",id:"data-extraction",level:4},{value:"Data Transformation",id:"data-transformation",level:4},{value:"Data Loading",id:"data-loading",level:4},{value:"Why Combine CDC with Data Lakes?",id:"why-combine-cdc-with-data-lakes",level:2},{value:"Flexibility",id:"flexibility",level:3},{value:"Cost-effective",id:"cost-effective",level:3},{value:"Streamlined ETL Processes",id:"streamlined-etl-processes",level:3},{value:"Designing a CDC Architecture",id:"designing-a-cdc-architecture",level:2},{value:"Solution:",id:"solution",level:3},{value:"Revised architecture:",id:"revised-architecture",level:4},{value:"Implementation Blogs/Guides",id:"implementation-blogsguides",level:2},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const a={a:"a",h2:"h2",h3:"h3",h4:"h4",h5:"h5",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.h2,{id:"introduction",children:"Introduction"}),"\n",(0,n.jsx)(a.p,{children:"In data management, two concepts have garnered significant attention: data lakes and change data capture (CDC)."}),"\n",(0,n.jsx)(a.h3,{id:"data-lake",children:"Data Lake"}),"\n",(0,n.jsx)(a.p,{children:"Data lakes serve as vast repositories that store raw data in its native format until needed for analytics."}),"\n",(0,n.jsx)(a.h3,{id:"change-data-capture",children:"Change Data Capture"}),"\n",(0,n.jsx)(a.p,{children:"Change Data Capture (CDC) is a technique used to identify and capture data changes, ensuring that the data remains fresh and consistent across various systems."}),"\n",(0,n.jsx)(a.p,{children:"Combining CDC with data lakes can significantly simplify data management by addressing several challenges commonly faced by ETL pipelines delivering data from transactional databases to analytical databases. These include maintaining data freshness, ensuring consistency, and improving efficiency in data handling. This article will explore the integration between data lakes and CDC, their benefits, implementation methods, key technologies and tools involved, best practices, and how to choose the right tools for your needs."}),"\n",(0,n.jsx)(a.h2,{id:"cdc-architecture-pattern",children:"CDC architecture pattern"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"CDC Architecture",src:t(9077).A+"",width:"632",height:"260"})}),"\n",(0,n.jsx)(a.h3,{id:"common-cdc-components",children:"Common CDC Components"}),"\n",(0,n.jsx)(a.h4,{id:"change-detection",children:"Change Detection"}),"\n",(0,n.jsx)(a.h5,{id:"timestamp-based--query-based",children:"Timestamp-based / Query-based:"}),"\n",(0,n.jsx)(a.p,{children:"This method relies on table schemas to include a column to indicate when it was previously modified, i.e. LAST_UPDATED etc. Whenever the source system is updated, the LAST_UPDATED column should be designed to get updated with the current timestamp. This column can then be queried by consumer applications to get the records, and process the records that have been previously updated."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Timestamp-based CDC",src:t(49887).A+"",width:"1121",height:"359"})}),"\n",(0,n.jsx)(a.h5,{id:"pros",children:"Pros:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Its simple to implement and use"}),"\n"]}),"\n",(0,n.jsx)(a.h5,{id:"cons",children:"Cons:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"If source applications did not have the timestamp columns, the database design needs to be changed to include it"}),"\n",(0,n.jsx)(a.li,{children:"Only supports soft deletes and not DELETE operations in the source table. This is because, once a DELETE operation is performed on the source database the record is removed and the consumer applications cannot track it automatically without the help of a custom log table or an audit trail."}),"\n",(0,n.jsx)(a.li,{children:"As there is no metadata to track, schema evolution scenarios require custom implementations to track the source database schema changes and update the target database schema appropriately. This is complex and hard to implement."}),"\n"]}),"\n",(0,n.jsx)(a.h5,{id:"trigger-based",children:"Trigger-based:"}),"\n",(0,n.jsxs)(a.p,{children:["In a trigger-based CDC design, database triggers are used to detect changes in the data and are used to update target tables accordingly. This method involves having trigger functions automatically executed to capture and store any changes from the source table in the target table; these target tables are commonly referred to as ",(0,n.jsx)(a.strong,{children:"shadow tables"})," or ",(0,n.jsx)(a.strong,{children:"change tables"}),". For example, in this method, stored procedures are triggered when there are specific events in the source database, such as INSERTs, UPDATEs, DELETEs."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Trigger-based CDC",src:t(85600).A+"",width:"880",height:"285"})}),"\n",(0,n.jsx)(a.h5,{id:"pros-1",children:"Pros:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Simple to implement"}),"\n",(0,n.jsx)(a.li,{children:"Triggers are supported natively by most database engines"}),"\n"]}),"\n",(0,n.jsx)(a.h5,{id:"cons-1",children:"Cons:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Maintenance overhead - requires maintaining separate trigger for each operation in each table"}),"\n",(0,n.jsx)(a.li,{children:"Performance overhead - in a highly concurrent database, addition of these triggers may significantly impact performance"}),"\n",(0,n.jsx)(a.li,{children:"Trigger-based CDC does not inherently provide mechanisms for informing downstream applications about schema changes, complicating consumer-side adaptations."}),"\n"]}),"\n",(0,n.jsx)(a.h5,{id:"log-based",children:"Log-based:"}),"\n",(0,n.jsx)(a.p,{children:"Databases maintain transaction logs, a file that records all transactions and database modifications made by each transaction. By reading this log, CDC tools can identify what data has been changed, when it changed and the type of change. Because this method reads changes directly from the database transaction log, ensuring low-latency and minimal impact on database performance."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Log-based CDC",src:t(50790).A+"",width:"1301",height:"320"})}),"\n",(0,n.jsx)(a.h5,{id:"pros-2",children:"Pros:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Supports all kinds of database transactions i.e. INSERTs, UPDATEs, DELETEs"}),"\n",(0,n.jsx)(a.li,{children:"Minimal performance impact on the source/operational databases"}),"\n",(0,n.jsx)(a.li,{children:"No schema changes required in source databases"}),"\n",(0,n.jsxs)(a.li,{children:["With a table format support, i.e. Apache Hudi, schema evolution ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/schema_evolution/",children:"can be supported"})]}),"\n"]}),"\n",(0,n.jsx)(a.h5,{id:"cons-2",children:"Cons:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"No standardization in publishing the transactional logs between databases - this results in complex design and development overhead to implement support for different database vendors"}),"\n"]}),"\n",(0,n.jsx)(a.h4,{id:"data-extraction",children:"Data Extraction"}),"\n",(0,n.jsx)(a.p,{children:"Once changes are detected, the CDC system extracts the relevant data. This includes the type of operation (insert, update, delete), the affected rows, and the before-and-after state of the data if applicable."}),"\n",(0,n.jsx)(a.h4,{id:"data-transformation",children:"Data Transformation"}),"\n",(0,n.jsx)(a.p,{children:"Extracted data often needs to be transformed before it can be used. This might include converting data formats, applying business rules, or enriching the data with additional context."}),"\n",(0,n.jsx)(a.h4,{id:"data-loading",children:"Data Loading"}),"\n",(0,n.jsx)(a.p,{children:"The transformed data is then loaded into the target system. This could be another database, a data warehouse, a data lake, or a real-time analytics platform. The loading process ensures that the target system reflects the latest state of the source database."}),"\n",(0,n.jsx)(a.h2,{id:"why-combine-cdc-with-data-lakes",children:"Why Combine CDC with Data Lakes?"}),"\n",(0,n.jsx)(a.h3,{id:"flexibility",children:"Flexibility"}),"\n",(0,n.jsx)(a.p,{children:"In general, data lakes offer more flexibility at a lower cost, because of its tendency to support storing any type of data i.e. unstructured, semi-structured and structured data while data warehouses typically only support structured and in some cases semi-structured. This flexibility allows users to maintain a single source of truth and access the same dataset from different query engines. For example, the dataset stored in S3, can be queried using Redshift Spectrum and Amazon Athena."}),"\n",(0,n.jsx)(a.h3,{id:"cost-effective",children:"Cost-effective"}),"\n",(0,n.jsx)(a.p,{children:"Data lakes, when compared to data warehouses, are generally cheaper in terms of storage costs as the volume grows in time. This allows users to implement a medallion architecture which involves storing a huge volume of data in three different levels i.e. bronze, silver and gold layer tables. Over time, data lake users typically implement tiered storage which further reduces storage cost by moving infrequently accessed data to colder storage systems. In a traditional data warehouse implementation, storage costs will be higher to maintain different levels of data and will continue growing as the source database grows."}),"\n",(0,n.jsx)(a.h3,{id:"streamlined-etl-processes",children:"Streamlined ETL Processes"}),"\n",(0,n.jsx)(a.p,{children:"CDC simplifies the Extract, Transform, Load (ETL) processes by continuously capturing and applying changes to the data lake. This streamlining reduces the complexity and resource intensity of traditional ETL operations, often involving bulk data transfers and significant processing overhead. By only dealing with data changes, CDC makes the process more efficient and reduces the load on source systems."}),"\n",(0,n.jsx)(a.p,{children:"For organizations using multiple ingestion pipelines, for example a combination of CDC pipelines, ERP data ingestion, IOT sensor data, having a common storage layer may simplify data processing while giving you the opportunity to build unified tables combining data from different sources."}),"\n",(0,n.jsx)(a.h2,{id:"designing-a-cdc-architecture",children:"Designing a CDC Architecture"}),"\n",(0,n.jsx)(a.p,{children:"For organizations with specific needs or unique data environments, developing custom CDC solutions is a common practice, especially with open source tools/frameworks. These solutions offer flexibility and can be tailored to meet the exact requirements of the business. However, developing custom CDC solutions requires significant expertise and resources, making it a viable option for organizations with complex data needs. Examples include Debezium/Airbyte combined Apache Kafka."}),"\n",(0,n.jsx)(a.h3,{id:"solution",children:"Solution:"}),"\n",(0,n.jsxs)(a.p,{children:["Apache Hudi is an open-source framework designed to streamline incremental data processing and data pipeline development. It efficiently handles business requirements such as data lifecycle management and enhances data quality.\nStarting with Hudi 0.13.0, ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-0.13.0#change-data-capture",children:"the CDC feature was introduced natively"}),", allowing logging before and after images of the changed records, along with the associated write operation type."]}),"\n",(0,n.jsx)(a.p,{children:"This enables users to"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:'Perform record-level insert, update, and delete for privacy regulations and simplified pipelines \u2013 for privacy regulations like GDPR and CCPA, companies need to perform record-level updates and deletions to comply with individuals\' rights such as the "right to be forgotten" or consent changes. Without support for record-level updates/deletes this required custom solutions to track individual changes and rewrite large data sets for minor updates. With Apache Hudi, you can use familiar operations (insert, update, upsert, delete), and Hudi will track transactions and make granular changes in the data lake, simplifying your data pipelines.'}),"\n",(0,n.jsx)(a.li,{children:"Simplified and efficient file management and near real-time data access \u2013 Streaming IoT and ingestion pipelines need to handle data insertion and update events without creating performance issues due to numerous small files. Hudi automatically tracks changes and merges files to maintain optimal sizes, eliminating the need for custom solutions to manage and rewrite small files."}),"\n",(0,n.jsx)(a.li,{children:"Simplify CDC data pipeline development \u2013 meaning users can store data in the data lake using open storage formats, while integrations with Presto, Apache Hive, Apache Spark, and various data catalogs give you near real-time access to updated data using familiar tools."}),"\n"]}),"\n",(0,n.jsx)(a.h4,{id:"revised-architecture",children:"Revised architecture:"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"CDC Architecture with Apache Hudi",src:t(41634).A+"",width:"1337",height:"601"})}),"\n",(0,n.jsx)(a.p,{children:"In this architecture, with the addition of the data processing layer, we have added two important components"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"A data catalog"})," \u2013 acting as a metadata repository for all your data assets across various data sources. This component is updated by the writer i.e. Spark/Flink and is used by the readers i.e. Presto/Trino. Common examples include AWS Glue Catalog, Hive Metastore and Unity Catalog."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"A schema registry"})," \u2013 acting centralized repository for managing and validating schemas. It decouples schemas from producers and consumers, which allows applications to serialize and deserialize messages. Schema registry is also important to ensure data quality. Common examples include, Confluent schema registry, Apicurio schema registry and Glue schema registry."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Apache Hudi"})," \u2013 acting as a platform used in conjunction with Spark/Flink which refers to the schema registry and writes to the data lake and simultaneously catalogs the data to the data catalog."]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"The tables written by Spark/Flink + Hudi can now be queried from popular query engines such as Presto, Trino, Amazon Redshift, and Spark SQL."}),"\n",(0,n.jsx)(a.h2,{id:"implementation-blogsguides",children:"Implementation Blogs/Guides"}),"\n",(0,n.jsxs)(a.p,{children:["Over time, the Apache Hudi community has written great step-by-step blogs/guides to help implement Change Data Capture architectures. Some of these blogs can be referred to ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/tags/cdc",children:"here"}),"."]}),"\n",(0,n.jsx)(a.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,n.jsx)(a.p,{children:"Combining data lakes with Change Data Capture (CDC) techniques offers a powerful solution for addressing the challenges associated with maintaining data freshness, consistency, and efficiency in ETL pipelines."}),"\n",(0,n.jsx)(a.p,{children:"Several methods exist for implementing CDC, including timestamp-based, trigger-based, and log-based approaches, each with its own advantages and drawbacks. Log-based CDC, in particular, stands out for its minimal performance impact on source databases and support for various transactions, though it requires handling different database vendors' transaction log formats."}),"\n",(0,n.jsx)(a.p,{children:"Using tools like Apache Hudi can significantly enhance the CDC process by streamlining incremental data processing and data pipeline development. Hudi provides efficient storage management, supports record-level operations for privacy regulations, and offers near real-time access to data. It also simplifies the management of streaming data and ingestion pipelines by automatically tracking changes and optimizing file sizes, thereby reducing the need for custom solutions."})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},10251:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(22115),n=t(74848),s=t(28453),r=t(9230);const o={title:"How PayU built a secure enterprise AI assistant using Amazon Bedrock",author:"Deepesh Dhapola, Mudit Chopra, Rahmat Khan, Rahul Ghosh, Saikat Dey, and Sandeep Kumar Veerlapati",category:"blog",tags:["blog","Apache Hudi","AWS"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/machine-learning/how-payu-built-a-secure-enterprise-ai-assistant-using-amazon-bedrock/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},10341:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/schema_evolution-b6cbf3c7c40814a0d8fcbd9f9176ea72.png"},10549:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/jdpost-image3-c843eba5797513a93582fb8e5682b52c.jpg"},10601:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/07/27/Apache-Hudi-Revolutionizing-Big-Data-Management-for-Real-Time-Analytics","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-07-27-Apache-Hudi-Revolutionizing-Big-Data-Management-for-Real-Time-Analytics.mdx","source":"@site/blog/2023-07-27-Apache-Hudi-Revolutionizing-Big-Data-Management-for-Real-Time-Analytics.mdx","title":"Apache Hudi: Revolutionizing Big Data Management for Real-Time Analytics","description":"Redirecting... please wait!!","date":"2023-07-27T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"},{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Dev Jain","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Apache Hudi: Revolutionizing Big Data Management for Real-Time Analytics","authors":[{"name":"Dev Jain"}],"category":"blog","image":"/assets/images/blog/2023-07-27-Apache-Hudi-Revolutionizing-Big-Data-Management-for-Real-Time-Analytics.png","tags":["blog","medium","hudi"]},"unlisted":false,"prevItem":{"title":"Data lake Table formats: Apache Iceberg vs Apache Hudi vs Delta lake","permalink":"/blog/2023/08/03/Data-lake-Table-formats-Apache-Iceberg-vs-Apache-Hudi-vs-Delta-lake"},"nextItem":{"title":"AWS Glue Crawlers now supports Apache Hudi Tables","permalink":"/blog/2023/07/21/AWS-Glue-Crawlers-now-supports-Apache-Hudi-Tables"}}')},10891:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(5647),n=t(74848),s=t(28453),r=t(9230);const o={title:"StarRocks query performance with Apache Hudi and Onehouse",excerpt:"StarRocks Query Performance with Apache Hudi",authors:[{name:"Albert Wong"}],category:"blog",image:"/assets/images/blog/2023-10-11-starrocks-query-performance-with-apache-hudi-and-onehouse.png",tags:["starrocks","medium","blog","query performance","apache hudi"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@atwong/starrocks-query-performance-with-apache-hudi-and-onehouse-04b859fff86c",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},11313:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(74059),n=t(74848),s=t(28453),r=t(9230);const o={title:"Hands-On Guide: Reading Data from Hudi Tables Incrementally, Joining with Delta Tables using HudiStreamer and SQL-Based Transformer",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2024-04-03-hands-on-guide-reading-data-from-hudi-tables-joining-delta.png",tags:["blog","apache hudi","deltastreamer","hudi streamer","delta","sql transformer","linkedin"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.linkedin.com/pulse/hands-on-guide-reading-data-from-hudi-tables-joining-delta-shah-vqivf/?trk=public_post_main-feed-card_feed-article-content",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},11466:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(70413),n=t(74848),s=t(28453),r=t(9230);const o={title:"Data Lakehouse Architecture for Big Data with Apache Hudi",authors:[{name:"Tauno Treier"}],category:"blog",image:"/assets/images/blog/2023-08-05-Data-Lakehouse-Architecture-for-Big-Data-with-Apache-Hudi.png",tags:["blog","apache hudi","data lakehouse","big data","google scholar"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://scholar.googleusercontent.com/scholar?q=cache:Cwi32O7nQCUJ:scholar.google.com/+apache+hudi&hl=en&as_sdt=0,7",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},11896:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/04/19/Corrections-in-data-lakehouse-table-format-comparisons","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-04-19-Corrections-in-data-lakehouse-table-format-comparisons.mdx","source":"@site/blog/2022-04-19-Corrections-in-data-lakehouse-table-format-comparisons.mdx","title":"Corrections in data lakehouse table format comparisons","description":"Redirecting... please wait!!","date":"2022-04-19T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"bytearray","permalink":"/blog/tags/bytearray"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Vinoth Chandar","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Corrections in data lakehouse table format comparisons","authors":[{"name":"Vinoth Chandar"}],"category":"blog","image":"/assets/images/blog/2022-04-19-corrections-in-data-lakehouse-table-format-comparisons.png","tags":["blog","lakehouse","bytearray"]},"unlisted":false,"prevItem":{"title":"Multi-Modal Index for the Lakehouse in Apache Hudi","permalink":"/blog/2022/05/17/Introducing-Multi-Modal-Index-for-the-Lakehouse-in-Apache-Hudi"},"nextItem":{"title":"Key Learnings on Using Apache HUDI in building Lakehouse Architecture @ Halodoc","permalink":"/blog/2022/04/04/Key-Learnings-on-Using-Apache-HUDI-in-building-Lakehouse-Architecture-at-Halodoc"}}')},11913:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/04/29/can-you-concurrently-write-data-to-apache-hudi-w-o-any-lock-provider","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-04-29-can-you-concurrently-write-data-to-apache-hudi-w-o-any-lock-provider.mdx","source":"@site/blog/2023-04-29-can-you-concurrently-write-data-to-apache-hudi-w-o-any-lock-provider.mdx","title":"Can you concurrently write data to Apache Hudi w/o any lock provider?","description":"Redirecting... please wait!!","date":"2023-04-29T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"concurrency","permalink":"/blog/tags/concurrency"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.15,"hasTruncateMarker":false,"authors":[{"name":"Sivabalan Narayanan","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Can you concurrently write data to Apache Hudi w/o any lock provider?","authors":[{"name":"Sivabalan Narayanan"}],"category":"blog","image":"/assets/images/blog/2023-04-29-can-you-concurrently-write-data-to-apache-hudi-w-o-any-lock-provider.gif","tags":["how-to","concurrency","medium"]},"unlisted":false,"prevItem":{"title":"An Introduction to the Hudi and Flink Integration","permalink":"/blog/2023/05/02/intro-to-hudi-and-flink"},"nextItem":{"title":"Delta, Hudi, and Iceberg: The Data Lakehouse Trifecta","permalink":"/blog/2023/04/26/the-lakehouse-trifecta"}}')},12022:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(83540),n=t(74848),s=t(28453);const r={title:"Efficient Migration of Large Parquet Tables to Apache Hudi",excerpt:"Migrating a large parquet table to Apache Hudi without having to rewrite the entire dataset.",author:"vbalaji",category:"blog",image:"/assets/images/blog/2020-08-20-skeleton.png",tags:["how-to","migration","bootstrap","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Motivation:",id:"motivation",level:2},{value:"High Level Idea:",id:"high-level-idea",level:2},{value:"Per Record Metadata:",id:"per-record-metadata",level:3},{value:"Design Deep Dive:",id:"design-deep-dive",level:2},{value:"Migration:",id:"migration",level:2},{value:"Query Engine Support:",id:"query-engine-support",level:3},{value:"Ways To Migrate :",id:"ways-to-migrate-",level:3},{value:"Configurations:",id:"configurations",level:3},{value:"Spark Data Source:",id:"spark-data-source",level:3},{value:"Hoodie DeltaStreamer:",id:"hoodie-deltastreamer",level:3},{value:"Known Caveats",id:"known-caveats",level:3}];function c(e){const a={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.p,{children:"We will look at how to migrate a large parquet table to Hudi without having to rewrite the entire dataset."}),"\n",(0,n.jsx)(a.h2,{id:"motivation",children:"Motivation:"}),"\n",(0,n.jsxs)(a.p,{children:["Apache Hudi maintains per record metadata to perform core operations such as upserts and incremental pull. To take advantage of Hudi\u2019s upsert and incremental processing support, users would need to rewrite their whole dataset to make it an Apache Hudi table.  Hudi 0.6.0 comes with an ",(0,n.jsx)(a.em,{children:(0,n.jsx)(a.strong,{children:"experimental feature"})})," to support efficient migration of large Parquet tables to Hudi without the need to rewrite the entire dataset."]}),"\n",(0,n.jsx)(a.h2,{id:"high-level-idea",children:"High Level Idea:"}),"\n",(0,n.jsx)(a.h3,{id:"per-record-metadata",children:"Per Record Metadata:"}),"\n",(0,n.jsx)(a.p,{children:"Apache Hudi maintains record level metadata for perform efficient upserts and incremental pull."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Per Record Metadata",src:t(22007).A+"",width:"1061",height:"300"})}),"\n",(0,n.jsx)(a.p,{children:"Apache HUDI physical file contains 3 parts"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsx)(a.li,{children:"For each record, 5 HUDI metadata fields with column indices 0 to 4"}),"\n",(0,n.jsx)(a.li,{children:"For each record, the original data columns that comprises the record (Original Data)"}),"\n",(0,n.jsx)(a.li,{children:"Additional Hudi Metadata at file footer for index lookup"}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"The parts (1) and (3) constitute what we term as  \u201cHudi skeleton\u201d. Hudi skeleton contains additional metadata that it maintains in each physical parquet file for supporting Hudi primitives. The conceptual idea is to decouple Hudi skeleton data from original data (2). Hudi skeleton can be stored in a Hudi file while the original data is stored in an external non-Hudi file. A migration of large parquet would result in creating only Hudi skeleton files without having to rewrite original data."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"skeleton",src:t(82759).A+"",width:"513",height:"600"})}),"\n",(0,n.jsx)(a.h2,{id:"design-deep-dive",children:"Design Deep Dive:"}),"\n",(0,n.jsxs)(a.p,{children:["For a deep dive on the internals, please take a look at the ",(0,n.jsx)(a.a,{href:"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+12+%3A+Efficient+Migration+of+Large+Parquet+Tables+to+Apache+Hudi",children:"RFC document"})]}),"\n",(0,n.jsx)(a.h2,{id:"migration",children:"Migration:"}),"\n",(0,n.jsx)(a.p,{children:"Hudi supports 2 modes when migrating parquet tables.  We will use the term bootstrap and migration interchangeably in this document."}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"METADATA_ONLY : In this mode, record level metadata alone is generated for each source record and stored in new bootstrap location."}),"\n",(0,n.jsx)(a.li,{children:"FULL_RECORD : In this mode, record level metadata is generated for each source record and both original record and metadata for each record copied"}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:'You can pick and choose these modes at partition level. One of the common strategy would be to use FULL_RECORD mode for a small set of "hot" partitions which are accessed more frequently and METADATA_ONLY for a larger set of "warm" partitions.'}),"\n",(0,n.jsx)(a.h3,{id:"query-engine-support",children:"Query Engine Support:"}),"\n",(0,n.jsx)(a.p,{children:"For a METADATA_ONLY bootstrapped table, Spark - data source, Spark-Hive and native Hive query engines are supported. Presto support is in the works."}),"\n",(0,n.jsx)(a.h3,{id:"ways-to-migrate-",children:"Ways To Migrate :"}),"\n",(0,n.jsx)(a.p,{children:"There are 2 ways to migrate a large parquet table to Hudi."}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Spark Datasource Write"}),"\n",(0,n.jsx)(a.li,{children:"Hudi DeltaStreamer"}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"We will look at how to migrate using both these approaches."}),"\n",(0,n.jsx)(a.h3,{id:"configurations",children:"Configurations:"}),"\n",(0,n.jsx)(a.p,{children:"These are bootstrap specific configurations that needs to be set in addition to regular hudi write configurations."}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{children:"Configuration Name"}),(0,n.jsx)(a.th,{children:"Default"}),(0,n.jsx)(a.th,{children:"Mandatory ?"}),(0,n.jsx)(a.th,{children:"Description"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"hoodie.bootstrap.base.path"}),(0,n.jsx)(a.td,{}),(0,n.jsx)(a.td,{children:"Yes"}),(0,n.jsx)(a.td,{children:"Base Path of  source parquet table."})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"hoodie.bootstrap.parallelism"}),(0,n.jsx)(a.td,{children:"1500"}),(0,n.jsx)(a.td,{children:"Yes"}),(0,n.jsx)(a.td,{children:"Spark Parallelism used when running bootstrap"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"hoodie.bootstrap.keygen.class"}),(0,n.jsx)(a.td,{}),(0,n.jsx)(a.td,{children:"Yes"}),(0,n.jsx)(a.td,{children:"Bootstrap Index internally used by Hudi to map Hudi skeleton and source parquet files."})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"hoodie.bootstrap.mode.selector"}),(0,n.jsx)(a.td,{children:"org.apache.hudi.client.bootstrap.selector.MetadataOnlyBootstrapModeSelector"}),(0,n.jsx)(a.td,{children:"Yes"}),(0,n.jsx)(a.td,{children:"Bootstap Mode Selector class. By default, Hudi employs METADATA_ONLY boostrap for all partitions."})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"hoodie.bootstrap.partitionpath.translator.class"}),(0,n.jsx)(a.td,{children:"org.apache.hudi.client.bootstrap.translator. IdentityBootstrapPartitionPathTranslator"}),(0,n.jsx)(a.td,{children:"No"}),(0,n.jsx)(a.td,{children:"For METADATA_ONLY bootstrap, this class allows customization of partition paths used in Hudi target dataset. By default, no customization is done and the partition paths reflects what is available in source parquet table."})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"hoodie.bootstrap.full.input.provider"}),(0,n.jsx)(a.td,{children:"org.apache.hudi.bootstrap.SparkParquetBootstrapDataProvider"}),(0,n.jsx)(a.td,{children:"No"}),(0,n.jsx)(a.td,{children:"For FULL_RECORD bootstrap, this class provides the input RDD of Hudi records to write."})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"hoodie.bootstrap.mode.selector.regex.mode"}),(0,n.jsx)(a.td,{children:"METADATA_ONLY"}),(0,n.jsx)(a.td,{children:"No"}),(0,n.jsx)(a.td,{children:"Bootstrap Mode used when the partition matches the regex pattern in hoodie.bootstrap.mode.selector.regex . Used only when hoodie.bootstrap.mode.selector set to BootstrapRegexModeSelector."})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"hoodie.bootstrap.mode.selector.regex"}),(0,n.jsx)(a.td,{children:".*"}),(0,n.jsx)(a.td,{children:"No"}),(0,n.jsx)(a.td,{children:"Partition Regex used when  hoodie.bootstrap.mode.selector set to BootstrapRegexModeSelector."})]})]})]}),"\n",(0,n.jsx)(a.h3,{id:"spark-data-source",children:"Spark Data Source:"}),"\n",(0,n.jsx)(a.p,{children:"Here, we use a Spark Datasource Write to perform bootstrap.\nHere is an example code snippet to perform METADATA_ONLY bootstrap."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-properties",children:'import org.apache.hudi.{DataSourceWriteOptions, HoodieDataSourceHelpers}\nimport org.apache.hudi.config.{HoodieBootstrapConfig, HoodieWriteConfig}\nimport org.apache.hudi.keygen.SimpleKeyGenerator\nimport org.apache.spark.sql.SaveMode\n \nval bootstrapDF = spark.emptyDataFrame\nbootstrapDF.write\n      .format("hudi")\n      .option(HoodieWriteConfig.TABLE_NAME, "hoodie_test")\n      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.BOOTSTRAP_OPERATION_OPT_VAL)\n      .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY, "_row_key")\n      .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY, "datestr")\n      .option(HoodieBootstrapConfig.BOOTSTRAP_BASE_PATH_PROP, srcPath)\n      .option(HoodieBootstrapConfig.BOOTSTRAP_KEYGEN_CLASS, classOf[SimpleKeyGenerator].getName)\n      .mode(SaveMode.Overwrite)\n      .save(basePath)\n'})}),"\n",(0,n.jsx)(a.p,{children:"Here is an example code snippet to perform METADATA_ONLY bootstrap for August 20 2020 - August 29 2020 partitions and FULL_RECORD bootstrap for other partitions."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-properties",children:'import org.apache.hudi.bootstrap.SparkParquetBootstrapDataProvider\nimport org.apache.hudi.client.bootstrap.selector.BootstrapRegexModeSelector\nimport org.apache.hudi.{DataSourceWriteOptions, HoodieDataSourceHelpers}\nimport org.apache.hudi.config.{HoodieBootstrapConfig, HoodieWriteConfig}\nimport org.apache.hudi.keygen.SimpleKeyGenerator\nimport org.apache.spark.sql.SaveMode\n \nval bootstrapDF = spark.emptyDataFrame\nbootstrapDF.write\n      .format("hudi")\n      .option(HoodieWriteConfig.TABLE_NAME, "hoodie_test")\n      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.BOOTSTRAP_OPERATION_OPT_VAL)\n      .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY, "_row_key")\n      .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY, "datestr")\n      .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY, "timestamp")\n      .option(HoodieBootstrapConfig.BOOTSTRAP_BASE_PATH_PROP, srcPath)\n      .option(HoodieBootstrapConfig.BOOTSTRAP_KEYGEN_CLASS, classOf[SimpleKeyGenerator].getName)\n      .option(HoodieBootstrapConfig.BOOTSTRAP_MODE_SELECTOR, classOf[BootstrapRegexModeSelector].getName)\n      .option(HoodieBootstrapConfig.BOOTSTRAP_MODE_SELECTOR_REGEX, "2020/08/2[0-9]")\n      .option(HoodieBootstrapConfig.BOOTSTRAP_MODE_SELECTOR_REGEX_MODE, "METADATA_ONLY")\n      .option(HoodieBootstrapConfig.FULL_BOOTSTRAP_INPUT_PROVIDER, classOf[SparkParquetBootstrapDataProvider].getName)\n      .mode(SaveMode.Overwrite)\n      .save(basePath)\n'})}),"\n",(0,n.jsx)(a.h3,{id:"hoodie-deltastreamer",children:"Hoodie DeltaStreamer:"}),"\n",(0,n.jsx)(a.p,{children:"Hoodie Deltastreamer allows bootstrap to be performed using --run-bootstrap command line option."}),"\n",(0,n.jsx)(a.p,{children:"If you are planning to use delta-streamer after the initial boostrap to incrementally ingest data to the new hudi dataset, you need to pass either --checkpoint or --initial-checkpoint-provider to set the initial checkpoint for the deltastreamer."}),"\n",(0,n.jsx)(a.p,{children:"Here is an example for running METADATA_ONLY bootstrap using Delta Streamer."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-properties",children:"spark-submit --package org.apache.hudi:hudi-spark-bundle_2.11:0.6.0\n--conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \\\n--class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer  \\\n--run-bootstrap \\\n--target-base-path <Hudi_Base_Path> \\\n--target-table <Hudi_Table_Name> \\\n--props <props_file> \\\n--checkpoint <initial_checkpoint_if_you_are_going_to_use_deltastreamer_to_incrementally_ingest> \\\n--hoodie-conf hoodie.bootstrap.base.path=<Parquet_Source_base_Path> \\\n--hoodie-conf hoodie.datasource.write.recordkey.field=_row_key \\\n--hoodie-conf hoodie.datasource.write.partitionpath.field=datestr \\\n--hoodie-conf hoodie.bootstrap.keygen.class=org.apache.hudi.keygen.SimpleKeyGenerator\n"})}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-properties",children:"spark-submit --package org.apache.hudi:hudi-spark-bundle_2.11:0.6.0\n--conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \\\n--class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer  \\\n--run-bootstrap \\\n--target-base-path <Hudi_Base_Path> \\\n--target-table <Hudi_Table_Name> \\\n--props <props_file> \\\n--checkpoint <initial_checkpoint_if_you_are_going_to_use_deltastreamer_to_incrementally_ingest> \\\n--hoodie-conf hoodie.bootstrap.base.path=<Parquet_Source_base_Path> \\\n--hoodie-conf hoodie.datasource.write.recordkey.field=_row_key \\\n--hoodie-conf hoodie.datasource.write.partitionpath.field=datestr \\\n--hoodie-conf hoodie.bootstrap.keygen.class=org.apache.hudi.keygen.SimpleKeyGenerator \\\n--hoodie-conf hoodie.bootstrap.full.input.provider=org.apache.hudi.bootstrap.SparkParquetBootstrapDataProvider \\\n--hoodie-conf hoodie.bootstrap.mode.selector=org.apache.hudi.client.bootstrap.selector.BootstrapRegexModeSelector \\\n--hoodie-conf hoodie.bootstrap.mode.selector.regex=\"2020/08/2[0-9]\" \\\n--hoodie-conf hoodie.bootstrap.mode.selector.regex.mode=METADATA_ONLY\n"})}),"\n",(0,n.jsx)(a.h3,{id:"known-caveats",children:"Known Caveats"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsxs)(a.li,{children:["Need proper defaults for the bootstrap config : hoodie.bootstrap.full.input.provider. Here is the ",(0,n.jsx)(a.a,{href:"https://issues.apache.org/jira/browse/HUDI-1213",children:"ticket"})]}),"\n",(0,n.jsxs)(a.li,{children:["DeltaStreamer manages checkpoints inside hoodie commit files and expects checkpoints in previously committed metadata. Users are expected to pass checkpoint or initial checkpoint provider when performing bootstrap through deltastreamer. Such support is not present when doing bootstrap using Spark Datasource. Here is the ",(0,n.jsx)(a.a,{href:"https://issues.apache.org/jira/browse/HUDI-1214",children:"ticket"}),"."]}),"\n"]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},12102:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/12/16/lakehouse-concurrency-control-are-we-too-optimistic","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-12-16-lakehouse-concurrency-control-are-we-too-optimistic.mdx","source":"@site/blog/2021-12-16-lakehouse-concurrency-control-are-we-too-optimistic.mdx","title":"Lakehouse Concurrency Control: Are we too optimistic?","description":"Transactions on data lakes are now considered a key characteristic of a Lakehouse these days. But what has actually been accomplished so far? What are the current approaches? How do they fare in real-world scenarios? These questions are the focus of this blog.","date":"2021-12-16T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"concurrency-control","permalink":"/blog/tags/concurrency-control"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":8.6,"hasTruncateMarker":true,"authors":[{"name":"vinoth","key":null,"page":null}],"frontMatter":{"title":"Lakehouse Concurrency Control: Are we too optimistic?","excerpt":"Vinoth Chandar, original creator of Apache Hudi, dives into concurrency control mechanisms","author":"vinoth","category":"blog","image":"/assets/images/blog/concurrency/MultiWriter.gif","tags":["blog","concurrency-control","apache hudi"]},"unlisted":false,"prevItem":{"title":"New features from Apache Hudi 0.7.0 and 0.8.0 available on Amazon EMR","permalink":"/blog/2021/12/20/New-features-from-Apache-Hudi-0.7.0-and-0.8.0-available-on-Amazon-EMR"},"nextItem":{"title":"Apache Hudi Architecture Tools and Best Practices","permalink":"/blog/2021/11/22/Apache-Hudi-Architecture-Tools-and-Best-Practices"}}')},12147:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(31321),n=t(74848),s=t(28453),r=t(9230);const o={title:"Onehouse brings a fully-managed lakehouse to Apache Hudi",authors:[{name:"Paul Sawers"}],category:"blog",image:"/assets/images/blog/2022-02-03-onehouse_billboard.png",tags:["blog","lakehouse","venturebeat"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://venturebeat.com/2022/02/03/onehouse-brings-a-fully-managed-lakehouse-to-apache-hudi/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},12231:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(25673),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi on AWS Glue",author:"Sagar Lakshmipathy",category:"blog",image:"/assets/images/blog/2024-05-19-apache-hudi-on-aws-glue.png",tags:["blog","apache hudi","aws glue","dev to"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://dev.to/sagarlakshmipathy/apache-hudi-on-aws-glue-450l",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},12274:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig-4-Sample-Architecture-of-PuppyGraph-Hudi-adcd64f4ffaf1a8c9b6e13f7d9d07e4a.png"},12557:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/09/01/building-eb-level-data-lake-using-hudi-at-bytedance","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-09-01-building-eb-level-data-lake-using-hudi-at-bytedance.md","source":"@site/blog/2021-09-01-building-eb-level-data-lake-using-hudi-at-bytedance.md","title":"Building an ExaByte-level Data Lake Using Apache Hudi at ByteDance","description":"Ziyue Guan from Bytedance shares the experience of building an ExaByte(EB)-level data lake using Apache Hudi at Bytedance.","date":"2021-09-01T00:00:00.000Z","tags":[{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":9.48,"hasTruncateMarker":true,"authors":[{"name":"Ziyue Guan, translated to English by yihua","key":null,"page":null}],"frontMatter":{"title":"Building an ExaByte-level Data Lake Using Apache Hudi at ByteDance","excerpt":"Ziyue Guan from Bytedance shares the production experience of building an ExaByte-level data lake using Apache Hudi and how it is used in the recommendation system at Bytedance.","author":"Ziyue Guan, translated to English by yihua","category":"blog","image":"/assets/images/blog/bytedance_hudi.png","tags":["use-case","apache hudi"]},"unlisted":false,"prevItem":{"title":"Data Platform 2.0 - Part I","permalink":"/blog/2021/10/05/Data-Platform-2.0-Part-I"},"nextItem":{"title":"Asynchronous Clustering using Hudi","permalink":"/blog/2021/08/23/async-clustering"}}')},12610:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/10/26/moving-large-tables-from-snowflake-to-s3-using-the-copy-into-command-and-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-10-26-moving-large-tables-from-snowflake-to-s3-using-the-copy-into-command-and-hudi.mdx","source":"@site/blog/2024-10-26-moving-large-tables-from-snowflake-to-s3-using-the-copy-into-command-and-hudi.mdx","title":"Moving Large Tables from Snowflake to S3 Using the COPY INTO Command and Hudi Bootstrapping to Build Data Lakes | Hands-On Labs","description":"Redirecting... please wait!!","date":"2024-10-26T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"aws s3","permalink":"/blog/tags/aws-s-3"},{"inline":true,"label":"bootstrap","permalink":"/blog/tags/bootstrap"},{"inline":true,"label":"linkedin","permalink":"/blog/tags/linkedin"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"Soumil Shah","key":null,"page":null}],"frontMatter":{"title":"Moving Large Tables from Snowflake to S3 Using the COPY INTO Command and Hudi Bootstrapping to Build Data Lakes | Hands-On Labs","author":"Soumil Shah","category":"blog","image":"/assets/images/blog/2024-10-26-moving-large-tables-from-snowflake-to-s3-using-the-copy-into-command-and-hudi.png","tags":["blog","Apache Hudi","aws s3","bootstrap","linkedin"]},"unlisted":false,"prevItem":{"title":"I spent 5 hours exploring the story behind Apache Hudi.","permalink":"/blog/2024/10/27/I-spent-5-hours-exploring-the-story-behind-Apache-Hudi"},"nextItem":{"title":"Using Apache Hudi with Apache Flink","permalink":"/blog/2024/10/23/Using-Apache-Hudi-with-Apache-Flink"}}')},12881:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(31818),n=t(74848),s=t(28453),r=t(9230);const o={title:"MLOps Wars: Versioned Feature Data with a Lakehouse",authors:[{name:"David Bzhalava"},{name:"Jim Dowling"}],category:"blog",image:"/assets/images/blog/2021-08-03-mlops-wars.png",tags:["use-case","mlops","feature store","incremental processing","time travel query","logicalclocks"]},l=void 0,d={authorsImageUrls:[void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.logicalclocks.com/blog/mlops-wars-versioned-feature-data-with-a-lakehouse",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},12888:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(54800),n=t(74848),s=t(28453),r=t(9230);const o={title:"Incremental Queries with Apache Hudi and Apache Flink",excerpt:"Incremental Queries with Apache Hudi and Apache Flink",author:"nello",category:"blog",image:"/assets/images/blog/2023-08-31-Incremental-Queries-with-Apache-Hudi-and-Apache-Flink.png",tags:["incremental query","blog","apache flink","apache hudi","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@acmilanellosw/incremental-queries-with-apache-hudi-and-apache-flink-5a90d088327",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},12915:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(25521),n=t(74848),s=t(28453),r=t(9230);const o={title:"Hudi Best Practices: Handling Failed Inserts/Upserts with Error Tables",authors:[{name:"Soumil Shah"}],category:"blog",image:"/assets/images/blog/2023-07-02-Hudi-Best-Practices-Handling-Failed-Inserts-Upserts-with-Error-Tables.png",tags:["blog","linkedin","apache hudi","inserts","upserts"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.linkedin.com/pulse/hudi-best-practices-handling-failed-insertsupserts-error-soumil-shah/?utm_source=share&utm_medium=member_ios&utm_campaign=share_via",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},13062:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(70755),n=t(74848),s=t(28453),r=t(9230);const o={title:"Curious Engineering Facts (Lakehouse | Apache Hudi | Daft |Positional argument|) : March Release 19 : 25",author:"Gayan Sanjeewa",category:"blog",image:"/assets/images/blog/2025-02-23-curious-engineering-facts-lakehouse-apache-hudi-daft-positional-argument.jpeg",tags:["blog","apache hudi","daft","streamlit","cow","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@kkgsanjeewac77/curious-engineering-facts-lakehouse-apache-hudi-daft-positional-argument-march-release-d0fee8151736",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},13135:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/03/23/options-on-kafka-sink-to-open-table-formats-apache-iceberg-and-apache-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-03-23-options-on-kafka-sink-to-open-table-formats-apache-iceberg-and-apache-hudi.mdx","source":"@site/blog/2024-03-23-options-on-kafka-sink-to-open-table-formats-apache-iceberg-and-apache-hudi.mdx","title":"Options on Kafka sink to open table Formats: Apache Iceberg and Apache Hudi","description":"Redirecting... please wait!!","date":"2024-03-23T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"apache iceberg","permalink":"/blog/tags/apache-iceberg"},{"inline":true,"label":"apache Kafka","permalink":"/blog/tags/apache-kafka"},{"inline":true,"label":"kafka connect","permalink":"/blog/tags/kafka-connect"},{"inline":true,"label":"starrocks","permalink":"/blog/tags/starrocks"},{"inline":true,"label":"devgenius","permalink":"/blog/tags/devgenius"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Albert Wong","key":null,"page":null}],"frontMatter":{"title":"Options on Kafka sink to open table Formats: Apache Iceberg and Apache Hudi","author":"Albert Wong","category":"blog","image":"/assets/images/blog/2024-03-23-options-on-kafka-sink-to-open-table-formats-apache-iceberg-and-apache-hudi.png","tags":["blog","apache hudi","apache iceberg","apache Kafka","kafka connect","starrocks","devgenius"]},"unlisted":false,"prevItem":{"title":"Record Level Indexing in Apache Hudi Delivers 70% Faster Point Lookups","permalink":"/blog/2024/03/30/record-level-indexing-apache-hudi-delivers-70-faster-point"},"nextItem":{"title":"Cost Optimization Strategies for scalable Data Lakehouse","permalink":"/blog/2024/03/22/data-lake-cost-optimisation-strategies"}}')},13202:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/MultiWriter-fec6bf4269df78d4fa91e7a353144def.gif"},13288:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/s3_events_source_design-897267ee0a269c5e796ebb92faa8c149.png"},13490:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide16-358d556fb2c770f517d56ced6a880a66.png"},13662:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/10/22/Tipico-Facilitates-Faster-Data-Access-with-a-Modern-Data-Strategy-on-AWS","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-10-22-Tipico-Facilitates-Faster-Data-Access-with-a-Modern-Data-Strategy-on-AWS.mdx","source":"@site/blog/2023-10-22-Tipico-Facilitates-Faster-Data-Access-with-a-Modern-Data-Strategy-on-AWS.mdx","title":"Tipico Facilitates Faster Data Access with a Modern Data Strategy on AWS","description":"Redirecting... please wait!! s","date":"2023-10-22T00:00:00.000Z","tags":[{"inline":true,"label":"case study","permalink":"/blog/tags/case-study"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":0.11,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Tipico Facilitates Faster Data Access with a Modern Data Strategy on AWS","category":"blog","image":"/assets/images/blog/2023-10-22-Tipico-Facilitates-Faster-Data-Access-with-a-Modern-Data-Strategy-on-AWS.png","tags":["case study","amazon","apache hudi"]},"unlisted":false,"prevItem":{"title":"UPSERT Performance Evaluation of Hudi 0.14 and Spark 3.4.1: Record Level Index vs. Global Bloom & Global Simple Indexes","permalink":"/blog/2023/10/29/UPSERT-Performance-Evaluation-of-Hudi-0-14-and-Spark-3-4-1-Record-Level-Index-Global-Bloom-Global-Simple-Indexes"},"nextItem":{"title":"It\'s Time for the Universal Data Lakehouse","permalink":"/blog/2023/10/20/Its-Time-for-the-Universal-Data-Lakehouse"}}')},13680:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(21063),n=t(74848),s=t(28453),r=t(9230);const o={title:"Use Flink Hudi to Build a Streaming Data Lake Platform",authors:[{name:"Chen Yuzhao"},{name:"Liu Dalong"}],category:"blog",image:"/assets/images/blog/2022-08-12-Use-Flink-Hudi-to-Build-a-Streaming-Data-Lake-Platform.png",tags:["blog","apache flink","alibabacloud","streaming ingestion"]},l=void 0,d={authorsImageUrls:[void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.alibabacloud.com/blog/use-flink-hudi-to-build-a-streaming-data-lake-platform_599240",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},13738:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/build-your-first-hudi-lakehouse-12-19-diagram-7bec1745b0437f71e86e4ab659bee730.jpg"},14023:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(34628),n=t(74848),s=t(28453);const r={title:"Employing the right indexes for fast updates, deletes in Apache Hudi",excerpt:"Detailing different indexing mechanisms in Hudi and when to use each of them",author:"vinoth",category:"blog",image:"/assets/images/blog/hudi-indexes/with-and-without-index.png",tags:["how-to","indexing","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Index Types in Hudi",id:"index-types-in-hudi",level:2},{value:"Workload: Late arriving updates to fact tables",id:"workload-late-arriving-updates-to-fact-tables",level:2},{value:"Workload: De-Duplication in event tables",id:"workload-de-duplication-in-event-tables",level:2},{value:"Workload: Random updates/deletes to a dimension table",id:"workload-random-updatesdeletes-to-a-dimension-table",level:2},{value:"Summary",id:"summary",level:2}];function c(e){const a={a:"a",code:"code",em:"em",h2:"h2",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(a.p,{children:["Apache Hudi employs an index to locate the file group, that an update/delete belongs to. For Copy-On-Write tables, this enables\nfast upsert/delete operations, by avoiding the need to join against the entire dataset to determine which files to rewrite.\nFor Merge-On-Read tables, this design allows Hudi to bound the amount of records any given base file needs to be merged against.\nSpecifically, a given base file needs to merged only against updates for records that are part of that base file. In contrast,\ndesigns without an indexing component (e.g: ",(0,n.jsx)(a.a,{href:"https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions",children:"Apache Hive ACID"}),"),\ncould end up having to merge all the base files against all incoming updates/delete records."]}),"\n",(0,n.jsxs)(a.p,{children:["At a high level, an index maps a record key + an optional partition path to a file group ID on storage (explained\nmore in detail ",(0,n.jsx)(a.a,{href:"/docs/concepts",children:"here"}),") and during write operations, we lookup this mapping to route an incoming update/delete\nto a log file attached to the base file (MOR) or to the latest base file that now needs to be merged against (COW). The index also enables\nHudi to enforce unique constraints based on the record keys."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{alt:"Fact table",src:t(16097).A+"",width:"1200",height:"600"}),"\n",(0,n.jsx)(a.em,{children:"Figure: Comparison of merge cost for updates (yellow blocks) against base files (white blocks)"})]}),"\n",(0,n.jsx)(a.p,{children:"Given that Hudi already supports few different indexing techniques and is also continuously improving/adding more to its toolkit, the rest of the blog\nattempts to explain different categories of workloads, from our experience and suggests what index types to use for each. We will also interlace\ncommentary on existing limitations, upcoming work and optimizations/tradeoffs along the way."}),"\n",(0,n.jsx)(a.h2,{id:"index-types-in-hudi",children:"Index Types in Hudi"}),"\n",(0,n.jsx)(a.p,{children:"Currently, Hudi supports the following indexing options."}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Bloom Index (default):"})," Employs bloom filters built out of the record keys, optionally also pruning candidate files using record key ranges."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Simple Index:"})," Performs a lean join of the incoming update/delete records against keys extracted from the table on storage."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"HBase Index:"})," Manages the index mapping in an external Apache HBase table."]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["Writers can pick one of these options using ",(0,n.jsx)(a.code,{children:"hoodie.index.type"})," config option. Additionally, a custom index implementation can also be employed\nusing ",(0,n.jsx)(a.code,{children:"hoodie.index.class"})," and supplying a subclass of ",(0,n.jsx)(a.code,{children:"SparkHoodieIndex"})," (for Apache Spark writers)"]}),"\n",(0,n.jsxs)(a.p,{children:["Another key aspect worth understanding is the difference between global and non-global indexes. Both bloom and simple index have\nglobal options - ",(0,n.jsx)(a.code,{children:"hoodie.index.type=GLOBAL_BLOOM"})," and ",(0,n.jsx)(a.code,{children:"hoodie.index.type=GLOBAL_SIMPLE"})," - respectively. HBase index is by nature a global index."]}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Global index:"}),"  Global indexes enforce uniqueness of keys across all partitions of a table i.e guarantees that exactly\none record exists in the table for a given record key. Global indexes offer stronger guarantees, but the update/delete cost grows\nwith size of the table ",(0,n.jsx)(a.code,{children:"O(size of table)"}),", which might still be acceptable for smaller tables."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Non Global index:"})," On the other hand, the default index implementations enforce this constraint only within a specific partition.\nAs one might imagine, non global indexes depends on the writer to provide the same consistent partition path for a given record key during update/delete,\nbut can deliver much better performance since the index lookup operation becomes ",(0,n.jsx)(a.code,{children:"O(number of records updated/deleted)"})," and\nscales well with write volume."]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Since data comes in at different volumes, velocity and has different access patterns, different indices could be used for different workloads.\nNext, let\u2019s walk through some typical workloads and see how to leverage the right Hudi index for such use-cases."}),"\n",(0,n.jsx)(a.h2,{id:"workload-late-arriving-updates-to-fact-tables",children:"Workload: Late arriving updates to fact tables"}),"\n",(0,n.jsx)(a.p,{children:"Many companies store large volumes of transactional data in NoSQL data stores. For eg, trip tables in case of ride-sharing, buying and selling of shares,\norders in an e-commerce site. These tables are usually ever growing with random updates on most recent data with long tail updates going to older data, either\ndue to transactions settling at a later date/data corrections. In other words, most updates go into the latest partitions with few updates going to older ones."}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{alt:"Fact table",src:t(37629).A+"",width:"1280",height:"720"}),"\n",(0,n.jsx)(a.em,{children:"Figure: Typical update pattern for Fact tables"})]}),"\n",(0,n.jsxs)(a.p,{children:["For such workloads, the ",(0,n.jsx)(a.code,{children:"BLOOM"})," index performs well, since index look-up will prune a lot of data files based on a well-sized bloom filter.\nAdditionally, if the keys can be constructed such that they have a certain ordering, the number of files to be compared is further reduced by range pruning.\nHudi constructs an interval tree with all the file key ranges and efficiently filters out the files that don't match any key ranges in the updates/deleted records."]}),"\n",(0,n.jsxs)(a.p,{children:["In order to efficiently compare incoming record keys against bloom filters i.e with minimal number of bloom filter reads and uniform distribution of work across\nthe executors, Hudi leverages caching of input records and employs a custom partitioner that can iron out data skews using statistics. At times, if the bloom filter\nfalse positive ratio is high, it could increase the amount of data shuffled to perform the lookup. Hudi supports dynamic bloom filters\n(enabled using ",(0,n.jsx)(a.code,{children:"hoodie.bloom.index.filter.type=DYNAMIC_V0"}),"), which adjusts its size based on the number of records stored in a given file to deliver the\nconfigured false positive ratio."]}),"\n",(0,n.jsxs)(a.p,{children:["In the near future, we plan to introduce a much speedier version of the BLOOM index that tracks bloom filters/ranges in an internal Hudi metadata table, indexed for fast\npoint lookups. This would avoid any current limitations around reading bloom filters/ranges from the base files themselves, to perform the lookup. (see\n",(0,n.jsx)(a.a,{href:"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+15%3A+HUDI+File+Listing+and+Query+Planning+Improvements?src=contextnavpagetreemode",children:"RFC-15"})," for the general design)"]}),"\n",(0,n.jsx)(a.h2,{id:"workload-de-duplication-in-event-tables",children:"Workload: De-Duplication in event tables"}),"\n",(0,n.jsx)(a.p,{children:'Event Streaming is everywhere. Events coming from Apache Kafka or similar message bus are typically 10-100x the size of fact tables and often treat "time" (event\'s arrival time/processing\ntime) as a first class citizen. For eg, IoT event stream, click stream data, ad impressions etc. Inserts and updates only span the last few partitions as these are mostly append only data.\nGiven duplicate events can be introduced anywhere in the end-end pipeline, de-duplication before storing on the data lake is a common requirement.'}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{alt:"Event table",src:t(97047).A+"",width:"1280",height:"720"}),"\n",(0,n.jsx)(a.em,{children:"Figure showing the spread of updates for Event table."})]}),"\n",(0,n.jsxs)(a.p,{children:["In general, this is a very challenging problem to solve at lower cost. Although, we could even employ a key value store to perform this de-duplication ala HBASE index, the index storage\ncosts would grow linear with number of events and thus can be prohibitively expensive. In fact, ",(0,n.jsx)(a.code,{children:"BLOOM"})," index with range pruning is the optimal solution here. One can leverage the fact\nthat time is often a first class citizen and construct a key such as ",(0,n.jsx)(a.code,{children:"event_ts + event_id"})," such that the inserted records have monotonically increasing keys. This yields great returns\nby pruning large amounts of files even within the latest table partitions."]}),"\n",(0,n.jsx)(a.h2,{id:"workload-random-updatesdeletes-to-a-dimension-table",children:"Workload: Random updates/deletes to a dimension table"}),"\n",(0,n.jsx)(a.p,{children:"These types of tables usually contain high dimensional data and hold reference data e.g user profile, merchant information. These are high fidelity tables where the updates are often small but also spread\nacross a lot of partitions and data files ranging across the dataset from old to new. Often times, these tables are also un-partitioned, since there is also not a good way to partition these tables."}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{alt:"Dimensions table",src:t(59331).A+"",width:"1280",height:"720"}),"\n",(0,n.jsx)(a.em,{children:"Figure showing the spread of updates for Dimensions table."})]}),"\n",(0,n.jsxs)(a.p,{children:["As discussed before, the ",(0,n.jsx)(a.code,{children:"BLOOM"})," index may not yield benefits if a good number of files cannot be pruned out by comparing ranges/filters. In such a random write workload, updates end up touching\nmost files within in the table and thus bloom filters will typically indicate a true positive for all files based on some incoming update. Consequently, we would end up comparing ranges/filter, only\nto finally check the incoming updates against all files. The ",(0,n.jsx)(a.code,{children:"SIMPLE"})," Index will be a better fit as it does not do any upfront pruning based, but directly joins with interested fields from every data file.\n",(0,n.jsx)(a.code,{children:"HBASE"})," index can be employed, if the operational overhead is acceptable and would provide much better lookup times for these tables."]}),"\n",(0,n.jsxs)(a.p,{children:["When using a global index, users should also consider setting ",(0,n.jsx)(a.code,{children:"hoodie.bloom.index.update.partition.path=true"})," or ",(0,n.jsx)(a.code,{children:"hoodie.simple.index.update.partition.path=true"})," to deal with cases where the\npartition path value could change due to an update e.g users table partitioned by home city; user relocates to a different city. These tables are also excellent candidates for the Merge-On-Read table type."]}),"\n",(0,n.jsxs)(a.p,{children:["Going forward, we plan to build ",(0,n.jsx)(a.a,{href:"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+08+%3A+Record+level+indexing+mechanisms+for+Hudi+datasets?src=contextnavpagetreemode",children:"record level indexing"}),"\nright within Hudi, which will improve the index look-up time and will also avoid additional overhead of maintaining an external system like hbase."]}),"\n",(0,n.jsx)(a.h2,{id:"summary",children:"Summary"}),"\n",(0,n.jsxs)(a.p,{children:["Without the indexing capabilities in Hudi, it would not been possible to make upserts/deletes happen at ",(0,n.jsx)(a.a,{href:"https://eng.uber.com/apache-hudi-graduation/",children:"very large scales"}),".\nHopefully this post gave you good enough context on the indexing mechanisms today and how different tradeoffs play out."]}),"\n",(0,n.jsx)(a.p,{children:"Some interesting work underway in this area:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Apache Flink based writing with a RocksDB state store backed indexing mechanism, unlocking true streaming upserts on data lakes."}),"\n",(0,n.jsx)(a.li,{children:"A brand new MetadataIndex, which reimagines the bloom index today on top of the metadata table in Hudi."}),"\n",(0,n.jsx)(a.li,{children:"Record level index implementation, as a secondary index using another Hudi table."}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["Going forward, this will remain an area of active investment for the project. we are always looking for contributors who can drive these roadmap items forward.\nPlease ",(0,n.jsx)(a.a,{href:"/community/get-involved",children:"engage"})," with our community if you want to get involved."]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},14054:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/01/20/Data-Engineering-Bootstrapping-Data-lake-with-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-01-20-Data-Engineering-Bootstrapping-Data-lake-with-Apache-Hudi.mdx","source":"@site/blog/2024-01-20-Data-Engineering-Bootstrapping-Data-lake-with-Apache-Hudi.mdx","title":"Data Engineering: Bootstrapping Data lake with Apache Hudi","description":"Redirecting... please wait!!","date":"2024-01-20T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"},{"inline":true,"label":"beginner","permalink":"/blog/tags/beginner"},{"inline":true,"label":"ETL","permalink":"/blog/tags/etl"},{"inline":true,"label":"aws glue","permalink":"/blog/tags/aws-glue"},{"inline":true,"label":"apache spark","permalink":"/blog/tags/apache-spark"},{"inline":true,"label":"aws s3","permalink":"/blog/tags/aws-s-3"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Krishna Prasad","key":null,"page":null}],"frontMatter":{"title":"Data Engineering: Bootstrapping Data lake with Apache Hudi","excerpt":"Data Engineering: Bootstrapping Data lake with Apache Hudi","author":"Krishna Prasad","category":"blog","image":"/assets/images/blog/2024-01-20-Data-Engineering-Bootstrapping-Data-lake-with-Apache-Hudi.png","tags":["blog","apache hudi","medium","beginner","ETL","aws glue","apache spark","aws s3"]},"unlisted":false,"prevItem":{"title":"Use Amazon Athena with Spark SQL for your open-source transactional table formats","permalink":"/blog/2024/01/24/Use-Amazon-Athena-with-Spark-SQL-for-your-open-source-transactional-table-formats"},"nextItem":{"title":"Learn How to Move Data From MongoDB to Apache Hudi Using PySpark","permalink":"/blog/2024/01/20/Learn-How-to-Move-Data-From-MongoDB-to-Apache-Hudi-Using-PySpark"}}')},14127:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/Retain_latest_commits-e387b7c19e4ee4d9cbef7b0bb0466983.png"},14234:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(90167),n=t(74848),s=t(28453),r=t(9230);const o={title:"Building a RAG-based AI Recommender (Part 1/2)",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2025-07-10-building-a-rag-based-ai-recommender.png",tags:["blog","Apache Hudi","AI","RAG","Artificial Intelligence","data lakehouse","Lakehouse","use-case","datumagic"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blog.datumagic.ai/p/building-a-rag-based-ai-recommender",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},14358:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/KafkaAvroSchemaDeserializer-7077d39b24f01312dbefecdc9cfb937a.png"},14520:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(69800),n=t(74848),s=t(28453),r=t(9230);const o={title:"Build a federated query solution with Apache Doris, Apache Flink, and Apache Hudi",excerpt:"Build a federated query solution with Apache Doris, Apache Flink, and Apache Hudi",author:"Apache Doris",category:"blog",image:"/assets/images/blog/2024-01-02-Build-a-federated-query-solution-with-Apache-Doris-Apache-Flink-and-Apache-Hudi.png",tags:["blog","apache hudi","dev to","beginner","apache doris","apache flink"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://dev.to/apachedoris/build-a-federated-query-solution-with-apache-doris-apache-flink-and-apache-hudi-40io",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},14522:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(89746),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi: From Zero To One (9/10)",excerpt:"Hudi Streamer - a Swiss Army knife for ingestion",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2024-03-05-Apache-Hudi-From-Zero-To-One-blog-9.png",tags:["hudi","spark","blog","course","tutorial","datumagic","data lake","lakehouse","apache hudi","apache spark"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blog.datumagic.ai/p/apache-hudi-from-zero-to-one-910",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},14546:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/02/23/Enabling-near-real-time-data-analytics-on-the-data-lake","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-02-23-Enabling-near-real-time-data-analytics-on-the-data-lake.mdx","source":"@site/blog/2024-02-23-Enabling-near-real-time-data-analytics-on-the-data-lake.mdx","title":"Enabling near real-time data analytics on the data lake","description":"Redirecting... please wait!!","date":"2024-02-23T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"near real-time analytics","permalink":"/blog/tags/near-real-time-analytics"},{"inline":true,"label":"mor","permalink":"/blog/tags/mor"},{"inline":true,"label":"grab","permalink":"/blog/tags/grab"}],"readingTime":0.1,"hasTruncateMarker":false,"authors":[{"name":"Shi Kai Ng and Shuguang Xiang","key":null,"page":null}],"frontMatter":{"title":"Enabling near real-time data analytics on the data lake","author":"Shi Kai Ng and Shuguang Xiang","category":"blog","image":"/assets/images/blog/2024-02-23-Enabling-near-real-time-data-analytics-on-the-data-lake.jpg","tags":["blog","apache hudi","near real-time analytics","mor","grab"]},"unlisted":false,"prevItem":{"title":"Empowering data-driven excellence: How the Bluestone Data Platform embraced data mesh for success","permalink":"/blog/2024/02/27/empowering-data-driven-excellence-how-the-bluestone-data-platform-embraced-data-mesh-for-success"},"nextItem":{"title":"How a POC became a production-ready Hudi data lakehouse through close team collaboration","permalink":"/blog/2024/02/12/How-a-POC-became-a-production-ready-Hudi-data-lakehouse-through-close-team-collaboration"}}')},14661:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(77969),n=t(74848),s=t(28453),r=t(9230);const o={title:"Different Query types with Apache Hudi",authors:[{name:"Sivabalan Narayanan"}],category:"blog",tags:["blog","snapshot query","real-time query","time travel query","timestamp as of query","read optimized query","incremental query","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@simpsons/different-query-types-with-apache-hudi-e14c2064cfd6",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},14792:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(65662),n=t(74848),s=t(28453),r=t(9230);const o={title:"The Architect\u2019s Guide to Open Table Formats and Object Storage",author:"Brenna Buuck",category:"blog",image:"/assets/images/blog/2025-01-08-the-future-of-data-lakehouses-a-fireside.jpg",tags:["blog","apache hudi","apache iceberg","delta lake","data lakehouse","lakehouse","table formats","thenewstack"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://thenewstack.io/the-architects-guide-to-open-table-formats-and-object-storage/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},14912:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/12/28/how-lakehouse-handles-concurrent-read-and-writes","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-12-28-how-lakehouse-handles-concurrent-read-and-writes.mdx","source":"@site/blog/2024-12-28-how-lakehouse-handles-concurrent-read-and-writes.mdx","title":"How lakehouse handles concurrent Read and Writes","description":"Redirecting... please wait!!","date":"2024-12-28T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"concurrency","permalink":"/blog/tags/concurrency"},{"inline":true,"label":"concurrency-control","permalink":"/blog/tags/concurrency-control"},{"inline":true,"label":"non-blocking concurrency-control","permalink":"/blog/tags/non-blocking-concurrency-control"},{"inline":true,"label":"nbcc","permalink":"/blog/tags/nbcc"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Sanjeet Shukla","key":null,"page":null}],"frontMatter":{"title":"How lakehouse handles concurrent Read and Writes","author":"Sanjeet Shukla","category":"blog","image":"/assets/images/blog/2024-12-28-how-lakehouse-handles-concurrent-read-and-writes.jpeg","tags":["blog","apache hudi","concurrency","concurrency-control","non-blocking concurrency-control","nbcc","medium"]},"unlisted":false,"prevItem":{"title":"Apache Hudi 2024: A Year In Review","permalink":"/blog/2024/12/29/apache-hudi-2024-a-year-in-review"},"nextItem":{"title":"Announcing Apache Hudi 1.0 and the Next Generation of Data Lakehouses","permalink":"/blog/2024/12/16/announcing-hudi-1-0-0"}}')},14949:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(1579),n=t(74848),s=t(28453),r=t(9230);const o={title:"Data Lakehouse: Building the Next Generation of Data Lakes using Apache Hudi",authors:[{name:"Ryan D'Souza"},{name:"Brandon Stanley"}],category:"blog",image:"/assets/images/blog/2021-03-01-Data-Lakehouse-Building-the-Next-Generation-of-Data-Lakes-using-Apache-Hudi.png",tags:["blog","data-lakehouse","medium"]},l=void 0,d={authorsImageUrls:[void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/slalom-build/data-lakehouse-building-the-next-generation-of-data-lakes-using-apache-hudi-41550f62f5f",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},15188:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/09/10/Demystifying-Copy-on-Write-in-Apache-Hudi-Understanding-Read-and-Write-Operations","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-09-10-Demystifying-Copy-on-Write-in-Apache-Hudi-Understanding-Read-and-Write-Operations.mdx","source":"@site/blog/2023-09-10-Demystifying-Copy-on-Write-in-Apache-Hudi-Understanding-Read-and-Write-Operations.mdx","title":"Demystifying Copy-on-Write in Apache Hudi: Understanding Read and Write Operations","description":"Redirecting... please wait!!","date":"2023-09-10T00:00:00.000Z","tags":[{"inline":true,"label":"reads","permalink":"/blog/tags/reads"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"writes","permalink":"/blog/tags/writes"},{"inline":true,"label":"cow","permalink":"/blog/tags/cow"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Eswaramoorthy P","key":null,"page":null}],"frontMatter":{"title":"Demystifying Copy-on-Write in Apache Hudi: Understanding Read and Write Operations","excerpt":"COW Overview","author":"Eswaramoorthy P","category":"blog","image":"/assets/images/blog/2023-09-10-Demystifying-Copy-on-Write-in-Apache-Hudi-Understanding-Read-and-Write-Operations.png","tags":["reads","medium","blog","apache hudi","writes","cow"]},"unlisted":false,"prevItem":{"title":"Lakehouse or Warehouse? Part 2 of 2","permalink":"/blog/2023/09/12/Lakehouse-or-Warehouse-Part-2-of-2"},"nextItem":{"title":"Apache Hudi: From Zero To One (2/10)","permalink":"/blog/2023/09/06/Apache-Hudi-From-Zero-To-One-blog-2"}}')},15205:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(14546),n=t(74848),s=t(28453),r=t(9230);const o={title:"Enabling near real-time data analytics on the data lake",author:"Shi Kai Ng and Shuguang Xiang",category:"blog",image:"/assets/images/blog/2024-02-23-Enabling-near-real-time-data-analytics-on-the-data-lake.jpg",tags:["blog","apache hudi","near real-time analytics","mor","grab"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://engineering.grab.com/enabling-near-realtime-data-analytics",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},15569:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi-design-diagrams_-_Page_5-5ca4af1d2d91e19dc4e9e9e5138bb2b7.png"},15624:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(65833),n=t(74848),s=t(28453),r=t(9230);const o={title:"Lakehouse at Fortune 1 Scale",authors:[{name:"Samuel Guleff"}],category:"blog",image:"/assets/images/blog/2023-05-03-lakehouse-at-fortune-1-scale.jpeg",tags:["use-case","comparison","performance","walmartglobaltech"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/walmartglobaltech/lakehouse-at-fortune-1-scale-480bcb10391b",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},15799:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/03/03/record-mergers-in-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-03-03-record-mergers-in-hudi.mdx","source":"@site/blog/2025-03-03-record-mergers-in-hudi.mdx","title":"Record Mergers in Apache Hudi","description":"The Challenge of Unordered Streams of Events","date":"2025-03-03T00:00:00.000Z","tags":[{"inline":true,"label":"Data Lake","permalink":"/blog/tags/data-lake"},{"inline":true,"label":"Data Lakehouse","permalink":"/blog/tags/data-lakehouse"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"Record Mergers","permalink":"/blog/tags/record-mergers"},{"inline":true,"label":"Record payloads","permalink":"/blog/tags/record-payloads"},{"inline":true,"label":"Late Arriving Data","permalink":"/blog/tags/late-arriving-data"}],"readingTime":10.4,"hasTruncateMarker":false,"authors":[{"name":"Aditya Goenka","key":null,"page":null}],"frontMatter":{"title":"Record Mergers in Apache Hudi","excerpt":"Explain need for record mergers in Apache Hudi and implemenation details","author":"Aditya Goenka","category":"blog","image":"/assets/images/blog/2025-03-03-record-mergers-in-apache-hudi.png","tags":["Data Lake","Data Lakehouse","Apache Hudi","Record Mergers","Record payloads","Late Arriving Data"]},"unlisted":false,"prevItem":{"title":"21 Unique Reasons Why Apache Hudi Should Be Your Next Data Lakehouse","permalink":"/blog/2025/03/05/hudi-21-unique-differentiators"},"nextItem":{"title":"Curious Engineering Facts ( Trace Agents | Hudi| Daft : 1) : March Release 18 : 25","permalink":"/blog/2025/02/25/curious-engineering-facts-trace-agents-hudi-daft-1"}}')},15924:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/07/07/Skip-rocks-and-files-Turbocharge-Trino-queries-with-Hudi-multi-modal-indexing-subsystem","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-07-07-Skip-rocks-and-files-Turbocharge-Trino-queries-with-Hudi-multi-modal-indexing-subsystem.mdx","source":"@site/blog/2023-07-07-Skip-rocks-and-files-Turbocharge-Trino-queries-with-Hudi-multi-modal-indexing-subsystem.mdx","title":"Skip rocks and files: Turbocharge Trino queries with Hudi\u2019s multi-modal indexing subsystem","description":"Redirecting... please wait!!","date":"2023-07-07T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"conference","permalink":"/blog/tags/conference"},{"inline":true,"label":"trino","permalink":"/blog/tags/trino"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"multi modal indexing","permalink":"/blog/tags/multi-modal-indexing"},{"inline":true,"label":"queries","permalink":"/blog/tags/queries"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Nadine Farah","socials":{},"key":null,"page":null},{"name":"Sagar Sumit","socials":{},"key":null,"page":null},{"name":"Cole Bowden","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Skip rocks and files: Turbocharge Trino queries with Hudi\u2019s multi-modal indexing subsystem","authors":[{"name":"Nadine Farah"},{"name":"Sagar Sumit"},{"name":"Cole Bowden"}],"category":"blog","image":"/assets/images/blog/2023-07-07-Skip-rocks-and-files-Turbocharge-Trino-queries-with-Hudi-multi-modal-indexing-subsystem.png","tags":["blog","conference","trino","apache hudi","multi modal indexing","queries"]},"unlisted":false,"prevItem":{"title":"Quickly start using Apache Hudi on AWS EMR","permalink":"/blog/2023/07/08/Quickly-start-using-Apache-Hudi-on-AWS-EMR"},"nextItem":{"title":"Hudi Best Practices: Handling Failed Inserts/Upserts with Error Tables","permalink":"/blog/2023/07/02/Hudi-Best-Practices-Handling-Failed-Inserts-Upserts-with-Error-Tables"}}')},16097:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/with-and-without-index-81d481917e61e4cd1be2426c12994b8b.png"},16187:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(53640),n=t(74848),s=t(28453),r=t(9230);const o={title:"Small Talk about Apache Hudi",excerpt:"Small Talk about Apache Hudi",author:"Ashok Kumar Kunkala",category:"blog",image:"/assets/images/blog/2024-01-05-Small-Talk-about-Apache-Hudi.png",tags:["blog","apache hudi","linkedin","beginner","inserts","upserts","cow","mor"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.linkedin.com/pulse/small-talk-apache-hudi-ashok-kumar-kunkala-3ldge/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},16562:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/03/16/Open-Table-Formats-part-1-Apache-Hudi-Hadoop-Upserts-Deletes-and-Incrementals","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-03-16-Open-Table-Formats-part-1-Apache-Hudi-Hadoop-Upserts-Deletes-and-Incrementals.mdx","source":"@site/blog/2024-03-16-Open-Table-Formats-part-1-Apache-Hudi-Hadoop-Upserts-Deletes-and-Incrementals.mdx","title":"Open Table Formats (part-1): Apache Hudi (Hadoop Upserts Deletes and Incrementals)","description":"Redirecting... please wait!!","date":"2024-03-16T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"beginner","permalink":"/blog/tags/beginner"},{"inline":true,"label":"defogdata","permalink":"/blog/tags/defogdata"}],"readingTime":0.15,"hasTruncateMarker":false,"authors":[{"name":"Vivek L Alex","key":null,"page":null}],"frontMatter":{"title":"Open Table Formats (part-1): Apache Hudi (Hadoop Upserts Deletes and Incrementals)","author":"Vivek L Alex","category":"blog","image":"/assets/images/blog/2024-03-16-Open-Table-Formats-part-1-Apache-Hudi-Hadoop-Upserts-Deletes-and-Incrementals.jpg","tags":["blog","apache hudi","beginner","defogdata"]},"unlisted":false,"prevItem":{"title":"Cost Optimization Strategies for scalable Data Lakehouse","permalink":"/blog/2024/03/22/data-lake-cost-optimisation-strategies"},"nextItem":{"title":"Modern Datalakes with Hudi, MinIO, and HMS","permalink":"/blog/2024/03/14/Modern-Datalakes-with-Hudi--MinIO--and-HMS"}}')},16580:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(4171),n=t(74848),s=t(28453),r=t(9230);const o={title:"Mastering Slowly Changing Dimensions with Apache Hudi & Spark SQL",author:"Sameer Shaik",category:"blog",image:"/assets/images/blog/2024-10-07-mastering-slowly-changing-dimensions-with-apache-hudi-and-spark-sql.png",tags:["blog","Apache Hudi","scd1","scd2","scd3","spark-sql","linkedin"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.linkedin.com/pulse/mastering-slowly-changing-dimensions-apache-hudi-spark-sameer-shaik-7zkjf/?trackingId=1qCeO8FIRJy32LcpHIvy3Q%3D%3D",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},16738:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig-11-PuppyGraph-Query-4-66848df36086049fd3ac42b78a5de47c.png"},16837:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(68820),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi vs. Delta Lake: Choosing the Right Tool for Your Data Lake on AWS",author:"Siladitya Ghosh",category:"blog",image:"/assets/images/blog/2024-05-27-apache-hudi-vs-delta-lake-choosing-the-right-tool-for-your-data-lake-on-aws.png",tags:["blog","apache hudi","delta lake","comparison","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@siladityaghosh/apache-hudi-vs-delta-lake-choosing-the-right-tool-for-your-data-lake-on-aws-8b97c66a5a12",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},16865:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image2-971db03016b54c2da63fec8a2df4f412.png"},17022:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/05/17/Introducing-Multi-Modal-Index-for-the-Lakehouse-in-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-05-17-Introducing-Multi-Modal-Index-for-the-Lakehouse-in-Apache-Hudi.mdx","source":"@site/blog/2022-05-17-Introducing-Multi-Modal-Index-for-the-Lakehouse-in-Apache-Hudi.mdx","title":"Multi-Modal Index for the Lakehouse in Apache Hudi","description":"Redirecting... please wait!!","date":"2022-05-17T00:00:00.000Z","tags":[{"inline":true,"label":"design","permalink":"/blog/tags/design"},{"inline":true,"label":"multi modal indexing","permalink":"/blog/tags/multi-modal-indexing"},{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"onehouse","permalink":"/blog/tags/onehouse"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Sivabalan Narayanan","socials":{},"key":null,"page":null},{"name":"Ethan Guo","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Multi-Modal Index for the Lakehouse in Apache Hudi","authors":[{"name":"Sivabalan Narayanan"},{"name":"Ethan Guo"}],"category":"blog","image":"/assets/images/blog/2022-05-17-multimodal-index.gif","tags":["design","multi modal indexing","lakehouse","onehouse"]},"unlisted":false,"prevItem":{"title":"The story of building a data lake that can be deleted on a record-by-record basis using Apache Hudi","permalink":"/blog/2022/05/25/Record-by-record-deletable-data-lake-using-Apache-Hudi"},"nextItem":{"title":"Corrections in data lakehouse table format comparisons","permalink":"/blog/2022/04/19/Corrections-in-data-lakehouse-table-format-comparisons"}}')},17087:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(55826),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi vs Delta Lake vs Apache Iceberg - Lakehouse Feature Comparison",authors:[{name:"Kyle Weller"}],category:"blog",image:"/assets/images/blog/2022-08-18-apache_hudi_vs_delta_lake_vs_apache_iceberg_feature_comparison.png",tags:["lakehouse","datalake","comparison","onehouse"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.onehouse.ai/blog/apache-hudi-vs-delta-lake-vs-apache-iceberg-lakehouse-feature-comparison",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},17149:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(3695),n=t(74848),s=t(28453),r=t(9230);const o={title:"Getting Started: Incrementally process data with Apache Hudi",authors:[{name:"Raymond Xu"}],category:"blog",image:"/assets/images/blog/2023-04-18-getting-started-incrementally-process-data-with-apache-hudi.png",tags:["how-to","incremental processing","onehouse"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.onehouse.ai/blog/getting-started-incrementally-process-data-with-apache-hudi",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},17165:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(75171),n=t(74848),s=t(28453),r=t(9230);const o={title:"Timeline Server in Apache Hudi",authors:[{name:"Sivabalan Narayanan"}],category:"blog",image:"/assets/images/blog/2023-06-20-timeline-server-in-apache-hudi.png",tags:["blog","timeline Server","FileSystemView","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@simpsons/timeline-server-in-apache-hudi-b5be25f85e47",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},17198:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(9368),n=t(74848),s=t(28453);const r={title:"Ingest multiple tables using Hudi",excerpt:"Ingesting multiple tables using Hudi at a single go is now possible. This blog gives a detailed explanation of how to achieve the same using `HoodieMultiTableDeltaStreamer.java`",author:"pratyakshsharma",category:"blog",tags:["how-to","multi deltastreamer","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Configuration",id:"configuration",level:3},{value:"Configuring schema providers",id:"configuring-schema-providers",level:3},{value:"Run Command",id:"run-command",level:3},{value:"Example",id:"example",level:3}];function c(e){const a={a:"a",code:"code",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(a.p,{children:["When building a change data capture pipeline for already existing or newly created relational databases, one of the most common problems that one faces is simplifying the onboarding process for multiple tables. Ingesting multiple tables to Hudi dataset at a single go is now possible using ",(0,n.jsx)(a.code,{children:"HoodieMultiTableDeltaStreamer"})," class which is a wrapper on top of the more popular ",(0,n.jsx)(a.code,{children:"HoodieDeltaStreamer"})," class. Currently ",(0,n.jsx)(a.code,{children:"HoodieMultiTableDeltaStreamer"})," supports ",(0,n.jsx)(a.strong,{children:"COPY_ON_WRITE"})," storage type only and the ingestion is done in a ",(0,n.jsx)(a.strong,{children:"sequential"})," way."]}),"\n",(0,n.jsxs)(a.p,{children:["This blog will guide you through configuring and running ",(0,n.jsx)(a.code,{children:"HoodieMultiTableDeltaStreamer"}),"."]}),"\n",(0,n.jsx)(a.h3,{id:"configuration",children:"Configuration"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.code,{children:"HoodieMultiTableDeltaStreamer"})," expects users to maintain table wise overridden properties in separate files in a dedicated config folder. Common properties can be configured via common properties file also."]}),"\n",(0,n.jsxs)(a.li,{children:["By default, hudi datasets are created under the path ",(0,n.jsx)(a.code,{children:"<base-path-prefix>/<database_name>/<name_of_table_to_be_ingested>"}),". You need to provide the names of tables to be ingested via the property ",(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.ingestion.tablesToBeIngested"})," in the format ",(0,n.jsx)(a.code,{children:"<database>.<table>"}),", for example"]}),"\n"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"hoodie.deltastreamer.ingestion.tablesToBeIngested=db1.table1,db2.table2\n"})}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["If you do not provide database name, then it is assumed the table belongs to default database and the hudi dataset for the concerned table is created under the path ",(0,n.jsx)(a.code,{children:"<base-path-prefix>/default/<name_of_table_to_be_ingested>"}),". Also there is a provision to override the default path for hudi datasets. You can create hudi dataset for a particular table by setting the property ",(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.ingestion.targetBasePath"})," in table level config file"]}),"\n",(0,n.jsx)(a.li,{children:"There are a lot of properties that one might like to override per table, for example"}),"\n"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"hoodie.datasource.write.recordkey.field=_row_key\nhoodie.datasource.write.partitionpath.field=created_at\nhoodie.deltastreamer.source.kafka.topic=topic2\nhoodie.deltastreamer.keygen.timebased.timestamp.type=UNIX_TIMESTAMP\nhoodie.deltastreamer.keygen.timebased.input.dateformat=yyyy-MM-dd HH:mm:ss.S\nhoodie.datasource.hive_sync.table=short_trip_uber_hive_dummy_table\nhoodie.deltastreamer.ingestion.targetBasePath=s3:///temp/hudi/table1\n"})}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Properties like above need to be set for every table to be ingested. As already suggested at the beginning, users are expected to maintain separate config files for every table by setting the below property"}),"\n"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"hoodie.deltastreamer.ingestion.<db>.<table>.configFile=s3:///tmp/config/config1.properties\n"})}),"\n",(0,n.jsxs)(a.p,{children:["If you do not want to set the above property for every table, you can simply create config files for every table to be ingested under the config folder with the name - ",(0,n.jsx)(a.code,{children:"<database>_<table>_config.properties"}),". For example if you want to ingest table1 and table2 from dummy database, where config folder is set to ",(0,n.jsx)(a.code,{children:"s3:///tmp/config"}),", then you need to create 2 config files on the given paths - ",(0,n.jsx)(a.code,{children:"s3:///tmp/config/dummy_table1_config.properties"})," and ",(0,n.jsx)(a.code,{children:"s3:///tmp/config/dummy_table2_config.properties"}),"."]}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Finally you can specify all the common properties in a common properties file. Common properties file does not necessarily have to lie under config folder but it is advised to keep it along with other config files. This file will contain the below properties"}),"\n"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"hoodie.deltastreamer.ingestion.tablesToBeIngested=db1.table1,db2.table2\nhoodie.deltastreamer.ingestion.db1.table1.configFile=s3:///tmp/config_table1.properties\nhoodie.deltastreamer.ingestion.db2.table2.configFile=s3:///tmp/config_table2.properties\n"})}),"\n",(0,n.jsx)(a.h3,{id:"configuring-schema-providers",children:"Configuring schema providers"}),"\n",(0,n.jsxs)(a.p,{children:["It is possible to configure different schema providers for different tables or same schema provider class for all tables. All you need to do is configure the property ",(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.schemaprovider.class"})," accordingly as per your use case as below -"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"hoodie.deltastreamer.schemaprovider.class=org.apache.hudi.utilities.schema.FilebasedSchemaProvider\n"})}),"\n",(0,n.jsxs)(a.p,{children:["Further it is also possible to configure different source and target schema registry urls with ",(0,n.jsx)(a.code,{children:"SchemaRegistryProvider"})," as the schemaprovider class. Originally HoodieMultiTableDeltaStreamer was designed to cater to use cases where subject naming strategy is set to ",(0,n.jsx)(a.a,{href:"https://docs.confluent.io/platform/current/schema-registry/serdes-develop/index.html#subject-name-strategy",children:"TopicNameStrategy"})," which is the default provided by Confluent.\nWith this default strategy in place, the subject name is same as the topic name being used in kafka. Source and target schema registry urls can be configured as below with TopicNameStrategy -"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"hoodie.deltastreamer.schemaprovider.registry.baseUrl=http://localhost:8081/subjects/\nhoodie.deltastreamer.schemaprovider.registry.urlSuffix=-value/versions/latest\n"})}),"\n",(0,n.jsx)(a.p,{children:"If you want to consume different versions of your source and target subjects, you can configure as below -"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"hoodie.deltastreamer.schemaprovider.registry.baseUrl=http://localhost:8081/subjects/\nhoodie.deltastreamer.schemaprovider.registry.sourceUrlSuffix=-value/versions/latest\nhoodie.deltastreamer.schemaprovider.registry.targetUrlSuffix=-value/versions/1\n"})}),"\n",(0,n.jsx)(a.p,{children:"If you are looking to configure the schema registry urls in the most straight forward way, you can do that as below"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"hoodie.deltastreamer.schemaprovider.registry.url=http://localhost:8081/subjects/random-value/versions/latest\nhoodie.deltastreamer.schemaprovider.registry.targetUrl=http://localhost:8081/subjects/random-value/versions/latest\n"})}),"\n",(0,n.jsx)(a.h3,{id:"run-command",children:"Run Command"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.code,{children:"HoodieMultiTableDeltaStreamer"})," can be run similar to how one runs ",(0,n.jsx)(a.code,{children:"HoodieDeltaStreamer"}),". Please refer to the example given below for the command."]}),"\n",(0,n.jsx)(a.h3,{id:"example",children:"Example"}),"\n",(0,n.jsxs)(a.p,{children:["Suppose you want to ingest table1 and table2 from db1 and want to ingest the 2 tables under the path ",(0,n.jsx)(a.code,{children:"s3:///temp/hudi"}),". You can ingest them using the below command"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"[hoodie]$ spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieMultiTableDeltaStreamer `ls packaging/hudi-utilities-bundle/target/hudi-utilities-bundle-*.jar` \\\n  --props s3:///temp/hudi-ingestion-config/kafka-source.properties \\\n  --config-folder s3:///temp/hudi-ingestion-config \\\n  --schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \\\n  --source-class org.apache.hudi.utilities.sources.AvroKafkaSource \\\n  --source-ordering-field impresssiontime \\\n  --base-path-prefix s3:///temp/hudi \\ \n  --target-table dummy_table \\\n  --op UPSERT\n"})}),"\n",(0,n.jsx)(a.p,{children:"s3:///temp/config/kafka-source.properties"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"hoodie.deltastreamer.ingestion.tablesToBeIngested=db1.table1,db1.table2\nhoodie.deltastreamer.ingestion.db1.table1.configFile=s3:///temp/hudi-ingestion-config/config_table1.properties\nhoodie.deltastreamer.ingestion.db21.table2.configFile=s3:///temp/hudi-ingestion-config/config_table2.properties\n\n#Kafka props\nbootstrap.servers=localhost:9092\nauto.offset.reset=earliest\nschema.registry.url=http://localhost:8081\n\nhoodie.datasource.write.keygenerator.class=org.apache.hudi.keygen.CustomKeyGenerator\n"})}),"\n",(0,n.jsx)(a.p,{children:"s3:///temp/hudi-ingestion-config/config_table1.properties"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"hoodie.datasource.write.recordkey.field=_row_key1\nhoodie.datasource.write.partitionpath.field=created_at\nhoodie.deltastreamer.source.kafka.topic=topic1\n"})}),"\n",(0,n.jsx)(a.p,{children:"s3:///temp/hudi-ingestion-config/config_table2.properties"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"hoodie.datasource.write.recordkey.field=_row_key2\nhoodie.datasource.write.partitionpath.field=created_at\nhoodie.deltastreamer.source.kafka.topic=topic2\n"})}),"\n",(0,n.jsxs)(a.p,{children:["Contributions are welcome for extending multiple tables ingestion support to ",(0,n.jsx)(a.strong,{children:"MERGE_ON_READ"})," storage type and enabling ",(0,n.jsx)(a.code,{children:"HoodieMultiTableDeltaStreamer"})," ingest multiple tables parallely."]}),"\n",(0,n.jsx)(a.p,{children:"Happy ingesting!"})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},17211:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(91251),n=t(74848),s=t(28453);const r={title:"Apache Hudi - The Data Lake Platform",excerpt:"It's been called many things. But, we have always been building a data lake platform",author:"vinoth",category:"blog",image:"/assets/images/blog/hudi_streaming.png",tags:["datalake platform","blog","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Data Lake Platform",id:"data-lake-platform",level:2},{value:"Is Hudi a \u201cformat\u201d?",id:"is-hudi-a-format",level:3},{value:"Is Hudi a transactional layer?",id:"is-hudi-a-transactional-layer",level:3},{value:"Hudi Stack",id:"hudi-stack",level:2},{value:"Lake Storage",id:"lake-storage",level:2},{value:"File Format",id:"file-format",level:2},{value:"Table Format",id:"table-format",level:2},{value:"Indexes",id:"indexes",level:2},{value:"Concurrency Control",id:"concurrency-control",level:2},{value:"Writers",id:"writers",level:2},{value:"Readers",id:"readers",level:2},{value:"Table Services",id:"table-services",level:2},{value:"Data Services",id:"data-services",level:2},{value:"Timeline Metaserver",id:"timeline-metaserver",level:2},{value:"Lake Cache",id:"lake-cache",level:2},{value:"Onwards",id:"onwards",level:2}];function c(e){const a={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",p:"p",strong:"strong",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(a.p,{children:["As early as 2016, we set out a ",(0,n.jsx)(a.a,{href:"https://www.oreilly.com/content/ubers-case-for-incremental-processing-on-hadoop/",children:"bold, new vision"})," reimagining batch data processing through a new \u201c",(0,n.jsx)(a.strong,{children:"incremental"}),"\u201d data processing stack - alongside the existing batch and streaming stacks.\nWhile a stream processing pipeline does row-oriented processing, delivering a few seconds of processing latency, an incremental pipeline would apply the same principles to ",(0,n.jsx)(a.em,{children:"columnar"})," data in the data lake,\ndelivering orders of magnitude improvements in processing efficiency within few minutes, on extremely scalable batch storage/compute infrastructure. This new stack would be able to effortlessly support regular batch processing for bulk reprocessing/backfilling as well.\nHudi was built as the manifestation of this vision, rooted in real, hard problems faced at ",(0,n.jsx)(a.a,{href:"https://eng.uber.com/uber-big-data-platform/",children:"Uber"})," and later took a life of its own in the open source community. Together, we have been able to\nusher in fully incremental data ingestion and moderately complex ETLs on data lakes already."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"the different components that make up the stream and batch processing stack today, showing how an incremental stack blends the best of both the worlds.",src:t(3798).A+"",width:"1229",height:"961"})}),"\n",(0,n.jsxs)(a.p,{children:["Today, this grand vision of being able to express almost any batch pipeline incrementally is more attainable than it ever was. Stream processing is ",(0,n.jsx)(a.a,{href:"https://flink.apache.org/blog/",children:"maturing rapidly"})," and gaining ",(0,n.jsx)(a.a,{href:"https://www.confluent.io/blog/every-company-is-becoming-software/",children:"tremendous momentum"}),",\nwith ",(0,n.jsx)(a.a,{href:"https://flink.apache.org/2021/03/11/batch-execution-mode.html",children:"generalization"})," of stream processing APIs to work over a batch execution model. Hudi completes the missing pieces of the puzzle by providing streaming optimized lake storage,\nmuch like how Kafka/Pulsar enable efficient storage for event streaming. ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/powered_by",children:"Many organizations"})," have already reaped real benefits of adopting a streaming model for their data lakes, in terms of fresh data, simplified architecture and great cost reductions."]}),"\n",(0,n.jsxs)(a.p,{children:["But first, we needed to tackle the basics - transactions and mutability - on the data lake. In many ways, Apache Hudi pioneered the transactional data lake movement as we know it today. Specifically, during a time when more special-purpose systems were being born, Hudi introduced a server-less, transaction layer, which worked over the general-purpose Hadoop FileSystem abstraction on Cloud Stores/HDFS. This model helped Hudi to scale writers/readers to 1000s of cores on day one, compared to warehouses which offer a richer set of transactional guarantees but are often bottlenecked by the 10s of servers that need to handle them. We also experience a lot of joy to see similar systems (Delta Lake for e.g) later adopt the same server-less transaction layer model that we originally shared way back in ",(0,n.jsx)(a.a,{href:"https://eng.uber.com/hoodie/",children:"early '17"}),". We consciously introduced two table types Copy On Write (with simpler operability) and Merge On Read (for greater flexibility) and now these terms are used in ",(0,n.jsx)(a.a,{href:"https://github.com/apache/iceberg/pull/1862",children:"projects"})," outside Hudi, to refer to similar ideas being borrowed from Hudi. Through open sourcing and ",(0,n.jsx)(a.a,{href:"https://blogs.apache.org/foundation/entry/the-apache-software-foundation-announces64",children:"graduating"})," from the Apache Incubator, we have made some great progress elevating these ideas ",(0,n.jsx)(a.a,{href:"http://hudi.apache.org/docs/powered_by.html",children:"across the industry"}),", as well as bringing them to life with a cohesive software stack. Given the exciting developments in the past year or so that have propelled data lakes further mainstream, we thought some perspective can help users see Hudi with the right lens, appreciate what it stands for, and be a part of where it\u2019s headed. At this time, we also wanted to shine some light on all the great work done by ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/graphs/contributors",children:"180+ contributors"})," on the project, working with more than 2000 unique users over slack/github/jira, contributing all the different capabilities Hudi has gained over the past years, from its humble beginnings."]}),"\n",(0,n.jsx)(a.p,{children:"This is going to be a rather long post, but we will do our best to make it worth your time. Let\u2019s roll."}),"\n",(0,n.jsx)(a.h2,{id:"data-lake-platform",children:"Data Lake Platform"}),"\n",(0,n.jsxs)(a.p,{children:["We have noticed that, Hudi is sometimes positioned as a \u201c",(0,n.jsx)(a.a,{href:"https://cloud.google.com/blog/products/data-analytics/getting-started-with-new-table-formats-on-dataproc",children:"table format"}),"\u201d or \u201ctransactional layer\u201d. While this is not incorrect, this does not do full justice to all that Hudi has to offer."]}),"\n",(0,n.jsx)(a.h3,{id:"is-hudi-a-format",children:"Is Hudi a \u201cformat\u201d?"}),"\n",(0,n.jsx)(a.p,{children:"Hudi was not designed as a general purpose table format, tracking files/folders for batch processing. Rather, the functionality provided by a table format is merely one layer in the Hudi software stack. Hudi was designed to play well with the Hive format (if you will), given how popular and widespread it is. Over time, to solve scaling challenges or bring in additional functionality, we have invested in our own native table format with an eye for incremental processing vision. for e.g, we need to support shorter transactions that commit every few seconds. We believe these requirements would fully subsume challenges solved by general purpose table formats over time. But, we are also open to plugging in or syncing to other open table formats, so their users can also benefit from the rest of the Hudi stack. Unlike the file formats, a table format is merely a representation of table metadata and it\u2019s actually quite possible to translate from Hudi to other formats/vice versa if users are willing to accept the trade-offs."}),"\n",(0,n.jsx)(a.h3,{id:"is-hudi-a-transactional-layer",children:"Is Hudi a transactional layer?"}),"\n",(0,n.jsxs)(a.p,{children:["Of course, Hudi had to provide transactions for implementing deletes/updates, but Hudi\u2019s transactional layer is designed around an ",(0,n.jsx)(a.a,{href:"https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying",children:"event log"})," that is also well-integrated with an entire set of built-in table/data services. For e.g compaction is aware of clustering actions already scheduled and optimizes by skipping over the files being clustered - while the user is blissfully unaware of all this. Hudi also provides out-of-box tools for ingesting, ETLing data, and much more. We have always been thinking of Hudi as solving a database problem around stream processing - areas that are actually ",(0,n.jsx)(a.a,{href:"https://www.infoq.com/presentations/streaming-databases/",children:"very related to each other"}),". In fact, Stream processing is enabled by logs (capture/emit event streams, rewind/reprocess) and databases (state stores, updatable sinks). With Hudi, the idea was that if we build a database supporting efficient updates and extracting data streams while remaining optimized for large batch queries, incremental pipelines can be built using Hudi tables as state store & update-able sinks."]}),"\n",(0,n.jsxs)(a.p,{children:["Thus, the best way to describe Apache Hudi is as a ",(0,n.jsx)(a.strong,{children:"Streaming Data Lake Platform"})," built around a ",(0,n.jsx)(a.em,{children:"database kernel"}),". The words carry significant meaning."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"/assets/images/blog/datalake-platform/Screen_Shot_2021-07-20_at_5.35.47_PM.png",src:t(42664).A+"",width:"1650",height:"413"})}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Streaming"}),": At its core, by optimizing for fast upserts & change streams, Hudi provides the primitives to data lake workloads that are comparable to what ",(0,n.jsx)(a.a,{href:"https://kafka.apache.org/",children:"Apache Kafka"})," does for event-streaming (namely, incremental produce/consume of events and a state-store for interactive querying)."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Data Lake"}),": Nonetheless, Hudi provides an optimized, self-managing data plane for large scale data processing on the lake (adhoc queries, ML pipelines, batch pipelines), powering arguably the ",(0,n.jsx)(a.a,{href:"https://eng.uber.com/apache-hudi-graduation/",children:"largest transactional lake"})," in the world. While Hudi can be used to build a ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2024/07/11/what-is-a-data-lakehouse/",children:"lakehouse"}),", given its transactional capabilities, Hudi goes beyond and unlocks an end-to-end streaming architecture. In contrast, the word \u201cstreaming\u201d appears just 3 times in the lakehouse ",(0,n.jsx)(a.a,{href:"http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf",children:"paper"}),", and one of them is talking about Hudi."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Platform"}),": Oftentimes in open source, there is great tech, but there is just too many of them - all differing ever so slightly in their opinionated ways, ultimately making the integration task onerous on the end user. Lake users deserve the same great usability that cloud warehouses provide, with the additional freedom and transparency of a true open source community. Hudi\u2019s data and table services, tightly integrated with the Hudi \u201ckernel\u201d, gives us the ability to deliver cross layer optimizations with reliability and ease of use."]}),"\n",(0,n.jsx)(a.h2,{id:"hudi-stack",children:"Hudi Stack"}),"\n",(0,n.jsxs)(a.p,{children:["The following stack captures layers of software components that make up Hudi, with each layer depending on and drawing strength from the layer below. Typically, data lake users write data out once using an open file format like Apache ",(0,n.jsx)(a.a,{href:"http://parquet.apache.org/",children:"Parquet"}),"/",(0,n.jsx)(a.a,{href:"https://orc.apache.org/",children:"ORC"})," stored on top of extremely scalable cloud storage or distributed file systems. Hudi provides a self-managing data plane to ingest, transform and manage this data, in a way that unlocks incremental data processing on them."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Figure showing the Hudi stack",src:t(27046).A+"",width:"796",height:"809"})}),"\n",(0,n.jsxs)(a.p,{children:["Furthermore, Hudi either already provides or plans to add components that make this data universally accessible to all the different query engines out there. The features annotated with ",(0,n.jsx)(a.code,{children:"*"})," represent work in progress and dotted boxes represent planned future work, to complete our vision for the project.\nWhile we have strawman designs outlined for the newer components in the blog, we welcome with open arms fresh perspectives from the community.\nRest of the blog will delve into each layer in our stack - explaining what it does, how it's designed for incremental processing and how it will evolve in the future."]}),"\n",(0,n.jsx)(a.h2,{id:"lake-storage",children:"Lake Storage"}),"\n",(0,n.jsxs)(a.p,{children:["Hudi interacts with lake storage using the ",(0,n.jsx)(a.a,{href:"https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/FileSystem.html",children:"Hadoop FileSystem API"}),", which makes it compatible with all of its implementations ranging from HDFS to Cloud Stores to even in-memory filesystems like ",(0,n.jsx)(a.a,{href:"https://www.alluxio.io/blog/building-high-performance-data-lake-using-apache-hudi-and-alluxio-at-t3go/",children:"Alluxio"}),"/Ignite. Hudi internally implements its own ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/9d2a65a6a6ff9add81411147f1cddd03f7c08e6c/hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java",children:"wrapper filesystem"})," on top to provide additional storage optimizations (e.g: file sizing), performance optimizations (e.g: buffering), and metrics. Uniquely, Hudi takes full advantage of append support, for storage schemes that support it, like HDFS. This helps Hudi deliver streaming writes without causing an explosion in file counts/table metadata. Unfortunately, most cloud/object storages do not offer append capability today (except maybe ",(0,n.jsx)(a.a,{href:"https://azure.microsoft.com/en-us/updates/append-blob-support-in-azure-data-lake-storage-is-now-generally-available/",children:"Azure"}),"). In the future, we plan to leverage the lower-level APIs of major cloud object stores, to provide similar controls over file counts at streaming ingest latencies."]}),"\n",(0,n.jsx)(a.h2,{id:"file-format",children:"File Format"}),"\n",(0,n.jsxs)(a.p,{children:["Hudi is designed around the notion of base file and delta log files that store updates/deltas to a given base file (called a file slice). Their formats are pluggable, with Parquet (columnar access) and HFile (indexed access) being the supported base file formats today. The delta logs encode data in ",(0,n.jsx)(a.a,{href:"http://avro.apache.org/",children:"Avro"})," (row oriented) format for speedier logging (just like Kafka topics for e.g). Going forward, we plan to ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/pull/3228",children:"inline any base file format"})," into log blocks in the coming releases, providing columnar access to delta logs depending on block sizes. Future plans also include Orc base/log file formats, unstructured data formats (free form json, images), and even tiered storage layers in event-streaming systems/OLAP engines/warehouses, work with their native file formats."]}),"\n",(0,n.jsx)(a.p,{children:"Zooming one level up, Hudi's unique file layout scheme encodes all changes to a given base file, as a sequence of blocks (data blocks, delete blocks, rollback blocks) that are merged in order to derive newer base files. In essence, this makes up a self contained redo log that the lets us implement interesting features on top. For e.g, most of today's data privacy enforcement happens by masking data read off the lake storage on-the-fly, invoking hashing/encryption algorithms over and over on the same set of records and incurring significant compute overhead/cost. Users would be able to keep multiple pre-masked/encrypted copies of the same key in the logs and hand out the correct one based on a policy, avoiding all the overhead."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Hudi base and delta logs",src:t(53338).A+"",width:"1649",height:"653"})}),"\n",(0,n.jsx)(a.h2,{id:"table-format",children:"Table Format"}),"\n",(0,n.jsxs)(a.p,{children:["The term \u201ctable format\u201d is new and still means many things to many people. Drawing an analogy to file formats, a table format simply consists of : the file layout of the table, table\u2019s schema and metadata tracking changes to the table. Hudi is not a table format, it implements one internally. Hudi uses Avro schemas to store, manage and evolve a table\u2019s schema. Currently, Hudi enforces schema-on-write, which although stricter than schema-on-read, is adopted ",(0,n.jsx)(a.a,{href:"https://docs.confluent.io/platform/current/schema-registry/avro.html",children:"widely"})," in the stream processing world to ensure pipelines don't break from non backwards compatible changes."]}),"\n",(0,n.jsx)(a.p,{children:"Hudi consciously lays out files within a table/partition into groups and maintains a mapping between an incoming record\u2019s key to an existing file group. All updates are recorded into delta log files specific to a given file group and this design ensures low merge overhead compared to approaches like Hive ACID, which have to merge all delta records against all base files to satisfy queries. For e.g, with uuid keys (used very widely) all base files are very likely to overlap with all delta logs, rendering any range based pruning useless. Much like state stores, Hudi\u2019s design anticipates fast key based upserts/deletes and only requires merging delta logs within each file group. This design choice also lets Hudi provide more capabilities for writing/querying as we will explain below."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Shows the Hudi table format components",src:t(6218).A+"",width:"1520",height:"820"})}),"\n",(0,n.jsxs)(a.p,{children:["The ",(0,n.jsx)(a.em,{children:"timeline"})," is the source-of-truth event log for all Hudi\u2019s table metadata, stored under the ",(0,n.jsx)(a.code,{children:".hoodie"})," folder, that provides an ordered log of all actions performed on the table. Events are retained on the timeline up to a configured interval of time/activity. Each file group is also designed as it\u2019s own self-contained log, which means that even if an action that affected a file group is archived from the timeline, the right state of the records in each file group can be reconstructed by simply locally applying the delta logs to the base file. This design bounds the metadata size, proportional to how often the table is being written to/operated on, independent of how large the entire table is. This is a critical design element needs for supporting frequent writes/commits to tables."]}),"\n",(0,n.jsxs)(a.p,{children:["Lastly, new events on the timeline are then consumed and reflected onto an internal metadata table, implemented as another merge-on-read table offering low write amplification. Hudi is able to absorb quick/rapid changes to table\u2019s metadata, unlike table formats designed for slow-moving data. Additionally, the metadata table uses the ",(0,n.jsx)(a.a,{href:"https://hbase.apache.org/2.0/devapidocs/org/apache/hadoop/hbase/io/hfile/HFile.html",children:"HFile"})," base file format, which provides indexed lookups of keys avoiding the need for reading the entire metadata table to satisfy metadata reads. It currently stores all the physical file paths that are part of the table, to avoid expensive cloud file listings."]}),"\n",(0,n.jsx)(a.p,{children:"A key challenge faced by all the table formats out there today, is the need for expiring snapshots/controlling retention for time travel queries such that it does not interfere with query planning/performance. In the future, we plan to build an indexed timeline in Hudi, which can span the entire history of the table, supporting a time travel look back window of several months/years."}),"\n",(0,n.jsx)(a.h2,{id:"indexes",children:"Indexes"}),"\n",(0,n.jsxs)(a.p,{children:["Indexes help databases plan better queries, that reduce the overall amount of I/O and deliver faster response times. Table metadata about file listings and column statistics are often enough for lake query engines to generate optimized, engine specific query plans quickly. This is however not sufficient for Hudi to realize fast upserts. Hudi already supports different key based indexing schemes to quickly map incoming record keys into the file group they reside in. For this purpose, Hudi exposes a pluggable indexing layer to the writer implementations, with built-in support for range pruning (when keys are ordered and largely arrive in order) using interval trees and bloom filters (e.g: for uuid based keys where ordering is of very little help). Hudi also implements a HBase backed external index which is much more performant although more expensive to operate. Hudi also consciously exploits the partitioning scheme of the table to implement global and non-global indexing schemes. Users can choose to enforce key constraints only within a partition, in return for ",(0,n.jsx)(a.em,{children:(0,n.jsx)(a.code,{children:"O(num_affected_partitions)"})})," upsert performance as opposed to ",(0,n.jsx)(a.em,{children:(0,n.jsx)(a.code,{children:"O(total_partitions)"})})," in the global indexing scenarios. We refer you to this ",(0,n.jsx)(a.a,{href:"http://hudi.apache.org/blog/2020/11/11/hudi-indexing-mechanisms",children:"blog"}),", that goes over indexing in detail. Ultimately, Hudi's writer path ensures the index is always kept in sync with the timeline and data, which is cumbersome and error prone to implement on top of a table format by hand."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"/assets/images/blog/datalake-platform/hudi-design-diagrams_-_Page_5.png",src:t(15569).A+"",width:"1608",height:"570"})}),"\n",(0,n.jsxs)(a.p,{children:["In the future, we intend to add additional forms of indexing as new partitions on the metadata table. Let\u2019s discuss the role\xa0 each one has to play briefly. Query engines typically rely on partitioning to cut down the number of files read for a given query. In database terms, a Hive partition is nothing but a coarse range index, that maps a set of columns to a list of files. Table formats born in the cloud like Iceberg/Delta Lake, have built-in tracking of column ranges per file in a single flat file (json/avro), that helps avoid planning costs for large/poorly sized tables. This need has been largely reduced for Hudi tables thus far, given Hudi automatically enforces file sizes which help bound time taken to read out stats from parquet footers for e.g. However, with the advent of features like clustering, there is a need for writing smaller files first and then reclustering in a query optimized way. We plan to add indexed column ranges, that can scale to lots of small files and support faster mutations . See ",(0,n.jsx)(a.a,{href:"https://cwiki.apache.org/confluence/display/HUDI/RFC-27+Data+skipping+index+to+improve+query+performance",children:"RFC-27"})," to track the design process and get involved."]}),"\n",(0,n.jsxs)(a.p,{children:["While Hudi already supports external indexes for random write workloads, we would like to support ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/pull/2487",children:"point-lookup-ish queries"})," right on top of lake storage, which helps avoid the overhead of an additional database for many classes of data applications. We also anticipate that uuid/key based joins will be sped up a lot, by leveraging record level indexing schemes, we build out for fast upsert performance. We also plan to move our tracking of bloom filters out of the file footers and into its ",(0,n.jsx)(a.a,{href:"https://issues.apache.org/jira/browse/HUDI-1295",children:"own partition"})," on the metadata table. Ultimately, we look to exposing all of this to the queries as well in the coming releases."]}),"\n",(0,n.jsx)(a.h2,{id:"concurrency-control",children:"Concurrency Control"}),"\n",(0,n.jsxs)(a.p,{children:["Concurrency control defines how different writers/readers coordinate access to the table. Hudi ensures atomic writes, by way of publishing commits atomically to the timeline, stamped with an instant time that denotes the time at which the action is deemed to have occurred. Unlike general purpose file version control, Hudi draws clear distinction between writer processes (that issue user\u2019s upserts/deletes), table services (that write data/metadata to optimize/perform bookkeeping) and readers (that execute queries and read data). Hudi provides snapshot isolation between all three types of processes, meaning they all operate on a consistent snapshot of the table. Hudi provides ",(0,n.jsx)(a.a,{href:"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+22+%3A+Snapshot+Isolation+using+Optimistic+Concurrency+Control+for+multi-writers",children:"optimistic concurrency control"})," (OCC) between writers, while providing lock-free, non-blocking MVCC\xa0 based concurrency control between writers and table-services and between different table services."]}),"\n",(0,n.jsx)(a.p,{children:"Projects that solely rely on OCC deal with competing operations, by either implementing a lock or relying on atomic renames. Such approaches are optimistic that real contention never happens and resort to failing one of the writer operations if conflicts occur, which can cause significant resource wastage or operational overhead. Imagine a scenario of two writer processes : an ingest writer job producing new data every 30 minutes and a deletion writer job that is enforcing GDPR taking 2 hours to issue deletes. If there were to overlap on the same files (very likely to happen in real situations with random deletes), the deletion job is almost guaranteed to starve and fail to commit each time, wasting tons of cluster resources. Hudi takes a very different approach that we believe is more apt for lake transactions, which are typically long-running. For e.g async compaction that can keep deleting records in the background without blocking the ingest job. This is implemented via a file level, log based concurrency control protocol which orders actions based on their start instant times on the timeline."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Figure showing competing transactions leading to starvation with just OCC",src:t(32549).A+"",width:"1110",height:"881"})}),"\n",(0,n.jsxs)(a.p,{children:["We are hard at work, improving our OCC based implementation around early detection of conflicts for concurrent writers and terminate early without burning up CPU resources. We are also working on ",(0,n.jsx)(a.a,{href:"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+22+%3A+Snapshot+Isolation+using+Optimistic+Concurrency+Control+for+multi-writers#RFC22:SnapshotIsolationusingOptimisticConcurrencyControlformultiwriters-FutureWork(LockFree-ishConcurrencyControl)",children:"adding fully log based"}),", non-blocking concurrency control between writers, where writers proceed to write deltas and conflicts are resolved later in some deterministic timeline order - again much like how stream processing programs are written. This is possible only due to Hudi\u2019s unique design that sequences actions into an ordered event log and the transaction handling code is aware of the relationship/interdependence of actions to each other."]}),"\n",(0,n.jsx)(a.h2,{id:"writers",children:"Writers"}),"\n",(0,n.jsxs)(a.p,{children:["Hudi tables can be used as sinks for Spark/Flink pipelines and the Hudi writing path provides several enhanced capabilities over file writing done by vanilla parquet/avro sinks. Hudi classifies write operations carefully into incremental (",(0,n.jsx)(a.code,{children:"insert"}),", ",(0,n.jsx)(a.code,{children:"upsert"}),", ",(0,n.jsx)(a.code,{children:"delete"}),") and batch/bulk operations (",(0,n.jsx)(a.code,{children:"insert_overwrite"}),", ",(0,n.jsx)(a.code,{children:"insert_overwrite_table"}),", ",(0,n.jsx)(a.code,{children:"delete_partition"}),", ",(0,n.jsx)(a.code,{children:"bulk_insert"}),") and provides relevant functionality for each operation in a performant and cohesive way. Both upsert and delete operations automatically handle merging of records with the same key in the input stream (say, a CDC stream obtained from upstream table) and then lookup the index, finally invoke a bin packing algorithm to pack data into files, while ",(0,n.jsx)(a.a,{href:"http://hudi.apache.org/blog/2021/03/01/hudi-file-sizing",children:"respecting a pre-configured target file size"}),". An insert operation on the other hand, is intelligent enough to avoid the precombining and index lookup, while retaining the benefits of the rest of the pipeline. Similarly, bulk_insert operation provides several sort modes for controlling initial file sizes and file counts, when importing data from an external table to Hudi. The other batch write operations provide MVCC based implementations of typical overwrite semantics used in batch data pipelines, while retaining all the transactional and incremental processing capabilities, making it seamless to switch between incremental pipelines for regular runs and batch pipelines for backfilling/dropping older partitions. The write pipeline also contains lower layers optimizations around handling large merges by spilling to ",(0,n.jsx)(a.a,{href:"https://rocksdb.org/",children:"rocksDB"})," or an external spillable map, multi-threaded/concurrent I/O to improve write performance."]}),"\n",(0,n.jsxs)(a.p,{children:["Keys are first class citizens inside Hudi and the pre-combining/index lookups done before upsert/deletes ensure a key is unique across partitions or within partitions, as desired. In contrast with other approaches where this is left to data engineer to co-ordinate using ",(0,n.jsx)(a.code,{children:"MERGE INTO"})," statements, this approach ensures quality data especially for critical use-cases. Hudi also ships with several ",(0,n.jsx)(a.a,{href:"http://hudi.apache.org/blog/2021/02/13/hudi-key-generators/",children:"built-in key generators"})," that can parse all common date/timestamps, handle malformed data with an extensible framework for defining custom key generators. Keys are also materialized with the records using the ",(0,n.jsx)(a.code,{children:"_hoodie_record_key"})," meta column, which makes it possible to change the key fields and perform repairs on older data with incorrect keys for e.g. Finally, Hudi provides a ",(0,n.jsx)(a.code,{children:"HoodieRecordPayload"})," interface is very similar to processor APIs in Flink or Kafka Streams, and allows for expressing arbitrary merge conditions, between the base and delta log records. This allows users to express partial merges (e.g log only updated columns to the delta log for efficiency) and avoid reading all the base records before every merge. Routinely, we find users leverage such custom merge logic during replaying/backfilling older data onto a table, while ensuring newer updates are not overwritten causing the table's snapshot to go back in time. This is achieved by simply using the  ",(0,n.jsx)(a.code,{children:"HoodieDefaultPayload"})," where latest value for a given key is picked based a configured precombine field value in the data."]}),"\n",(0,n.jsxs)(a.p,{children:["Hudi writers add metadata to each record, that codify the commit time and a sequence number for each record within that commit (comparable to a Kafka offset), which make it possible to derive record level change streams. Hudi also provides users the ability to specify event time fields in incoming data streams and track them in the timeline.Mapping these to stream processing concepts, Hudi contains both ",(0,n.jsx)(a.a,{href:"https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/",children:"arrival and event time"})," for records for each commit, that can help us build good ",(0,n.jsx)(a.a,{href:"https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/datastream/event-time/generating_watermarks/",children:"watermarks"})," that inform complex incremental processing pipelines. In the near future, we are looking to add new metadata columns, that encode the source operation (insert, update, delete) for each record, before we embark on this grand goal of full end-end incremental ETL pipelines. All said, we realized many users may simply want to use Hudi as an efficient write layer that supports transactions, fast updates/deletes. We are looking into adding support for ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/pull/3306",children:"virtual keys"})," and making the ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/pull/3247",children:"meta columns optional"}),", to lower storage overhead, while still making rest of Hudi's capabilities (metadata table, table services, ..) available."]}),"\n",(0,n.jsx)(a.h2,{id:"readers",children:"Readers"}),"\n",(0,n.jsxs)(a.p,{children:["Hudi provides snapshot isolation between writers and readers and allows for any table snapshot to be queries consistently from all major lake query engines (Spark, Hive, Flink, Presto, Trino, Impala) and even cloud warehouses like Redshift. In fact, we would love to bring Hudi tables as external tables with BigQuery/Snowflake as well, once they also embrace the lake table formats more natively. Our design philosophy around query performance has been to make Hudi as lightweight as possible whenever only base columnar files are read (CoW  snapshot, MOR read-optimized queries),  employing the engine specific vectorized readers in Presto, Trino, Spark for e.g to be employed. This model is far more scalable than maintaining our own readers and users to benefit from engine specific optimizations. For e.g ",(0,n.jsx)(a.a,{href:"https://prestodb.io/blog/2021/02/04/raptorx",children:"Presto"}),", ",(0,n.jsx)(a.a,{href:"https://trino.io/docs/current/connector/hive-caching.html",children:"Trino"})," all have their own data/metadata caches. Whenever, Hudi has to merge base and log files for a query, Hudi takes control and employs several mechanisms (spillable maps, lazy reading) to improve merge performance, while also providing a read-optimized query on the data that trades off data freshness for query performance. In the near future, we are investing deeply into improving MoR snapshot query performance in many ways such as inlining parquet data, special handling of overwrite payloads/merges."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Log merging done for incremental queries",src:t(18023).A+"",width:"1280",height:"907"})}),"\n",(0,n.jsxs)(a.p,{children:["True to its design goals, Hudi provides some very powerful incremental querying capabilities that tied together the meta fields added during writing and the file group based storage layout. While table formats that merely track files, are only able to provide information about files that changed during each snapshot or commits, Hudi generates the exact set of records that changed given a point in the timeline, due to tracking of record level event and arrival times. Further more, this design allows large commits to be consumed in smaller chunks by an incremental query, fully decoupling the writing and incremental querying of data. Time travel is merely implemented as an incremental query that starts and stops at an older portion of the timeline. Since Hudi ensures that a key is atomically mapped to a single file group at any point in time, it makes it possible to support full CDC capabilities on Hudi tables, such as providing all possible values for a given record since time ",(0,n.jsx)(a.code,{children:"t"}),", CDC streams with both before and after images. All of these functionalities can be built local to each file group, given each file group is a self-contained log. Much of our future work in this area will be around bringing such a powerful set of ",(0,n.jsx)(a.a,{href:"https://debezium.io/",children:"debezium"})," like capabilities to life in the coming months."]}),"\n",(0,n.jsx)(a.h2,{id:"table-services",children:"Table Services"}),"\n",(0,n.jsx)(a.p,{children:"What defines and sustains a project\u2019s value over years are its fundamental design principles and the subtle trade offs. Databases often consist of several internal components, working in tandem to deliver efficiency, performance and great operability to its users. True to intent to act as state store for incremental data pipelines, we designed Hudi with built-in table services and self-managing runtime that can orchestrate/trigger these services to optimize everything internally. In fact, if we compare rocksDB (a very popular stream processing state-store) and Hudi\u2019s components, the similarities become obvious."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"/assets/images/blog/datalake-platform/hudi-design-diagrams_-_Page_4.png",src:t(1352).A+"",width:"1740",height:"780"})}),"\n",(0,n.jsxs)(a.p,{children:["There are several built-in table services, all with the goal of ensuring performant table storage layout and metadata management, which are automatically invoked either synchronously after each write operation, or asynchronously as a separate background job. Furthermore, Spark (and Flink) streaming writers can run in continuous mode, and invoke table services asynchronously sharing the underlying executors intelligently with writers. Archival service ensures that the timeline holds sufficient history for inter service co-ordination (e.g compactions wait for other compactions to complete on the same file group), incremental queries. Once events expire from the timeline, the archival service cleans up any side-effects from lake storage (e.g. rolling back of failing concurrent transactions). Hudi's transaction management implementation allows all of these services to be idempotent and thus resilient to failure via just simple retries.  ",(0,n.jsx)(a.a,{href:"http://hudi.apache.org/blog/2021/06/10/employing-right-configurations-for-hudi-cleaner",children:"Cleaner"})," service works off the timeline incrementally (eating our own incremental design dog food), removing file slices that are past the configured retention period for incremental queries, while also allowing sufficient time for long running batch jobs (e.g Hive ETLs) to finish running. Compaction service comes with built-in strategies (date partitioning based, I/O bounded), that merges a base file with a set of delta log files to produce new base file, all while allowing writes to happen concurrently to the file group. This is only possible due to Hudi's grouping of files into groups and support for flexible log merging, and unlocks non-blocking execution of deletes while concurrent updates are being issues to the same set of records. ",(0,n.jsx)(a.a,{href:"http://hudi.apache.org/blog/2021/01/27/hudi-clustering-intro/",children:"Clustering"})," service functions similar to what users find in BigQuery or Snowflake, where users can group records that are often queried together by sort keys or control file sizes by coalescing smaller base files into larger ones. Clustering is fully aware of other actions on the timeline such as cleaning, compaction, and it helps Hudi implement intelligent optimizations like avoiding compaction on file groups that are already being clustered, to save on I/O. Hudi also performs rollback of partial writes and cleans up any uncommitted data from lake storage, by use of marker files that track any files created as a part of write operations. Finally, the bootstrap service performs one time zero copy migration of plain parquet tables to Hudi, while allowing both pipelines to operate in parallel, for data validation purposes. Cleaner service is once again aware of these bootstrapped base files and can optionally clean them up, to ensure use-cases like GDPR compliance are met."]}),"\n",(0,n.jsxs)(a.p,{children:["We are always looking for ways to improve and enhance our table services in meaningful ways. In the coming releases, we are working towards a much more ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/pull/3233",children:"scalable model"})," of cleaning up partial writes, by consolidating marker file creation using our timeline metaserver, which avoids expensive full table scans to seek out and remove uncommitted files. We also have ",(0,n.jsx)(a.a,{href:"https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=181307144",children:"various proposals"})," to add more clustering schemes, unlock clustering with concurrent updates using fully log based concurrency control."]}),"\n",(0,n.jsx)(a.h2,{id:"data-services",children:"Data Services"}),"\n",(0,n.jsxs)(a.p,{children:["As noted at the start, we wanted to make Hudi immediately usable for common end-end use-cases and thus invested deeply into a set of data services, that provide functionality that is data/workload specific, sitting on top of the table services, writers/readers directly. Foremost in that list, is the Hudi DeltaStreamer utility, which has been an extremely popular choice for painlessly building a data lake out of  Kafka streams and files landing in different formats on top of lake storage. Over time, we have also built out sources that cover all major systems like a JDBC source for RDBMS/other warehouses, Hive source and even incrementally pulling data from other Hudi tables. The utility supports automatic checkpoint management tracking source checkpoints as a part of target Hudi table metadata, with support for backfills/one-off runs. DeltaStreamer also integrates with major schema registries such as Confluent's and also provides checkpoint translation from other popular mechanisms like Kafka connect. It also supports de-duplication of data, multi-level configuration management system, built in transformers that take arbitrary SQL or coerce ",(0,n.jsx)(a.a,{href:"http://hudi.apache.org/blog/2020/10/19/hudi-meets-aws-emr-and-aws-dms/",children:"CDC log changes"})," into writable forms, that combined with other aforementioned features can be used for deploying production grade incremental pipelines. Finally, just like the Spark/Flink streaming writers, DeltaStreamer is able to run in a continuous mode, with automatic management of table services. Hudi also provides several other tools for snapshotting and incrementally exporting Hudi tables, also importing/",(0,n.jsx)(a.a,{href:"http://hudi.apache.org/blog/2020/03/22/exporting-hudi-datasets/",children:"exporting"}),"/bootstrapping new tables into Hudi. Hudi also provides commit notifications into Http endpoints or Kafka topics, about table commit activity, which can be used for analytics or building data sensors in workflow managers like Airflow to trigger pipelines."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"/assets/images/blog/datalake-platform/hudi-design-diagrams_-_Page_8.png",src:t(45316).A+"",width:"1440",height:"680"})}),"\n",(0,n.jsxs)(a.p,{children:["Going forward, we would love contributions to enhance our ",(0,n.jsx)(a.a,{href:"http://hudi.apache.org/blog/2020/08/22/ingest-multiple-tables-using-hudi/",children:"multi delta streamer utility"}),", which can ingest entire Kafka clusters in a single large Spark application, to be on par and hardened. To further our progress towards end-end complex incremental pipelines, we plan to work towards enhancing the delta streamer utility and its SQL transformers to be triggered by multiple source streams (as opposed to just the one today) and unlock materialized views at scale. We would like to bring an array of useful transformers that perform masking or data monitoring, and extend support for egress of data off Hudi tables into other external sinks as well. Finally, we would love to merge the FlinkStreamer and the DeltaStreamer utilities into one cohesive utility, that can be used across engines. We are constantly improving existing sources (e.g support for parallelized listings of DFS sources) and adding new ones (e.g S3 event based DFS source)"]}),"\n",(0,n.jsx)(a.h2,{id:"timeline-metaserver",children:"Timeline Metaserver"}),"\n",(0,n.jsxs)(a.p,{children:["Storing and serving table metadata right on the lake storage is scalable, but can be much less performant compared to RPCs against a scalable meta server. Most cloud warehouses internally are built on a metadata layer that leverages an external database (e.g ",(0,n.jsx)(a.a,{href:"https://www.snowflake.com/blog/how-foundationdb-powers-snowflake-metadata-forward/",children:"Snowflake uses foundationDB"}),"). Hudi also provides a metadata server, called the \u201cTimeline server\u201d, which offers an alternative backing store for Hudi\u2019s table metadata. Currently, the timeline server runs embedded in the Hudi writer processes, serving file listings out of a local rocksDB store/",(0,n.jsx)(a.a,{href:"https://javalin.io/",children:"Javalin"})," REST API during the write process, without needing to repeatedly list the cloud storage. Given we have hardened this as the default option since our 0.6.0 release, we are considering standalone timeline server installations, with support for horizontal scaling, database/table mappings, security and all the features necessary to turn it into a highly performant next generation lake metastore."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"/assets/images/blog/datalake-platform/hudi-design-diagrams_-_Page_6.png",src:t(85402).A+"",width:"1300",height:"980"})}),"\n",(0,n.jsx)(a.h2,{id:"lake-cache",children:"Lake Cache"}),"\n",(0,n.jsxs)(a.p,{children:["There is a fundamental tradeoff today in data lakes between faster writing and great query performance. Faster writing typically involves writing smaller files (and later clustering them) or logging deltas (and later merging on read). While this provides good performance already, the pursuit of great query performance often warrants opening fewer number of files/objects on lake storage and may be pre-materializing the merges between base and delta logs. After all, most databases employ a ",(0,n.jsx)(a.a,{href:"https://dev.mysql.com/doc/refman/8.0/en/innodb-buffer-pool.html",children:"buffer pool"})," or ",(0,n.jsx)(a.a,{href:"https://github.com/facebook/rocksdb/wiki/Block-Cache",children:"block cache"}),", to amortize the cost of accessing storage. Hudi already contains several design elements that are conducive for building a caching tier (write-through or even just populated by an incremental query), that will be multi-tenant and can cache pre-merged images of the latest file slices, consistent with the timeline. Hudi timeline can be used to simply communicate caching policies, just like how we perform inter table service co-ordination. Historically, caching has been done closer to the query engines or via intermediate in-memory file systems. By placing a caching tier closer and more tightly integrated with a transactional lake storage like Hudi, all query engines would be able to share and amortize the cost of the cache, while supporting updates/deletes as well. We look forward to building a buffer pool for the lake that works across all major engines, with the contributions from the rest of the community."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"/assets/images/blog/datalake-platform/hudi-design-diagrams_-_Page_7.png",src:t(2979).A+"",width:"1410",height:"801"})}),"\n",(0,n.jsx)(a.h2,{id:"onwards",children:"Onwards"}),"\n",(0,n.jsxs)(a.p,{children:["We hope that this blog painted a complete picture of Apache Hudi, staying true to its founding principles. Interested users and readers can expect blogs delving into each layer of the stack and an overhaul of our docs along these lines in the coming weeks/months. We view the current efforts around table formats as merely removing decade-old bottlenecks in data lake storage/query planes, problems which have been already solved very well in cloud warehouses like Big Query/Snowflake. We would like to underscore that our vision here is much greater, much more technically challenging. We as an industry are just wrapping our heads around many of these deep, open-ended problems, that need to be solved to marry stream processing and data lakes, with scale and simplicity. We hope to continue to put community first and build/solve these hard problems together. If these challenges excite you and you would like to build for that exciting future, please come join our ",(0,n.jsx)(a.a,{href:"http://hudi.apache.org/contribute/get-involved",children:"community"}),"."]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},17539:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/04/25/apache-hudi-vs-apache-iceberg-a-comprehensive-comparison","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-04-25-apache-hudi-vs-apache-iceberg-a-comprehensive-comparison.mdx","source":"@site/blog/2024-04-25-apache-hudi-vs-apache-iceberg-a-comprehensive-comparison.mdx","title":"Apache Hudi vs Apache Iceberg: A Comprehensive Comparison","description":"Redirecting... please wait!!","date":"2024-04-25T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"apache iceberg","permalink":"/blog/tags/apache-iceberg"},{"inline":true,"label":"comparison","permalink":"/blog/tags/comparison"},{"inline":true,"label":"risingwave","permalink":"/blog/tags/risingwave"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"RisingWave marketing team","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi vs Apache Iceberg: A Comprehensive Comparison","author":"RisingWave marketing team","category":"blog","image":"/assets/images/blog/2024-04-25-apache-hudi-vs-apache-iceberg-a-comprehensive-comparison.png","tags":["blog","apache hudi","apache iceberg","comparison","risingwave"]},"unlisted":false,"prevItem":{"title":"How to Query Apache Hudi Tables with Python Using Daft: A Spark-Free Approach","permalink":"/blog/2024/05/02/how-query-apache-hudi-tables-python-using-daft-spark-free"},"nextItem":{"title":"Understanding Apache Hudi\'s Consistency Model Part 1","permalink":"/blog/2024/04/24/understanding-apache-hudi-consistency-model-part-1"}}')},17586:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(40222),n=t(74848),s=t(28453),r=t(9230);const o={title:"Table service deployment models in Apache Hudi",authors:[{name:"Sivabalan Narayanan"}],category:"blog",tags:["how-to","table services","deployment","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@simpsons/table-service-deployment-models-in-apache-hudi-9cfa5a44addf",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},17721:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(4219),n=t(74848),s=t(28453);const r={title:"Export Hudi datasets as a copy or as different formats",excerpt:"Learn how to copy or export HUDI dataset in various formats.",author:"rxu",category:"blog",tags:["how-to","snapshot exporter","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Copy to Hudi dataset",id:"copy-to-hudi-dataset",level:3},{value:"Export to json or parquet dataset",id:"export-to-json-or-parquet-dataset",level:3},{value:"Re-partitioning",id:"re-partitioning",level:3},{value:"<code>--output-partition-field</code>",id:"--output-partition-field",level:4},{value:"<code>--output-partitioner</code>",id:"--output-partitioner",level:4}];function c(e){const a={code:"code",h3:"h3",h4:"h4",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.h3,{id:"copy-to-hudi-dataset",children:"Copy to Hudi dataset"}),"\n",(0,n.jsxs)(a.p,{children:["Similar to the existing  ",(0,n.jsx)(a.code,{children:"HoodieSnapshotCopier"}),", the Exporter scans the source dataset and then makes a copy of it to the target output path."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:'spark-submit \\\n  --jars "packaging/hudi-spark-bundle/target/hudi-spark-bundle_2.11-0.6.0-SNAPSHOT.jar" \\\n  --deploy-mode "client" \\\n  --class "org.apache.hudi.utilities.HoodieSnapshotExporter" \\\n      packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.6.0-SNAPSHOT.jar \\\n  --source-base-path "/tmp/" \\\n  --target-output-path "/tmp/exported/hudi/" \\\n  --output-format "hudi"\n'})}),"\n",(0,n.jsx)(a.h3,{id:"export-to-json-or-parquet-dataset",children:"Export to json or parquet dataset"}),"\n",(0,n.jsx)(a.p,{children:'The Exporter can also convert the source dataset into other formats. Currently only "json" and "parquet" are supported.'}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:'spark-submit \\\n  --jars "packaging/hudi-spark-bundle/target/hudi-spark-bundle_2.11-0.6.0-SNAPSHOT.jar" \\\n  --deploy-mode "client" \\\n  --class "org.apache.hudi.utilities.HoodieSnapshotExporter" \\\n      packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.6.0-SNAPSHOT.jar \\\n  --source-base-path "/tmp/" \\\n  --target-output-path "/tmp/exported/json/" \\\n  --output-format "json"  # or "parquet"\n'})}),"\n",(0,n.jsx)(a.h3,{id:"re-partitioning",children:"Re-partitioning"}),"\n",(0,n.jsx)(a.p,{children:"When export to a different format, the Exporter takes parameters to do some custom re-partitioning. By default, if neither of the 2 parameters below is given, the output dataset will have no partition."}),"\n",(0,n.jsx)(a.h4,{id:"--output-partition-field",children:(0,n.jsx)(a.code,{children:"--output-partition-field"})}),"\n",(0,n.jsxs)(a.p,{children:["This parameter uses an existing non-metadata field as the output partitions. All  ",(0,n.jsx)(a.code,{children:"_hoodie_*"}),"  metadata field will be stripped during export."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:'spark-submit \\\n  --jars "packaging/hudi-spark-bundle/target/hudi-spark-bundle_2.11-0.6.0-SNAPSHOT.jar" \\\n  --deploy-mode "client" \\\n  --class "org.apache.hudi.utilities.HoodieSnapshotExporter" \\\n      packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.6.0-SNAPSHOT.jar \\  \n  --source-base-path "/tmp/" \\\n  --target-output-path "/tmp/exported/json/" \\\n  --output-format "json" \\\n  --output-partition-field "symbol"  # assume the source dataset contains a field `symbol`\n'})}),"\n",(0,n.jsx)(a.p,{children:"The output directory will look like this"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"`_SUCCESS symbol=AMRS symbol=AYX symbol=CDMO symbol=CRC symbol=DRNA ...`\n"})}),"\n",(0,n.jsx)(a.h4,{id:"--output-partitioner",children:(0,n.jsx)(a.code,{children:"--output-partitioner"})}),"\n",(0,n.jsxs)(a.p,{children:["This parameter takes in a fully-qualified name of a class that implements  ",(0,n.jsx)(a.code,{children:"HoodieSnapshotExporter.Partitioner"}),". This parameter takes higher precedence than  ",(0,n.jsx)(a.code,{children:"--output-partition-field"}),", which will be ignored if this is provided."]}),"\n",(0,n.jsx)(a.p,{children:"An example implementation is shown below:"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"MyPartitioner.java"})}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:'package com.foo.bar;\npublic class MyPartitioner implements HoodieSnapshotExporter.Partitioner {\n\n  private static final String PARTITION_NAME = "date";\n \n  @Override\n  public DataFrameWriter<Row> partition(Dataset<Row> source) {\n    // use the current hoodie partition path as the output partition\n    return source\n        .withColumnRenamed(HoodieRecord.PARTITION_PATH_METADATA_FIELD, PARTITION_NAME)\n        .repartition(new Column(PARTITION_NAME))\n        .write()\n        .partitionBy(PARTITION_NAME);\n  }\n}\n'})}),"\n",(0,n.jsxs)(a.p,{children:["After putting this class in ",(0,n.jsx)(a.code,{children:"my-custom.jar"}),", which is then placed on the job classpath, the submit command will look like this:"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:'spark-submit \\\n  --jars "packaging/hudi-spark-bundle/target/hudi-spark-bundle_2.11-0.6.0-SNAPSHOT.jar,my-custom.jar" \\\n  --deploy-mode "client" \\\n  --class "org.apache.hudi.utilities.HoodieSnapshotExporter" \\\n      packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.6.0-SNAPSHOT.jar \\\n  --source-base-path "/tmp/" \\\n  --target-output-path "/tmp/exported/json/" \\\n  --output-format "json" \\\n  --output-partitioner "com.foo.bar.MyPartitioner"\n'})})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},18023:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi-design-diagram_-incr-read-1c9bc7f09b69e8d9f1b2d439e3232a01.png"},18316:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/03/01/Create-a-low-latency-source-to-data-lake-pipeline-using-Amazon-MSK-Connect-Apache-Flink-and-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-03-01-Create-a-low-latency-source-to-data-lake-pipeline-using-Amazon-MSK-Connect-Apache-Flink-and-Apache-Hudi.mdx","source":"@site/blog/2022-03-01-Create-a-low-latency-source-to-data-lake-pipeline-using-Amazon-MSK-Connect-Apache-Flink-and-Apache-Hudi.mdx","title":"Create a low-latency source-to-data lake pipeline using Amazon MSK Connect, Apache Flink, and Apache Hudi","description":"Redirecting... please wait!!","date":"2022-03-01T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"streaming ingestion","permalink":"/blog/tags/streaming-ingestion"},{"inline":true,"label":"apache flink","permalink":"/blog/tags/apache-flink"},{"inline":true,"label":"apache kafka","permalink":"/blog/tags/apache-kafka"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.18,"hasTruncateMarker":false,"authors":[{"name":"Ali Alemi","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Create a low-latency source-to-data lake pipeline using Amazon MSK Connect, Apache Flink, and Apache Hudi","authors":[{"name":"Ali Alemi"}],"category":"blog","image":"/assets/images/blog/2022-03-01-low-latency-pipeline-using-msk-flink-hudi.png","tags":["how-to","streaming ingestion","apache flink","apache kafka","amazon"]},"unlisted":false,"prevItem":{"title":"Build a serverless pipeline to analyze streaming data using AWS Glue, Apache Hudi, and Amazon S3","permalink":"/blog/2022/03/09/Build-a-serverless-pipeline-to-analyze-streaming-data-using-AWS-Glue-Apache-Hudi-and-Amazon-S3"},"nextItem":{"title":"Understanding its core concepts from hudi persistence files","permalink":"/blog/2022/02/20/Understanding-its-core-concepts-from-hudi-persistence-files"}}')},18422:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(18316),n=t(74848),s=t(28453),r=t(9230);const o={title:"Create a low-latency source-to-data lake pipeline using Amazon MSK Connect, Apache Flink, and Apache Hudi",authors:[{name:"Ali Alemi"}],category:"blog",image:"/assets/images/blog/2022-03-01-low-latency-pipeline-using-msk-flink-hudi.png",tags:["how-to","streaming ingestion","apache flink","apache kafka","amazon"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/create-a-low-latency-source-to-data-lake-pipeline-using-amazon-msk-connect-apache-flink-and-apache-hudi/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},18461:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/01/18/Why-and-How-I-Integrated-Airbyte-and-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-01-18-Why-and-How-I-Integrated-Airbyte-and-Apache-Hudi.mdx","source":"@site/blog/2022-01-18-Why-and-How-I-Integrated-Airbyte-and-Apache-Hudi.mdx","title":"Why and How I Integrated Airbyte and Apache Hudi","description":"Redirecting... please wait!!","date":"2022-01-18T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"deltastreamer","permalink":"/blog/tags/deltastreamer"},{"inline":true,"label":"selectfrom","permalink":"/blog/tags/selectfrom"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Harsha Teja Kanna","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Why and How I Integrated Airbyte and Apache Hudi","authors":[{"name":"Harsha Teja Kanna"}],"category":"blog","image":"/assets/images/blog/2022-01-18-airbyte-hudi-integration.png","tags":["how-to","deltastreamer","selectfrom"]},"unlisted":false,"prevItem":{"title":"Hudi powering data lake efforts at Walmart and Disney+ Hotstar","permalink":"/blog/2022/01/20/Hudi-powering-data-lake-efforts-at-Walmart-and-Disney-Hotstar"},"nextItem":{"title":"Change Data Capture with Debezium and Apache Hudi","permalink":"/blog/2022/01/14/change-data-capture-with-debezium-and-apache-hudi"}}')},18773:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(98750),n=t(74848),s=t(28453),r=t(9230);const o={title:"Time travel operations in Hopsworks Feature Store",category:"blog",image:"/assets/images/blog/2021-02-24-featurestore_incremental_pull.png",tags:["use-case","incremental processing","feature store","time travel query","hopsworks"]},l=void 0,d={authorsImageUrls:[]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://examples.hopsworks.ai/master/featurestore/hsfs/time_travel/time_travel_scala/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},18908:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/03/05/hudi-21-unique-differentiators","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-03-05-hudi-21-unique-differentiators.mdx","source":"@site/blog/2025-03-05-hudi-21-unique-differentiators.mdx","title":"21 Unique Reasons Why Apache Hudi Should Be Your Next Data Lakehouse","description":"Apache Hudi is continuously redefining the data lakehouse, pushing the technical boundaries and offering cutting-edge features to handle data quickly and efficiently. If you have ever wondered how Apache Hudi has sustained its position over the years as the most comprehensive, open, high-performance data lakehouse project, this blog aims to give you some concise answers. Below, we shine a light on some unique capabilities in Hudi, that go beyond the lowest-common-denominator across the different projects in the space.","date":"2025-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Data Lake","permalink":"/blog/tags/data-lake"},{"inline":true,"label":"Data Lakehouse","permalink":"/blog/tags/data-lakehouse"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"Apache Iceberg","permalink":"/blog/tags/apache-iceberg"},{"inline":true,"label":"Delta Lake","permalink":"/blog/tags/delta-lake"},{"inline":true,"label":"Table Format","permalink":"/blog/tags/table-format"}],"readingTime":10.6,"hasTruncateMarker":false,"authors":[{"name":"Vinoth Chandar","key":null,"page":null}],"frontMatter":{"title":"21 Unique Reasons Why Apache Hudi Should Be Your Next Data Lakehouse","excerpt":"Unique Differentiators of Apache Hudi, that stand out from other projects","author":"Vinoth Chandar","category":"blog","image":"/assets/images/blog/2025-03-05-21-reasons-why.png","tags":["Data Lake","Data Lakehouse","Apache Hudi","Apache Iceberg","Delta Lake","Table Format"]},"unlisted":false,"prevItem":{"title":"From Transactional Bottlenecks to Lightning-Fast Analytics","permalink":"/blog/2025/03/13/lightning-fast-analytics"},"nextItem":{"title":"Record Mergers in Apache Hudi","permalink":"/blog/2025/03/03/record-mergers-in-hudi"}}')},18949:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-10-02-Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph.mdx","source":"@site/blog/2025-10-02-Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph.mdx","title":"Real-Time Cloud Security Graphs with Apache Hudi and PuppyGraph","description":"CrowdStrike\u2019s 2025 Global Threat Report puts average eCrime breakout time at 48 minutes, with the fastest at 51 seconds. This means that by the time security teams are even alerted about the potential breach, attackers have already long infiltrated the system. And that\u2019s assuming they even get alerted. Cloud environments generate massive amounts of access logs, configuration changes, alerts, and telemetry. Reviewing these events in isolation rarely surfaces patterns like lateral movement or privilege escalation.","date":"2025-10-02T00:00:00.000Z","tags":[{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"PuppyGraph","permalink":"/blog/tags/puppy-graph"},{"inline":true,"label":"security","permalink":"/blog/tags/security"}],"readingTime":10.8,"hasTruncateMarker":false,"authors":[{"name":"Jaz Samantha Ku, in collaboration with Shiyan Xu","key":null,"page":null}],"frontMatter":{"title":"Real-Time Cloud Security Graphs with Apache Hudi and PuppyGraph","excerpt":"Hudi tables support fast upserts and incremental processing. PuppyGraph queries relationships in place using openCypher or Gremlin. In this blog, we explore how to get started with real-time security graph analytics at scale using the data already stored in your Hudi lakehouse tables.","author":"Jaz Samantha Ku, in collaboration with Shiyan Xu","category":"blog","image":"/assets/images/blog/2025-10-02-Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph/fig-4-Sample-Architecture-of-PuppyGraph-Hudi.png","tags":["Apache Hudi","PuppyGraph","security"]},"unlisted":false,"prevItem":{"title":"Modernizing Upstox\'s Data Platform with Apache Hudi, dbt, and EMR Serverless","permalink":"/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless"},"nextItem":{"title":"Automatic Record Key Generation in Apache Hudi","permalink":"/blog/2025/09/17/hudi-auto-gen-keys"}}')},19120:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2020/04/27/apache-hudi-apache-zepplin","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-04-27-apache-hudi-apache-zepplin.md","source":"@site/blog/2020-04-27-apache-hudi-apache-zepplin.md","title":"Apache Hudi Support on Apache Zeppelin","description":"1. Introduction","date":"2020-04-27T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"apache zeppelin","permalink":"/blog/tags/apache-zeppelin"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":2.27,"hasTruncateMarker":true,"authors":[{"name":"leesf","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi Support on Apache Zeppelin","excerpt":"Integrating HUDI\'s real-time and read-optimized query capabilities into Apache Zeppelin\u2019s notebook","author":"leesf","category":"blog","tags":["how-to","apache zeppelin","apache hudi"]},"unlisted":false,"prevItem":{"title":"Monitor Hudi metrics with Datadog","permalink":"/blog/2020/05/28/monitoring-hudi-metrics-with-datadog"},"nextItem":{"title":"Export Hudi datasets as a copy or as different formats","permalink":"/blog/2020/03/22/exporting-hudi-datasets"}}')},19146:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/05/29/lsm-timeline","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-05-29-lsm-timeline.md","source":"@site/blog/2025-05-29-lsm-timeline.md","title":"Exploring Apache Hudi\u2019s New Log-Structured Merge (LSM) Timeline","description":"Apache Hudi 1.0 introduces a new LSM Timeline to scale metadata management for long-lived tables. By restructuring timeline storage into a compacted, versioned tree layout, Hudi enables faster metadata access, snapshot isolation, and support for Non-Blocking Concurrency Control.","date":"2025-05-29T00:00:00.000Z","tags":[{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"LSM Tree","permalink":"/blog/tags/lsm-tree"},{"inline":true,"label":"Performance","permalink":"/blog/tags/performance"},{"inline":true,"label":"Non-Blocking Concurrency Control","permalink":"/blog/tags/non-blocking-concurrency-control"}],"readingTime":11.04,"hasTruncateMarker":false,"authors":[{"name":"Dipankar Mazumdar","key":null,"page":null}],"frontMatter":{"title":"Exploring Apache Hudi\u2019s New Log-Structured Merge (LSM) Timeline","excerpt":"What is the new LSM timeline in Hudi & how is it implemented","author":"Dipankar Mazumdar","category":"blog","image":"/assets/images/blog/lsm-1200x600.jpg","tags":["Apache Hudi","LSM Tree","Performance","Non-Blocking Concurrency Control"]},"unlisted":false,"prevItem":{"title":"Optimizing Apache Hudi Workflows: Automation for Clustering, Resizing & Concurrency","permalink":"/blog/2025/06/13/Optimizing-Apache-Hudi-Workflows-Automation-for-Clustering-Resizing-Concurrency"},"nextItem":{"title":"How Doris + Hudi Turned the Impossible Into the Everyday","permalink":"/blog/2025/04/14/doris-hudi-making-impossible-possible"}}')},19439:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(22266),n=t(74848),s=t(28453);const r={title:"Modernizing Upstox's Data Platform with Apache Hudi, dbt, and EMR Serverless",excerpt:"",author:"The Hudi Community",category:"blog",image:"/assets/images/blog/2025-10-16-Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless/fig1.png",tags:["hudi","upstox","dbt","data lakehouse"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Introduction",id:"introduction",level:2},{value:"Data Sources",id:"data-sources",level:3},{value:"The Challenges with Initial Data Platform",id:"the-challenges-with-initial-data-platform",level:2},{value:"Data Ingestion Issues",id:"data-ingestion-issues",level:3},{value:"Downstream Consumption Struggles",id:"downstream-consumption-struggles",level:3},{value:"The Modern Lakehouse Architecture",id:"the-modern-lakehouse-architecture",level:2},{value:"The Solution: A Modern Stack with Hudi, dbt, and EMR Serverless",id:"the-solution-a-modern-stack-with-hudi-dbt-and-emr-serverless",level:3},{value:"CI/CD and Orchestration",id:"cicd-and-orchestration",level:3},{value:"The Impact",id:"the-impact",level:3},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const a={a:"a",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.h2,{id:"introduction",children:"Introduction"}),"\n",(0,n.jsxs)(a.p,{children:["In ",(0,n.jsx)(a.a,{href:"https://www.youtube.com/watch?v=dAM2zOvnPmw",children:"this community sharing session"}),", Manish Gaurav from Upstox shared insights into the complexities of managing data ingestion at scale. Drawing from the company\u2019s experience as a leading online trading platform in India, the discussion highlighted challenges around file-level upserts, ensuring atomic operations, and handling small files effectively. Upstox shared how they built a modern data platform using Apache Hudi and dbt to address these issues. In this blog post, we\u2019ll break down their solution and why it matters."]}),"\n",(0,n.jsx)(a.p,{children:"Upstox is a leading online trading platform that enables millions of users to invest in equities, commodities, derivatives, and currencies. With over 12 million customers generating 300,000 data requests daily, the company's data team is responsible for delivering the real-time insights that power key products, including:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Search functionality"}),"\n",(0,n.jsx)(a.li,{children:"A customer service chatbot (powered by OpenAI)"}),"\n",(0,n.jsx)(a.li,{children:"Personalized portfolio recommendations"}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{src:t(96766).A+"",width:"1999",height:"1312"})}),"\n",(0,n.jsx)(a.h3,{id:"data-sources",children:"Data Sources"}),"\n",(0,n.jsx)(a.p,{children:"Upstox ingests 250\u2013300 GB of structured and semi-structured data per day from a variety of sources:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Order and transaction data from exchanges"}),"\n",(0,n.jsx)(a.li,{children:"Microservice telemetry from Cloudflare"}),"\n",(0,n.jsx)(a.li,{children:"Customer support data from platforms like Freshdesk and SquadStack"}),"\n",(0,n.jsx)(a.li,{children:"Behavioral analytics from Mixpanel"}),"\n",(0,n.jsx)(a.li,{children:"Data from operational databases (MongoDB, MySQL, and MS SQL) via AWS DMS"}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"the-challenges-with-initial-data-platform",children:"The Challenges with Initial Data Platform"}),"\n",(0,n.jsx)(a.p,{children:"As Upstox grew, so did the complexity of its data operations. Here are some of the early bottlenecks the company faced:"}),"\n",(0,n.jsx)(a.h3,{id:"data-ingestion-issues",children:"Data Ingestion Issues"}),"\n",(0,n.jsx)(a.p,{children:"Prior to 2023, Upstox relied on no-code ingestion platforms like Hevo. While easy to adopt, these platforms introduced several limitations, including high licensing costs and a lack of fine-grained control over ingestion logic. File-level upserts required complex joins between incoming CDC (change data capture) datasets and target tables. Additionally, a lack of atomicity often led to inconsistent data writes, and small-file issues were rampant. To combat these problems, the team had to implement time-consuming re-partitioning and coalescing, along with complex salting strategies to distribute data evenly."}),"\n",(0,n.jsx)(a.h3,{id:"downstream-consumption-struggles",children:"Downstream Consumption Struggles"}),"\n",(0,n.jsx)(a.p,{children:"Analytics queries were primarily served through Amazon Athena, which presented several key limitations. For instance, it frequently timed out when querying large datasets and often exceeded the maximum number of partitions it could handle. Additionally, Athena's lack of support for stored procedures made it challenging to manage and reuse complex query logic. Attempts to improve performance with bucketing often created more small files, and the lack of native support for incremental queries further complicated their analytics workflow."}),"\n",(0,n.jsx)(a.h2,{id:"the-modern-lakehouse-architecture",children:"The Modern Lakehouse Architecture"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{src:t(82517).A+"",width:"1934",height:"1016"})}),"\n",(0,n.jsx)(a.p,{children:"To tackle these problems, Upstox implemented a medallion architecture, organizing data into bronze, silver, and gold layers:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Bronze (Raw Data):"})," Data is ingested and stored in its raw format as Parquet files."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Silver (Cleaned and Filtered):"})," Data is cleaned, filtered, and stored in Apache Hudi tables, which are updated incrementally."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Gold (Business-Ready):"})," Data is aggregated for specific business use cases, modeled with dbt, and stored in Hudi."]}),"\n"]}),"\n",(0,n.jsx)(a.h3,{id:"the-solution-a-modern-stack-with-hudi-dbt-and-emr-serverless",children:"The Solution: A Modern Stack with Hudi, dbt, and EMR Serverless"}),"\n",(0,n.jsx)(a.p,{children:"Upstox re-architected its platform using Apache Hudi as the core data lake technology, dbt for transformations, and EMR Serverless for scalable compute. Airflow was used to orchestrate the entire workflow. Here's how this new stack addressed their challenges:"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Simplified Data Updates:"})," Hudi provides built-in support for record-level upserts with atomic guarantees and snapshot isolation. This helped Upstox overcome the challenge of ensuring consistent updates to their fact and dimension tables."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Improved Upsert Performance:"})," To optimize upsert performance, the team leveraged Bloom index, especially for transaction-heavy fact tables. Indexing strategies were chosen based on data characteristics to balance latency and efficiency."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Resolved Small-File Issues:"})," Small files, which are common in streaming workloads, were mitigated using clustering jobs supported by Hudi. This process was scheduled to run weekly and ensured efficient file sizes and reduced storage overhead without manual intervention."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Enabled Incremental Processing:"})," Incremental joins allowed Upstox to process only new data daily. This enabled timely updates to the aggregated tables in the gold layer that power user-facing dashboards\u2014a task that was not feasible with traditional Athena queries."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Managed Metadata Growth:"})," The accumulation of commit and metadata files in the Hudi table\u2019s `.hoodie/` directory increased S3 listing costs and slowed down operations. Hudi's archival feature helped manage this by archiving older commits after a certain threshold, keeping metadata lean and efficient."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Streamlined Data Modeling:"})," The team used dbt on EMR Serverless to create materialized views over the Hudi datasets. This enabled the creation of efficient transformation layers (silver and gold) using familiar SQL workflows and managed compute."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Flexible Data Materialization:"})," dbt supported a variety of model types, including tables, views, and ephemeral models (Common Table Expressions, or CTEs). This gave teams the flexibility to optimize for performance, reuse, or simplicity, depending on the use case."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Out-of-the-Box Lineage and Documentation:"})," dbt helps visualize how data flows from one table to another, making it easier to debug and understand dependencies. The glossary feature allows teams to document column meanings and transformations clearly."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Enforced Data Quality:"})," With dbt, specific data quality rules can be added to individual tables or pipelines. This adds an extra layer of validation beyond the basic checks performed during data ingestion."]}),"\n",(0,n.jsx)(a.h3,{id:"cicd-and-orchestration",children:"CI/CD and Orchestration"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{src:t(46604).A+"",width:"1932",height:"882"})}),"\n",(0,n.jsx)(a.p,{children:"Upstox uses Apache Airflow for orchestration, with dbt pipelines deployed via a Git-based CI/CD process. Merging a pull request in GitLab triggers the CI/CD pipeline, which automatically builds a new dbt image and publishes the updated data catalog. Airflow then runs the corresponding dbt jobs daily or on-demand, automating the entire transformation workflow."}),"\n",(0,n.jsx)(a.h3,{id:"the-impact",children:"The Impact"}),"\n",(0,n.jsx)(a.p,{children:"The adoption of this modern data stack had a significant impact on Upstox's data platform. The company achieved extremely high data availability and consistency for critical datasets, reducing SLA breaches for complex joins by 70%. Furthermore, pipeline costs dropped by 40%, and query performance improved drastically thanks to Hudi's clustering and optimized joins."}),"\n",(0,n.jsx)(a.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,n.jsx)(a.p,{children:"By leveraging Apache Hudi, dbt, and EMR Serverless, Upstox built a robust and cost-efficient data platform to serve its 12M+ customers, overcoming the significant challenges of data ingestion and analytics at scale. This transformation resolved critical issues like inconsistent data writes, small-file problems, and query timeouts, leading to tangible improvements in both performance and efficiency. With a 70% reduction in SLA breaches and a 40% drop in pipeline costs, the new architecture has empowered their BI and ML teams to move faster. Ultimately, this success story demonstrates how a modern data stack can not only solve immediate technical bottlenecks but also lay the groundwork for a scalable, self-service future that enables continued innovation."})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},19481:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(58646),n=t(74848),s=t(28453);const r={title:"Apply record level changes from relational databases to Amazon S3 data lake using Apache Hudi on Amazon EMR and AWS Database Migration Service",excerpt:"AWS blog showing how to build a CDC pipeline that captures data from an Amazon RDS for MySQL database using AWS DMS and applies those changes to an Amazon S3 dataset using Apache Hudi on Amazon EMR.",author:"aws",category:"blog",image:"/assets/images/blog/2020-10-19-hudi-meets-aws-emr-and-aws-dms.jpeg",tags:["blog","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[];function c(e){const a={a:"a",p:"p",...(0,s.R)(),...e.components};return(0,n.jsxs)(a.p,{children:["This ",(0,n.jsx)(a.a,{href:"https://aws.amazon.com/blogs/big-data/apply-record-level-changes-from-relational-databases-to-amazon-s3-data-lake-using-apache-hudi-on-amazon-emr-and-aws-database-migration-service/",children:"blog"})," published by AWS shows how to build a CDC pipeline that captures data from an Amazon Relational Database Service (Amazon RDS) for MySQL database using AWS Database Migration Service (AWS DMS) and applies those changes to a dataset in Amazon S3 using Apache Hudi on Amazon EMR."]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},19484:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(79802),n=t(74848),s=t(28453),r=t(9230);const o={title:"Change query support in Apache Hudi (0.15)",author:"Jack Vanlightly",category:"blog",image:"/assets/images/blog/2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.png",tags:["blog","apache hudi","CDC","Change Data Capture","jack-vanlightly"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://jack-vanlightly.com/analyses/2024/9/27/change-query-support-in-apache-hudi",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},19635:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/02/09/ACID-transformations-on-Distributed-file-system","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-02-09-ACID-transformations-on-Distributed-file-system.mdx","source":"@site/blog/2022-02-09-ACID-transformations-on-Distributed-file-system.mdx","title":"ACID transformations on Distributed file system","description":"Redirecting... please wait!!","date":"2022-02-09T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"walmartglobaltech","permalink":"/blog/tags/walmartglobaltech"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Rajasekhar","socials":{},"key":null,"page":null}],"frontMatter":{"title":"ACID transformations on Distributed file system","authors":[{"name":"Rajasekhar"}],"category":"blog","image":"/assets/images/blog/2022-02-09-acid-transformations-on-distributed-files-systems.png","tags":["blog","walmartglobaltech"]},"unlisted":false,"prevItem":{"title":"Open Source Data Lake Table Formats: Evaluating Current Interest and Rate of Adoption","permalink":"/blog/2022/02/12/Open-Source-Data-Lake-Table-Formats-Evaluating-Current-Interest-and-Rate-of-Adoption"},"nextItem":{"title":"Onehouse brings a fully-managed lakehouse to Apache Hudi","permalink":"/blog/2022/02/03/Onehouse-brings-a-fully-managed-lakehouse-to-Apache-Hudi"}}')},19640:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/jdpost-image2-205cca47f2d4b91f38bc923e5c937be5.jpg"},19980:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig4-d28ef18dd51d094cfb34e8bae0e65420.png"},19990:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/confluent_deserializer-acede4110283a5d72af7029f3c4a98a6.png"},20086:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(2090),n=t(74848),s=t(28453);const r={title:"Deep Dive Into Hudi\u2019s Indexing Subsystem (Part 1 of 2)",excerpt:"",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2025-10-29-deep-dive-into-hudis-indexing-subsystem-part-1-of-2/fig1.png",tags:["hudi","indexing","data lakehouse","data skipping"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"The Metadata Table",id:"the-metadata-table",level:2},{value:"Multimodal indexing",id:"multimodal-indexing",level:3},{value:"HFile format",id:"hfile-format",level:3},{value:"Default behaviors",id:"default-behaviors",level:3},{value:"Data Skipping with Files, Column Stats, and Partition Stats",id:"data-skipping-with-files-column-stats-and-partition-stats",level:2},{value:"The data skipping process",id:"the-data-skipping-process",level:3},{value:"SQL examples",id:"sql-examples",level:3},{value:"Only the files index",id:"only-the-files-index",level:4},{value:"Enabling column stats",id:"enabling-column-stats",level:4},{value:"Enabling column stats and partition stats",id:"enabling-column-stats-and-partition-stats",level:4},{value:"Configure relevant columns to be indexed",id:"configure-relevant-columns-to-be-indexed",level:3},{value:"Key Takeaways and What&#39;s Next",id:"key-takeaways-and-whats-next",level:2}];function c(e){const a={a:"a",blockquote:"blockquote",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.p,{children:"For decades, databases have relied on indexes\u2014specialized data structures\u2014to dramatically improve read and write performance by quickly locating specific records. Apache Hudi extends this fundamental principle to the data lakehouse with a unique and powerful approach. Every Hudi table contains a self-managed metadata table that functions as an indexing subsystem, enabling efficient data skipping and fast record lookups across a wide range of read and write scenarios."}),"\n",(0,n.jsxs)(a.p,{children:["This two-part series dives into Hudi\u2019s indexing subsystem. Part 1 explains the internal layout and data-skipping capabilities. ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2025/11/12/deep-dive-into-hudis-indexing-subsystem-part-2-of-2/",children:"Part 2"})," covers advanced features\u2014record, secondary, and expression indexes\u2014and asynchronous index maintenance. By the end, you\u2019ll know how to leverage Hudi\u2019s multimodal index to build more efficient lakehouse tables."]}),"\n",(0,n.jsx)(a.h2,{id:"the-metadata-table",children:"The Metadata Table"}),"\n",(0,n.jsx)(a.p,{children:"Within a Hudi table (the data table), the metadata table itself is a Hudi Merge-on-Read (MOR) table. Unlike a typical data table, it features a specialized layout. The table is physically partitioned by index type, with each partition containing the relevant index entries. For its physical storage, the metadata table uses HFile as the base file format. This choice is deliberate: HFile is exceptionally efficient at handling key lookups\u2014the predominant query pattern for indexing. Let\u2019s explore the partitioned layout and HFile\u2019s internal structure."}),"\n",(0,n.jsx)(a.h3,{id:"multimodal-indexing",children:"Multimodal indexing"}),"\n",(0,n.jsx)(a.p,{children:"The metadata table is often referred to as a multimodal index because it houses a diverse range of index types, providing versatile capabilities to accelerate various query patterns. The following diagram illustrates the layout of the metadata table and its relationship with the main data table."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Metadata table and data table layout",src:t(53233).A+"",width:"956",height:"650"})}),"\n",(0,n.jsxs)(a.p,{children:["The metadata table is located in the ",(0,n.jsx)(a.code,{children:".hoodie/metadata/"})," directory under the data table\u2019s base path. It contains partitions for different indexes, such as the files index (under the ",(0,n.jsx)(a.code,{children:"files/"})," partition) for tracking the data table\u2019s partitions and files, and the column stats index (under the ",(0,n.jsx)(a.code,{children:"column_stats/"})," partition) for tracking file-level statistics (e.g., min/max values) for specific columns. Each index partition stores mapping entries tailored to its specific purpose."]}),"\n",(0,n.jsxs)(a.p,{children:["This partitioned design provides great flexibility, allowing you to enable only the indexes that suit your workload. It also ensures extensibility, making it straightforward to support new index types in the future. For example, the ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/rfc/rfc-92/rfc-92.md",children:"bitmap index"})," and the vector search index are on the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/roadmap",children:"roadmap"})," and will be maintained in their own dedicated partitions."]}),"\n",(0,n.jsx)(a.p,{children:"When committing to a data table, the metadata table is updated within the same transactional write. This crucial step ensures that index entries are always synchronized with data table records, upholding data integrity across the table. Therefore, choosing Merge-on-Read (MOR) as the table type for the metadata table is an obvious choice. MOR offers the advantage of absorbing high-frequency write operations, preventing the metadata table\u2019s update process from becoming a bottleneck for overall table writes. To ensure efficient reading, Hudi automatically performs compaction on the metadata table based on its compaction configuration. By default, an inline compaction will be executed every 10 writes to the metadata table, merging accumulated log files with base files to produce a new set of read-optimized base files in HFile format."}),"\n",(0,n.jsx)(a.h3,{id:"hfile-format",children:"HFile format"}),"\n",(0,n.jsxs)(a.p,{children:["The HFile format stores key-value pairs in a sorted, immutable, and block-indexed way, modeled after Google\u2019s SSTable introduced by the ",(0,n.jsx)(a.a,{href:"https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf",children:"Bigtable paper"}),". Here is the description of SSTable quoted from the paper:"]}),"\n",(0,n.jsxs)(a.blockquote,{children:["\n",(0,n.jsx)(a.p,{children:"An SSTable provides a persistent, ordered immutable map from keys to values, where both keys and values are arbitrary byte strings. Operations are provided to look up the value associated with a specified key, and to iterate over all key/value pairs in a specified key range. Internally, each SSTable contains a sequence of blocks (typically each block is 64KB in size, but this is configurable). A block index (stored at the end of the SSTable) is used to locate blocks; the index is loaded into memory when the SSTable is opened. A lookup can be performed with a single disk seek: we first find the appropriate block by performing a binary search in the in-memory index, and then reading the appropriate block from disk."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"As you can tell, by implementing the SSTable, HFile is especially efficient at performing random access, which is the primary query pattern for indexing\u2014given a specific piece of information, like a record key or a partition value, return matching results, such as the file ID that contains the record key, or the list of files that belong to the partition."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"HFile structure",src:t(32122).A+"",width:"647",height:"594"})}),"\n",(0,n.jsx)(a.p,{children:"Because the keys in an HFile are stored in lexicographic order, a batched lookup with a common key prefix is also highly efficient, requiring only a sequential read of nearby keys."}),"\n",(0,n.jsx)(a.h3,{id:"default-behaviors",children:"Default behaviors"}),"\n",(0,n.jsxs)(a.p,{children:["When a Hudi table is created, the metadata table will be enabled with three partitions by default: ",(0,n.jsx)(a.em,{children:"files"}),", ",(0,n.jsx)(a.em,{children:"column stats"}),", and ",(0,n.jsx)(a.em,{children:"partition stats"}),":"]}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Files"}),": stores the list of all partitions and the lists of all base files and log files of each partition, located at the ",(0,n.jsx)(a.code,{children:"files/"})," partition of the metadata table."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Column stats"}),": stores file-level statistics like min, max, value count, and null count for specified columns, located at the ",(0,n.jsx)(a.code,{children:"column_stats/"})," partition of the metadata table."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Partition stats"}),": stores partition-level statistics like min, max, value count, and null count for specified columns, located at the ",(0,n.jsx)(a.code,{children:"partition_stats/"})," partition of the metadata table."]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["By default, when no column is specified for column_stats and partition_stats, Hudi will index the first 32 columns (controlled by ",(0,n.jsx)(a.code,{children:"hoodie.metadata.index.column.stats.max.columns.to.index"}),") available in the table schema."]}),"\n",(0,n.jsx)(a.p,{children:"Whenever a new write is performed on the data table, the metadata table will be updated accordingly. For any available index, new index entries will be upserted to its corresponding partition. For example, if the new write creates a new partition in the data table with some new base files, the files partition will be updated and contain the latest partition and file lists. Similarly, the column stats and partition stats partitions will receive new entries indicating the updated statistics for the new files and partitions."}),"\n",(0,n.jsxs)(a.p,{children:["Note that by design, you cannot disable the files partition, as it is a fundamental index that serves both read and write processes. You can still, although not recommended, disable the entire metadata table by setting ",(0,n.jsx)(a.code,{children:"hoodie.metadata.enable=false"})," during a write."]}),"\n",(0,n.jsx)(a.p,{children:"We will discuss more details about how the default indexes work to improve read and write performance. We will also introduce more indexes supported by the metadata table with usage examples in the following sections."}),"\n",(0,n.jsx)(a.h2,{id:"data-skipping-with-files-column-stats-and-partition-stats",children:"Data Skipping with Files, Column Stats, and Partition Stats"}),"\n",(0,n.jsxs)(a.p,{children:["Data skipping is a core optimization technique that avoids unnecessary data scanning. Its most basic form is physical partitioning, where data is organized into directories based on columns like ",(0,n.jsx)(a.code,{children:"order_date"})," in a customer order table. When a query filters on a partitioned column, the engine uses ",(0,n.jsx)(a.em,{children:"partition pruning"})," to read only the relevant directories. More advanced techniques store lightweight statistics\u2014such as min/max values\u2014for data within each file. The query engine consults this metadata first; if the stats indicate a file cannot contain the required data, the engine skips reading it entirely. This reduction in I/O is a key strategy for accelerating queries and lowering compute costs."]}),"\n",(0,n.jsx)(a.h3,{id:"the-data-skipping-process",children:"The data skipping process"}),"\n",(0,n.jsx)(a.p,{children:"Hudi\u2019s indexing subsystem implements a multi-level skipping strategy using a combination of indexes. Query engines like Spark or Trino can leverage Hudi\u2019s files, partition stats, and column stats indexes to improve performance dramatically. The process, illustrated in the figure below, unfolds in several stages."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Data skipping process flow",src:t(79875).A+"",width:"981",height:"706"})}),"\n",(0,n.jsxs)(a.p,{children:["First, the query engine parses the input SQL and extracts relevant filter predicates, such as ",(0,n.jsx)(a.code,{children:"price >= 300"}),". These predicates are pushed down to Hudi\u2019s integration component, which manages the index lookup process."]}),"\n",(0,n.jsx)(a.p,{children:"The component then consults the files index to get an initial list of partitions. It prunes this list using the partition stats index, which holds partition-level statistics like min/max values. For example, any partition with a maximum price below 300 is skipped entirely."}),"\n",(0,n.jsx)(a.p,{children:"After this initial pruning, the component consults the files index again to retrieve the list of data files within the remaining partitions. This file list is pruned further using the column stats index, which provides the same min/max statistics at the file level."}),"\n",(0,n.jsx)(a.p,{children:"This multi-step process ensures that the query engine reads only the minimum set of files required to satisfy the query, significantly reducing the total amount of data processed."}),"\n",(0,n.jsx)(a.h3,{id:"sql-examples",children:"SQL examples"}),"\n",(0,n.jsx)(a.p,{children:"The following examples demonstrate data skipping in action. We will create a Hudi table and execute Spark SQL queries against it, starting with both partition and column stats disabled to establish a baseline."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"CREATE TABLE orders (\n    order_id STRING,\n    price DECIMAL(12,2),\n    order_status STRING,\n    update_ts BIGINT,\n    shipping_date DATE,\n    shipping_country STRING\n) USING HUDI\nPARTITIONED BY (shipping_country)\nOPTIONS (\n    primaryKey = 'order_id',\n    preCombineField = 'update_ts',\n    hoodie.metadata.index.column.stats.enable = 'false',\n    hoodie.metadata.index.partition.stats.enable = 'false'\n);\n"})}),"\n",(0,n.jsx)(a.p,{children:"And insert some sample data:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"INSERT INTO orders VALUES\n('ORD001', 389.99, 'PENDING',    17495166353, DATE '2023-01-01', 'A'),\n('ORD002', 199.99, 'CONFIRMED',  17495167353, DATE '2023-01-01', 'A'),\n('ORD003', 59.50,  'SHIPPED',    17495168353, DATE '2023-01-11', 'B'),\n('ORD004', 99.00,  'PENDING',    17495169353, DATE '2023-02-09', 'B'),\n('ORD005', 19.99,  'PENDING',    17495170353, DATE '2023-06-12', 'C'),\n('ORD006', 5.99,   'SHIPPED',    17495171353, DATE '2023-07-31', 'C');\n"})}),"\n",(0,n.jsx)(a.h4,{id:"only-the-files-index",children:"Only the files index"}),"\n",(0,n.jsx)(a.p,{children:"With both column stats and partition stats disabled, only the files index is built during the insert operation. We\u2019ll use the SQL below for our test:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"SELECT order_id, price, shipping_country\nFROM orders\nWHERE price > 300;\n"})}),"\n",(0,n.jsx)(a.p,{children:"This query looks for orders with price greater than 300, which only exist in partition 'A' (shipping_country = 'A'). After running the SQL, here's what we see in the Spark UI:"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Spark UI: files index only",src:t(19980).A+"",width:"747",height:"504"})}),"\n",(0,n.jsx)(a.p,{children:"Spark read all 3 partitions and 3 files to find potential matches, but only 1 record from partition A actually satisfied the query condition."}),"\n",(0,n.jsx)(a.h4,{id:"enabling-column-stats",children:"Enabling column stats"}),"\n",(0,n.jsx)(a.p,{children:"Now let's enable column stats while keeping partition stats disabled. Note that we can't do it the other way around\u2014partition stats requires column stats to be enabled first."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"CREATE TABLE orders (\n    order_id STRING,\n    price DECIMAL(12,2),\n    order_status STRING,\n    update_ts BIGINT,\n    shipping_date DATE,\n    shipping_country STRING\n) USING HUDI\nPARTITIONED BY (shipping_country)\nOPTIONS (\n    primaryKey = 'order_id',\n    preCombineField = 'update_ts',\n    hoodie.metadata.index.column.stats.enable = 'true',\n    hoodie.metadata.index.partition.stats.enable = 'false'\n);\n"})}),"\n",(0,n.jsx)(a.p,{children:"Running the same SQL gives us this in the Spark UI:"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Spark UI: column stats enabled",src:t(55893).A+"",width:"692",height:"461"})}),"\n",(0,n.jsx)(a.p,{children:"Now it shows all 3 partitions but only 1 file was scanned. Without partition stats, the query engine couldn't prune partitions, but column stats successfully filtered out the non-matching files. The compute cost of examining those 2 irrelevant partitions and their files could have been avoided with partition stats enabled."}),"\n",(0,n.jsx)(a.h4,{id:"enabling-column-stats-and-partition-stats",children:"Enabling column stats and partition stats"}),"\n",(0,n.jsx)(a.p,{children:"Now let's enable partition stats as well. Since both indexes are enabled by default in Hudi 1.x, we can simply omit those additional configs from the CREATE statement:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"CREATE TABLE orders (\n    order_id STRING,\n    price DECIMAL(12,2),\n    order_status STRING,\n    update_ts BIGINT,\n    shipping_date DATE,\n    shipping_country STRING\n) USING HUDI\nPARTITIONED BY (shipping_country)\nOPTIONS (\n    primaryKey = 'order_id',\n    preCombineField = 'update_ts'\n);\n"})}),"\n",(0,n.jsx)(a.p,{children:"Running the same SQL gives us this in the Spark UI:"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Spark UI: column + partition stats enabled",src:t(70142).A+"",width:"685",height:"457"})}),"\n",(0,n.jsxs)(a.p,{children:["Now we see the full pruning effect happened\u2014only 1 relevant partition and 1 relevant file were scanned, thanks to both indexes working together. ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2025/10/22/Partition_Stats_Enhancing_Column_Stats_in_Hudi_1.0/",children:"This blog"})," shows a 93% reduction in query time running on a 1 TB dataset."]}),"\n",(0,n.jsx)(a.h3,{id:"configure-relevant-columns-to-be-indexed",children:"Configure relevant columns to be indexed"}),"\n",(0,n.jsx)(a.p,{children:"By default, Hudi indexes the first 32 columns for both partition stats and column stats. This limit prevents excessive metadata overhead\u2014each indexed column requires computing min, max, null-count, and value-count statistics for every partition and data file. In most cases, you only need to index a small subset of columns that are frequently used in query predicates. You can specify which columns to be indexed to reduce the maintenance costs:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"CREATE TABLE orders (\n    order_id STRING,\n    price DECIMAL(12,2),\n    order_status STRING,\n    update_ts BIGINT,\n    shipping_date DATE,\n    shipping_country STRING\n) USING HUDI\nPARTITIONED BY (shipping_country)\nOPTIONS (\n    primaryKey = 'order_id',\n    preCombineField = 'update_ts',\n    'hoodie.metadata.index.column.stats.column.list' = 'price,shipping_date'\n);\n"})}),"\n",(0,n.jsxs)(a.p,{children:["The config ",(0,n.jsx)(a.code,{children:"hoodie.metadata.index.column.stats.column.list"})," applies to both partition stats and column stats. By indexing just the ",(0,n.jsx)(a.code,{children:"price"})," and ",(0,n.jsx)(a.code,{children:"shipping_date"})," columns, queries filtering on price comparisons or shipping date ranges will already see significant performance improvements."]}),"\n",(0,n.jsx)(a.h2,{id:"key-takeaways-and-whats-next",children:"Key Takeaways and What's Next"}),"\n",(0,n.jsxs)(a.p,{children:["Hudi\u2019s metadata table is itself a Hudi Merge\u2011on\u2011Read (MOR) table that acts as a multimodal indexing subsystem. It is physically partitioned by index type (for example, ",(0,n.jsx)(a.code,{children:"files/"}),", ",(0,n.jsx)(a.code,{children:"column_stats/"}),", ",(0,n.jsx)(a.code,{children:"partition_stats/"}),") and stores base files in the HFile (SSTable\u2011like) format. This layout provides fast point lookups and efficient batched scans by key prefix\u2014exactly the access patterns indexing needs at lakehouse scale."]}),"\n",(0,n.jsx)(a.p,{children:"Index maintenance happens transactionally alongside data writes, keeping index entries consistent with the data table. Periodic compaction merges log files into read\u2011optimized HFile base files to keep point lookups fast and predictable. On the read path, Hudi composes multiple indexes to minimize I/O: the files index enumerates candidates, partition stats prune irrelevant partitions, and column stats prune non\u2011matching files. In effect, the engine scans only the minimum set of files required to satisfy a query."}),"\n",(0,n.jsxs)(a.p,{children:["In practice, the defaults are a strong starting point. Keep the metadata table enabled and explicitly list only the columns you frequently filter on via ",(0,n.jsx)(a.code,{children:"hoodie.metadata.index.column.stats.column.list"})," to control metadata overhead. In ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2025/11/12/deep-dive-into-hudis-indexing-subsystem-part-2-of-2/",children:"part 2"}),", we\u2019ll go deeper into accelerating equality\u2011matching and expression\u2011based predicates using the record, secondary, and expression indexes, and discuss how asynchronous index maintenance keeps writers unblocked while indexes build in the background."]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},20402:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2019/01/18/asf-incubation","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2019-01-18-asf-incubation.md","source":"@site/blog/2019-01-18-asf-incubation.md","title":"Hudi entered Apache Incubator","description":"In the coming weeks, we will be moving in our new home on the Apache Incubator.","date":"2019-01-18T00:00:00.000Z","tags":[],"readingTime":0.08,"hasTruncateMarker":false,"authors":[{"name":"admin","key":null,"page":null}],"frontMatter":{"title":"Hudi entered Apache Incubator","author":"admin","date":"2019-01-18T00:00:00.000Z","category":"blog"},"unlisted":false,"prevItem":{"title":"Big Batch vs Incremental Processing","permalink":"/blog/2019/03/07/batch-vs-incremental"},"nextItem":{"title":"Hoodie: Uber Engineering\'s Incremental Processing Framework on Hadoop","permalink":"/blog/2017/03/12/Hoodie-Uber-Engineerings-Incremental-Processing-Framework-on-Hadoop"}}')},20828:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/05/16/how-zoom-implemented-streaming-log-ingestion-and-efficient-gdpr-deletes-using-apache-hudi-on-amazon-emr","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-05-16-how-zoom-implemented-streaming-log-ingestion-and-efficient-gdpr-deletes-using-apache-hudi-on-amazon-emr.mdx","source":"@site/blog/2023-05-16-how-zoom-implemented-streaming-log-ingestion-and-efficient-gdpr-deletes-using-apache-hudi-on-amazon-emr.mdx","title":"How Zoom implemented streaming log ingestion and efficient GDPR deletes using Apache Hudi on Amazon EMR","description":"Redirecting... please wait!!","date":"2023-05-16T00:00:00.000Z","tags":[{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"streaming ingestion","permalink":"/blog/tags/streaming-ingestion"},{"inline":true,"label":"gdpr deletion","permalink":"/blog/tags/gdpr-deletion"},{"inline":true,"label":"deletes","permalink":"/blog/tags/deletes"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"Sekar Srinivasan","socials":{},"key":null,"page":null},{"name":"Amit Kumar Agrawal","socials":{},"key":null,"page":null},{"name":"Chandra Dhandapani","socials":{},"key":null,"page":null},{"name":"Viral Shah","socials":{},"key":null,"page":null}],"frontMatter":{"title":"How Zoom implemented streaming log ingestion and efficient GDPR deletes using Apache Hudi on Amazon EMR","authors":[{"name":"Sekar Srinivasan"},{"name":"Amit Kumar Agrawal"},{"name":"Chandra Dhandapani"},{"name":"Viral Shah"}],"category":"blog","image":"/assets/images/blog/2023-05-16-how-zoom-implemented-streaming-log-ingestion-and-efficient-gdpr-deletes-using-apache-hudi-on-amazon-emr.png","tags":["use-case","streaming ingestion","gdpr deletion","deletes","amazon"]},"unlisted":false,"prevItem":{"title":"Hudi Metafields demystified","permalink":"/blog/2023/05/19/hudi-metafields-demystified"},"nextItem":{"title":"Ingesting data to Apache Hudi using Spark sql","permalink":"/blog/2023/05/12/ingesting-data-to-apache-hudi-using-spark-sql"}}')},20880:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/09/06/Lakehouse-or-Warehouse-Part-1-of-2","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-09-06-Lakehouse-or-Warehouse-Part-1-of-2.mdx","source":"@site/blog/2023-09-06-Lakehouse-or-Warehouse-Part-1-of-2.mdx","title":"Lakehouse or Warehouse? Part 1 of 2","description":"Redirecting... please wait!!","date":"2023-09-06T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"onehouse","permalink":"/blog/tags/onehouse"},{"inline":true,"label":"data lakehouse","permalink":"/blog/tags/data-lakehouse"},{"inline":true,"label":"data warehouse","permalink":"/blog/tags/data-warehouse"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Floyd Smith","key":null,"page":null}],"frontMatter":{"title":"Lakehouse or Warehouse? Part 1 of 2","excerpt":"Lakehouse or Warehouse? Part 1 of 2","author":"Floyd Smith","category":"blog","image":"/assets/images/blog/2023-09-06-Lakehouse-or-Warehouse-Part-1-of-2.png","tags":["blog","onehouse","data lakehouse","data warehouse","apache hudi"]},"unlisted":false,"prevItem":{"title":"Apache Hudi: From Zero To One (2/10)","permalink":"/blog/2023/09/06/Apache-Hudi-From-Zero-To-One-blog-2"},"nextItem":{"title":"Incremental Queries with Apache Hudi and Apache Flink","permalink":"/blog/2023/08/31/Incremental-Queries-with-Apache-Hudi-and-Apache-Flink"}}')},20952:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(3865),n=t(74848),s=t(28453),r=t(9230);const o={title:"Storing 200 Billion Entities: Notion\u2019s Data Lake Project",author:"ByteByteGo",category:"blog",image:"/assets/images/blog/2024-11-12-storing-200-billion-entities-notions.jpeg",tags:["blog","apache hudi","use-case","bytebytego"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blog.bytebytego.com/p/storing-200-billion-entities-notions",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},20990:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(85782),n=t(74848),s=t(28453),r=t(9230);const o={title:"Fresher Data Lake on AWS S3",authors:[{name:"Balaji Varadarajan"}],category:"blog",image:"/assets/images/blog/2022-02-17-fresher-data-lake-on-aws-s3.png",tags:["use-case","incremental processing","robinhood"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://robinhood.engineering/author-balaji-varadarajan-e3f496815ebf",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},21063:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/08/12/Use-Flink-Hudi-to-Build-a-Streaming-Data-Lake-Platform","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-08-12-Use-Flink-Hudi-to-Build-a-Streaming-Data-Lake-Platform.mdx","source":"@site/blog/2022-08-12-Use-Flink-Hudi-to-Build-a-Streaming-Data-Lake-Platform.mdx","title":"Use Flink Hudi to Build a Streaming Data Lake Platform","description":"Redirecting... please wait!!","date":"2022-08-12T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache flink","permalink":"/blog/tags/apache-flink"},{"inline":true,"label":"alibabacloud","permalink":"/blog/tags/alibabacloud"},{"inline":true,"label":"streaming ingestion","permalink":"/blog/tags/streaming-ingestion"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Chen Yuzhao","socials":{},"key":null,"page":null},{"name":"Liu Dalong","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Use Flink Hudi to Build a Streaming Data Lake Platform","authors":[{"name":"Chen Yuzhao"},{"name":"Liu Dalong"}],"category":"blog","image":"/assets/images/blog/2022-08-12-Use-Flink-Hudi-to-Build-a-Streaming-Data-Lake-Platform.png","tags":["blog","apache flink","alibabacloud","streaming ingestion"]},"unlisted":false,"prevItem":{"title":"Implementation of SCD-2 (Slowly Changing Dimension) with Apache Hudi & Spark","permalink":"/blog/2022/08/24/Implementation-of-SCD-2-with-Apache-Hudi-and-Spark"},"nextItem":{"title":"How NerdWallet uses AWS and Apache Hudi to build a serverless, real-time analytics platform","permalink":"/blog/2022/08/09/How-NerdWallet-uses-AWS-and-Apache-Hudi-to-build-a-serverless-real-time-analytics-platform"}}')},21064:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2020/12/01/high-perf-data-lake-with-hudi-and-alluxio-t3go","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-12-01-high-perf-data-lake-with-hudi-and-alluxio-t3go.md","source":"@site/blog/2020-12-01-high-perf-data-lake-with-hudi-and-alluxio-t3go.md","title":"Building High-Performance Data Lake Using Apache Hudi and Alluxio at T3Go","description":"Building High-Performance Data Lake Using Apache Hudi and Alluxio at T3Go","date":"2020-12-01T00:00:00.000Z","tags":[{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"near real-time analytics","permalink":"/blog/tags/near-real-time-analytics"},{"inline":true,"label":"incremental processing","permalink":"/blog/tags/incremental-processing"},{"inline":true,"label":"caching","permalink":"/blog/tags/caching"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":8.17,"hasTruncateMarker":true,"authors":[{"name":"t3go","key":null,"page":null}],"frontMatter":{"title":"Building High-Performance Data Lake Using Apache Hudi and Alluxio at T3Go","excerpt":"How T3Go\u2019s high-performance data lake using Apache Hudi and Alluxio shortened the time for data ingestion into the lake by up to a factor of 2. Data analysts using Presto, Hudi, and Alluxio in conjunction to query data on the lake saw queries speed up by 10 times faster.","author":"t3go","category":"blog","image":"/assets/images/blog/2020-12-01-t3go-architecture.png","tags":["use-case","near real-time analytics","incremental processing","caching","apache hudi"]},"unlisted":false,"prevItem":{"title":"Optimize Data lake layout using Clustering in Apache Hudi","permalink":"/blog/2021/01/27/hudi-clustering-intro"},"nextItem":{"title":"Can Big Data Solutions Be Affordable?","permalink":"/blog/2020/11/29/Can-Big-Data-Solutions-Be-Affordable"}}')},21499:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(67411),n=t(74848),s=t(28453),r=t(9230);const o={title:"The story of building a data lake that can be deleted on a record-by-record basis using Apache Hudi",authors:[{name:"Shota Ejima"}],category:"blog",image:"/assets/images/blog/2022-05-25-data-lake-at-yahoo-advertising-at-yahoo-japan.png",tags:["use-case","gdpr deletion","yahoo"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://techblog.yahoo.co.jp/entry/2022052530303179/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},21541:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/06/26/Unlimited-Big-Data-Exchange-A-Wonderful-Review-of-Apache-DolphinScheduler-and-Hudi-Hangzhou-Meetup","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-06-26-Unlimited-Big-Data-Exchange-A-Wonderful-Review-of-Apache-DolphinScheduler-and-Hudi-Hangzhou-Meetup.mdx","source":"@site/blog/2023-06-26-Unlimited-Big-Data-Exchange-A-Wonderful-Review-of-Apache-DolphinScheduler-and-Hudi-Hangzhou-Meetup.mdx","title":"Unlimited Big Data Exchange: A Wonderful Review of Apache DolphinScheduler & Hudi Hangzhou Meetup","description":"Redirecting... please wait!!","date":"2023-06-26T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"Apache DolphinScheduler","permalink":"/blog/tags/apache-dolphin-scheduler"},{"inline":true,"label":"meetup","permalink":"/blog/tags/meetup"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.15,"hasTruncateMarker":false,"authors":[{"name":"Apache DolphinScheduler","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Unlimited Big Data Exchange: A Wonderful Review of Apache DolphinScheduler & Hudi Hangzhou Meetup","authors":[{"name":"Apache DolphinScheduler"}],"category":"blog","image":"/assets/images/blog/2023-06-26-Unlimited-Big-Data-Exchange-A-Wonderful-Review-of-Apache-DolphinScheduler-and-Hudi-Hangzhou-Meetup.jpeg","tags":["blog","Apache DolphinScheduler","meetup","medium"]},"unlisted":false,"prevItem":{"title":"What about Apache Hudi, Apache Iceberg, and Delta Lake?","permalink":"/blog/2023/06/30/What-about-Apache-Hudi-Apache-Iceberg-and-Delta-Lake"},"nextItem":{"title":"Multi-writer support with Apache Hudi","permalink":"/blog/2023/06/24/multi-writer-support-in-apache-hudi"}}')},21582:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/s3-endpoint-list-8d89e05bd7f4d82958a6c11a0cc0c8ea.png"},21873:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(49969),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi: From Zero To One (8/10)",excerpt:"Read and process incrementally",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2024-01-05-Apache-Hudi-From-Zero-To-One-blog-8.png",tags:["hudi","spark","blog","course","tutorial","datumagic","data lake","lakehouse","apache hudi","apache spark"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blog.datumagic.ai/p/apache-hudi-from-zero-to-one-810",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},21967:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(92580),n=t(74848),s=t(28453),r=t(9230);const o={title:"Modern Datalakes with Hudi, MinIO, and HMS",author:"Brenna Buuck",category:"blog",image:"/assets/images/blog/2024-03-14-Modern-Datalakes-with-Hudi--MinIO--and-HMS.jpg",tags:["blog","apache hudi","minio","hms","hive metastore","min"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blog.min.io/datalakes-with-hudi-and-hms/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},22007:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABCUAAAEsCAAAAADRVFYhAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAJcEhZcwAAGJsAABibAUl1g5QAAAAHdElNRQfjDA0UGBIBExTtAAAVdUlEQVR42u3dzbW7KACHYcqgCquwCZqwB2uwBkqwAyvwnFmzHteeMwsXDqD5TrjBJIrJ+5yZe28S5U+I/gJ+ihEAQsTWFQCQOFICQBgpASCMlAAQRkoACCMlAISREgDCSAkAYaQEgDBSAkAYKQEgjJQAEEZKAAgjJQCEkRIAwkgJAGGkBIAwUgJAGCkBIIyUABBGSgAIIyUAhJESAMJICQBhpASAMFICQBgpASCMlAAQRkoACCMlAISREgDCSAkAYaQEgDBSAkAYKQEgjJQAEEZKAAgjJQCEkRKIIZK3dQt9IxoVMbbOAFJiCzQqYqS+FqZev32iUREj9bUw9frtE42KGKmvhanXb59oVMRIfS1MvX77RKMiRuprYer12ycaFTFSXwtTr98+0aiIkfpamHr99olGRYzU18LU67dPNCpiPLEWdpePVHP2qLl49Pfzn6gfotGoiPHHWjjoXAiRlcPxGSP02ev64tGd5407fFIWuruawPRvqR8WeU+jbn1ULsftriXckn1mV/HK/Wjjij1LiUzrQorrNHmQLpH1wzKkBGKEW7IQfuRgCpFNvYnh0AXoh8spzeF54184SwnlfrVSlPPr89P6/OHS+mGZd6XE1u9j3/Xbj2BLdkLMIwUpartm216F9ut3X4qsdX9p/79pM5HVdjL7WwjV3abE2ElhA6Z2rxe9fdkyx4dL64eFSAnECLZkLbL5r1IUdoWXWWFcNtj0KJssO6ZEljWVFMM4SNm0pes13KSELaEde/d6ISqXMqUZjg+X1g8LkRKIEWxJfVyDW7uyG+G6Ay4lGpcBgzylxDilQFe6DoXLi9uUqG1ZpnCvCzmnyOnhwvphIVICMYItWYp6/quzSWBEPk4pUdrRwjiqY0q4TQ5zLnTGZPbV25RobW/EPTZGilNfY364sH5YiJRAjD/6EsX8V+37Eq5n4VJCuU7FnBB6XuPdz6GYNi3fSwn3TK8Om57966eHC+uHhUgJxAi2ZHt81W09mPZLTCnhNmpWNylRuYAY7/clcjsiKYU0bkQyp8Tp4cL6YSFSAjGCLTlIMR0nMbjV/5QS2j+d36RELhrbQ7jbl6jcvlRfXHfsS5weLqwfFiIlECPcklr4AyZM7jY9nFKiFXk3aHGTEoWdrLejjvI6JdpSuERwxbhhRmlHMOX5w6X1wzKkBGL80ZLuwAZ/UMNwnhKDXemzorxJiUaIXMhaiPryCG0xH7xZu8Jy9/qQCdWdHi6uHxYhJRDjr5bsdKGU9uOO6Tyv6WdfulFF7c/qms7s8j/bUul+rLU5nu3VKavQ7XSoZlOouh+1NnZKO+Hp4eL6YQlSAjEWtWTfutgo5LBg3jXqhz+QEoixqCWHXNSmevaErfXrhz+QEoixrCUHfz75Cl0JPumPICUQY2FLDsaYlOuHIFICMVJvydTrt0+kBGKk3pKp12+fSAnESL0lU6/fPpESiJF6S6Zev30iJRAjviWHVjdXl5fSn9spyif9CaQEYsS2ZJdPB1xfXBOblNiZ7VKiN73/f4H5kqqfrR/uiWzJTgqpu7YS8vyDJiV25rMpoeej8xt9mwanE3+sXh/uwNDpv5cg7a999Ib6IVZkS8r5Ay6n69M0pV8g3Mdu/CLhftr/26pyJ4X7V91TbenPBBlqbSK/EPikP+GzKXG4pIi6s1pPi8r8vDmeD1xclVXcuenTVUoUf94XimXnXeJashNiXsn9ZWhsx0LI8vAFYcbpk9SidIOSxp1PmrunGndNKrvkGCkyGXnIJp/0J2ybEkfuhGG/NAxXVywb7nVOL1Ni+Lv/yrLzLnEt2RwWAMeIrHGXnuhuUkI27iTxou9z4a+qXw99Zv9yF9C1RZQx/yKf9CeslxJTv2H+WejBXPYl/HWL/AUTfVm9Vq7/aUrhzi0emlJV05DE2J6DX8IOz91O8nz9ECuuJcvzS0EU/kFjf16nROEL9rflcC8qP2czlnEBsaB+eM56KXF2UVTbo8yy6ny7hI2MzE+bZ9qVVYpMuds7Nbn9o7PzZ7nwlz/LhMwy3zU5PHc9SUz9ECuuJavzm2dMHcrOZsJ1ShwvoD+9UM9/9XYE8jD431I/POfDKZEbL79IiUbk/djIq5So3IWWO1G5lOhEPox9frh4oru7g11m/KC16N0ljszZc1eTxNQPsWJHHGcfiJrGlCK7SQm3EPgvlPMX3A04SinckvCx+uE5H06Jo/OUUP5iqeoqJTr3HVKJTovDhgd3s5fpsmi92x6eiW7+PnLxcXruapKY+iFW9NbLaefWYMeJlf/s3IUt9fGKueU8/rjoS7Tj2Q048ri9pnzSn/DhlMi0l12lRDde7Ql1a3qe2dXcfdG4GHAXNsv9TR38xRO1feiiJvPfR4VbkI7PXU0SUz/EimxJNd/W0+0Jbf3nVM8jjsYPRrLjQuALvrgs5tC4uDDHuwp+on54yhbbJaau501K1KLr/FeLK095800d7BBDaS1tIdPNGKrpyqrzc1eTxNQPseKPqhKqrTI/bhCi7Co5Lwq9yBqjZCAl7HdF3feRl7jik/6EtVOiPN4QrrhOiUGUbttEJc73nPoIUK4cf4+HqRuifNSo8/s+nB7G1A+xYluy90doy9IfQuWOiyjaeThR2acbHUqJXrp52S6xvfVSovX7tTLfaXDLQX6dEmMhZeGmnbLE9g/qfrqNnO8itO4WDX7QOrg7O5yeu5okpn6IFd+SvdHHjUX9dN7XdGD+UA/jcDxM31/L6nTU/vSza5rIQ/j5pD9hvZQYhNRNJo83qhc3KTGt5C4lejtFq9w5Qr2Qpi+FbEt364bOCFHZcYXvkByeu5okpn6IlXpLpl6/fVrx2Mtaiqz1cWCUcDdruU6JUcpxSomxc/vAMnf0g+2Yat9trfyt7V26SDdWPXvuapKI+iFW6i2Zev326bMpYQ6ncE3n7Jipi+me6M46m+M4P911ftqzZ+zvaXpzXuY89eG5m0merh9ipd6Sqddvn7i+BGKk3pKp12+fSAnESL0lU6/fPpESiJF6S6Zev30iJRAj9ZZMvX77REogRuotmXr99omUQIzUWzL1+u0TKYEYqbdk6vXbJ1ICMVJvydTrt0+kBGKk3pKp12+fSAnESL0lU6/fPpESiJF6S6Zev30iJRBDJG/rFvpGpARibJ0BpMQWSAlsxt2rqdi6EvgbKYHN1DYloq59i22QEthMIeThUvxIGSmBzQjRPLwdGxJCSmArxnYk1IJ7gWJtpAS2ov3NANkwkT5SAlvJbD+ie3QLFSSElMBGpoDIGHKkj5TARio/2ND+NsJIGimBjUh/P+GevRzpe1dKpG7rdsa1Zj5UIj/c2gnJIiWwjWxOh4btl8lj9cEmTuGQ0ZlIHSmBTZyygc5E8kgJbOE8GuhMpI6UwAYGeZYMLbs5EkdKYAPVxbmgSshh6xohgJTA+owQ+uxhL/yhE0gVKYHVDZnILjoPWoh260rhMVICqyuu92oMuZBcjSZdpATWVl+ON5xOinzrauEhUgIr68SdPZ+N4NzQdJESWJftNtzbo1HedjCQClICq+ptSHT3Xsg5aiJZpATWNOSPjsceiIlkkRJYUReIAmIiWaQE1tPJUBC4CGHbRIpICazGyHBvwfUm2NORIFICa9HiwYbLIxcTOYdXJSfRlDBbVwDvNighsu6vqUqbJBysnZo0U6JjK9a3aexoo3jizE83XcUZomlJMyUMG7G+S2c7EqJ+blI76pB8SyQlzZRoSIlvMmibEerP0cbF1GbrSuMkzZTQXOLse/RaRvYOfM+DnEgHKYGP6kp3o4PYLQ1N5nKCzZiJSDMlFDei/gp97dZ2qRfs3PQ5Ictnhyn4pERTIs1qIYapcteNyPTCPRbNNHvZcADF1tJcHbM0q4XndEaXyt9STZavjBp6nflSskK3Zus39cvSXB3TvI+LXfh1oRAkT3ddzKr25QMfOq3O7uOYb/3uUqd184k1h5R4ytCW+da3Ot2TXOn3DRSMzeat39CeyKJ+8+acJFPCpHUKcV8dEiJXpUZQY4z5xCbH3pbbbv3mUlepQ5xmLw31rqWaEskcVjXUU0Tk+iMLP/BevakLP+zLqrctsEmmRCVSuXNk7/f2yzcMsIH1dNp/t73ryLQkU8Jt2U5htZwy4q19N2AdXSXflhMppkTj1s3tbwk3VK4bsXRvP7C16QjWN4w7EkyJXgpVbb+Xw53CTEZg1/yRaa+fiJ9gSigh+5s7Sa6tdxuLyQjsnfuyy14dM6eXErW/EIHZdszRyvd01YCN+YHzi+tScimh5/0b1ZaXLLL/uHzumilA6rqXryaaWEr4CxX5N+QulJqZTSrhr9FKRwLfYij/vC5xWFIp4a9SdLg44tRT2qA74a/3zhYJfJFGvnTR4UUpMZgPaHUhLi+gbPyOHH/M79s97oB1Mq3jw4HXvbZUR6dEV2UfPFHl8irLvjvxKQ+OdCck8I1eWq4jU6L/4Ml5qrg9jXBoPnk64J2ND91rXTMgUa/chjUuJWq387UyZut3/DpjfJ/o+qQy15Rst8Q3eny79z/FpITrSHzTnRLcQRGX3Qlue43v1culezoiUmJwRxp91TUIB7e99LzdSkIC38sOp5cd0RyREsWzd2faEdudyE+PmhROMgM+xQhRLJnv+ZRoE7o0zPucv6vuIjKAr6OXfdM/nRLDl65CxWnMkc+HfQLfSi3aOv90ShRfuvH/lH42Z9kHiu9mF/cFl4F7NiX6rxxvOO0cDr0U5dZ1AT6sXTLmeDYlmjSuMfcJ2ZQOSshvfYfA0ZLl/NmUKJZtHN2DSri7kpov3IMD3LCjgug+87Mpob51wOHzwb/Br9w4C1zRQsRupH82Jb54y55xrZbYjYKATxniN8A93ZfY6Iowa5Cqs12JbOtqAKvQ0dsYk7oKzXboSuBn2M5E5OYDUsIr6UrgZ5RCxs1ASjg2XdnBgV/RxW5lJCWcJn6zL7BbeeT2S1LCKVK5ezGwgjpy+yUpYQ1su8Qv6SMXeFJi9Me2c3A2foiKG3KQEqM7RpvjLvFLdNw+PVJidBtzuEQVfomJ21xPSvjNEl97+DlwT9yGCVLCByubJfBT4s7eJCXcfqHIQ9GAnaui9v2TEm5TDkdL4LfEbb4kJVzvi42X+C3TRVWeRUq4S9p97RV2gLv6qItdkxJue6/ZugrAuqIWelKClMAPIiUikRL4OaREJFICP0fGHFZFSoyjUl951zLgsUqREgDehpQAEEZKAAgjJQCEkRIAwkgJAGGkBIAwUgJAGCkBIIyUABBGSgAIezYlBIAvQ0oACHt/Sryp8/KG8lKqyx7Lwz5tt1yREr9XHvaJlFhrXsrDXpESa81LedgrUmKteSkPe0VKrDUv5WGvSIm15qU87BUpsda8lIe9IiXWmpfysFekxFrzUh72ipRYa17Kw16REmvNS3nYK1JirXkpD3tFSqw1L+Vhr0iJtealPOwVKbHWvJSHvSIl1pqX8rBXpMRa81Ie9oqUWGteysNekRJrzUt52CtSYq15KQ97RUqsNS/lYa9IibXmpTzs1fekhGkuHjbmA+/47rxd869+/M+98t6GU6G9nn93by5veKE8/IjwcnC2WI26Pz5hlpZ3MeVbanjSVGN1WIWaZqyaF8t7et5cGTUq/bbyDgYlhCjnB2aashbqneX1mcjM0vLwM0LLgV+s1OHLRpjRLVxubRDLyruaMr6G0xefqe0vMzSD/Ra33+SD6cbBPtV3nainyvZ52Xd93w3N3Kfo6uFeeW9pLSPq/4xPibbxMer+xaHV3VndxuF+XyPY+jZ9hlJUrtxhXqtNXixOiXvllbIrxbCsPPyO0HKQ5+1QucXKr5dTSkjRb5YSjchVM2YyF80oMikrKYpR5VKUUspRq0aUvZ/QZPbrXWs7Sa6knVjZqZvb8t7TWlrof4RNicF+L2fGCPsv1mMhMvfvqrluxv7II1vLCNczsimTZUqaaa0eBr04Je6VZ7XZwvLwOwLLQSfa0X9/z+ulsY+MUmqzlMjrsdGd/e6rilHkgxSmtKun7HNRN6Kz64+vomO/2W1KCN2KrM/K3q4fSt2W96bWEu6LWekm64dMG1uxXI3aDJma61aLriwGc6zck62l59W3lfY7vzys1ctT4n55vawWloffEVgOmvmlw3ppRpcSvf1W3iglfA107hd0m2CqmlZP/5QwNynR2NXCTqZV7W5fKuNr+OS7m1OidP+KMraxbE1aaf8ebc9nqlvmXtNPljdr56GAywWdv54S98tT+bCwPPyOYF9i6r8f1kszupQYtRw2SomsGXvjvgptjYTfFDCnhLqbEsY/bV/tLoben0qJ2g8p3NqnbZTaQYdNCTPXragiyjtwg6S+qF25ung9Je6WV5fs48CfQstBVtuRcKlP66VPiTEvN0qJKm9kM9hxRK4fpISacm0sMnOWEqMs2qy6Le9Kp9x/o+rcfxHvbk6JXmhbvSklhrzUmWhUPdetlo2W3ZPlHd+uKLUdVfly63mfhClz0y9s/TvldbI1i8vbQK+1dhuYGvvbVtvYX8Y+tL9cR21+eGei5q+J+ocT6YcT/ZDgSFYUtRRuvWym9XJKCSM2Somhzm0dujJvBrciV41bq+3PpnLrtv1ZH1Zvo5qq6fzT7lU7i/m7hktTQs3VMKWdz5Vg/8VB1YOqjnUbG3X3SzvcWrbapa23LbcZOt+HaJTVPJ4jtjz9UnnrM25QZ3+7nW/GLaDTOM6N58bjwzsTqb8mMg8nEg8n+iHB99tXedmerZf2T/eVHOr1cuwl5X0OKbGN7zn2cp5YzB/lR95x6mth6uW9ipTYxpelxIfLS6kueyzvVZ0dILnebGV/d9MAzI2XlN8/f3h4Z6Lqr4m6hxOphxP9EFJirXkpD3tFSqw1L+Vhr0iJtealPOwVKbHWvJSHvSIl1pqX8rBXpMRa81Ie9oqUWGteysNekRJrzUt52CtSYq15KQ97RUqsNS/lYa9IibXmpTzsFSmx1ryUh70iJdaal/KwV6TEWvNSHvaKlFhrXsrDXpESa81LedgrUmKteSkPe0VKrDUv5WGvSIm15qU87BUpsda8lIe9IiXWmpfysFekxFrzUh72ipRYa17Kw17tISUAfBdSAkDYu1MCwK8iJQCEkRIAwkgJAGGkBIAwUgJAGCkBIIyUABBGSgAIIyUAhJESAMJICQBhpASAMFICQBgpASCMlAAQRkoACCMlAISREgDCSAkAYaQEgDBSAkAYKQEgjJQAEEZKAAgjJQCEkRIAwkgJAGGkBIAwUgJAGCkBIIyUABBGSgAIIyUAhJESAMJICQBhpASAMFICQBgpASDsf9sljhd5AS0JAAAAJXRFWHRkYXRlOmNyZWF0ZQAyMDE5LTEyLTEzVDIwOjI0OjE4KzAwOjAwJAPM5gAAACV0RVh0ZGF0ZTptb2RpZnkAMjAxOS0xMi0xM1QyMDoyNDoxOCswMDowMFVedFoAAAAASUVORK5CYII="},22115:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/07/15/PayU-built-a-secure-enterprise-AI-assistant","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-07-15-PayU-built-a-secure-enterprise-AI-assistant.mdx","source":"@site/blog/2025-07-15-PayU-built-a-secure-enterprise-AI-assistant.mdx","title":"How PayU built a secure enterprise AI assistant using Amazon Bedrock","description":"Redirecting... please wait!!","date":"2025-07-15T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"AWS","permalink":"/blog/tags/aws"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Deepesh Dhapola, Mudit Chopra, Rahmat Khan, Rahul Ghosh, Saikat Dey, and Sandeep Kumar Veerlapati","key":null,"page":null}],"frontMatter":{"title":"How PayU built a secure enterprise AI assistant using Amazon Bedrock","author":"Deepesh Dhapola, Mudit Chopra, Rahmat Khan, Rahul Ghosh, Saikat Dey, and Sandeep Kumar Veerlapati","category":"blog","tags":["blog","Apache Hudi","AWS"]},"unlisted":false,"prevItem":{"title":"Modernizing Data Infrastructure at Peloton Using Apache Hudi","permalink":"/blog/2025/07/15/modernizing-datainfra-peloton-hudi"},"nextItem":{"title":"Building a RAG-based AI Recommender (Part 1/2)","permalink":"/blog/2025/07/10/building-a-rag-based-ai-recommender"}}')},22153:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(91242),n=t(74848),s=t(28453),r=t(9230);const o={title:"Introduction to Apache Hudi",authors:[{name:"Itamar Syn-Hershko"}],category:"blog",image:"/assets/images/blog/2023-03-17-introduction-to-apache-hudi.png",tags:["how-to","guide","introduction"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://bigdataboutique.com/blog/introduction-to-apache-hudi-c83367",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},22188:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig-3-Architecture-with-Graph-Database-vs-with-PuppyGraph-c6601e01f1e7ae2fcd8ffcc58aec90ac.png"},22266:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-10-16-Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless.md","source":"@site/blog/2025-10-16-Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless.md","title":"Modernizing Upstox\'s Data Platform with Apache Hudi, dbt, and EMR Serverless","description":"Introduction","date":"2025-10-16T00:00:00.000Z","tags":[{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"upstox","permalink":"/blog/tags/upstox"},{"inline":true,"label":"dbt","permalink":"/blog/tags/dbt"},{"inline":true,"label":"data lakehouse","permalink":"/blog/tags/data-lakehouse"}],"readingTime":5.73,"hasTruncateMarker":false,"authors":[{"name":"The Hudi Community","key":null,"page":null}],"frontMatter":{"title":"Modernizing Upstox\'s Data Platform with Apache Hudi, dbt, and EMR Serverless","excerpt":"","author":"The Hudi Community","category":"blog","image":"/assets/images/blog/2025-10-16-Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless/fig1.png","tags":["hudi","upstox","dbt","data lakehouse"]},"unlisted":false,"prevItem":{"title":"Partition Stats: Enhancing Column Stats in Hudi 1.0","permalink":"/blog/2025/10/22/Partition_Stats_Enhancing_Column_Stats_in_Hudi_1.0"},"nextItem":{"title":"Real-Time Cloud Security Graphs with Apache Hudi and PuppyGraph","permalink":"/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph"}}')},22415:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(11896),n=t(74848),s=t(28453),r=t(9230);const o={title:"Corrections in data lakehouse table format comparisons",authors:[{name:"Vinoth Chandar"}],category:"blog",image:"/assets/images/blog/2022-04-19-corrections-in-data-lakehouse-table-format-comparisons.png",tags:["blog","lakehouse","bytearray"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://bytearray.io/corrections-in-data-lakehouse-table-format-comparisons-b72eb63ece32",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},22429:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/2020-05-28-datadog-metrics-demo-fff08d34cd7ef2473f16e9b48dd66793.png"},22606:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(90097),n=t(74848),s=t(28453),r=t(9230);const o={title:"New \u2013 Insert, Update, Delete Data on S3 with Amazon EMR and Apache Hudi",authors:[{name:"Danilo Poccia"}],category:"blog",image:"/assets/images/blog/aws.jpg",tags:["blog","amazon"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/aws/new-insert-update-delete-data-on-s3-with-amazon-emr-and-apache-hudi/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},22667:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/10/20/Its-Time-for-the-Universal-Data-Lakehouse","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-10-20-Its-Time-for-the-Universal-Data-Lakehouse.mdx","source":"@site/blog/2023-10-20-Its-Time-for-the-Universal-Data-Lakehouse.mdx","title":"It\'s Time for the Universal Data Lakehouse","description":"Redirecting... please wait!! s","date":"2023-10-20T00:00:00.000Z","tags":[{"inline":true,"label":"data lakehouse","permalink":"/blog/tags/data-lakehouse"},{"inline":true,"label":"onehouse","permalink":"/blog/tags/onehouse"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"interoperability","permalink":"/blog/tags/interoperability"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Vinoth Chandar","key":null,"page":null}],"frontMatter":{"title":"It\'s Time for the Universal Data Lakehouse","excerpt":"Universal Lakehouse","author":"Vinoth Chandar","category":"blog","image":"/assets/images/blog/2023-10-20-Its-Time-for-the-Universal-Data-Lakehouse.png","tags":["data lakehouse","onehouse","blog","apache hudi","interoperability"]},"unlisted":false,"prevItem":{"title":"Tipico Facilitates Faster Data Access with a Modern Data Strategy on AWS","permalink":"/blog/2023/10/22/Tipico-Facilitates-Faster-Data-Access-with-a-Modern-Data-Strategy-on-AWS"},"nextItem":{"title":"Load data incrementally from transactional data lakes to data warehouses","permalink":"/blog/2023/10/19/load-data-incrementally-from-transactional-data-lakes-to-data-warehouses"}}')},23155:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig-6-Schema-Page-in-PuppyGraph-UI-ff0eb0aac444ad00b098145a9c84fe69.png"},23167:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/04/03/integrate-apache-doris-hudi-data-querying-migration","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-04-03-integrate-apache-doris-hudi-data-querying-migration.mdx","source":"@site/blog/2025-04-03-integrate-apache-doris-hudi-data-querying-migration.mdx","title":"Integrating Apache Doris and Hudi for Data Querying and Migration","description":"Redirecting... please wait!!","date":"2025-04-03T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"Apache Doris","permalink":"/blog/tags/apache-doris"},{"inline":true,"label":"real-time query","permalink":"/blog/tags/real-time-query"},{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"dzone","permalink":"/blog/tags/dzone"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"li yy","key":null,"page":null}],"frontMatter":{"title":"Integrating Apache Doris and Hudi for Data Querying and Migration","author":"li yy","category":"blog","image":"/assets/images/blog/2025-04-03-integrate-apache-doris-hudi-data-querying-migration.png","tags":["blog","Apache Hudi","Apache Doris","real-time query","how-to","dzone"]},"unlisted":false,"prevItem":{"title":" From Swamp to Stream: How Apache Hudi Transforms the Modern Data Lake","permalink":"/blog/2025/04/06/from-swamp-to-stream-how-apache-hudi-transforms-the-modern-data-lake"},"nextItem":{"title":"Introducing Secondary Index in Apache Hudi Lakehouse Platform","permalink":"/blog/2025/04/02/secondary-index"}}')},23176:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/01/11/In-House-Data-Lake-with-CDC-Processing-Hudi-Docker","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-01-11-In-House-Data-Lake-with-CDC-Processing-Hudi-Docker.mdx","source":"@site/blog/2024-01-11-In-House-Data-Lake-with-CDC-Processing-Hudi-Docker.mdx","title":"In-House Data Lake with CDC Processing, Hudi, Docker","description":"Redirecting... please wait!!","date":"2024-01-11T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"},{"inline":true,"label":"intermediate","permalink":"/blog/tags/intermediate"},{"inline":true,"label":"docker","permalink":"/blog/tags/docker"},{"inline":true,"label":"cdc","permalink":"/blog/tags/cdc"},{"inline":true,"label":"apache kafka","permalink":"/blog/tags/apache-kafka"},{"inline":true,"label":"debezium","permalink":"/blog/tags/debezium"},{"inline":true,"label":"apache spark","permalink":"/blog/tags/apache-spark"},{"inline":true,"label":"aws s3","permalink":"/blog/tags/aws-s-3"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Rahul","key":null,"page":null}],"frontMatter":{"title":"In-House Data Lake with CDC Processing, Hudi, Docker","excerpt":"In-House Data Lake with CDC Processing, Hudi, Docker","author":"Rahul","category":"blog","image":"/assets/images/blog/2024-01-11-In-House-Data-Lake-with-CDC-Processing-Hudi-Docker.png","tags":["blog","apache hudi","medium","intermediate","docker","cdc","apache kafka","debezium","apache spark","aws s3"]},"unlisted":false,"prevItem":{"title":"Enforce fine-grained access control on Open Table Formats via Amazon EMR integrated with AWS Lake Formation","permalink":"/blog/2024/01/17/Enforce-fine-grained-access-control-on-Open-Table-Formats-via-Amazon-EMR-integrated-with-AWS-Lake-Formation"},"nextItem":{"title":"Introduction to Apache Hudi","permalink":"/blog/2024/01/09/introduction-to-apache-hudi"}}')},23209:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/Initial_timeline-fd0812aa0c22d797d2192745d103bc41.png"},23483:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(78219),n=t(74848),s=t(28453),r=t(9230);const o={title:"Getting Started: Manage your Hudi tables with the admin Hudi-CLI tool",authors:[{name:"Sivabalan Narayanan"}],category:"blog",image:"/assets/images/blog/2023-02-22-Getting-Started-Manage-your-Hudi-tables-with-the-admin-Hudi-CLI-tool.png",tags:["how-to","hudi cli","onehouse"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.onehouse.ai/blog/getting-started-manage-your-hudi-tables-with-the-admin-hudi-cli-tool",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},23504:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/2024-11-19-automated-small-file-handling-benchmarks-5340e7e5e0e586c3803f6e06796b5daf.png"},23652:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(3239),n=t(74848),s=t(28453),r=t(9230);const o={title:"Building an Amazon Sales Analytics Pipeline with Apache Hudi on Databricks",author:"Sameer Shaik",category:"blog",image:"/assets/images/blog/hudi-aws-dbr.png",tags:["blog","apache hudi","aws","databricks"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.linkedin.com/pulse/building-amazon-sales-analytics-pipeline-apache-hudi-databricks-ruotf/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},23808:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide14-df2e6e49dd6da70d80a106fb39950575.png"},23816:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(8969),n=t(74848),s=t(28453),r=t(9230);const o={title:"How to use Apache Hudi with Databricks",author:"Sagar Lakshmipathy",category:"blog",image:"/assets/images/blog/2024-06-18-how-to-use-apache-hudi-with-databricks.jpeg",tags:["blog","apache hudi","databricks","onehouse"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.onehouse.ai/blog/how-to-use-apache-hudi-with-databricks",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},23860:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(37157),n=t(74848),s=t(28453);const r={title:"Optimize Data lake layout using Clustering in Apache Hudi",excerpt:"Introduce clustering feature to change data layout",author:"satish.kotha",category:"blog",image:"/assets/images/blog/2021-01-27-hudi-clustering-intro.png",tags:["design","clustering","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Background",id:"background",level:2},{value:"Clustering Architecture",id:"clustering-architecture",level:2},{value:"Overall, there are 2 parts to clustering",id:"overall-there-are-2-parts-to-clustering",level:4},{value:"Scheduling clustering",id:"scheduling-clustering",level:4},{value:"Running clustering",id:"running-clustering",level:4},{value:"Setting up clustering",id:"setting-up-clustering",level:4},{value:"Table Query Performance",id:"table-query-performance",level:2},{value:"Before Clustering",id:"before-clustering",level:3},{value:"After Clustering",id:"after-clustering",level:3},{value:"Summary",id:"summary",level:2}];function c(e){const a={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.h2,{id:"background",children:"Background"}),"\n",(0,n.jsxs)(a.p,{children:["Apache Hudi brings stream processing to big data, providing fresh data while being an order of magnitude efficient over traditional batch processing. In a data lake/warehouse, one of the key trade-offs is between ingestion speed and query performance. Data ingestion typically prefers small files to improve parallelism and make data available to queries as soon as possible. However, query performance degrades poorly with a lot of small files. Also, during ingestion, data is typically co-located based on arrival time. However, the query engines perform better when the data frequently queried is co-located together. In most architectures each of these systems tend to add optimizations independently to improve performance which hits limitations due to un-optimized data layouts. This blog introduces a new kind of table service called clustering ",(0,n.jsx)(a.a,{href:"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+19+Clustering+data+for+freshness+and+query+performance",children:"[RFC-19]"})," to reorganize data for improved query performance without compromising on ingestion speed."]}),"\n",(0,n.jsx)(a.h2,{id:"clustering-architecture",children:"Clustering Architecture"}),"\n",(0,n.jsxs)(a.p,{children:["At a high level, Hudi provides different operations such as insert/upsert/bulk_insert through it\u2019s write client API to be able to write data to a Hudi table. To be able to choose a trade-off between file size and ingestion speed, Hudi provides a knob ",(0,n.jsx)(a.code,{children:"hoodie.parquet.small.file.limit"})," to be able to configure the smallest allowable file size. Users are able to configure the small file ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/configurations#hoodieparquetsmallfilelimit",children:"soft limit"})," to ",(0,n.jsx)(a.code,{children:"0"})," to force new data to go into a new set of filegroups or set it to a higher value to ensure new data gets \u201cpadded\u201d to existing files until it meets that limit that adds to ingestion latencies."]}),"\n",(0,n.jsx)(a.p,{children:"To be able to support an architecture that allows for fast ingestion without compromising query performance, we have introduced a \u2018clustering\u2019 service to rewrite the data to optimize Hudi data lake file layout."}),"\n",(0,n.jsx)(a.p,{children:"Clustering table service can run asynchronously or synchronously adding a new action type called \u201cREPLACE\u201d, that will mark the clustering action in the Hudi metadata timeline."}),"\n",(0,n.jsx)(a.h4,{id:"overall-there-are-2-parts-to-clustering",children:"Overall, there are 2 parts to clustering"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsx)(a.li,{children:"Scheduling clustering: Create a clustering plan using a pluggable clustering strategy."}),"\n",(0,n.jsx)(a.li,{children:"Execute clustering: Process the plan using an execution strategy to create new files and replace old files."}),"\n"]}),"\n",(0,n.jsx)(a.h4,{id:"scheduling-clustering",children:"Scheduling clustering"}),"\n",(0,n.jsx)(a.p,{children:"Following steps are followed to schedule clustering."}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsx)(a.li,{children:"Identify files that are eligible for clustering: Depending on the clustering strategy chosen, the scheduling logic will identify the files eligible for clustering."}),"\n",(0,n.jsx)(a.li,{children:"Group files that are eligible for clustering based on specific criteria. Each group is expected to have data size in multiples of \u2018targetFileSize\u2019. Grouping is done as part of \u2018strategy\u2019 defined in the plan. Additionally, there is an option to put a cap on group size to improve parallelism and avoid shuffling large amounts of data."}),"\n",(0,n.jsxs)(a.li,{children:["Finally, the clustering plan is saved to the timeline in an avro ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/hudi-common/src/main/avro/HoodieClusteringPlan.avsc",children:"metadata format"}),"."]}),"\n"]}),"\n",(0,n.jsx)(a.h4,{id:"running-clustering",children:"Running clustering"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsx)(a.li,{children:"Read the clustering plan and get the \u2018clusteringGroups\u2019 that mark the file groups that need to be clustered."}),"\n",(0,n.jsx)(a.li,{children:"For each group, we instantiate appropriate strategy class with strategyParams (example: sortColumns) and apply that strategy to rewrite the data."}),"\n",(0,n.jsxs)(a.li,{children:["Create a \u201cREPLACE\u201d commit and update the metadata in ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceCommitMetadata.java",children:"HoodieReplaceCommitMetadata"}),"."]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Clustering Service builds on Hudi\u2019s MVCC based design to allow for writers to continue to insert new data while clustering action runs in the background to reformat data layout, ensuring snapshot isolation between concurrent readers and writers."}),"\n",(0,n.jsx)(a.p,{children:"NOTE: Clustering can only be scheduled for tables / partitions not receiving any concurrent updates. In the future, concurrent updates use-case will be supported as well."}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{alt:"Clustering example",src:t(94367).A+"",width:"1200",height:"600"}),"\n",(0,n.jsx)(a.em,{children:"Figure: Illustrating query performance improvements by clustering"})]}),"\n",(0,n.jsx)(a.h4,{id:"setting-up-clustering",children:"Setting up clustering"}),"\n",(0,n.jsx)(a.p,{children:"Inline clustering can be setup easily using spark dataframe options. See sample below"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-scala",children:'import org.apache.hudi.QuickstartUtils._\nimport scala.collection.JavaConversions._\nimport org.apache.spark.sql.SaveMode._\nimport org.apache.hudi.DataSourceReadOptions._\nimport org.apache.hudi.DataSourceWriteOptions._\nimport org.apache.hudi.config.HoodieWriteConfig._\n\n\nval df =  //generate data frame\ndf.write.format("org.apache.hudi").\n        options(getQuickstartWriteConfigs).\n        option(PRECOMBINE_FIELD_OPT_KEY, "ts").\n        option(RECORDKEY_FIELD_OPT_KEY, "uuid").\n        option(PARTITIONPATH_FIELD_OPT_KEY, "partitionpath").\n        option(TABLE_NAME, "tableName").\n        option("hoodie.parquet.small.file.limit", "0").\n        option("hoodie.clustering.inline", "true").\n        option("hoodie.clustering.inline.max.commits", "4").\n        option("hoodie.clustering.plan.strategy.target.file.max.bytes", "1073741824").\n        option("hoodie.clustering.plan.strategy.small.file.limit", "629145600").\n        option("hoodie.clustering.plan.strategy.sort.columns", "column1,column2"). //optional, if sorting is needed as part of rewriting data\n        mode(Append).\n        save("dfs://location");\n'})}),"\n",(0,n.jsxs)(a.p,{children:["For more advanced usecases, async clustering pipeline can also be setup. See an example ",(0,n.jsx)(a.a,{href:"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+19+Clustering+data+for+freshness+and+query+performance#RFC19Clusteringdataforfreshnessandqueryperformance-SetupforAsyncclusteringJob",children:"here"}),"."]}),"\n",(0,n.jsx)(a.h2,{id:"table-query-performance",children:"Table Query Performance"}),"\n",(0,n.jsx)(a.p,{children:"We created a dataset from one partition of a known production style table with ~20M records and on-disk size of ~200GB. The dataset has rows for multiple \u201csessions\u201d. Users always query this data using a predicate on session. Data for a single session is spread across multiple data files because ingestion groups data based on arrival time. The below experiment shows that by clustering on session, we are able to improve the data locality and reduce query execution time by more than 50%."}),"\n",(0,n.jsx)(a.p,{children:"Query:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-scala",children:'spark.sql("select  *  from table where session_id=123")\n'})}),"\n",(0,n.jsx)(a.h3,{id:"before-clustering",children:"Before Clustering"}),"\n",(0,n.jsx)(a.p,{children:"Query took 2.2 minutes to complete. Note that the number of output rows in the \u201cscan parquet\u201d part of the query plan includes all 20M rows in the table."}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{alt:"Query Plan Before Clustering",src:t(24451).A+"",width:"1600",height:"658"}),"\n",(0,n.jsx)(a.em,{children:"Figure: Spark SQL query details before clustering"})]}),"\n",(0,n.jsx)(a.h3,{id:"after-clustering",children:"After Clustering"}),"\n",(0,n.jsx)(a.p,{children:"The query plan is similar to above. But, because of improved data locality and predicate push down, spark is able to prune a lot of rows. After clustering, the same query only outputs 110K rows (out of 20M rows) while scanning parquet files. This cuts query time to less than a minute from 2.2 minutes."}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{alt:"Query Plan Before Clustering",src:t(23938).A+"",width:"1600",height:"661"}),"\n",(0,n.jsx)(a.em,{children:"Figure: Spark SQL query details after clustering"})]}),"\n",(0,n.jsx)(a.p,{children:"The table below summarizes query performance improvements from experiments run using Spark3"}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{children:"Table State"}),(0,n.jsx)(a.th,{children:"Query runtime"}),(0,n.jsx)(a.th,{children:"Num Records Processed"}),(0,n.jsx)(a.th,{children:"Num files on disk"}),(0,n.jsx)(a.th,{children:"Size of each file"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.strong,{children:"Unclustered"})}),(0,n.jsx)(a.td,{children:"130,673 ms"}),(0,n.jsx)(a.td,{children:"~20M"}),(0,n.jsx)(a.td,{children:"13642"}),(0,n.jsx)(a.td,{children:"~150 MB"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.strong,{children:"Clustered"})}),(0,n.jsx)(a.td,{children:"55,963 ms"}),(0,n.jsx)(a.td,{children:"~110K"}),(0,n.jsx)(a.td,{children:"294"}),(0,n.jsx)(a.td,{children:"~600 MB"})]})]})]}),"\n",(0,n.jsxs)(a.p,{children:["Query runtime is reduced by 60% after clustering. Similar results were observed on other sample datasets. See example query plans and more details at the ",(0,n.jsx)(a.a,{href:"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+19+Clustering+data+for+freshness+and+query+performance#RFC19Clusteringdataforfreshnessandqueryperformance-PerformanceEvaluation",children:"RFC-19 performance evaluation"}),"."]}),"\n",(0,n.jsx)(a.p,{children:"We expect dramatic speedup for large tables, where the query runtime is almost entirely dominated by actual I/O and not query planning, unlike the example above."}),"\n",(0,n.jsx)(a.h2,{id:"summary",children:"Summary"}),"\n",(0,n.jsx)(a.p,{children:"Using clustering, we can improve query performance by"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsxs)(a.li,{children:["Leveraging concepts such as ",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Z-order_curve",children:"space filling curves"})," to adapt data lake layout and reduce the amount of data read during queries."]}),"\n",(0,n.jsx)(a.li,{children:"Stitch small files into larger ones and reduce the total number of files that need to be scanned by the query engine."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Clustering also enables stream processing over big data. Ingestion can write small files to satisfy latency requirements of stream processing. Clustering can be used in the background to stitch these small files into larger files and reduce file count."}),"\n",(0,n.jsx)(a.p,{children:"Besides this, the clustering framework also provides the flexibility to asynchronously rewrite data based on specific requirements. We foresee many other use-cases adopting clustering framework with custom pluggable strategies to satisfy on-demand data lake management activities. Some such notable use-cases that are actively being solved using clustering:"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsx)(a.li,{children:"Rewrite data and encrypt data at rest."}),"\n",(0,n.jsx)(a.li,{children:"Prune unused columns from tables and reduce storage footprint."}),"\n"]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},23938:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/Query_Plan_After_Clustering-e669cd3887eaadb0f9d23cd4a773535e.png"},23960:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/12/04/use-open-table-format-libraries-on-aws-glue-5-0-for-apache-spark","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-12-04-use-open-table-format-libraries-on-aws-glue-5-0-for-apache-spark.mdx","source":"@site/blog/2024-12-04-use-open-table-format-libraries-on-aws-glue-5-0-for-apache-spark.mdx","title":"Use open table format libraries on AWS Glue 5.0 for Apache Spark","description":"Redirecting... please wait!!","date":"2024-12-04T00:00:00.000Z","tags":[{"inline":true,"label":"announcement","permalink":"/blog/tags/announcement"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"aws glue","permalink":"/blog/tags/aws-glue"},{"inline":true,"label":"apache spark","permalink":"/blog/tags/apache-spark"},{"inline":true,"label":"table format","permalink":"/blog/tags/table-format"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.16,"hasTruncateMarker":false,"authors":[{"name":"Sotaro Hikita and  Noritaka Sekiyama","key":null,"page":null}],"frontMatter":{"title":"Use open table format libraries on AWS Glue 5.0 for Apache Spark","author":"Sotaro Hikita and  Noritaka Sekiyama","category":"blog","image":"/assets/images/blog/2024-12-04-use-open-table-format-libraries-on-aws-glue-5-0-for-apache-spark.png","tags":["announcement","blog","apache hudi","aws glue","apache spark","table format","amazon"]},"unlisted":false,"prevItem":{"title":"Introducing Hudi\'s Non-blocking Concurrency Control for streaming, high-frequency writes","permalink":"/blog/2024/12/06/non-blocking-concurrency-control"},"nextItem":{"title":"Apache Iceberg vs Hudi: Key Features, Performance & Use Cases","permalink":"/blog/2024/12/03/apache-iceberg-vs-apache-hudi"}}')},24375:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(62766),n=t(74848),s=t(28453),r=t(9230);const o={title:"Bulk Insert Sort Modes with Apache Hudi",authors:[{name:"Sivabalan Narayanan"}],category:"blog",tags:["blog","bulk-insert","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@simpsons/bulk-insert-sort-modes-with-apache-hudi-c781e77841bc",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},24451:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/Query_Plan_Before_Clustering-cbf67c7e66765c7b42ad88521739d39f.png"},24672:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/11/16/How-GE-Aviation-built-cloud-native-data-pipelines-at-enterprise-scale-using-the-AWS-platform","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-11-16-How-GE-Aviation-built-cloud-native-data-pipelines-at-enterprise-scale-using-the-AWS-platform.mdx","source":"@site/blog/2021-11-16-How-GE-Aviation-built-cloud-native-data-pipelines-at-enterprise-scale-using-the-AWS-platform.mdx","title":"How GE Aviation built cloud-native data pipelines at enterprise scale using the AWS platform","description":"Redirecting... please wait!!","date":"2021-11-16T00:00:00.000Z","tags":[{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"analytics at scale","permalink":"/blog/tags/analytics-at-scale"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"Alcuin Weidus","socials":{},"key":null,"page":null},{"name":"Suresh Patnam","socials":{},"key":null,"page":null}],"frontMatter":{"title":"How GE Aviation built cloud-native data pipelines at enterprise scale using the AWS platform","authors":[{"name":"Alcuin Weidus"},{"name":"Suresh Patnam"}],"category":"blog","image":"/assets/images/blog/2021-11-16-ge-aviation-cloud-native-data-pipelines.png","tags":["use-case","analytics at scale","amazon"]},"unlisted":false,"prevItem":{"title":"Apache Hudi Architecture Tools and Best Practices","permalink":"/blog/2021/11/22/Apache-Hudi-Architecture-Tools-and-Best-Practices"},"nextItem":{"title":"Practice of Apache Hudi in building real-time data lake at station B","permalink":"/blog/2021/10/21/Practice-of-Apache-Hudi-in-building-real-time-data-lake-at-station-B"}}')},25101:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/09/22/hands-on-with-apache-hudi-and-spark","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-09-22-hands-on-with-apache-hudi-and-spark.mdx","source":"@site/blog/2024-09-22-hands-on-with-apache-hudi-and-spark.mdx","title":"Hands-on with Apache Hudi and Spark","description":"Redirecting... please wait!!","date":"2024-09-22T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"Apache Spark","permalink":"/blog/tags/apache-spark"},{"inline":true,"label":"devgenius","permalink":"/blog/tags/devgenius"}],"readingTime":0.11,"hasTruncateMarker":false,"authors":[{"name":"Sanjeet Shukla","key":null,"page":null}],"frontMatter":{"title":"Hands-on with Apache Hudi and Spark","author":"Sanjeet Shukla","category":"blog","image":"/assets/images/blog/2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.png","tags":["blog","Apache Hudi","Apache Spark","devgenius"]},"unlisted":false,"prevItem":{"title":"Hudi, Iceberg and Delta Lake: Data Lake Table Formats Compared","permalink":"/blog/2024/09/24/hudi-iceberg-and-delta-lake-data-lake-table-formats-compared"},"nextItem":{"title":"How Apache Hudi transformed Yuno\u2019s data lake","permalink":"/blog/2024/09/17/how-apache-hudi-transformed-yuno-s-data-lake"}}')},25245:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(69342),n=t(74848),s=t(28453);const r={title:"Reliable ingestion from AWS S3 using Hudi",excerpt:"From listing to log-based approach, a reliable way of ingesting data from AWS S3 into Hudi.",author:"codope",category:"blog",image:"/assets/images/blog/s3_events_source_design.png",tags:["design","deltastreamer","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Design",id:"design",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Configuration and Setup",id:"configuration-and-setup",level:2},{value:"Conclusion and Future Work",id:"conclusion-and-future-work",level:2}];function c(e){const a={a:"a",code:"code",em:"em",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(a.p,{children:["In this post we will talk about a new deltastreamer source which reliably and efficiently processes new data files as they arrive in AWS S3.\nAs of today, to ingest data from S3 into Hudi, users leverage DFS source whose ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/178767948e906f673d6d4a357c65c11bc574f619/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DFSPathSelector.java",children:"path selector"})," would identify the source files modified since the last checkpoint based on max modification time.\nThe problem with this approach is that modification time precision is upto seconds in S3. It maybe possible that there were many files (beyond what the configurable source limit allows) modifed in that second and some files might be skipped.\nFor more details, please refer to ",(0,n.jsx)(a.a,{href:"https://issues.apache.org/jira/browse/HUDI-1723",children:"HUDI-1723"}),".\nWhile the workaround is to ignore the source limit and keep reading, the problem motivated us to redesign so that users can reliably ingest from S3."]}),"\n",(0,n.jsx)(a.h2,{id:"design",children:"Design"}),"\n",(0,n.jsxs)(a.p,{children:["For use-cases where seconds granularity does not suffice, we have a new source in deltastreamer using log-based approach.\nThe new ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/178767948e906f673d6d4a357c65c11bc574f619/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/S3EventsSource.java",children:"S3 events source"})," relies on change notification and incremental processing to ingest from S3.\nThe architecture is as shown in the figure below."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Different components in the design",src:t(13288).A+"",width:"1200",height:"600"})}),"\n",(0,n.jsxs)(a.p,{children:["In this approach, users need to ",(0,n.jsx)(a.a,{href:"https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html",children:"enable S3 event notifications"}),".\nThere will be two types of deltastreamers as detailed below."]}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/178767948e906f673d6d4a357c65c11bc574f619/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/S3EventsSource.java",children:"S3EventsSource"}),": Create Hudi S3 metadata table.\nThis source leverages AWS ",(0,n.jsx)(a.a,{href:"https://aws.amazon.com/sns",children:"SNS"})," and ",(0,n.jsx)(a.a,{href:"https://aws.amazon.com/sqs/",children:"SQS"})," services that subscribe to file events from the source bucket.","\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Events from SQS will be written to this table, which serves as a changelog for the subsequent incremental puller."}),"\n",(0,n.jsx)(a.li,{children:"When the events are committed to the S3 metadata table they will be deleted from SQS."}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/178767948e906f673d6d4a357c65c11bc574f619/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/S3EventsHoodieIncrSource.java",children:"S3EventsHoodieIncrSource"})," and uses the metadata table written by S3EventsSource.","\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Read the S3 metadata table and get the objects that were added or modified. These objects contain the S3 path for the source files that were added or modified."}),"\n",(0,n.jsx)(a.li,{children:"Write to Hudi table with source data corresponding to the source files in the S3 bucket."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"advantages",children:"Advantages"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Decoupling"}),": Every step in the pipeline is decoupled. The two sources can be started independent of each other. We imagine most users run a single deltastreamer to get all changes for a given bucket and can fan-out multiple tables off that."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Performance and Scale"}),": The previous approach used to list all files, sort by modification time and then filter based on checkpoint. While it did prune partition paths, the directory listing could still become a bottleneck. By relying on change notification and native cloud APIs, the new approach avoids directory listing and scales with the number of files being ingested."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Reliability"}),": Since there is no longer any dependency on the max modification time and the fact that S3 events are being recorded in the metadata table, users can rest assured that all the events will be processed eventually."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Fault Tolerance"}),": There are two levels of fault toerance in this design. Firstly, if some of the messages are not committed to the S3 metadata table, then those messages will remain in the queue so that they can be reprocessed in the next round. Secondly, if the incremental puller fails, then users can query the S3 metadata table for the last commit point and resume the incremental puller from that point onwards (kinda like how Kafka consumers can reset offset)."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Asynchronous backfills"}),': With the log-based approach, it becomes much easier to trigger backfills. See the "Conclusion and Future Work" section for more details.']}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"configuration-and-setup",children:"Configuration and Setup"}),"\n",(0,n.jsx)(a.p,{children:"Users only need to specify the SQS queue url and region name to start the S3EventsSource (metadata source)."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.s3.source.queue.url=https://sqs.us-west-2.amazonaws.com/queue/url\nhoodie.deltastreamer.s3.source.queue.region=us-west-2\n"})}),"\n",(0,n.jsx)(a.p,{children:"There are a few other configurations for the metadata source which can be tuned to suit specific requirements:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.em,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.s3.source.queue.long.poll.wait"})}),": Value can range in [0, 20] seconds. If set to 0 then metadata source will consume messages from SQS using short polling. It is recommended to use long polling because it will reduce false empty responses and reduce the cost of using SQS. By default, this value is set to 20 seconds."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.em,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.s3.source.queue.visibility.timeout"})}),": Value can range in [0, 43200] seconds (i.e. max 12 hours). SQS does not automatically delete the messages once consumed. It is the responsibility of metadata source to delete the message after committing. SQS will move the consumed message to in-flight state during which it becomes invisible for the configured timeout period. By default, this value is set to 30 seconds."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.em,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.s3.source.queue.max.messages.per.batch"})}),": Maximum number of messages in a batch of one round of metadata source run. By default, this value is set to 5."]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["To setup the pipeline, first ",(0,n.jsx)(a.a,{href:"https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html",children:"enable S3 event notifications"}),".\nDownload the ",(0,n.jsx)(a.a,{href:"https://mvnrepository.com/artifact/com.amazonaws/aws-java-sdk-sqs",children:"aws-java-sdk-sqs"})," jar.\nThen start the S3EventsSource and  S3EventsHoodieIncrSource using the HoodieDeltaStreamer utility as shown in sample commands below."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:'# To start S3EventsSource\nspark-submit \\\n--jars "/home/hadoop/hudi-utilities-bundle_2.11-0.9.0.jar,/usr/lib/spark/external/lib/spark-avro.jar,/home/hadoop/aws-java-sdk-sqs-1.12.22.jar" \\\n--master yarn --deploy-mode client \\\n--class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer /home/hadoop/hudi-packages/hudi-utilities-bundle_2.11-0.9.0-SNAPSHOT.jar \\\n--table-type COPY_ON_WRITE --source-ordering-field eventTime \\\n--target-base-path s3://bucket_name/path/for/s3_meta_table \\\n--target-table s3_meta_table  --continuous \\\n--min-sync-interval-seconds 10 \\\n--hoodie-conf hoodie.datasource.write.recordkey.field="s3.object.key,eventName" \\\n--hoodie-conf hoodie.datasource.write.keygenerator.class=org.apache.hudi.keygen.ComplexKeyGenerator \\\n--hoodie-conf hoodie.datasource.write.partitionpath.field=s3.bucket.name --enable-hive-sync \\\n--hoodie-conf hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor \\\n--hoodie-conf hoodie.datasource.write.hive_style_partitioning=true \\\n--hoodie-conf hoodie.datasource.hive_sync.database=default \\\n--hoodie-conf hoodie.datasource.hive_sync.table=s3_meta_table \\\n--hoodie-conf hoodie.datasource.hive_sync.partition_fields=bucket \\\n--source-class org.apache.hudi.utilities.sources.S3EventsSource \\\n--hoodie-conf hoodie.deltastreamer.s3.source.queue.url=https://sqs.us-west-2.amazonaws.com/queue/url\n--hoodie-conf hoodie.deltastreamer.s3.source.queue.region=us-west-2\n\n# To start S3EventsHoodieIncrSource use following command along with ordering field, record key(s) and \n# partition field(s) from the source s3 data.\nspark-submit \\\n--jars "/home/hadoop/hudi-utilities-bundle_2.11-0.9.0.jar,/usr/lib/spark/external/lib/spark-avro.jar,/home/hadoop/aws-java-sdk-sqs-1.12.22.jar" \\\n--master yarn --deploy-mode client \\\n--class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer /home/hadoop/hudi-packages/hudi-utilities-bundle_2.11-0.9.0-SNAPSHOT.jar \\\n--table-type COPY_ON_WRITE \\\n--source-ordering-field <ordering key from source data> --target-base-path s3://bucket_name/path/for/s3_hudi_table \\\n--target-table s3_hudi_table  --continuous --min-sync-interval-seconds 10 \\\n--hoodie-conf hoodie.datasource.write.recordkey.field="<record key from source data>" \\\n--hoodie-conf hoodie.datasource.write.keygenerator.class=org.apache.hudi.keygen.SimpleKeyGenerator \\\n--hoodie-conf hoodie.datasource.write.partitionpath.field=<partition key from source data> --enable-hive-sync \\\n--hoodie-conf hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor \\\n--hoodie-conf hoodie.datasource.write.hive_style_partitioning=true \\\n--hoodie-conf hoodie.datasource.hive_sync.database=default \\\n--hoodie-conf hoodie.datasource.hive_sync.table=s3_hudi_v6 \\\n--hoodie-conf hoodie.datasource.hive_sync.partition_fields=bucket \\\n--source-class org.apache.hudi.utilities.sources.S3EventsHoodieIncrSource \\\n--hoodie-conf hoodie.deltastreamer.source.hoodieincr.path=s3://bucket_name/path/for/s3_meta_table \\\n--hoodie-conf hoodie.deltastreamer.source.hoodieincr.read_latest_on_missing_ckpt=true\n'})}),"\n",(0,n.jsx)(a.h2,{id:"conclusion-and-future-work",children:"Conclusion and Future Work"}),"\n",(0,n.jsx)(a.p,{children:"This post introduced a log-based approach to ingest data from S3 into Hudi tables reliably and efficiently. We are actively improving this along the following directions."}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"One stream of work is to add support for other cloud-based object storage like Google Cloud Storage, Azure Blob Storage, etc. with this revamped design."}),"\n",(0,n.jsx)(a.li,{children:"Another stream of work is to add resource manager that allows users to setup notifications and delete resources when no longer needed."}),"\n",(0,n.jsxs)(a.li,{children:["Another interesting piece of work is to support ",(0,n.jsx)(a.strong,{children:"asynchronous backfills"}),". Notification systems are evntually consistent and typically do not guarantee perfect delivery of all files right away. The log-based approach provides enough flexibility to trigger automatic backfills at a configurable interval e.g. once a day or once a week."]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["Please follow this ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/issues/14794",children:"GitHub issue"})," to learn more about active development on this issue.\nWe look forward to contributions from the community. Hope you enjoyed this post."]}),"\n",(0,n.jsx)(a.p,{children:"Put your Hudi on and keep streaming!"})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},25423:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/01/18/Deleting-Items-from-Apache-Hudi-using-Delta-Streamer-in-UPSERT-Mode-with-Kafka-Avro-Messages","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-01-18-Deleting-Items-from-Apache-Hudi-using-Delta-Streamer-in-UPSERT-Mode-with-Kafka-Avro-Messages.mdx","source":"@site/blog/2024-01-18-Deleting-Items-from-Apache-Hudi-using-Delta-Streamer-in-UPSERT-Mode-with-Kafka-Avro-Messages.mdx","title":"Deleting Items from Apache Hudi using Delta Streamer in UPSERT Mode with Kafka Avro Messages","description":"Redirecting... please wait!!","date":"2024-01-18T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"linkedin","permalink":"/blog/tags/linkedin"},{"inline":true,"label":"beginner","permalink":"/blog/tags/beginner"},{"inline":true,"label":"hudi streamer","permalink":"/blog/tags/hudi-streamer"},{"inline":true,"label":"deltastreamer","permalink":"/blog/tags/deltastreamer"},{"inline":true,"label":"apache kafka","permalink":"/blog/tags/apache-kafka"},{"inline":true,"label":"apache avro","permalink":"/blog/tags/apache-avro"},{"inline":true,"label":"upsert","permalink":"/blog/tags/upsert"},{"inline":true,"label":"delete","permalink":"/blog/tags/delete"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"Soumil Shah","key":null,"page":null}],"frontMatter":{"title":"Deleting Items from Apache Hudi using Delta Streamer in UPSERT Mode with Kafka Avro Messages","excerpt":"Deleting Items from Apache Hudi using Delta Streamer in UPSERT Mode with Kafka Avro Messages","author":"Soumil Shah","category":"blog","image":"/assets/images/blog/2024-01-18-Deleting-Items-from-Apache-Hudi-using-Delta-Streamer-in-UPSERT-Mode-with-Kafka-Avro-Messages.png","tags":["blog","apache hudi","linkedin","beginner","hudi streamer","deltastreamer","apache kafka","apache avro","upsert","delete"]},"unlisted":false,"prevItem":{"title":"Learn How to Move Data From MongoDB to Apache Hudi Using PySpark","permalink":"/blog/2024/01/20/Learn-How-to-Move-Data-From-MongoDB-to-Apache-Hudi-Using-PySpark"},"nextItem":{"title":"Enforce fine-grained access control on Open Table Formats via Amazon EMR integrated with AWS Lake Formation","permalink":"/blog/2024/01/17/Enforce-fine-grained-access-control-on-Open-Table-Formats-via-Amazon-EMR-integrated-with-AWS-Lake-Formation"}}')},25521:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/07/02/Hudi-Best-Practices-Handling-Failed-Inserts-Upserts-with-Error-Tables","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-07-02-Hudi-Best-Practices-Handling-Failed-Inserts-Upserts-with-Error-Tables.mdx","source":"@site/blog/2023-07-02-Hudi-Best-Practices-Handling-Failed-Inserts-Upserts-with-Error-Tables.mdx","title":"Hudi Best Practices: Handling Failed Inserts/Upserts with Error Tables","description":"Redirecting... please wait!!","date":"2023-07-02T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"linkedin","permalink":"/blog/tags/linkedin"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"inserts","permalink":"/blog/tags/inserts"},{"inline":true,"label":"upserts","permalink":"/blog/tags/upserts"}],"readingTime":0.16,"hasTruncateMarker":false,"authors":[{"name":"Soumil Shah","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Hudi Best Practices: Handling Failed Inserts/Upserts with Error Tables","authors":[{"name":"Soumil Shah"}],"category":"blog","image":"/assets/images/blog/2023-07-02-Hudi-Best-Practices-Handling-Failed-Inserts-Upserts-with-Error-Tables.png","tags":["blog","linkedin","apache hudi","inserts","upserts"]},"unlisted":false,"prevItem":{"title":"Skip rocks and files: Turbocharge Trino queries with Hudi\u2019s multi-modal indexing subsystem","permalink":"/blog/2023/07/07/Skip-rocks-and-files-Turbocharge-Trino-queries-with-Hudi-multi-modal-indexing-subsystem"},"nextItem":{"title":"Monitoring Table Size stats","permalink":"/blog/2023/07/01/monitoring-table-size-stats"}}')},25642:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/01/17/Enforce-fine-grained-access-control-on-Open-Table-Formats-via-Amazon-EMR-integrated-with-AWS-Lake-Formation","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-01-17-Enforce-fine-grained-access-control-on-Open-Table-Formats-via-Amazon-EMR-integrated-with-AWS-Lake-Formation.mdx","source":"@site/blog/2024-01-17-Enforce-fine-grained-access-control-on-Open-Table-Formats-via-Amazon-EMR-integrated-with-AWS-Lake-Formation.mdx","title":"Enforce fine-grained access control on Open Table Formats via Amazon EMR integrated with AWS Lake Formation","description":"Redirecting... please wait!!","date":"2024-01-17T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"aws","permalink":"/blog/tags/aws"},{"inline":true,"label":"intermediate","permalink":"/blog/tags/intermediate"},{"inline":true,"label":"amazon emr","permalink":"/blog/tags/amazon-emr"},{"inline":true,"label":"aws lake formation","permalink":"/blog/tags/aws-lake-formation"},{"inline":true,"label":"aws glue","permalink":"/blog/tags/aws-glue"},{"inline":true,"label":"aws s3","permalink":"/blog/tags/aws-s-3"},{"inline":true,"label":"amazon sagemaker","permalink":"/blog/tags/amazon-sagemaker"},{"inline":true,"label":"aws cloud9","permalink":"/blog/tags/aws-cloud-9"},{"inline":true,"label":"amazon athena","permalink":"/blog/tags/amazon-athena"},{"inline":true,"label":"access control","permalink":"/blog/tags/access-control"}],"readingTime":0.18,"hasTruncateMarker":false,"authors":[{"name":"Raymond Lai, Aditya Shah, Bin Wang, and Melody Yang","key":null,"page":null}],"frontMatter":{"title":"Enforce fine-grained access control on Open Table Formats via Amazon EMR integrated with AWS Lake Formation","excerpt":"Enforce fine-grained access control on Open Table Formats via Amazon EMR integrated with AWS Lake Formation","author":"Raymond Lai, Aditya Shah, Bin Wang, and Melody Yang","category":"blog","image":"/assets/images/blog/2024-01-17-Enforce-fine-grained-access-control-on-Open-Table-Formats-via-Amazon-EMR-integrated-with-AWS-Lake-Formation.png","tags":["blog","apache hudi","aws","intermediate","amazon emr","aws lake formation","aws glue","aws s3","amazon sagemaker","aws cloud9","amazon athena","access control"]},"unlisted":false,"prevItem":{"title":"Deleting Items from Apache Hudi using Delta Streamer in UPSERT Mode with Kafka Avro Messages","permalink":"/blog/2024/01/18/Deleting-Items-from-Apache-Hudi-using-Delta-Streamer-in-UPSERT-Mode-with-Kafka-Avro-Messages"},"nextItem":{"title":"In-House Data Lake with CDC Processing, Hudi, Docker","permalink":"/blog/2024/01/11/In-House-Data-Lake-with-CDC-Processing-Hudi-Docker"}}')},25673:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/05/19/apache-hudi-on-aws-glue","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-05-19-apache-hudi-on-aws-glue.mdx","source":"@site/blog/2024-05-19-apache-hudi-on-aws-glue.mdx","title":"Apache Hudi on AWS Glue","description":"Redirecting... please wait!!","date":"2024-05-19T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"aws glue","permalink":"/blog/tags/aws-glue"},{"inline":true,"label":"dev to","permalink":"/blog/tags/dev-to"}],"readingTime":0.11,"hasTruncateMarker":false,"authors":[{"name":"Sagar Lakshmipathy","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi on AWS Glue","author":"Sagar Lakshmipathy","category":"blog","image":"/assets/images/blog/2024-05-19-apache-hudi-on-aws-glue.png","tags":["blog","apache hudi","aws glue","dev to"]},"unlisted":false,"prevItem":{"title":"Use AWS Data Exchange to seamlessly share Apache Hudi datasets","permalink":"/blog/2024/05/22/use-aws-data-exchange-to-seamlessly-share-apache-hudi-datasets"},"nextItem":{"title":"Building Analytical Apps on the Lakehouse using Apache Hudi, Daft & Streamlit","permalink":"/blog/2024/05/10/building-analytical-apps-on-the-lakehouse-using-apache-hudi-daft-streamlit"}}')},25678:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(61815),n=t(74848),s=t(28453);const r={title:"How FreeWheel Uses Apache Hudi to Power Its Data Lakehouse",excerpt:"How FreeWheel unified batch and streaming with an Apache Hudi\u2013powered lakehouse to improve freshness, simplify operations, and scale analytics.",author:"The Hudi Community",category:"blog",image:"/assets/images/blog/2025-11-07-how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse/image1.png",tags:["hudi","lakehouse","case-study","freewheel"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Data freshness issues",id:"data-freshness-issues",level:2},{value:"Complex ingestion",id:"complex-ingestion",level:2},{value:"Query performance bottlenecks",id:"query-performance-bottlenecks",level:2},{value:"Use Case 1: Lambda Architecture and Its Drawbacks",id:"use-case-1-lambda-architecture-and-its-drawbacks",level:2},{value:"Use Case 2: Real-Time Inventory Management",id:"use-case-2-real-time-inventory-management",level:2},{value:"Use Case 3: Scalable Audience Data Processing",id:"use-case-3-scalable-audience-data-processing",level:2},{value:"Hudi in practice 1: Billion\u2011scale updates for audience\u2011segment ingestion",id:"hudi-in-practice-1-billionscale-updates-for-audiencesegment-ingestion",level:2},{value:"Use case overview",id:"use-case-overview",level:3},{value:"Key architecture and design principles",id:"key-architecture-and-design-principles",level:3},{value:"<strong>Partitioning and orchestration</strong>",id:"partitioning-and-orchestration",level:4},{value:"<strong>Decoupled ingestion pipeline</strong>",id:"decoupled-ingestion-pipeline",level:4},{value:"Challenges of input data at scale",id:"challenges-of-input-data-at-scale",level:3},{value:"Metrics and performance insights",id:"metrics-and-performance-insights",level:3},{value:"<strong>Operational optimizations</strong>",id:"operational-optimizations",level:3},{value:"Hudi in practice 2: Real\u2011time aggregated ingestion using Spark Streaming + clustering",id:"hudi-in-practice-2-realtime-aggregated-ingestion-using-spark-streaming--clustering",level:2},{value:"Pipeline overview",id:"pipeline-overview",level:3},{value:"Data ingestion flow",id:"data-ingestion-flow",level:3},{value:"Results at a glance",id:"results-at-a-glance",level:3},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const a={a:"a",blockquote:"blockquote",em:"em",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Talk title slide",src:t(68284).A+"",width:"1257",height:"437"})}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsxs)(a.em,{children:["This post summarizes a FreeWheel talk from the Apache Hudi Community Sync. Watch the recording on ",(0,n.jsx)(a.a,{href:"https://www.youtube.com/watch?v=hQNSf82o3Rk",children:"YouTube"}),"."]})}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.a,{href:"https://www.freewheel.com/",children:"FreeWheel"}),", a division of Comcast, provides advanced video advertising solutions across TV and digital platforms. As the business scaled, FreeWheel faced growing challenges maintaining consistency, freshness, and operational efficiency in its data systems. To address these challenges, the team began transitioning from a legacy Lambda architecture to a modern, ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/",children:"Apache Hudi"}),"-powered lakehouse approach."]}),"\n",(0,n.jsxs)(a.p,{children:["Their original stack, shown below, used multiple systems like ",(0,n.jsx)(a.strong,{children:"Presto"}),", ",(0,n.jsx)(a.strong,{children:"ClickHouse"}),", and ",(0,n.jsx)(a.strong,{children:"Druid"})," to serve analytical and real-time use cases. However, the architecture had some limitations:"]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Original multi-engine architecture",src:t(10039).A+"",width:"1999",height:"848"})}),"\n",(0,n.jsx)(a.h2,{id:"data-freshness-issues",children:"Data freshness issues"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Presto tables had a 3\u20134 hour delay, which was too slow for operational use cases."}),"\n",(0,n.jsx)(a.li,{children:"Only ClickHouse and Druid offered near\u2011real\u2011time access (~5 minutes) but added complexity."}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"complex-ingestion",children:"Complex ingestion"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Data came from logs, CDC streams, files, and databases."}),"\n",(0,n.jsx)(a.li,{children:"Each system had its own ingestion pipeline and refresh logic."}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"query-performance-bottlenecks",children:"Query performance bottlenecks"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"With ~15 PB of data and 20M+ queries/day, scaling across three engines was costly and hard to operate."}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"use-case-1-lambda-architecture-and-its-drawbacks",children:"Use Case 1: Lambda Architecture and Its Drawbacks"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Lambda architecture overview",src:t(80814).A+"",width:"1999",height:"857"})}),"\n",(0,n.jsx)(a.p,{children:"FreeWheel initially followed a traditional Lambda architecture, which separated the processing of batch and real\u2011time data. This approach created several problems: it required duplicate pipelines for batch and real\u2011time processing (leading to inefficient engineering workflows), and it struggled to scale ClickHouse for large aggregates."}),"\n",(0,n.jsxs)(a.p,{children:["By consolidating on Hudi as the table format for both streaming and historical data, FreeWheel unified the storage layer and eliminated duplicate pipelines. Hudi\u2019s ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/write_operations/#upsert",children:"upserts"})," by key and ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/table_types/#incremental-queries",children:"incremental processing"})," make it possible to serve near\u2013real\u2011time analytics. The result is simpler operations, consistent logic, and a platform that scales with data volume and query complexity."]}),"\n",(0,n.jsx)(a.h2,{id:"use-case-2-real-time-inventory-management",children:"Use Case 2: Real-Time Inventory Management"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Real-time inventory with Hudi",src:t(98529).A+"",width:"1999",height:"1621"})}),"\n",(0,n.jsx)(a.p,{children:"Historically, daily ad inventory updates were a significant challenge. This method led to low forecasting accuracy and frequent delivery-performance mismatches."}),"\n",(0,n.jsx)(a.p,{children:"By modernizing the platform with Hudi, FreeWheel updates inventory within minutes. Order changes are applied as upserts to Hudi tables and become queryable shortly thereafter, dramatically improving forecast accuracy and reducing delivery\u2011vs\u2011forecast mismatches."}),"\n",(0,n.jsx)(a.h2,{id:"use-case-3-scalable-audience-data-processing",children:"Use Case 3: Scalable Audience Data Processing"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Audience data architecture with Hudi snapshot",src:t(77400).A+"",width:"941",height:"746"})}),"\n",(0,n.jsx)(a.p,{children:"FreeWheel uses Aerospike to ingest audience segments for its online services, which involves handling high\u2011frequency, real\u2011time data. However, this setup brought a few key challenges\u2014chiefly, the need for analytical insights on top of real\u2011time data and the need to efficiently manage bulk loads alongside frequent updates."}),"\n",(0,n.jsxs)(a.p,{children:["To address these challenges, FreeWheel introduced Hudi into the data pipeline. Hudi maintains a snapshot table for all audience data, enabling more flexible and efficient data management. It supports ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/write_operations/#bulk_insert",children:"bulk inserts"}),", ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/write_operations/#upsert",children:"upserts"}),", and change data capture (CDC), enabling smoother handling of updates and large\u2011scale data loads. Using CDC, large batches of audience updates are applied incrementally to the snapshot. With Hudi in place, the back\u2011end analytics system became much stronger, while the responsiveness of the online systems was preserved. This setup also improved the stability of the online targeting system, as heavy analytical workloads were moved off the key\u2011value store, reducing pressure on Aerospike and enhancing overall performance."]}),"\n",(0,n.jsx)(a.h2,{id:"hudi-in-practice-1-billionscale-updates-for-audiencesegment-ingestion",children:"Hudi in practice 1: Billion\u2011scale updates for audience\u2011segment ingestion"}),"\n",(0,n.jsx)(a.h3,{id:"use-case-overview",children:"Use case overview"}),"\n",(0,n.jsx)(a.p,{children:"This implementation showcases how a large\u2011scale platform ingests and updates audience\u2011segmentation data at the billion\u2011record scale using Hudi tables.\nThe architecture efficiently handles high\u2011frequency updates across more than 63,000 partitions and a table over 600 TB, with performance optimizations at both the data and infrastructure levels."}),"\n",(0,n.jsx)(a.h3,{id:"key-architecture-and-design-principles",children:"Key architecture and design principles"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Audience ingestion architecture and scheduler",src:t(45536).A+"",width:"1333",height:"1360"})}),"\n",(0,n.jsx)(a.h4,{id:"partitioning-and-orchestration",children:(0,n.jsx)(a.strong,{children:"Partitioning and orchestration"})}),"\n",(0,n.jsxs)(a.p,{children:["FreeWheel uses the audience\u2011segment ID as the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/key_generation",children:"partition key"}),". Each partition can be processed independently, allowing many Spark jobs to run in parallel. Each job upserts data to the Hudi lakehouse table."]}),"\n",(0,n.jsx)(a.p,{children:"A central scheduler allocates work based on input size, priority, and write concurrency limits. This enables dynamic scaling across more than 63,000 partitions, where per-partition input sizes range from 1 million to 100 billion records."}),"\n",(0,n.jsx)(a.h4,{id:"decoupled-ingestion-pipeline",children:(0,n.jsx)(a.strong,{children:"Decoupled ingestion pipeline"})}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["Scheduler: allocates resources based on input size and supports job priority, ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/concurrency_control/",children:"multi-writer concurrency control"}),", and concurrency planning."]}),"\n",(0,n.jsx)(a.li,{children:"Ingestion job: Spark jobs process data and write it to the Hudi segment table in the lakehouse."}),"\n"]}),"\n",(0,n.jsx)(a.h3,{id:"challenges-of-input-data-at-scale",children:"Challenges of input data at scale"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Table size: over 600 TB."}),"\n",(0,n.jsx)(a.li,{children:"Partition count: 63,000 audience\u2011segment partitions."}),"\n",(0,n.jsx)(a.li,{children:"Data skew: massive variation in partition sizes, ranging from 1 million to 100 billion records."}),"\n"]}),"\n",(0,n.jsx)(a.h3,{id:"metrics-and-performance-insights",children:"Metrics and performance insights"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Ingestion metrics and throughput",src:t(67946).A+"",width:"1999",height:"979"})}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["Cost optimization","\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Unit cost on AWS: ~$0.10 per million records updated."}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.li,{children:"Throughput: the pipeline supports up to 12 million upserts per second."}),"\n"]}),"\n",(0,n.jsx)(a.h3,{id:"operational-optimizations",children:(0,n.jsx)(a.strong,{children:"Operational optimizations"})}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Handle S3 throttling by increasing partition parallelism. Hash partition prefixes and coordinate with AWS to raise per\u2011bucket request caps and remove I/O bottlenecks."}),"\n",(0,n.jsx)(a.li,{children:"Balance SLA and cost with adaptive resource provisioning through the scheduler; choose resources based on input size to keep jobs stable while controlling spend."}),"\n",(0,n.jsx)(a.li,{children:"Deduplicate before commit: group by record key, order by event timestamp, and write only the latest value to reduce churn and speed up writes."}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"hudi-in-practice-2-realtime-aggregated-ingestion-using-spark-streaming--clustering",children:"Hudi in practice 2: Real\u2011time aggregated ingestion using Spark Streaming + clustering"}),"\n",(0,n.jsx)(a.h3,{id:"pipeline-overview",children:"Pipeline overview"}),"\n",(0,n.jsxs)(a.p,{children:["This implementation showcases an efficient pipeline where Spark Streaming ingests aggregated data into a Hudi lakehouse using the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/write_operations/#bulk_insert",children:"bulk_insert"})," operation, followed by ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2021/08/23/async-clustering/",children:"asynchronous clustering"}),"."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Streaming ingestion and clustering flow",src:t(44541).A+"",width:"1003",height:"445"})}),"\n",(0,n.jsx)(a.h3,{id:"data-ingestion-flow",children:"Data ingestion flow"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Kafka: raw events are streamed into Kafka."}),"\n",(0,n.jsx)(a.li,{children:"Spark SQL on Streaming: consumes Kafka messages and performs near\u2011real\u2011time aggregations."}),"\n",(0,n.jsxs)(a.li,{children:["bulk_insert into Hudi lakehouse: aggregated data is appended using ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/write_operations/#bulk_insert",children:"bulk_insert"}),"."]}),"\n",(0,n.jsx)(a.li,{children:"Clustering plan generation: clustering plans are created asynchronously."}),"\n",(0,n.jsxs)(a.li,{children:["HoodieClusteringJob: a cron job runs hourly to execute ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/clustering/",children:"clustering"})," and consolidate small files."]}),"\n"]}),"\n",(0,n.jsx)(a.h3,{id:"results-at-a-glance",children:"Results at a glance"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Massive file reduction: clustering reduced total file count by nearly 90%, minimizing small\u2011file pressure and improving metadata performance."}),"\n",(0,n.jsx)(a.li,{children:"Write throughput boost: increased by about 114% due to optimized file layout."}),"\n",(0,n.jsx)(a.li,{children:"Faster queries: Presto query performance improved significantly after clustering."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"However, Spark Streaming is a macro\u2011batch system, typically executing every one or two minutes. As a result, it does not trigger clustering jobs immediately but instead generates clustering plans for later execution. In production, clustering jobs are scheduled to run hourly and apply only to stable partitions, ensuring compaction and file optimization without impacting real\u2011time ingestion."}),"\n",(0,n.jsx)(a.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,n.jsx)(a.p,{children:"FreeWheel\u2019s journey with Hudi transformed its data architecture\u2014offering unified access, real\u2011time freshness, and scalable operations. The team credits Hudi\u2019s community and feature set as key to its success."}),"\n",(0,n.jsxs)(a.blockquote,{children:["\n",(0,n.jsx)(a.p,{children:"\u201cWe\u2019re lucky to choose Hudi as our Lakehouse. Thanks to the powerful Hudi community!\u201d \u2013 Bing Jiang"}),"\n"]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},25881:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(61616),n=t(74848),s=t(28453),r=t(9230);const o={title:"Run Apache Hudi at scale on AWS",authors:[{name:"Imtiaz Sayed,"},{name:"Shana Schipers"},{name:"Dylan Qu"},{name:"Carlos Rodrigues"},{name:"Arun A K"},{name:"Francisco Morillo"}],category:"technical guide",image:"/assets/images/blog/run-hudi-at-scale-on-aws.png",tags:["aws","guide","apache hudi"]},l=void 0,d={authorsImageUrls:[void 0,void 0,void 0,void 0,void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://pages.awscloud.com/GLOBAL-devadopt-DL-Apache-Hudi-Technical-Guide-2023-learn.html?sc_channel=sm&sc_campaign=DB_Blog&sc_publisher=LINKEDIN&sc_geo=GLOBAL&sc_outcome=awareness&trk=DB_Blog&linkId=205888417/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},26123:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/jdpost-image5-07e8b4eb07ad352e0410e07c0aab1f61.jpg"},26161:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(61536),n=t(74848),s=t(28453),r=t(9230);const o={title:"Real-Time Data Processing with Postgres, Debezium, Kafka, Schema Registry, and Delta Streamer Guide for Begineers",excerpt:"Real-Time Data Processing with Postgres, Debezium, Kafka, Schema Registry, and Delta Streamer Guide for Begineers",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2023-11-26-Real-Time-Data-Processing-with-Postgres-Debezium-Kafka-Schema-Registry-and-DeltaStreamer-Guide-for-Begineers.png",tags:["apache hudi","postgres","how-to","debezium","apache kafka","deltastreamer","linkedin"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.linkedin.com/pulse/real-time-data-processing-postgres-debezium-kafka-schema-soumil-shah-li19e/?utm_source=share&utm_medium=member_ios&utm_campaign=share_via",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},26311:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/07/15/modernizing-datainfra-peloton-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-07-15-modernizing-datainfra-peloton-hudi.md","source":"@site/blog/2025-07-15-modernizing-datainfra-peloton-hudi.md","title":"Modernizing Data Infrastructure at Peloton Using Apache Hudi","description":"Peloton re-architected its data platform using Apache Hudi to overcome snapshot delays, rigid service coupling, and high operational costs. By adopting CDC-based ingestion from PostgreSQL and DynamoDB, moving from CoW to MoR tables, and leveraging asynchronous services with fine-grained schema control, Peloton achieved 10-minute ingestion cycles, reduced compute/storage overhead, and enabled time travel and GDPR compliance.","date":"2025-07-15T00:00:00.000Z","tags":[{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"Peloton","permalink":"/blog/tags/peloton"},{"inline":true,"label":"Community","permalink":"/blog/tags/community"}],"readingTime":8.91,"hasTruncateMarker":false,"authors":[{"name":"Amaresh Bingumalla, Thinh Kenny Vu, Gabriel Wang, Arun Vasudevan in collaboration with Dipankar Mazumdar","key":null,"page":null}],"frontMatter":{"title":"Modernizing Data Infrastructure at Peloton Using Apache Hudi","excerpt":"How Peloton\'s Data Platform team scaled their data infrastructure using Hudi","author":"Amaresh Bingumalla, Thinh Kenny Vu, Gabriel Wang, Arun Vasudevan in collaboration with Dipankar Mazumdar","category":"blog","image":"/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/peloton-1200x600.jpg","tags":["Apache Hudi","Peloton","Community"]},"unlisted":false,"prevItem":{"title":"A Deep Dive on Merge-on-Read (MoR) in Lakehouse Table Formats","permalink":"/blog/2025/07/21/mor-comparison"},"nextItem":{"title":"How PayU built a secure enterprise AI assistant using Amazon Bedrock","permalink":"/blog/2025/07/15/PayU-built-a-secure-enterprise-AI-assistant"}}')},26515:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(53919),n=t(74848),s=t(28453);const r={title:"Ingesting Database changes via Sqoop/Hudi",excerpt:"Learn how to ingesting changes from a HUDI dataset using Sqoop/Hudi",author:"vinoth",category:"blog",tags:["how-to","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[];function c(e){const a={a:"a",code:"code",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.p,{children:"Very simple in just 2 steps."}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Step 1"}),": Extract new changes to users table in MySQL, as avro data files on DFS"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"// Command to extract incrementals using sqoop\nbin/sqoop import \\\n  -Dmapreduce.job.user.classpath.first=true \\\n  --connect jdbc:mysql://localhost/users \\\n  --username root \\\n  --password ******* \\\n  --table users \\\n  --as-avrodatafile \\\n  --target-dir \\ \n  s3:///tmp/sqoop/import-1/users\n"})}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Step 2"}),": Use your fav datasource to read extracted data and directly \u201cupsert\u201d the users table on DFS/Hive"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-scala",children:'// Spark Datasource\nimport org.apache.hudi.DataSourceWriteOptions._\n// Use Spark datasource to read avro\nval inputDataset = spark.read.avro("s3://tmp/sqoop/import-1/users/*");\n     \n// save it as a Hudi dataset\ninputDataset.write.format("org.apache.hudi\u201d)\n  .option(HoodieWriteConfig.TABLE_NAME, "hoodie.users")\n  .option(RECORDKEY_FIELD_OPT_KEY(), "userID")\n  .option(PARTITIONPATH_FIELD_OPT_KEY(),"country")\n  .option(PRECOMBINE_FIELD_OPT_KEY(), "last_mod")\n  .option(OPERATION_OPT_KEY(), UPSERT_OPERATION_OPT_VAL())\n  .mode(SaveMode.Append)\n  .save("/path/on/dfs");\n'})}),"\n",(0,n.jsxs)(a.p,{children:["Alternatively, you can also use the Hudi ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/hoodie_streaming_ingestion#hudi-streamer",children:"DeltaStreamer"})," tool with the DFSSource."]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},26620:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(403),n=t(74848),s=t(28453);const r={title:"Employing correct configurations for Hudi's cleaner table service",excerpt:"Ensuring isolation between Hudi writers and readers using `HoodieCleaner.java`",author:"pratyakshsharma",category:"blog",image:"/assets/images/blog/hoodie-cleaner/Initial_timeline.png",tags:["how-to","cleaner","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Reclaiming space and keeping your data lake storage costs in check",id:"reclaiming-space-and-keeping-your-data-lake-storage-costs-in-check",level:3},{value:"Problem Statement",id:"problem-statement",level:3},{value:"Deeper dive into Hudi Cleaner",id:"deeper-dive-into-hudi-cleaner",level:3},{value:"Cleaning Policies",id:"cleaning-policies",level:3},{value:"Examples",id:"examples",level:3},{value:"Configurations",id:"configurations",level:3},{value:"Run command",id:"run-command",level:3},{value:"Future Scope",id:"future-scope",level:3}];function c(e){const a={a:"a",code:"code",em:"em",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.p,{children:"Apache Hudi provides snapshot isolation between writers and readers. This is made possible by Hudi\u2019s MVCC concurrency model. In this blog, we will explain how to employ the right configurations to manage multiple file versions. Furthermore, we will discuss mechanisms available to users on how to maintain just the required number of old file versions so that long running readers do not fail."}),"\n",(0,n.jsx)(a.h3,{id:"reclaiming-space-and-keeping-your-data-lake-storage-costs-in-check",children:"Reclaiming space and keeping your data lake storage costs in check"}),"\n",(0,n.jsxs)(a.p,{children:["Hudi provides different table management services to be able to manage your tables on the data lake. One of these services is called the ",(0,n.jsx)(a.strong,{children:"Cleaner"}),". As you write more data to your table, for every batch of updates received, Hudi can either generate a new version of the data file with updates applied to records (COPY_ON_WRITE) or write these delta updates to a log file, avoiding rewriting newer version of an existing file (MERGE_ON_READ). In such situations, depending on the frequency of your updates, the number of file versions of log files can grow indefinitely. If your use-cases do not require keeping an infinite history of these versions, it is imperative to have a process that reclaims older versions of the data. This is Hudi\u2019s cleaner service."]}),"\n",(0,n.jsx)(a.h3,{id:"problem-statement",children:"Problem Statement"}),"\n",(0,n.jsx)(a.p,{children:"In a data lake architecture, it is a very common scenario to have readers and writers concurrently accessing the same table. As the Hudi cleaner service periodically reclaims older file versions, scenarios arise where a long running query might be accessing a file version that is deemed to be reclaimed by the cleaner. Here, we need to employ the correct configs to ensure readers (aka queries) don\u2019t fail."}),"\n",(0,n.jsx)(a.h3,{id:"deeper-dive-into-hudi-cleaner",children:"Deeper dive into Hudi Cleaner"}),"\n",(0,n.jsx)(a.p,{children:"To deal with the mentioned scenario, lets understand the  different cleaning policies that Hudi offers and the corresponding properties that need to be configured. Options are available to schedule cleaning asynchronously or synchronously. Before going into more details, we would like to explain a few underlying concepts:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Hudi base file"}),": Columnar file which consists of final data after compaction. A base file\u2019s name follows the following naming convention: ",(0,n.jsx)(a.code,{children:"<fileId>_<writeToken>_<instantTime>.parquet"}),". In subsequent writes of this file, file id remains the same and commit time gets updated to show the latest version. This also implies any particular version of a record, given its partition path, can be uniquely located using the file id and instant time."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"File slice"}),": A file slice consists of the base file and any log files consisting of the delta, in case of MERGE_ON_READ table type."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Hudi File Group"}),": Any file group in Hudi is uniquely identified by the partition path and the  file id that the files in this group have as part of their name. A file group consists of all the file slices in a particular partition path. Also any partition path can have multiple file groups."]}),"\n"]}),"\n",(0,n.jsx)(a.h3,{id:"cleaning-policies",children:"Cleaning Policies"}),"\n",(0,n.jsx)(a.p,{children:"Hudi cleaner currently supports below cleaning policies:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"KEEP_LATEST_COMMITS"}),": This is the default policy. This is a temporal cleaning policy that ensures the effect of having lookback into all the changes that happened in the last X commits. Suppose a writer is ingesting data  into a Hudi dataset every 30 minutes and the longest running query can take 5 hours to finish, then the user should retain atleast the last 10 commits. With such a configuration, we ensure that the oldest version of a file is kept on disk for at least 5 hours, thereby preventing the longest running query from failing at any point in time. Incremental cleaning is also possible using this policy."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"KEEP_LATEST_FILE_VERSIONS"}),": This policy has the effect of keeping N number of file versions irrespective of time. This policy is useful when it is known how many MAX versions of the file does one want to keep at any given time. To achieve the same behaviour as before of preventing long running queries from failing, one should do their calculations based on data patterns. Alternatively, this policy is also useful if a user just wants to maintain 1 latest version of the file."]}),"\n"]}),"\n",(0,n.jsx)(a.h3,{id:"examples",children:"Examples"}),"\n",(0,n.jsx)(a.p,{children:"Suppose a user is ingesting data into a hudi dataset of type COPY_ON_WRITE every 30 minutes as shown below:"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{alt:"Initial timeline",src:t(23209).A+"",width:"1200",height:"600"}),"\n",(0,n.jsx)(a.em,{children:"Figure1: Incoming records getting ingested into a hudi dataset every 30 minutes"})]}),"\n",(0,n.jsx)(a.p,{children:"The figure shows a particular partition on DFS where commits and corresponding file versions are color coded. 4 different file groups are created in this partition as depicted by fileGroup1, fileGroup2, fileGroup3 and fileGroup4. File group corresponding to fileGroup2 has records ingested from all the 5 commits, while the group corresponding to fileGroup4 has records from the latest 2 commits only."}),"\n",(0,n.jsx)(a.p,{children:"Suppose the user uses the below configs for cleaning:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"hoodie.cleaner.policy=KEEP_LATEST_COMMITS\nhoodie.cleaner.commits.retained=2\n"})}),"\n",(0,n.jsx)(a.p,{children:"Cleaner selects the versions of files to be cleaned by taking care of the following:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Latest version of a file should not be cleaned."}),"\n",(0,n.jsxs)(a.li,{children:["The commit times of the last 2 (configured) + 1 commits are determined. In Figure1, ",(0,n.jsx)(a.code,{children:"commit 10:30"})," and ",(0,n.jsx)(a.code,{children:"commit 10:00"})," correspond to the latest 2 commits in the timeline. One extra commit is included because the time window for retaining commits is essentially equal to the longest query run time. So if the longest query takes 1 hour to finish, and ingestion happens every 30 minutes, you need to retain last 2 commits since 2*30 = 60 (1 hour). At this point of time, the longest query can still be using files written in 3rd commit in reverse order. Essentially this means if a query started executing after ",(0,n.jsx)(a.code,{children:"commit 9:30"}),", it will still be running when clean action is triggered right after ",(0,n.jsx)(a.code,{children:"commit 10:30"})," as in Figure2."]}),"\n",(0,n.jsxs)(a.li,{children:["Now for any file group, only those file slices are scheduled for cleaning which are not savepointed (another Hudi table service) and whose commit time is less than the 3rd commit (",(0,n.jsx)(a.code,{children:"commit 9:30"})," in figure below) in reverse order."]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{alt:"Retain latest commits",src:t(14127).A+"",width:"1418",height:"1176"}),"\n",(0,n.jsx)(a.em,{children:"Figure2: Files corresponding to latest 3 commits are retained"})]}),"\n",(0,n.jsx)(a.p,{children:"Now, suppose the user uses the below configs for cleaning:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"hoodie.cleaner.policy=KEEP_LATEST_FILE_VERSIONS\nhoodie.cleaner.fileversions.retained=1\n"})}),"\n",(0,n.jsx)(a.p,{children:"Cleaner does the following:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["For any file group, latest version (including any for pending compaction) of file slices are kept and the rest are scheduled for cleaning. Clearly as shown in Figure3, if clean action is triggered right after ",(0,n.jsx)(a.code,{children:"commit 10:30"}),", the cleaner will simply leave the latest version in every file group and delete the rest."]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{alt:"Retain latest versions",src:t(31505).A+"",width:"1440",height:"1182"}),"\n",(0,n.jsx)(a.em,{children:"Figure3: Latest file version in every file group is retained"})]}),"\n",(0,n.jsx)(a.h3,{id:"configurations",children:"Configurations"}),"\n",(0,n.jsxs)(a.p,{children:["You can find the details about all the possible configurations along with the default values ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/configurations#compaction-configs",children:"here"}),"."]}),"\n",(0,n.jsx)(a.h3,{id:"run-command",children:"Run command"}),"\n",(0,n.jsxs)(a.p,{children:["Hudi's cleaner table service can be run as a separate process or along with your data ingestion. As mentioned earlier, it basically cleans up any stale/old files lying around. In case you want to run it along with ingesting data, configs are available which enable you to run it ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/configurations#withAsyncClean",children:"synchronously or asynchronously"}),". You can use the below command for running the cleaner independently:"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"[hoodie]$ spark-submit --class org.apache.hudi.utilities.HoodieCleaner \\\n  --props s3:///temp/hudi-ingestion-config/kafka-source.properties \\\n  --target-base-path s3:///temp/hudi \\\n  --spark-master yarn-cluster\n"})}),"\n",(0,n.jsx)(a.p,{children:"In case you wish to run the cleaner service asynchronously with writing, please configure the below:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"hoodie.clean.automatic=true\nhoodie.clean.async=true\n"})}),"\n",(0,n.jsxs)(a.p,{children:["Further you can use ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/deployment#cli",children:"Hudi CLI"})," for managing your Hudi dataset. CLI provides the below commands for cleaner service:"]}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.code,{children:"cleans show"})}),"\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.code,{children:"clean showpartitions"})}),"\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.code,{children:"cleans run"})}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["You can find more details and the relevant code for these commands in ",(0,n.jsxs)(a.a,{href:"https://github.com/apache/hudi/blob/master/hudi-cli/src/main/java/org/apache/hudi/cli/commands/CleansCommand.java",children:[(0,n.jsx)(a.code,{children:"org.apache.hudi.cli.commands.CleansCommand"})," class"]}),"."]}),"\n",(0,n.jsx)(a.h3,{id:"future-scope",children:"Future Scope"}),"\n",(0,n.jsxs)(a.p,{children:["Work is currently going on for introducing a new cleaning policy based on time elapsed. This will help in achieving a consistent retention throughout regardless of how frequently ingestion happens. You may track the progress ",(0,n.jsx)(a.a,{href:"https://issues.apache.org/jira/browse/HUDI-349",children:"here"}),"."]}),"\n",(0,n.jsxs)(a.p,{children:["We hope this blog gives you an idea about how to configure the Hudi cleaner and the supported cleaning policies. Please visit the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog",children:"blog section"})," for a deeper understanding of various Hudi concepts. Cheers!"]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},26825:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(19120),n=t(74848),s=t(28453);const r={title:"Apache Hudi Support on Apache Zeppelin",excerpt:"Integrating HUDI's real-time and read-optimized query capabilities into Apache Zeppelin\u2019s notebook",author:"leesf",category:"blog",tags:["how-to","apache zeppelin","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"1. Introduction",id:"1-introduction",level:2},{value:"2. Achieve the effect",id:"2-achieve-the-effect",level:2},{value:"2.1 Hive",id:"21-hive",level:3},{value:"2.1.1 Read optimized view",id:"211-read-optimized-view",level:3},{value:"2.1.2 Real-time view",id:"212-real-time-view",level:3},{value:"2.2 Spark SQL",id:"22-spark-sql",level:3},{value:"2.2.1 Read optimized view",id:"221-read-optimized-view",level:3},{value:"2.2.2 Real-time view",id:"222-real-time-view",level:3},{value:"3. Common problems",id:"3-common-problems",level:2},{value:"3.1 Hudi package adaptation",id:"31-hudi-package-adaptation",level:3},{value:"3.2 Parquet jar package adaptation",id:"32-parquet-jar-package-adaptation",level:3},{value:"3.3 Spark Interpreter adaptation",id:"33-spark-interpreter-adaptation",level:3},{value:"4. Hudi incremental view",id:"4-hudi-incremental-view",level:2}];function c(e){const a={code:"code",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.h2,{id:"1-introduction",children:"1. Introduction"}),"\n",(0,n.jsx)(a.p,{children:"Apache Zeppelin is a web-based notebook that provides interactive data analysis. It is convenient for you to make beautiful documents that can be data-driven, interactive, and collaborative, and supports multiple languages, including Scala (using Apache Spark), Python (Apache Spark), SparkSQL, Hive, Markdown, Shell, and so on. Hive and SparkSQL currently support querying Hudi\u2019s read-optimized view and real-time view. So in theory, Zeppelin\u2019s notebook should also have such query capabilities."}),"\n",(0,n.jsx)(a.h2,{id:"2-achieve-the-effect",children:"2. Achieve the effect"}),"\n",(0,n.jsx)(a.h3,{id:"21-hive",children:"2.1 Hive"}),"\n",(0,n.jsx)(a.h3,{id:"211-read-optimized-view",children:"2.1.1 Read optimized view"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Read Optimized View",src:t(55813).A+"",width:"1845",height:"950"})}),"\n",(0,n.jsx)(a.h3,{id:"212-real-time-view",children:"2.1.2 Real-time view"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Real-time View",src:t(89355).A+"",width:"1845",height:"956"})}),"\n",(0,n.jsx)(a.h3,{id:"22-spark-sql",children:"2.2 Spark SQL"}),"\n",(0,n.jsx)(a.h3,{id:"221-read-optimized-view",children:"2.2.1 Read optimized view"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Read Optimized View",src:t(99019).A+"",width:"1833",height:"449"})}),"\n",(0,n.jsx)(a.h3,{id:"222-real-time-view",children:"2.2.2 Real-time view"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Real-time View",src:t(78685).A+"",width:"1833",height:"432"})}),"\n",(0,n.jsx)(a.h2,{id:"3-common-problems",children:"3. Common problems"}),"\n",(0,n.jsx)(a.h3,{id:"31-hudi-package-adaptation",children:"3.1 Hudi package adaptation"}),"\n",(0,n.jsx)(a.p,{children:"Zeppelin will load the packages under lib by default when starting. For external dependencies such as Hudi, it is suitable to be placed directly under zeppelin / lib to avoid Hive or Spark SQL not finding the corresponding Hudi dependency on the cluster."}),"\n",(0,n.jsx)(a.h3,{id:"32-parquet-jar-package-adaptation",children:"3.2 Parquet jar package adaptation"}),"\n",(0,n.jsx)(a.p,{children:"The parquet version of the Hudi package is 1.10, and the current parquet version of the CDH cluster is 1.9, so when executing the Hudi table query, many jar package conflict errors will be reported."}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Solution"}),": upgrade the parquet package to 1.10 in the spark / jars directory of the node where zepeelin is located.\n",(0,n.jsx)(a.strong,{children:"Side effects"}),": The tasks of saprk jobs other than zeppelin assigned to the cluster nodes of parquet 1.10 may fail.\n",(0,n.jsx)(a.strong,{children:"Suggestions"}),": Clients other than zeppelin will also have jar conflicts. Therefore, it is recommended to fully upgrade the spark jar, parquet jar and related dependent jars of the cluster to better adapt to Hudi\u2019s capabilities."]}),"\n",(0,n.jsx)(a.h3,{id:"33-spark-interpreter-adaptation",children:"3.3 Spark Interpreter adaptation"}),"\n",(0,n.jsx)(a.p,{children:"The same SQL using Spark SQL query on Zeppelin will have more records than the hive query."}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Cause of the problem"}),": When reading and writing Parquet tables to the Hive metastore, Spark SQL will use the Parquet SerDe (SerDe: Serialize / Deserilize for short) for Spark serialization and deserialization, not the Hive\u2019s SerDe, because Spark SQL\u2019s own SerDe has better performance."]}),"\n",(0,n.jsx)(a.p,{children:"This causes Spark SQL to only query Hudi\u2019s pipeline records, not the final merge result."}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Solution"}),": set ",(0,n.jsx)(a.code,{children:"spark.sql.hive.convertMetastoreParquet=false"})]}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Method 1"}),": Edit properties directly on the page**\n",(0,n.jsx)(a.img,{src:t(74857).A+"",width:"1818",height:"105"})]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Method 2"}),": Edit ",(0,n.jsx)(a.code,{children:"zeppelin / conf / interpreter.json"})," and add**"]}),"\n"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-json",children:'"spark.sql.hive.convertMetastoreParquet": {\n  "name": "spark.sql.hive.convertMetastoreParquet",\n  "value": false,\n  "type": "checkbox"\n}\n'})}),"\n",(0,n.jsx)(a.h2,{id:"4-hudi-incremental-view",children:"4. Hudi incremental view"}),"\n",(0,n.jsx)(a.p,{children:"For Hudi incremental view, currently only supports pulling by writing Spark code. Considering that Zeppelin has the ability to execute code and shell commands directly on the notebook, later consider packaging these notebooks to query Hudi incremental views in a way that supports SQL."})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},27046:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi-data-lake-platform_-_Copy_of_Page_1_3-2d54eeaee61f34b3146391bec58c11e5.png"},27078:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(30776),n=t(74848),s=t(28453),r=t(9230);const o={title:"The Apache Software Foundation Announces Apache\xae Hudi\u2122 as a Top-Level Project",category:"blog",image:"/assets/images/asf_logo.svg",tags:["blog","apache"]},l=void 0,d={authorsImageUrls:[]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blogs.apache.org/foundation/entry/the-apache-software-foundation-announces64",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},27098:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(19146),n=t(74848),s=t(28453);const r={title:"Exploring Apache Hudi\u2019s New Log-Structured Merge (LSM) Timeline",excerpt:"What is the new LSM timeline in Hudi & how is it implemented",author:"Dipankar Mazumdar",category:"blog",image:"/assets/images/blog/lsm-1200x600.jpg",tags:["Apache Hudi","LSM Tree","Performance","Non-Blocking Concurrency Control"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Apache Hudi\u2019s Timeline",id:"apache-hudis-timeline",level:2},{value:"Active Timeline",id:"active-timeline",level:3},{value:"Archived Timeline",id:"archived-timeline",level:3},{value:"Problem Statement - Why move to an LSM Timeline?",id:"problem-statement---why-move-to-an-lsm-timeline",level:2},{value:"Introducing the LSM Timeline",id:"introducing-the-lsm-timeline",level:2},{value:"How It Works / Design",id:"how-it-works--design",level:3},{value:"File Organization",id:"file-organization",level:4},{value:"Compaction Strategy",id:"compaction-strategy",level:4},{value:"Version &amp; Manifest Management: Snapshot Isolation",id:"version--manifest-management-snapshot-isolation",level:4},{value:"Reader Workflow",id:"reader-workflow",level:4},{value:"Cleaning Strategy",id:"cleaning-strategy",level:4},{value:"What It Brings to the Table (Benefits)",id:"what-it-brings-to-the-table-benefits",level:3},{value:"Performance",id:"performance",level:3}];function c(e){const a={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.admonition,{title:"TL;DR",type:"tip",children:(0,n.jsx)(a.p,{children:"Apache Hudi 1.0 introduces a new LSM Timeline to scale metadata management for long-lived tables. By restructuring timeline storage into a compacted, versioned tree layout, Hudi enables faster metadata access, snapshot isolation, and support for Non-Blocking Concurrency Control."})}),"\n",(0,n.jsx)(a.h2,{id:"apache-hudis-timeline",children:"Apache Hudi\u2019s Timeline"}),"\n",(0,n.jsxs)(a.p,{children:["At the heart of Apache Hudi\u2019s architecture is the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/timeline",children:"Timeline"})," - a log-structured system that acts as the single source of truth for the table\u2019s state at any point in time. The timeline records every change and operation performed on a Hudi table, encompassing writes, schema evolutions, compactions, cleanings, and clustering operations. This meticulous record-keeping empowers Hudi to deliver ",(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/blog/acid-transactions-in-an-open-data-lakehouse",children:"ACID guarantees"}),", robust ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2025/01/28/concurrency-control",children:"concurrency control"}),", and advanced capabilities such as incremental processing, rollback/recovery, and time travel."]}),"\n",(0,n.jsxs)(a.p,{children:["In essence, the timeline functions like a ",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Write-ahead_logging",children:"Write-Ahead Log (WAL)"}),", maintaining a sequence of immutable actions. Each action is recorded as a unique ",(0,n.jsx)(a.em,{children:"instant"})," - a unit of work identified by its action type (e.g., commit, clean, compaction), a timestamp that marks when the action was initiated, and its lifecycle state. In Hudi, an ",(0,n.jsx)(a.em,{children:"instant"})," refers to this combination of action, timestamp, and state (REQUESTED, INFLIGHT, or COMPLETED), and serves as the atomic unit of change on the timeline. These timeline entries are the backbone of Hudi\u2019s transactional integrity, ensuring that every table change is atomically recorded and timeline-consistent. Every operation progresses through a lifecycle of ",(0,n.jsx)(a.em,{children:"states"}),":"]}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"REQUESTED: The action is planned and registered but not yet started."}),"\n",(0,n.jsx)(a.li,{children:"INFLIGHT: The action is actively being performed, modifying table state."}),"\n",(0,n.jsx)(a.li,{children:"COMPLETED: The action has successfully executed, and all data/metadata updates are finalized."}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["These ",(0,n.jsx)(a.em,{children:"instants"})," serve as both log entries and transaction markers, defining exactly what data is valid and visible at any given time. Whether you're issuing a snapshot query for the latest view, running an incremental query to fetch changes since the last checkpoint, or rolling back to a prior state, the timeline ensures that each action\u2019s impact is precisely tracked. Every ",(0,n.jsx)(a.em,{children:"action"}),", such as commit, clean, compaction, or rollback is explicitly recorded, allowing compute engines and tools to reason precisely about the table\u2019s state transitions and history. This strict sequencing and lifecycle management also underpin Hudi\u2019s ability to provide serializable isolation (the \u201cI\u201d in ACID) guarantees, ensuring that readers only observe committed data and consistent snapshots."]}),"\n",(0,n.jsx)(a.p,{children:"To optimize both performance and long-term storage scalability, Apache Hudi splits the timeline into two distinct components that work together to provide fast access to recent actions while ensuring historical records are retained efficiently. Let\u2019s understand these in detail."}),"\n",(0,n.jsx)(a.h3,{id:"active-timeline",children:"Active Timeline"}),"\n",(0,n.jsxs)(a.p,{children:["The ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/timeline#active-timeline",children:"Active timeline"})," is the front line of Hudi\u2019s transaction log. It contains the most recent and in-progress actions that are critical for building a consistent and up-to-date view of the table. Every time a new operation, such as a data write, compaction, clean, or rollback is initiated, it is immediately recorded here as a new instant file under the ",(0,n.jsx)(a.code,{children:".hoodie/"})," directory. Each of these files holds metadata about the action\u2019s lifecycle, moving through the standard states of REQUESTED \u2192 INFLIGHT \u2192 COMPLETED."]}),"\n",(0,n.jsx)(a.p,{children:"The active timeline is consulted constantly - whether you are issuing a query, running compaction, or planning a new write operation. Compute engines read from the active timeline to determine what data files are valid and visible, making it the source of truth for the table\u2019s latest state. To maintain performance, Hudi enforces a retention policy on the active timeline, i.e. it deliberately keeps only a window of the most recent actions, ensuring the timeline remains lightweight and quick to scan."}),"\n",(0,n.jsx)(a.h3,{id:"archived-timeline",children:"Archived Timeline"}),"\n",(0,n.jsx)(a.p,{children:"Tables naturally accumulate many more actions over time, especially in high-ingestion or update environments. As the number of instants grows, the active timeline can become bloated if left unchecked, introducing latency and performance penalties during reads and writes."}),"\n",(0,n.jsxs)(a.p,{children:["To solve this, Hudi implements an archival process. Once the number of active instants crosses a configured threshold, older actions are offloaded from the active timeline into the Archived Timeline stored in the ",(0,n.jsx)(a.code,{children:".hoodie/archive/"})," directory. This design ensures that while the active timeline remains lean and fast for day-to-day operations, the complete transactional history of the table is still preserved for auditing, recovery, and time travel purposes."]}),"\n",(0,n.jsx)(a.p,{children:"Although the archived timeline is optimized for long-term retention, accessing deep history can incur higher latency and overhead, especially in workloads with a large number of archived instants. This limitation is precisely what set the stage for the LSM Timeline innovation introduced in Hudi 1.0."}),"\n",(0,n.jsx)(a.h2,{id:"problem-statement---why-move-to-an-lsm-timeline",children:"Problem Statement - Why move to an LSM Timeline?"}),"\n",(0,n.jsx)(a.p,{children:"Apache Hudi\u2019s original timeline design served well for many workloads. By maintaining a lightweight active timeline for fast operations and offloading historical instants to the archive, Hudi struck a balance between performance and durability. However, there were some aspects to think about with the previous timeline design."}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Linear Growth"}),": The timeline grows linearly with each table action, whether it\u2019s a commit, compaction, clustering, or rollback. Although Hudi\u2019s archival process offloads older instants to keep the active timeline lean, the total number of instants (active + archived) continues to grow unbounded in long-lived tables. Over time, the accumulation of these instants can inflate metadata size, leading to slower scans and degraded query planning performance, especially for use cases like time travel and incremental queries."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Latency & Cost"}),": Accessing the archived timeline, which is often required for time-travel, or recovery operations introduces high read latencies. This is because the archival format was optimized for durability and storage efficiency (many small Avro files), not for fast access. As the number of archived instants balloons, reading deep history involves scanning and deserializing large volumes of metadata, increasing both latency and compute cost. This can noticeably slow down operations like incremental syncs and historical audits."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Cloud Storage Limitations"}),": In cloud object stores like S3 or GCS, appending to existing files is not supported (or is highly inefficient). As a result, every new archival batch creates new small files, leading to a small-file problem. Over time, these fragmented archives accumulate, creating operational challenges in storage management and performance bottlenecks during metadata access, especially when files must be scanned individually across large object stores."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Emerging Use Cases"}),": Apache Hudi has evolved to support next-generation features such as non-blocking concurrency control (NBCC), infinite time travel, and fine-grained transaction metadata. These capabilities place heavier demands on the timeline architecture, requiring high-throughput writes and faster lookups across both recent and historical data."]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"introducing-the-lsm-timeline",children:"Introducing the LSM Timeline"}),"\n",(0,n.jsxs)(a.p,{children:["To overcome the scaling challenges of the original timeline architecture, Apache Hudi 1.0 introduced the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/timeline#timeline-components",children:"LSM (Log-Structured Merge)"})," Timeline - a fundamentally new way to store and manage timeline metadata. This redesign brings together principles of ",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Log-structured_merge-tree",children:"log-structured storage"}),", tiered compaction, and snapshot versioning to deliver a highly scalable, cloud-native solution for tracking table history."]}),"\n",(0,n.jsxs)(a.p,{children:["Hudi introduces a critical change in how time is represented on the timeline. Previously, Hudi treated time as instantaneous, i.e. each action appeared to take effect at a single instant. While effective for basic operations, this model proved limiting when implementing certain advanced features like ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control/",children:"Non-Blocking Concurrency Control (NBCC)"}),", which require reasoning about actions as intervals of time to detect overlaps and resolve conflicts."]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/lsm_1.png",alt:"index",width:"800",align:"middle"}),"\n",(0,n.jsxs)(a.p,{children:["To address this, every action on the Hudi timeline now records both a ",(0,n.jsx)(a.em,{children:"requested time"})," (when the action is initiated) and a ",(0,n.jsx)(a.em,{children:"completion time"})," (when it finishes). This allows Hudi to track not just when an action was scheduled, but also how it interacts with other concurrent actions over time. To ensure global consistency across distributed processes, Hudi formalized the use of ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/timeline#truetime-generation",children:"TrueTime semantics"}),", guaranteeing that all instant times are monotonically increasing and globally ordered. This is a foundational requirement for precise conflict detection and robust transaction isolation."]}),"\n",(0,n.jsx)(a.h3,{id:"how-it-works--design",children:"How It Works / Design"}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/lsm_2.png",alt:"index",width:"800",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"At its core, the LSM timeline replaces the flat archival model with a layered tree structure, allowing Hudi to manage metadata for millions of historical instants efficiently, without compromising on read performance or consistency. Here\u2019s how it\u2019s designed:"}),"\n",(0,n.jsx)(a.h4,{id:"file-organization",children:"File Organization"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Metadata files are organized into layers (L0, L1, L2, \u2026) following a Log-Structured Merge (LSM) tree layout."}),"\n",(0,n.jsx)(a.li,{children:"Each file is a Parquet file that stores a batch of timeline instants. Their metadata entries are sorted chronologically by timestamp."}),"\n",(0,n.jsxs)(a.li,{children:["The files follow a precise naming convention: ",(0,n.jsx)(a.code,{children:"${min_instant}_${max_instant}_${level}.parquet"})," where ",(0,n.jsx)(a.code,{children:"min_instant"})," and ",(0,n.jsx)(a.code,{children:"max_instant"})," represent the range of instants in the file and ",(0,n.jsx)(a.code,{children:"level"})," denotes the layer (e.g., L0, L1, L2)."]}),"\n",(0,n.jsx)(a.li,{children:"Files in the same layer may have overlapping time ranges, but the system tracks them via manifest files (more on that below)."}),"\n"]}),"\n",(0,n.jsx)(a.h4,{id:"compaction-strategy",children:"Compaction Strategy"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"The LSM timeline uses a universal compaction strategy, similar to designs seen in modern databases."}),"\n",(0,n.jsx)(a.li,{children:"Whenever N files (default: 10) accumulate in a given layer (e.g., L0), they are merged and flushed into the next layer (e.g., L1)."}),"\n",(0,n.jsx)(a.li,{children:"Compaction is governed by a size-based policy (default max file size ~1 GB), ensuring that write amplification is controlled and files stay within optimal size limits."}),"\n",(0,n.jsx)(a.li,{children:"There\u2019s no hard limit on the number of layers. The LSM tree naturally scales to handle massive tables with deep histories."}),"\n"]}),"\n",(0,n.jsx)(a.h4,{id:"version--manifest-management-snapshot-isolation",children:"Version & Manifest Management: Snapshot Isolation"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"The LSM timeline introduces manifest files that record the current valid set of Parquet files representing the latest snapshot of the timeline."}),"\n",(0,n.jsx)(a.li,{children:"Version files are generated alongside manifest files to maintain snapshot isolation, ensuring that readers and writers do not conflict."}),"\n",(0,n.jsxs)(a.li,{children:["This system supports multiple valid snapshot versions simultaneously (default: 3), enabling:","\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Consistent reads even during compaction."}),"\n",(0,n.jsx)(a.li,{children:"Seamless evolution of the timeline without impacting query correctness."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.h4,{id:"reader-workflow",children:"Reader Workflow"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["When a query is made on the timeline:","\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"The engine first fetches the latest version file."}),"\n",(0,n.jsx)(a.li,{children:"It reads the corresponding manifest file to get the list of valid data files."}),"\n",(0,n.jsx)(a.li,{children:"It scans only the relevant Parquet files, often using timestamp-based filtering to skip irrelevant data early."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.h4,{id:"cleaning-strategy",children:"Cleaning Strategy"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"The LSM timeline performs cleaning only after successful compaction, ensuring that no active snapshot is disrupted."}),"\n",(0,n.jsx)(a.li,{children:"By default, Hudi retains 3 valid snapshot versions to support concurrent readers/writers."}),"\n",(0,n.jsx)(a.li,{children:"Files are retained for at least 3 archival trigger intervals, providing a grace period before old data is purged."}),"\n"]}),"\n",(0,n.jsx)(a.h3,{id:"what-it-brings-to-the-table-benefits",children:"What It Brings to the Table (Benefits)"}),"\n",(0,n.jsx)(a.p,{children:"The LSM timeline unlocks significant advancements in how Apache Hudi handles metadata, providing both performance improvements and new capabilities."}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Scalability:"})," The LSM timeline architecture allows Hudi to manage virtually infinite timeline history while keeping both read and write performance predictable. Whether it's thousands or millions of instants, the layered compaction model ensures stable metadata performance over time, supporting efficient query and metadata access even as tables grow in size and history length."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Efficient Reads:"})," Readers benefit from manifest-guided lookups, allowing them to scan only the specific set of files relevant to their query. By using Parquet\u2019s columnar format and timestamp-based filtering, Hudi dramatically reduces the overhead of accessing deep historical metadata."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Non-Blocking Concurrency Control (NBCC):"})," One of the most powerful capabilities enabled by the LSM timeline is Non-Blocking Concurrency Control, allowing multiple writers to operate concurrently on the same table (and even the same file group) without the need for explicit locks - except during final commit metadata updates."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Cloud-Native Optimization"}),": By compacting small files into large Parquet files, the LSM timeline avoids the small-file problem common in cloud storage systems like Amazon S3 or Google Cloud Storage. This improves both query performance and storage cost efficiency."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Snapshot Isolation & Consistency"}),": The manifest + version file mechanism ensures that concurrent operations remain isolated and consistent, even as background compaction and cleaning occur. This provides strong transactional guarantees without sacrificing performance."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Maintenance-Free Scalability"}),": The universal compaction and smart cleaning strategies keep the timeline healthy over time, requiring minimal manual tuning, while ensuring that old data is cleaned up safely only after valid snapshots are no longer in use."]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.h3,{id:"performance",children:"Performance"}),"\n",(0,n.jsxs)(a.p,{children:["Micro-benchmarks show that the LSM Timeline scales efficiently even as the number of timeline actions grows by orders of magnitude. Reading just the instant times for ",(0,n.jsx)(a.code,{children:"10,000"})," actions takes around ",(0,n.jsx)(a.code,{children:"32ms"}),", while fetching full metadata takes ",(0,n.jsx)(a.code,{children:"150ms"}),". At larger scales, such as ",(0,n.jsx)(a.code,{children:"10 million"})," actions, metadata reads completes in about ",(0,n.jsx)(a.code,{children:"162 seconds"}),"."]}),"\n",(0,n.jsx)(a.p,{children:"These results demonstrate that Hudi's LSM timeline can handle decades of high-frequency commits (e.g., one every 30 seconds for 10+ years) while keeping metadata access performant."}),"\n",(0,n.jsx)(a.p,{children:"The LSM timeline represents a natural progression in Apache Hudi\u2019s timeline architecture, designed to address the growing demands of large-scale and long-lived tables. Hudi\u2019s timeline has been foundational for transactional integrity, time travel, and incremental processing capabilities. The new LSM-based design enhances scalability and operational efficiency by introducing a layered, compacted structure with manifest-driven snapshot isolation. This improvement allows Hudi to manage extensive metadata histories more efficiently, maintain predictable performance, and better support advanced use cases such as non-blocking concurrency control."}),"\n",(0,n.jsx)(a.hr,{})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},27189:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(97183),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi: Copy on Write(CoW) Table",excerpt:"Apache Hudi: Copy on Write(CoW) Table",authors:[{name:"Ankur Ranjan"}],category:"blog",image:"/assets/images/blog/2023-10-06-Apache-Hudi-Copy-on-Write-CoW-Table.png",tags:["medium","blog","cow","deep dive","apache hudi"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@ranjanankur/apache-hudi-copy-on-write-cow-table-77fb2b849733",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},27392:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/10/19/load-data-incrementally-from-transactional-data-lakes-to-data-warehouses","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-10-19-load-data-incrementally-from-transactional-data-lakes-to-data-warehouses.mdx","source":"@site/blog/2023-10-19-load-data-incrementally-from-transactional-data-lakes-to-data-warehouses.mdx","title":"Load data incrementally from transactional data lakes to data warehouses","description":"Redirecting... please wait!!","date":"2023-10-19T00:00:00.000Z","tags":[{"inline":true,"label":"incremental updates","permalink":"/blog/tags/incremental-updates"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"},{"inline":true,"label":"how to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"querying","permalink":"/blog/tags/querying"},{"inline":true,"label":"aws","permalink":"/blog/tags/aws"},{"inline":true,"label":"amazon redshift","permalink":"/blog/tags/amazon-redshift"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Noritaka Sekiyama","key":null,"page":null}],"frontMatter":{"title":"Load data incrementally from transactional data lakes to data warehouses","excerpt":"Load data incrementally from Apache Hudi table to Amazon Redshift using a Hudi incremental query","author":"Noritaka Sekiyama","category":"blog","image":"/assets/images/blog/2023-10-19-load-data-incrementally-from-transactional-data-lakes-to-data-warehouses.png","tags":["incremental updates","amazon","how to","querying","aws","amazon redshift","apache hudi"]},"unlisted":false,"prevItem":{"title":"It\'s Time for the Universal Data Lakehouse","permalink":"/blog/2023/10/20/Its-Time-for-the-Universal-Data-Lakehouse"},"nextItem":{"title":"Apache Hudi: From Zero To One (5/10)","permalink":"/blog/2023/10/18/Apache-Hudi-From-Zero-To-One-blog-5"}}')},27751:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(18461),n=t(74848),s=t(28453),r=t(9230);const o={title:"Why and How I Integrated Airbyte and Apache Hudi",authors:[{name:"Harsha Teja Kanna"}],category:"blog",image:"/assets/images/blog/2022-01-18-airbyte-hudi-integration.png",tags:["how-to","deltastreamer","selectfrom"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://selectfrom.dev/why-and-how-i-integrated-airbyte-and-apache-hudi-c18aff3af21a",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},27825:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/12/03/apache-iceberg-vs-apache-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-12-03-apache-iceberg-vs-apache-hudi.mdx","source":"@site/blog/2024-12-03-apache-iceberg-vs-apache-hudi.mdx","title":"Apache Iceberg vs Hudi: Key Features, Performance & Use Cases","description":"Redirecting... please wait!!","date":"2024-12-03T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"apache iceberg","permalink":"/blog/tags/apache-iceberg"},{"inline":true,"label":"comparison","permalink":"/blog/tags/comparison"},{"inline":true,"label":"estuary","permalink":"/blog/tags/estuary"}],"readingTime":0.1,"hasTruncateMarker":false,"authors":[{"name":"Dani P\xe1lma","key":null,"page":null}],"frontMatter":{"title":"Apache Iceberg vs Hudi: Key Features, Performance & Use Cases","author":"Dani P\xe1lma","category":"blog","image":"/assets/images/blog/2024-12-03-apache-iceberg-vs-apache-hudi.jpeg","tags":["blog","apache hudi","apache iceberg","comparison","estuary"]},"unlisted":false,"prevItem":{"title":"Use open table format libraries on AWS Glue 5.0 for Apache Spark","permalink":"/blog/2024/12/04/use-open-table-format-libraries-on-aws-glue-5-0-for-apache-spark"},"nextItem":{"title":"Hudi\u2019s Automatic File Sizing Delivers Unmatched Performance","permalink":"/blog/2024/11/19/automated-small-file-handling"}}')},27934:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide9-92ae2d70a81caf694e03658351410de8.png"},28075:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(45456),n=t(74848),s=t(28453),r=t(9230);const o={title:"Experts primer on Apache Hudi",authors:[{name:"Stephanie Simone"}],category:"blog",image:"/assets/images/blog/data-summit-connect.jpeg",tags:["blog","dbta"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.dbta.com/Editorial/News-Flashes/Experts-Present-a-Primer-on-Apache-Hudi-at-Data-Summit-Connect-2021-146834.aspx",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},28292:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/08/03/Apache-Hudi-on-AWS-Glue-A-Step-by-Step-Guide","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-08-03-Apache-Hudi-on-AWS-Glue-A-Step-by-Step-Guide.mdx","source":"@site/blog/2023-08-03-Apache-Hudi-on-AWS-Glue-A-Step-by-Step-Guide.mdx","title":"Apache Hudi on AWS Glue: A Step-by-Step Guide","description":"Redirecting... please wait!!","date":"2023-08-03T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"aws-glue","permalink":"/blog/tags/aws-glue"},{"inline":true,"label":"apache-hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Dev Jain","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Apache Hudi on AWS Glue: A Step-by-Step Guide","authors":[{"name":"Dev Jain"}],"category":"blog","image":"/assets/images/blog/2023-08-03-Apache-Hudi-on-AWS-Glue-A-Step-by-Step-Guide.png","tags":["how-to","aws-glue","apache-hudi","medium"]},"unlisted":false,"prevItem":{"title":"Data Lakehouse Architecture for Big Data with Apache Hudi","permalink":"/blog/2023/08/05/Data-Lakehouse-Architecture-for-Big-Data-with-Apache-Hudi"},"nextItem":{"title":"Create an Apache Hudi-based-near-real-time transactional data lake using AWS DMS, Amazon Kinesis, AWS Glue streaming ETL, and data visualization using Amazon QuickSight","permalink":"/blog/2023/08/03/Create-an-Apache-Hudi-based-near-real-time-transactional-data lake-using-AWS-DMS-Amazon-Kinesis-AWS-Glue-streaming-ETL-and-data-visualization-using-Amazon-QuickSight"}}')},28436:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/08/25/Delta-Hudi-Iceberg-Which-is-most-popular","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-08-25-Delta-Hudi-Iceberg-Which-is-most-popular.mdx","source":"@site/blog/2023-08-25-Delta-Hudi-Iceberg-Which-is-most-popular.mdx","title":"Delta, Hudi, Iceberg \u2014 Which is most popular?","description":"Redirecting... please wait!!","date":"2023-08-25T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"delta lake","permalink":"/blog/tags/delta-lake"},{"inline":true,"label":"iceberg","permalink":"/blog/tags/iceberg"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Kyle Weller","key":null,"page":null}],"frontMatter":{"title":"Delta, Hudi, Iceberg \u2014 Which is most popular?","excerpt":"Popular Lakehoue Project","author":"Kyle Weller","category":"blog","image":"/assets/images/blog/2023-08-25-Delta-Hudi-Iceberg-Which-is-most-popular.png","tags":["blog","apache hudi","delta lake","iceberg","medium"]},"unlisted":false,"prevItem":{"title":"Delta, Hudi, Iceberg \u2014 A Benchmark Compilation","permalink":"/blog/2023/08/28/Delta-Hudi-Iceberg-A-Benchmark-Compilation"},"nextItem":{"title":"Exploring various storage types in Apache Hudi","permalink":"/blog/2023/08/22/Exploring-various-storage-types-in-Apache-Hudi"}}')},28453:(e,a,t)=>{"use strict";t.d(a,{R:()=>r,x:()=>o});var i=t(96540);const n={},s=i.createContext(n);function r(e){const a=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function o(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:r(e.components),i.createElement(s.Provider,{value:a},e.children)}},28791:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(27825),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Iceberg vs Hudi: Key Features, Performance & Use Cases",author:"Dani P\xe1lma",category:"blog",image:"/assets/images/blog/2024-12-03-apache-iceberg-vs-apache-hudi.jpeg",tags:["blog","apache hudi","apache iceberg","comparison","estuary"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://estuary.dev/apache-iceberg-vs-apache-hudi/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},28866:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(58683),n=t(74848),s=t(28453);const r={title:"Asynchronous Clustering using Hudi",excerpt:"How to setup Hudi for asynchronous clustering",author:"codope",category:"blog",image:"/assets/images/blog/clustering/example_perf_improvement.png",tags:["design","clustering","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Introduction",id:"introduction",level:2},{value:"Clustering Strategies",id:"clustering-strategies",level:2},{value:"Plan Strategy",id:"plan-strategy",level:3},{value:"Execution Strategy",id:"execution-strategy",level:3},{value:"Update Strategy",id:"update-strategy",level:3},{value:"Asynchronous Clustering",id:"asynchronous-clustering",level:2},{value:"HoodieClusteringJob",id:"hoodieclusteringjob",level:3},{value:"HoodieDeltaStreamer",id:"hoodiedeltastreamer",level:3},{value:"Spark Structured Streaming",id:"spark-structured-streaming",level:3},{value:"Conclusion and Future Work",id:"conclusion-and-future-work",level:2}];function c(e){const a={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(a.p,{children:["In one of the ",(0,n.jsx)(a.a,{href:"/blog/2021/01/27/hudi-clustering-intro",children:"previous blog"})," posts, we introduced a new\nkind of table service called clustering to reorganize data for improved query performance without compromising on\ningestion speed. We learnt how to setup inline clustering. In this post, we will discuss what has changed since then and\nsee how asynchronous clustering can be setup using HoodieClusteringJob as well as DeltaStreamer utility."]}),"\n",(0,n.jsx)(a.h2,{id:"introduction",children:"Introduction"}),"\n",(0,n.jsxs)(a.p,{children:["On a high level, clustering creates a plan based on a configurable strategy, groups eligible files based on specific\ncriteria and then executes the plan. Hudi supports ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/concurrency_control#enabling-multi-writing",children:"multi-writers"})," which provides\nsnapshot isolation between multiple table services, thus allowing writers to continue with ingestion while clustering\nruns in the background. For a more detailed overview of the clustering architecture please check out the previous blog\npost."]}),"\n",(0,n.jsx)(a.h2,{id:"clustering-strategies",children:"Clustering Strategies"}),"\n",(0,n.jsx)(a.p,{children:"As mentioned before, clustering plan as well as execution depends on configurable strategy. These strategies can be\nbroadly classified into three types: clustering plan strategy, execution strategy and update strategy."}),"\n",(0,n.jsx)(a.h3,{id:"plan-strategy",children:"Plan Strategy"}),"\n",(0,n.jsxs)(a.p,{children:["This strategy comes into play while creating clustering plan. It helps to decide what file groups should be clustered.\nLet's look at different plan strategies that are available with Hudi. Note that these strategies are easily pluggable\nusing this ",(0,n.jsx)(a.a,{href:"/docs/next/configurations#hoodieclusteringplanstrategyclass",children:"config"}),"."]}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.code,{children:"SparkSizeBasedClusteringPlanStrategy"}),": It selects file slices based on\nthe ",(0,n.jsx)(a.a,{href:"/docs/next/configurations/#hoodieclusteringplanstrategysmallfilelimit",children:"small file limit"}),"\nof base files and creates clustering groups upto max file size allowed per group. The max size can be specified using\nthis ",(0,n.jsx)(a.a,{href:"/docs/next/configurations/#hoodieclusteringplanstrategymaxbytespergroup",children:"config"}),". This\nstrategy is useful for stitching together medium-sized files into larger ones to reduce lot of files spread across\ncold partitions."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.code,{children:"SparkRecentDaysClusteringPlanStrategy"}),": It looks back previous 'N' days partitions and creates a plan that will\ncluster the 'small' file slices within those partitions. This is the default strategy. It could be useful when the\nworkload is predictable and data is partitioned by time."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.code,{children:"SparkSelectedPartitionsClusteringPlanStrategy"}),": In case you want to cluster only specific partitions within a range,\nno matter how old or new are those partitions, then this strategy could be useful. To use this strategy, one needs\nto set below two configs additionally (both begin and end partitions are inclusive):"]}),"\n"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:"hoodie.clustering.plan.strategy.cluster.begin.partition\nhoodie.clustering.plan.strategy.cluster.end.partition\n"})}),"\n",(0,n.jsx)(a.admonition,{type:"note",children:(0,n.jsx)(a.p,{children:"All the strategies are partition-aware and the latter two are still bound by the size limits of the first strategy."})}),"\n",(0,n.jsx)(a.h3,{id:"execution-strategy",children:"Execution Strategy"}),"\n",(0,n.jsxs)(a.p,{children:["After building the clustering groups in the planning phase, Hudi applies execution strategy, for each group, primarily\nbased on sort columns and size. The strategy can be specified using this ",(0,n.jsx)(a.a,{href:"/docs/next/configurations/#hoodieclusteringexecutionstrategyclass",children:"config"}),"."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.code,{children:"SparkSortAndSizeExecutionStrategy"})," is the default strategy. Users can specify the columns to sort the data by, when\nclustering using\nthis ",(0,n.jsx)(a.a,{href:"/docs/next/configurations/#hoodieclusteringplanstrategysortcolumns",children:"config"}),". Apart from\nthat, we can also set ",(0,n.jsx)(a.a,{href:"/docs/next/configurations/#hoodieparquetmaxfilesize",children:"max file size"}),"\nfor the parquet files produced due to clustering. The strategy uses bulk insert to write data into new files, in which\ncase, Hudi implicitly uses a partitioner that does sorting based on specified columns. In this way, the strategy changes\nthe data layout in a way that not only improves query performance but also balance rewrite overhead automatically."]}),"\n",(0,n.jsxs)(a.p,{children:["Now this strategy can be executed either as a single spark job or multiple jobs depending on number of clustering groups\ncreated in the planning phase. By default, Hudi will submit multiple spark jobs and union the results. In case you want\nto force Hudi to use single spark job, set the execution strategy\nclass ",(0,n.jsx)(a.a,{href:"/docs/next/configurations/#hoodieclusteringexecutionstrategyclass",children:"config"}),"\nto ",(0,n.jsx)(a.code,{children:"SingleSparkJobExecutionStrategy"}),"."]}),"\n",(0,n.jsx)(a.h3,{id:"update-strategy",children:"Update Strategy"}),"\n",(0,n.jsxs)(a.p,{children:["Currently, clustering can only be scheduled for tables/partitions not receiving any concurrent updates. By default,\nthe ",(0,n.jsx)(a.a,{href:"/docs/next/configurations/#hoodieclusteringupdatesstrategy",children:"config for update strategy"})," is\nset to ",(0,n.jsx)(a.em,{children:(0,n.jsx)(a.strong,{children:"SparkRejectUpdateStrategy"})}),". If some file group has updates during clustering then it will reject updates and\nthrow an exception. However, in some use-cases updates are very sparse and do not touch most file groups. The default\nstrategy to simply reject updates does not seem fair. In such use-cases, users can set the config to ",(0,n.jsx)(a.em,{children:(0,n.jsx)(a.strong,{children:"SparkAllowUpdateStrategy"})}),"."]}),"\n",(0,n.jsxs)(a.p,{children:["We discussed the critical strategy configurations. All other configurations related to clustering are\nlisted ",(0,n.jsx)(a.a,{href:"/docs/next/configurations/#Clustering-Configs",children:"here"}),". Out of this list, a few\nconfigurations that will be very useful are:"]}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{children:"Config key"}),(0,n.jsx)(a.th,{children:"Remarks"}),(0,n.jsx)(a.th,{children:"Default"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.clustering.async.enabled"})}),(0,n.jsx)(a.td,{children:"Enable running of clustering service, asynchronously as writes happen on the table."}),(0,n.jsx)(a.td,{children:"False"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.clustering.async.max.commits"})}),(0,n.jsx)(a.td,{children:"Control frequency of async clustering by specifying after how many commits clustering should be triggered."}),(0,n.jsx)(a.td,{children:"4"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.clustering.preserve.commit.metadata"})}),(0,n.jsx)(a.td,{children:"When rewriting data, preserves existing _hoodie_commit_time. This means users can run incremental queries on clustered data without any side-effects."}),(0,n.jsx)(a.td,{children:"False"})]})]})]}),"\n",(0,n.jsx)(a.h2,{id:"asynchronous-clustering",children:"Asynchronous Clustering"}),"\n",(0,n.jsxs)(a.p,{children:["Previously, we have seen how users\ncan ",(0,n.jsx)(a.a,{href:"/blog/2021/01/27/hudi-clustering-intro#setting-up-clustering",children:"setup inline clustering"}),".\nAdditionally, users can\nleverage ",(0,n.jsx)(a.a,{href:"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+19+Clustering+data+for+freshness+and+query+performance#RFC19Clusteringdataforfreshnessandqueryperformance-SetupforAsyncclusteringJob",children:"HoodieClusteringJob"}),"\nto setup 2-step asynchronous clustering."]}),"\n",(0,n.jsx)(a.h3,{id:"hoodieclusteringjob",children:"HoodieClusteringJob"}),"\n",(0,n.jsxs)(a.p,{children:["With the release of Hudi version 0.9.0, we can schedule as well as execute clustering in the same step. We just need to\nspecify the ",(0,n.jsx)(a.code,{children:"\u2014mode"})," or ",(0,n.jsx)(a.code,{children:"-m"})," option. There are three modes:"]}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.code,{children:"schedule"}),": Make a clustering plan. This gives an instant which can be passed in execute mode."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.code,{children:"execute"}),": Execute a clustering plan at given instant which means --instant-time is required here."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.code,{children:"scheduleAndExecute"}),": Make a clustering plan first and execute that plan immediately."]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Note that to run this job while the original writer is still running, please enable multi-writing:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:"hoodie.write.concurrency.mode=optimistic_concurrency_control\nhoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.ZookeeperBasedLockProvider\n"})}),"\n",(0,n.jsx)(a.p,{children:"A sample spark-submit command to setup HoodieClusteringJob is as below:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"spark-submit \\\n--class org.apache.hudi.utilities.HoodieClusteringJob \\\n/path/to/hudi-utilities-bundle/target/hudi-utilities-bundle_2.12-0.9.0-SNAPSHOT.jar \\\n--props /path/to/config/clusteringjob.properties \\\n--mode scheduleAndExecute \\\n--base-path /path/to/hudi_table/basePath \\\n--table-name hudi_table_schedule_clustering \\\n--spark-memory 1g\n"})}),"\n",(0,n.jsxs)(a.p,{children:["A sample ",(0,n.jsx)(a.code,{children:"clusteringjob.properties"})," file:"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:"hoodie.clustering.async.enabled=true\nhoodie.clustering.async.max.commits=4\nhoodie.clustering.plan.strategy.target.file.max.bytes=1073741824\nhoodie.clustering.plan.strategy.small.file.limit=629145600\nhoodie.clustering.execution.strategy.class=org.apache.hudi.client.clustering.run.strategy.SparkSortAndSizeExecutionStrategy\nhoodie.clustering.plan.strategy.sort.columns=column1,column2\n"})}),"\n",(0,n.jsx)(a.h3,{id:"hoodiedeltastreamer",children:"HoodieDeltaStreamer"}),"\n",(0,n.jsxs)(a.p,{children:["This brings us to our users' favorite utility in Hudi. Now, we can trigger asynchronous clustering with DeltaStreamer.\nJust set the ",(0,n.jsx)(a.code,{children:"hoodie.clustering.async.enabled"})," config to true and specify other clustering config in properties file\nwhose location can be pased as ",(0,n.jsx)(a.code,{children:"\u2014props"})," when starting the deltastreamer (just like in the case of HoodieClusteringJob)."]}),"\n",(0,n.jsx)(a.p,{children:"A sample spark-submit command to setup HoodieDeltaStreamer is as below:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"spark-submit \\\n--class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer \\\n/path/to/hudi-utilities-bundle/target/hudi-utilities-bundle_2.12-0.9.0-SNAPSHOT.jar \\\n--props /path/to/config/clustering_kafka.properties \\\n--schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \\\n--source-class org.apache.hudi.utilities.sources.AvroKafkaSource \\\n--source-ordering-field impresssiontime \\\n--table-type COPY_ON_WRITE \\\n--target-base-path /path/to/hudi_table/basePath \\\n--target-table impressions_cow_cluster \\\n--op INSERT \\\n--hoodie-conf hoodie.clustering.async.enabled=true \\\n--continuous\n"})}),"\n",(0,n.jsx)(a.h3,{id:"spark-structured-streaming",children:"Spark Structured Streaming"}),"\n",(0,n.jsx)(a.p,{children:"We can also enable asynchronous clustering with Spark structured streaming sink as shown below."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-scala",children:'val commonOpts = Map(\n   "hoodie.insert.shuffle.parallelism" -> "4",\n   "hoodie.upsert.shuffle.parallelism" -> "4",\n   DataSourceWriteOptions.RECORDKEY_FIELD.key -> "_row_key",\n   DataSourceWriteOptions.PARTITIONPATH_FIELD.key -> "partition",\n   DataSourceWriteOptions.PRECOMBINE_FIELD.key -> "timestamp",\n   HoodieWriteConfig.TBL_NAME.key -> "hoodie_test"\n)\n\ndef getAsyncClusteringOpts(isAsyncClustering: String, \n                           clusteringNumCommit: String, \n                           executionStrategy: String):Map[String, String] = {\n   commonOpts + (DataSourceWriteOptions.ASYNC_CLUSTERING_ENABLE.key -> isAsyncClustering,\n           HoodieClusteringConfig.ASYNC_CLUSTERING_MAX_COMMITS.key -> clusteringNumCommit,\n           HoodieClusteringConfig.EXECUTION_STRATEGY_CLASS_NAME.key -> executionStrategy\n   )\n}\n\ndef initStreamingWriteFuture(hudiOptions: Map[String, String]): Future[Unit] = {\n   val streamingInput = // define the source of streaming\n   Future {\n      println("streaming starting")\n      streamingInput\n              .writeStream\n              .format("org.apache.hudi")\n              .options(hudiOptions)\n              .option("checkpointLocation", basePath + "/checkpoint")\n              .mode(Append)\n              .start()\n              .awaitTermination(10000)\n      println("streaming ends")\n   }\n}\n\ndef structuredStreamingWithClustering(): Unit = {\n   val df = //generate data frame\n   val hudiOptions = getClusteringOpts("true", "1", "org.apache.hudi.client.clustering.run.strategy.SparkSortAndSizeExecutionStrategy")\n   val f1 = initStreamingWriteFuture(hudiOptions)\n   Await.result(f1, Duration.Inf)\n}\n'})}),"\n",(0,n.jsx)(a.h2,{id:"conclusion-and-future-work",children:"Conclusion and Future Work"}),"\n",(0,n.jsx)(a.p,{children:"In this post, we discussed different clustering strategies and how to setup asynchronous clustering. The story is not\nover yet and future work entails:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Support clustering with updates."}),"\n",(0,n.jsx)(a.li,{children:"CLI tools to support clustering."}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["Please follow this ",(0,n.jsx)(a.a,{href:"https://issues.apache.org/jira/browse/HUDI-1042",children:"JIRA"})," to learn more about active development on\nthis issue. We look forward to contributions from the community. Hope you enjoyed this post. Put your Hudi on and keep\nstreaming!"]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},28955:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(42314),n=t(74848),s=t(28453),r=t(9230);const o={title:"Introducing Apache Hudi support with AWS Glue crawlers",excerpt:"Introducing Apache Hudi support with AWS Glue crawlers",author:"Noritaka Sekiyama, Kyle Duong, Sandeep Adwankar",category:"blog",image:"/assets/images/blog/2023-11-22-Introducing-Apache-Hudi-support-with-AWS-Glue-crawlers.png",tags:["apache hudi","how-to","aws glue crawlers"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/introducing-apache-hudi-support-with-aws-glue-crawlers/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},29273:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2020/10/21/Data-Lake-Change-Capture-using-Apache-Hudi-and-Amazon-AMS-EMR","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-10-21-Data-Lake-Change-Capture-using-Apache-Hudi-and-Amazon-AMS-EMR.mdx","source":"@site/blog/2020-10-21-Data-Lake-Change-Capture-using-Apache-Hudi-and-Amazon-AMS-EMR.mdx","title":"Data Lake Change Capture using Apache Hudi & Amazon AMS/EMR","description":"Redirecting... please wait!!","date":"2020-10-21T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"change data capture","permalink":"/blog/tags/change-data-capture"},{"inline":true,"label":"cdc","permalink":"/blog/tags/cdc"},{"inline":true,"label":"towardsdatascience","permalink":"/blog/tags/towardsdatascience"}],"readingTime":0.16,"hasTruncateMarker":false,"authors":[{"name":"Manoj Kukreja","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Data Lake Change Capture using Apache Hudi & Amazon AMS/EMR","authors":[{"name":"Manoj Kukreja"}],"category":"blog","image":"/assets/images/blog/2020-10-21-Data-Lake-Change-Capture-using-Apache-Hudi-and-Amazon-AMS-EMR.jpeg","tags":["how-to","change data capture","cdc","towardsdatascience"]},"unlisted":false,"prevItem":{"title":"Architecting Data Lakes for the Modern Enterprise at Data Summit Connect Fall 2020","permalink":"/blog/2020/10/21/Architecting-Data-Lakes-for-the-Modern-Enterprise-at-Data-Summit-Connect-Fall-2020"},"nextItem":{"title":"Apply record level changes from relational databases to Amazon S3 data lake using Apache Hudi on Amazon EMR and AWS Database Migration Service","permalink":"/blog/2020/10/19/hudi-meets-aws-emr-and-aws-dms"}}')},29287:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/02/12/Open-Source-Data-Lake-Table-Formats-Evaluating-Current-Interest-and-Rate-of-Adoption","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-02-12-Open-Source-Data-Lake-Table-Formats-Evaluating-Current-Interest-and-Rate-of-Adoption.mdx","source":"@site/blog/2022-02-12-Open-Source-Data-Lake-Table-Formats-Evaluating-Current-Interest-and-Rate-of-Adoption.mdx","title":"Open Source Data Lake Table Formats: Evaluating Current Interest and Rate of Adoption","description":"Redirecting... please wait!!","date":"2022-02-12T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"datalake","permalink":"/blog/tags/datalake"},{"inline":true,"label":"comparison","permalink":"/blog/tags/comparison"},{"inline":true,"label":"community","permalink":"/blog/tags/community"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Gary Stafford","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Open Source Data Lake Table Formats: Evaluating Current Interest and Rate of Adoption","authors":[{"name":"Gary Stafford"}],"category":"blog","image":"/assets/images/blog/2022-02-12-open-source-data-lake-formats.png","tags":["blog","datalake","comparison","community","medium"]},"unlisted":false,"prevItem":{"title":"Fresher Data Lake on AWS S3","permalink":"/blog/2022/02/17/Fresher-Data-Lake-on-AWS-S3"},"nextItem":{"title":"ACID transformations on Distributed file system","permalink":"/blog/2022/02/09/ACID-transformations-on-Distributed-file-system"}}')},29564:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(58588),n=t(74848),s=t(28453),r=t(9230);const o={title:"Part1: Query apache hudi dataset in an amazon S3 data lake with amazon athena : Read optimized queries",authors:[{name:"Dhiraj Thakur"},{name:"Sameer Goel"},{name:"Imtiaz Sayed"}],category:"blog",image:"/assets/images/blog/2021-07-16-query-hudi-using-athena-ro-queries.png",tags:["how-to","read optimized query","amazon"]},l=void 0,d={authorsImageUrls:[void 0,void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/part-1-query-an-apache-hudi-dataset-in-an-amazon-s3-data-lake-with-amazon-athena-part-1-read-optimized-queries/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},30005:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(95508),n=t(74848),s=t(28453);const r={title:"Monitor Hudi metrics with Datadog",excerpt:"Introducing the feature of reporting Hudi metrics via Datadog HTTP API",author:"rxu",category:"blog",tags:["how-to","metrics","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Availability",id:"availability",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Configurations",id:"configurations",level:2},{value:"Demo",id:"demo",level:2}];function c(e){const a={a:"a",code:"code",em:"em",h2:"h2",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.h2,{id:"availability",children:"Availability"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"0.6.0 (unreleased)"})}),"\n",(0,n.jsx)(a.h2,{id:"introduction",children:"Introduction"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.a,{href:"https://www.datadoghq.com/",children:"Datadog"})," is a popular monitoring service. In the upcoming ",(0,n.jsx)(a.code,{children:"0.6.0"})," release of Apache Hudi, we will introduce the feature of reporting Hudi metrics via Datadog HTTP API, in addition to the current reporter types: Graphite and JMX."]}),"\n",(0,n.jsx)(a.h2,{id:"configurations",children:"Configurations"}),"\n",(0,n.jsx)(a.p,{children:"Similar to other supported reporters, turning on Datadog reporter requires these 2 properties."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-properties",children:"hoodie.metrics.on=true\nhoodie.metrics.reporter.type=DATADOG\n"})}),"\n",(0,n.jsxs)(a.p,{children:["The following property sets the Datadog API site. It determines whether the requests will be sent to ",(0,n.jsx)(a.code,{children:"api.datadoghq.eu"})," (EU) or ",(0,n.jsx)(a.code,{children:"api.datadoghq.com"})," (US). Set this according to your Datadog account settings."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-properties",children:"hoodie.metrics.datadog.api.site=EU # or US\n"})}),"\n",(0,n.jsxs)(a.p,{children:["The property ",(0,n.jsx)(a.code,{children:"hoodie.metrics.datadog.api.key"})," allows you to set the api key directly from the configuration."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-properties",children:"hoodie.metrics.datadog.api.key=<your api key>\nhoodie.metrics.datadog.api.key.supplier=<your api key supplier>\n"})}),"\n",(0,n.jsxs)(a.p,{children:["Due to security consideration in some cases, you may prefer to return the api key at runtime. To go with this approach, implement ",(0,n.jsx)(a.code,{children:"java.util.function.Supplier<String>"})," and set the implementation's FQCN to ",(0,n.jsx)(a.code,{children:"hoodie.metrics.datadog.api.key.supplier"}),", and make sure ",(0,n.jsx)(a.code,{children:"hoodie.metrics.datadog.api.key"})," is ",(0,n.jsx)(a.em,{children:"not"})," set since it will take higher precedence."]}),"\n",(0,n.jsx)(a.p,{children:"The following property helps segregate metrics by setting different prefixes for different jobs."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-properties",children:"hoodie.metrics.datadog.metric.prefix=<your metrics prefix>\n"})}),"\n",(0,n.jsxs)(a.p,{children:["Note that it will use ",(0,n.jsx)(a.code,{children:"."})," to delimit the prefix and the metric name. For example, if the prefix is set to ",(0,n.jsx)(a.code,{children:"foo"}),", then ",(0,n.jsx)(a.code,{children:"foo."})," will be prepended to the metric name."]}),"\n",(0,n.jsx)(a.p,{children:"There are other optional properties, which are explained in the configuration reference page."}),"\n",(0,n.jsx)(a.h2,{id:"demo",children:"Demo"}),"\n",(0,n.jsxs)(a.p,{children:["In this demo, we ran a ",(0,n.jsx)(a.code,{children:"HoodieDeltaStreamer"})," job with metrics turn on and other configurations set properly."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"datadog metrics demo",src:t(22429).A+"",width:"752",height:"925"})}),"\n",(0,n.jsx)(a.p,{children:"As shown above, we were able to collect Hudi's action-related metrics like"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.code,{children:"<prefix>.<table name>.commit.totalScanTime"})}),"\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.code,{children:"<prefix>.<table name>.clean.duration"})}),"\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.code,{children:"<prefix>.<table name>.index.lookup.duration"})}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["as well as ",(0,n.jsx)(a.code,{children:"HoodieDeltaStreamer"}),"-specific metrics"]}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.code,{children:"<prefix>.<table name>.deltastreamer.duration"})}),"\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.code,{children:"<prefix>.<table name>.deltastreamer.hiveSyncDuration"})}),"\n"]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},30204:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(32877),n=t(74848),s=t(28453);const r={title:"Delete support in Hudi",excerpt:"Deletes are supported at a record level in Hudi with 0.5.1 release. This blog is a \u201chow to\u201d blog on how to delete records in hudi.",author:"shivnarayan",category:"blog",tags:["how-to","deletes","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Delete using RDD Level APIs",id:"delete-using-rdd-level-apis",level:3},{value:"Deletion with Datasource",id:"deletion-with-datasource",level:3},{value:"Deletion with HoodieDeltaStreamer",id:"deletion-with-hoodiedeltastreamer",level:2}];function c(e){const a={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.p,{children:'Deletes are supported at a record level in Hudi with 0.5.1 release. This blog is a "how to" blog on how to delete records in hudi. Deletes can be done with 3 flavors: Hudi RDD APIs, with Spark data source and with DeltaStreamer.'}),"\n",(0,n.jsx)(a.h3,{id:"delete-using-rdd-level-apis",children:"Delete using RDD Level APIs"}),"\n",(0,n.jsxs)(a.p,{children:["If you have embedded  ",(0,n.jsx)(a.em,{children:"HoodieWriteClient"})," , then deletion is as simple as passing in a  ",(0,n.jsx)(a.em,{children:"JavaRDD<HoodieKey>"})," to the delete api."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"// Fetch list of HoodieKeys from elsewhere that needs to be deleted\n// convert to JavaRDD if required. JavaRDD<HoodieKey> toBeDeletedKeys\nList<WriteStatus> statuses = writeClient.delete(toBeDeletedKeys, commitTime);\n"})}),"\n",(0,n.jsx)(a.h3,{id:"deletion-with-datasource",children:"Deletion with Datasource"}),"\n",(0,n.jsx)(a.p,{children:"Now we will walk through an example of how to perform deletes on a sample dataset using the Datasource API. Quick Start has the same example as below. Feel free to check it out."}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Step 1"})," : Launch spark shell"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"bin/spark-shell --packages org.apache.hudi:hudi-spark-bundle:0.5.1-incubating \\\n  --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer'\n"})}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Step 2"})," : Import as required and set up table name, etc for sample dataset"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-scala",children:'import org.apache.hudi.QuickstartUtils._\nimport scala.collection.JavaConversions._\nimport org.apache.spark.sql.SaveMode._\nimport org.apache.hudi.DataSourceReadOptions._\nimport org.apache.hudi.DataSourceWriteOptions._\nimport org.apache.hudi.config.HoodieWriteConfig._\n     \nval tableName = "hudi_cow_table"\nval basePath = "file:///tmp/hudi_cow_table"\nval dataGen = new DataGenerator\n'})}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Step 3"})," : Insert data. Generate some new trips, load them into a DataFrame and write the DataFrame into the Hudi dataset as below."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-scala",children:'val inserts = convertToStringList(dataGen.generateInserts(10))\nval df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\ndf.write.format("org.apache.hudi").\n  options(getQuickstartWriteConfigs).\n  option(PRECOMBINE_FIELD_OPT_KEY, "ts").\n  option(RECORDKEY_FIELD_OPT_KEY, "uuid").\n  option(PARTITIONPATH_FIELD_OPT_KEY, "partitionpath").\n  option(TABLE_NAME, tableName).\n  mode(Overwrite).\n  save(basePath);\n'})}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Note:"})," For non-partitioned table, set"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:'option(KEYGENERATOR_CLASS_PROP, "org.apache.hudi.keygen.NonpartitionedKeyGenerator")\n'})}),"\n",(0,n.jsxs)(a.p,{children:["Checkout ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2021/02/13/hudi-key-generators",children:"https://hudi.apache.org/blog/2021/02/13/hudi-key-generators"})," for more options"]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Step 4"})," : Query data. Load the data files into a DataFrame."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-scala",children:'val roViewDF = spark.read.\n  format("org.apache.hudi").\n  load(basePath + "/*/*/*/*")\nroViewDF.createOrReplaceTempView("hudi_ro_table")\nspark.sql("select count(*) from hudi_ro_table").show() // should return 10 (number of records inserted above)\nval riderValue = spark.sql("select distinct rider from hudi_ro_table").show()\n// copy the value displayed to be used in next step\n'})}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Step 5"}),' : Fetch records that needs to be deleted, with the above rider value. This example is just to illustrate how to delete. In real world, use a select query using spark sql to fetch records that needs to be deleted and from the result we could invoke deletes as given below. Example rider value used is "rider-213".']}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-scala",children:"val df = spark.sql(\"select uuid, partitionPath from hudi_ro_table where rider = 'rider-213'\")\n"})}),"\n",(0,n.jsx)(a.p,{children:"// Replace the above query with any other query that will fetch records to be deleted."}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Step 6"})," : Issue deletes"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-scala",children:'val deletes = dataGen.generateDeletes(df.collectAsList())\nval df = spark.read.json(spark.sparkContext.parallelize(deletes, 2));\ndf.write.format("org.apache.hudi").\n  options(getQuickstartWriteConfigs).\n  option(OPERATION_OPT_KEY,"delete").\n  option(PRECOMBINE_FIELD_OPT_KEY, "ts").\n  option(RECORDKEY_FIELD_OPT_KEY, "uuid").\n  option(PARTITIONPATH_FIELD_OPT_KEY, "partitionpath").\n  option(TABLE_NAME, tableName).\n  mode(Append).\n  save(basePath);\n'})}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Note:"})," For non-partitioned table, set"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:'option(KEYGENERATOR_CLASS_PROP, "org.apache.hudi.keygen.NonpartitionedKeyGenerator")\n'})}),"\n",(0,n.jsxs)(a.p,{children:["Checkout ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2021/02/13/hudi-key-generators",children:"https://hudi.apache.org/blog/2021/02/13/hudi-key-generators"})," for more options"]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Step 7"})," : Reload the table and verify that the records are deleted"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-scala",children:'val roViewDFAfterDelete = spark.\n  read.\n  format("org.apache.hudi").\n  load(basePath + "/*/*/*/*")\nroViewDFAfterDelete.createOrReplaceTempView("hudi_ro_table")\nspark.sql("select uuid, partitionPath from hudi_ro_table where rider = \'rider-213\'").show() // should not return any rows\n'})}),"\n",(0,n.jsx)(a.h2,{id:"deletion-with-hoodiedeltastreamer",children:"Deletion with HoodieDeltaStreamer"}),"\n",(0,n.jsxs)(a.p,{children:["Deletion with ",(0,n.jsx)(a.code,{children:"HoodieDeltaStreamer"}),' takes the same path as upsert and so it relies on a specific field called  "',(0,n.jsx)(a.em,{children:"_hoodie_is_deleted"}),'" of type boolean in each record.']}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["If a record has the field value set to  ",(0,n.jsx)(a.em,{children:"false"})," or it's not present, then it is considered a regular upsert"]}),"\n",(0,n.jsxs)(a.li,{children:["if not (if the value is set to  ",(0,n.jsx)(a.em,{children:"true"})," ), then its considered to be deleted record."]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"This essentially means that the schema has to be changed for the source, to add this field and all incoming records are expected to have this field set. We will be working to relax this in future releases."}),"\n",(0,n.jsx)(a.p,{children:"Lets say the original schema is:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-json",children:'{\n  "type":"record",\n  "name":"example_tbl",\n  "fields":[{\n     "name": "uuid",\n     "type": "String"\n  }, {\n     "name": "ts",\n     "type": "string"\n  },  {\n     "name": "partitionPath",\n     "type": "string"\n  }, {\n     "name": "rank",\n     "type": "long"\n  }\n]}\n'})}),"\n",(0,n.jsxs)(a.p,{children:["To leverage deletion capabilities of ",(0,n.jsx)(a.code,{children:"DeltaStreamer"}),", you have to change the schema as below."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-json",children:'{\n  "type":"record",\n  "name":"example_tbl",\n  "fields":[{\n     "name": "uuid",\n     "type": "String"\n  }, {\n     "name": "ts",\n     "type": "string"\n  },  {\n     "name": "partitionPath",\n     "type": "string"\n  }, {\n     "name": "rank",\n     "type": "long"\n  }, {\n    "name" : "_hoodie_is_deleted",\n    "type" : "boolean",\n    "default" : false\n  }\n]}\n'})}),"\n",(0,n.jsx)(a.p,{children:"Example incoming record for upsert"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-json",children:'{\n  "ts": 0.0,\n  "uuid":"69cdb048-c93e-4532-adf9-f61ce6afe605",\n  "rank": 1034,\n  "partitionpath":"americas/brazil/sao_paulo",\n  "_hoodie_is_deleted":false\n}\n'})}),"\n",(0,n.jsx)(a.p,{children:"Example incoming record that needs to be deleted"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-json",children:'{\n  "ts": 0.0,\n  "uuid": "19tdb048-c93e-4532-adf9-f61ce6afe10",\n  "rank": 1045,\n  "partitionpath":"americas/brazil/sao_paulo",\n  "_hoodie_is_deleted":true\n}\n'})}),"\n",(0,n.jsx)(a.p,{children:"These are one time changes. Once these are in, then the DeltaStreamer pipeline will handle both upserts and deletions within every batch. Each batch could contain a mix of upserts and deletes and no additional step or changes are required after this. Note that this is to perform hard deletion instead of soft deletion."})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},30205:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(25423),n=t(74848),s=t(28453),r=t(9230);const o={title:"Deleting Items from Apache Hudi using Delta Streamer in UPSERT Mode with Kafka Avro Messages",excerpt:"Deleting Items from Apache Hudi using Delta Streamer in UPSERT Mode with Kafka Avro Messages",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2024-01-18-Deleting-Items-from-Apache-Hudi-using-Delta-Streamer-in-UPSERT-Mode-with-Kafka-Avro-Messages.png",tags:["blog","apache hudi","linkedin","beginner","hudi streamer","deltastreamer","apache kafka","apache avro","upsert","delete"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.linkedin.com/pulse/deleting-items-from-apache-hudi-using-delta-streamer-upsert-shah-sxlce/?utm_source=share&utm_medium=member_ios&utm_campaign=share_via",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},30597:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(77731),n=t(74848),s=t(28453),r=t(9230);const o={title:"Build a serverless pipeline to analyze streaming data using AWS Glue, Apache Hudi, and Amazon S3",authors:[{name:"Nikhil Khokhar"},{name:"Dipta Bhattacharya"}],category:"blog",image:"/assets/images/blog/2022-03-09-serverless-pipeline-using-glue-hudi-s3.png",tags:["how-to","streaming ingestion","amazon"]},l=void 0,d={authorsImageUrls:[void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/build-a-serverless-pipeline-to-analyze-streaming-data-using-aws-glue-apache-hudi-and-amazon-s3/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},30776:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2020/06/04/The-Apache-Software-Foundation-Announces-Apache-Hudi-as-a-Top-Level-Project","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-06-04-The-Apache-Software-Foundation-Announces-Apache-Hudi-as-a-Top-Level-Project.mdx","source":"@site/blog/2020-06-04-The-Apache-Software-Foundation-Announces-Apache-Hudi-as-a-Top-Level-Project.mdx","title":"The Apache Software Foundation Announces Apache\xae Hudi\u2122 as a Top-Level Project","description":"Redirecting... please wait!!","date":"2020-06-04T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache","permalink":"/blog/tags/apache"}],"readingTime":0.11,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"The Apache Software Foundation Announces Apache\xae Hudi\u2122 as a Top-Level Project","category":"blog","image":"/assets/images/asf_logo.svg","tags":["blog","apache"]},"unlisted":false,"prevItem":{"title":"Building a Large-scale Transactional Data Lake at Uber Using Apache Hudi","permalink":"/blog/2020/06/09/Building-a-Large-scale-Transactional-Data-Lake-at-Uber-Using-Apache-Hudi"},"nextItem":{"title":"Monitor Hudi metrics with Datadog","permalink":"/blog/2020/05/28/monitoring-hudi-metrics-with-datadog"}}')},30833:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/04/12/Build-Slowly-Changing-Dimensions-Type-2-SCD2-with-Apache-Spark-and-Apache-Hudi-on-Amazon-EMR","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-04-12-Build-Slowly-Changing-Dimensions-Type-2-SCD2-with-Apache-Spark-and-Apache-Hudi-on-Amazon-EMR.mdx","source":"@site/blog/2021-04-12-Build-Slowly-Changing-Dimensions-Type-2-SCD2-with-Apache-Spark-and-Apache-Hudi-on-Amazon-EMR.mdx","title":"Build Slowly Changing Dimensions Type 2 (SCD2) with Apache Spark and Apache Hudi on Amazon EMR","description":"Redirecting... please wait!!","date":"2021-04-12T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"scd2","permalink":"/blog/tags/scd-2"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"David Greenshtein","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Build Slowly Changing Dimensions Type 2 (SCD2) with Apache Spark and Apache Hudi on Amazon EMR","authors":[{"name":"David Greenshtein"}],"category":"blog","image":"/assets/images/blog/aws.jpg","tags":["how-to","scd2","amazon"]},"unlisted":false,"prevItem":{"title":"Experts primer on Apache Hudi","permalink":"/blog/2021/05/12/Experts-primer-on-Apache-Hudi"},"nextItem":{"title":"New features from Apache hudi in Amazon EMR","permalink":"/blog/2021/03/11/New-features-from-Apache-hudi-in-Amazon-EMR"}}')},30938:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(47901),n=t(74848),s=t(28453),r=t(9230);const o={title:"Speed up your write latencies using Bucket Index in Apache Hudi",authors:[{name:"Sivabalan Narayanan"}],category:"blog",image:"/assets/images/blog/2023-04-07-Speed-up-your-write-latencies-using-Bucket-Index-in-Apache-Hudi.png",tags:["how-to","indexing","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@simpsons/speed-up-your-write-latencies-using-bucket-index-in-apache-hudi-2f7c297493dc",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},31007:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(54386),n=t(74848),s=t(28453);const r={title:"Adding support for Virtual Keys in Hudi",excerpt:"Supporting Virtual keys in Hudi for reducing storage overhead",author:"shivnarayan",category:"blog",tags:["design","metadata","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Virtual Key support",id:"virtual-key-support",level:2},{value:"Configurations",id:"configurations",level:3},{value:"Supported Key Generators with CopyOnWrite(COW) table:",id:"supported-key-generators-with-copyonwritecow-table",level:4},{value:"Supported Key Generators with MergeOnRead(MOR) table:",id:"supported-key-generators-with-mergeonreadmor-table",level:4},{value:"Supported Index types:",id:"supported-index-types",level:4},{value:"Supported Operations",id:"supported-operations",level:3},{value:"Sample Output",id:"sample-output",level:3},{value:"Incremental Queries",id:"incremental-queries",level:3},{value:"Conclusion",id:"conclusion",level:3}];function c(e){const a={a:"a",admonition:"admonition",br:"br",code:"code",h2:"h2",h3:"h3",h4:"h4",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(a.p,{children:["Apache Hudi helps you build and manage data lakes with different table types, config knobs to cater to everyone's need.\nHudi adds per record metadata fields like ",(0,n.jsx)(a.code,{children:"_hoodie_record_key"}),", ",(0,n.jsx)(a.code,{children:"_hoodie_partition path"}),", ",(0,n.jsx)(a.code,{children:"_hoodie_commit_time"})," which serves multiple purposes.\nThey assist in avoiding re-computing the record key, partition path during merges, compaction and other table operations\nand also assists in supporting ",(0,n.jsx)(a.a,{href:"/blog/2021/07/21/streaming-data-lake-platform#readers",children:"record-level"})," incremental queries (in comparison to other table formats, that merely track files).\nIn addition, it ensures data quality by ensuring unique key constraints are enforced even if the key field changes for a given table, during its lifetime.\nBut one of the repeated asks from the community is to leverage existing fields and not to add additional meta fields, for simple use-cases where such benefits are not desired or key changes are very rare."]}),"\n",(0,n.jsx)(a.h2,{id:"virtual-key-support",children:"Virtual Key support"}),"\n",(0,n.jsx)(a.p,{children:"Hudi now supports virtual keys, where Hudi meta fields can be computed on demand from the data fields. Currently, the meta fields are\ncomputed once and stored as per record metadata and re-used across various operations. If one does not need incremental query support,\nthey can start leveraging Hudi's Virtual key support and still go about using Hudi to build and manage their data lake to reduce the storage\noverhead due to per record metadata."}),"\n",(0,n.jsx)(a.h3,{id:"configurations",children:"Configurations"}),"\n",(0,n.jsxs)(a.p,{children:["Virtual keys can be enabled for a given table using the below config. When set to ",(0,n.jsx)(a.code,{children:"hoodie.populate.meta.fields=false"}),",\nHudi will use virtual keys for the corresponding table. Default value for this config is ",(0,n.jsx)(a.code,{children:"true"}),", which means, all  meta fields will be added by default."]}),"\n",(0,n.jsxs)(a.p,{children:["Once virtual keys are enabled, it can't be disabled for a given hudi table, because already stored records may not have\nthe meta fields populated. But if you have an existing table from an older version of hudi, virtual keys can be enabled.\nAnother constraint w.r.t virtual key support is that, Key generator properties for a given table cannot be changed through\nthe course of the lifecycle of a given hudi table. In this model, the user also shares responsibility of ensuring uniqueness\nof key within a table. For instance, if you configure record key to point to ",(0,n.jsx)(a.code,{children:"field_5"})," for few batches of write and later switch to ",(0,n.jsx)(a.code,{children:"field_10"}),",\nHudi cannot guarantee uniqueness of key, since older writes could have had duplicates for ",(0,n.jsx)(a.code,{children:"field_10"}),"."]}),"\n",(0,n.jsx)(a.p,{children:"With virtual keys, keys will have to be re-computed everytime when in need (merges, compaction, MOR snapshot read). Hence we\nsupport virtual keys for all built-in key generators on Copy-On-Write tables. Supporting all key generators on Merge-On-Read table\nwould entail reading all fields out of base and delta logs, sacrificing core columnar query performance, which will be prohibitively expensive\nfor users. Thus, we support only simple key generators (the default key generator, where both record key and partition path refer\nto an existing field ) for now."}),"\n",(0,n.jsx)(a.h4,{id:"supported-key-generators-with-copyonwritecow-table",children:"Supported Key Generators with CopyOnWrite(COW) table:"}),"\n",(0,n.jsx)(a.p,{children:"SimpleKeyGenerator, ComplexKeyGenerator, CustomKeyGenerator, TimestampBasedKeyGenerator and NonpartitionedKeyGenerator."}),"\n",(0,n.jsx)(a.h4,{id:"supported-key-generators-with-mergeonreadmor-table",children:"Supported Key Generators with MergeOnRead(MOR) table:"}),"\n",(0,n.jsx)(a.p,{children:"SimpleKeyGenerator"}),"\n",(0,n.jsx)(a.h4,{id:"supported-index-types",children:"Supported Index types:"}),"\n",(0,n.jsx)(a.p,{children:'Only "SIMPLE" and "GLOBAL_SIMPLE" index types are supported in the first cut. We plan to add support for other index\n(BLOOM, etc) in future releases.'}),"\n",(0,n.jsx)(a.h3,{id:"supported-operations",children:"Supported Operations"}),"\n",(0,n.jsx)(a.p,{children:"All existing features are supported for a hudi table with virtual keys, except the incremental\nqueries. Which means, cleaning, archiving, metadata table, clustering, etc can be enabled for a hudi table with\nvirtual keys enabled. So, you are able to merely use Hudi as a transactional table format with all the awesome\ntable service runtimes and platform services, if you wish to do so, without incurring any overheads associated with\nsupport for incremental data processing."}),"\n",(0,n.jsx)(a.h3,{id:"sample-output",children:"Sample Output"}),"\n",(0,n.jsxs)(a.p,{children:["As called out earlier, one has to set ",(0,n.jsx)(a.code,{children:"hoodie.populate.meta.fields=false"})," to enable virtual keys. Let's see the\ndifference between records of a hudi table with and without virtual keys."]}),"\n",(0,n.jsx)(a.p,{children:"Here are some sample records for a regular hudi table (virtual keys disabled)"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:"+--------------------+--------------------------------------+--------------------------------------+---------+---------+-------------------+\n|_hoodie_commit_time |           _hoodie_record_key         |        _hoodie_partition_path        |  rider  | driver  |        fare       |\n+--------------------+--------------------------------------+--------------------------------------+---------+---------+-------------------+\n|   20210825154123   | eb7819f1-6f04-429d-8371-df77620b9527 | americas/united_states/san_francisco |rider-284|driver-284|98.3428192817987  |\n|   20210825154123   | 37ea44f1-fda7-4ec4-84de-f43f5b5a4d84 | americas/united_states/san_francisco |rider-213|driver-213|19.179139106643607|\n|   20210825154123   | aa601d6b-7cc5-4b82-9687-675d0081616e | americas/united_states/san_francisco |rider-213|driver-213|93.56018115236618 |\n|   20210825154123   | 494bc080-881c-48be-8f8a-8f1739781816 | americas/united_states/san_francisco |rider-284|driver-284|90.9053809533154  |\n|   20210825154123   | 09573277-e1c1-4cdd-9b45-57176f184d4d | americas/united_states/san_francisco |rider-284|driver-284|49.527694252432056|\n|   20210825154123   | c9b055ed-cd28-4397-9704-93da8b2e601f | americas/brazil/sao_paulo            |rider-213|driver-213|43.4923811219014  |\n|   20210825154123   | e707355a-b8c0-432d-a80f-723b93dc13a8 | americas/brazil/sao_paulo            |rider-284|driver-284|63.72504913279929 |\n|   20210825154123   | d3c39c9e-d128-497a-bf3e-368882f45c28 | americas/brazil/sao_paulo            |rider-284|driver-284|91.99515909032544 |\n|   20210825154123   | 159441b0-545b-460a-b671-7cc2d509f47b | asia/india/chennai                   |rider-284|driver-284|9.384124531808036 |\n|   20210825154123   | 16031faf-ad8d-4968-90ff-16cead211d3c | asia/india/chennai                   |rider-284|driver-284|90.25710109008239 |\n+--------------------+--------------------------------------+--------------------------------------+---------+----------+------------------+\n"})}),"\n",(0,n.jsx)(a.p,{children:"And here are some sample records for a hudi table with virtual keys enabled."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:"+--------------------+------------------------+-------------------------+---------+---------+-------------------+\n|_hoodie_commit_time |    _hoodie_record_key  |  _hoodie_partition_path |  rider  | driver  |        fare       |\n+--------------------+------------------------+-------------------------+---------+---------+-------------------+\n|        null        |            null        |          null           |rider-284|driver-284|98.3428192817987  |\n|        null        |            null        |          null           |rider-213|driver-213|19.179139106643607|\n|        null        |            null        |          null           |rider-213|driver-213|93.56018115236618 |\n|        null        |            null        |          null           |rider-284|driver-284|90.9053809533154  |\n|        null        |            null        |          null           |rider-284|driver-284|49.527694252432056|\n|        null        |            null        |          null           |rider-213|driver-213|43.4923811219014  |\n|        null        |            null        |          null           |rider-284|driver-284|63.72504913279929 |\n|        null        |            null        |          null           |rider-284|driver-284|91.99515909032544 |\n|        null        |            null        |          null           |rider-284|driver-284|9.384124531808036 |\n|        null        |            null        |          null           |rider-284|driver-284|90.25710109008239 |\n+--------------------+------------------------+-------------------------+---------+----------+------------------+\n"})}),"\n",(0,n.jsx)(a.admonition,{type:"note",children:(0,n.jsx)(a.p,{children:"As you could see, all meta fields are null in storage, but all users fields remain intact similar to a regular table."})}),"\n",(0,n.jsx)(a.h3,{id:"incremental-queries",children:"Incremental Queries"}),"\n",(0,n.jsxs)(a.p,{children:["Since hudi does not maintain any metadata (like commit time at a record level) for a table with virtual keys enabled,",(0,n.jsx)(a.br,{}),"\n","incremental queries are not supported. An exception will be thrown as below when an incremental query is triggered for such\na table."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:'scala> val tripsIncrementalDF = spark.read.format("hudi").\n     |   option(QUERY_TYPE_OPT_KEY, QUERY_TYPE_INCREMENTAL_OPT_VAL).\n     |   option(BEGIN_INSTANTTIME_OPT_KEY, "20210827180901").load(basePath)\norg.apache.hudi.exception.HoodieException: Incremental queries are not supported when meta fields are disabled\n  at org.apache.hudi.IncrementalRelation.<init>(IncrementalRelation.scala:69)\n  at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:120)\n  at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:67)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:344)\n  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)\n  at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)\n  at scala.Option.getOrElse(Option.scala:189)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:232)\n  ... 61 elided\n'})}),"\n",(0,n.jsx)(a.h3,{id:"conclusion",children:"Conclusion"}),"\n",(0,n.jsxs)(a.p,{children:["Hope this blog was useful for you to learn yet another feature in Apache Hudi. If you are interested in\nHudi and looking to contribute, do check out ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/contribute/get-involved",children:"here"}),"."]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},31321:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/02/03/Onehouse-brings-a-fully-managed-lakehouse-to-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-02-03-Onehouse-brings-a-fully-managed-lakehouse-to-Apache-Hudi.mdx","source":"@site/blog/2022-02-03-Onehouse-brings-a-fully-managed-lakehouse-to-Apache-Hudi.mdx","title":"Onehouse brings a fully-managed lakehouse to Apache Hudi","description":"Redirecting... please wait!!","date":"2022-02-03T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"venturebeat","permalink":"/blog/tags/venturebeat"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Paul Sawers","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Onehouse brings a fully-managed lakehouse to Apache Hudi","authors":[{"name":"Paul Sawers"}],"category":"blog","image":"/assets/images/blog/2022-02-03-onehouse_billboard.png","tags":["blog","lakehouse","venturebeat"]},"unlisted":false,"prevItem":{"title":"ACID transformations on Distributed file system","permalink":"/blog/2022/02/09/ACID-transformations-on-Distributed-file-system"},"nextItem":{"title":"Onehouse Commitment to Openness","permalink":"/blog/2022/02/02/Onehouse-Commitment-to-Openness"}}')},31427:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(5709),n=t(74848),s=t(28453),r=t(9230);const o={title:"Understanding Apache Hudi's Consistency Model Part 2",author:"Jack Vanlightly",category:"blog",image:"/assets/images/blog/2024-04-24-understanding-apache-hudi-consistency-model-part-2.png",tags:["blog","consistency","concurrency control","multi writer","monotonic timestamp","timestamp collision","jack-vanlightly"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://jack-vanlightly.com/analyses/2024/4/24/understanding-apache-hudi-consistency-model-part-2",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},31505:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/Retain_latest_versions-723f83313beb86b46c9cd1fcb8ea0b25.png"},31818:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/08/03/MLOps-Wars-Versioned-Feature-Data-with-a-Lakehouse","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-08-03-MLOps-Wars-Versioned-Feature-Data-with-a-Lakehouse.mdx","source":"@site/blog/2021-08-03-MLOps-Wars-Versioned-Feature-Data-with-a-Lakehouse.mdx","title":"MLOps Wars: Versioned Feature Data with a Lakehouse","description":"Redirecting... please wait!!","date":"2021-08-03T00:00:00.000Z","tags":[{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"mlops","permalink":"/blog/tags/mlops"},{"inline":true,"label":"feature store","permalink":"/blog/tags/feature-store"},{"inline":true,"label":"incremental processing","permalink":"/blog/tags/incremental-processing"},{"inline":true,"label":"time travel query","permalink":"/blog/tags/time-travel-query"},{"inline":true,"label":"logicalclocks","permalink":"/blog/tags/logicalclocks"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"David Bzhalava","socials":{},"key":null,"page":null},{"name":"Jim Dowling","socials":{},"key":null,"page":null}],"frontMatter":{"title":"MLOps Wars: Versioned Feature Data with a Lakehouse","authors":[{"name":"David Bzhalava"},{"name":"Jim Dowling"}],"category":"blog","image":"/assets/images/blog/2021-08-03-mlops-wars.png","tags":["use-case","mlops","feature store","incremental processing","time travel query","logicalclocks"]},"unlisted":false,"prevItem":{"title":"Cost-Efficient Open Source Big Data Platform at Uber","permalink":"/blog/2021/08/11/Cost-Efficient-Open-Source-Big-Data-Platform-at-Uber"},"nextItem":{"title":"Baixin bank\u2019s real-time data lake evolution scheme based on Apache Hudi","permalink":"/blog/2021/07/26/Baixin-banksreal-time-data-lake-evolution-scheme-based-on-Apache-Hudi"}}')},31882:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image1-ad0429306e0ebe38c0eacbf4b2aed222.png"},32122:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig2-686d385820b946c158ab5110d3bcbaf8.png"},32345:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/06/16/Exploring-New-Frontiers-How-Apache-Flink-Apache-Hudi-and-Presto-Power-New-Insights-at-Scale","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-06-16-Exploring-New-Frontiers-How-Apache-Flink-Apache-Hudi-and-Presto-Power-New-Insights-at-Scale.mdx","source":"@site/blog/2023-06-16-Exploring-New-Frontiers-How-Apache-Flink-Apache-Hudi-and-Presto-Power-New-Insights-at-Scale.mdx","title":"Exploring New Frontiers: How Apache Flink, Apache Hudi and Presto Power New Insights at Scale","description":"Redirecting... please wait!!","date":"2023-06-16T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"prestocon","permalink":"/blog/tags/prestocon"},{"inline":true,"label":"flink","permalink":"/blog/tags/flink"},{"inline":true,"label":"presto","permalink":"/blog/tags/presto"},{"inline":true,"label":"streaming","permalink":"/blog/tags/streaming"},{"inline":true,"label":"incremental etl","permalink":"/blog/tags/incremental-etl"}],"readingTime":0.16,"hasTruncateMarker":false,"authors":[{"name":"Nadine Farah","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Exploring New Frontiers: How Apache Flink, Apache Hudi and Presto Power New Insights at Scale","authors":[{"name":"Nadine Farah"}],"category":"blog","image":"/assets/images/blog/2023-06-16-Exploring-New-Frontiers-How-Apache-Flink-Apache-Hudi-and-Presto-Power-New-Insights-at-Scale.png","tags":["blog","prestocon","flink","presto","streaming","incremental etl"]},"unlisted":false,"prevItem":{"title":"Timeline Server in Apache Hudi","permalink":"/blog/2023/06/20/timeline-server-in-apache-hudi"},"nextItem":{"title":"Cleaner and Archival in Apache Hudi","permalink":"/blog/2023/06/11/cleaner-and-archival-in-apache-hudi"}}')},32350:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/08/09/How-NerdWallet-uses-AWS-and-Apache-Hudi-to-build-a-serverless-real-time-analytics-platform","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-08-09-How-NerdWallet-uses-AWS-and-Apache-Hudi-to-build-a-serverless-real-time-analytics-platform.mdx","source":"@site/blog/2022-08-09-How-NerdWallet-uses-AWS-and-Apache-Hudi-to-build-a-serverless-real-time-analytics-platform.mdx","title":"How NerdWallet uses AWS and Apache Hudi to build a serverless, real-time analytics platform","description":"Redirecting... please wait!!","date":"2022-08-09T00:00:00.000Z","tags":[{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"near real-time analytics","permalink":"/blog/tags/near-real-time-analytics"},{"inline":true,"label":"incremental processing","permalink":"/blog/tags/incremental-processing"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"Kevin Chun","socials":{},"key":null,"page":null},{"name":"Dylan Qu","socials":{},"key":null,"page":null}],"frontMatter":{"title":"How NerdWallet uses AWS and Apache Hudi to build a serverless, real-time analytics platform","authors":[{"name":"Kevin Chun"},{"name":"Dylan Qu"}],"category":"blog","image":"/assets/images/blog/2022-08-09-How-NerdWallet-uses-AWS-and-Apache-Hudi-to-build-a-serverless-real-time-analytics-platform.png","tags":["use-case","near real-time analytics","incremental processing","amazon"]},"unlisted":false,"prevItem":{"title":"Use Flink Hudi to Build a Streaming Data Lake Platform","permalink":"/blog/2022/08/12/Use-Flink-Hudi-to-Build-a-Streaming-Data-Lake-Platform"},"nextItem":{"title":"Build Open Lakehouse using Apache Hudi & dbt","permalink":"/blog/2022/07/11/build-open-lakehouse-using-apache-hudi-and-dbt"}}')},32549:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/Hudi_design_diagram_-_Page_2_1-fee4ee8ef3b5ba97b06ffb9d47644a79.png"},32753:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/12/06/non-blocking-concurrency-control","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-12-06-non-blocking-concurrency-control.md","source":"@site/blog/2024-12-06-non-blocking-concurrency-control.md","title":"Introducing Hudi\'s Non-blocking Concurrency Control for streaming, high-frequency writes","description":"Introduction","date":"2024-12-06T00:00:00.000Z","tags":[{"inline":true,"label":"design","permalink":"/blog/tags/design"},{"inline":true,"label":"streaming ingestion","permalink":"/blog/tags/streaming-ingestion"},{"inline":true,"label":"multi-writer","permalink":"/blog/tags/multi-writer"},{"inline":true,"label":"concurrency-control","permalink":"/blog/tags/concurrency-control"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"}],"readingTime":6.89,"hasTruncateMarker":false,"authors":[{"name":"Danny Chan","key":null,"page":null}],"frontMatter":{"title":"Introducing Hudi\'s Non-blocking Concurrency Control for streaming, high-frequency writes","excerpt":"Announcing the Non-blocking Concurrency Control in Apache Hudi","author":"Danny Chan","category":"blog","image":"/assets/images/blog/non-blocking-concurrency-control/lsm_archive_timeline.png","tags":["design","streaming ingestion","multi-writer","concurrency-control","blog"]},"unlisted":false,"prevItem":{"title":"Announcing Apache Hudi 1.0 and the Next Generation of Data Lakehouses","permalink":"/blog/2024/12/16/announcing-hudi-1-0-0"},"nextItem":{"title":"Use open table format libraries on AWS Glue 5.0 for Apache Spark","permalink":"/blog/2024/12/04/use-open-table-format-libraries-on-aws-glue-5-0-for-apache-spark"}}')},32759:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(93902),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi vs Delta Lake - Transparent TPC-DS Lakehouse Performance Benchmarks",authors:[{name:"Alexey Kudinkin"}],category:"blog",image:"/assets/images/blog/2022-06-29-apache_hudi_vs_delta_lake_tpc_ds_benchmarks.png",tags:["performance","datalake","comparison","onehouse"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.onehouse.ai/blog/apache-hudi-vs-delta-lake-transparent-tpc-ds-lakehouse-performance-benchmarks",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},32817:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/03/10/navigating-the-future-the-evolutionary-journey-of-upstoxs-data-platform","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-03-10-navigating-the-future-the-evolutionary-journey-of-upstoxs-data-platform.mdx","source":"@site/blog/2024-03-10-navigating-the-future-the-evolutionary-journey-of-upstoxs-data-platform.mdx","title":"Navigating the Future: The Evolutionary Journey of Upstox\u2019s Data Platform","description":"Redirecting... please wait!!","date":"2024-03-10T00:00:00.000Z","tags":[{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"upstox-engineering","permalink":"/blog/tags/upstox-engineering"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Manish Gaurav","key":null,"page":null}],"frontMatter":{"title":"Navigating the Future: The Evolutionary Journey of Upstox\u2019s Data Platform","author":"Manish Gaurav","category":"blog","image":"/assets/images/blog/2024-03-10-navigating-the-future-the-evolutionary-journey-of-upstoxs-data-platform.png","tags":["use-case","apache hudi","upstox-engineering"]},"unlisted":false,"prevItem":{"title":"Modern Datalakes with Hudi, MinIO, and HMS","permalink":"/blog/2024/03/14/Modern-Datalakes-with-Hudi--MinIO--and-HMS"},"nextItem":{"title":"Apache Hudi: From Zero To One (9/10)","permalink":"/blog/2024/03/05/Apache-Hudi-From-Zero-To-One-blog-9"}}')},32877:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2020/01/15/delete-support-in-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-01-15-delete-support-in-hudi.md","source":"@site/blog/2020-01-15-delete-support-in-hudi.md","title":"Delete support in Hudi","description":"Deletes are supported at a record level in Hudi with 0.5.1 release. This blog is a \\"how to\\" blog on how to delete records in hudi. Deletes can be done with 3 flavors: Hudi RDD APIs, with Spark data source and with DeltaStreamer.","date":"2020-01-15T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"deletes","permalink":"/blog/tags/deletes"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":3.91,"hasTruncateMarker":true,"authors":[{"name":"shivnarayan","key":null,"page":null}],"frontMatter":{"title":"Delete support in Hudi","excerpt":"Deletes are supported at a record level in Hudi with 0.5.1 release. This blog is a \u201chow to\u201d blog on how to delete records in hudi.","author":"shivnarayan","category":"blog","tags":["how-to","deletes","apache hudi"]},"unlisted":false,"prevItem":{"title":"Change Capture Using AWS Database Migration Service and Hudi","permalink":"/blog/2020/01/20/change-capture-using-aws"},"nextItem":{"title":"New \u2013 Insert, Update, Delete Data on S3 with Amazon EMR and Apache Hudi","permalink":"/blog/2019/11/15/New-Insert-Update-Delete-Data-on-S3-with-Amazon-EMR-and-Apache-Hudi"}}')},32901:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2020/06/09/Building-a-Large-scale-Transactional-Data-Lake-at-Uber-Using-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-06-09-Building-a-Large-scale-Transactional-Data-Lake-at-Uber-Using-Apache-Hudi.mdx","source":"@site/blog/2020-06-09-Building-a-Large-scale-Transactional-Data-Lake-at-Uber-Using-Apache-Hudi.mdx","title":"Building a Large-scale Transactional Data Lake at Uber Using Apache Hudi","description":"Redirecting... please wait!!","date":"2020-06-09T00:00:00.000Z","tags":[{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"datalake","permalink":"/blog/tags/datalake"},{"inline":true,"label":"analytics at scale","permalink":"/blog/tags/analytics-at-scale"},{"inline":true,"label":"uber","permalink":"/blog/tags/uber"}],"readingTime":0.09,"hasTruncateMarker":false,"authors":[{"name":"Nishith Agarwal","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Building a Large-scale Transactional Data Lake at Uber Using Apache Hudi","authors":[{"name":"Nishith Agarwal"}],"category":"blog","image":"/assets/images/blog/2020-06-09-Building-a-Large-scale-Transactional-Data-Lake-at-Uber-Using-Apache-Hudi.png","tags":["use-case","datalake","analytics at scale","uber"]},"unlisted":false,"prevItem":{"title":"Apache Hudi grows cloud data lake maturity","permalink":"/blog/2020/06/16/Apache-Hudi-grows-cloud-data-lake-maturity"},"nextItem":{"title":"The Apache Software Foundation Announces Apache\xae Hudi\u2122 as a Top-Level Project","permalink":"/blog/2020/06/04/The-Apache-Software-Foundation-Announces-Apache-Hudi-as-a-Top-Level-Project"}}')},32929:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/03/30/record-level-indexing-apache-hudi-delivers-70-faster-point","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-03-30-record-level-indexing-apache-hudi-delivers-70-faster-point.mdx","source":"@site/blog/2024-03-30-record-level-indexing-apache-hudi-delivers-70-faster-point.mdx","title":"Record Level Indexing in Apache Hudi Delivers 70% Faster Point Lookups","description":"Redirecting... please wait!!","date":"2024-03-30T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"record level index","permalink":"/blog/tags/record-level-index"},{"inline":true,"label":"performance","permalink":"/blog/tags/performance"},{"inline":true,"label":"linkedin","permalink":"/blog/tags/linkedin"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Soumil Shah","key":null,"page":null}],"frontMatter":{"title":"Record Level Indexing in Apache Hudi Delivers 70% Faster Point Lookups","author":"Soumil Shah","category":"blog","image":"/assets/images/blog/2024-03-30-record-level-indexing-apache-hudi-delivers-70-faster-point.png","tags":["blog","apache hudi","record level index","performance","linkedin"]},"unlisted":false,"prevItem":{"title":"Hands-On Guide: Reading Data from Hudi Tables Incrementally, Joining with Delta Tables using HudiStreamer and SQL-Based Transformer","permalink":"/blog/2024/04/03/hands-on-guide-reading-data-from-hudi-tables-joining-delta"},"nextItem":{"title":"Options on Kafka sink to open table Formats: Apache Iceberg and Apache Hudi","permalink":"/blog/2024/03/23/options-on-kafka-sink-to-open-table-formats-apache-iceberg-and-apache-hudi"}}')},33221:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(89988),n=t(74848),s=t(28453),r=t(9230);const o={title:"Introduction to Apache Hudi",excerpt:"Introduction to Apache Hudi",author:"Andrew Savchyns",category:"blog",image:"/assets/images/blog/2024-01-09-introduction-to-apache-hudi.png",tags:["blog","apache hudi","medium","beginner","apache spark"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/blue-orange-digital/introduction-to-apache-hudi-209521970112",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},33383:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/01/25/Cost-Efficiency-Scale-in-Big-Data-File-Format","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-01-25-Cost-Efficiency-Scale-in-Big-Data-File-Format.mdx","source":"@site/blog/2022-01-25-Cost-Efficiency-Scale-in-Big-Data-File-Format.mdx","title":"Cost Efficiency @ Scale in Big Data File Format","description":"Redirecting... please wait!!","date":"2022-01-25T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"cost efficiency","permalink":"/blog/tags/cost-efficiency"},{"inline":true,"label":"compression","permalink":"/blog/tags/compression"},{"inline":true,"label":"analytics at scale","permalink":"/blog/tags/analytics-at-scale"},{"inline":true,"label":"uber","permalink":"/blog/tags/uber"}],"readingTime":0.1,"hasTruncateMarker":false,"authors":[{"name":"Xinli Shang","socials":{},"key":null,"page":null},{"name":"Kai Jiang","socials":{},"key":null,"page":null},{"name":"Zheng Shao","socials":{},"key":null,"page":null},{"name":"Mohammad Islam","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Cost Efficiency @ Scale in Big Data File Format","authors":[{"name":"Xinli Shang"},{"name":"Kai Jiang"},{"name":"Zheng Shao"},{"name":"Mohammad Islam"}],"category":"blog","image":"/assets/images/blog/2022-01-25-cost-efficiency-at-scale-in-big-data-file-format.png","tags":["blog","cost efficiency","compression","analytics at scale","uber"]},"unlisted":false,"prevItem":{"title":"Onehouse Commitment to Openness","permalink":"/blog/2022/02/02/Onehouse-Commitment-to-Openness"},"nextItem":{"title":"Hudi powering data lake efforts at Walmart and Disney+ Hotstar","permalink":"/blog/2022/01/20/Hudi-powering-data-lake-efforts-at-Walmart-and-Disney-Hotstar"}}')},33582:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(33383),n=t(74848),s=t(28453),r=t(9230);const o={title:"Cost Efficiency @ Scale in Big Data File Format",authors:[{name:"Xinli Shang"},{name:"Kai Jiang"},{name:"Zheng Shao"},{name:"Mohammad Islam"}],category:"blog",image:"/assets/images/blog/2022-01-25-cost-efficiency-at-scale-in-big-data-file-format.png",tags:["blog","cost efficiency","compression","analytics at scale","uber"]},l=void 0,d={authorsImageUrls:[void 0,void 0,void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://eng.uber.com/cost-efficiency-big-data/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},33750:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(63176),n=t(74848),s=t(28453),r=t(9230);const o={title:"Mastering Open Table Formats: A Guide to Apache Iceberg, Hudi, and Delta Lake",author:"Naresh Dulam",category:"blog",image:"/assets/images/blog/2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.png",tags:["blog","Apache Hudi","Apache Iceberg","Delta Lake","comparison","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/itversity/understanding-open-table-formats-a-comprehensive-guide-ba6f072167fb",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},34159:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(76679),n=t(74848),s=t(28453),r=t(9230);const o={title:"Key Learnings on Using Apache HUDI in building Lakehouse Architecture @ Halodoc",authors:[{name:"Jitendra Shah"}],category:"blog",image:"/assets/images/blog/2022-04-04-halodoc-lakehouse-architecture.png",tags:["use-case","lakehouse","incremental processing","halodoc"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blogs.halodoc.io/key-learnings-on-using-apache-hudi-in-building-lakehouse-architecture-halodoc/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},34508:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig1-103edc705ab1254fb8b23ba25db76fd6.jpg"},34628:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2020/11/11/hudi-indexing-mechanisms","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-11-11-hudi-indexing-mechanisms.md","source":"@site/blog/2020-11-11-hudi-indexing-mechanisms.md","title":"Employing the right indexes for fast updates, deletes in Apache Hudi","description":"Apache Hudi employs an index to locate the file group, that an update/delete belongs to. For Copy-On-Write tables, this enables","date":"2020-11-11T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"indexing","permalink":"/blog/tags/indexing"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":8.12,"hasTruncateMarker":true,"authors":[{"name":"vinoth","key":null,"page":null}],"frontMatter":{"title":"Employing the right indexes for fast updates, deletes in Apache Hudi","excerpt":"Detailing different indexing mechanisms in Hudi and when to use each of them","author":"vinoth","category":"blog","image":"/assets/images/blog/hudi-indexes/with-and-without-index.png","tags":["how-to","indexing","apache hudi"]},"unlisted":false,"prevItem":{"title":"Can Big Data Solutions Be Affordable?","permalink":"/blog/2020/11/29/Can-Big-Data-Solutions-Be-Affordable"},"nextItem":{"title":"Architecting Data Lakes for the Modern Enterprise at Data Summit Connect Fall 2020","permalink":"/blog/2020/10/21/Architecting-Data-Lakes-for-the-Modern-Enterprise-at-Data-Summit-Connect-Fall-2020"}}')},34883:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/11/12/record-level-indexing-in-apache-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-11-12-record-level-indexing-in-apache-hudi.mdx","source":"@site/blog/2024-11-12-record-level-indexing-in-apache-hudi.mdx","title":"Record Level Indexing in Apache Hudi","description":"Redirecting... please wait!!","date":"2024-11-12T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"record index","permalink":"/blog/tags/record-index"},{"inline":true,"label":"record level index","permalink":"/blog/tags/record-level-index"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Bibhu Pala","key":null,"page":null}],"frontMatter":{"title":"Record Level Indexing in Apache Hudi","author":"Bibhu Pala","category":"blog","image":"/assets/images/blog/2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.png","tags":["blog","apache hudi","record index","record level index","medium"]},"unlisted":false,"prevItem":{"title":"Hudi\u2019s Automatic File Sizing Delivers Unmatched Performance","permalink":"/blog/2024/11/19/automated-small-file-handling"},"nextItem":{"title":"Storing 200 Billion Entities: Notion\u2019s Data Lake Project","permalink":"/blog/2024/11/12/storing-200-billion-entities-notions"}}')},34939:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/03/11/New-features-from-Apache-hudi-in-Amazon-EMR","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-03-11-New-features-from-Apache-hudi-in-Amazon-EMR.mdx","source":"@site/blog/2021-03-11-New-features-from-Apache-hudi-in-Amazon-EMR.mdx","title":"New features from Apache hudi in Amazon EMR","description":"Redirecting... please wait!!","date":"2021-03-11T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Udit Mehrotra","socials":{},"key":null,"page":null}],"frontMatter":{"title":"New features from Apache hudi in Amazon EMR","authors":[{"name":"Udit Mehrotra"}],"category":"blog","image":"/assets/images/blog/aws.jpg","tags":["blog","amazon"]},"unlisted":false,"prevItem":{"title":"Build Slowly Changing Dimensions Type 2 (SCD2) with Apache Spark and Apache Hudi on Amazon EMR","permalink":"/blog/2021/04/12/Build-Slowly-Changing-Dimensions-Type-2-SCD2-with-Apache-Spark-and-Apache-Hudi-on-Amazon-EMR"},"nextItem":{"title":"Build a data lake using amazon kinesis data stream for amazon dynamodb and apache hudi","permalink":"/blog/2021/03/04/Build-a-data-lake-using-amazon-kinesis-data-stream-for-amazon-dynamodb-and-apache-hudi"}}')},35388:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(66524),n=t(74848),s=t(28453),r=t(9230);const o={title:"Using Apache Hudi with Apache Flink",author:"amazon",category:"blog",image:"/assets/images/blog/2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.png",tags:["blog","apache hudi","apache flink","beginner","aws","amazon"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/tutorial-hudi-for-flink.html",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},35681:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(16562),n=t(74848),s=t(28453),r=t(9230);const o={title:"Open Table Formats (part-1): Apache Hudi (Hadoop Upserts Deletes and Incrementals)",author:"Vivek L Alex",category:"blog",image:"/assets/images/blog/2024-03-16-Open-Table-Formats-part-1-Apache-Hudi-Hadoop-Upserts-Deletes-and-Incrementals.jpg",tags:["blog","apache hudi","beginner","defogdata"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://defogdata.substack.com/p/table-format-series-apache-hudi-hadoop?r=37wv2a&utm_campaign=post&utm_medium=web&triedRedirect=true",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},35752:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/10/14/How-Amazon-Transportation-Service-enabled-near-real-time-event-analytics-at-petabyte-scale-using-AWS-Glue-with-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-10-14-How-Amazon-Transportation-Service-enabled-near-real-time-event-analytics-at-petabyte-scale-using-AWS-Glue-with-Apache-Hudi.mdx","source":"@site/blog/2021-10-14-How-Amazon-Transportation-Service-enabled-near-real-time-event-analytics-at-petabyte-scale-using-AWS-Glue-with-Apache-Hudi.mdx","title":"How Amazon Transportation Service enabled near-real-time event analytics at petabyte scale using AWS Glue with Apache Hudi","description":"Redirecting... please wait!!","date":"2021-10-14T00:00:00.000Z","tags":[{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"near real-time analytics","permalink":"/blog/tags/near-real-time-analytics"},{"inline":true,"label":"analytics at scale","permalink":"/blog/tags/analytics-at-scale"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.19,"hasTruncateMarker":false,"authors":[{"name":"Madhavan Sriram","socials":{},"key":null,"page":null},{"name":"Diego Menin","socials":{},"key":null,"page":null},{"name":"Gabriele Cacciola","socials":{},"key":null,"page":null},{"name":"Kunal Gautam","socials":{},"key":null,"page":null}],"frontMatter":{"title":"How Amazon Transportation Service enabled near-real-time event analytics at petabyte scale using AWS Glue with Apache Hudi","authors":[{"name":"Madhavan Sriram"},{"name":"Diego Menin"},{"name":"Gabriele Cacciola"},{"name":"Kunal Gautam"}],"category":"blog","image":"/assets/images/blog/2021-10-14-near-real-time-analytics-at-amazon-transportation-service.png","tags":["use-case","near real-time analytics","analytics at scale","amazon"]},"unlisted":false,"prevItem":{"title":"Practice of Apache Hudi in building real-time data lake at station B","permalink":"/blog/2021/10/21/Practice-of-Apache-Hudi-in-building-real-time-data-lake-at-station-B"},"nextItem":{"title":"Data Platform 2.0 - Part I","permalink":"/blog/2021/10/05/Data-Platform-2.0-Part-I"}}')},35845:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/04/02/secondary-index","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-04-02-secondary-index.md","source":"@site/blog/2025-04-02-secondary-index.md","title":"Introducing Secondary Index in Apache Hudi Lakehouse Platform","description":"Apache Hudi 1.0 introduces Secondary Indexes, enabling faster queries on non-primary key fields. This improves data retrieval in Lakehouse architectures by reducing data scans. Hudi also offers asynchronous indexing for scalability and efficient index maintenance without disrupting data ingestion. By the end of this blog, you\'ll understand how these features enhance Hudi\'s capabilities as a high-performance lakehouse platform.","date":"2025-04-02T00:00:00.000Z","tags":[{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"Indexing","permalink":"/blog/tags/indexing"},{"inline":true,"label":"Performance","permalink":"/blog/tags/performance"}],"readingTime":9.87,"hasTruncateMarker":false,"authors":[{"name":"Dipankar Mazumdar, Aditya Goenka","key":null,"page":null}],"frontMatter":{"title":"Introducing Secondary Index in Apache Hudi Lakehouse Platform","excerpt":"What\'s & How\'s of Secondary indexes in Hudi 1.0","author":"Dipankar Mazumdar, Aditya Goenka","category":"blog","image":"/assets/images/blog/sec-thumb.jpg","tags":["Apache Hudi","Indexing","Performance"]},"unlisted":false,"prevItem":{"title":"Integrating Apache Doris and Hudi for Data Querying and Migration","permalink":"/blog/2025/04/03/integrate-apache-doris-hudi-data-querying-migration"},"nextItem":{"title":"Powering Amazon Unit Economics at Scale Using Apache Hudi","permalink":"/blog/2025/03/31/amazon-hudi"}}')},36093:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(1050),n=t(74848),s=t(28453),r=t(9230);const o={title:"Cleaner and Archival in Apache Hudi",authors:[{name:"Sivabalan Narayanan"}],category:"blog",image:"/assets/images/blog/2023-06-11-cleaner-and-archival-in-apache-hudi.jpg",tags:["blog","cleaner","timeline","active timeline","archival timeline","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@simpsons/cleaner-and-archival-in-apache-hudi-9e15b08b2933",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},36095:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(23167),n=t(74848),s=t(28453),r=t(9230);const o={title:"Integrating Apache Doris and Hudi for Data Querying and Migration",author:"li yy",category:"blog",image:"/assets/images/blog/2025-04-03-integrate-apache-doris-hudi-data-querying-migration.png",tags:["blog","Apache Hudi","Apache Doris","real-time query","how-to","dzone"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://dzone.com/articles/integrate-apache-doris-hudi-data-querying-migration",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},36179:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(53186),n=t(74848),s=t(28453),r=t(9230);const o={title:"Exploring the Architecture of Apache Iceberg, Delta Lake, and Apache Hudi",excerpt:"Exploring the Architecture of Apache Iceberg, Delta Lake, and Apache Hudi",author:"Alex Merced",category:"blog",image:"/assets/images/blog/2023-09-22-Exploring-the-Architecture-of-Apache-Iceberg-Delta-Lake-and-Apache-Hudi.png",tags:["apache hudi","apache iceberg","blog","delta lake","dremio","architecture"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.dremio.com/blog/exploring-the-architecture-of-apache-iceberg-delta-lake-and-apache-hudi/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},36411:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(40012),n=t(74848),s=t(28453),r=t(9230);const o={title:"Getting started with Apache Hudi",excerpt:"Getting started with Apache Hudi",author:"DataCouch",category:"blog",image:"/assets/images/blog/2023-12-01-Getting-started-with-Apache-Hudi.png",tags:["apache hudi","apache spark","how-to","getting started","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://datacouch.medium.com/getting-started-with-apache-hudi-711b89c107aa",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},36425:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(30833),n=t(74848),s=t(28453),r=t(9230);const o={title:"Build Slowly Changing Dimensions Type 2 (SCD2) with Apache Spark and Apache Hudi on Amazon EMR",authors:[{name:"David Greenshtein"}],category:"blog",image:"/assets/images/blog/aws.jpg",tags:["how-to","scd2","amazon"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/build-slowly-changing-dimensions-type-2-scd2-with-apache-spark-and-apache-hudi-on-amazon-emr/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},36632:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/04/09/why-walmart-chose-apache-hudi-for-their-lakehouse","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-04-09-why-walmart-chose-apache-hudi-for-their-lakehouse.mdx","source":"@site/blog/2025-04-09-why-walmart-chose-apache-hudi-for-their-lakehouse.mdx","title":"Why Walmart Chose Apache Hudi for Their Lakehouse","description":"Redirecting... please wait!!","date":"2025-04-09T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"det","permalink":"/blog/tags/det"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Vu Trinh","key":null,"page":null}],"frontMatter":{"title":"Why Walmart Chose Apache Hudi for Their Lakehouse","author":"Vu Trinh","category":"blog","image":"/assets/images/blog/2025-04-09-why-walmart-chose-apache-hudi-for-their-lakehouse.jpg","tags":["blog","Apache Hudi","use-case","det"]},"unlisted":false,"prevItem":{"title":"How Doris + Hudi Turned the Impossible Into the Everyday","permalink":"/blog/2025/04/14/doris-hudi-making-impossible-possible"},"nextItem":{"title":" From Swamp to Stream: How Apache Hudi Transforms the Modern Data Lake","permalink":"/blog/2025/04/06/from-swamp-to-stream-how-apache-hudi-transforms-the-modern-data-lake"}}')},36810:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(34939),n=t(74848),s=t(28453),r=t(9230);const o={title:"New features from Apache hudi in Amazon EMR",authors:[{name:"Udit Mehrotra"}],category:"blog",image:"/assets/images/blog/aws.jpg",tags:["blog","amazon"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/new-features-from-apache-hudi-available-in-amazon-emr/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},36927:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(13662),n=t(74848),s=t(28453),r=t(9230);const o={title:"Tipico Facilitates Faster Data Access with a Modern Data Strategy on AWS",category:"blog",image:"/assets/images/blog/2023-10-22-Tipico-Facilitates-Faster-Data-Access-with-a-Modern-Data-Strategy-on-AWS.png",tags:["case study","amazon","apache hudi"]},l=void 0,d={authorsImageUrls:[]},c=[];function h(e){const a={p:"p",...(0,s.R)(),...e.components};return(0,n.jsxs)(a.p,{children:[(0,n.jsx)(r.A,{url:"https://aws.amazon.com/solutions/case-studies/tipico-case-study/",children:"Redirecting... please wait!! "}),"s"]})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h(e)}},36995:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/direct-marker-file-mechanism-b97b82f80430598f1d6a9b96521bb1a0.png"},37157:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/01/27/hudi-clustering-intro","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-01-27-hudi-clustering-intro.md","source":"@site/blog/2021-01-27-hudi-clustering-intro.md","title":"Optimize Data lake layout using Clustering in Apache Hudi","description":"Background","date":"2021-01-27T00:00:00.000Z","tags":[{"inline":true,"label":"design","permalink":"/blog/tags/design"},{"inline":true,"label":"clustering","permalink":"/blog/tags/clustering"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":6.23,"hasTruncateMarker":true,"authors":[{"name":"satish.kotha","key":null,"page":null}],"frontMatter":{"title":"Optimize Data lake layout using Clustering in Apache Hudi","excerpt":"Introduce clustering feature to change data layout","author":"satish.kotha","category":"blog","image":"/assets/images/blog/2021-01-27-hudi-clustering-intro.png","tags":["design","clustering","apache hudi"]},"unlisted":false,"prevItem":{"title":"Apache Hudi Key Generators","permalink":"/blog/2021/02/13/hudi-key-generators"},"nextItem":{"title":"Building High-Performance Data Lake Using Apache Hudi and Alluxio at T3Go","permalink":"/blog/2020/12/01/high-perf-data-lake-with-hudi-and-alluxio-t3go"}}')},37183:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/07/02/Lakehouse-Architecture-apache-hudi-and-apache-iceberg","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-07-02-Lakehouse-Architecture-apache-hudi-and-apache-iceberg.mdx","source":"@site/blog/2025-07-02-Lakehouse-Architecture-apache-hudi-and-apache-iceberg.mdx","title":"Lakehouse Architecture - Apache Hudi and Apache Iceberg","description":"Redirecting... please wait!!","date":"2025-07-02T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"Apache Iceberg","permalink":"/blog/tags/apache-iceberg"},{"inline":true,"label":"Lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"det","permalink":"/blog/tags/det"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"beCloudReady","key":null,"page":null}],"frontMatter":{"title":"Lakehouse Architecture - Apache Hudi and Apache Iceberg","author":"beCloudReady","category":"blog","image":"/assets/images/blog/2025-07-02-Lakehouse-Architecture-apache-hudi-and-apache-iceberg.png","tags":["blog","Apache Hudi","Apache Iceberg","Lakehouse","use-case","det"]},"unlisted":false,"prevItem":{"title":"Why Uber Built Hudi: The Strategic Decision Behind a Custom Table Format","permalink":"/blog/2025/07/03/why-uber-built-hudi-the-strategic-decision-behind-a-custom-table-format"},"nextItem":{"title":"Scaling Complex Data Workflows at Uber Using Apache Hudi","permalink":"/blog/2025/06/30/uber-hudi"}}')},37343:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2020/08/21/async-compaction-deployment-model","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-08-21-async-compaction-deployment-model.md","source":"@site/blog/2020-08-21-async-compaction-deployment-model.md","title":"Async Compaction Deployment Models","description":"We will look at different deployment models for executing compactions asynchronously.","date":"2020-08-21T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"compaction","permalink":"/blog/tags/compaction"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":2.02,"hasTruncateMarker":true,"authors":[{"name":"vbalaji","key":null,"page":null}],"frontMatter":{"title":"Async Compaction Deployment Models","excerpt":"Mechanisms for executing compaction jobs in Hudi asynchronously","author":"vbalaji","category":"blog","tags":["how-to","compaction","apache hudi"]},"unlisted":false,"prevItem":{"title":"Ingest multiple tables using Hudi","permalink":"/blog/2020/08/22/ingest-multiple-tables-using-hudi"},"nextItem":{"title":"Efficient Migration of Large Parquet Tables to Apache Hudi","permalink":"/blog/2020/08/20/efficient-migration-of-large-parquet-tables"}}')},37629:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/Fact20tables-0255e82a96683124f7116060e9f76cbe.gif"},37647:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/09/24/hudi-iceberg-and-delta-lake-data-lake-table-formats-compared","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-09-24-hudi-iceberg-and-delta-lake-data-lake-table-formats-compared.mdx","source":"@site/blog/2024-09-24-hudi-iceberg-and-delta-lake-data-lake-table-formats-compared.mdx","title":"Hudi, Iceberg and Delta Lake: Data Lake Table Formats Compared","description":"Redirecting... please wait!!","date":"2024-09-24T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"apache iceberg","permalink":"/blog/tags/apache-iceberg"},{"inline":true,"label":"delta lake","permalink":"/blog/tags/delta-lake"},{"inline":true,"label":"comparison","permalink":"/blog/tags/comparison"},{"inline":true,"label":"lakefs","permalink":"/blog/tags/lakefs"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Oz Katz","key":null,"page":null}],"frontMatter":{"title":"Hudi, Iceberg and Delta Lake: Data Lake Table Formats Compared","author":"Oz Katz","category":"blog","image":"/assets/images/blog/2024-09-24-hudi-iceberg-and-delta-lake-data-lake-table-formats-compared.png","tags":["blog","apache hudi","apache iceberg","delta lake","comparison","lakefs"]},"unlisted":false,"prevItem":{"title":"Change query support in Apache Hudi (0.15)","permalink":"/blog/2024/09/30/change-query-support-in-apache-hudi-0-15"},"nextItem":{"title":"Hands-on with Apache Hudi and Spark","permalink":"/blog/2024/09/22/hands-on-with-apache-hudi-and-spark"}}')},37728:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(37343),n=t(74848),s=t(28453);const r={title:"Async Compaction Deployment Models",excerpt:"Mechanisms for executing compaction jobs in Hudi asynchronously",author:"vbalaji",category:"blog",tags:["how-to","compaction","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Compaction",id:"compaction",level:2},{value:"Async Compaction",id:"async-compaction",level:2},{value:"Deployment Models",id:"deployment-models",level:2},{value:"Spark Structured Streaming",id:"spark-structured-streaming",level:3},{value:"DeltaStreamer Continuous Mode",id:"deltastreamer-continuous-mode",level:3},{value:"Hudi CLI",id:"hudi-cli",level:3},{value:"Hudi Compactor Script",id:"hudi-compactor-script",level:3}];function c(e){const a={br:"br",code:"code",em:"em",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.p,{children:"We will look at different deployment models for executing compactions asynchronously."}),"\n",(0,n.jsx)(a.h2,{id:"compaction",children:"Compaction"}),"\n",(0,n.jsx)(a.p,{children:"For Merge-On-Read table, data is stored using a combination of columnar (e.g parquet) + row based (e.g avro) file formats.\nUpdates are logged to delta files & later compacted to produce new versions of columnar files synchronously or\nasynchronously. One of th main motivations behind Merge-On-Read is to reduce data latency when ingesting records.\nHence, it makes sense to run compaction asynchronously without blocking ingestion."}),"\n",(0,n.jsx)(a.h2,{id:"async-compaction",children:"Async Compaction"}),"\n",(0,n.jsx)(a.p,{children:"Async Compaction is performed in 2 steps:"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.em,{children:(0,n.jsx)(a.strong,{children:"Compaction Scheduling"})}),": This is done by the ingestion job. In this step, Hudi scans the partitions and selects ",(0,n.jsx)(a.strong,{children:"file\nslices"})," to be compacted. A compaction plan is finally written to Hudi timeline."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.em,{children:(0,n.jsx)(a.strong,{children:"Compaction Execution"})}),": A separate process reads the compaction plan and performs compaction of file slices."]}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"deployment-models",children:"Deployment Models"}),"\n",(0,n.jsx)(a.p,{children:"There are few ways by which we can execute compactions asynchronously."}),"\n",(0,n.jsx)(a.h3,{id:"spark-structured-streaming",children:"Spark Structured Streaming"}),"\n",(0,n.jsx)(a.p,{children:"With 0.6.0, we now have support for running async compactions in Spark\nStructured Streaming jobs. Compactions are scheduled and executed asynchronously inside the\nstreaming job.  Async Compactions are enabled by default for structured streaming jobs\non Merge-On-Read table."}),"\n",(0,n.jsx)(a.p,{children:"Here is an example snippet in java"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-properties",children:'import org.apache.hudi.DataSourceWriteOptions;\nimport org.apache.hudi.HoodieDataSourceHelpers;\nimport org.apache.hudi.config.HoodieCompactionConfig;\nimport org.apache.hudi.config.HoodieWriteConfig;\n\nimport org.apache.spark.sql.streaming.OutputMode;\nimport org.apache.spark.sql.streaming.ProcessingTime;\n\n\n DataStreamWriter<Row> writer = streamingInput.writeStream().format("org.apache.hudi")\n        .option(DataSourceWriteOptions.OPERATION_OPT_KEY(), operationType)\n        .option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY(), tableType)\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), "_row_key")\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), "partition")\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), "timestamp")\n        .option(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP, "10")\n        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE_OPT_KEY(), "true")\n        .option(HoodieWriteConfig.TABLE_NAME, tableName).option("checkpointLocation", checkpointLocation)\n        .outputMode(OutputMode.Append());\n writer.trigger(new ProcessingTime(30000)).start(tablePath);\n'})}),"\n",(0,n.jsx)(a.h3,{id:"deltastreamer-continuous-mode",children:"DeltaStreamer Continuous Mode"}),"\n",(0,n.jsxs)(a.p,{children:["Hudi DeltaStreamer provides continuous ingestion mode where a single long running spark application",(0,n.jsx)(a.br,{}),"\n","ingests data to Hudi table continuously from upstream sources. In this mode, Hudi supports managing asynchronous\ncompactions. Here is an example snippet for running in continuous mode with async compactions"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-properties",children:"spark-submit --packages org.apache.hudi:hudi-utilities-bundle_2.11:0.6.0 \\\n--class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer \\\n--table-type MERGE_ON_READ \\\n--target-base-path <hudi_base_path> \\\n--target-table <hudi_table> \\\n--source-class org.apache.hudi.utilities.sources.JsonDFSSource \\\n--source-ordering-field ts \\\n--schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider \\\n--props /path/to/source.properties \\\n--continous\n"})}),"\n",(0,n.jsx)(a.h3,{id:"hudi-cli",children:"Hudi CLI"}),"\n",(0,n.jsx)(a.p,{children:"Hudi CLI is yet another way to execute specific compactions asynchronously. Here is an example"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-properties",children:"hudi:trips->compaction run --tableName <table_name> --parallelism <parallelism> --compactionInstant <InstantTime>\n...\n"})}),"\n",(0,n.jsx)(a.h3,{id:"hudi-compactor-script",children:"Hudi Compactor Script"}),"\n",(0,n.jsx)(a.p,{children:"Hudi provides a standalone tool to also execute specific compactions asynchronously. Here is an example"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-properties",children:"spark-submit --packages org.apache.hudi:hudi-utilities-bundle_2.11:0.6.0 \\\n--class org.apache.hudi.utilities.HoodieCompactor \\\n--base-path <base_path> \\\n--table-name <table_name> \\\n--instant-time <compaction_instant> \\\n--schema-file <schema_file>\n"})})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},37815:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig-5-PuppyGraph-Login-Page-c4b9d16af5789a65c1f796e0411dfd40.png"},37950:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(25642),n=t(74848),s=t(28453),r=t(9230);const o={title:"Enforce fine-grained access control on Open Table Formats via Amazon EMR integrated with AWS Lake Formation",excerpt:"Enforce fine-grained access control on Open Table Formats via Amazon EMR integrated with AWS Lake Formation",author:"Raymond Lai, Aditya Shah, Bin Wang, and Melody Yang",category:"blog",image:"/assets/images/blog/2024-01-17-Enforce-fine-grained-access-control-on-Open-Table-Formats-via-Amazon-EMR-integrated-with-AWS-Lake-Formation.png",tags:["blog","apache hudi","aws","intermediate","amazon emr","aws lake formation","aws glue","aws s3","amazon sagemaker","aws cloud9","amazon athena","access control"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/enforce-fine-grained-access-control-on-open-table-formats-via-amazon-emr-integrated-with-aws-lake-formation/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},38156:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(22667),n=t(74848),s=t(28453),r=t(9230);const o={title:"It's Time for the Universal Data Lakehouse",excerpt:"Universal Lakehouse",author:"Vinoth Chandar",category:"blog",image:"/assets/images/blog/2023-10-20-Its-Time-for-the-Universal-Data-Lakehouse.png",tags:["data lakehouse","onehouse","blog","apache hudi","interoperability"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){const a={p:"p",...(0,s.R)(),...e.components};return(0,n.jsxs)(a.p,{children:[(0,n.jsx)(r.A,{url:"https://www.onehouse.ai/blog/its-time-for-the-universal-data-lakehouse",children:"Redirecting... please wait!! "}),"s"]})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h(e)}},38175:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(66871),n=t(74848),s=t(28453),r=t(9230);const o={title:"How Doris + Hudi Turned the Impossible Into the Everyday",author:"Zen Hua",category:"blog",image:"/assets/images/blog/2025-04-03-integrate-apache-doris-hudi-data-querying-migration.png",tags:["blog","Apache Hudi","Apache Doris","use-case","federated querying","dzone"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://dzone.com/articles/doris-hudi-making-impossible-possible",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},38229:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/02/07/automate-schema-evolution-at-scale-with-apache-hudi-in-aws-glue","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-02-07-automate-schema-evolution-at-scale-with-apache-hudi-in-aws-glue.mdx","source":"@site/blog/2023-02-07-automate-schema-evolution-at-scale-with-apache-hudi-in-aws-glue.mdx","title":"Automate schema evolution at scale with Apache Hudi in AWS Glue | Amazon Web Services","description":"Redirecting... please wait!!","date":"2023-02-07T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"schema evolution","permalink":"/blog/tags/schema-evolution"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Subhro Bose","socials":{},"key":null,"page":null},{"name":"Eva Fang","socials":{},"key":null,"page":null},{"name":"Ketan Karalkar","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Automate schema evolution at scale with Apache Hudi in AWS Glue | Amazon Web Services","authors":[{"name":"Subhro Bose"},{"name":"Eva Fang"},{"name":"Ketan Karalkar"}],"category":"blog","image":"/assets/images/blog/automate-schema-evolution-at-scale-with-apache-hudi-in-aws-glue.png","tags":["how-to","schema evolution","amazon"]},"unlisted":false,"prevItem":{"title":"Table service deployment models in Apache Hudi","permalink":"/blog/2023/02/12/table-service-deployment-models-in-apache-hudi"},"nextItem":{"title":"Introducing native support for Apache Hudi, Delta Lake, and Apache Iceberg on AWS Glue for Apache Spark, Part 1: Getting Started","permalink":"/blog/2023/01/27/Introducing-native-support-for-Apache-Hudi-Delta-Lake-Apache-Iceberg-on-AWS-Glue-for-Apache-Spark"}}')},38326:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/7-flink-write-throughput-chart-040f5478de7a2bf3fa8ddb03ffa499c0.png"},38501:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(48059),n=t(74848),s=t(28453);const r={title:"Connect with us at Strata San Jose March 2017",author:"admin",date:new Date("2016-12-30T00:00:00.000Z"),category:"blog"},o=void 0,l={authorsImageUrls:[void 0]},d=[];function c(e){const a={p:"p",strong:"strong",...(0,s.R)(),...e.components};return(0,n.jsxs)(a.p,{children:["We will be presenting Hudi & general concepts around how incremental processing works at Uber.\nCatch our talk ",(0,n.jsx)(a.strong,{children:'"Incremental Processing on Hadoop At Uber"'})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},38637:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide2-33dbf59c0d2ac48489e6e8e0c1918b44.png"},38655:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/2020-12-01-t3go-microbenchmark-f8bb8b80b32eaedbcc990260459b319b.png"},38681:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig-8-PuppyGraph-Query-1-9d56715c417b1f35667d91d153df9547.png"},38746:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/1-pluggable-TF-36f7e26bf8dc4a479bcaff713a24debd.png"},38846:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/2-metadata-table-lookup-451d218e2a4fc0aac40b6d3c37522572.png"},38865:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(60725),n=t(74848),s=t(28453);const r={title:"Apache Hudi Key Generators",excerpt:"Different key generators available with Apache Hudi",author:"shivnarayan",category:"blog",tags:["blog","key generators","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Key Generators",id:"key-generators",level:2},{value:"SimpleKeyGenerator",id:"simplekeygenerator",level:3},{value:"ComplexKeyGenerator",id:"complexkeygenerator",level:3},{value:"GlobalDeleteKeyGenerator",id:"globaldeletekeygenerator",level:3},{value:"TimestampBasedKeyGenerator",id:"timestampbasedkeygenerator",level:3},{value:"Timestamp is GMT",id:"timestamp-is-gmt",level:4},{value:"Timestamp is DATE_STRING",id:"timestamp-is-date_string",level:4},{value:"Scalar examples",id:"scalar-examples",level:4},{value:"ISO8601WithMsZ with Single Input format",id:"iso8601withmsz-with-single-input-format",level:4},{value:"ISO8601WithMsZ with Multiple Input formats",id:"iso8601withmsz-with-multiple-input-formats",level:4},{value:"ISO8601NoMs with offset using multiple input formats",id:"iso8601noms-with-offset-using-multiple-input-formats",level:4},{value:"Input as short date string and expect date in date format",id:"input-as-short-date-string-and-expect-date-in-date-format",level:4},{value:"CustomKeyGenerator",id:"customkeygenerator",level:3},{value:"NonpartitionedKeyGenerator",id:"nonpartitionedkeygenerator",level:3}];function c(e){const a={a:"a",code:"code",h2:"h2",h3:"h3",h4:"h4",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.p,{children:"Every record in Hudi is uniquely identified by a primary key, which is a pair of record key and partition path where\nthe record belongs to. Using primary keys, Hudi can impose a) partition level uniqueness integrity constraint\nb) enable fast updates and deletes on records. One should choose the partitioning scheme wisely as it could be a\ndetermining factor for your ingestion and query latency."}),"\n",(0,n.jsx)(a.p,{children:"In general, Hudi supports both partitioned and global indexes. For a dataset with partitioned index(which is most\ncommonly used), each record is uniquely identified by a pair of record key and partition path. But for a dataset with\nglobal index, each record is uniquely identified by just the record key. There won't be any duplicate record keys across\npartitions."}),"\n",(0,n.jsx)(a.h2,{id:"key-generators",children:"Key Generators"}),"\n",(0,n.jsx)(a.p,{children:"Hudi provides several key generators out of the box that users can use based on their need, while having a pluggable\nimplementation for users to implement and use their own KeyGenerator. This blog goes over all different types of key\ngenerators that are readily available to use."}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/KeyGenerator.java",children:"Here"}),"\nis the interface for KeyGenerator in Hudi for your reference."]}),"\n",(0,n.jsx)(a.p,{children:"Before diving into different types of key generators, let\u2019s go over some of the common configs required to be set for\nkey generators."}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{children:"Config"}),(0,n.jsx)(a.th,{style:{textAlign:"center"},children:"Meaning/purpose"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.datasource.write.recordkey.field"})}),(0,n.jsx)(a.td,{style:{textAlign:"center"},children:"Refers to record key field. This is a mandatory field."})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.datasource.write.partitionpath.field"})}),(0,n.jsx)(a.td,{style:{textAlign:"center"},children:"Refers to partition path field. This is a mandatory field."})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.datasource.write.keygenerator.class"})}),(0,n.jsx)(a.td,{style:{textAlign:"center"},children:"Refers to Key generator class(including full path). Could refer to any of the available ones or user defined one. This is a mandatory field."})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.datasource.write.partitionpath.urlencode"})}),(0,n.jsx)(a.td,{style:{textAlign:"center"},children:"When set to true, partition path will be url encoded. Default value is false."})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.datasource.write.hive_style_partitioning"})}),(0,n.jsx)(a.td,{style:{textAlign:"center"},children:"When set to true, uses hive style partitioning. Partition field name will be prefixed to the value. Format: \u201c<partition_path_field_name>=<partition_path_value>\u201d. Default value is false."})]})]})]}),"\n",(0,n.jsxs)(a.p,{children:["NOTE:\nPlease use ",(0,n.jsx)(a.code,{children:"hoodie.datasource.write.keygenerator.class"})," instead of ",(0,n.jsx)(a.code,{children:"hoodie.datasource.write.keygenerator.type"}),". The second config was introduced more recently.\nand will internally instantiate the correct KeyGenerator class based on the type name. The second one is intended for ease of use and is being actively worked on.\nWe still recommend using the first config until it is marked as deprecated."]}),"\n",(0,n.jsx)(a.p,{children:"There are few more configs involved if you are looking for TimestampBasedKeyGenerator. Will cover those in the respective section."}),"\n",(0,n.jsx)(a.p,{children:"Lets go over different key generators available to be used with Hudi."}),"\n",(0,n.jsx)(a.h3,{id:"simplekeygenerator",children:(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/SimpleKeyGenerator.java",children:"SimpleKeyGenerator"})}),"\n",(0,n.jsx)(a.p,{children:"Record key refers to one field(column in dataframe) by name and partition path refers to one field (single column in dataframe)\nby name. This is one of the most commonly used one. Values are interpreted as is from dataframe and converted to string."}),"\n",(0,n.jsx)(a.h3,{id:"complexkeygenerator",children:(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/ComplexKeyGenerator.java",children:"ComplexKeyGenerator"})}),"\n",(0,n.jsxs)(a.p,{children:["Both record key and partition paths comprise one or more than one field by name(combination of multiple fields). Fields\nare expected to be comma separated in the config value. For example ",(0,n.jsx)(a.code,{children:'"Hoodie.datasource.write.recordkey.field" : \u201ccol1,col4\u201d'})]}),"\n",(0,n.jsx)(a.h3,{id:"globaldeletekeygenerator",children:(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/GlobalDeleteKeyGenerator.java",children:"GlobalDeleteKeyGenerator"})}),"\n",(0,n.jsx)(a.p,{children:"Global index deletes do not require partition value. So this key generator avoids using partition value for generating HoodieKey."}),"\n",(0,n.jsx)(a.h3,{id:"timestampbasedkeygenerator",children:(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/TimestampBasedKeyGenerator.java",children:"TimestampBasedKeyGenerator"})}),"\n",(0,n.jsx)(a.p,{children:"This key generator relies on timestamps for the partition field. The field values are interpreted as timestamps\nand not just converted to string while generating partition path value for records.  Record key is same as before where it is chosen by\nfield name.  Users are expected to set few more configs to use this KeyGenerator."}),"\n",(0,n.jsx)(a.p,{children:"Configs to be set:"}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{children:"Config"}),(0,n.jsx)(a.th,{children:"Meaning/purpose"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.timestamp.type"})}),(0,n.jsx)(a.td,{children:"One of the timestamp types supported(UNIX_TIMESTAMP, DATE_STRING, MIXED, EPOCHMILLISECONDS, SCALAR)"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.output.dateformat"})}),(0,n.jsx)(a.td,{children:"Output date format"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.timezone"})}),(0,n.jsx)(a.td,{children:"Timezone of the data format"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"oodie.deltastreamer.keygen.timebased.input.dateformat"})}),(0,n.jsx)(a.td,{children:"Input date format"})]})]})]}),"\n",(0,n.jsx)(a.p,{children:"Let's go over some example values for TimestampBasedKeyGenerator."}),"\n",(0,n.jsx)(a.h4,{id:"timestamp-is-gmt",children:"Timestamp is GMT"}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{children:"Config field"}),(0,n.jsx)(a.th,{children:"Value"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.timestamp.type"})}),(0,n.jsx)(a.td,{children:'"EPOCHMILLISECONDS"'})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.output.dateformat"})}),(0,n.jsx)(a.td,{children:'"yyyy-MM-dd hh"'})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.timezone"})}),(0,n.jsx)(a.td,{children:'"GMT+8:00"'})]})]})]}),"\n",(0,n.jsxs)(a.p,{children:["Input Field value: \u201c1578283932000L\u201d ",(0,n.jsx)("br",{}),"\nPartition path generated from key generator: \u201c2020-01-06 12\u201d"]}),"\n",(0,n.jsxs)(a.p,{children:["If input field value is null for some rows. ",(0,n.jsx)("br",{}),"\nPartition path generated from key generator: \u201c1970-01-01 08\u201d"]}),"\n",(0,n.jsx)(a.h4,{id:"timestamp-is-date_string",children:"Timestamp is DATE_STRING"}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{children:"Config field"}),(0,n.jsx)(a.th,{children:"Value"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.timestamp.type"})}),(0,n.jsx)(a.td,{children:'"DATE_STRING"'})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.output.dateformat"})}),(0,n.jsx)(a.td,{children:'"yyyy-MM-dd hh"'})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.timezone"})}),(0,n.jsx)(a.td,{children:'"GMT+8:00"'})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.input.dateformat"})}),(0,n.jsxs)(a.td,{children:['"yyyy-MM-dd hh:mm',":ss",'"']})]})]})]}),"\n",(0,n.jsxs)(a.p,{children:["Input field value: \u201c2020-01-06 12:12:12\u201d ",(0,n.jsx)("br",{}),"\nPartition path generated from key generator: \u201c2020-01-06 12\u201d"]}),"\n",(0,n.jsxs)(a.p,{children:["If input field value is null for some rows. ",(0,n.jsx)("br",{}),"\nPartition path generated from key generator: \u201c1970-01-01 12:00:00\u201d"]}),"\n",(0,n.jsx)("br",{}),"\n",(0,n.jsx)(a.h4,{id:"scalar-examples",children:"Scalar examples"}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{children:"Config field"}),(0,n.jsx)(a.th,{children:"Value"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.timestamp.type"})}),(0,n.jsx)(a.td,{children:'"SCALAR"'})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.output.dateformat"})}),(0,n.jsx)(a.td,{children:'"yyyy-MM-dd hh"'})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.timezone"})}),(0,n.jsx)(a.td,{children:'"GMT"'})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.timestamp.scalar.time.unit"})}),(0,n.jsx)(a.td,{children:'"days"'})]})]})]}),"\n",(0,n.jsxs)(a.p,{children:["Input field value: \u201c20000L\u201d ",(0,n.jsx)("br",{}),"\nPartition path generated from key generator: \u201c2024-10-04 12\u201d"]}),"\n",(0,n.jsxs)(a.p,{children:["If input field value is null. ",(0,n.jsx)("br",{}),"\nPartition path generated from key generator: \u201c1970-01-02 12\u201d"]}),"\n",(0,n.jsx)(a.h4,{id:"iso8601withmsz-with-single-input-format",children:"ISO8601WithMsZ with Single Input format"}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{children:"Config field"}),(0,n.jsx)(a.th,{children:"Value"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.timestamp.type"})}),(0,n.jsx)(a.td,{children:'"DATE_STRING"'})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.input.dateformat"})}),(0,n.jsxs)(a.td,{children:["\"yyyy-MM-dd'T'HH:mm",":ss",'.SSSZ"']})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.input.dateformat.list.delimiter.regex"})}),(0,n.jsx)(a.td,{children:'""'})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.input.timezone"})}),(0,n.jsx)(a.td,{children:'""'})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.output.dateformat"})}),(0,n.jsx)(a.td,{children:'"yyyyMMddHH"'})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.output.timezone"})}),(0,n.jsx)(a.td,{children:'"GMT"'})]})]})]}),"\n",(0,n.jsxs)(a.p,{children:['Input field value: "2020-04-01T13:01:33.428Z" ',(0,n.jsx)("br",{}),'\nPartition path generated from key generator: "2020040113"']}),"\n",(0,n.jsx)(a.h4,{id:"iso8601withmsz-with-multiple-input-formats",children:"ISO8601WithMsZ with Multiple Input formats"}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{children:"Config field"}),(0,n.jsx)(a.th,{children:"Value"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.timestamp.type"})}),(0,n.jsx)(a.td,{children:'"DATE_STRING"'})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.input.dateformat"})}),(0,n.jsxs)(a.td,{children:["\"yyyy-MM-dd'T'HH:mm",":ssZ",",yyyy-MM-dd'T'HH:mm",":ss",'.SSSZ"']})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.input.dateformat.list.delimiter.regex"})}),(0,n.jsx)(a.td,{children:'""'})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.input.timezone"})}),(0,n.jsx)(a.td,{children:'""'})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.output.dateformat"})}),(0,n.jsx)(a.td,{children:'"yyyyMMddHH"'})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.output.timezone"})}),(0,n.jsx)(a.td,{children:'"UTC"'})]})]})]}),"\n",(0,n.jsxs)(a.p,{children:['Input field value: "2020-04-01T13:01:33.428Z" ',(0,n.jsx)("br",{}),'\nPartition path generated from key generator: "2020040113"']}),"\n",(0,n.jsx)(a.h4,{id:"iso8601noms-with-offset-using-multiple-input-formats",children:"ISO8601NoMs with offset using multiple input formats"}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{children:"Config field"}),(0,n.jsx)(a.th,{children:"Value"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.timestamp.type"})}),(0,n.jsx)(a.td,{children:'"DATE_STRING"'})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.input.dateformat"})}),(0,n.jsxs)(a.td,{children:["\"yyyy-MM-dd'T'HH:mm",":ssZ",",yyyy-MM-dd'T'HH:mm",":ss",'.SSSZ"']})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.input.dateformat.list.delimiter.regex"})}),(0,n.jsx)(a.td,{children:'""'})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.input.timezone"})}),(0,n.jsx)(a.td,{children:'""'})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.output.dateformat"})}),(0,n.jsx)(a.td,{children:'"yyyyMMddHH"'})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.output.timezone"})}),(0,n.jsx)(a.td,{children:'"UTC"'})]})]})]}),"\n",(0,n.jsxs)(a.p,{children:['Input field value: "2020-04-01T13:01:33-',(0,n.jsx)(a.strong,{children:"05:00"}),'" ',(0,n.jsx)("br",{}),'\nPartition path generated from key generator: "2020040118"']}),"\n",(0,n.jsx)(a.h4,{id:"input-as-short-date-string-and-expect-date-in-date-format",children:"Input as short date string and expect date in date format"}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{children:"Config field"}),(0,n.jsx)(a.th,{children:"Value"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.timestamp.type"})}),(0,n.jsx)(a.td,{children:'"DATE_STRING"'})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.input.dateformat"})}),(0,n.jsxs)(a.td,{children:["\"yyyy-MM-dd'T'HH:mm",":ssZ",",yyyy-MM-dd'T'HH:mm",":ss",'.SSSZ,yyyyMMdd"']})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.input.dateformat.list.delimiter.regex"})}),(0,n.jsx)(a.td,{children:'""'})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.input.timezone"})}),(0,n.jsx)(a.td,{children:'"UTC"'})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.output.dateformat"})}),(0,n.jsx)(a.td,{children:'"MM/dd/yyyy"'})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.keygen.timebased.output.timezone"})}),(0,n.jsx)(a.td,{children:'"UTC"'})]})]})]}),"\n",(0,n.jsxs)(a.p,{children:['Input field value: "220200401" ',(0,n.jsx)("br",{}),'\nPartition path generated from key generator: "04/01/2020"']}),"\n",(0,n.jsx)(a.h3,{id:"customkeygenerator",children:(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/CustomKeyGenerator.java",children:"CustomKeyGenerator"})}),"\n",(0,n.jsxs)(a.p,{children:["This is a generic implementation of KeyGenerator where users are able to leverage the benefits of SimpleKeyGenerator,\nComplexKeyGenerator and TimestampBasedKeyGenerator all at the same time. One can configure record key and partition\npaths as a single field or a combination of fields. This keyGenerator is particularly useful if you want to define\ncomplex partition paths involving regular fields and timestamp based fields. It expects value for prop ",(0,n.jsx)(a.code,{children:'"hoodie.datasource.write.partitionpath.field"'}),'\nin a specific format. The format should be "field1',":PartitionKeyType1",",field2",":PartitionKeyType2",'..."']}),"\n",(0,n.jsxs)(a.p,{children:["The complete partition path is created as\n",(0,n.jsx)(a.code,{children:"<value for field1 basis PartitionKeyType1>/<value for field2 basis PartitionKeyType2> "}),"\nand so on. Each partition key type could either be SIMPLE or TIMESTAMP."]}),"\n",(0,n.jsxs)(a.p,{children:["Example config value: ",(0,n.jsx)(a.code,{children:"\u201cfield_3:simple,field_5:timestamp\u201d"})]}),"\n",(0,n.jsx)(a.p,{children:"RecordKey config value is either single field incase of SimpleKeyGenerator or a comma separate field names if referring to ComplexKeyGenerator.\nEg: \u201ccol1\u201d or \u201ccol3,col4\u201d."}),"\n",(0,n.jsx)(a.h3,{id:"nonpartitionedkeygenerator",children:(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/NonpartitionedKeyGenerator.java",children:"NonpartitionedKeyGenerator"})}),"\n",(0,n.jsx)(a.p,{children:"If your hudi dataset is not partitioned, you could use this \u201cNonpartitionedKeyGenerator\u201d which will return an empty\npartition for all records. In other words, all records go to the same partition (which is empty \u201c\u201d)"}),"\n",(0,n.jsx)(a.p,{children:"Hope this blog gave you a good understanding of different types of Key Generators available in Apache Hudi. Thanks for your continued support for Hudi's community."})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},38968:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(66847),n=t(74848),s=t(28453),r=t(9230);const o={title:"Practice of Apache Hudi in building real-time data lake at station B",authors:[{name:"Yu Zhaojing"}],category:"blog",image:"/assets/images/blog/2021-10-21-station-b-real-time-data-lake-using-hudi.png",tags:["use-case","real-time datalake","developpaper"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://developpaper.com/practice-of-apache-hudi-in-building-real-time-data-lake-at-station-b/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},39106:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(32901),n=t(74848),s=t(28453),r=t(9230);const o={title:"Building a Large-scale Transactional Data Lake at Uber Using Apache Hudi",authors:[{name:"Nishith Agarwal"}],category:"blog",image:"/assets/images/blog/2020-06-09-Building-a-Large-scale-Transactional-Data-Lake-at-Uber-Using-Apache-Hudi.png",tags:["use-case","datalake","analytics at scale","uber"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://eng.uber.com/apache-hudi-graduation/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},39110:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(98636),n=t(74848),s=t(28453),r=t(9230);const o={title:"How a POC became a production-ready Hudi data lakehouse through close team collaboration",excerpt:"How a POC became a production-ready Hudi data lakehouse through close team collaboration",author:"Xiaoxiao Rey and Hussein Awala",category:"blog",image:"/assets/images/blog/2024-02-12-How-a-POC-became-a-production-ready-Hudi-data-lakehouse-through-close-team-collaboration.png",tags:["use-case","apache hudi","leboncoin-tech-blog","beginner","delete","gdpr deletion","upsert"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/leboncoin-tech-blog/how-a-poc-became-a-production-ready-hudi-data-lakehouse-through-close-team-collaboration-c7f33eb746a8",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},39291:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(72338),n=t(74848),s=t(28453),r=t(9230);const o={title:"The Case for incremental processing on Hadoop",authors:[{name:"Vinoth Chandar"}],category:"blog",image:"/assets/images/blog/2016-08-04-The-Case-for-incremental-processing-on-Hadoop.png",tags:["use-case","incremental processing","oreilly"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.oreilly.com/ideas/ubers-case-for-incremental-processing-on-hadoop",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},39464:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/11/13/Apache-Hudi-From-Zero-To-One-blog-6","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-11-13-Apache-Hudi-From-Zero-To-One-blog-6.mdx","source":"@site/blog/2023-11-13-Apache-Hudi-From-Zero-To-One-blog-6.mdx","title":"Apache Hudi: From Zero To One (6/10)","description":"Redirecting... please wait!!","date":"2023-11-13T00:00:00.000Z","tags":[{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"spark","permalink":"/blog/tags/spark"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"course","permalink":"/blog/tags/course"},{"inline":true,"label":"tutorial","permalink":"/blog/tags/tutorial"},{"inline":true,"label":"datumagic","permalink":"/blog/tags/datumagic"},{"inline":true,"label":"data lake","permalink":"/blog/tags/data-lake"},{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"apache spark","permalink":"/blog/tags/apache-spark"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Shiyan Xu","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi: From Zero To One (6/10)","excerpt":"Demystify clustering and space-filling curves","author":"Shiyan Xu","category":"blog","image":"/assets/images/blog/2023-11-13-Apache-Hudi-From-Zero-To-One-blog-6.png","tags":["hudi","spark","blog","course","tutorial","datumagic","data lake","lakehouse","apache hudi","apache spark"]},"unlisted":false,"prevItem":{"title":"Hudi Streamer (Delta Streamer) Hands-On Guide: Local Ingestion from Parquet Source","permalink":"/blog/2023/11/19/Hudi-Streamer-DeltaStreamer-Hands-On-Guide-Local-Ingestion-from-Parquet-Source"},"nextItem":{"title":"Record Level Index: Hudi\'s blazing fast indexing for large-scale datasets","permalink":"/blog/2023/11/01/record-level-index"}}')},40012:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/12/01/Getting-started-with-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-12-01-Getting-started-with-Apache-Hudi.mdx","source":"@site/blog/2023-12-01-Getting-started-with-Apache-Hudi.mdx","title":"Getting started with Apache Hudi","description":"Redirecting... please wait!!","date":"2023-12-01T00:00:00.000Z","tags":[{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"apache spark","permalink":"/blog/tags/apache-spark"},{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"getting started","permalink":"/blog/tags/getting-started"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.11,"hasTruncateMarker":false,"authors":[{"name":"DataCouch","key":null,"page":null}],"frontMatter":{"title":"Getting started with Apache Hudi","excerpt":"Getting started with Apache Hudi","author":"DataCouch","category":"blog","image":"/assets/images/blog/2023-12-01-Getting-started-with-Apache-Hudi.png","tags":["apache hudi","apache spark","how-to","getting started","medium"]},"unlisted":false,"prevItem":{"title":"Apache Hudi: From Zero To One (7/10)","permalink":"/blog/2023/12/06/Apache-Hudi-From-Zero-To-One-blog-7"},"nextItem":{"title":"Mastering Data Lakes: A Deep Dive into MINIO, Hudi, and Delta Streamer","permalink":"/blog/2023/11/30/Mastering-Data-Lakes-A-Deep-Dive-into-MINIO-Hudi-and-Delta-Streamer"}}')},40222:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/02/12/table-service-deployment-models-in-apache-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-02-12-table-service-deployment-models-in-apache-hudi.mdx","source":"@site/blog/2023-02-12-table-service-deployment-models-in-apache-hudi.mdx","title":"Table service deployment models in Apache Hudi","description":"Redirecting... please wait!!","date":"2023-02-12T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"table services","permalink":"/blog/tags/table-services"},{"inline":true,"label":"deployment","permalink":"/blog/tags/deployment"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Sivabalan Narayanan","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Table service deployment models in Apache Hudi","authors":[{"name":"Sivabalan Narayanan"}],"category":"blog","tags":["how-to","table services","deployment","medium"]},"unlisted":false,"prevItem":{"title":"Bulk Insert Sort Modes with Apache Hudi","permalink":"/blog/2023/02/19/bulk-insert-sort-modes-with-apache-hudi"},"nextItem":{"title":"Automate schema evolution at scale with Apache Hudi in AWS Glue | Amazon Web Services","permalink":"/blog/2023/02/07/automate-schema-evolution-at-scale-with-apache-hudi-in-aws-glue"}}')},40474:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/2020-12-01-t3go-architecture-alluxio-b29648cdbfd10db14fde73b66ec499a5.png"},40584:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(72906),n=t(74848),s=t(28453);const r={title:"Concurrency Control in Open Data Lakehouse",excerpt:"How various concurrency control techniques works in Apache Hudi, Apache Iceberg & Delta Lake",author:"Dipankar Mazumdar",category:"blog",image:"/assets/images/blog/concurrency_control/concurrency_blog_thumb.jpg",tags:["multi-writer","concurrency","concurrency-control","non-blocking concurrency-control","Apache Hudi","Apache Iceberg","Delta Lake","blog","design"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Introduction",id:"introduction",level:2},{value:"Concurrency Control Foundations",id:"concurrency-control-foundations",level:2},{value:"Isolation and Serializability",id:"isolation-and-serializability",level:3},{value:"Pessimistic Concurrency Control (2PL)",id:"pessimistic-concurrency-control-2pl",level:4},{value:"Optimistic Concurrency Control (OCC)",id:"optimistic-concurrency-control-occ",level:4},{value:"Multi-Version Concurrency Control (MVCC)",id:"multi-version-concurrency-control-mvcc",level:4},{value:"Concurrency Control in Open Table Formats",id:"concurrency-control-in-open-table-formats",level:2},{value:"Apache Hudi",id:"apache-hudi",level:3},{value:"OCC (Multi Writers)",id:"occ-multi-writers",level:4},{value:"MVCC (Writer-Table Service and Table Service-Table Service)",id:"mvcc-writer-table-service-and-table-service-table-service",level:4},{value:"Non-Blocking Concurrency Control (Multi Writers)",id:"non-blocking-concurrency-control-multi-writers",level:4},{value:"Concurrency Control Deployment Modes in Hudi",id:"concurrency-control-deployment-modes-in-hudi",level:3},{value:"Single Writer with Inline Table Services",id:"single-writer-with-inline-table-services",level:4},{value:"Single Writer with Async Table Services",id:"single-writer-with-async-table-services",level:4},{value:"Multi-Writer Configuration",id:"multi-writer-configuration",level:4},{value:"How to use OCC with Apache Hudi and Apache Spark",id:"how-to-use-occ-with-apache-hudi-and-apache-spark",level:3},{value:"Apache Iceberg",id:"apache-iceberg",level:3},{value:"Delta Lake",id:"delta-lake",level:3},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const a={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.h2,{id:"introduction",children:"Introduction"}),"\n",(0,n.jsxs)(a.p,{children:["Concurrency control is critical in database management systems to ensure consistent and safe access to shared data by multiple users. Relational databases (RDBMS) such as ",(0,n.jsx)(a.a,{href:"https://dev.mysql.com/doc/refman/5.7/en/innodb-locking-transaction-model.html",children:"MySQL (InnoDB)"})," and analytical databases (such as data warehouses) have been offering robust concurrency control mechanisms to effectively deal with this. As data grows in scale and complexity, managing concurrent access becomes more challenging, especially in large distributed systems like Data Lakes or ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2024/07/11/what-is-a-data-lakehouse/",children:"Lakehouses"}),", which are expected to handle different types of workloads in the analytics realm. While data lakes have traditionally struggled with concurrent operations due to the lack of a ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/hudi_stack#storage-engine",children:"storage engine"})," and ACID guarantees, lakehouse architectures with open table formats like Apache Hudi, Apache Iceberg, and Delta Lake take inspiration from some of the widely used concurrency control methods to support high concurrent workloads."]}),"\n",(0,n.jsx)(a.p,{children:"This blog goes into the fundamentals of concurrency control, explores why it is essential for lakehouses, and examines how open table formats such as Apache Hudi enable strong concurrency control mechanisms to uphold the ACID properties and deal with varied workloads."}),"\n",(0,n.jsx)(a.h2,{id:"concurrency-control-foundations",children:"Concurrency Control Foundations"}),"\n",(0,n.jsxs)(a.p,{children:["At the core of concurrency control are the concepts of Isolation and Serializability, which define the expected behavior for concurrent transactions and ensure the ",(0,n.jsx)(a.strong,{children:'"I"'})," in ACID properties. Let\u2019s quickly go over these concepts from a general database system perspective."]}),"\n",(0,n.jsx)(a.h3,{id:"isolation-and-serializability",children:"Isolation and Serializability"}),"\n",(0,n.jsx)(a.p,{children:'In transactional systems, Isolation ensures that each transaction operates independently of others, as if it were executed in a single-user environment. This means a transaction should be "all by itself," free from interference by other concurrent operations, preventing concurrency anomalies like dirty reads or lost updates. This isolation allows end users (such as developers or analysts) to understand the impact of a transaction without worrying about conflicts from other simultaneous operations.'}),"\n",(0,n.jsx)(a.p,{children:"Serializability takes this idea further by defining the correct execution order for concurrent transactions. It guarantees that the outcome of executing transactions concurrently will be the same as if they had been executed serially, one after the other. In other words, even if transactions are interleaved, their combined effect should appear as though there were no parallel execution at all. Serializability is thus a rigorous correctness criterion that concurrency control models in databases strive to enforce, providing a predictable environment for transactional workloads."}),"\n",(0,n.jsx)(a.p,{children:'For example, imagine an online concert ticketing system where multiple customers are attempting to purchase tickets for the same concert at the same time. Suppose there are only 5 tickets left, and two customers - Customer A and Customer B try to buy 3 tickets each simultaneously. Without proper concurrency control, these transactions might interfere with each other, leading to scenarios where more tickets are "sold" than available in inventory, resulting in inconsistencies. To maintain serializability, the system must ensure that the outcome of processing these transactions concurrently is the same as if they were processed one at a time (serially), i.e. no more than 5 tickets are sold, ensuring inventory consistency.'}),"\n",(0,n.jsx)(a.p,{children:"Concurrency control methods can be broadly classified into three approaches: Pessimistic Concurrency Control, Optimistic Concurrency Control, and Multi-Version Concurrency Control (MVCC)."}),"\n",(0,n.jsx)(a.h4,{id:"pessimistic-concurrency-control-2pl",children:"Pessimistic Concurrency Control (2PL)"}),"\n",(0,n.jsx)(a.p,{children:"Pessimistic Concurrency Control assumes that conflicts between transactions can happen often and avoids having \u2018problems\u2019 in the first place. The most commonly used method, Strict Two-Phase Locking (2PL), works in this way:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Transactions acquire a shared lock before reading data and an exclusive lock before writing."}),"\n",(0,n.jsx)(a.li,{children:"Locks are held until the transaction commits or aborts but releases immediately after the commit command executes, ensuring serializability."}),"\n"]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/concurrency_control/2PL.png",alt:"2PL",width:"1000",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"If we take our online concert ticketing system example, where we have 5 tickets left and Customer A and Customer B both attempt to buy 3 tickets simultaneously. With Strict Two-Phase Locking (2PL), Transaction T1 (Customer A\u2019s purchase) acquires an exclusive lock on the inventory, preventing Transaction T2 (Customer B\u2019s purchase) from accessing it until T1 completes. T1 checks the inventory, deducts 3 tickets for Customer A, reducing the count to 2, and then releases the lock. Only then can T2 proceed, locking the inventory, seeing the updated 2 tickets, and completing the purchase for Customer B. This ensures serializability by isolating transactions through locking, yielding the same result as if the transactions had run one after the other."}),"\n",(0,n.jsx)(a.p,{children:"While Strict 2PL guarantees correctness, it comes with some downsides:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Transactions waiting to acquire locks may be blocked for long durations, especially in high-contention scenarios, leading to reduced throughput."}),"\n",(0,n.jsx)(a.li,{children:"If two transactions hold locks on different resources and wait for each other to release them, a deadlock occurs, requiring intervention (e.g., by aborting one transaction)."}),"\n",(0,n.jsx)(a.li,{children:"The strict correctness requirements can lead to long transaction times, making it less suitable for high-concurrency workloads."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Strict 2PL is present in relational database systems such as PostgreSQL, and Oracle Database."}),"\n",(0,n.jsx)(a.h4,{id:"optimistic-concurrency-control-occ",children:"Optimistic Concurrency Control (OCC)"}),"\n",(0,n.jsx)(a.p,{children:"Optimistic concurrency control takes the opposite approach - it assumes that conflicts happen rarely, and if there are such scenarios, then it would deal with it at the time of the conflict. OCC works this way:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Transactions track read and write operations and, upon completion, validate these changes to check for conflicts."}),"\n",(0,n.jsx)(a.li,{children:"If conflicts are detected, one or more conflicting transactions are rolled back and can be retried if needed be."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"OCC is particularly effective in low-contention environments, where conflicts between transactions are infrequent. However, in scenarios with frequent conflicts, such as multiple transactions attempting to modify the same data, OCC may result in a high number of rollbacks, reducing its efficiency. Its ability to allow multiple transactions to proceed without locking makes it a good choice for workloads where contention is low and throughput is prioritized over strict blocking mechanisms."}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/concurrency_control/OCC.png",alt:"OCC",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"For our example, with OCC, both transactions will proceed, each reading the initial count of 5 tickets and preparing to deduct 3. When they try to commit, a conflict check (history) will reveal that reducing by 3 tickets would oversell the inventory. As a result, one transaction (e.g., Customer B\u2019s) is rolled back, allowing Customer A to complete their purchase, reducing the inventory to 2. Customer B then retries, sees only 2 tickets left, and adjusts accordingly."}),"\n",(0,n.jsx)(a.h4,{id:"multi-version-concurrency-control-mvcc",children:"Multi-Version Concurrency Control (MVCC)"}),"\n",(0,n.jsx)(a.p,{children:"MVCC enables concurrent transactions by maintaining multiple versions of each data item, allowing transactions to read data as it appeared at a specific point in time. Here\u2019s how MVCC works at a high-level:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:'Each transaction is split into a "read set" and a "write set." This separation of read and write sets enhances concurrency by reducing conflicts.'}),"\n",(0,n.jsx)(a.li,{children:"All reads in a transaction operate as if they are accessing a single, consistent \u2018snapshot\u2019 of the data at a particular moment."}),"\n",(0,n.jsx)(a.li,{children:"Writes are applied as if they are part of a \u2018later snapshot\u2019, ensuring that any changes made by the transaction are isolated from other concurrent transactions until the transaction completes."}),"\n"]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/concurrency_control/MVCC.png",alt:"MVCC",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"In our example, with MVCC, each customer sees a consistent snapshot of 5 tickets when they start. Customer A completes their purchase first, reducing the inventory to 2 tickets. When Customer B finishes, they commit their transaction based on the latest snapshot, seeing only 2 tickets left and adjusting their purchase accordingly."}),"\n",(0,n.jsx)(a.h2,{id:"concurrency-control-in-open-table-formats",children:"Concurrency Control in Open Table Formats"}),"\n",(0,n.jsx)(a.p,{children:'Data lakes were built for scalable storage, cheaper cost, and to address some of the limitations of data warehouses (such as handling varied data types), but they lack the transactional storage engine needed to enforce ACID guarantees. We learnt in our previous section how isolation (the "I" in ACID) plays a critical role in managing concurrency by ensuring that each transaction operates independently without unintended interference from others. This level of isolation is essential for preventing concurrency anomalies like dirty reads, lost updates, and other issues that can compromise data integrity. Data lakehouse architecture with open table formats such as Apache Hudi, Apache Iceberg, and Delta Lake as the foundation for the storage layer addresses this problem by applying some of the concurrency control methods available in the database systems.'}),"\n",(0,n.jsxs)(a.p,{children:["Let\u2019s take a look at what type of concurrency control methods are available within these formats with a focus on ",(0,n.jsx)(a.strong,{children:"Apache Hudi"}),"."]}),"\n",(0,n.jsx)(a.h3,{id:"apache-hudi",children:"Apache Hudi"}),"\n",(0,n.jsxs)(a.p,{children:["Most of the concurrency control implementations today in lakehouse table formats focus on optimistically handling conflicts. OCC relies on the assumption that conflicts are rare, making it suitable for simple, append-only jobs but inadequate for scenarios that require frequent updates or deletes. In OCC, each job typically takes a table-level lock to check for conflicts by determining if there are overlapping files that multiple jobs have impacted. If a conflict is detected, the job will abort its operation ",(0,n.jsx)(a.em,{children:"entirely"}),". This could be a problem with certain types of workloads. For example, an ingest job writing data every 30 minutes and a deletion job running every two hours may often conflict, causing the deletion job to fail. In such cases especially with long-running transactions, OCC is problematic because the chance of conflicts increases over time."]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/concurrency_control/concur_blog.png",alt:"Hudi concurrency control methods",width:"900",align:"middle"}),"\n",(0,n.jsxs)(a.p,{children:["Apache Hudi\u2019s uniqueness lies in the fact that it clearly distinguishes the different actors interacting with the format, i.e. writer processes (that issue user\u2019s upserts/deletes), table services (such as clustering, compaction) and readers (that execute queries and read data). Hudi provides ",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Snapshot_isolation",children:"Snapshot Isolation"})," between all three types of processes, meaning they all operate on a consistent snapshot of the table. For writers, Hudi implements a variant of Serializable ",(0,n.jsx)(a.a,{href:"https://distributed-computing-musings.com/2022/02/transactions-serializable-snapshot-isolation/",children:"Snapshot Isolation (SSI)"}),". Here\u2019s how Hudi supports different types of concurrency control methods, offering fine-grained control over concurrent data access and updates."]}),"\n",(0,n.jsx)(a.h4,{id:"occ-multi-writers",children:"OCC (Multi Writers)"}),"\n",(0,n.jsx)(a.p,{children:"OCC is primarily used to manage concurrent writer processes in Hudi. For example, two different Spark jobs interacting with the same Hudi table to perform updates. Hudi\u2019s OCC workflow involves a series of checks to detect and handle conflicts, ensuring that only one writer can successfully commit changes to a particular file group at any given time. Here\u2019s a quick summary of what file groups and slices mean in Hudi."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.em,{children:"File group: Groups multiple versions of a base file (e.g. Parquet). The file group is uniquely identified by a File id. Each version corresponds to the commit's timestamp recording updates to records in the file."})}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.em,{children:"File slice: A File group can further be split into multiple slices. Each file slice within the file-group is uniquely identified by the commit's timestamp that created it."})}),"\n",(0,n.jsx)(a.p,{children:"OCC works in three phases - read, validate and write. When a writer begins a transaction, it first makes the changes, i.e. commits in isolation. During the validation phase, writers compare their proposed changes against existing file groups in the timeline to detect conflicts. Finally, in the write phase, the changes are either committed if no conflicts are found or rolled back if conflicts are detected."}),"\n",(0,n.jsxs)(a.p,{children:["For multi-writing scenarios, when a writer begins the commit process, it acquires a short-duration lock from the lock provider, typically implemented with an external service such as Zookeeper, Hive Metastore, or DynamoDB. Once the lock is secured, the writer loads the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/next/timeline",children:"current timeline"})," to check for previously ",(0,n.jsx)(a.code,{children:"completed"})," actions on the targeted file group. After that, it scans for any instances marked as completed with a timestamp greater than the target file slice's timestamp. If any such completed instances are found, it indicates that another writer has already modified the target file group, leading to a conflict. In this case, Hudi\u2019s OCC logic prevents the current transaction from proceeding by aborting the writer\u2019s operation, ensuring that only one writer\u2019s updates are committed. If no conflicting instant exists, the transaction is allowed to proceed, and the writer completes the write operation, adding a new file slice to the timeline. Finally, Hudi updates the timeline with the location of the new file slice and releases the table lock, allowing other transactions to proceed. This approach adheres to the ACID principles providing consistency guarantees."]}),"\n",(0,n.jsxs)(a.p,{children:["It is important to note that Hudi acquires locks ",(0,n.jsx)(a.strong,{children:"only"})," at critical points, such as during the commit or while scheduling table services, rather than across the entire transaction. This approach significantly improves concurrency by allowing writers to work in parallel without contention."]}),"\n",(0,n.jsx)(a.p,{children:"Additionally, Hudi\u2019s OCC operates at the file level, meaning conflicts are detected and resolved based on the files being modified. For instance, when two writers work on non-overlapping files, both writes are allowed to succeed. However, if their operations overlap and modify the same set of files, only one transaction will succeed, and the other will be rolled back. This file-level granularity is a significant advantage in many real-world scenarios, as it enables multiple writers to proceed without issues as long as they are working on different files, improving concurrency and overall throughput."}),"\n",(0,n.jsx)(a.h4,{id:"mvcc-writer-table-service-and-table-service-table-service",children:"MVCC (Writer-Table Service and Table Service-Table Service)"}),"\n",(0,n.jsxs)(a.p,{children:["Apache Hudi provides support for Multiversion Concurrency Control (MVCC) between writers and table-services (for example, an update Spark job and ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/clustering",children:"clustering"}),") and between different table services (such as ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/compaction",children:"compaction"})," and clustering). Similar to OCC, the Hudi timeline is instrumental in Hudi\u2019s MVCC implementation, which keeps a track of all the events (instants) happening in a particular Hudi table. Every writer and reader relies on the file system\u2019s state to decide where to carry out the operations, thereby providing read-write isolation."]}),"\n",(0,n.jsxs)(a.p,{children:["When a write operation begins, Hudi marks the action as either ",(0,n.jsx)(a.code,{children:"requested"})," or ",(0,n.jsx)(a.code,{children:"inflight"})," on the timeline, making all processes aware of the ongoing operation. This ensures that table management operations such as compaction and clustering are aware of active writes and do not include the file slices currently being modified. With Hudi 1.0's new ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/timeline/",children:"timeline"})," design, compaction and clustering operations are now based on both the requested and completion times of actions, treating these timestamps as ",(0,n.jsx)(a.em,{children:"intervals"})," to dynamically determine file slices. This means a service like compaction no longer needs to block ongoing writes and can be scheduled at any instant without interfering with active operations."]}),"\n",(0,n.jsx)(a.p,{children:"Under the new design, file slicing includes only those file slices whose completion times precede the start of the compaction or clustering process. This intelligent slicing mechanism ensures that these table management services work only on finalized data while new writes seamlessly continue without impacting the base files being compacted. By decoupling the scheduling of table services from active writes, Hudi 1.0 eliminates the need for strict scheduling sequences or blocking behaviors."}),"\n",(0,n.jsx)(a.h4,{id:"non-blocking-concurrency-control-multi-writers",children:"Non-Blocking Concurrency Control (Multi Writers)"}),"\n",(0,n.jsxs)(a.p,{children:["In a generic sense, Non-Blocking Concurrency Control (NBCC) allows multiple transactions to proceed simultaneously without locking, reducing delays and improving throughput in high-concurrency environments. ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0",children:"Hudi 1.0"})," introduces a new concurrency mode, ",(0,n.jsx)(a.code,{children:"NON_BLOCKING_CONCURRENCY_CONTROL"}),", where, unlike OCC, multiple writers can operate on the same table simultaneously with non-blocking conflict resolution. This approach eliminates the need for explicit locks to serialize writes, enabling higher concurrency. Instead of requiring each writer to wait, NBCC allows concurrent writes to proceed, making it ideal for real-time applications that demand faster data ingestion."]}),"\n",(0,n.jsxs)(a.p,{children:["In NBCC, the only lock required is for writing the commit metadata to the Hudi timeline, which ensures that the order and state of completed transactions is tracked accurately. With the release of version 1.0, Hudi introduces ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/timeline#truetime-generation",children:"TrueTime"})," semantics for instant times on the timeline, ensuring unique and monotonically increasing instant values. Each action on the Hudi timeline now includes both a ",(0,n.jsx)(a.em,{children:"requested time"})," and a ",(0,n.jsx)(a.em,{children:"completion time"}),", enabling these actions to be treated as intervals. This allows for more precise conflict detection by reasoning about overlapping actions within these time intervals.  The final serialization of writes in NBCC is determined by the ",(0,n.jsx)(a.em,{children:"completion"})," times. This means multiple writers can modify the same file group, with conflicts resolved automatically by query readers and the compactor. NBCC is available with the new Hudi 1.0 release, thereby providing more controls to balance speed with data consistency, even under heavy concurrent workloads."]}),"\n",(0,n.jsx)(a.h3,{id:"concurrency-control-deployment-modes-in-hudi",children:"Concurrency Control Deployment Modes in Hudi"}),"\n",(0,n.jsx)(a.p,{children:"Hudi offers several deployment models to handle different concurrency needs, allowing users to optimize for performance, simplicity, or high-concurrency scenarios depending on the requirements."}),"\n",(0,n.jsx)(a.h4,{id:"single-writer-with-inline-table-services",children:"Single Writer with Inline Table Services"}),"\n",(0,n.jsxs)(a.p,{children:["In this model, only one writer handles data ingestion or updates, with table services (such as cleaning, compaction, and clustering) running inline sequentially after every write. This approach ",(0,n.jsx)(a.em,{children:"eliminates"})," the need for concurrency control as all operations occur in a single process. MVCC in Hudi guarantees that readers see consistent snapshots, isolating them from ongoing writes and table services. This model is ideal for straightforward use cases where the focus is on getting data into the lakehouse without the complexity of managing multiple writers."]}),"\n",(0,n.jsx)(a.h4,{id:"single-writer-with-async-table-services",children:"Single Writer with Async Table Services"}),"\n",(0,n.jsx)(a.p,{children:"For workloads that require higher throughput without blocking writers, Hudi supports asynchronous table services. In this model, a single writer continuously ingests data, while table services such as compaction and clustering run asynchronously in the same process. MVCC allows these background jobs to operate concurrently with ingestion without creating conflicts, as they coordinate to avoid race conditions. This model suits applications where ingestion speed is essential, as async services help optimize the table in the background, reducing operational complexity without the need for external orchestration."}),"\n",(0,n.jsx)(a.h4,{id:"multi-writer-configuration",children:"Multi-Writer Configuration"}),"\n",(0,n.jsxs)(a.p,{children:["In cases where multiple writer jobs need to access the same table, Hudi supports multi-writer setups. This model allows disparate processes, such as multiple ingestion writers or a mix of ingestion and separate table service jobs to write concurrently. To manage conflicts, Hudi uses OCC with file-level conflict resolution, allowing non-overlapping writes to proceed while conflicting writes are resolved by allowing only one to succeed. For these types of multi-writer setups, ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/concurrency_control#external-locking-and-lock-providers",children:(0,n.jsx)(a.em,{children:"external"})})," lock providers like Amazon DynamoDB, Zookeeper, or Hive Metastore are required to coordinate concurrent access. This setup is ideal for production-level, high-concurrency environments where different processes need to modify the table simultaneously."]}),"\n",(0,n.jsx)(a.p,{children:"Note that while Hudi provides OCC to deal with multiple writers, table services can still run asynchronously and without locks if they operate in the same process as the writer. This is because Hudi intelligently differentiates between the different types of actors (writers, table services) that interact with the table."}),"\n",(0,n.jsx)(a.p,{children:"You will need to set the following properties to activate OCC with locks."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:"hoodie.write.concurrency.mode=optimistic_concurrency_control\nhoodie.write.lock.provider=<lock-provider-classname>\nhoodie.cleaner.policy.failed.writes=LAZY\n"})}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.code,{children:"Hoodie.write.lock.provider"})," defines the lock provider class that manages locks for concurrent writes. Default is ",(0,n.jsx)(a.code,{children:"org.apache.hudi.client.transaction.lock.ZookeeperBasedLockProvider"})]}),"\n",(0,n.jsxs)(a.p,{children:["The ",(0,n.jsx)(a.code,{children:"LAZY"})," mode cleans failed writes only after a heartbeat timeout when the cleaning service runs and is recommended when using multiple writers."]}),"\n",(0,n.jsx)(a.h3,{id:"how-to-use-occ-with-apache-hudi-and-apache-spark",children:"How to use OCC with Apache Hudi and Apache Spark"}),"\n",(0,n.jsxs)(a.p,{children:["This is a simple example where we configure OCC by setting the ",(0,n.jsx)(a.code,{children:"hoodie.write.concurrency.mode"})," to ",(0,n.jsx)(a.code,{children:"optimistic_concurrency_control"}),". We also specify a lock provider (in this case, Zookeeper) to manage concurrent access, along with essential table options like the precombine field, record key, and partition path."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-python",children:'from pyspark.sql import SparkSession\n\n# Initialize Spark session\nspark = SparkSession.builder \\\n    .appName("Hudi Example with OCC") \\\n    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \\\n    .getOrCreate()\n\n# Sample DataFrame\ninputDF = spark.createDataFrame([\n    (1, "2024-11-19 10:00:00", "A", "partition1"),\n    (2, "2024-11-19 10:05:00", "B", "partition1")\n], ["uuid", "ts", "value", "partitionpath"])\n\ntableName = "my_hudi_table"\nbasePath = "s3://path-to-your-hudi-table"\n\n# Write DataFrame to Hudi with OCC and Zookeeper lock provider\ninputDF.write.format("hudi") \\\n    .option("hoodie.datasource.write.precombine.field", "ts") \\\n    .option("hoodie.cleaner.policy.failed.writes", "LAZY") \\\n    .option("hoodie.write.concurrency.mode", "optimistic_concurrency_control") \\\n    .option("hoodie.write.lock.provider", "org.apache.hudi.client.transaction.lock.ZookeeperBasedLockProvider") \\\n    .option("hoodie.write.lock.zookeeper.url", "zk-cs.hudi-infra.svc.cluster.local") \\\n    .option("hoodie.write.lock.zookeeper.port", "2181") \\\n    .option("hoodie.write.lock.zookeeper.base_path", "/test") \\\n    .option("hoodie.datasource.write.recordkey.field", "uuid") \\\n    .option("hoodie.datasource.write.partitionpath.field", "partitionpath") \\\n    .option("hoodie.table.name", tableName) \\\n    .mode("overwrite") \\\n    .save(basePath)\n\nspark.stop()\n'})}),"\n",(0,n.jsx)(a.h3,{id:"apache-iceberg",children:"Apache Iceberg"}),"\n",(0,n.jsxs)(a.p,{children:["Apache Iceberg supports multiple concurrent writes through Optimistic Concurrency Control (OCC). The most important part to note here is that Iceberg needs a ",(0,n.jsx)(a.em,{children:"catalog"})," component to adhere to the ACID guarantees. Each writer assumes it is the only one making changes, generating new table metadata for its operation. When a writer completes its updates, it attempts to commit the changes by performing an ",(0,n.jsx)(a.em,{children:"atomic swap"})," of the latest ",(0,n.jsx)(a.code,{children:"metadata.json"})," file in the catalog, replacing the existing metadata file with the new one."]}),"\n",(0,n.jsx)(a.p,{children:"If this atomic swap fails (due to another writer committing changes in the meantime), the writer\u2019s commit is rejected. The writer then retries the entire process by creating a new metadata tree based on the latest state of the table and attempting the atomic swap again."}),"\n",(0,n.jsx)(a.p,{children:"When it comes to table maintenance tasks, such as optimizations (e.g., compaction) or large delete jobs, Iceberg treats these as regular writes. These operations can overlap with ingestion jobs, but they follow the same OCC principles - conflicts are resolved by retrying based on the latest table state. Users are recommended to schedule such jobs during official maintenance periods to avoid contention, as frequent retries due to conflicts can impact performance."}),"\n",(0,n.jsx)(a.h3,{id:"delta-lake",children:"Delta Lake"}),"\n",(0,n.jsxs)(a.p,{children:["Delta Lake provides concurrency control through Optimistic Concurrency Control (OCC) for transactional guarantees between writes. OCC allows multiple writers to attempt changes independently, assuming conflicts are infrequent. When a writer tries to commit, it checks for any conflicting updates from other transactions in the ",(0,n.jsx)(a.a,{href:"https://www.databricks.com/blog/2019/08/21/diving-into-delta-lake-unpacking-the-transaction-log.html",children:"transaction log"}),". If a conflict is found, the transaction is rolled back, and the writer retries based on the latest version of the data."]}),"\n",(0,n.jsx)(a.p,{children:"Additionally, Delta Lake employs Multi-Version Concurrency Control (MVCC) within the file system to separate reads from writes. By keeping data objects and the transaction log immutable, MVCC allows readers to access a consistent snapshot of the data, even as new writes are added. This not only protects existing data from modification during concurrent transactions but also enables time-travel queries, allowing users to query historical snapshots."}),"\n",(0,n.jsx)(a.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,n.jsx)(a.p,{children:"Concurrency control is critical for Open lakehouse architectures, especially when your architecture has multiple concurrent pipelines interacting with the same table. Open table formats such as Apache Hudi bring well-established concurrency control methods from traditional database systems into the Lakehouse architecture to handle these operations while maintaining data consistency and scalability. Apache Hudi\u2019s unique design to distinguish between writers, table services, and readers ensures snapshot isolation across all three processes. By supporting multiple concurrency control methods, such as OCC for managing writer conflicts, MVCC for isolating background table services and writers, and a novel NBCC for non-blocking, real-time ingestion, Hudi offers greater flexibility with complex workloads."}),"\n",(0,n.jsx)(a.hr,{})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},40686:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(32345),n=t(74848),s=t(28453),r=t(9230);const o={title:"Exploring New Frontiers: How Apache Flink, Apache Hudi and Presto Power New Insights at Scale",authors:[{name:"Nadine Farah"}],category:"blog",image:"/assets/images/blog/2023-06-16-Exploring-New-Frontiers-How-Apache-Flink-Apache-Hudi-and-Presto-Power-New-Insights-at-Scale.png",tags:["blog","prestocon","flink","presto","streaming","incremental etl"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.onehouse.ai/blog/exploring-new-frontiers-how-apache-flink-apache-hudi-and-presto-power-new-insights-at-scale",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},40748:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(9757),n=t(74848),s=t(28453),r=t(9230);const o={title:"Text-Based Search: From Elastic Search to Vector Search",authors:[{name:"Kaushik Muniandi"}],category:"blog",image:"/assets/images/blog/2023-06-03-text-based-search-from-elastic-search-to-vector-search.png",tags:["blog","vector search","indexing","bloom","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@m.kaushik90/text-based-search-from-elastic-search-to-vector-search-15d686258bf2",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},40789:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/01/30/Leverage-Partition-Paths-of-your-data-lake-tables-to-Optimize-Data-Retrieval-Costs-on-the-cloud","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-01-30-Leverage-Partition-Paths-of-your-data-lake-tables-to-Optimize-Data-Retrieval-Costs-on-the-cloud.mdx","source":"@site/blog/2024-01-30-Leverage-Partition-Paths-of-your-data-lake-tables-to-Optimize-Data-Retrieval-Costs-on-the-cloud.mdx","title":"Leverage Partition Paths of your data lake tables to Optimize Data Retrieval Costs on the cloud","description":"Redirecting... please wait!!","date":"2024-01-30T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"},{"inline":true,"label":"intermediate","permalink":"/blog/tags/intermediate"},{"inline":true,"label":"aws glue","permalink":"/blog/tags/aws-glue"},{"inline":true,"label":"cost","permalink":"/blog/tags/cost"},{"inline":true,"label":"apache spark","permalink":"/blog/tags/apache-spark"},{"inline":true,"label":"partition","permalink":"/blog/tags/partition"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"Krishna Prasad","key":null,"page":null}],"frontMatter":{"title":"Leverage Partition Paths of your data lake tables to Optimize Data Retrieval Costs on the cloud","excerpt":"Leverage Partition Paths of your data lake tables to Optimize Data Retrieval Costs on the cloud","author":"Krishna Prasad","category":"blog","image":"/assets/images/blog/2024-01-30-Leverage-Partition-Paths-of-your-data-lake-tables-to-Optimize-Data-Retrieval-Costs-on-the-cloud.png","tags":["blog","apache hudi","medium","intermediate","aws glue","cost","apache spark","partition"]},"unlisted":false,"prevItem":{"title":"Apache Hudi: Managing Partition on a petabyte-scale table","permalink":"/blog/2024/02/04/Apache-Hudi-Managing-Partition-on-a-petabyte-scale-table"},"nextItem":{"title":"Use Amazon Athena with Spark SQL for your open-source transactional table formats","permalink":"/blog/2024/01/24/Use-Amazon-Athena-with-Spark-SQL-for-your-open-source-transactional-table-formats"}}')},41515:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(43536),n=t(74848),s=t(28453),r=t(9230);const o={title:"Use Amazon Athena with Spark SQL for your open-source transactional table formats",excerpt:"Use Amazon Athena with Spark SQL for your open-source transactional table formats",author:"Pathik Shah, Raj Devnath",category:"blog",image:"/assets/images/blog/2024-01-24-Use-Amazon-Athena-with-Spark-SQL-for-your-open-source-transactional-table-formats.png",tags:["blog","apache hudi","aws","beginner","aws glue","aws athena","time travel query","clustering","compaction","aws s3","apache iceberg","delta lake"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/use-amazon-athena-with-spark-sql-for-your-open-source-transactional-table-formats/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},41546:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/05/02/how-query-apache-hudi-tables-python-using-daft-spark-free","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-05-02-how-query-apache-hudi-tables-python-using-daft-spark-free.mdx","source":"@site/blog/2024-05-02-how-query-apache-hudi-tables-python-using-daft-spark-free.mdx","title":"How to Query Apache Hudi Tables with Python Using Daft: A Spark-Free Approach","description":"Redirecting... please wait!!","date":"2024-05-02T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"python","permalink":"/blog/tags/python"},{"inline":true,"label":"daft","permalink":"/blog/tags/daft"},{"inline":true,"label":"linkedin","permalink":"/blog/tags/linkedin"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Soumil Shah","key":null,"page":null}],"frontMatter":{"title":"How to Query Apache Hudi Tables with Python Using Daft: A Spark-Free Approach","author":"Soumil Shah","category":"blog","image":"/assets/images/blog/2024-05-02-how-query-apache-hudi-tables-python-using-daft-spark-free.png","tags":["blog","apache hudi","python","daft","linkedin"]},"unlisted":false,"prevItem":{"title":"Learn how to read Hudi data with AWS Glue Ray using Daft (No Spark)","permalink":"/blog/2024/05/07/learn-how-read-hudi-data-aws-glue-ray-using-daft-spark"},"nextItem":{"title":"Apache Hudi vs Apache Iceberg: A Comprehensive Comparison","permalink":"/blog/2024/04/25/apache-hudi-vs-apache-iceberg-a-comprehensive-comparison"}}')},41634:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi-cdc-263ca6e0f40b6bff91517bd02c798e2d.jpg"},41733:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(23960),n=t(74848),s=t(28453),r=t(9230);const o={title:"Use open table format libraries on AWS Glue 5.0 for Apache Spark",author:"Sotaro Hikita and  Noritaka Sekiyama",category:"blog",image:"/assets/images/blog/2024-12-04-use-open-table-format-libraries-on-aws-glue-5-0-for-apache-spark.png",tags:["announcement","blog","apache hudi","aws glue","apache spark","table format","amazon"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/use-open-table-format-libraries-on-aws-glue-5-0-for-apache-spark/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},41893:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(89941),n=t(74848),s=t(28453);const r={title:"Automatic Record Key Generation in Apache Hudi",excerpt:"",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2025-09-17-hudi-auto-gen-keys/2025-09-17-hudi-auto-gen-keys.fig2.jpg",tags:["hudi","record key generation","database","data lakehouse"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"First-Class Support of Record Keys",id:"first-class-support-of-record-keys",level:2},{value:"Automatic Key Generation",id:"automatic-key-generation",level:2},{value:"Design Considerations",id:"design-considerations",level:3},{value:"Determining the Format",id:"determining-the-format",level:3},{value:"Summary",id:"summary",level:2}];function c(e){const a={a:"a",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(a.p,{children:["In database systems, the primary key is a foundational design principle for managing data at the record level. Its function is to provide each record with a unique and stable logical identifier, which decouples the record's identity from its physical location on storage. While using direct physical address pointers (e.g., position inside a file being used as a key) can be convenient, the physical address can change when records are moved around within the table for things like clustering or z-ordering (",(0,n.jsx)(a.a,{href:"https://x.com/apachehudi/status/1641572485325017089",children:"called out here"}),")."]}),"\n",(0,n.jsx)(a.p,{children:"By using a primary key that is stable across record movement, a system can efficiently perform operations like updates and deletes, enabling critical features like relational integrity."}),"\n",(0,n.jsx)(a.h2,{id:"first-class-support-of-record-keys",children:"First-Class Support of Record Keys"}),"\n",(0,n.jsx)(a.p,{children:"Apache Hudi was the first lakehouse storage project to introduce the notion of record keys. For mutable workloads, this addressed a significant architectural challenge. In a typical data lake table, updating records usually required rewriting entire partitions\u2014a process that is slow and expensive. By supporting the record key as the stable identifier for every record, Hudi offered unique and advanced capabilities among lakehouse frameworks:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["Hudi supports ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2023/11/01/record-level-index/",children:"record-level indexing"})," for directly locating records in ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/storage_layouts",children:"file groups"})," for highly efficient upserts and queries, and ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2025/04/02/secondary-index/",children:"secondary indexes"})," that enable performant lookups for predicates on non-record key fields."]}),"\n",(0,n.jsxs)(a.li,{children:["Hudi implements ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2025/03/03/record-mergers-in-hudi/",children:"merge modes"}),", standardizing record-merging semantics to handle requirements such as unordered events, duplicate records, and custom merge logic."]}),"\n",(0,n.jsxs)(a.li,{children:["By materializing record keys along with other ",(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/blog/hudi-metafields-demystified",children:"record-level meta-fields"}),", Hudi unlocks features such as efficient ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2024/07/30/data-lake-cdc/",children:"change data capture (CDC)"})," that serves record-level change streams, near-infinite history for time-travel queries, and the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/clustering",children:"clustering table service"})," that can significantly optimize file sizes."]}),"\n"]}),"\n",(0,n.jsx)("figure",{children:(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{src:t(82357).A+"",width:"647",height:"351"}),"\n",(0,n.jsx)("figcaption",{children:"Replicating operational databases to a Hudi lakehouse using CDC"})]})}),"\n",(0,n.jsxs)(a.p,{children:["Append-only writes are very common in the data lakehouse, such as ingesting application logs streamed continuously from numerous servers or capturing clickstream events from user interactions on a website. Even for this kind of scenario, having record keys is beneficial in scenarios like concurrently running data-fixing backfill writers (e.g., a GDPR deletion process) with ongoing writers to the same table. Without record keys, engineers typically had to coordinate the backfill to run on different partitions than the active writes to avoid conflicts. With record keys and the support provided by Hudi\u2019s ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/concurrency_control",children:"concurrency control"})," and merge modes, this restriction can be lifted, with Hudi handling the concurrent writes properly."]}),"\n",(0,n.jsxs)(a.p,{children:["Given the advantages of supporting record keys, Hudi required users to set one or multiple record key fields when creating a table prior to ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-0.14.0",children:"release 0.14"}),". However, this requirement created friction for users in cases where there were no natural record keys in the incoming stream for simply setting another config variable. Even for users who understood the benefits of record keys, they had to put careful thought into their record key generation to ensure uniqueness and idempotency. The initial friction of generating keys was a barrier to adoption for teams who simply wanted to land their append-only workloads in a lakehouse with as few lines of code and configuration as possible."]}),"\n",(0,n.jsx)(a.h2,{id:"automatic-key-generation",children:"Automatic Key Generation"}),"\n",(0,n.jsx)(a.p,{children:"With the release of version 0.14 (this is actually old news), Hudi has introduced automatic record key generation, a feature designed to simplify the user experience with append-only writes. This enhancement eliminates the mandatory requirement to specify record key fields for every write operation."}),"\n",(0,n.jsx)("figure",{children:(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{src:t(90282).A+"",width:"639",height:"316"}),"\n",(0,n.jsx)("figcaption",{children:"Hudi's auto key generation for append-only writes"})]})}),"\n",(0,n.jsxs)(a.p,{children:["Now, to perform append-only writes, you can simply omit the ",(0,n.jsx)(a.code,{children:"primaryKey"})," property in ",(0,n.jsx)(a.code,{children:"CREATE TABLE"})," statements (see the example below) or skip setting the ",(0,n.jsx)(a.code,{children:"hoodie.datasource.write.recordkey.field"})," or ",(0,n.jsx)(a.code,{children:"hoodie.table.recordkey.fields"})," configurations."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"CREATE TABLE hudi_table (\n    ts BIGINT,\n    uuid STRING,\n    rider STRING,\n    driver STRING,\n    fare DOUBLE,\n    city STRING\n) USING HUDI\nPARTITIONED BY (city);\n"})}),"\n",(0,n.jsxs)(a.p,{children:["In this example, you\u2019re creating a Copy-on-Write table partitioned by ",(0,n.jsx)(a.code,{children:"city"}),". Because the ",(0,n.jsx)(a.code,{children:"primaryKey"})," property is not present, Hudi automatically detects the omission and engages the auto key generation feature."]}),"\n",(0,n.jsx)(a.h3,{id:"design-considerations",children:"Design Considerations"}),"\n",(0,n.jsx)(a.p,{children:"Designing a key generation mechanism that operates efficiently at petabyte scale requires careful thought. We established five core requirements for the auto-generated keys:"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Global Uniqueness:"})," Keys must be unique across the entire table to maintain the integrity of a primary key."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Low Storage Footprint:"})," The keys should be highly compressible to add minimal storage overhead."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Computational Efficiency:"})," The encoding and decoding process must be lightweight so as not to slow down the write process."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Idempotency:"})," The generation process must be resilient to task retries, producing the same key for the same record every time."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Engine Agnostic:"})," The logic must be reusable and implemented consistently across different execution engines like Spark and Flink."]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"These principles guided the technical design. To align with primary key semantics, global uniqueness was non-negotiable. To minimize storage footprint, the generated keys needed to be compact and highly compressible, especially for tables with billions of records. The computational cost was also critical; any expensive operation would be amplified by the number of records, creating a significant performance overhead. Furthermore, in distributed systems where task failures and retries are common, the key generation process had to be idempotent\u2014ensuring the same input record always produces the exact same key. Finally, the solution needed to be engine-agnostic to provide consistent behavior, whether data is written via Spark, Flink, or another supported engine."}),"\n",(0,n.jsx)(a.h3,{id:"determining-the-format",children:"Determining the Format"}),"\n",(0,n.jsxs)(a.p,{children:["Based on the requirements mentioned previously, we eliminated several common ID generation techniques. For instance, we cannot use simple auto-incrementing IDs for each batch of writes, as it will not satisfy global uniqueness in the table across different writes. We also rule out using the ",(0,n.jsx)(a.code,{children:"monotonically_increasing_id"})," function in Spark, as it does not guarantee global uniqueness either. Furthermore, using such functions violates the rule of being engine-agnostic. We do not use random ID generation such as UUID (v4, v6, and v7) and ULID, which do not satisfy the idempotency requirement. The final format that we chose is a deterministic, composite key with the following structure:"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-text",children:"<write action start time>-<workload partition ID>-<record sequence ID>\n"})}),"\n",(0,n.jsx)(a.p,{children:"Each component serves a specific purpose:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Write Action Start Time:"})," The timestamp from the Hudi timeline that marks the beginning of a write transaction."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Workload Partition ID:"})," An internal identifier that execution engines use to track the specific data split being processed by a given distributed write task."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Record Sequence ID:"})," A counter that uniquely identifies each record within that data split."]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Together, these three components\u2014all readily accessible during the write process\u2014form a record identifier that satisfies the requirements of global uniqueness, idempotency, and being engine-agnostic."}),"\n",(0,n.jsxs)(a.p,{children:["Next, we evaluate the generated keys against the requirements of low storage footprint and computational efficiency. The following tables highlight some experiment numbers based on the ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/rfc/rfc-76/rfc-76.md",children:"RFC document"})," of the auto key generation feature."]}),"\n",(0,n.jsx)(a.p,{children:"For storage efficiency, we compare the original strings with UUID v6/7, Base64, and ASCII encoding schemes:"}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Format"}),(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Uncompressed size (bytes)"}),(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Compressed size (bytes)"}),(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Compression ratio"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"Original string"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"4,000,185"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"244,373"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"11.1"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"UUID v6/7"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"4,000,184"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"1,451,897"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"2.74"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"Base64"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"2,400,184"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"202,095"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"11.9"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"ASCII"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"1,900,185"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"176,606"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"10.8"})]})]})]}),"\n",(0,n.jsx)(a.p,{children:"We also compare their compute efficiency using the original string format as the baseline:"}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Format"}),(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Average runtime (ms)"}),(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Ratio to baseline"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"Original string"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"0.00001"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"1"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"UUID v6/7"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"0.0001"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"10"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"Base64"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"0.004"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"400"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"ASCII"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"0.004"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"400"})]})]})]}),"\n",(0,n.jsx)(a.p,{children:"Based on the micro-benchmarking results, UUID v6/7 resulted in a much larger and undesired compressed size compared to others. Base64 and ASCII encoding had a lower storage footprint compared to the original string, with around 17% and 28% reduction respectively. However, both Base64 and ASCII require 400x more CPU power for encoding than the original string format. Given that write performance is often more critical than marginal storage savings in high-throughput data systems, we opted for the original string format for auto-generating record keys."}),"\n",(0,n.jsx)(a.h2,{id:"summary",children:"Summary"}),"\n",(0,n.jsx)(a.p,{children:"Hudi\u2019s first-class support for record keys provides a database-like experience for lakehouses, enabling powerful features such as record-level indexing, merge modes, and CDC. The introduction of automatic record key generation thoughtfully extends the record key support, removing a barrier for teams performing append-only writes. By following the design principles of uniqueness, idempotency, and efficiency, the feature allows more users to easily adopt Hudi and benefit from its rich set of lakehouse capabilities without the initial overhead of manual key generation. This enhancement reinforces Hudi\u2019s position as a versatile and user-friendly platform for building modern data lakehouses."})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},41905:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(47725),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi: A Deep Dive with Python Code Examples",author:"Harsh Daiya",category:"blog",image:"/assets/images/blog/2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.png",tags:["blog","apache hudi","python","pyspark","harshdaiya"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blog.harshdaiya.com/apache-hudi-a-deep-dive-with-python-code-examples",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},41911:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/11/22/Build-your-Apache-Hudi-data-lake-on-AWS-using-Amazon-EMR-Part-1","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-11-22-Build-your-Apache-Hudi-data-lake-on-AWS-using-Amazon-EMR-Part-1.mdx","source":"@site/blog/2022-11-22-Build-your-Apache-Hudi-data-lake-on-AWS-using-Amazon-EMR-Part-1.mdx","title":"Build your Apache Hudi data lake on AWS using Amazon EMR \u2013 Part 1","description":"Redirecting... please wait!!","date":"2022-11-22T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"best practices","permalink":"/blog/tags/best-practices"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.16,"hasTruncateMarker":false,"authors":[{"name":"Suthan Phillips","socials":{},"key":null,"page":null},{"name":"Dylan Qu","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Build your Apache Hudi data lake on AWS using Amazon EMR \u2013 Part 1","authors":[{"name":"Suthan Phillips"},{"name":"Dylan Qu"}],"category":"blog","image":"/assets/images/blog/2022-11-22-aws_hudi_best_practices_part1.png","tags":["how-to","best practices","amazon"]},"unlisted":false,"prevItem":{"title":"Run Apache Hudi at scale on AWS","permalink":"/blog/2022/12/01/Run-apache-hudi-at-scale-on-aws"},"nextItem":{"title":"How Hudl built a cost-optimized AWS Glue pipeline with Apache Hudi datasets","permalink":"/blog/2022/11/10/How-Hudl-built-a-cost-optimized-AWS-Glue-pipeline-with-Apache-Hudi-datasets"}}')},41970:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(95878),n=t(74848),s=t(28453),r=t(9230);const o={title:"Get started with Apache Hudi using AWS Glue by implementing key design concepts \u2013 Part 1",excerpt:"Get started with Apache Hudi using AWS Glue by implementing key design concepts \u2013 Part 1",authors:[{name:"Srinivas Kandi"},{name:"Ravi Itha"}],category:"blog",image:"/assets/images/blog/2023-10-17-Get-started-with-Apache-Hudi-using-AWS-Glue-by-implementing-key-design-concepts-Part-1.png",tags:["aws glue","apache hudi","how-to","amazon","design","aws glue","upserts","bulk insert","indexing"]},l=void 0,d={authorsImageUrls:[void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/part-1-get-started-with-apache-hudi-using-aws-glue-by-implementing-key-design-concepts/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},42261:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2017/03/12/Hoodie-Uber-Engineerings-Incremental-Processing-Framework-on-Hadoop","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2017-03-12-Hoodie-Uber-Engineerings-Incremental-Processing-Framework-on-Hadoop.mdx","source":"@site/blog/2017-03-12-Hoodie-Uber-Engineerings-Incremental-Processing-Framework-on-Hadoop.mdx","title":"Hoodie: Uber Engineering\'s Incremental Processing Framework on Hadoop","description":"Redirecting... please wait!!","date":"2017-03-12T00:00:00.000Z","tags":[{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"incremental processing","permalink":"/blog/tags/incremental-processing"},{"inline":true,"label":"uber","permalink":"/blog/tags/uber"}],"readingTime":0.08,"hasTruncateMarker":false,"authors":[{"name":"Prasanna Rajaperumal","socials":{},"key":null,"page":null},{"name":"Vinoth Chandar","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Hoodie: Uber Engineering\'s Incremental Processing Framework on Hadoop","authors":[{"name":"Prasanna Rajaperumal"},{"name":"Vinoth Chandar"}],"category":"blog","image":"/assets/images/blog/2017-03-12-Hoodie-Uber-Engineerings-Incremental-Processing-Framework-on-Hadoop.png","tags":["use-case","incremental processing","uber"]},"unlisted":false,"prevItem":{"title":"Hudi entered Apache Incubator","permalink":"/blog/2019/01/18/asf-incubation"},"nextItem":{"title":"Connect with us at Strata San Jose March 2017","permalink":"/blog/2016/12/30/strata-talk-2017"}}')},42314:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/11/22/Introducing-Apache-Hudi-support-with-AWS-Glue-crawlers","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-11-22-Introducing-Apache-Hudi-support-with-AWS-Glue-crawlers.mdx","source":"@site/blog/2023-11-22-Introducing-Apache-Hudi-support-with-AWS-Glue-crawlers.mdx","title":"Introducing Apache Hudi support with AWS Glue crawlers","description":"Redirecting... please wait!!","date":"2023-11-22T00:00:00.000Z","tags":[{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"aws glue crawlers","permalink":"/blog/tags/aws-glue-crawlers"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Noritaka Sekiyama, Kyle Duong, Sandeep Adwankar","key":null,"page":null}],"frontMatter":{"title":"Introducing Apache Hudi support with AWS Glue crawlers","excerpt":"Introducing Apache Hudi support with AWS Glue crawlers","author":"Noritaka Sekiyama, Kyle Duong, Sandeep Adwankar","category":"blog","image":"/assets/images/blog/2023-11-22-Introducing-Apache-Hudi-support-with-AWS-Glue-crawlers.png","tags":["apache hudi","how-to","aws glue crawlers"]},"unlisted":false,"prevItem":{"title":"Real-Time Data Processing with Postgres, Debezium, Kafka, Schema Registry, and Delta Streamer Guide for Begineers","permalink":"/blog/2023/11/26/Real-Time-Data-Processing-with-Postgres-Debezium-Kafka-Schema-Registry-and-DeltaStreamer-Guide-for-Begineers"},"nextItem":{"title":"Hudi Streamer (Delta Streamer) Hands-On Guide: Local Ingestion from Parquet Source","permalink":"/blog/2023/11/19/Hudi-Streamer-DeltaStreamer-Hands-On-Guide-Local-Ingestion-from-Parquet-Source"}}')},42396:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(48198),n=t(74848),s=t(28453),r=t(9230);const o={title:"Understanding its core concepts from hudi persistence files",authors:[{name:"QbertsBrother"}],category:"blog",image:"/assets/images/blog/2022-02-20-understanding-core-concepts-from-hudi-persistence-files.png",tags:["blog","storage spec","programmer"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://programmer.ink/think/understanding-its-core-concepts-from-hudi-persistence-files.html",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},42638:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(90204),n=t(74848),s=t(28453);const r={title:"Registering sample dataset to Hive via beeline",excerpt:"How to manually register HUDI dataset into Hive using beeline",author:"vinoth",category:"blog",tags:["how-to","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[];function c(e){const a={code:"code",em:"em",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.p,{children:"Hudi hive sync tool typically handles registration of the dataset into Hive metastore. In case, there are issues with quickstart around this, following page shows commands that can be used to do this manually via beeline."}),"\n",(0,n.jsxs)(a.p,{children:["Add in the ",(0,n.jsx)(a.em,{children:"packaging/hoodie-hive-bundle/target/hoodie-hive-bundle-0.4.6-SNAPSHOT.jar,"})," so that Hive can read the Hudi dataset and answer the query."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"hive> set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;\nhive> set hive.stats.autogather=false;\nhive> add jar file:///path/to/hoodie-hive-bundle-0.5.2-SNAPSHOT.jar;\nAdded [file:///path/to/hoodie-hive-bundle-0.5.2-SNAPSHOT.jar] to class path\nAdded resources: [file:///path/to/hoodie-hive-bundle-0.5.2-SNAPSHOT.jar]\n"})}),"\n",(0,n.jsxs)(a.p,{children:["Then, you need to create a ",(0,n.jsx)(a.em,{children:"ReadOptimized"})," Hive table as below and register the sample partitions"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"DROP TABLE hoodie_test;\nCREATE EXTERNAL TABLE hoodie_test(`_row_key` string,\n    `_hoodie_commit_time` string,\n    `_hoodie_commit_seqno` string,\n    rider string,\n    driver string,\n    begin_lat double,\n    begin_lon double,\n    end_lat double,\n    end_lon double,\n    fare double)\n    PARTITIONED BY (`datestr` string)\n    ROW FORMAT SERDE\n    'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n    STORED AS INPUTFORMAT\n    'com.uber.hoodie.hadoop.HoodieInputFormat'\n    OUTPUTFORMAT\n    'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n    LOCATION\n    'hdfs:///tmp/hoodie/sample-table';\n     \nALTER TABLE `hoodie_test` ADD IF NOT EXISTS PARTITION (datestr='2016-03-15') LOCATION 'hdfs:///tmp/hoodie/sample-table/2016/03/15';\nALTER TABLE `hoodie_test` ADD IF NOT EXISTS PARTITION (datestr='2015-03-16') LOCATION 'hdfs:///tmp/hoodie/sample-table/2015/03/16';\nALTER TABLE `hoodie_test` ADD IF NOT EXISTS PARTITION (datestr='2015-03-17') LOCATION 'hdfs:///tmp/hoodie/sample-table/2015/03/17';\n     \nset mapreduce.framework.name=yarn;\n"})}),"\n",(0,n.jsxs)(a.p,{children:["And you can add a ",(0,n.jsx)(a.em,{children:"Realtime"})," Hive table, as below"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"DROP TABLE hoodie_rt;\nCREATE EXTERNAL TABLE hoodie_rt(\n    `_hoodie_commit_time` string,\n    `_hoodie_commit_seqno` string,\n    `_hoodie_record_key` string,\n    `_hoodie_partition_path` string,\n    `_hoodie_file_name` string,\n    timestamp double,\n    `_row_key` string,\n    rider string,\n    driver string,\n    begin_lat double,\n    begin_lon double,\n    end_lat double,\n    end_lon double,\n    fare double)\n    PARTITIONED BY (`datestr` string)\n    ROW FORMAT SERDE\n    'com.uber.hoodie.hadoop.realtime.HoodieParquetSerde'\n    STORED AS INPUTFORMAT\n    'com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat'\n    OUTPUTFORMAT\n    'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n    LOCATION\n    'file:///tmp/hoodie/sample-table';\n     \nALTER TABLE `hoodie_rt` ADD IF NOT EXISTS PARTITION (datestr='2016-03-15') LOCATION 'file:///tmp/hoodie/sample-table/2016/03/15';\nALTER TABLE `hoodie_rt` ADD IF NOT EXISTS PARTITION (datestr='2015-03-16') LOCATION 'file:///tmp/hoodie/sample-table/2015/03/16';\nALTER TABLE `hoodie_rt` ADD IF NOT EXISTS PARTITION (datestr='2015-03-17') LOCATION 'file:///tmp/hoodie/sample-table/2015/03/17';\n"})})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},42664:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/Screen_Shot_2021-07-20_at_5.35.47_PM-0a1e607d305e2293307a6fa9a980813d.png"},42679:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(52999),n=t(74848),s=t(28453),r=t(9230);const o={title:"Curious Engineering Facts ( Trace Agents | Hudi| Daft : 1) : March Release 18 : 25",author:"Gayan Sanjeewa",category:"blog",image:"/assets/images/blog/2025-02-25-curious-engineering-facts-trace-agents-hudi-daft-1.jpeg",tags:["blog","apache hudi","daft","trace agents","openai","llm","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@kkgsanjeewac77/curious-engineering-facts-trace-agents-hudi-daft-1-march-release-18-25-bedc00e05ecd",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},42698:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/11/19/automated-small-file-handling","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-11-19-automated-small-file-handling.md","source":"@site/blog/2024-11-19-automated-small-file-handling.md","title":"Hudi\u2019s Automatic File Sizing Delivers Unmatched Performance","description":"Introduction","date":"2024-11-19T00:00:00.000Z","tags":[{"inline":true,"label":"Data Lake","permalink":"/blog/tags/data-lake"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":8.09,"hasTruncateMarker":false,"authors":[{"name":"Aditya Goenka","key":null,"page":null}],"frontMatter":{"title":"Hudi\u2019s Automatic File Sizing Delivers Unmatched Performance","excerpt":"Explains how Hudi handles small files during ingestion and its benefits","author":"Aditya Goenka","category":"blog","image":"/assets/images/blog/2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.png","tags":["Data Lake","Apache Hudi"]},"unlisted":false,"prevItem":{"title":"Apache Iceberg vs Hudi: Key Features, Performance & Use Cases","permalink":"/blog/2024/12/03/apache-iceberg-vs-apache-hudi"},"nextItem":{"title":"Record Level Indexing in Apache Hudi","permalink":"/blog/2024/11/12/record-level-indexing-in-apache-hudi"}}')},43019:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(6895),n=t(74848),s=t(28453),r=t(9230);const o={title:"Lakehouse Trifecta \u2014 Delta Lake, Apache Iceberg & Apache Hudi",authors:[{name:"Sandip Roy"}],category:"blog",image:"/assets/images/blog/2023-08-09-Lakehouse-Trifecta-Delta-Lake-Apache-Iceberg-and-Apache-Hudi.png",tags:["blog","hudi","delta lake","iceberg","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://roysandip.medium.com/lakehouse-trifecta-delta-lake-apache-iceberg-apache-hudi-747e99c467b",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},43160:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(75341),n=t(74848),s=t(28453),r=t(9230);const o={title:"A Beginner\u2019s Guide to Apache Hudi with PySpark \u2014 Part 1 of 2",author:"Sagar Lakshmipathy",category:"blog",image:"/assets/images/blog/2023-09-19-A-Beginners-Guide-to-Apache-Hudi-with-PySpark-Part-1-of-2.png",tags:["pyspark","apache hudi","how-to","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@sagarlakshmipathy/a-beginners-guide-to-apache-hudi-with-pyspark-part-1-of-2-8a4e78f6ad2e",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},43267:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(79663),n=t(74848),s=t(28453),r=t(9230);const o={title:"Getting started with Apache Hudi",excerpt:"Getting started with Apache Hudi",author:"DataCouch",category:"blog",image:"/assets/images/blog/2023-12-09-Getting-started-with-Apache-Hudi.png",tags:["blog","apache hudi","medium","beginner"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://datacouch.medium.com/getting-started-with-apache-hudi-711b89c107aa",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},43536:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/01/24/Use-Amazon-Athena-with-Spark-SQL-for-your-open-source-transactional-table-formats","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-01-24-Use-Amazon-Athena-with-Spark-SQL-for-your-open-source-transactional-table-formats.mdx","source":"@site/blog/2024-01-24-Use-Amazon-Athena-with-Spark-SQL-for-your-open-source-transactional-table-formats.mdx","title":"Use Amazon Athena with Spark SQL for your open-source transactional table formats","description":"Redirecting... please wait!!","date":"2024-01-24T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"aws","permalink":"/blog/tags/aws"},{"inline":true,"label":"beginner","permalink":"/blog/tags/beginner"},{"inline":true,"label":"aws glue","permalink":"/blog/tags/aws-glue"},{"inline":true,"label":"aws athena","permalink":"/blog/tags/aws-athena"},{"inline":true,"label":"time travel query","permalink":"/blog/tags/time-travel-query"},{"inline":true,"label":"clustering","permalink":"/blog/tags/clustering"},{"inline":true,"label":"compaction","permalink":"/blog/tags/compaction"},{"inline":true,"label":"aws s3","permalink":"/blog/tags/aws-s-3"},{"inline":true,"label":"apache iceberg","permalink":"/blog/tags/apache-iceberg"},{"inline":true,"label":"delta lake","permalink":"/blog/tags/delta-lake"}],"readingTime":0.16,"hasTruncateMarker":false,"authors":[{"name":"Pathik Shah, Raj Devnath","key":null,"page":null}],"frontMatter":{"title":"Use Amazon Athena with Spark SQL for your open-source transactional table formats","excerpt":"Use Amazon Athena with Spark SQL for your open-source transactional table formats","author":"Pathik Shah, Raj Devnath","category":"blog","image":"/assets/images/blog/2024-01-24-Use-Amazon-Athena-with-Spark-SQL-for-your-open-source-transactional-table-formats.png","tags":["blog","apache hudi","aws","beginner","aws glue","aws athena","time travel query","clustering","compaction","aws s3","apache iceberg","delta lake"]},"unlisted":false,"prevItem":{"title":"Leverage Partition Paths of your data lake tables to Optimize Data Retrieval Costs on the cloud","permalink":"/blog/2024/01/30/Leverage-Partition-Paths-of-your-data-lake-tables-to-Optimize-Data-Retrieval-Costs-on-the-cloud"},"nextItem":{"title":"Data Engineering: Bootstrapping Data lake with Apache Hudi","permalink":"/blog/2024/01/20/Data-Engineering-Bootstrapping-Data-lake-with-Apache-Hudi"}}')},43537:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(94183),n=t(74848),s=t(28453),r=t(9230);const o={title:"I spent 5 hours exploring the story behind Apache Hudi.",author:"Vu Trinh",category:"blog",image:"/assets/images/blog/2024-10-27-I-spent-5-hours-exploring-the-story-behind-Apache-Hudi.jpeg",tags:["blog","apache hudi","beginner","det"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blog.det.life/i-spent-5-hours-exploring-the-story-behind-apache-hudi-dacad829394d",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},43793:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/11/22/Apache-Hudi-Architecture-Tools-and-Best-Practices","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-11-22-Apache-Hudi-Architecture-Tools-and-Best-Practices.mdx","source":"@site/blog/2021-11-22-Apache-Hudi-Architecture-Tools-and-Best-Practices.mdx","title":"Apache Hudi Architecture Tools and Best Practices","description":"Redirecting... please wait!!","date":"2021-11-22T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"xenonstack","permalink":"/blog/tags/xenonstack"}],"readingTime":0.1,"hasTruncateMarker":false,"authors":[{"name":"Chandan Gaur","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Apache Hudi Architecture Tools and Best Practices","authors":[{"name":"Chandan Gaur"}],"category":"blog","image":"/assets/images/blog/2021-11-22-hudi-architecture-tools-best-practices.png","tags":["blog","xenonstack"]},"unlisted":false,"prevItem":{"title":"Lakehouse Concurrency Control: Are we too optimistic?","permalink":"/blog/2021/12/16/lakehouse-concurrency-control-are-we-too-optimistic"},"nextItem":{"title":"How GE Aviation built cloud-native data pipelines at enterprise scale using the AWS platform","permalink":"/blog/2021/11/16/How-GE-Aviation-built-cloud-native-data-pipelines-at-enterprise-scale-using-the-AWS-platform"}}')},43898:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(59499),n=t(74848),s=t(28453),r=t(9230);const o={title:"ACID Transactions in an Open Data Lakehouse",author:"Dipankar Mazumdar",category:"blog",image:"/assets/images/blog/acid.png",tags:["blog","Apache Hudi","Apache Iceberg","Delta Lake","ACID"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.onehouse.ai/blog/acid-transactions-in-an-open-data-lakehouse",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},43900:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/02/04/Apache-Hudi-Managing-Partition-on-a-petabyte-scale-table","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-02-04-Apache-Hudi-Managing-Partition-on-a-petabyte-scale-table.mdx","source":"@site/blog/2024-02-04-Apache-Hudi-Managing-Partition-on-a-petabyte-scale-table.mdx","title":"Apache Hudi: Managing Partition on a petabyte-scale table","description":"Redirecting... please wait!!","date":"2024-02-04T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"},{"inline":true,"label":"intermediate","permalink":"/blog/tags/intermediate"},{"inline":true,"label":"partition","permalink":"/blog/tags/partition"},{"inline":true,"label":"aws glue","permalink":"/blog/tags/aws-glue"},{"inline":true,"label":"apache spark","permalink":"/blog/tags/apache-spark"},{"inline":true,"label":"aws s3","permalink":"/blog/tags/aws-s-3"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Krishna Prasad","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi: Managing Partition on a petabyte-scale table","excerpt":"Apache Hudi: Managing Partition on a petabyte-scale table","author":"Krishna Prasad","category":"blog","image":"/assets/images/blog/2024-02-04-Apache-Hudi-Managing-Partition-on-a-petabyte-scale-table.png","tags":["blog","apache hudi","medium","intermediate","partition","aws glue","apache spark","aws s3"]},"unlisted":false,"prevItem":{"title":"Combine Transactional Integrity and Data Lake Operations with YugabyteDB and Apache Hudi","permalink":"/blog/2024/02/06/Combine-Transactional-Integrity-and-Data-Lake-Operations-with-YugabyteDB-and-Apache-Hudi"},"nextItem":{"title":"Leverage Partition Paths of your data lake tables to Optimize Data Retrieval Costs on the cloud","permalink":"/blog/2024/01/30/Leverage-Partition-Paths-of-your-data-lake-tables-to-Optimize-Data-Retrieval-Costs-on-the-cloud"}}')},43983:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2020/08/18/hudi-incremental-processing-on-data-lakes","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-08-18-hudi-incremental-processing-on-data-lakes.md","source":"@site/blog/2020-08-18-hudi-incremental-processing-on-data-lakes.md","title":"Incremental Processing on the Data Lake","description":"NOTE: This article is a translation of the infoq.cn article, found here, with minor edits","date":"2020-08-18T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"datalake","permalink":"/blog/tags/datalake"},{"inline":true,"label":"incremental processing","permalink":"/blog/tags/incremental-processing"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":18.05,"hasTruncateMarker":true,"authors":[{"name":"vinoyang","key":null,"page":null}],"frontMatter":{"title":"Incremental Processing on the Data Lake","excerpt":"How Apache Hudi provides ability for incremental data processing.","author":"vinoyang","category":"blog","image":"/assets/images/blog/incr-processing/image7.png","tags":["blog","datalake","incremental processing","apache hudi"]},"unlisted":false,"prevItem":{"title":"Efficient Migration of Large Parquet Tables to Apache Hudi","permalink":"/blog/2020/08/20/efficient-migration-of-large-parquet-tables"},"nextItem":{"title":"PrestoDB and Apache Hudi","permalink":"/blog/2020/08/04/PrestoDB-and-Apache-Hudi"}}')},44501:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(83980),n=t(74848),s=t(28453),r=t(9230);const o={title:"Mastering Data Lakes: A Deep Dive into MINIO, Hudi, and Delta Streamer",excerpt:"A Deep Dive into MINIO, Hudi, and Delta Streamer",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2023-11-30-Mastering-Data-Lakes-A-Deep-Dive-into-MINIO-Hudi-and-Delta-Streamer.png",tags:["apache hudi","mino","how-to","deltastreamer","linkedin"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.linkedin.com/pulse/mastering-data-lakes-deep-dive-minio-hudi-delta-streamer-soumil-shah-wxzsf/?utm_source=share&utm_medium=member_ios&utm_campaign=share_via",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},44541:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image8-20b16694604281c92df9a8a9079c22b5.png"},44615:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(42261),n=t(74848),s=t(28453),r=t(9230);const o={title:"Hoodie: Uber Engineering's Incremental Processing Framework on Hadoop",authors:[{name:"Prasanna Rajaperumal"},{name:"Vinoth Chandar"}],category:"blog",image:"/assets/images/blog/2017-03-12-Hoodie-Uber-Engineerings-Incremental-Processing-Framework-on-Hadoop.png",tags:["use-case","incremental processing","uber"]},l=void 0,d={authorsImageUrls:[void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://eng.uber.com/hoodie/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},44691:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(92916),n=t(74848),s=t(28453),r=t(9230);const o={title:"Spark ETL Chapter 8 with Lakehouse | Apache HUDI",authors:[{name:"Kalpan Shah"}],category:"blog",image:"/assets/images/blog/2023-03-23-Spark-ETL-Chapter-8-with-Lakehouse-Apache-HUDI.png",tags:["how-to","guide","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/plumbersofdatascience/spark-etl-chapter-8-with-lakehouse-apache-hudi-d4794b8a79e6",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},44692:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/03/22/data-lake-cost-optimisation-strategies","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-03-22-data-lake-cost-optimisation-strategies.mdx","source":"@site/blog/2024-03-22-data-lake-cost-optimisation-strategies.mdx","title":"Cost Optimization Strategies for scalable Data Lakehouse","description":"Redirecting... please wait!!","date":"2024-03-22T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"amazon s3","permalink":"/blog/tags/amazon-s-3"},{"inline":true,"label":"amazon emr","permalink":"/blog/tags/amazon-emr"},{"inline":true,"label":"apcache spark","permalink":"/blog/tags/apcache-spark"},{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"cost optimization","permalink":"/blog/tags/cost-optimization"},{"inline":true,"label":"halodoc","permalink":"/blog/tags/halodoc"}],"readingTime":0.1,"hasTruncateMarker":false,"authors":[{"name":"Suresh Hasundi","key":null,"page":null}],"frontMatter":{"title":"Cost Optimization Strategies for scalable Data Lakehouse","author":"Suresh Hasundi","category":"blog","image":"/assets/images/blog/2024-03-22-data-lake-cost-optimisation-strategies.png","tags":["blog","apache hudi","amazon s3","amazon emr","apcache spark","lakehouse","cost optimization","halodoc"]},"unlisted":false,"prevItem":{"title":"Options on Kafka sink to open table Formats: Apache Iceberg and Apache Hudi","permalink":"/blog/2024/03/23/options-on-kafka-sink-to-open-table-formats-apache-iceberg-and-apache-hudi"},"nextItem":{"title":"Open Table Formats (part-1): Apache Hudi (Hadoop Upserts Deletes and Incrementals)","permalink":"/blog/2024/03/16/Open-Table-Formats-part-1-Apache-Hudi-Hadoop-Upserts-Deletes-and-Incrementals"}}')},44854:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2020/10/06/cdc-solution-using-hudi-by-nclouds","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-10-06-cdc-solution-using-hudi-by-nclouds.md","source":"@site/blog/2020-10-06-cdc-solution-using-hudi-by-nclouds.md","title":"How nClouds Helps Accelerate Data Delivery with Apache Hudi on Amazon EMR","description":"This blog published by nClouds in partnership with AWS shows how to build a CDC pipeline using Apache Hudi on Amazon EMR and other managed services like Amazon RDS and AWS DMS, including Amazon QuickSight for data visualization.","date":"2020-10-06T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache flink","permalink":"/blog/tags/apache-flink"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":0.27,"hasTruncateMarker":false,"authors":[{"name":"nclouds","key":null,"page":null}],"frontMatter":{"title":"How nClouds Helps Accelerate Data Delivery with Apache Hudi on Amazon EMR","excerpt":"Solution to set up a new data and analytics platform using Apache Hudi on Amazon EMR and other managed services, including Amazon QuickSight for data visualization.","author":"nclouds","category":"blog","image":"/assets/images/blog/2020-10-06-cdc-solution-using-hudi-by-nclouds.jpg","tags":["blog","apache flink","apache hudi"]},"unlisted":false,"prevItem":{"title":"Apache Hudi meets Apache Flink","permalink":"/blog/2020/10/15/apache-hudi-meets-apache-flink"},"nextItem":{"title":"Ingest multiple tables using Hudi","permalink":"/blog/2020/08/22/ingest-multiple-tables-using-hudi"}}')},44952:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(50887),n=t(74848),s=t(28453),r=t(9230);const o={title:"Implementation of SCD-2 (Slowly Changing Dimension) with Apache Hudi & Spark",authors:[{name:"Jayasheel Kalgal"},{name:"Esha Dhing"},{name:"Prashant Mishra"}],category:"blog",image:"/assets/images/blog/2022-08-24_implementation_of_scd_2_with_hudi_and_spark.jpeg",tags:["use-case","scd2","walmartglobaltech"]},l=void 0,d={authorsImageUrls:[void 0,void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/walmartglobaltech/implementation-of-scd-2-slowly-changing-dimension-with-apache-hudi-465e0eb94a5",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},45134:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/09/13/Simplify-operational-data-processing-in-data-lakes-using-AWS-Glue-and-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-09-13-Simplify-operational-data-processing-in-data-lakes-using-AWS-Glue-and-Apache-Hudi.mdx","source":"@site/blog/2023-09-13-Simplify-operational-data-processing-in-data-lakes-using-AWS-Glue-and-Apache-Hudi.mdx","title":"Simplify operational data processing in data lakes using AWS Glue and Apache Hudi","description":"Redirecting... please wait!!","date":"2023-09-13T00:00:00.000Z","tags":[{"inline":true,"label":"aws glue","permalink":"/blog/tags/aws-glue"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"},{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"data processing","permalink":"/blog/tags/data-processing"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":0.16,"hasTruncateMarker":false,"authors":[{"name":"Srinivas Kandi","socials":{},"key":null,"page":null},{"name":"Ravi Itha","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Simplify operational data processing in data lakes using AWS Glue and Apache Hudi","excerpt":"Use AWS Glue and Apache Hudi for data processing","authors":[{"name":"Srinivas Kandi"},{"name":"Ravi Itha"}],"category":"blog","image":"/assets/images/blog/2023-09-13-Simplify-operational-data-processing-in-data-lakes-using-AWS-Glue-and-Apache-Hudi.png","tags":["aws glue","amazon","how-to","data processing","apache hudi"]},"unlisted":false,"prevItem":{"title":"Apache Hudi: From Zero To One (3/10)","permalink":"/blog/2023/09/15/Apache-Hudi-From-Zero-To-One-blog-3"},"nextItem":{"title":"Lakehouse or Warehouse? Part 2 of 2","permalink":"/blog/2023/09/12/Lakehouse-or-Warehouse-Part-2-of-2"}}')},45316:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi-design-diagrams_-_Page_8-5f886f1e198375aa996a989c03a707e9.png"},45456:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/05/12/Experts-primer-on-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-05-12-Experts-primer-on-Apache-Hudi.mdx","source":"@site/blog/2021-05-12-Experts-primer-on-Apache-Hudi.mdx","title":"Experts primer on Apache Hudi","description":"Redirecting... please wait!!","date":"2021-05-12T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"dbta","permalink":"/blog/tags/dbta"}],"readingTime":0.16,"hasTruncateMarker":false,"authors":[{"name":"Stephanie Simone","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Experts primer on Apache Hudi","authors":[{"name":"Stephanie Simone"}],"category":"blog","image":"/assets/images/blog/data-summit-connect.jpeg","tags":["blog","dbta"]},"unlisted":false,"prevItem":{"title":"Apache Hudi: How Uber gets data a ride to its destination","permalink":"/blog/2021/06/04/Apache-Hudi-How-Uber-gets-data-a-ride-to-its-destination"},"nextItem":{"title":"Build Slowly Changing Dimensions Type 2 (SCD2) with Apache Spark and Apache Hudi on Amazon EMR","permalink":"/blog/2021/04/12/Build-Slowly-Changing-Dimensions-Type-2-SCD2-with-Apache-Spark-and-Apache-Hudi-on-Amazon-EMR"}}')},45536:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image6-c6d05709a20d20f096539322b86f933d.png"},45672:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(93717),n=t(74848),s=t(28453);const r={title:"Column File Formats: How Hudi Leverages Parquet and ORC ",excerpt:"Explains how Hudi uses Parquet and ORC",author:"Albert Wong",category:"blog",image:"/assets/images/blog/hudi-parquet-orc.png",tags:["Data Lake","Apache Hudi","Apache Parquet","Apache ORC"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Introduction",id:"introduction",level:2},{value:"How does data storage work in Apache Hudi",id:"how-does-data-storage-work-in-apache-hudi",level:2},{value:"Parquet vs ORC for your Apache Hudi Base File",id:"parquet-vs-orc-for-your-apache-hudi-base-file",level:2},{value:"Apache Parquet",id:"apache-parquet",level:3},{value:"Optimized Row Columnar (ORC)",id:"optimized-row-columnar-orc",level:3},{value:"Choosing the Right Format:",id:"choosing-the-right-format",level:2},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const a={a:"a",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.h2,{id:"introduction",children:"Introduction"}),"\n",(0,n.jsxs)(a.p,{children:["Apache Hudi emerges as a game-changer in the big data ecosystem by transforming data lakes into transactional hubs. Unlike traditional data lakes which struggle with updates and deletes, Hudi empowers users with functionalities like data ingestion, streaming updates (upserts), and even deletions. This allows for efficient incremental processing, keeping your data pipelines agile and data fresh for real-time analytics. Hudi seamlessly integrates with existing storage solutions and boasts compatibility with popular columnar file formats like ",(0,n.jsx)(a.a,{href:"https://parquet.apache.org/",children:"Parquet"})," and ",(0,n.jsx)(a.a,{href:"https://orc.apache.org/",children:"ORC"}),". Choosing the right file format is crucial for optimized performance and efficient data manipulation within Hudi, as it directly impacts processing speed and storage efficiency. This blog will delve deeper into these features, and explore the significance of file format selection."]}),"\n",(0,n.jsx)(a.h2,{id:"how-does-data-storage-work-in-apache-hudi",children:"How does data storage work in Apache Hudi"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{src:"https://miro.medium.com/v2/resize:fit:600/format:webp/0*_NFdQLaRGiqDuK3V.png",alt:"Hudi COW MOR"})}),"\n",(0,n.jsx)(a.p,{children:"Apache Hudi offers two table storage options: Copy-on-Write (COW) and Merge-on-Read (MOR)."}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/table_types#copy-on-write-table",children:"COW tables"}),":","\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Data is stored in base files, with Parquet and ORC being the supported formats."}),"\n",(0,n.jsx)(a.li,{children:"Updates involve rewriting the entire base file with the modified data."}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/table_types#merge-on-read-table",children:"MOR tables"}),":","\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Data resides in base files, again supporting Parquet and ORC formats."}),"\n",(0,n.jsx)(a.li,{children:"Updates are stored in separate delta files (using Apache Avro format) and later merged with the base file by a periodic compaction process in the background."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"parquet-vs-orc-for-your-apache-hudi-base-file",children:"Parquet vs ORC for your Apache Hudi Base File"}),"\n",(0,n.jsx)(a.p,{children:"Choosing the right file format for your Hudi environment depends on your specific needs. Here's a breakdown of Parquet, and ORC along with their strengths, weaknesses, and ideal use cases within Hudi:"}),"\n",(0,n.jsx)(a.h3,{id:"apache-parquet",children:"Apache Parquet"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.a,{href:"https://parquet.apache.org/",children:"Apache Parquet"})," is a columnar storage file format. It\u2019s designed for efficiency and performance, and it\u2019s particularly well-suited for running complex queries on large datasets."]}),"\n",(0,n.jsx)(a.p,{children:"Pros of Parquet:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Columnar Storage: Unlike row-based files, Parquet is columnar-oriented. This means it stores data by columns, which allows for more efficient disk I/O and compression. It reduces the amount of data transferred from disk to memory, leading to faster query performance."}),"\n",(0,n.jsx)(a.li,{children:"Compression: Parquet has good compression and encoding schemes. It reduces the disk storage space and improves performance, especially for columnar data retrieval, which is a common case in data analytics."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Cons of Parquet:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Write-heavy Workloads: Since Parquet performs column-wise compression and encoding, the cost of writing data can be high for write-heavy workloads."}),"\n",(0,n.jsx)(a.li,{children:"Small Data Sets: Parquet may not be the best choice for small datasets because the advantages of its columnar storage model aren\u2019t as pronounced."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Use Cases for Parquet:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["Parquet is an excellent choice when dealing with large, complex, and nested data structures, especially for read-heavy workloads. Its columnar storage approach makes it an excellent choice for ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2024/07/11/what-is-a-data-lakehouse/",children:"data lakehouse"})," solutions where aggregation queries are common."]}),"\n"]}),"\n",(0,n.jsx)(a.h3,{id:"optimized-row-columnar-orc",children:"Optimized Row Columnar (ORC)"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.a,{href:"https://orc.apache.org/",children:"Apache ORC"})," is another popular file format that is self-describing, and type-aware columnar file format."]}),"\n",(0,n.jsx)(a.p,{children:"Pros of ORC:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Compression: ORC provides impressive compression rates that minimize storage space. It also includes lightweight indexes stored within the file, helping to improve read performance."}),"\n",(0,n.jsx)(a.li,{children:"Complex Types: ORC supports complex types, including structs, lists, maps, and union types."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Cons of ORC:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Less Community Support: Compared to Parquet, ORC has less community support, meaning fewer resources, libraries, and tools for this file format."}),"\n",(0,n.jsx)(a.li,{children:"Write Costs: Similar to Parquet, ORC may have high write costs due to its columnar nature."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Use Cases for ORC:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"ORC is commonly used in cases where high-speed writing is necessary."}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"choosing-the-right-format",children:"Choosing the Right Format:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Prioritize query performance: If complex analytical queries are your primary use case, Parquet is the clear winner due to its superior columnar access."}),"\n",(0,n.jsx)(a.li,{children:"Balance performance and cost: ORC offers a good balance between read/write performance and compression, making it suitable for general-purpose data storage in Hudi."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Remember, the best format depends on your specific Hudi application. Consider your workload mix, and performance requirements to make an informed decision."}),"\n",(0,n.jsx)(a.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,n.jsx)(a.p,{children:"In conclusion, understanding file formats is crucial for optimizing your Hudi data management. Parquet for COW and MOR tables excels in analytical queries with its columnar storage and rich metadata. ORC for COW and MOR tables strikes a balance between read/write performance and compression for general-purpose storage. Avro comes into play for storing delta table data in MOR tables. By considering these strengths, you can make informed decisions on file formats to best suit your big data workloads within the Hudi framework."}),"\n",(0,n.jsxs)(a.p,{children:["Unleash the power of Apache Hudi for your big data challenges! Head over to ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/",children:"https://hudi.apache.org/"})," and dive into the quickstarts to get started. Want to learn more? Join our vibrant Hudi community! Attend the monthly Community Call or hop into the Apache Hudi Slack to ask questions and gain deeper insights."]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},45731:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(28292),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi on AWS Glue: A Step-by-Step Guide",authors:[{name:"Dev Jain"}],category:"blog",image:"/assets/images/blog/2023-08-03-Apache-Hudi-on-AWS-Glue-A-Step-by-Step-Guide.png",tags:["how-to","aws-glue","apache-hudi","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@devjain1299/apache-hudi-on-aws-glue-a-step-by-step-guide-503c34a9aa95",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},45960:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/backwards-compat-writing-6299b055646e2577964069b755ee1f3d.png"},46017:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(48037),n=t(74848),s=t(28453),r=t(9230);const o={title:"AWS Glue Crawlers now supports Apache Hudi Tables",authors:[{name:"AWS Team"}],category:"blog",image:"/assets/images/blog/2023-07-21-AWS-Glue-Crawlers-now-supports-Apache-Hudi-Tables.png",tags:["blog","aws glue","hudi","glue crawler"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/about-aws/whats-new/2023/07/aws-glue-crawlers-apache-hudi-tables/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},46171:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(75801),n=t(74848),s=t(28453);const r={title:"Build Open Lakehouse using Apache Hudi & dbt",excerpt:"How to style blog focused projects on teaching how to build an open Lakehouse using Apache Hudi & dbt",author:"Vinoth Govindarajan",category:"blog",image:"/assets/images/blog/hudi_dbt_lakehouse.png",tags:["how-to","deltastreamer","incremental processing","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"What is Apache Hudi?",id:"what-is-apache-hudi",level:2},{value:"What is dbt?",id:"what-is-dbt",level:2},{value:"What is a Lakehouse?",id:"what-is-a-lakehouse",level:2},{value:"How to build an open lakehouse?",id:"how-to-build-an-open-lakehouse",level:2},{value:"Step 1: How to extract &amp; load the raw data datasets?",id:"step-1-how-to-extract--load-the-raw-data-datasets",level:2},{value:"Step 2: How to configure hudi with the dbt project?",id:"step-2-how-to-configure-hudi-with-the-dbt-project",level:2},{value:"Step 3: How to read the raw data incrementally?",id:"step-3-how-to-read-the-raw-data-incrementally",level:2},{value:"How to apply filters on an incremental run?",id:"how-to-apply-filters-on-an-incremental-run",level:3},{value:"How to define the uniqueness constraint?",id:"how-to-define-the-uniqueness-constraint",level:3},{value:"Step 4: How to use the upsert feature while writing datasets?",id:"step-4-how-to-use-the-upsert-feature-while-writing-datasets",level:2},{value:"How to perform field-level updates?",id:"how-to-perform-field-level-updates",level:3},{value:"How to configure additional hoodie custom configs?",id:"how-to-configure-additional-hoodie-custom-configs",level:3}];function c(e){const a={a:"a",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(a.p,{children:["The focus of this blog is to show you how to build an open lakehouse leveraging incremental data processing and performing field-level updates. We are excited to announce that you can now use Apache Hudi + dbt for building open ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2024/07/11/what-is-a-data-lakehouse/",children:"data lakehouses"}),"."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"/assets/images/blog/hudi_dbt_lakehouse.png",src:t(86738).A+"",width:"1200",height:"600"})}),"\n",(0,n.jsx)(a.p,{children:"Let's first clarify a few terminologies used in this blog before we dive into the details."}),"\n",(0,n.jsx)(a.h2,{id:"what-is-apache-hudi",children:"What is Apache Hudi?"}),"\n",(0,n.jsxs)(a.p,{children:["Apache Hudi brings ACID transactions, record-level updates/deletes, and change streams to ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2024/07/11/what-is-a-data-lakehouse/",children:"data lakehouses"}),"."]}),"\n",(0,n.jsx)(a.p,{children:"Apache Hudi is an open-source data management framework used to simplify incremental data processing and data pipeline development. This framework more efficiently manages business requirements like data lifecycle and improves data quality."}),"\n",(0,n.jsx)(a.h2,{id:"what-is-dbt",children:"What is dbt?"}),"\n",(0,n.jsx)(a.p,{children:"dbt (data build tool) is a data transformation tool that enables data analysts and engineers to transform, test, and document data in the cloud data warehouses."}),"\n",(0,n.jsx)(a.p,{children:"dbt enables analytics engineers to transform data in their warehouses by simply writing select statements. dbt handles turning these select statements into tables and views."}),"\n",(0,n.jsx)(a.p,{children:"dbt does the T in ELT (Extract, Load, Transform) processes \u2013 it doesn\u2019t extract or load data, but it\u2019s extremely good at transforming data that\u2019s already loaded into your warehouse."}),"\n",(0,n.jsx)(a.h2,{id:"what-is-a-lakehouse",children:"What is a Lakehouse?"}),"\n",(0,n.jsx)(a.p,{children:"A lakehouse is a new, open architecture that combines the best elements of data lakes and data warehouses. Lakehouses are enabled by a new system design: implementing transaction management and data management features similar to those in a data warehouse directly on top of low-cost cloud storage in open formats. They are what you would get if you had to redesign data warehouses in the modern world, now that cheap and highly reliable storage (in the form of object stores) are available."}),"\n",(0,n.jsx)(a.p,{children:"In other words, while data lakes historically have been viewed as a bunch of files added to cloud storage folders, lakehouse tables support transactions, updates, deletes, and in the case of Apache Hudi, even database-like functionality like indexing or change capture."}),"\n",(0,n.jsx)(a.h2,{id:"how-to-build-an-open-lakehouse",children:"How to build an open lakehouse?"}),"\n",(0,n.jsx)(a.p,{children:"Now, we know what is a lakehouse, so let's build one, In order to build an open lakehouse, you need a few components:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["Open table format which supports ACID transactions","\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Apache Hudi (integrated with dbt)"}),"\n",(0,n.jsx)(a.li,{children:"Delta Lake (proprietary features locked to Databricks runtime)"}),"\n",(0,n.jsx)(a.li,{children:"Apache Iceberg (currently not integrated with dbt)"}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["Data transformation tool","\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Open source dbt is the de-facto popular choice for transformation layer"}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["Distributed data processing engine","\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Apache Spark is the de-facto popular choice for compute engine"}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["Cloud Storage","\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"You can choose any of the cost-effective cloud stores or HDFS"}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.li,{children:"Bring your favorite query engine"}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"To build the lakehouse you need a way to extract and load the data into Hudi table format and then transform in-place using dbt."}),"\n",(0,n.jsxs)(a.p,{children:["DBT supports Hudi out of the box with the ",(0,n.jsx)(a.a,{href:"https://github.com/dbt-labs/dbt-spark",children:"dbt-spark"})," adapter package. When creating modeled datasets using dbt you can choose Hudi as the format for your tables."]}),"\n",(0,n.jsxs)(a.p,{children:["You can follow the instructions on this ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/hudi-examples/hudi-examples-dbt/README.md",children:"page"})," to learn how to install and configure dbt+hudi."]}),"\n",(0,n.jsx)(a.h2,{id:"step-1-how-to-extract--load-the-raw-data-datasets",children:"Step 1: How to extract & load the raw data datasets?"}),"\n",(0,n.jsx)(a.p,{children:"This is the first step in building your data lake and there are many choices here to load the data into our open lakehouse. I\u2019m going to go with one of the Hudi\u2019s native tools called Delta Streamer since all the ingestion features are pre-built and battle-tested in production at scale."}),"\n",(0,n.jsxs)(a.p,{children:["Hudi\u2019s ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/hoodie_streaming_ingestion",children:"DeltaStreamer"})," does the EL in ELT (Extract, Load, Transform) processes \u2013 it\u2019s extremely good at extracting, loading, and optionally ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/hoodie_streaming_ingestion#transformers",children:"transforming data"})," that\u2019s already loaded into your lakehouse."]}),"\n",(0,n.jsx)(a.h2,{id:"step-2-how-to-configure-hudi-with-the-dbt-project",children:"Step 2: How to configure hudi with the dbt project?"}),"\n",(0,n.jsx)(a.p,{children:"To use the Hudi with your dbt project,  all you need to do is choose the file format as Hudi. The file format config can either be specified in specific models, or for all the models in your dbt_project.yml file:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-yml",metastring:'title="dbt_project.yml"',children:"models:\n   +file_format: hudi\n"})}),"\n",(0,n.jsx)(a.p,{children:"or:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",metastring:'title="model/my_model.sql"',children:"{{ config(\n   materialized = 'incremental',\n   incremental_strategy = 'merge',\n   file_format = 'hudi',\n   unique_key = 'id',\n   \u2026\n) }}\n"})}),"\n",(0,n.jsx)(a.p,{children:"After choosing hudi as the file_format you can create materialized datasets using dbt, which offers additional benefits that are unique to the Hudi table format such as field-level upserts/deletes."}),"\n",(0,n.jsx)(a.h2,{id:"step-3-how-to-read-the-raw-data-incrementally",children:"Step 3: How to read the raw data incrementally?"}),"\n",(0,n.jsx)(a.p,{children:"Before we learn how to build incremental materialization, let\u2019s quickly learn, What are materializations in dbt? Materializations are strategies for persisting dbt models in a lakehouse. There are four types of materializations built into dbt. They are:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"table"}),"\n",(0,n.jsx)(a.li,{children:"view"}),"\n",(0,n.jsx)(a.li,{children:"incremental"}),"\n",(0,n.jsx)(a.li,{children:"ephemeral"}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Among all the materialization types, only incremental models allow dbt to insert or update records into a table since the last time that dbt was run, which unlocks the powers of Hudi, we will dive into the details."}),"\n",(0,n.jsx)(a.p,{children:"To use incremental models, you need to perform these two activities:"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsx)(a.li,{children:"Tell dbt how to filter the rows on the incremental executions"}),"\n",(0,n.jsx)(a.li,{children:"Define the uniqueness constraint of the model (required when using >= Hudi 0.10.1 version)"}),"\n"]}),"\n",(0,n.jsx)(a.h3,{id:"how-to-apply-filters-on-an-incremental-run",children:"How to apply filters on an incremental run?"}),"\n",(0,n.jsxs)(a.p,{children:["dbt provides you a macro ",(0,n.jsx)(a.code,{children:"is_incremental()"})," which is very useful to define the filters exclusively for incremental materializations."]}),"\n",(0,n.jsxs)(a.p,{children:['Often, you\'ll want to filter for "new" rows, as in, rows that have been created since the last time dbt ran this model. The best way to find the timestamp of the most recent run of this model is by checking the most recent timestamp in your target table. dbt makes it easy to query your target table by using the "',(0,n.jsxs)(a.a,{href:"https://docs.getdbt.com/reference/dbt-jinja-functions/this",children:["{",this,"}"]}),'" variable.']}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",metastring:'title="models/my_model.sql"',children:"{{\n   config(\n       materialized='incremental',\n       file_format='hudi',\n   )\n}}\n\nselect\n   *\nfrom raw_app_data.events\n{% if is_incremental() %}\n   -- this filter will only be applied on an incremental run\n   where event_time > (select max(event_time) from {{ this }})\n{% endif %}\n"})}),"\n",(0,n.jsx)(a.h3,{id:"how-to-define-the-uniqueness-constraint",children:"How to define the uniqueness constraint?"}),"\n",(0,n.jsx)(a.p,{children:"A unique_key is the primary key of the dataset, which determines whether a record has new values and should be updated/deleted, or inserted."}),"\n",(0,n.jsx)(a.p,{children:"You can define the unique_key in the configuration block at the top of your model. This unique_key will act as the primaryKey (hoodie.datasource.write.recordkey.field) on the hudi table."}),"\n",(0,n.jsx)(a.h2,{id:"step-4-how-to-use-the-upsert-feature-while-writing-datasets",children:"Step 4: How to use the upsert feature while writing datasets?"}),"\n",(0,n.jsx)(a.p,{children:"dbt offers multiple load strategies when loading the transformed datasets, such as:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"append (default)"}),"\n",(0,n.jsx)(a.li,{children:"insert_overwrite (optional)"}),"\n",(0,n.jsx)(a.li,{children:"merge (optional, Only available for Hudi and Delta formats)"}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"By default dbt uses the append strategy, which may cause duplicate rows when you execute dbt run command multiple times on the same payload."}),"\n",(0,n.jsx)(a.p,{children:"When you choose the insert_overwrite strategy, dbt will overwrite the entire partition or full table load for every dbt run, which causes unnecessary overheads and is very expensive."}),"\n",(0,n.jsx)(a.p,{children:"In addition to all the existing strategies to load the data, with hudi you can use the exclusive merge strategy when using incremental materialization. Using the merge strategy you can perform field-level updates/deletes on your data lakehouse which is performant and cost-efficient. As a result, you will get access to fresher data and accelerated insights."}),"\n",(0,n.jsx)(a.h3,{id:"how-to-perform-field-level-updates",children:"How to perform field-level updates?"}),"\n",(0,n.jsx)(a.p,{children:"If you are using the merge strategy and have specified a unique_key, by default, dbt will entirely overwrite matched rows with new values."}),"\n",(0,n.jsxs)(a.p,{children:["Since Apache Spark adapter supports the merge strategy, you may optionally pass a list of column names to a ",(0,n.jsx)(a.code,{children:"merge_update_columns"})," config. In that case, dbt will update only the columns specified by the config, and keep the previous values of other columns."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",metastring:'title="models/my_model.sql"',children:"{{ config(\n   materialized = 'incremental',\n   incremental_strategy = 'merge',\n   file_format = 'hudi',\n   unique_key = 'id',\n   merge_update_columns = ['msg', 'updated_ts'],\n) }}\n"})}),"\n",(0,n.jsx)(a.h3,{id:"how-to-configure-additional-hoodie-custom-configs",children:"How to configure additional hoodie custom configs?"}),"\n",(0,n.jsx)(a.p,{children:"When you want to specify additional hudi configs, you can do that with the options config:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",metastring:'title="models/my_model.sql"',children:"{{ config(\n   materialized='incremental',\n   file_format='hudi',\n   incremental_strategy='merge',\n   options={\n       'type': 'mor',\n       'primaryKey': 'id',\n       'precombineKey': 'ts',\n   },\n   unique_key='id',\n   partition_by='datestr',\n   pre_hook=[\"set spark.sql.datetime.java8API.enabled=false;\"],\n  )\n}}\n"})}),"\n",(0,n.jsx)(a.p,{children:"Hope you understood the benefits of using Apache Hudi with dbt to build your next open lakehouse, good luck!"})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},46247:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide8-1d407f163ced76b9b0a6a1c3a45ce6d6.png"},46265:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(57012),n=t(74848),s=t(28453),r=t(9230);const o={title:"Monitoring Table Size stats",authors:[{name:"Sivabalan Narayanan"}],category:"blog",image:"/assets/images/blog/2023-07-01-monitoring-table-size-stats.png",tags:["blog","table size stats","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@simpsons/monitoring-table-stats-22684eb70ee1",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},46294:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(64933),n=t(74848),s=t(28453),r=t(9230);const o={title:"Hudi Metafields demystified",authors:[{name:"Bhavani Sudha Saktheeswaran"}],category:"blog",image:"/assets/images/blog/2023-05-19-Hudi-Metafields-demystified.png",tags:["design","metadata","metafields","onehouse"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.onehouse.ai/blog/hudi-metafields-demystified",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},46380:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/12/19/Build-Your-First-Hudi-Lakehouse-with-AWS-Glue-and-AWS-S3","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-12-19-Build-Your-First-Hudi-Lakehouse-with-AWS-Glue-and-AWS-S3.md","source":"@site/blog/2022-12-19-Build-Your-First-Hudi-Lakehouse-with-AWS-Glue-and-AWS-S3.md","title":"Build Your First Hudi Lakehouse with AWS S3 and AWS Glue","description":"/assets/images/blog/DataCouncil.jpg","date":"2022-12-19T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"aws s3","permalink":"/blog/tags/aws-s-3"},{"inline":true,"label":"aws glue","permalink":"/blog/tags/aws-glue"}],"readingTime":1.59,"hasTruncateMarker":false,"authors":[{"name":"Nadine Farah","key":null,"page":null}],"frontMatter":{"title":"Build Your First Hudi Lakehouse with AWS S3 and AWS Glue","excerpt":"Follow this tutorial on building your first hudi lakehouse with AWS S3 & AWS Glue","author":"Nadine Farah","category":"blog","image":"/assets/images/blog/DataCouncil.jpg","tags":["how-to","use-case","apache hudi","aws s3","aws glue"]},"unlisted":false,"prevItem":{"title":"Apache Hudi 2022 - A year in Review","permalink":"/blog/2022/12/29/Apache-Hudi-2022-A-Year-In-Review"},"nextItem":{"title":"Run Apache Hudi at scale on AWS","permalink":"/blog/2022/12/01/Run-apache-hudi-at-scale-on-aws"}}')},46486:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(50495),n=t(74848),s=t(28453),r=t(9230);const o={title:"Building an Open Source Data Lake House with Hudi, Postgres Hive Metastore, Minio, and StarRocks",excerpt:"Building an Open Source Data Lake House with Hudi, Postgres Hive Metastore, Minio, and StarRocks",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2024-02-06-Building-an-Open-Source-Data-Lake-House-with-Hudi-Postgres-Hive-Metastore-Minio-and-StarRocks.png",tags:["blog","apache hudi","linkedin","beginner","apache spark","apache hive","hive metastore","minio","starrocks","docker","python","postgres","postgresql"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.linkedin.com/pulse/building-open-source-data-lake-house-hudi-postgres-hive-soumil-shah-wwyye/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},46601:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/03/31/amazon-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-03-31-amazon-hudi.md","source":"@site/blog/2025-03-31-amazon-hudi.md","title":"Powering Amazon Unit Economics at Scale Using Apache Hudi","description":"Amazon\u2019s Profit Intelligence team built Nexus, a configuration-driven platform powered by Apache Hudi, to scale unit economics across thousands of retail use cases. Nexus manages over 1,200 tables, processes hundreds of billions of rows daily, and handles ~1 petabyte of data churn each month. This blog dives into their data lakehouse journey, Nexus architecture, Hudi integration, and key operational learnings.","date":"2025-03-31T00:00:00.000Z","tags":[{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"Amazon","permalink":"/blog/tags/amazon"},{"inline":true,"label":"Community","permalink":"/blog/tags/community"}],"readingTime":8.88,"hasTruncateMarker":false,"authors":[{"name":"Jason, Abhishek, Sethu in collaboration with Dipankar","key":null,"page":null}],"frontMatter":{"title":"Powering Amazon Unit Economics at Scale Using Apache Hudi","excerpt":"How Amazon\'s Profit Intelligence team uses Apache Hudi to power hundreds of pipelines","author":"Jason, Abhishek, Sethu in collaboration with Dipankar","category":"blog","image":"/assets/images/blog/amz-1200x600.jpg","tags":["Apache Hudi","Amazon","Community"]},"unlisted":false,"prevItem":{"title":"Introducing Secondary Index in Apache Hudi Lakehouse Platform","permalink":"/blog/2025/04/02/secondary-index"},"nextItem":{"title":"ACID Transactions in an Open Data Lakehouse","permalink":"/blog/2025/03/26/acid-transactions"}}')},46604:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig3-3a9b031ca307c28c17434af09a0ee7bc.png"},46871:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(94852),n=t(74848),s=t(28453),r=t(9230);const o={title:"Data Lake / Lakehouse Guide: Powered by Data Lake Table Formats (Delta Lake, Iceberg, Hudi)",authors:[{name:"Simon Sp\xe4ti"}],category:"blog",image:"/assets/images/blog/2022-08-25-Data-Lake-Lakehouse-Guide-Powered-by-Data-Lake-Table-Formats-Delta-Lake-Iceberg-Hudi.png",tags:["blog","datalake","lakehouse","comparison","airbyte"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},47058:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(41546),n=t(74848),s=t(28453),r=t(9230);const o={title:"How to Query Apache Hudi Tables with Python Using Daft: A Spark-Free Approach",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2024-05-02-how-query-apache-hudi-tables-python-using-daft-spark-free.png",tags:["blog","apache hudi","python","daft","linkedin"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.linkedin.com/pulse/how-query-apache-hudi-tables-python-using-daft-spark-free-soumil-shah-hpdwf/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},47153:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/10/06/Ingest-streaming-data-to-Apache-Hudi-using-AWS-Glue-and-DeltaStreamer","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-10-06-Ingest-streaming-data-to-Apache-Hudi-using-AWS-Glue-and-DeltaStreamer.mdx","source":"@site/blog/2022-10-06-Ingest-streaming-data-to-Apache-Hudi-using-AWS-Glue-and-DeltaStreamer.mdx","title":"Ingest streaming data to Apache Hudi tables using AWS Glue and Apache Hudi DeltaStreamer","description":"Redirecting... please wait!!","date":"2022-10-06T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"streaming ingestion","permalink":"/blog/tags/streaming-ingestion"},{"inline":true,"label":"deltastreamer","permalink":"/blog/tags/deltastreamer"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.16,"hasTruncateMarker":false,"authors":[{"name":"Vishal Pathak","socials":{},"key":null,"page":null},{"name":"Anand Prakash","socials":{},"key":null,"page":null},{"name":"Noritaka Sekiyama","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Ingest streaming data to Apache Hudi tables using AWS Glue and Apache Hudi DeltaStreamer","authors":[{"name":"Vishal Pathak"},{"name":"Anand Prakash"},{"name":"Noritaka Sekiyama"}],"category":"blog","image":"/assets/images/blog/2022-10-06_Ingest_streaming_data_to_Apache_Hudi_tables_using_AWS_Glue_and_DeltaStreamer.png","tags":["how-to","streaming ingestion","deltastreamer","amazon"]},"unlisted":false,"prevItem":{"title":"What, Why and How : Apache Hudi\u2019s Bloom Index","permalink":"/blog/2022/10/08/what-why-and-how-apache-hudis-bloom-index"},"nextItem":{"title":"Data processing with Spark: time traveling","permalink":"/blog/2022/09/28/Data-processing-with-Spark-time-traveling"}}')},47340:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(380),n=t(74848),s=t(28453);const r={title:"Next Generation Lakehouse: New Engine for the Intelligent Future | Apache Hudi Meetup Asia Recap",excerpt:"A comprehensive recap of the Apache Hudi Meetup Asia held at JD.com headquarters, featuring insights from Onehouse, JD.com, Kuaishou, and Huawei on Hudi 1.1, AI-native architectures, and production optimizations.",author:"Team at JD.com",category:"blog",image:"/assets/images/blog/2025-12-01-apache-hudi-JD-meetup-asia-2025-recap/jdpost-image7.jpg",tags:["hudi","meetup","lakehouse","community"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Hudi Community Leader Joined Remotely",id:"hudi-community-leader-joined-remotely",level:2},{value:"JD Retail: Data Lake Technical Challenges and Outlook",id:"jd-retail-data-lake-technical-challenges-and-outlook",level:2},{value:"Apache Hudi 1.1 Preview and AI-Native Lakehouse Evolution",id:"apache-hudi-11-preview-and-ai-native-lakehouse-evolution",level:2},{value:"Latest Architecture Evolution of Apache Hudi at JD.com",id:"latest-architecture-evolution-of-apache-hudi-at-jdcom",level:2},{value:"How Kuaishou&#39;s Real-time Lake Ingestion Empowered BI &amp; AI Scenario Architecture Upgrade",id:"how-kuaishous-real-time-lake-ingestion-empowered-bi--ai-scenario-architecture-upgrade",level:2},{value:"Deep Optimization and AI Exploration of Apache Hudi on Huawei Cloud",id:"deep-optimization-and-ai-exploration-of-apache-hudi-on-huawei-cloud",level:2},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const a={a:"a",em:"em",h2:"h2",hr:"hr",img:"img",p:"p",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.hr,{}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsxs)(a.em,{children:["This blog was translated from the ",(0,n.jsx)(a.a,{href:"https://mp.weixin.qq.com/s/LNMZGl-kXJTblOCO6s0BxQ",children:"original blog in Chinese"}),"."]})}),"\n",(0,n.jsx)(a.hr,{}),"\n",(0,n.jsx)(a.p,{children:"Recently, the Apache Hudi Meetup Asia, hosted by JD.com, was successfully held at JD.com Group headquarters. Four technical experts from Onehouse, JD.com, Kuaishou, and Huawei gathered together, not only bringing a preview of Apache Hudi release 1.1, but also sharing their unique approaches to building data lakehouses. From AI scenario support to real-time data processing and cost optimization, each topic directly addressed the pain points that data engineers care about most."}),"\n",(0,n.jsx)(a.h2,{id:"hudi-community-leader-joined-remotely",children:"Hudi Community Leader Joined Remotely"}),"\n",(0,n.jsx)(a.p,{children:"First, Vinoth Chandar, CEO & Founder of Onehouse and Apache Hudi PMC Chair, delivered the opening remarks via video. He stated that after eight years of development, Hudi has become an important cornerstone in the data lake domain, and its vision has transformed into widely recognized achievements in the industry. The 1.0 version released last year marked the project's entry into a mature stage, bringing many database-like capabilities to the lakehouse."}),"\n",(0,n.jsx)(a.p,{children:"Currently, the community is steadily advancing the 1.x series of versions, focusing on improving Flink performance, launching a new Trino connector, and enhancing interoperability through a pluggable table format layer. Facing the rapid development in the data lake field, Vinoth emphasized that excellent technology and robust design are the keys to long-term success. Hudi has now achieved many capabilities that commercial engines have not been able to deliver, thanks to its intelligent and creative community. Looking ahead, the community will be committed to building Hudi into a storage engine that supports all scenarios from BI to AI, exploring trending areas including unstructured data management and vector search."}),"\n",(0,n.jsx)(a.p,{children:"Vinoth specially thanked JD.com for its significant contributions to Apache Hudi. Among the top 100 contributors, 6 were from JD.com. Finally, he also invited more developers to join this vibrant community to jointly promote innovation and development in data infrastructure."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"image 1",src:t(3319).A+"",width:"1075",height:"576"})}),"\n",(0,n.jsx)(a.h2,{id:"jd-retail-data-lake-technical-challenges-and-outlook",children:"JD Retail: Data Lake Technical Challenges and Outlook"}),"\n",(0,n.jsx)(a.p,{children:"As the co-host of the event, Zhang Ke, Head of AI Infra & Big Data Computing at JD Retail, welcomed guests and attendees who participated in this Meetup. He also pointed out two core challenges facing the data domain:"}),"\n",(0,n.jsx)(a.p,{children:'At the BI level, the long-standing problem of "unified stream and batch processing" has not yet been perfectly solved, forcing data R&D personnel to duplicate work across multiple systems. This requires fundamentally reconstructing the data architecture and finding a new paradigm for unified stream and batch processing.'}),"\n",(0,n.jsx)(a.p,{children:"At the AI level, with the arrival of the multimodal era, traditional solutions that only handle structured data can no longer meet the needs. Whether it is data supply efficiency for model training, real-time feature computation for recommendation systems, or knowledge base construction required for large models, there is an urgent need for an underlying support system that can unify storage of multimodal data while balancing cost and performance."}),"\n",(0,n.jsx)(a.p,{children:"The industry is looking forward to building a storage foundation through open-source technologies like Apache Hudi that can uniformly carry batch processing, stream computing, data analysis, and AI workloads."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"image 2",src:t(19640).A+"",width:"4032",height:"3024"})}),"\n",(0,n.jsx)(a.h2,{id:"apache-hudi-11-preview-and-ai-native-lakehouse-evolution",children:"Apache Hudi 1.1 Preview and AI-Native Lakehouse Evolution"}),"\n",(0,n.jsx)(a.p,{children:'In the session "Apache Hudi 1.1 Preview and AI-Native Lakehouse Evolution," Ethan Guo (Yihua Guo), Data Architecture Engineer at Onehouse and Apache Hudi PMC member, shared Hudi\'s technical evolution path and future outlook. As the top contributor to the Hudi codebase, he systematically elaborated on the project positioning, version planning, and AI-native architecture.'}),"\n",(0,n.jsx)(a.p,{children:'Ethan pointed out that Apache Hudi\'s positioning goes far beyond being an open table format\u2014it is an embedded, headless, distributed database system built on top of cloud storage. Hudi is moving from "a transactional database on the lakehouse" toward "an AI-native Lakehouse platform."'}),"\n",(0,n.jsx)(a.p,{children:'In the then-upcoming 1.1 release (now released), Hudi has achieved several important breakthroughs. Among them, the pluggable table format architecture effectively solves the pain point of format fragmentation in the current data lake ecosystem, enabling users to "write once, read in multiple formats." At the same time, Hudi has deeply optimized Flink integration, solving the throughput bottleneck in streaming writes through an asynchronous generation mechanism, and building a brand-new native writer that achieves end-to-end processing from Avro format to Flink RowData, significantly reducing serialization overhead and GC pressure. Real-world tests showed that Hudi 1.1\'s throughput performance in streaming lake ingestion scenarios was 3.5 times that of version 1.0.'}),"\n",(0,n.jsx)(a.p,{children:"Facing new challenges brought by the AI era, Hudi is actively building a native AI data foundation. By supporting unstructured data storage, optimizing column group structures for multimodal data, providing built-in vector indexing capabilities, and building a unified storage layer that supports transactions and version control, Hudi is committed to providing highly real-time, traceable, and easily extensible data support for AI workflows. This series of evolutions will propel Apache Hudi from an excellent data lake framework to a core data infrastructure supporting the AI era."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"image 3",src:t(10549).A+"",width:"4032",height:"3024"})}),"\n",(0,n.jsx)(a.h2,{id:"latest-architecture-evolution-of-apache-hudi-at-jdcom",children:"Latest Architecture Evolution of Apache Hudi at JD.com"}),"\n",(0,n.jsx)(a.p,{children:'In the session "Latest Architecture Evolution of Apache Hudi at JD.com," Han Fei, Head of JD Real-time Data Platform, systematically introduced the latest architectural evolution and implementation results of Hudi in JD\'s production environment.'}),"\n",(0,n.jsx)(a.p,{children:'Addressing the performance bottleneck of native MOR tables in high-throughput scenarios, JD\'s Data Lake team reconstructed the data organization protocol of Hudi MOR tables based on LSM-Tree architecture. By replacing the original "Avro + Append" update mode with "Parquet + Create" mode, lock-free concurrent write capability was achieved. Combined with a series of optimization methods such as Engine-Native data format, Remote Partitioner strategy, and streaming incremental Compaction scheduling mechanism, read and write performance were significantly improved. Benchmark test results showed that the MOR-LSM solution\'s read and write performance was 2-10 times that of the native MOR-Avro solution, demonstrating significant technical advantages.'}),"\n",(0,n.jsx)(a.p,{children:"Facing the growing near-real-time requirements of BI scenarios, streaming dimension widening had gradually become a common challenge for multi-subject domain data processing. Traditional Flink streaming Join had problems such as state bloat and high maintenance complexity. JD's Data Lake team, drawing on Hudi's partial-update multi-stream splicing approach, built an indexing mechanism that supported primary-foreign key mapping. This mechanism efficiently completed streaming dimension association and real-time updates through the coordinated operation of forward and reverse indexes. At the same time, pluggable HBase was introduced as index storage, ensuring high-performance access capability in point query scenarios."}),"\n",(0,n.jsx)(a.p,{children:"In exploring AI scenarios, the team designed and implemented the Hudi NativeIO SDK. This SDK builds four core modules: data invocation layer, cross-language Transformation layer, Hudi view management layer, and high-performance query layer, creating an end-to-end process for sample training engines to complete training directly based on data lake tables."}),"\n",(0,n.jsx)(a.p,{children:"JD had deeply integrated these capabilities with business scenarios, applying them to the near-real-time transformation of the traffic data warehouse ADM layer. After a series of optimizations, the write throughput of the traffic browsing link increased from 45 million per minute to 80 million, Compaction execution efficiency doubled, and real-time consistency maintenance of SKU dimension information was achieved, completing a comprehensive transformation from T+1 offline repair mode to real-time processing mode."}),"\n",(0,n.jsx)(a.p,{children:"While promoting self-developed technology, JD also actively gave back to the open-source community, with a total of 109 contributed and merged PRs. In the future, the team will continue to deepen Hudi's application in the real-time data lake domain, providing stronger data support capabilities for business innovation."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"image 4",src:t(99662).A+"",width:"4032",height:"3024"})}),"\n",(0,n.jsx)(a.h2,{id:"how-kuaishous-real-time-lake-ingestion-empowered-bi--ai-scenario-architecture-upgrade",children:"How Kuaishou's Real-time Lake Ingestion Empowered BI & AI Scenario Architecture Upgrade"}),"\n",(0,n.jsx)(a.p,{children:"In the session \"How Kuaishou's Real-time Lake Ingestion Empowers BI & AI Scenario Architecture Upgrade,\" Wang Zeyu, Data Architecture R&D Engineer at Kuaishou, introduced Kuaishou's complete evolution path and practical experience in building a real-time data lake based on Apache Hudi."}),"\n",(0,n.jsx)(a.p,{children:"For traditional BI data warehouse scenarios, Kuaishou achieved an architecture upgrade from Mysql2Hive to Mysql2Hudi2.0. By introducing Hudi hourly partition tables, supporting multiple query modes such as full, incremental, and snapshot, and innovatively designing Full Compact and Minor Compact mechanisms to optimize data layout, Kuaishou improved the overall architecture. The introduction of bucket heterogeneity allowed full partitions and incremental partitions to support different bucket numbers, significantly reducing lake ingestion resource consumption. Compared with the original architecture, the new solution naturally supported long lifecycles and richer query behaviors. While reducing storage costs, it achieved a leap in data readiness time from day-level to minute-level."}),"\n",(0,n.jsx)(a.p,{children:"At the AI storage architecture level, Kuaishou built a unified stream-batch data lake architecture, solving the core pain point of inconsistent offline and real-time training data. Through unified storage media, support for unified stream-batch consumption, logical wide table column splicing, and other capabilities, unified management and efficient reuse of training data were achieved. The metadata management mechanism based on Event-time timeline not only ensured data orderliness but also guaranteed real-time write performance through lock-free design."}),"\n",(0,n.jsx)(a.p,{children:"In the future, Kuaishou will continue to improve the data lake's service capabilities in training, retrieval, analysis, and other multi-scenarios, promoting the evolution of the data lake toward a more intelligent and unified direction. Kuaishou's practice fully proves that the real-time data lake architecture based on Hudi can effectively support the modernization and upgrade needs of large-scale BI and AI scenarios."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"image 5",src:t(26123).A+"",width:"4032",height:"3024"})}),"\n",(0,n.jsx)(a.h2,{id:"deep-optimization-and-ai-exploration-of-apache-hudi-on-huawei-cloud",children:"Deep Optimization and AI Exploration of Apache Hudi on Huawei Cloud"}),"\n",(0,n.jsx)(a.p,{children:'In the session "Deep Optimization and AI Exploration of Apache Hudi on Huawei Cloud," Yang Xuan, Big Data Lakehouse Kernel R&D Engineer at Huawei, shared Huawei Cloud\'s technical practices and innovative breakthroughs in building a new generation Lakehouse architecture based on Apache Hudi. Facing challenges in real-time performance, intelligence, and management efficiency for enterprise-level data platforms, Huawei conducted in-depth exploration in three dimensions: platform architecture, kernel optimization, and ecosystem integration.'}),"\n",(0,n.jsx)(a.p,{children:"At the platform architecture level, Huawei developed the LDMS unified lakehouse management service platform, achieving fully managed operation and maintenance of table services. Through core capabilities such as intelligent data layout optimization and CBO statistics collection, this platform significantly reduced the operational complexity of the lakehouse platform, allowing users to focus more on business logic rather than underlying maintenance."}),"\n",(0,n.jsx)(a.p,{children:"In terms of kernel optimization, Huawei made multiple deep modifications to Apache Hudi. Through de-Avro serialization optimization implemented via RFC-84/87, Flink write performance improved up to 10 times while significantly reducing GC pressure; the innovative LogIndex mechanism effectively solved the streaming read performance bottleneck in object storage scenarios; dynamic Schema change support made CDC lake ingestion processes more flexible; and the introduction of the column clustering mechanism provided a feasible solution for real-time processing of thousand-column sparse wide tables."}),"\n",(0,n.jsx)(a.p,{children:"Hudi Native built a high-performance IO acceleration layer by rewriting Parquet read/write logic using Rust and adopting Arrow memory format to replace Avro. By providing a unified high-performance Java read/write interface through JNI, it achieved seamless integration with compute engines such as Spark and Flink, laying a solid foundation for future performance breakthroughs."}),"\n",(0,n.jsx)(a.p,{children:"In ecosystem integration and AI exploration, Huawei built a management architecture supporting multimodal data. By using lake table formats to manage metadata of unstructured data, with actual files stored in object storage, it ensured ACID properties while avoiding data redundancy. At the same time, it integrated LanceDB to provide efficient vector retrieval capabilities, providing comprehensive data infrastructure support for AI application scenarios such as document retrieval and intelligent Q&A."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"image 6",src:t(58428).A+"",width:"4032",height:"3024"})}),"\n",(0,n.jsx)(a.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,n.jsx)(a.p,{children:'This meetup made us believe that the vast ocean of data lakehouses could not be separated from the "collective effort" of the open-source community and enterprises. Those technologies tempered on the business battlefield ultimately gave back as nutrients nourishing the entire ecosystem. This may be the purest romance of technology: making complex things simple and making the impossible possible. The road ahead is full of imagination, and together, we are shaping a more elegant and powerful future for data processing.'}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"image 7",src:t(72153).A+"",width:"4032",height:"3024"})})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},47354:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/08/18/improving-marker-mechanism","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-08-18-improving-marker-mechanism.md","source":"@site/blog/2021-08-18-improving-marker-mechanism.md","title":"Improving Marker Mechanism in Apache Hudi","description":"Hudi supports fully automatic cleanup of uncommitted data on storage during its write operations. Write operations in an Apache Hudi table use markers to efficiently track the data files written to storage. In this blog, we dive into the design of the existing direct marker file mechanism and explain its performance problems on cloud storage like AWS S3 for","date":"2021-08-18T00:00:00.000Z","tags":[{"inline":true,"label":"design","permalink":"/blog/tags/design"},{"inline":true,"label":"timeline-server","permalink":"/blog/tags/timeline-server"},{"inline":true,"label":"markers","permalink":"/blog/tags/markers"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":8.73,"hasTruncateMarker":true,"authors":[{"name":"yihua","key":null,"page":null}],"frontMatter":{"title":"Improving Marker Mechanism in Apache Hudi","excerpt":"We introduce a new marker mechanism leveraging the timeline server to address performance bottlenecks due to rate-limiting on cloud storage like AWS S3.","author":"yihua","category":"blog","image":"/assets/images/blog/marker-mechanism/timeline-server-based-marker-mechanism.png","tags":["design","timeline-server","markers","apache hudi"]},"unlisted":false,"prevItem":{"title":"Reliable ingestion from AWS S3 using Hudi","permalink":"/blog/2021/08/23/s3-events-source"},"nextItem":{"title":"Adding support for Virtual Keys in Hudi","permalink":"/blog/2021/08/18/virtual-keys"}}')},47400:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/02/27/Building-Data-Lakes-on-AWS-with-Kafka-Connect-Debezium-Apicurio-Registry-and-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-02-27-Building-Data-Lakes-on-AWS-with-Kafka-Connect-Debezium-Apicurio-Registry-and-Apache-Hudi.mdx","source":"@site/blog/2024-02-27-Building-Data-Lakes-on-AWS-with-Kafka-Connect-Debezium-Apicurio-Registry-and-Apache-Hudi.mdx","title":"Building Data Lakes on AWS with Kafka Connect, Debezium, Apicurio Registry, and Apache Hudi","description":"Redirecting... please wait!!","date":"2024-02-27T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"itnext","permalink":"/blog/tags/itnext"},{"inline":true,"label":"beginner","permalink":"/blog/tags/beginner"},{"inline":true,"label":"apache kafka","permalink":"/blog/tags/apache-kafka"},{"inline":true,"label":"kafka connect","permalink":"/blog/tags/kafka-connect"},{"inline":true,"label":"debezium","permalink":"/blog/tags/debezium"},{"inline":true,"label":"apicurio registry","permalink":"/blog/tags/apicurio-registry"},{"inline":true,"label":"aws","permalink":"/blog/tags/aws"},{"inline":true,"label":"apache spark","permalink":"/blog/tags/apache-spark"},{"inline":true,"label":"deltastreamer","permalink":"/blog/tags/deltastreamer"},{"inline":true,"label":"hudi streamer","permalink":"/blog/tags/hudi-streamer"},{"inline":true,"label":"amazon rds","permalink":"/blog/tags/amazon-rds"},{"inline":true,"label":"amazon mks","permalink":"/blog/tags/amazon-mks"},{"inline":true,"label":"amazon eks","permalink":"/blog/tags/amazon-eks"},{"inline":true,"label":"aws glue","permalink":"/blog/tags/aws-glue"},{"inline":true,"label":"amazon emr","permalink":"/blog/tags/amazon-emr"}],"readingTime":0.16,"hasTruncateMarker":false,"authors":[{"name":"Gary A. Stafford","key":null,"page":null}],"frontMatter":{"title":"Building Data Lakes on AWS with Kafka Connect, Debezium, Apicurio Registry, and Apache Hudi","excerpt":"Building Data Lakes on AWS with Kafka Connect, Debezium, Apicurio Registry, and Apache Hudi","author":"Gary A. Stafford","category":"blog","image":"/assets/images/blog/2024-02-27-Building-Data-Lakes-on-AWS-with-Kafka-Connect-Debezium-Apicurio-Registry-and-Apache-Hudi.png","tags":["blog","apache hudi","itnext","beginner","apache kafka","kafka connect","debezium","apicurio registry","aws","apache spark","deltastreamer","hudi streamer","amazon rds","amazon mks","amazon eks","aws glue","amazon emr"]},"unlisted":false,"prevItem":{"title":"Apache Hudi: From Zero To One (9/10)","permalink":"/blog/2024/03/05/Apache-Hudi-From-Zero-To-One-blog-9"},"nextItem":{"title":"Empowering data-driven excellence: How the Bluestone Data Platform embraced data mesh for success","permalink":"/blog/2024/02/27/empowering-data-driven-excellence-how-the-bluestone-data-platform-embraced-data-mesh-for-success"}}')},47513:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(67834),n=t(74848),s=t(28453);const r={title:"Schema evolution with DeltaStreamer using KafkaSource",excerpt:"Evolve schema used in Kafkasource of DeltaStreamer to keep data up to date with business",author:"sbernauer",category:"blog",image:"/assets/images/blog/hudi_schemaevolution.png",tags:["design","deltastreamer","schema","apache hudi","apache kafka"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"What do we want to achieve?",id:"what-do-we-want-to-achieve",level:2},{value:"What is the problem?",id:"what-is-the-problem",level:2},{value:"Solution",id:"solution",level:2},{value:"Configurations",id:"configurations",level:2},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const a={a:"a",code:"code",h2:"h2",img:"img",p:"p",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.p,{children:"The schema used for data exchange between services can change rapidly with new business requirements.\nApache Hudi is often used in combination with kafka as a event stream where all events are transmitted according to a record schema.\nIn our case a Confluent schema registry is used to maintain the schema and as schema evolves, newer versions are updated in the schema registry."}),"\n",(0,n.jsx)(a.h2,{id:"what-do-we-want-to-achieve",children:"What do we want to achieve?"}),"\n",(0,n.jsxs)(a.p,{children:["We have multiple instances of DeltaStreamer running, consuming many topics with different schemas ingesting to multiple Hudi tables. Deltastreamer is a utility in Hudi to assist in ingesting data from multiple sources like DFS, kafka, etc into Hudi. If interested, you can read more about DeltaStreamer tool ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/hoodie_streaming_ingestion#hudi-streamer",children:"here"}),"\nIdeally every topic should be able to evolve the schema to match new business requirements. Producers start producing data with a new schema version and the DeltaStreamer picks up the new schema and ingests the data with the new schema. For this to work, we run our DeltaStreamer instances with the latest schema version available from the Schema Registry to ensure that we always use the freshest schema with all attributes.\nA prerequisites is that all the mentioned Schema evolutions must be ",(0,n.jsx)(a.code,{children:"BACKWARD_TRANSITIVE"})," compatible (see ",(0,n.jsx)(a.a,{href:"https://docs.confluent.io/platform/current/schema-registry/avro.html",children:"Schema Evolution and Compatibility of Avro Schema changes"}),". This ensures that every record in the kafka topic can always be read using the latest schema."]}),"\n",(0,n.jsx)(a.h2,{id:"what-is-the-problem",children:"What is the problem?"}),"\n",(0,n.jsxs)(a.p,{children:["The normal operation looks like this. Multiple (or a single) producers write records to the kafka topic.\nIn regular flow of events, all records are in the same schema v1 and is in sync with schema registry.\n",(0,n.jsx)(a.img,{alt:"Normal operation",src:t(75077).A+"",width:"871",height:"141"}),(0,n.jsx)("br",{}),"\nThings get complicated when a producer switches to a new Writer-Schema v2 (in this case ",(0,n.jsx)(a.code,{children:"Producer A"}),"). ",(0,n.jsx)(a.code,{children:"Producer B"})," remains on Schema v1. E.g. an attribute ",(0,n.jsx)(a.code,{children:"myattribute"})," was added to the schema, resulting in schema version v2.\nDeltastreamer is capable of handling such schema evolution, if all incoming records were evolved and serialized with evolved schema. But the complication is that, some records are serialized with schema version v1 and some are serialized with schema version v2."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{alt:"Schema evolution",src:t(10341).A+"",width:"871",height:"247"}),(0,n.jsx)("br",{}),"\nThe default deserializer used by Hudi ",(0,n.jsx)(a.code,{children:"io.confluent.kafka.serializers.KafkaAvroDeserializer"})," uses the schema that the record was serialized with for deserialization. This causes Hudi to get records with multiple different schema from the kafka client. E.g. Event #13 has the new attribute ",(0,n.jsx)(a.code,{children:"myattribute"}),", Event #14 does not have the new attribute ",(0,n.jsx)(a.code,{children:"myattribute"}),". This makes things complicated and error-prone for Hudi."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{alt:"Confluent Deserializer",src:t(19990).A+"",width:"702",height:"181"}),(0,n.jsx)("br",{})]}),"\n",(0,n.jsx)(a.h2,{id:"solution",children:"Solution"}),"\n",(0,n.jsxs)(a.p,{children:["Hudi added a new custom Deserializer ",(0,n.jsx)(a.code,{children:"KafkaAvroSchemaDeserializer"})," to solve this problem of different producers producing records in different schema versions, but to use the latest schema from schema registry to deserialize all the records.",(0,n.jsx)("br",{}),"\nAs first step the Deserializer gets the latest schema from the Hudi SchemaProvider. The SchemaProvider can get the schema for example from a Confluent Schema-Registry or a file.\nThe Deserializer then reads the records from the topic using the schema the record was written with. As next step it will convert all the records to the latest schema from the SchemaProvider, in our case the latest schema. As a result, the kafka client will return all records with a unified schema i.e. the latest schema as per schema registry. Hudi does not need to handle different schemas inside a single batch."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{alt:"KafkaAvroSchemaDeserializer",src:t(14358).A+"",width:"702",height:"181"}),(0,n.jsx)("br",{})]}),"\n",(0,n.jsx)(a.h2,{id:"configurations",children:"Configurations"}),"\n",(0,n.jsx)(a.p,{children:"As of upcoming release 0.9.0, normal Confluent Deserializer is used by default. One has to explicitly set KafkaAvroSchemaDeserializer as below,\nin order to ensure smooth schema evolution with different producers producing records in different versions."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.code,{children:"hoodie.deltastreamer.source.kafka.value.deserializer.class=org.apache.hudi.utilities.deser.KafkaAvroSchemaDeserializer"})}),"\n",(0,n.jsx)(a.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,n.jsxs)(a.p,{children:["Hope this blog helps in ingesting data from kafka into Hudi using Deltastreamer tool catering to different schema evolution\nneeds. Hudi has a very active development community and we look forward for more contributions.\nPlease check out ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/contribute/get-involved",children:"this"})," link to start contributing."]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},47645:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/11/12/deep-dive-into-hudis-indexing-subsystem-part-2-of-2","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-11-12-deep-dive-into-hudis-indexing-subsystem-part-2-of-2.md","source":"@site/blog/2025-11-12-deep-dive-into-hudis-indexing-subsystem-part-2-of-2.md","title":"Deep Dive Into Hudi\'s Indexing Subsystem (Part 2 of 2)","description":"In part 1, we explored how Hudi\'s metadata table functions as a self-managed, multimodal indexing subsystem. We covered its internal architecture\u2014a partitioned Hudi Merge-on-Read (MOR) table using HFile format for efficient key lookups\u2014and how the files, column stats, and partition stats indexes work together to implement powerful data skipping. These indexes dramatically reduce I/O by pruning partitions and files that don\'t contain the data your query needs.","date":"2025-11-12T00:00:00.000Z","tags":[{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"indexing","permalink":"/blog/tags/indexing"},{"inline":true,"label":"data lakehouse","permalink":"/blog/tags/data-lakehouse"},{"inline":true,"label":"data skipping","permalink":"/blog/tags/data-skipping"}],"readingTime":10.69,"hasTruncateMarker":false,"authors":[{"name":"Shiyan Xu","key":null,"page":null}],"frontMatter":{"title":"Deep Dive Into Hudi\'s Indexing Subsystem (Part 2 of 2)","excerpt":"Explore advanced indexing in Apache Hudi: record and secondary indexes for fast point lookups, expression indexes for transformed predicates, and async indexing for building indexes without blocking writes.","author":"Shiyan Xu","category":"blog","image":"/assets/images/blog/2025-11-12-deep-dive-into-hudis-indexing-subsystem-part-2-of-2/fig1.png","tags":["hudi","indexing","data lakehouse","data skipping"]},"unlisted":false,"prevItem":{"title":"Apache Hudi 1.1 is Here\u2014Building the Foundation for the Next Generation of Lakehouse","permalink":"/blog/2025/11/25/apache-hudi-release-1-1-announcement"},"nextItem":{"title":"How FreeWheel Uses Apache Hudi to Power Its Data Lakehouse","permalink":"/blog/2025/11/07/how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse"}}')},47725:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/06/07/apache-hudi-a-deep-dive-with-python-code-examples","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.mdx","source":"@site/blog/2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.mdx","title":"Apache Hudi: A Deep Dive with Python Code Examples","description":"Redirecting... please wait!!","date":"2024-06-07T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"python","permalink":"/blog/tags/python"},{"inline":true,"label":"pyspark","permalink":"/blog/tags/pyspark"},{"inline":true,"label":"harshdaiya","permalink":"/blog/tags/harshdaiya"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Harsh Daiya","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi: A Deep Dive with Python Code Examples","author":"Harsh Daiya","category":"blog","image":"/assets/images/blog/2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.png","tags":["blog","apache hudi","python","pyspark","harshdaiya"]},"unlisted":false,"prevItem":{"title":"How to use Apache Hudi with Databricks","permalink":"/blog/2024/06/18/how-to-use-apache-hudi-with-databricks"},"nextItem":{"title":"Apache Hudi vs. Delta Lake: Choosing the Right Tool for Your Data Lake on AWS","permalink":"/blog/2024/05/27/apache-hudi-vs-delta-lake-choosing-the-right-tool-for-your-data-lake-on-aws"}}')},47736:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/change-capture-architecture-dc9c69c50296a6a38721ec93fee9ba71.png"},47901:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/04/07/Speed-up-your-write-latencies-using-Bucket-Index-in-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-04-07-Speed-up-your-write-latencies-using-Bucket-Index-in-Apache-Hudi.mdx","source":"@site/blog/2023-04-07-Speed-up-your-write-latencies-using-Bucket-Index-in-Apache-Hudi.mdx","title":"Speed up your write latencies using Bucket Index in Apache Hudi","description":"Redirecting... please wait!!","date":"2023-04-07T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"indexing","permalink":"/blog/tags/indexing"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Sivabalan Narayanan","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Speed up your write latencies using Bucket Index in Apache Hudi","authors":[{"name":"Sivabalan Narayanan"}],"category":"blog","image":"/assets/images/blog/2023-04-07-Speed-up-your-write-latencies-using-Bucket-Index-in-Apache-Hudi.png","tags":["how-to","indexing","medium"]},"unlisted":false,"prevItem":{"title":"Getting Started: Incrementally process data with Apache Hudi","permalink":"/blog/2023/04/18/getting-started-incrementally-process-data-with-apache-hudi"},"nextItem":{"title":"Global vs Non-global index in Apache Hudi","permalink":"/blog/2023/04/02/global-vs-non-global-index-in-apache-hudi"}}')},47999:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(68660),n=t(74848),s=t(28453),r=t(9230);const o={title:"What is Clustering in an Open Data Lakehouse?",author:"Dipankar Mazumdar",category:"blog",image:"/assets/images/blog/cluster.png",tags:["blog","Apache Hudi","Apache Iceberg","Delta Lake","Clustering","Z-order"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.onehouse.ai/blog/what-is-clustering-in-an-open-data-lakehouse",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},48037:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/07/21/AWS-Glue-Crawlers-now-supports-Apache-Hudi-Tables","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-07-21-AWS-Glue-Crawlers-now-supports-Apache-Hudi-Tables.mdx","source":"@site/blog/2023-07-21-AWS-Glue-Crawlers-now-supports-Apache-Hudi-Tables.mdx","title":"AWS Glue Crawlers now supports Apache Hudi Tables","description":"Redirecting... please wait!!","date":"2023-07-21T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"aws glue","permalink":"/blog/tags/aws-glue"},{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"glue crawler","permalink":"/blog/tags/glue-crawler"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"AWS Team","socials":{},"key":null,"page":null}],"frontMatter":{"title":"AWS Glue Crawlers now supports Apache Hudi Tables","authors":[{"name":"AWS Team"}],"category":"blog","image":"/assets/images/blog/2023-07-21-AWS-Glue-Crawlers-now-supports-Apache-Hudi-Tables.png","tags":["blog","aws glue","hudi","glue crawler"]},"unlisted":false,"prevItem":{"title":"Apache Hudi: Revolutionizing Big Data Management for Real-Time Analytics","permalink":"/blog/2023/07/27/Apache-Hudi-Revolutionizing-Big-Data-Management-for-Real-Time-Analytics"},"nextItem":{"title":"Backfilling Apache Hudi Tables in Production: Techniques & Approaches Using AWS Glue by Job Target LLC","permalink":"/blog/2023/07/20/Backfilling-Apache-Hudi-Tables-in-Production-Techniques-and-Approaches-Using-AWS-Glue-by-Job-Target-LLC"}}')},48059:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2016/12/30/strata-talk-2017","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2016-12-30-strata-talk-2017.md","source":"@site/blog/2016-12-30-strata-talk-2017.md","title":"Connect with us at Strata San Jose March 2017","description":"We will be presenting Hudi & general concepts around how incremental processing works at Uber.","date":"2016-12-30T00:00:00.000Z","tags":[],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"admin","key":null,"page":null}],"frontMatter":{"title":"Connect with us at Strata San Jose March 2017","author":"admin","date":"2016-12-30T00:00:00.000Z","category":"blog"},"unlisted":false,"prevItem":{"title":"Hoodie: Uber Engineering\'s Incremental Processing Framework on Hadoop","permalink":"/blog/2017/03/12/Hoodie-Uber-Engineerings-Incremental-Processing-Framework-on-Hadoop"},"nextItem":{"title":"The Case for incremental processing on Hadoop","permalink":"/blog/2016/08/04/The-Case-for-incremental-processing-on-Hadoop"}}')},48198:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/02/20/Understanding-its-core-concepts-from-hudi-persistence-files","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-02-20-Understanding-its-core-concepts-from-hudi-persistence-files.mdx","source":"@site/blog/2022-02-20-Understanding-its-core-concepts-from-hudi-persistence-files.mdx","title":"Understanding its core concepts from hudi persistence files","description":"Redirecting... please wait!!","date":"2022-02-20T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"storage spec","permalink":"/blog/tags/storage-spec"},{"inline":true,"label":"programmer","permalink":"/blog/tags/programmer"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"QbertsBrother","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Understanding its core concepts from hudi persistence files","authors":[{"name":"QbertsBrother"}],"category":"blog","image":"/assets/images/blog/2022-02-20-understanding-core-concepts-from-hudi-persistence-files.png","tags":["blog","storage spec","programmer"]},"unlisted":false,"prevItem":{"title":"Create a low-latency source-to-data lake pipeline using Amazon MSK Connect, Apache Flink, and Apache Hudi","permalink":"/blog/2022/03/01/Create-a-low-latency-source-to-data-lake-pipeline-using-Amazon-MSK-Connect-Apache-Flink-and-Apache-Hudi"},"nextItem":{"title":"Fresher Data Lake on AWS S3","permalink":"/blog/2022/02/17/Fresher-Data-Lake-on-AWS-S3"}}')},48209:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(91623),n=t(74848),s=t(28453),r=t(9230);const o={title:"Baixin bank\u2019s real-time data lake evolution scheme based on Apache Hudi",category:"blog",image:"/assets/images/blog/2021-07-26-baixin-bank-real-time-data-lake.png",tags:["use-case","real-time datalake","incremental processing","developpaper"]},l=void 0,d={authorsImageUrls:[]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://developpaper.com/baixin-banks-real-time-data-lake-evolution-scheme-based-on-apache-hudi/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},48279:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2020/11/29/Can-Big-Data-Solutions-Be-Affordable","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-11-29-Can-Big-Data-Solutions-Be-Affordable.mdx","source":"@site/blog/2020-11-29-Can-Big-Data-Solutions-Be-Affordable.mdx","title":"Can Big Data Solutions Be Affordable?","description":"Redirecting... please wait!!","date":"2020-11-29T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"big-data","permalink":"/blog/tags/big-data"},{"inline":true,"label":"near real-time analytics","permalink":"/blog/tags/near-real-time-analytics"},{"inline":true,"label":"analyticsinsight","permalink":"/blog/tags/analyticsinsight"}],"readingTime":0.11,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Can Big Data Solutions Be Affordable?","category":"blog","image":"/assets/images/blog/2020-11-29-Can-Big-Data-Solutions-Be-Affordable.jpg","tags":["blog","big-data","near real-time analytics","analyticsinsight"]},"unlisted":false,"prevItem":{"title":"Building High-Performance Data Lake Using Apache Hudi and Alluxio at T3Go","permalink":"/blog/2020/12/01/high-perf-data-lake-with-hudi-and-alluxio-t3go"},"nextItem":{"title":"Employing the right indexes for fast updates, deletes in Apache Hudi","permalink":"/blog/2020/11/11/hudi-indexing-mechanisms"}}')},48402:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/change-logs-mysql-a76f7760403ba59c5d11ba48b12cd4d6.png"},48713:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(34883),n=t(74848),s=t(28453),r=t(9230);const o={title:"Record Level Indexing in Apache Hudi",author:"Bibhu Pala",category:"blog",image:"/assets/images/blog/2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.png",tags:["blog","apache hudi","record index","record level index","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@prasadpal107/record-level-indexing-in-apache-hudi-0615804608ec",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},48840:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(8792),n=t(74848),s=t(28453);const r={title:"Apache Hudi 1.1 is Here\u2014Building the Foundation for the Next Generation of Lakehouse",excerpt:"",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2025-11-25-apache-hudi-release-1-1-announcement/1-pluggable-TF.png",tags:["hudi","release","feature","performance"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Pluggable Table Format\u2014The Foundation for Multi-Format Support",id:"pluggable-table-formatthe-foundation-for-multi-format-support",level:2},{value:"Vision and Design",id:"vision-and-design",level:3},{value:"Key Architectural Components",id:"key-architectural-components",level:3},{value:"Indexing Improvements\u2014Faster and Smarter Lookups",id:"indexing-improvementsfaster-and-smarter-lookups",level:2},{value:"Partitioned Record Index",id:"partitioned-record-index",level:3},{value:"Partition-level Bucket Index",id:"partition-level-bucket-index",level:3},{value:"Indexing Performance Optimizations",id:"indexing-performance-optimizations",level:3},{value:"Faster Clustering with Parquet File Binary Copy",id:"faster-clustering-with-parquet-file-binary-copy",level:2},{value:"Storage-Based Lock Provider\u2014Eliminating External Dependencies for Concurrent Writers",id:"storage-based-lock-providereliminating-external-dependencies-for-concurrent-writers",level:2},{value:"Use Merge Modes and Custom Mergers\u2014Say Goodbye to Payload Classes",id:"use-merge-modes-and-custom-mergerssay-goodbye-to-payload-classes",level:2},{value:"Merge Modes\u2014Declarative Record Merging",id:"merge-modesdeclarative-record-merging",level:3},{value:"Custom Mergers\u2014The Flexible Approach",id:"custom-mergersthe-flexible-approach",level:3},{value:"Apache Spark Integration Improvements",id:"apache-spark-integration-improvements",level:2},{value:"Spark 4.0 Support",id:"spark-40-support",level:3},{value:"Metadata Table Streaming Writes",id:"metadata-table-streaming-writes",level:3},{value:"New and Enhanced SQL Procedures",id:"new-and-enhanced-sql-procedures",level:3},{value:"Apache Flink Integration Improvements",id:"apache-flink-integration-improvements",level:2},{value:"Flink 2.0 Support",id:"flink-20-support",level:3},{value:"Engine-Native Record Support",id:"engine-native-record-support",level:3},{value:"Buffer Sort",id:"buffer-sort",level:3},{value:"New Integration: Apache Polaris (Incubating)",id:"new-integration-apache-polaris-incubating",level:2},{value:"What\u2019s Next\u2014Join Us in Building the Future",id:"whats-nextjoin-us-in-building-the-future",level:2}];function c(e){const a={a:"a",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(a.p,{children:["The Hudi community is excited to announce the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-1.1.0",children:"release of Hudi 1.1"}),", a major milestone that sets the stage for the next generation of data lakehouse capabilities. This release represents months of focused engineering on foundational improvements, engine-specific optimizations, and key architectural enhancements, laying the foundation for ambitious features coming in future releases."]}),"\n",(0,n.jsx)(a.p,{children:"Hudi continues to evolve rapidly, with contributions from a vibrant community of developers and users. The 1.1 release brings over 700 commits addressing performance bottlenecks, expanding engine support, and introducing new capabilities that make Hudi tables more reliable, faster, and easier to operate. Let\u2019s dive into the highlights."}),"\n",(0,n.jsx)(a.h2,{id:"pluggable-table-formatthe-foundation-for-multi-format-support",children:"Pluggable Table Format\u2014The Foundation for Multi-Format Support"}),"\n",(0,n.jsxs)(a.p,{children:["Hudi 1.1 introduces a ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/hudi_stack#pluggable-table-format",children:"pluggable table format"})," framework that opens up the powerful storage engine capabilities beyond Hudi\u2019s native storage format to other table formats like Apache Iceberg and Delta Lake. This framework represents a fundamental shift in how Hudi approaches table format support, enabling native integration of multiple formats and giving you a unified system with total read-write compatibility across formats."]}),"\n",(0,n.jsx)(a.h3,{id:"vision-and-design",children:"Vision and Design"}),"\n",(0,n.jsx)(a.p,{children:"The table format landscape in the modern lakehouse ecosystem is diverse and evolving. Like a game of rock-paper-scissors, different formats\u2014Hudi, Iceberg, Delta Lake\u2014each have unique strengths for specific use cases. Rather than forcing a one-size-fits-all approach, Hudi 1.1 introduces a pluggable table format framework that embraces the open lakehouse ecosystem and prevents vendor lock-in."}),"\n",(0,n.jsxs)(a.p,{children:["The framework is built on a clean abstraction layer that decouples Hudi\u2019s core capabilities\u2014transaction management, indexing, concurrency control, and table services\u2014from the specific storage format used for data files. At the heart of this design is the ",(0,n.jsx)(a.code,{children:"HoodieTableFormat"})," interface, which different format implementations can extend."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"pluggable table format",src:t(38746).A+"",width:"894",height:"665"})}),"\n",(0,n.jsx)(a.h3,{id:"key-architectural-components",children:"Key Architectural Components"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Storage engine: Hudi\u2019s storage engine capabilities, such as timeline management, concurrency control mechanisms, indexes, and table services, can work across multiple table formats"}),"\n",(0,n.jsx)(a.li,{children:"Pluggable adapters: Format-specific implementations handle the generation of conforming metadata upon writes"}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["Hudi\u2019s artifact provides support for the native Hudi format, while ",(0,n.jsx)(a.a,{href:"https://xtable.apache.org/",children:"Apache XTable (incubating)"})," supplies pluggable format adapters. For example, ",(0,n.jsx)(a.a,{href:"https://github.com/apache/incubator-xtable/pull/723",children:"this XTable PR"})," implements the Iceberg adapter to allow you to add dependencies to your running pipelines as needed. This architecture enables organizations to choose the right format for each use case while maintaining a unified operational experience and leveraging Hudi\u2019s sophisticated storage engine across all of them."]}),"\n",(0,n.jsxs)(a.p,{children:["In the 1.1 release, the framework comes with native Hudi format support (configured via ",(0,n.jsx)(a.code,{children:"hoodie.table.format=native"})," by default). Existing users don't need to change anything\u2014tables continue to work exactly as before. The real excitement lies ahead: the framework paves the way for supporting additional formats like Iceberg and Delta Lake. Imagine writing high-frequency updates to a Hudi table efficiently with Hudi's record-level indexing capability while maintaining Iceberg metadata through the Iceberg adapter, which supports a wide range of catalogs for reads. The pluggable table format framework in 1.1 makes such usage patterns possible\u2014a game-changer for organizations that need flexibility and openness in their data architecture."]}),"\n",(0,n.jsx)(a.h2,{id:"indexing-improvementsfaster-and-smarter-lookups",children:"Indexing Improvements\u2014Faster and Smarter Lookups"}),"\n",(0,n.jsx)(a.p,{children:"Hudi\u2019s indexing subsystem is one of its most powerful features, enabling fast record lookups during writes and efficient data skipping during reads."}),"\n",(0,n.jsx)(a.h3,{id:"partitioned-record-index",children:"Partitioned Record Index"}),"\n",(0,n.jsxs)(a.p,{children:["Since version 0.14.0, Hudi has supported a global record index in the indexing subsystem\u2014a breakthrough that enables blazing-fast lookups on large datasets. While this is ideal for globally unique identifiers like order IDs or SSNs, many scenarios only require uniqueness within a partition\u2014for example, user events partitioned by date. Hudi 1.1 introduces the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/indexes#record-index",children:"partitioned record index"}),", a non-global variant of the record index that works with the combination of partition path and record key, leveraging partition information to prune irrelevant partitions during lookups and dramatically reducing the search space, and thus achieving efficient lookups even on very large datasets."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"-- Spark SQL: Create table with partitioned record index\nCREATE TABLE user_activity (\n  user_id STRING,\n  activity_type STRING,\n  timestamp BIGINT,\n  event_date DATE\n) USING hudi\nTBLPROPERTIES (\n  'primaryKey' = 'user_id',\n  'preCombineField' = 'timestamp',\n  -- Enable partitioned record index\n  'hoodie.metadata.record.level.index.enable' = 'true',\n  'hoodie.index.type' = 'RECORD_LEVEL_INDEX'\n)\nPARTITIONED BY (event_date);\n"})}),"\n",(0,n.jsx)(a.p,{children:"The partitioned record index enables index lookups that scale proportionally with partition size\u2014file group accesses correlate directly to the data partition size, optimizing performance across heterogeneous data distributions. The design also supports future clustering operations that can dynamically expand file groups within partitions as they grow."}),"\n",(0,n.jsx)(a.h3,{id:"partition-level-bucket-index",children:"Partition-level Bucket Index"}),"\n",(0,n.jsx)(a.p,{children:"The bucket index is a popular choice for high-throughput write workloads because it eliminates expensive record lookups by deterministically mapping keys to file groups. However, the existing bucket index has a key limitation: once you set the number of buckets, changing it requires rewriting the entire table."}),"\n",(0,n.jsx)(a.p,{children:"The 1.1 release introduces partition-level bucket index, which enables different bucket counts for different partitions using regex-based rules. This design allows tables to adapt as data volumes change over time\u2014for example, older, smaller partitions can use fewer buckets while newer, larger partitions can have more."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"-- Spark SQL: Create table with partition-level bucket index\nCREATE TABLE sales_transactions (\n  transaction_id BIGINT,\n  user_id BIGINT,\n  amount DOUBLE,\n  transaction_date DATE\n) USING hudi\nTBLPROPERTIES (\n  'primaryKey' = 'transaction_id',\n  -- Partition-level bucket index\n  'hoodie.index.type' = 'BUCKET',\n  'hoodie.bucket.index.hash.field' = 'transaction_id',\n  'hoodie.bucket.index.partition.rule.type' = 'regex',\n  'hoodie.bucket.index.partition.expressions' = '2023-.*,16;2024-.*,32;2025-.*,64',\n  'hoodie.bucket.index.num.buckets' = '8'\n)\nPARTITIONED BY (transaction_date);\n"})}),"\n",(0,n.jsxs)(a.p,{children:["The partition-level bucket index is ideal for time-series data where partition sizes vary significantly over time. The adaptive bucket sizing helps you maintain optimal write performance as your data volume changes. See the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/indexes#additional-writer-side-indexes",children:"docs"})," and ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/rfc/rfc-89/rfc-89.md",children:"RFC 89"})," for more information."]}),"\n",(0,n.jsx)(a.h3,{id:"indexing-performance-optimizations",children:"Indexing Performance Optimizations"}),"\n",(0,n.jsx)(a.p,{children:"Beyond new indexes, Hudi 1.1 delivers substantial performance improvements for metadata table operations:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"HFile block cache and prefetching: The new block cache stores recently accessed data blocks in memory, avoiding repeated reads from storage. For smaller HFiles, Hudi prefetches the entire file upfront rather than making multiple read requests. Benchmarks show approximately 4x speedup for repeated lookups, enabled by default."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"metadata table key lookup",src:t(38846).A+"",width:"960",height:"540"})}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["HFile Bloom filter: Adding Bloom filters to HFiles enables Hudi to quickly determine whether a key might exist in a file before fetching data blocks, avoiding unnecessary I/O and dramatically speeding up point lookups. You can enable it with ",(0,n.jsx)(a.code,{children:"hoodie.metadata.bloom.filter.enable=true"}),"."]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"These optimizations compound to make the metadata table significantly faster, directly improving both write and read performance across your Hudi tables. Additionally, Hudi 1.1 adds its own native HFile writer implementation, eliminating the dependency on HBase libraries. This refactoring significantly reduces the Hudi bundle size and provides the foundation for future HFile performance optimizations."}),"\n",(0,n.jsx)(a.h2,{id:"faster-clustering-with-parquet-file-binary-copy",children:"Faster Clustering with Parquet File Binary Copy"}),"\n",(0,n.jsx)(a.p,{children:"Clustering reorganizes data to improve query performance, but traditional approaches are expensive\u2014decompressing, decoding, transforming, re-encoding, and re-compressing data even when no transformation is needed."}),"\n",(0,n.jsx)(a.p,{children:"Hudi 1.1 implements Parquet file binary copy for clustering operations. Instead of processing records, this optimization directly copies Parquet RowGroups from source to destination files when schema-compatible, eliminating redundant transformations entirely."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"parquet binary copy",src:t(59731).A+"",width:"739",height:"407"})}),"\n",(0,n.jsx)(a.p,{children:"On 100GB test data, using Parquet file binary copy achieved 15x faster execution (18 minutes \u2192 1.2 minutes) and 95% reduction in compute (28.7 task-hours \u2192 1.3 task-hours) compared to the normal rewriting of Parquet files. Real-world validation with 1.7TB datasets (300 columns) showed approximately 5x performance improvement (35 min \u2192 7.7 min) with CPU usage dropping from 90% to 60%."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"parquet binary copy chart",src:t(73239).A+"",width:"960",height:"540"})}),"\n",(0,n.jsxs)(a.p,{children:["The optimization is currently supported for Copy-on-Write tables and enabled automatically when safe, with Hudi intelligently falling back to traditional clustering when schema reconciliation is required. You may refer to ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/pull/13365",children:"this PR"})," for more detail."]}),"\n",(0,n.jsx)(a.h2,{id:"storage-based-lock-providereliminating-external-dependencies-for-concurrent-writers",children:"Storage-Based Lock Provider\u2014Eliminating External Dependencies for Concurrent Writers"}),"\n",(0,n.jsx)(a.p,{children:"Multi-writer concurrency is critical for production data lakehouses, where multiple jobs need to write to the same table simultaneously. Historically, enabling multi-writer support in Hudi required setting up external lock providers like AWS DynamoDB, Apache Zookeeper, or Hive Metastore. While these work well, they add operational complexity\u2014you need to provision, maintain, and monitor additional infrastructure."}),"\n",(0,n.jsxs)(a.p,{children:["Hudi 1.1 introduces a storage-based lock provider that eliminates this dependency entirely by managing concurrency directly using the ",(0,n.jsx)(a.code,{children:".hoodie/"})," directory in your table's storage layer."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"storage based lock provider",src:t(72806).A+"",width:"708",height:"373"})}),"\n",(0,n.jsxs)(a.p,{children:["The implementation uses conditional writes on a single lock file under ",(0,n.jsx)(a.code,{children:".hoodie/.locks/"})," to ensure only one writer holds the lock at a time, with heartbeat-based renewal and automatic expiration for fault tolerance. To use the storage-based lock provider, you need to add the corresponding Hudi cloud bundle (",(0,n.jsx)(a.code,{children:"hudi-aws-bundle"})," for S3 and ",(0,n.jsx)(a.code,{children:"hudi-gcp-bundle"})," for GCS) and set the following configuration:"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-properties",children:"hoodie.write.lock.provider=org.apache.hudi.client.transaction.lock.StorageBasedLockProvider\n"})}),"\n",(0,n.jsxs)(a.p,{children:["This approach eliminates the need for DynamoDB, ZooKeeper, or Hive Metastore dependencies, reducing operational costs and infrastructure complexity. The cloud-native design works directly with S3 or GCS storage features, with support for additional storage systems planned, making Hudi easier to operate at scale in cloud-native environments. Check out the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/concurrency_control#storage-based-lock-provider",children:"docs"})," and ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/rfc/rfc-91/rfc-91.md",children:"RFC 91"})," for more detail."]}),"\n",(0,n.jsx)(a.h2,{id:"use-merge-modes-and-custom-mergerssay-goodbye-to-payload-classes",children:"Use Merge Modes and Custom Mergers\u2014Say Goodbye to Payload Classes"}),"\n",(0,n.jsx)(a.p,{children:"A core design principle of Hudi is enabling the storage layer to understand how to merge updates to the same record key, even when changes arrive out of order\u2014a common scenario with mobile apps, IoT devices, and distributed systems. Prior to Hudi 1.1, record merging logic was primarily implemented through payload classes, which were fragmented and lacked standardized semantics."}),"\n",(0,n.jsxs)(a.p,{children:["Hudi 1.1 deprecates payload classes and encourages users to adopt the new APIs introduced since 1.0 for record merging: merge modes and the ",(0,n.jsx)(a.code,{children:"HoodieRecordMerger"})," interface."]}),"\n",(0,n.jsx)(a.h3,{id:"merge-modesdeclarative-record-merging",children:"Merge Modes\u2014Declarative Record Merging"}),"\n",(0,n.jsxs)(a.p,{children:["For common use cases, the ",(0,n.jsx)(a.code,{children:"COMMIT_TIME_ORDERING"})," and ",(0,n.jsx)(a.code,{children:"EVENT_TIME_ORDERING"})," merge modes provide a declarative way to specify merge behavior:"]}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Merge mode"}),(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"What does it do?"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:(0,n.jsx)(a.code,{children:"COMMIT_TIME_ORDERING"})}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"Picks the record with the highest completion time/instant as the final merge result (standard relational semantics or arrival time processing)"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:(0,n.jsx)(a.code,{children:"EVENT_TIME_ORDERING"})}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"Picks the record with the highest value on a user-specified ordering field as the final merge result. Enables event time processing semantics for handling late-arriving data without corrupting record state."})]})]})]}),"\n",(0,n.jsxs)(a.p,{children:["The default behavior is adaptive: if no ordering field (",(0,n.jsx)(a.code,{children:"hoodie.table.ordering.fields"}),") is configured, Hudi defaults to ",(0,n.jsx)(a.code,{children:"COMMIT_TIME_ORDERING"}),"; if one or more ordering fields are set, it uses ",(0,n.jsx)(a.code,{children:"EVENT_TIME_ORDERING"}),". This makes Hudi work out-of-the-box for simple use cases while still supporting event-time ordering when needed."]}),"\n",(0,n.jsx)(a.h3,{id:"custom-mergersthe-flexible-approach",children:"Custom Mergers\u2014The Flexible Approach"}),"\n",(0,n.jsxs)(a.p,{children:["For complex merging logic\u2014such as field-level reconciliation, aggregating counters, or preserving audit fields\u2014the ",(0,n.jsx)(a.code,{children:"HoodieRecordMerger"})," interface provides a modern, engine-native alternative to payload classes. You need to set the merge mode to ",(0,n.jsx)(a.code,{children:"CUSTOM"})," and provide your own implementation of ",(0,n.jsx)(a.code,{children:"HoodieRecordMerger"}),". By using the new API, you can achieve consistent merging across all code paths: precombine, updating writes, compaction, and snapshot reads\u2014you are strongly encouraged to migrate to the new APIs. See ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/record_merger",children:"the docs"})," for more details. For migration guidance, see the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-1.1.0/",children:"release notes"})," and ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/pull/13499",children:"RFC-97"}),"."]}),"\n",(0,n.jsx)(a.h2,{id:"apache-spark-integration-improvements",children:"Apache Spark Integration Improvements"}),"\n",(0,n.jsx)(a.p,{children:"Spark remains one of the most popular engines for working with Hudi tables, and the 1.1 release brings several important enhancements."}),"\n",(0,n.jsx)(a.h3,{id:"spark-40-support",children:"Spark 4.0 Support"}),"\n",(0,n.jsxs)(a.p,{children:["Spark 4.0 brought significant performance gains for ML/AI workloads, smarter query optimization with automatic join strategy switching, dynamic partition skew mitigation, and enhanced streaming capabilities. Hudi 1.1 adds Spark 4.0 support to unlock these improvements for working with Hudi tables. To get started, use the new ",(0,n.jsx)(a.code,{children:"hudi-spark4.0-bundle_2.13:1.1.0"})," artifact in your dependency list."]}),"\n",(0,n.jsx)(a.h3,{id:"metadata-table-streaming-writes",children:"Metadata Table Streaming Writes"}),"\n",(0,n.jsx)(a.p,{children:"Hudi 1.1 introduces streaming writes to the metadata table, unifying data and metadata writes into a single RDD execution chain. The key design generates metadata records directly during data writes in parallel across executors, eliminating redundant file lookups that previously created bottlenecks and enhancing reliability when performing stage retries in Spark."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"spark upsert time chart",src:t(85337).A+"",width:"960",height:"540"})}),"\n",(0,n.jsx)(a.p,{children:"A benchmark with update-intensive workloads showed that this 1.1 feature delivered about 18% faster write times for tables with record index, compared to Hudi 1.0. The feature is enabled by default for Spark writers."}),"\n",(0,n.jsx)(a.h3,{id:"new-and-enhanced-sql-procedures",children:"New and Enhanced SQL Procedures"}),"\n",(0,n.jsxs)(a.p,{children:["Hudi 1.1 expands the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/procedures",children:"SQL procedure"})," library with useful additions and enhanced capabilities for table management and observability, bringing operational capabilities directly into Spark SQL."]}),"\n",(0,n.jsxs)(a.p,{children:["The new procedures, ",(0,n.jsx)(a.code,{children:"show_cleans"}),", ",(0,n.jsx)(a.code,{children:"show_clean_plans"}),", and ",(0,n.jsx)(a.code,{children:"show_cleans_metadata"}),", provide visibility into cleaning operations:"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"CALL show_cleans(table => 'hudi_table', limit => 10);\nCALL show_clean_plans(table => 'hudi_table', limit => 10);\nCALL show_cleans_metadata(table => 'hudi_table', limit => 10);\n"})}),"\n",(0,n.jsxs)(a.p,{children:["The enhanced ",(0,n.jsx)(a.code,{children:"run_clustering"})," procedure supports partition filtering with regex patterns:"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"-- Cluster all 2025 partitions matching a pattern\nCALL run_clustering(\n  table => 'hudi_table',\n  partition_regex_pattern => '2025-.*',\n);\n"})}),"\n",(0,n.jsxs)(a.p,{children:["All ",(0,n.jsx)(a.code,{children:"show"})," procedures, where applicable, were enhanced with ",(0,n.jsx)(a.code,{children:"path"})," and ",(0,n.jsx)(a.code,{children:"filter"})," parameters. ",(0,n.jsx)(a.code,{children:"path"})," helps when ",(0,n.jsx)(a.code,{children:"table_name"})," is not able to identify a table properly. ",(0,n.jsx)(a.code,{children:"filter"})," can support advanced predicate expressions. For example:"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"-- Find large files in recent partitions\nCALL show_file_status(\n  path => '/data/warehouse/transactions',\n  filter => \"partition LIKE '2025-11%' AND file_size > 524288000\"\n);\n"})}),"\n",(0,n.jsx)(a.p,{children:"The new and enhanced SQL procedures bring table management directly into Spark SQL, streamlining operations for SQL-focused workflows."}),"\n",(0,n.jsx)(a.h2,{id:"apache-flink-integration-improvements",children:"Apache Flink Integration Improvements"}),"\n",(0,n.jsx)(a.p,{children:"Flink is a popular choice for real-time data pipelines, and Hudi 1.1 brings substantial improvements to the Flink integration."}),"\n",(0,n.jsx)(a.h3,{id:"flink-20-support",children:"Flink 2.0 Support"}),"\n",(0,n.jsxs)(a.p,{children:["Hudi 1.1 brings support for Flink 2.0, the first major Flink release in nine years. Flink 2.0 introduced disaggregated state storage (ForSt) that decouples state from compute for unlimited scalability, asynchronous state execution for improved resource utilization, adaptive broadcast join for efficient query processing, and materialized tables for simplified stream-batch unification. Use the new ",(0,n.jsx)(a.code,{children:"hudi-flink2.0-bundle:1.1.0"})," artifact to get started."]}),"\n",(0,n.jsx)(a.h3,{id:"engine-native-record-support",children:"Engine-Native Record Support"}),"\n",(0,n.jsxs)(a.p,{children:["Hudi 1.1 eliminates expensive Avro conversions by processing Flink's native ",(0,n.jsx)(a.code,{children:"RowData"})," format directly, enabling zero-copy operations throughout the pipeline. This automatic change (no configuration required) delivers 2-3x improvement in write and read performance on average compared to Hudi 1.0."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"flink throughput chart",src:t(38326).A+"",width:"960",height:"540"})}),"\n",(0,n.jsx)(a.p,{children:"The above shows a benchmark that inserted 500 million records with a schema of 1 STRING and 10 BIGINT fields: Hudi 1.1 achieved 235.3k records per second and Hudi 1.0 67k records per second\u2014over 3 times higher throughput."}),"\n",(0,n.jsx)(a.h3,{id:"buffer-sort",children:"Buffer Sort"}),"\n",(0,n.jsxs)(a.p,{children:["For append-only tables, Hudi 1.1 introduces in-memory buffer sorting that pre-sorts records before flushing to Parquet. This delivers 15-30% better compression and faster queries through better min/max filtering. You can enable this feature with ",(0,n.jsx)(a.code,{children:"write.buffer.sort.enabled=true"})," and specify sort keys via ",(0,n.jsx)(a.code,{children:"write.buffer.sort.keys"}),' (e.g., "timestamp,event_type"). You may also adjust the buffer size for sorting via ',(0,n.jsx)(a.code,{children:"write.buffer.size"})," (default 1000 records)."]}),"\n",(0,n.jsx)(a.h2,{id:"new-integration-apache-polaris-incubating",children:"New Integration: Apache Polaris (Incubating)"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.a,{href:"https://polaris.apache.org/",children:"Polaris (incubating)"})," is an open-source catalog for lakehouse platforms that provides multi-engine interoperability and unified governance across diverse table formats and query engines. Its key feature is enabling data teams to use multiple engines\u2014Spark, Trino, Dremio, Flink, Presto\u2014on a single copy of data with consistent metadata, governed openly by a diverse committee including Snowflake, AWS, Google Cloud, Azure, and others to prevent vendor lock-in."]}),"\n",(0,n.jsxs)(a.p,{children:["Hudi 1.1 introduces ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/catalog_polaris",children:"native integration with Polaris"})," (pending a Polaris release that includes ",(0,n.jsx)(a.a,{href:"https://github.com/apache/polaris/pull/1862",children:"this PR"}),"), allowing users to register Hudi tables in the Polaris catalog and query them from any Polaris-compatible engine, simplifying multi-engine workflows and providing centralized role-based access control that works uniformly across S3, Azure Blob Storage, and Google Cloud Storage."]}),"\n",(0,n.jsx)(a.h2,{id:"whats-nextjoin-us-in-building-the-future",children:"What\u2019s Next\u2014Join Us in Building the Future"}),"\n",(0,n.jsx)(a.p,{children:"The future of Hudi is incredibly exciting, and we're building it together with a vibrant, global community of contributors. Building on the strong foundation of 1.1, we're actively developing transformative AI/ML-focused capabilities for Hudi 1.2 and beyond\u2014unstructured data types and column groups for efficient storage of embeddings and documents, Lance, Vortex, blob-optimized Parquet support, and vector search capabilities for lakehouse tables. This is just the beginning\u2014we're reimagining what's possible in the lakehouse, from multi-format interoperability to next-generation AI/ML workloads, and we need your ideas, code, and creativity to make it happen."}),"\n",(0,n.jsxs)(a.p,{children:["Join us in building the future. Check out the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-1.1.0",children:"1.1 release notes"})," to get started, join our ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/slack/",children:"Slack space"}),", follow us on ",(0,n.jsx)(a.a,{href:"https://www.linkedin.com/company/apache-hudi",children:"LinkedIn"})," and ",(0,n.jsx)(a.a,{href:"http://x.com/apachehudi",children:"X (twitter)"}),", and subscribe (send an empty email) to the ",(0,n.jsx)(a.a,{href:"mailto:dev@hudi.apache.org",children:"mailing list"}),"\u2014let's build the next generation of Hudi together."]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},49076:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2020/06/16/Apache-Hudi-grows-cloud-data-lake-maturity","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-06-16-Apache-Hudi-grows-cloud-data-lake-maturity.mdx","source":"@site/blog/2020-06-16-Apache-Hudi-grows-cloud-data-lake-maturity.mdx","title":"Apache Hudi grows cloud data lake maturity","description":"Redirecting... please wait!!","date":"2020-06-16T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"techtarget","permalink":"/blog/tags/techtarget"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Sean Michael Kerner","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Apache Hudi grows cloud data lake maturity","authors":[{"name":"Sean Michael Kerner"}],"category":"blog","image":"/assets/images/blog/2020-06-16-Apache-Hudi-grows-cloud-data-lake-maturity.jpeg","tags":["blog","techtarget"]},"unlisted":false,"prevItem":{"title":"PrestoDB and Apache Hudi","permalink":"/blog/2020/08/04/PrestoDB-and-Apache-Hudi"},"nextItem":{"title":"Building a Large-scale Transactional Data Lake at Uber Using Apache Hudi","permalink":"/blog/2020/06/09/Building-a-Large-scale-Transactional-Data-Lake-at-Uber-Using-Apache-Hudi"}}')},49462:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/04/24/understanding-apache-hudi-consistency-model-part-3","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-04-24-understanding-apache-hudi-consistency-model-part-3.mdx","source":"@site/blog/2024-04-24-understanding-apache-hudi-consistency-model-part-3.mdx","title":"Understanding Apache Hudi\'s Consistency Model Part 3","description":"Redirecting... please wait!!","date":"2024-04-24T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"tla+ specification","permalink":"/blog/tags/tla-specification"},{"inline":true,"label":"consistency","permalink":"/blog/tags/consistency"},{"inline":true,"label":"concurrency control","permalink":"/blog/tags/concurrency-control"},{"inline":true,"label":"multi writer","permalink":"/blog/tags/multi-writer"},{"inline":true,"label":"monotonic timestamp","permalink":"/blog/tags/monotonic-timestamp"},{"inline":true,"label":"jack-vanlightly","permalink":"/blog/tags/jack-vanlightly"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Jack Vanlightly","key":null,"page":null}],"frontMatter":{"title":"Understanding Apache Hudi\'s Consistency Model Part 3","author":"Jack Vanlightly","category":"blog","image":"/assets/images/blog/2024-04-24-understanding-apache-hudi-consistency-model-part-3.png","tags":["blog","apache hudi","tla+ specification","consistency","concurrency control","multi writer","monotonic timestamp","jack-vanlightly"]},"unlisted":false,"prevItem":{"title":"Understanding Apache Hudi\'s Consistency Model Part 2","permalink":"/blog/2024/04/24/understanding-apache-hudi-consistency-model-part-2"},"nextItem":{"title":"Build Real Time Streaming Pipeline with Kinesis, Apache Flink and Apache Hudi with Hands-on","permalink":"/blog/2024/04/21/build-real-time-streaming-pipeline-with-kinesis-apache-flink-and-apache-hudi"}}')},49683:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/10/17/Get-started-with-Apache-Hudi-using-AWS","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-10-17-Get-started-with-Apache-Hudi-using-AWS.mdx","source":"@site/blog/2022-10-17-Get-started-with-Apache-Hudi-using-AWS.mdx","title":"Get started with Apache Hudi using AWS Glue by implementing key design concepts \u2013 Part 1","description":"Redirecting... please wait!!","date":"2022-10-17T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"bulk-insert","permalink":"/blog/tags/bulk-insert"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"Amit Maindola","socials":{},"key":null,"page":null},{"name":"Srinivas Kandi","socials":{},"key":null,"page":null},{"name":"Mitesh Patel","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Get started with Apache Hudi using AWS Glue by implementing key design concepts \u2013 Part 1","authors":[{"name":"Amit Maindola"},{"name":"Srinivas Kandi"},{"name":"Mitesh Patel"}],"category":"blog","image":"/assets/images/blog/2022-10-17-Get_started_with_apache_hudi_using_glue.jpeg","tags":["how-to","bulk-insert","amazon"]},"unlisted":false,"prevItem":{"title":"How Hudl built a cost-optimized AWS Glue pipeline with Apache Hudi datasets","permalink":"/blog/2022/11/10/How-Hudl-built-a-cost-optimized-AWS-Glue-pipeline-with-Apache-Hudi-datasets"},"nextItem":{"title":"What, Why and How : Apache Hudi\u2019s Bloom Index","permalink":"/blog/2022/10/08/what-why-and-how-apache-hudis-bloom-index"}}')},49834:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(24672),n=t(74848),s=t(28453),r=t(9230);const o={title:"How GE Aviation built cloud-native data pipelines at enterprise scale using the AWS platform",authors:[{name:"Alcuin Weidus"},{name:"Suresh Patnam"}],category:"blog",image:"/assets/images/blog/2021-11-16-ge-aviation-cloud-native-data-pipelines.png",tags:["use-case","analytics at scale","amazon"]},l=void 0,d={authorsImageUrls:[void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/how-ge-aviation-built-cloud-native-data-pipelines-at-enterprise-scale-using-the-aws-platform/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},49887:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/ts-based-cdc-30ce5c2462ea39b02dbf9a93467a360a.png"},49938:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hstck_new-a0f2451aad8bf4e2003f1efb98c5e179.png"},49951:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(44854),n=t(74848),s=t(28453);const r={title:"How nClouds Helps Accelerate Data Delivery with Apache Hudi on Amazon EMR",excerpt:"Solution to set up a new data and analytics platform using Apache Hudi on Amazon EMR and other managed services, including Amazon QuickSight for data visualization.",author:"nclouds",category:"blog",image:"/assets/images/blog/2020-10-06-cdc-solution-using-hudi-by-nclouds.jpg",tags:["blog","apache flink","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[];function c(e){const a={a:"a",p:"p",...(0,s.R)(),...e.components};return(0,n.jsxs)(a.p,{children:["This ",(0,n.jsx)(a.a,{href:"https://aws.amazon.com/blogs/apn/how-nclouds-helps-accelerate-data-delivery-with-apache-hudi-on-amazon-emr/",children:"blog"})," published by nClouds in partnership with AWS shows how to build a CDC pipeline using Apache Hudi on Amazon EMR and other managed services like Amazon RDS and AWS DMS, including Amazon QuickSight for data visualization."]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},49969:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/01/05/Apache-Hudi-From-Zero-To-One-blog-8","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-01-05-Apache-Hudi-From-Zero-To-One-blog-8.mdx","source":"@site/blog/2024-01-05-Apache-Hudi-From-Zero-To-One-blog-8.mdx","title":"Apache Hudi: From Zero To One (8/10)","description":"Redirecting... please wait!!","date":"2024-01-05T00:00:00.000Z","tags":[{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"spark","permalink":"/blog/tags/spark"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"course","permalink":"/blog/tags/course"},{"inline":true,"label":"tutorial","permalink":"/blog/tags/tutorial"},{"inline":true,"label":"datumagic","permalink":"/blog/tags/datumagic"},{"inline":true,"label":"data lake","permalink":"/blog/tags/data-lake"},{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"apache spark","permalink":"/blog/tags/apache-spark"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Shiyan Xu","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi: From Zero To One (8/10)","excerpt":"Read and process incrementally","author":"Shiyan Xu","category":"blog","image":"/assets/images/blog/2024-01-05-Apache-Hudi-From-Zero-To-One-blog-8.png","tags":["hudi","spark","blog","course","tutorial","datumagic","data lake","lakehouse","apache hudi","apache spark"]},"unlisted":false,"prevItem":{"title":"Introduction to Apache Hudi","permalink":"/blog/2024/01/09/introduction-to-apache-hudi"},"nextItem":{"title":"Small Talk about Apache Hudi","permalink":"/blog/2024/01/05/Small-Talk-about-Apache-Hudi"}}')},50051:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(89366),n=t(74848),s=t(28453),r=t(9230);const o={title:"Learn how to read Hudi data with AWS Glue Ray using Daft (No Spark)",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2024-05-07-learn-how-read-hudi-data-aws-glue-ray-using-daft-spark.png",tags:["blog","apache hudi","aws glue","ray","daft","linkedin"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.linkedin.com/pulse/learn-how-read-hudi-data-aws-glue-ray-using-daft-spark-soumil-shah-kycbe/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},50495:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/02/06/Building-an-Open-Source-Data-Lake-House-with-Hudi-Postgres-Hive-Metastore-Minio-and-StarRocks","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-02-06-Building-an-Open-Source-Data-Lake-House-with-Hudi-Postgres-Hive-Metastore-Minio-and-StarRocks.mdx","source":"@site/blog/2024-02-06-Building-an-Open-Source-Data-Lake-House-with-Hudi-Postgres-Hive-Metastore-Minio-and-StarRocks.mdx","title":"Building an Open Source Data Lake House with Hudi, Postgres Hive Metastore, Minio, and StarRocks","description":"Redirecting... please wait!!","date":"2024-02-06T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"linkedin","permalink":"/blog/tags/linkedin"},{"inline":true,"label":"beginner","permalink":"/blog/tags/beginner"},{"inline":true,"label":"apache spark","permalink":"/blog/tags/apache-spark"},{"inline":true,"label":"apache hive","permalink":"/blog/tags/apache-hive"},{"inline":true,"label":"hive metastore","permalink":"/blog/tags/hive-metastore"},{"inline":true,"label":"minio","permalink":"/blog/tags/minio"},{"inline":true,"label":"starrocks","permalink":"/blog/tags/starrocks"},{"inline":true,"label":"docker","permalink":"/blog/tags/docker"},{"inline":true,"label":"python","permalink":"/blog/tags/python"},{"inline":true,"label":"postgres","permalink":"/blog/tags/postgres"},{"inline":true,"label":"postgresql","permalink":"/blog/tags/postgresql"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Soumil Shah","key":null,"page":null}],"frontMatter":{"title":"Building an Open Source Data Lake House with Hudi, Postgres Hive Metastore, Minio, and StarRocks","excerpt":"Building an Open Source Data Lake House with Hudi, Postgres Hive Metastore, Minio, and StarRocks","author":"Soumil Shah","category":"blog","image":"/assets/images/blog/2024-02-06-Building-an-Open-Source-Data-Lake-House-with-Hudi-Postgres-Hive-Metastore-Minio-and-StarRocks.png","tags":["blog","apache hudi","linkedin","beginner","apache spark","apache hive","hive metastore","minio","starrocks","docker","python","postgres","postgresql"]},"unlisted":false,"prevItem":{"title":"How a POC became a production-ready Hudi data lakehouse through close team collaboration","permalink":"/blog/2024/02/12/How-a-POC-became-a-production-ready-Hudi-data-lakehouse-through-close-team-collaboration"},"nextItem":{"title":"Combine Transactional Integrity and Data Lake Operations with YugabyteDB and Apache Hudi","permalink":"/blog/2024/02/06/Combine-Transactional-Integrity-and-Data-Lake-Operations-with-YugabyteDB-and-Apache-Hudi"}}')},50506:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(12610),n=t(74848),s=t(28453),r=t(9230);const o={title:"Moving Large Tables from Snowflake to S3 Using the COPY INTO Command and Hudi Bootstrapping to Build Data Lakes | Hands-On Labs",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2024-10-26-moving-large-tables-from-snowflake-to-s3-using-the-copy-into-command-and-hudi.png",tags:["blog","Apache Hudi","aws s3","bootstrap","linkedin"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.linkedin.com/pulse/moving-large-tables-from-snowflake-s3-using-copy-command-soumil-shah-csdse/?trackingId=8qFtCUc3R7CAo%2BP883rgUA%3D%3D",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},50790:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/log-based-cdc-92eff429e89653b892b63f1af3485ac6.png"},50887:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/08/24/Implementation-of-SCD-2-with-Apache-Hudi-and-Spark","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-08-24-Implementation-of-SCD-2-with-Apache-Hudi-and-Spark.mdx","source":"@site/blog/2022-08-24-Implementation-of-SCD-2-with-Apache-Hudi-and-Spark.mdx","title":"Implementation of SCD-2 (Slowly Changing Dimension) with Apache Hudi & Spark","description":"Redirecting... please wait!!","date":"2022-08-24T00:00:00.000Z","tags":[{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"scd2","permalink":"/blog/tags/scd-2"},{"inline":true,"label":"walmartglobaltech","permalink":"/blog/tags/walmartglobaltech"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Jayasheel Kalgal","socials":{},"key":null,"page":null},{"name":"Esha Dhing","socials":{},"key":null,"page":null},{"name":"Prashant Mishra","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Implementation of SCD-2 (Slowly Changing Dimension) with Apache Hudi & Spark","authors":[{"name":"Jayasheel Kalgal"},{"name":"Esha Dhing"},{"name":"Prashant Mishra"}],"category":"blog","image":"/assets/images/blog/2022-08-24_implementation_of_scd_2_with_hudi_and_spark.jpeg","tags":["use-case","scd2","walmartglobaltech"]},"unlisted":false,"prevItem":{"title":"Data Lake / Lakehouse Guide: Powered by Data Lake Table Formats (Delta Lake, Iceberg, Hudi)","permalink":"/blog/2022/08/25/Data-Lake-Lakehouse-Guide-Powered-by-Data-Lake-Table-Formats-Delta-Lake-Iceberg-Hudi"},"nextItem":{"title":"Use Flink Hudi to Build a Streaming Data Lake Platform","permalink":"/blog/2022/08/12/Use-Flink-Hudi-to-Build-a-Streaming-Data-Lake-Platform"}}')},50895:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(55891),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi: From Zero To One (2/10)",excerpt:"Dive into read operation flow and query types",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2023-09-06-Apache-Hudi-From-Zero-To-One-blog-2.png",tags:["hudi","spark","blog","course","tutorial","datumagic","data lake","lakehouse","apache hudi","apache spark"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blog.datumagic.ai/p/apache-hudi-from-zero-to-one-210",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},51931:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(25101),n=t(74848),s=t(28453),r=t(9230);const o={title:"Hands-on with Apache Hudi and Spark",author:"Sanjeet Shukla",category:"blog",image:"/assets/images/blog/2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.png",tags:["blog","Apache Hudi","Apache Spark","devgenius"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blog.devgenius.io/hands-on-with-apache-hudi-ce45869b5eff",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},52369:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/09/09/use-apache-hudi-tables-in-athena-for-spark","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-09-09-use-apache-hudi-tables-in-athena-for-spark.mdx","source":"@site/blog/2024-09-09-use-apache-hudi-tables-in-athena-for-spark.mdx","title":"Use Apache Hudi tables in Athena for Spark","description":"Redirecting... please wait!!","date":"2024-09-09T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"athena","permalink":"/blog/tags/athena"},{"inline":true,"label":"amazon spark","permalink":"/blog/tags/amazon-spark"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Amazon","key":null,"page":null}],"frontMatter":{"title":"Use Apache Hudi tables in Athena for Spark","author":"Amazon","category":"blog","image":"/assets/images/blog/2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.png","tags":["blog","apache hudi","athena","amazon spark","amazon"]},"unlisted":false,"prevItem":{"title":"Comparing Apache Hudi, Apache Iceberg, and Delta Lake","permalink":"/blog/2024/09/11/comparing-apache-hudi-apache-iceberg-and-delta-lake"},"nextItem":{"title":"Developer Guide: How to Submit Hudi PySpark(Python) Jobs to EMR Serverless (7.1.0) with AWS Glue Hive MetaStore","permalink":"/blog/2024/09/04/developer-guide-how-to-submit-hudi-pyspark-python-jobs-to-emr-serverless"}}')},52371:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(8375),n=t(74848),s=t(28453),r=t(9230);const o={title:"Streaming DynamoDB Data into a Hudi Table: AWS Glue in Action",author:"Rahul Kumar",category:"blog",image:"/assets/images/blog/2024-10-14-streaming-dynamodb-data-into-a-hudi-table-aws-glue-in-action.png",tags:["how-to","Apache Hudi","amazon s3","aws glue","amazon kinesis","amazon dynamodb","antstack"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.antstack.com/blog/Streaming-DynamoDB-Data-into-a-Hudi-Table/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},52400:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(10601),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi: Revolutionizing Big Data Management for Real-Time Analytics",authors:[{name:"Dev Jain"}],category:"blog",image:"/assets/images/blog/2023-07-27-Apache-Hudi-Revolutionizing-Big-Data-Management-for-Real-Time-Analytics.png",tags:["blog","medium","hudi"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@devjain1299/apache-hudi-revolutionizing-big-data-management-for-real-time-analytics-5130808e067a",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},52816:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/07/11/what-is-a-data-lakehouse","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-07-11-what-is-a-data-lakehouse.md","source":"@site/blog/2024-07-11-what-is-a-data-lakehouse.md","title":"What is a Data Lakehouse & How does it Work?","description":"A data lakehouse is a hybrid data architecture that combines the best attributes of data warehouses and data lakes to address their respective limitations. This innovative approach to data management brings the transactional capabilities of data warehouses to cloud-based data lakes, offering scalability at lower costs.","date":"2024-07-11T00:00:00.000Z","tags":[{"inline":true,"label":"data lakehouse","permalink":"/blog/tags/data-lakehouse"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"Apache Iceberg","permalink":"/blog/tags/apache-iceberg"},{"inline":true,"label":"Delta Lake","permalink":"/blog/tags/delta-lake"},{"inline":true,"label":"Open Architecture","permalink":"/blog/tags/open-architecture"}],"readingTime":16.7,"hasTruncateMarker":false,"authors":[{"name":"Dipankar Mazumdar","key":null,"page":null}],"frontMatter":{"title":"What is a Data Lakehouse & How does it Work?","excerpt":"Explains the concept of the lakehouse architecture","author":"Dipankar Mazumdar","category":"blog","image":"/assets/images/blog/dlh_1200.png","tags":["data lakehouse","Apache Hudi","Apache Iceberg","Delta Lake","Open Architecture"]},"unlisted":false,"prevItem":{"title":"Understanding Data Lake Change Data Capture","permalink":"/blog/2024/07/30/data-lake-cdc"},"nextItem":{"title":"How to use Apache Hudi with Databricks","permalink":"/blog/2024/06/18/how-to-use-apache-hudi-with-databricks"}}')},52977:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig-2-PuppyGraph_Supported_Data_Sources-530816b1bc98047b2113f8f1fb791b8c.png"},52999:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/02/25/curious-engineering-facts-trace-agents-hudi-daft-1","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-02-25-curious-engineering-facts-trace-agents-hudi-daft-1.mdx","source":"@site/blog/2025-02-25-curious-engineering-facts-trace-agents-hudi-daft-1.mdx","title":"Curious Engineering Facts ( Trace Agents | Hudi| Daft : 1) : March Release 18 : 25","description":"Redirecting... please wait!!","date":"2025-02-25T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"daft","permalink":"/blog/tags/daft"},{"inline":true,"label":"trace agents","permalink":"/blog/tags/trace-agents"},{"inline":true,"label":"openai","permalink":"/blog/tags/openai"},{"inline":true,"label":"llm","permalink":"/blog/tags/llm"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Gayan Sanjeewa","key":null,"page":null}],"frontMatter":{"title":"Curious Engineering Facts ( Trace Agents | Hudi| Daft : 1) : March Release 18 : 25","author":"Gayan Sanjeewa","category":"blog","image":"/assets/images/blog/2025-02-25-curious-engineering-facts-trace-agents-hudi-daft-1.jpeg","tags":["blog","apache hudi","daft","trace agents","openai","llm","medium"]},"unlisted":false,"prevItem":{"title":"Record Mergers in Apache Hudi","permalink":"/blog/2025/03/03/record-mergers-in-hudi"},"nextItem":{"title":"Building a Lakehouse Architecture on AWS with Terraform","permalink":"/blog/2025/02/24/building-a-lakehouse-architecture-on-aws-with-terraform"}}')},53071:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig3-ce0dd7f4352540791995b4b6b9b5387e.png"},53135:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/12/31/indexing-in-apache-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-12-31-indexing-in-apache-hudi.mdx","source":"@site/blog/2024-12-31-indexing-in-apache-hudi.mdx","title":"Indexing in Apache Hudi","description":"Redirecting... please wait!!","date":"2024-12-31T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"indexing","permalink":"/blog/tags/indexing"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.11,"hasTruncateMarker":false,"authors":[{"name":"Sanjeet Shukla","key":null,"page":null}],"frontMatter":{"title":"Indexing in Apache Hudi","author":"Sanjeet Shukla","category":"blog","image":"/assets/images/blog/2024-12-31-indexing-in-apache-hudi.jpeg","tags":["blog","apache hudi","indexing","medium"]},"unlisted":false,"prevItem":{"title":"How to Use the New Hudi Streamer with Hudi 1.0.0 on EMR Serverless 7.5.0 | Hands-on Labs","permalink":"/blog/2025/01/05/how-use-new-hudi-streamer-100-emr-serverless-750-hands-on"},"nextItem":{"title":"The Architect\u2019s Guide to Open Table Formats and Object Storage","permalink":"/blog/2024/12/31/the-architects-guide-to-open-table-formats-and-object-storage"}}')},53186:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/09/22/Exploring-the-Architecture-of-Apache-Iceberg-Delta-Lake-and-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-09-22-Exploring-the-Architecture-of-Apache-Iceberg-Delta-Lake-and-Apache-Hudi.mdx","source":"@site/blog/2023-09-22-Exploring-the-Architecture-of-Apache-Iceberg-Delta-Lake-and-Apache-Hudi.mdx","title":"Exploring the Architecture of Apache Iceberg, Delta Lake, and Apache Hudi","description":"Redirecting... please wait!!","date":"2023-09-22T00:00:00.000Z","tags":[{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"apache iceberg","permalink":"/blog/tags/apache-iceberg"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"delta lake","permalink":"/blog/tags/delta-lake"},{"inline":true,"label":"dremio","permalink":"/blog/tags/dremio"},{"inline":true,"label":"architecture","permalink":"/blog/tags/architecture"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Alex Merced","key":null,"page":null}],"frontMatter":{"title":"Exploring the Architecture of Apache Iceberg, Delta Lake, and Apache Hudi","excerpt":"Exploring the Architecture of Apache Iceberg, Delta Lake, and Apache Hudi","author":"Alex Merced","category":"blog","image":"/assets/images/blog/2023-09-22-Exploring-the-Architecture-of-Apache-Iceberg-Delta-Lake-and-Apache-Hudi.png","tags":["apache hudi","apache iceberg","blog","delta lake","dremio","architecture"]},"unlisted":false,"prevItem":{"title":"Apache Hudi: From Zero To One (4/10)","permalink":"/blog/2023/09/27/Apache-Hudi-From-Zero-To-One-blog-4"},"nextItem":{"title":"A Beginner\u2019s Guide to Apache Hudi with PySpark \u2014 Part 1 of 2","permalink":"/blog/2023/09/19/A-Beginners-Guide-to-Apache-Hudi-with-PySpark-Part-1-of-2"}}')},53233:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig1-7461f8a910c9f7c87745a4a5e15c3498.png"},53338:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi-design-diagrams_-_Page_2_1-d998a263b380ed3357fcb2006ffb5bfe.png"},53492:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/04/21/build-real-time-streaming-pipeline-with-kinesis-apache-flink-and-apache-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-04-21-build-real-time-streaming-pipeline-with-kinesis-apache-flink-and-apache-hudi.mdx","source":"@site/blog/2024-04-21-build-real-time-streaming-pipeline-with-kinesis-apache-flink-and-apache-hudi.mdx","title":"Build Real Time Streaming Pipeline with Kinesis, Apache Flink and Apache Hudi with Hands-on","description":"Redirecting... please wait!!","date":"2024-04-21T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"apache flink","permalink":"/blog/tags/apache-flink"},{"inline":true,"label":"amazon kinesis","permalink":"/blog/tags/amazon-kinesis"},{"inline":true,"label":"amazon s3","permalink":"/blog/tags/amazon-s-3"},{"inline":true,"label":"streaming ingestion","permalink":"/blog/tags/streaming-ingestion"},{"inline":true,"label":"real-time datalake","permalink":"/blog/tags/real-time-datalake"},{"inline":true,"label":"incremental processing","permalink":"/blog/tags/incremental-processing"},{"inline":true,"label":"devgenius","permalink":"/blog/tags/devgenius"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Md Shahid Afridi P","key":null,"page":null}],"frontMatter":{"title":"Build Real Time Streaming Pipeline with Kinesis, Apache Flink and Apache Hudi with Hands-on","author":"Md Shahid Afridi P","category":"blog","image":"/assets/images/blog/2024-04-21-build-real-time-streaming-pipeline-with-kinesis-apache-flink-and-apache-hudi.png","tags":["blog","apache hudi","apache flink","amazon kinesis","amazon s3","streaming ingestion","real-time datalake","incremental processing","devgenius"]},"unlisted":false,"prevItem":{"title":"Understanding Apache Hudi\'s Consistency Model Part 3","permalink":"/blog/2024/04/24/understanding-apache-hudi-consistency-model-part-3"},"nextItem":{"title":"Apache Hudi: From Zero To One (10/10)","permalink":"/blog/2024/04/13/Apache-Hudi-From-Zero-To-One-blog-10"}}')},53632:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(64035),n=t(74848),s=t(28453),r=t(9230);const o={title:"The Future of Data Lakehouses: A Fireside Chat with Vinoth Chandar - Founder CEO Onehouse & PMC Chair of Apache Hudi",author:"Ananth Packkildurai",category:"blog",image:"/assets/images/blog/2025-01-08-the-future-of-data-lakehouses-a-fireside.jpg",tags:["blog","apache hudi","data lakehouse","lakehouse","dataengineeringweekly"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.dataengineeringweekly.com/p/the-future-of-data-lakehouses-a-fireside",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},53640:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/01/05/Small-Talk-about-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-01-05-Small-Talk-about-Apache-Hudi.mdx","source":"@site/blog/2024-01-05-Small-Talk-about-Apache-Hudi.mdx","title":"Small Talk about Apache Hudi","description":"Redirecting... please wait!!","date":"2024-01-05T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"linkedin","permalink":"/blog/tags/linkedin"},{"inline":true,"label":"beginner","permalink":"/blog/tags/beginner"},{"inline":true,"label":"inserts","permalink":"/blog/tags/inserts"},{"inline":true,"label":"upserts","permalink":"/blog/tags/upserts"},{"inline":true,"label":"cow","permalink":"/blog/tags/cow"},{"inline":true,"label":"mor","permalink":"/blog/tags/mor"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Ashok Kumar Kunkala","key":null,"page":null}],"frontMatter":{"title":"Small Talk about Apache Hudi","excerpt":"Small Talk about Apache Hudi","author":"Ashok Kumar Kunkala","category":"blog","image":"/assets/images/blog/2024-01-05-Small-Talk-about-Apache-Hudi.png","tags":["blog","apache hudi","linkedin","beginner","inserts","upserts","cow","mor"]},"unlisted":false,"prevItem":{"title":"Apache Hudi: From Zero To One (8/10)","permalink":"/blog/2024/01/05/Apache-Hudi-From-Zero-To-One-blog-8"},"nextItem":{"title":"Build a federated query solution with Apache Doris, Apache Flink, and Apache Hudi","permalink":"/blog/2024/01/02/Build-a-federated-query-solution-with-Apache-Doris-Apache-Flink-and-Apache-Hudi"}}')},53697:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/08/03/Create-an-Apache-Hudi-based-near-real-time-transactional-data lake-using-AWS-DMS-Amazon-Kinesis-AWS-Glue-streaming-ETL-and-data-visualization-using-Amazon-QuickSight","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-08-03-Create-an-Apache-Hudi-based-near-real-time-transactional-data lake-using-AWS-DMS-Amazon-Kinesis-AWS-Glue-streaming-ETL-and-data-visualization-using-Amazon-QuickSight.mdx","source":"@site/blog/2023-08-03-Create-an-Apache-Hudi-based-near-real-time-transactional-data lake-using-AWS-DMS-Amazon-Kinesis-AWS-Glue-streaming-ETL-and-data-visualization-using-Amazon-QuickSight.mdx","title":"Create an Apache Hudi-based-near-real-time transactional data lake using AWS DMS, Amazon Kinesis, AWS Glue streaming ETL, and data visualization using Amazon QuickSight","description":"Redirecting... please wait!!","date":"2023-08-03T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"cdc","permalink":"/blog/tags/cdc"},{"inline":true,"label":"change data capture","permalink":"/blog/tags/change-data-capture"},{"inline":true,"label":"upserts","permalink":"/blog/tags/upserts"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.22,"hasTruncateMarker":false,"authors":[{"name":"Raj Ramasubbu","socials":{},"key":null,"page":null},{"name":"Sundeep Kumar","socials":{},"key":null,"page":null},{"name":"Rahul Sonawane","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Create an Apache Hudi-based-near-real-time transactional data lake using AWS DMS, Amazon Kinesis, AWS Glue streaming ETL, and data visualization using Amazon QuickSight","authors":[{"name":"Raj Ramasubbu"},{"name":"Sundeep Kumar"},{"name":"Rahul Sonawane"}],"category":"blog","image":"/assets/images/blog/2023-08-03-near-realtime-trans-datalake-aws-dms-kinesis.png","tags":["how-to","cdc","change data capture","upserts","amazon"]},"unlisted":false,"prevItem":{"title":"Apache Hudi on AWS Glue: A Step-by-Step Guide","permalink":"/blog/2023/08/03/Apache-Hudi-on-AWS-Glue-A-Step-by-Step-Guide"},"nextItem":{"title":"Data lake Table formats: Apache Iceberg vs Apache Hudi vs Delta lake","permalink":"/blog/2023/08/03/Data-lake-Table-formats-Apache-Iceberg-vs-Apache-Hudi-vs-Delta-lake"}}')},53825:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/09/14/Ubers-Big-Data-Revolution-From-MySQL-to-Hadoop-and-Beyond","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-09-14-Ubers-Big-Data-Revolution-From-MySQL-to-Hadoop-and-Beyond.mdx","source":"@site/blog/2024-09-14-Ubers-Big-Data-Revolution-From-MySQL-to-Hadoop-and-Beyond.mdx","title":"Uber\u2019s Big Data Revolution: From MySQL to Hadoop and Beyond","description":"Redirecting... please wait!!","date":"2024-09-14T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"substack","permalink":"/blog/tags/substack"}],"readingTime":0.11,"hasTruncateMarker":false,"authors":[{"name":"Vu Trinh","key":null,"page":null}],"frontMatter":{"title":"Uber\u2019s Big Data Revolution: From MySQL to Hadoop and Beyond","author":"Vu Trinh","category":"blog","image":"/assets/images/blog/2024-09-14-Ubers-Big-Data-Revolution-From-MySQL-to-Hadoop-and-Beyond.png","tags":["blog","apache hudi","use-case","substack"]},"unlisted":false,"prevItem":{"title":"How Apache Hudi transformed Yuno\u2019s data lake","permalink":"/blog/2024/09/17/how-apache-hudi-transformed-yuno-s-data-lake"},"nextItem":{"title":"Comparing Apache Hudi, Apache Iceberg, and Delta Lake","permalink":"/blog/2024/09/11/comparing-apache-hudi-apache-iceberg-and-delta-lake"}}')},53919:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2019/09/09/ingesting-database-changes","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2019-09-09-ingesting-database-changes.md","source":"@site/blog/2019-09-09-ingesting-database-changes.md","title":"Ingesting Database changes via Sqoop/Hudi","description":"Very simple in just 2 steps.","date":"2019-09-09T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":0.7,"hasTruncateMarker":true,"authors":[{"name":"vinoth","key":null,"page":null}],"frontMatter":{"title":"Ingesting Database changes via Sqoop/Hudi","excerpt":"Learn how to ingesting changes from a HUDI dataset using Sqoop/Hudi","author":"vinoth","category":"blog","tags":["how-to","apache hudi"]},"unlisted":false,"prevItem":{"title":"Hudi On Hops","permalink":"/blog/2019/10/22/Hudi-On-Hops"},"nextItem":{"title":"Registering sample dataset to Hive via beeline","permalink":"/blog/2019/05/14/registering-dataset-to-hive"}}')},53936:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/04/13/Apache-Hudi-From-Zero-To-One-blog-10","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-04-13-Apache-Hudi-From-Zero-To-One-blog-10.mdx","source":"@site/blog/2024-04-13-Apache-Hudi-From-Zero-To-One-blog-10.mdx","title":"Apache Hudi: From Zero To One (10/10)","description":"Redirecting... please wait!!","date":"2024-04-13T00:00:00.000Z","tags":[{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"spark","permalink":"/blog/tags/spark"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"course","permalink":"/blog/tags/course"},{"inline":true,"label":"tutorial","permalink":"/blog/tags/tutorial"},{"inline":true,"label":"datumagic","permalink":"/blog/tags/datumagic"},{"inline":true,"label":"data lake","permalink":"/blog/tags/data-lake"},{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"apache spark","permalink":"/blog/tags/apache-spark"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Shiyan Xu","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi: From Zero To One (10/10)","excerpt":"Becoming \'One\' - the upcoming 1.0 highlights","author":"Shiyan Xu","category":"blog","image":"/assets/images/blog/2024-04-13-Apache-Hudi-From-Zero-To-One-blog-10.jpg","tags":["hudi","spark","blog","course","tutorial","datumagic","data lake","lakehouse","apache hudi","apache spark"]},"unlisted":false,"prevItem":{"title":"Build Real Time Streaming Pipeline with Kinesis, Apache Flink and Apache Hudi with Hands-on","permalink":"/blog/2024/04/21/build-real-time-streaming-pipeline-with-kinesis-apache-flink-and-apache-hudi"},"nextItem":{"title":"Hands-On Guide: Reading Data from Hudi Tables Incrementally, Joining with Delta Tables using HudiStreamer and SQL-Based Transformer","permalink":"/blog/2024/04/03/hands-on-guide-reading-data-from-hudi-tables-joining-delta"}}')},54258:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/01/01/From-Data-lake-to-Microservices-Unleashing-the-Power-of-Apache-Hudi-Record-Level-Index-with-FastAPI-and-Spark-Connect","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-01-01-From-Data-lake-to-Microservices-Unleashing-the-Power-of-Apache-Hudi-Record-Level-Index-with-FastAPI-and-Spark-Connect.mdx","source":"@site/blog/2024-01-01-From-Data-lake-to-Microservices-Unleashing-the-Power-of-Apache-Hudi-Record-Level-Index-with-FastAPI-and-Spark-Connect.mdx","title":"From Data lake to Microservices: Unleashing the Power of Apache Hudi\'s Record Level Index with FastAPI and Spark Connect","description":"Redirecting... please wait!!","date":"2024-01-01T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"linkedin","permalink":"/blog/tags/linkedin"},{"inline":true,"label":"beginner","permalink":"/blog/tags/beginner"},{"inline":true,"label":"apache spark","permalink":"/blog/tags/apache-spark"},{"inline":true,"label":"record level index","permalink":"/blog/tags/record-level-index"},{"inline":true,"label":"pyspark","permalink":"/blog/tags/pyspark"},{"inline":true,"label":"upserts","permalink":"/blog/tags/upserts"},{"inline":true,"label":"FastAPI","permalink":"/blog/tags/fast-api"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"Soumil Shah","key":null,"page":null}],"frontMatter":{"title":"From Data lake to Microservices: Unleashing the Power of Apache Hudi\'s Record Level Index with FastAPI and Spark Connect","excerpt":"From Data lake to Microservices: Unleashing the Power of Apache Hudi\'s Record Level Index with FastAPI and Spark Connect","author":"Soumil Shah","category":"blog","image":"/assets/images/blog/2024-01-01-From-Data-lake-to-Microservices-Unleashing-the-Power-of-Apache-Hudi-Record-Level-Index-with-FastAPI-and-Spark-Connect.png","tags":["blog","apache hudi","linkedin","beginner","apache spark","record level index","pyspark","upserts","FastAPI"]},"unlisted":false,"prevItem":{"title":"Build a federated query solution with Apache Doris, Apache Flink, and Apache Hudi","permalink":"/blog/2024/01/02/Build-a-federated-query-solution-with-Apache-Doris-Apache-Flink-and-Apache-Hudi"},"nextItem":{"title":"Apache Hudi 2023: A Year In Review","permalink":"/blog/2023/12/28/apache-hudi-2023-a-year-in-review"}}')},54262:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(58113),n=t(74848),s=t(28453),r=t(9230);const o={title:"How to query data in Apache Hudi using StarRocks",authors:[{name:"Albert Wong"}],category:"blog",image:"/assets/images/blog/2023-06-20-How-to-query-data-in-Apache-Hudi-using-StarRocks.png",tags:["blog","starrocks","queries","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@atwong/how-to-query-data-in-apache-hudi-using-starrocks-bf0336eaa817",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},54386:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/08/18/virtual-keys","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-08-18-virtual-keys.md","source":"@site/blog/2021-08-18-virtual-keys.md","title":"Adding support for Virtual Keys in Hudi","description":"Apache Hudi helps you build and manage data lakes with different table types, config knobs to cater to everyone\'s need.","date":"2021-08-18T00:00:00.000Z","tags":[{"inline":true,"label":"design","permalink":"/blog/tags/design"},{"inline":true,"label":"metadata","permalink":"/blog/tags/metadata"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":5.35,"hasTruncateMarker":true,"authors":[{"name":"shivnarayan","key":null,"page":null}],"frontMatter":{"title":"Adding support for Virtual Keys in Hudi","excerpt":"Supporting Virtual keys in Hudi for reducing storage overhead","author":"shivnarayan","category":"blog","tags":["design","metadata","apache hudi"]},"unlisted":false,"prevItem":{"title":"Improving Marker Mechanism in Apache Hudi","permalink":"/blog/2021/08/18/improving-marker-mechanism"},"nextItem":{"title":"Schema evolution with DeltaStreamer using KafkaSource","permalink":"/blog/2021/08/16/kafka-custom-deserializer"}}')},54388:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/02/06/Combine-Transactional-Integrity-and-Data-Lake-Operations-with-YugabyteDB-and-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-02-06-Combine-Transactional-Integrity-and-Data-Lake-Operations-with-YugabyteDB-and-Apache-Hudi.mdx","source":"@site/blog/2024-02-06-Combine-Transactional-Integrity-and-Data-Lake-Operations-with-YugabyteDB-and-Apache-Hudi.mdx","title":"Combine Transactional Integrity and Data Lake Operations with YugabyteDB and Apache Hudi","description":"Redirecting... please wait!!","date":"2024-02-06T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"ACID","permalink":"/blog/tags/acid"},{"inline":true,"label":"transactions","permalink":"/blog/tags/transactions"},{"inline":true,"label":"real-time datalake","permalink":"/blog/tags/real-time-datalake"},{"inline":true,"label":"cdc","permalink":"/blog/tags/cdc"},{"inline":true,"label":"etl","permalink":"/blog/tags/etl"},{"inline":true,"label":"yugabyte","permalink":"/blog/tags/yugabyte"}],"readingTime":0.11,"hasTruncateMarker":false,"authors":[{"name":"Balachandar Seetharaman","key":null,"page":null}],"frontMatter":{"title":"Combine Transactional Integrity and Data Lake Operations with YugabyteDB and Apache Hudi","author":"Balachandar Seetharaman","category":"blog","image":"/assets/images/blog/2024-02-06-Combine-Transactional-Integrity-and-Data-Lake-Operations-with-YugabyteDB-and-Apache-Hudi.png","tags":["blog","apache hudi","ACID","transactions","real-time datalake","cdc","etl","yugabyte"]},"unlisted":false,"prevItem":{"title":"Building an Open Source Data Lake House with Hudi, Postgres Hive Metastore, Minio, and StarRocks","permalink":"/blog/2024/02/06/Building-an-Open-Source-Data-Lake-House-with-Hudi-Postgres-Hive-Metastore-Minio-and-StarRocks"},"nextItem":{"title":"Apache Hudi: Managing Partition on a petabyte-scale table","permalink":"/blog/2024/02/04/Apache-Hudi-Managing-Partition-on-a-petabyte-scale-table"}}')},54422:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/06/09/Singificant-queries-speedup-from-Hudi-Column-Stats-Index-and-Data-Skipping-features","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-06-09-Singificant-queries-speedup-from-Hudi-Column-Stats-Index-and-Data-Skipping-features.mdx","source":"@site/blog/2022-06-09-Singificant-queries-speedup-from-Hudi-Column-Stats-Index-and-Data-Skipping-features.mdx","title":"Hudi\u2019s Column Stats Index and Data Skipping feature help speed up queries by an orders of magnitude!","description":"Redirecting... please wait!!","date":"2022-06-09T00:00:00.000Z","tags":[{"inline":true,"label":"design","permalink":"/blog/tags/design"},{"inline":true,"label":"indexing","permalink":"/blog/tags/indexing"},{"inline":true,"label":"data skipping","permalink":"/blog/tags/data-skipping"},{"inline":true,"label":"onehouse","permalink":"/blog/tags/onehouse"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"Alexey Kudinkin","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Hudi\u2019s Column Stats Index and Data Skipping feature help speed up queries by an orders of magnitude!","authors":[{"name":"Alexey Kudinkin"}],"category":"blog","image":"/assets/images/blog/2022-06-09-col-stats-and-data-skipping.png","tags":["design","indexing","data skipping","onehouse"]},"unlisted":false,"prevItem":{"title":"Apache Hudi vs Delta Lake - Transparent TPC-DS Lakehouse Performance Benchmarks","permalink":"/blog/2022/06/29/Apache-Hudi-vs-Delta-Lake-transparent-tpc-ds-lakehouse-performance-benchmarks"},"nextItem":{"title":"Asynchronous Indexing using Hudi","permalink":"/blog/2022/06/04/Asynchronous-Indexing-Using-Hudi"}}')},54425:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(21541),n=t(74848),s=t(28453),r=t(9230);const o={title:"Unlimited Big Data Exchange: A Wonderful Review of Apache DolphinScheduler & Hudi Hangzhou Meetup",authors:[{name:"Apache DolphinScheduler"}],category:"blog",image:"/assets/images/blog/2023-06-26-Unlimited-Big-Data-Exchange-A-Wonderful-Review-of-Apache-DolphinScheduler-and-Hudi-Hangzhou-Meetup.jpeg",tags:["blog","Apache DolphinScheduler","meetup","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@ApacheDolphinScheduler/unlimited-big-data-exchange-a-wonderful-review-of-apache-dolphinscheduler-hudi-hangzhou-meetup-4e4e7dae0f55",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},54437:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(21064),n=t(74848),s=t(28453);const r={title:"Building High-Performance Data Lake Using Apache Hudi and Alluxio at T3Go",excerpt:"How T3Go\u2019s high-performance data lake using Apache Hudi and Alluxio shortened the time for data ingestion into the lake by up to a factor of 2. Data analysts using Presto, Hudi, and Alluxio in conjunction to query data on the lake saw queries speed up by 10 times faster.",author:"t3go",category:"blog",image:"/assets/images/blog/2020-12-01-t3go-architecture.png",tags:["use-case","near real-time analytics","incremental processing","caching","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Building High-Performance Data Lake Using Apache Hudi and Alluxio at T3Go",id:"building-high-performance-data-lake-using-apache-hudi-and-alluxio-at-t3go",level:2},{value:"I. T3Go data lake Overview",id:"i-t3go-data-lake-overview",level:2},{value:"II. Efficient Near Real-time Analytics Using Hudi",id:"ii-efficient-near-real-time-analytics-using-hudi",level:2},{value:"Enable Near real time data ingestion and analysis",id:"enable-near-real-time-data-ingestion-and-analysis",level:3},{value:"Enable Incremental processing pipeline",id:"enable-incremental-processing-pipeline",level:3},{value:"Accessing Data using Hudi as a unified format",id:"accessing-data-using-hudi-as-a-unified-format",level:3},{value:"III. Efficient Data Caching Using Alluxio",id:"iii-efficient-data-caching-using-alluxio",level:2},{value:"Data lake ingestion",id:"data-lake-ingestion",level:3},{value:"Data analysis on the lake",id:"data-analysis-on-the-lake",level:3},{value:"Concurrent accesses across multiple storage systems",id:"concurrent-accesses-across-multiple-storage-systems",level:3},{value:"Microbenchmark",id:"microbenchmark",level:3},{value:"IV. Next Step",id:"iv-next-step",level:2},{value:"V. Conclusion",id:"v-conclusion",level:2}];function c(e){const a={a:"a",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.h2,{id:"building-high-performance-data-lake-using-apache-hudi-and-alluxio-at-t3go",children:"Building High-Performance Data Lake Using Apache Hudi and Alluxio at T3Go"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.a,{href:"https://www.t3go.cn/",children:"T3Go"}),"  is China\u2019s first platform for smart travel based on the Internet of Vehicles. In this article, Trevor Zhang and Vino Yang from T3Go describe the evolution of their data lake architecture, built on cloud-native or open-source technologies including Alibaba OSS, Apache Hudi, and Alluxio. Today, their data lake stores petabytes of data, supporting hundreds of pipelines and tens of thousands of tasks daily. It is essential for business units at T3Go including Data Warehouse, Internet of Vehicles, Order Dispatching, Machine Learning, and self-service query analysis."]}),"\n",(0,n.jsx)(a.p,{children:"In this blog, you will see how we slashed data ingestion time by half using Hudi and Alluxio. Furthermore, data analysts using Presto, Hudi, and Alluxio saw the queries speed up by 10 times. We built our data lake based on data orchestration for multiple stages of our data pipeline, including ingestion and analytics."}),"\n",(0,n.jsx)(a.h2,{id:"i-t3go-data-lake-overview",children:"I. T3Go data lake Overview"}),"\n",(0,n.jsx)(a.p,{children:"Prior to the data lake, different business units within T3Go managed their own data processing solutions, utilizing different storage systems, ETL tools, and data processing frameworks. Data for each became siloed from every other unit, significantly increasing cost and complexity. Due to the rapid business expansion of T3Go, this inefficiency became our engineering bottleneck."}),"\n",(0,n.jsxs)(a.p,{children:["We moved to a unified data lake solution based on Alibaba OSS, an object store similar to AWS S3, to provide a centralized location to store structured and unstructured data, following the design principles of  ",(0,n.jsx)(a.em,{children:"Multi-cluster Shared-data Architecture"}),"; all the applications access OSS storage as the source of truth, as opposed to different data silos. This architecture allows us to store the data as-is, without having to first structure the data, and run different types of analytics to guide better decisions, building dashboards and visualizations from big data processing, real-time analytics, and machine learning."]}),"\n",(0,n.jsx)(a.h2,{id:"ii-efficient-near-real-time-analytics-using-hudi",children:"II. Efficient Near Real-time Analytics Using Hudi"}),"\n",(0,n.jsx)(a.p,{children:"Our business in smart travel drives the need to process and analyze data in a near real-time manner. With a traditional data warehouse, we faced the following challenges:"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsx)(a.li,{children:"High overhead when updating due to long-tail latency"}),"\n",(0,n.jsx)(a.li,{children:"High cost of order analysis due to the long window of a business session"}),"\n",(0,n.jsx)(a.li,{children:"Reduced query accuracy due to late or ad-hoc updates"}),"\n",(0,n.jsx)(a.li,{children:"Unreliability in data ingestion pipeline"}),"\n",(0,n.jsx)(a.li,{children:"Data lost in the distributed data pipeline that cannot be reconciled"}),"\n",(0,n.jsx)(a.li,{children:"High latency to access data storage"}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"As a result, we adopted Apache Hudi on top of OSS to address these issues. The following diagram outlines the architecture:"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"architecture",src:t(75367).A+"",width:"1200",height:"600"})}),"\n",(0,n.jsx)(a.h3,{id:"enable-near-real-time-data-ingestion-and-analysis",children:"Enable Near real time data ingestion and analysis"}),"\n",(0,n.jsx)(a.p,{children:"With Hudi, our data lake supports multiple data sources including Kafka, MySQL binlog, GIS, and other business logs in near real time. As a result, more than 60% of the company\u2019s data is stored in the data lake and this proportion continues to increase."}),"\n",(0,n.jsx)(a.p,{children:"We are also able to speed up the data ingestion time down to a few minutes by introducing Apache Hudi into the data pipeline. Combined with big data interactive query and analysis framework such as Presto and SparkSQL, real-time data analysis and insights are achieved."}),"\n",(0,n.jsx)(a.h3,{id:"enable-incremental-processing-pipeline",children:"Enable Incremental processing pipeline"}),"\n",(0,n.jsx)(a.p,{children:"With the help of Hudi, it is possible to provide incremental changes to the downstream derived table when the upstream table updates frequently. Even with a large number of interdependent tables, we can quickly run partial data updates. This also effectively avoids updating the full partitions of cold tables in the traditional Hive data warehouse."}),"\n",(0,n.jsx)(a.h3,{id:"accessing-data-using-hudi-as-a-unified-format",children:"Accessing Data using Hudi as a unified format"}),"\n",(0,n.jsx)(a.p,{children:"Traditional data warehouses often deploy Hadoop to store data and provide batch analysis. Kafka is used separately to distribute Hadoop data to other data processing frameworks, resulting in duplicated data. Hudi helps effectively solve this problem; we always use Spark pipelines to insert new updates into the Hudi tables, then incrementally read the update of Hudi tables. In other words, Hudi tables are used as the unified storage format to access data."}),"\n",(0,n.jsx)(a.h2,{id:"iii-efficient-data-caching-using-alluxio",children:"III. Efficient Data Caching Using Alluxio"}),"\n",(0,n.jsx)(a.p,{children:"In the early version of our data lake without Alluxio, data received from Kafka in real time is processed by Spark and then written to OSS data lake using Hudi DeltaStreamer tasks. With this architecture, Spark often suffered high network latency when writing to OSS directly. Since all data is in OSS storage, OLAP queries on Hudi data may also be slow due to lack of data locality."}),"\n",(0,n.jsx)(a.p,{children:"To address the latency issue, we deployed Alluxio as a data orchestration layer, co-located with computing engines such as Spark and Presto, and used Alluxio to accelerate read and write on the data lake as shown in the following diagram:"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"architecture-alluxio",src:t(40474).A+"",width:"1155",height:"612"})}),"\n",(0,n.jsx)(a.p,{children:"Data in formats such as Hudi, Parquet, ORC, and JSON are stored mostly on OSS, consisting of 95% of the data. Computing engines such as Flink, Spark, Kylin, and Presto are deployed in isolated clusters respectively. When each engine accesses OSS, Alluxio acts as a virtual distributed storage system to accelerate data, being co-located with each of the computing clusters."}),"\n",(0,n.jsx)(a.p,{children:"Specifically, here are a few applications leveraging Alluxio in the T3Go data lake."}),"\n",(0,n.jsx)(a.h3,{id:"data-lake-ingestion",children:"Data lake ingestion"}),"\n",(0,n.jsxs)(a.p,{children:["We mount the corresponding OSS path to the Alluxio file system and set Hudi\u2019s  ",(0,n.jsxs)(a.em,{children:["\u201c",(0,n.jsx)(a.strong,{children:"target-base-path"}),"\u201d"]}),"  parameter value to use the alluxio:// scheme in place of oss:// scheme. Spark pipelines with Hudi continuously ingest data to Alluxio. After data is written to Alluxio, it is asynchronously persisted from the Alluxio cache to the remote OSS every minute. These modifications allow Spark to write to a local Alluxio node instead of writing to remote OSS, significantly reducing the time for the data to be available in data lake after ingestion."]}),"\n",(0,n.jsx)(a.h3,{id:"data-analysis-on-the-lake",children:"Data analysis on the lake"}),"\n",(0,n.jsx)(a.p,{children:"We use Presto as an ad-hoc query engine to analyze the Hudi tables in the lake, co-locating Alluxio workers on each Presto worker node. When Presto and Alluxio services are co-located and running, Alluxio caches the input data locally in the Presto worker which greatly benefits Presto for subsequent retrievals. On a cache hit, Presto can read from the local Alluxio worker storage at memory speed without any additional data transfer over the network."}),"\n",(0,n.jsx)(a.h3,{id:"concurrent-accesses-across-multiple-storage-systems",children:"Concurrent accesses across multiple storage systems"}),"\n",(0,n.jsx)(a.p,{children:"In order to ensure the accuracy of training samples, our machine learning team often synchronizes desensitized data in production to an offline machine learning environment. During synchronization, the data flows across multiple file systems, from production OSS to an offline HDFS followed by another offline Machine Learning HDFS."}),"\n",(0,n.jsx)(a.p,{children:"This data migration process is not only inefficient but also error-prune for modelers because multiple different storages with varying configurations are involved. Alluxio helps in this specific scenario by mounting the destination storage systems under the same filesystem to be accessed by their corresponding logical paths in Alluxio namespace. By decoupling the physical storage, this allows applications with different APIs to access and transfer data seamlessly. This data access layout also improves performance."}),"\n",(0,n.jsx)(a.h3,{id:"microbenchmark",children:"Microbenchmark"}),"\n",(0,n.jsx)(a.p,{children:"Overall, we observed the following improvements with Alluxio:"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsx)(a.li,{children:"It supports a hierarchical and transparent caching mechanism"}),"\n",(0,n.jsx)(a.li,{children:"It supports cache promote omode mode when reading"}),"\n",(0,n.jsx)(a.li,{children:"It supports asynchronous writing mode"}),"\n",(0,n.jsx)(a.li,{children:"It supports LRU recycling strategy"}),"\n",(0,n.jsx)(a.li,{children:"It has pin and TTL features"}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"After comparison and verification, we choose to use Spark SQL as the query engine. Our performance testing queries the Hudi table, comparing Alluxio + OSS together against OSS directly as well as HDFS."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"microbench",src:t(38655).A+"",width:"741",height:"508"})}),"\n",(0,n.jsx)(a.p,{children:"In the stress test shown above, after the data volume is greater than a certain magnitude (2400W), the query speed using Alluxio+OSS surpasses the HDFS query speed of the hybrid deployment. After the data volume is greater than 1E, the query speed starts to double. After reaching 6E data, it is up to 12 times higher than querying native OSS and 8 times higher than querying native HDFS. The improvement depends on the machine configuration."}),"\n",(0,n.jsx)(a.p,{children:"Based on our performance benchmarking, we found that the performance can be improved by over 10 times with the help of Alluxio. Furthermore, the larger the data scale, the more prominent the performance improvement."}),"\n",(0,n.jsx)(a.h2,{id:"iv-next-step",children:"IV. Next Step"}),"\n",(0,n.jsx)(a.p,{children:"As T3Go\u2019s data lake ecosystem expands, we will continue facing the critical scenario of compute and storage segregation. With T3Go\u2019s growing data processing needs, our team plans to deploy Alluxio on a larger scale to accelerate our data lake storage."}),"\n",(0,n.jsx)(a.p,{children:"In addition to the deployment of Alluxio on the data lake computing engine, which currently is mainly SparkSQL, we plan to add a layer of Alluxio to the OLAP cluster using Apache Kylin and an ad_hoc cluster using Presto. The goal is to have Alluxio cover all computing scenarios, with Alluxio interconnected between each scene to improve the read and write efficiency of the data lake and the surrounding lake ecology."}),"\n",(0,n.jsx)(a.h2,{id:"v-conclusion",children:"V. Conclusion"}),"\n",(0,n.jsxs)(a.p,{children:["As mentioned earlier, Hudi and Alluxio covers all scenarios of Hudi\u2019s near real-time ingestion, near real-time analysis, incremental processing, and data distribution on DFS, among many others, and plays the role of a powerful accelerator on data ingestion and data analysis on the lake. With Hudi and Alluxio together,  ",(0,n.jsx)(a.strong,{children:"our R&D engineers shortened the time for data ingestion into the lake by up to a factor of 2. Data analysts using Presto, Hudi, and Alluxio in conjunction to query data on the lake saw their queries speed up by 10 times faster."})," Furthermore, the larger the data scale, the more prominent the performance improvement. Alluxio is an important part of T3Go\u2019s plan to become a leading enterprise data lake in China. We look forward to seeing further integration with Alluxio in T3Go\u2019s data lake ecosystem."]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},54512:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(2270),n=t(74848),s=t(28453),r=t(9230);const o={title:"Hudi powering data lake efforts at Walmart and Disney+ Hotstar",authors:[{name:"Sean Michael Kerner"}],category:"blog",image:"/assets/images/blog/2022-01-20-hudi-powering-datalake-efforts.png",tags:["use-case","techtarget"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.techtarget.com/searchdatamanagement/feature/Hudi-powering-data-lake-efforts-at-Walmart-and-Disney-Hotstar",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},54781:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/06/30/What-about-Apache-Hudi-Apache-Iceberg-and-Delta-Lake","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-06-30-What-about-Apache-Hudi-Apache-Iceberg-and-Delta-Lake.mdx","source":"@site/blog/2023-06-30-What-about-Apache-Hudi-Apache-Iceberg-and-Delta-Lake.mdx","title":"What about Apache Hudi, Apache Iceberg, and Delta Lake?","description":"Redirecting... please wait!!","date":"2023-06-30T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"vector search","permalink":"/blog/tags/vector-search"},{"inline":true,"label":"comparison","permalink":"/blog/tags/comparison"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"delta lake","permalink":"/blog/tags/delta-lake"},{"inline":true,"label":"iceberg","permalink":"/blog/tags/iceberg"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Martin Jurado Pedroza","socials":{},"key":null,"page":null}],"frontMatter":{"title":"What about Apache Hudi, Apache Iceberg, and Delta Lake?","authors":[{"name":"Martin Jurado Pedroza"}],"category":"blog","image":"/assets/images/blog/2023-06-30-What-about-Apache-Hudi-Apache-Iceberg-and-Delta-Lake.png","tags":["blog","vector search","comparison","apache hudi","delta lake","iceberg","medium"]},"unlisted":false,"prevItem":{"title":"Monitoring Table Size stats","permalink":"/blog/2023/07/01/monitoring-table-size-stats"},"nextItem":{"title":"Unlimited Big Data Exchange: A Wonderful Review of Apache DolphinScheduler & Hudi Hangzhou Meetup","permalink":"/blog/2023/06/26/Unlimited-Big-Data-Exchange-A-Wonderful-Review-of-Apache-DolphinScheduler-and-Hudi-Hangzhou-Meetup"}}')},54800:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/08/31/Incremental-Queries-with-Apache-Hudi-and-Apache-Flink","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-08-31-Incremental-Queries-with-Apache-Hudi-and-Apache-Flink.mdx","source":"@site/blog/2023-08-31-Incremental-Queries-with-Apache-Hudi-and-Apache-Flink.mdx","title":"Incremental Queries with Apache Hudi and Apache Flink","description":"Redirecting... please wait!!","date":"2023-08-31T00:00:00.000Z","tags":[{"inline":true,"label":"incremental query","permalink":"/blog/tags/incremental-query"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache flink","permalink":"/blog/tags/apache-flink"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"nello","key":null,"page":null}],"frontMatter":{"title":"Incremental Queries with Apache Hudi and Apache Flink","excerpt":"Incremental Queries with Apache Hudi and Apache Flink","author":"nello","category":"blog","image":"/assets/images/blog/2023-08-31-Incremental-Queries-with-Apache-Hudi-and-Apache-Flink.png","tags":["incremental query","blog","apache flink","apache hudi","medium"]},"unlisted":false,"prevItem":{"title":"Lakehouse or Warehouse? Part 1 of 2","permalink":"/blog/2023/09/06/Lakehouse-or-Warehouse-Part-1-of-2"},"nextItem":{"title":"Apache Hudi: From Zero To One (1/10)","permalink":"/blog/2023/08/28/Apache-Hudi-From-Zero-To-One"}}')},55036:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(8662),n=t(74848),s=t(28453);const r={title:"Change Capture Using AWS Database Migration Service and Hudi",excerpt:"In this blog, we will build an end-end solution for capturing changes from a MySQL instance running on AWS RDS to a Hudi table on S3, using capabilities in the Hudi 0.5.1 release.",author:"vinoth",category:"blog",image:"/assets/images/blog/change-capture-architecture.png",tags:["how-to","change data capture","cdc","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Extracting Change logs from MySQL",id:"extracting-change-logs-from-mysql",level:3},{value:"Applying Change Logs using Hudi DeltaStreamer",id:"applying-change-logs-using-hudi-deltastreamer",level:2}];function c(e){const a={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.p,{children:"One of the core use-cases for Apache Hudi is enabling seamless, efficient database ingestion to your data lake. Even though a lot has been talked about and even users already adopting this model, content on how to go about this is sparse."}),"\n",(0,n.jsxs)(a.p,{children:["In this blog, we will build an end-end solution for capturing changes from a MySQL instance running on AWS RDS to a Hudi table on S3, using capabilities in the Hudi  ",(0,n.jsx)(a.strong,{children:"0.5.1 release"})]}),"\n",(0,n.jsx)(a.p,{children:"We can break up the problem into two pieces."}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Extracting change logs from MySQL"}),"  : Surprisingly, this is still a pretty tricky problem to solve and often Hudi users get stuck here. Thankfully, at-least for AWS users, there is a  ",(0,n.jsx)(a.a,{href:"https://aws.amazon.com/dms/",children:"Database Migration service"}),"  (DMS for short), that does this change capture and uploads them as parquet files on S3"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Applying these change logs to your data lake table"}),"  : Once there are change logs in some form, the next step is to apply them incrementally to your table. This mundane task can be fully automated using the Hudi  ",(0,n.jsx)(a.a,{href:"http://hudi.apache.org/docs/hoodie_streaming_ingestion#hudi-streamer",children:"DeltaStreamer"}),"  tool."]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["The actual end-end architecture looks something like this.\n",(0,n.jsx)(a.img,{alt:"enter image description here",src:t(47736).A+"",width:"1200",height:"241"})]}),"\n",(0,n.jsxs)(a.p,{children:["Let's now illustrate how one can accomplish this using a simple ",(0,n.jsx)(a.em,{children:"orders"})," table, stored in MySQL (these instructions should broadly apply to other database engines like Postgres, or Aurora as well, though SQL/Syntax may change)"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"CREATE DATABASE hudi_dms;\nUSE hudi_dms;\n     \nCREATE TABLE orders(\n   order_id INTEGER,\n   order_qty INTEGER,\n   customer_name VARCHAR(100),\n   updated_at TIMESTAMP DEFAULT NOW() ON UPDATE NOW(),\n   created_at TIMESTAMP DEFAULT NOW(),\n   CONSTRAINT orders_pk PRIMARY KEY(order_id)\n);\n \nINSERT INTO orders(order_id, order_qty, customer_name) VALUES(1, 10, 'victor');\nINSERT INTO orders(order_id, order_qty, customer_name) VALUES(2, 20, 'peter');\n"})}),"\n",(0,n.jsxs)(a.p,{children:["In the table, ",(0,n.jsx)(a.em,{children:"order_id"})," is the primary key which will be enforced on the Hudi table as well. Since a batch of change records can contain changes to the same primary key, we also include ",(0,n.jsx)(a.em,{children:"updated_at"})," and ",(0,n.jsx)(a.em,{children:"created_at"})," fields, which are kept upto date as writes happen to the table."]}),"\n",(0,n.jsx)(a.h3,{id:"extracting-change-logs-from-mysql",children:"Extracting Change logs from MySQL"}),"\n",(0,n.jsxs)(a.p,{children:["Before we can configure DMS, we first need to ",(0,n.jsx)(a.a,{href:"https://aws.amazon.com/premiumsupport/knowledge-center/enable-binary-logging-aurora/",children:"prepare the MySQL instance"}),"  for change capture, by ensuring backups are enabled and binlog is turned on.\n",(0,n.jsx)(a.img,{src:t(48402).A+"",width:"2930",height:"302"})]}),"\n",(0,n.jsxs)(a.p,{children:["Now, proceed to create endpoints in DMS that capture MySQL data and  ",(0,n.jsx)(a.a,{href:"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3",children:"store in S3, as parquet files"}),"."]}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["Source ",(0,n.jsx)(a.em,{children:"hudi-source-db"})," endpoint, points to the DB server and provides basic authentication details"]}),"\n",(0,n.jsxs)(a.li,{children:["Target ",(0,n.jsx)(a.em,{children:"parquet-s3"})," endpoint, points to the bucket and folder on s3 to store the change logs records as parquet files\n",(0,n.jsx)(a.img,{src:t(74276).A+"",width:"836",height:"548"}),"\n",(0,n.jsx)(a.img,{src:t(4191).A+"",width:"849",height:"575"}),"\n",(0,n.jsx)(a.img,{src:t(21582).A+"",width:"1025",height:"305"})]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["Then proceed to create a migration task, as below. Give it a name, connect the source to the target and be sure to pick the right ",(0,n.jsx)(a.em,{children:"Migration type"})," as shown below, to ensure ongoing changes are continuously replicated to S3. Also make sure to specify, the rules using which DMS decides which MySQL schema/tables to replicate. In this example, we simply whitelist ",(0,n.jsx)(a.em,{children:"orders"})," table under the ",(0,n.jsx)(a.em,{children:"hudi_dms"})," schema, as specified in the table SQL above."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{src:t(9296).A+"",width:"507",height:"503"}),"\n",(0,n.jsx)(a.img,{src:t(62795).A+"",width:"816",height:"583"})]}),"\n",(0,n.jsx)(a.p,{children:"Starting the DMS task and should result in an initial load, like below."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{src:t(67441).A+"",width:"607",height:"436"})}),"\n",(0,n.jsx)(a.p,{children:"Simply reading the raw initial load file, shoud give the same values as the upstream table"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-scala",children:'scala> spark.read.parquet("s3://hudi-dms-demo/orders/hudi_dms/orders/*").sort("updated_at").show\n \n+--------+---------+-------------+-------------------+-------------------+\n|order_id|order_qty|customer_name|         updated_at|         created_at|\n+--------+---------+-------------+-------------------+-------------------+\n|       2|       10|        peter|2020-01-20 20:12:22|2020-01-20 20:12:22|\n|       1|       10|       victor|2020-01-20 20:12:31|2020-01-20 20:12:31|\n+--------+---------+-------------+-------------------+-------------------+\n\n'})}),"\n",(0,n.jsx)(a.h2,{id:"applying-change-logs-using-hudi-deltastreamer",children:"Applying Change Logs using Hudi DeltaStreamer"}),"\n",(0,n.jsxs)(a.p,{children:["Now, we are ready to start consuming the change logs. Hudi DeltaStreamer runs as Spark job on your favorite workflow scheduler (it also supports a continuous mode using ",(0,n.jsx)(a.em,{children:"--continuous"})," flag, where it runs as a long running Spark job), that tails a given path on S3 (or any DFS implementation) for new files and can issue an ",(0,n.jsx)(a.em,{children:"upsert"})," to a target hudi dataset. The tool automatically checkpoints itself and thus to repeatedly ingest, all one needs to do is to keep executing the DeltaStreamer periodically."]}),"\n",(0,n.jsx)(a.p,{children:"With an initial load already on S3, we then run the following command (deltastreamer command, here on) to ingest the full load first and create a Hudi dataset on S3."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer  \\\n  --packages org.apache.spark:spark-avro_2.11:2.4.4 \\\n  --master yarn --deploy-mode client \\\n  hudi-utilities-bundle_2.11-0.5.1-SNAPSHOT.jar \\\n  --table-type COPY_ON_WRITE \\\n  --source-ordering-field updated_at \\\n  --source-class org.apache.hudi.utilities.sources.ParquetDFSSource \\\n  --target-base-path s3://hudi-dms-demo/hudi_orders --target-table hudi_orders \\\n  --transformer-class org.apache.hudi.utilities.transform.AWSDmsTransformer \\\n  --payload-class org.apache.hudi.payload.AWSDmsAvroPayload \\\n  --hoodie-conf hoodie.datasource.write.recordkey.field=order_id,hoodie.datasource.write.partitionpath.field=customer_name,hoodie.deltastreamer.source.dfs.root=s3://hudi-dms-demo/orders/hudi_dms/orders\n"})}),"\n",(0,n.jsx)(a.p,{children:"A few things are going on here"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["First, we specify the ",(0,n.jsx)(a.em,{children:"--table-type"})," as COPY_ON_WRITE. Hudi also supports another _MERGE_ON_READ ty_pe you can use if you choose from."]}),"\n",(0,n.jsxs)(a.li,{children:["To handle cases where the input parquet files contain multiple updates/deletes or insert/updates to the same record, we use ",(0,n.jsx)(a.em,{children:"updated_at"})," as the ordering field. This ensures that the change record which has the latest timestamp will be reflected in Hudi."]}),"\n",(0,n.jsx)(a.li,{children:"We specify a target base path and a table table, all needed for creating and writing to the Hudi table"}),"\n",(0,n.jsxs)(a.li,{children:["We use a special payload class - ",(0,n.jsx)(a.em,{children:"AWSDMSAvroPayload"})," , to handle the different change operations correctly. The parquet files generated have an ",(0,n.jsx)(a.em,{children:"Op"})," field, that indicates whether a given change record is an insert (I), delete (D) or update (U) and the payload implementation uses this field to decide how to handle a given change record."]}),"\n",(0,n.jsxs)(a.li,{children:["You may also notice a special transformer class ",(0,n.jsx)(a.em,{children:"AWSDmsTransformer"})," , being specified. The reason here is tactical, but important. The initial load file does not contain an ",(0,n.jsx)(a.em,{children:"Op"})," field, so this adds one to Hudi table schema additionally."]}),"\n",(0,n.jsxs)(a.li,{children:["Finally, we specify the record key for the Hudi table as same as the upstream table. Then we specify partitioning by ",(0,n.jsx)(a.em,{children:"customer_name"}),"  and also the root of the DMS output."]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Once the command is run, the Hudi table should be created and have same records as the upstream table (with all the _hoodie fields as well)."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-scala",children:'scala> spark.read.format("org.apache.hudi").load("s3://hudi-dms-demo/hudi_orders/*/*.parquet").show\n+-------------------+--------------------+------------------+----------------------+--------------------+--------+---------+-------------+-------------------+-------------------+---+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|order_id|order_qty|customer_name|         updated_at|         created_at| Op|\n+-------------------+--------------------+------------------+----------------------+--------------------+--------+---------+-------------+-------------------+-------------------+---+\n|     20200120205028|  20200120205028_0_1|                 2|                 peter|af9a2525-a486-40e...|       2|       10|        peter|2020-01-20 20:12:22|2020-01-20 20:12:22|   |\n|     20200120205028|  20200120205028_1_1|                 1|                victor|8e431ece-d51c-4c7...|       1|       10|       victor|2020-01-20 20:12:31|2020-01-20 20:12:31|   |\n+-------------------+--------------------+------------------+----------------------+--------------------+--------+---------+-------------+-------------------+-------------------+---+\n'})}),"\n",(0,n.jsx)(a.p,{children:"Now, let's do an insert and an update"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"INSERT INTO orders(order_id, order_qty, customer_name) VALUES(3, 30, 'sandy');\nUPDATE orders set order_qty = 20 where order_id = 2;\n"})}),"\n",(0,n.jsx)(a.p,{children:"This will add a new parquet file to the DMS output folder and when the deltastreamer command is run again, it will go ahead and apply these to the Hudi table."}),"\n",(0,n.jsxs)(a.p,{children:["So, querying the Hudi table now would yield 3 rows and the ",(0,n.jsx)(a.em,{children:"hoodie_commit_time"})," accurately reflects when these writes happened. You can notice that order_qty for order_id=2, is updated from 10 to 20!"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"+-------------------+--------------------+------------------+----------------------+--------------------+---+--------+---------+-------------+-------------------+-------------------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| Op|order_id|order_qty|customer_name|         updated_at|         created_at|\n+-------------------+--------------------+------------------+----------------------+--------------------+---+--------+---------+-------------+-------------------+-------------------+\n|     20200120211526|  20200120211526_0_1|                 2|                 peter|af9a2525-a486-40e...|  U|       2|       20|        peter|2020-01-20 21:11:47|2020-01-20 20:12:22|\n|     20200120211526|  20200120211526_1_1|                 3|                 sandy|566eb34a-e2c5-44b...|  I|       3|       30|        sandy|2020-01-20 21:11:24|2020-01-20 21:11:24|\n|     20200120205028|  20200120205028_1_1|                 1|                victor|8e431ece-d51c-4c7...|   |       1|       10|       victor|2020-01-20 20:12:31|2020-01-20 20:12:31|\n+-------------------+--------------------+------------------+----------------------+--------------------+---+--------+---------+-------------+-------------------+-------------------+\n"})}),"\n",(0,n.jsx)(a.p,{children:"A nice debugging aid would be read all of the DMS output now and sort it by update_at, which should give us a sequence of changes that happened on the upstream table. As we can see, the Hudi table above is a compacted snapshot of this raw change log."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"+----+--------+---------+-------------+-------------------+-------------------+\n|  Op|order_id|order_qty|customer_name|         updated_at|         created_at|\n+----+--------+---------+-------------+-------------------+-------------------+\n|null|       2|       10|        peter|2020-01-20 20:12:22|2020-01-20 20:12:22|\n|null|       1|       10|       victor|2020-01-20 20:12:31|2020-01-20 20:12:31|\n|   I|       3|       30|        sandy|2020-01-20 21:11:24|2020-01-20 21:11:24|\n|   U|       2|       20|        peter|2020-01-20 21:11:47|2020-01-20 20:12:22|\n+----+--------+---------+-------------+-------------------+-------------------+\n"})}),"\n",(0,n.jsxs)(a.p,{children:["Initial load with no ",(0,n.jsx)(a.em,{children:"Op"})," field value , followed by an insert and an update."]}),"\n",(0,n.jsx)(a.p,{children:"Now, lets do deletes an inserts"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:"DELETE FROM orders WHERE order_id = 2;\nINSERT INTO orders(order_id, order_qty, customer_name) VALUES(4, 40, 'barry');\nINSERT INTO orders(order_id, order_qty, customer_name) VALUES(5, 50, 'nathan');\n"})}),"\n",(0,n.jsx)(a.p,{children:"This should result in more files on S3, written by DMS , which the DeltaStreamer command will continue to process incrementally (i.e only the newly written files are read each time)"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{src:t(79013).A+"",width:"1127",height:"568"})}),"\n",(0,n.jsxs)(a.p,{children:["Running the deltastreamer command again, would result in the follow state for the Hudi table. You can notice the two new records and that the ",(0,n.jsx)(a.em,{children:"order_id=2"})," is now gone"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"+-------------------+--------------------+------------------+----------------------+--------------------+---+--------+---------+-------------+-------------------+-------------------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| Op|order_id|order_qty|customer_name|         updated_at|         created_at|\n+-------------------+--------------------+------------------+----------------------+--------------------+---+--------+---------+-------------+-------------------+-------------------+\n|     20200120212522|  20200120212522_1_1|                 5|                nathan|3da94b20-c70b-457...|  I|       5|       50|       nathan|2020-01-20 21:23:00|2020-01-20 21:23:00|\n|     20200120212522|  20200120212522_2_1|                 4|                 barry|8cc46715-8f0f-48a...|  I|       4|       40|        barry|2020-01-20 21:22:49|2020-01-20 21:22:49|\n|     20200120211526|  20200120211526_1_1|                 3|                 sandy|566eb34a-e2c5-44b...|  I|       3|       30|        sandy|2020-01-20 21:11:24|2020-01-20 21:11:24|\n|     20200120205028|  20200120205028_1_1|                 1|                victor|8e431ece-d51c-4c7...|   |       1|       10|       victor|2020-01-20 20:12:31|2020-01-20 20:12:31|\n+-------------------+--------------------+------------------+----------------------+--------------------+---+--------+---------+-------------+-------------------+-------------------+\n"})}),"\n",(0,n.jsx)(a.p,{children:"Our little informal change log query yields the following."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"+----+--------+---------+-------------+-------------------+-------------------+\n|  Op|order_id|order_qty|customer_name|         updated_at|         created_at|\n+----+--------+---------+-------------+-------------------+-------------------+\n|null|       2|       10|        peter|2020-01-20 20:12:22|2020-01-20 20:12:22|\n|null|       1|       10|       victor|2020-01-20 20:12:31|2020-01-20 20:12:31|\n|   I|       3|       30|        sandy|2020-01-20 21:11:24|2020-01-20 21:11:24|\n|   U|       2|       20|        peter|2020-01-20 21:11:47|2020-01-20 20:12:22|\n|   D|       2|       20|        peter|2020-01-20 21:11:47|2020-01-20 20:12:22|\n|   I|       4|       40|        barry|2020-01-20 21:22:49|2020-01-20 21:22:49|\n|   I|       5|       50|       nathan|2020-01-20 21:23:00|2020-01-20 21:23:00|\n+----+--------+---------+-------------+-------------------+-------------------+\n"})}),"\n",(0,n.jsxs)(a.p,{children:["Note that the delete and update have the same ",(0,n.jsx)(a.em,{children:"updated_at,"})," value. thus it can very well order differently here.. In short this way of looking at the changelog has its caveats. For a true changelog of the Hudi table itself, you can issue an ",(0,n.jsx)(a.a,{href:"http://hudi.apache.org/docs/querying_data",children:"incremental query"}),"."]}),"\n",(0,n.jsx)(a.p,{children:"And Life goes on ..... Hope this was useful to all the data engineers out there!"})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},55204:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(86415),n=t(74848),s=t(28453),r=t(9230);const o={title:"Learn How to Move Data From MongoDB to Apache Hudi Using PySpark",excerpt:"Learn How to Move Data From MongoDB to Apache Hudi Using PySpark",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2024-01-20-Learn-How-to-Move-Data-From-MongoDB-to-Apache-Hudi-Using-PySpark.png",tags:["blog","apache hudi","linkedin","beginner","mongodb","apache spark","pyspark"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.linkedin.com/pulse/learn-how-move-data-from-mongodb-apache-hudi-using-pyspark-shah-cq3pe/?utm_source=share&utm_medium=member_ios&utm_campaign=share_via",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},55813:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/read_optimized_view-f86557dfea584b97e869ec2d1aa9a46e.png"},55826:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/01/11/Apache-Hudi-vs-Delta-Lake-vs-Apache-Iceberg-Lakehouse-Feature-Comparison","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-01-11-Apache-Hudi-vs-Delta-Lake-vs-Apache-Iceberg-Lakehouse-Feature-Comparison.mdx","source":"@site/blog/2023-01-11-Apache-Hudi-vs-Delta-Lake-vs-Apache-Iceberg-Lakehouse-Feature-Comparison.mdx","title":"Apache Hudi vs Delta Lake vs Apache Iceberg - Lakehouse Feature Comparison","description":"Redirecting... please wait!!","date":"2023-01-11T00:00:00.000Z","tags":[{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"datalake","permalink":"/blog/tags/datalake"},{"inline":true,"label":"comparison","permalink":"/blog/tags/comparison"},{"inline":true,"label":"onehouse","permalink":"/blog/tags/onehouse"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Kyle Weller","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Apache Hudi vs Delta Lake vs Apache Iceberg - Lakehouse Feature Comparison","authors":[{"name":"Kyle Weller"}],"category":"blog","image":"/assets/images/blog/2022-08-18-apache_hudi_vs_delta_lake_vs_apache_iceberg_feature_comparison.png","tags":["lakehouse","datalake","comparison","onehouse"]},"unlisted":false,"prevItem":{"title":"Introducing native support for Apache Hudi, Delta Lake, and Apache Iceberg on AWS Glue for Apache Spark, Part 1: Getting Started","permalink":"/blog/2023/01/27/Introducing-native-support-for-Apache-Hudi-Delta-Lake-Apache-Iceberg-on-AWS-Glue-for-Apache-Spark"},"nextItem":{"title":"Apache Hudi 2022 - A year in Review","permalink":"/blog/2022/12/29/Apache-Hudi-2022-A-Year-In-Review"}}')},55891:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/09/06/Apache-Hudi-From-Zero-To-One-blog-2","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-09-06-Apache-Hudi-From-Zero-To-One-blog-2.mdx","source":"@site/blog/2023-09-06-Apache-Hudi-From-Zero-To-One-blog-2.mdx","title":"Apache Hudi: From Zero To One (2/10)","description":"Redirecting... please wait!!","date":"2023-09-06T00:00:00.000Z","tags":[{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"spark","permalink":"/blog/tags/spark"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"course","permalink":"/blog/tags/course"},{"inline":true,"label":"tutorial","permalink":"/blog/tags/tutorial"},{"inline":true,"label":"datumagic","permalink":"/blog/tags/datumagic"},{"inline":true,"label":"data lake","permalink":"/blog/tags/data-lake"},{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"apache spark","permalink":"/blog/tags/apache-spark"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Shiyan Xu","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi: From Zero To One (2/10)","excerpt":"Dive into read operation flow and query types","author":"Shiyan Xu","category":"blog","image":"/assets/images/blog/2023-09-06-Apache-Hudi-From-Zero-To-One-blog-2.png","tags":["hudi","spark","blog","course","tutorial","datumagic","data lake","lakehouse","apache hudi","apache spark"]},"unlisted":false,"prevItem":{"title":"Demystifying Copy-on-Write in Apache Hudi: Understanding Read and Write Operations","permalink":"/blog/2023/09/10/Demystifying-Copy-on-Write-in-Apache-Hudi-Understanding-Read-and-Write-Operations"},"nextItem":{"title":"Lakehouse or Warehouse? Part 1 of 2","permalink":"/blog/2023/09/06/Lakehouse-or-Warehouse-Part-1-of-2"}}')},55893:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig5-8c6a7e86f7bb1789e91ac4c539fd1b78.png"},56305:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(69856),n=t(74848),s=t(28453),r=t(9230);const o={title:"Hudi On Hops",authors:[{name:"NETSANET GEBRETSADKAN KIDANE"}],category:"blog",tags:["blog","diva-portal"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.diva-portal.org/smash/get/diva2:1413103/FULLTEXT01.pdf",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},56338:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/05/09/amazon-athena-apache-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-05-09-amazon-athena-apache-hudi.mdx","source":"@site/blog/2023-05-09-amazon-athena-apache-hudi.mdx","title":"Amazon Athena now supports Apache Hudi 0.12.2","description":"Redirecting... please wait!!","date":"2023-05-09T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Amazon Athena now supports Apache Hudi 0.12.2","category":"blog","image":"/assets/images/blog/aws.jpg","tags":["blog","amazon"]},"unlisted":false,"prevItem":{"title":"Top 3 Things You Can Do to Get Fast Upsert Performance in Apache Hudi","permalink":"/blog/2023/05/10/top-3-things-you-can-do-to-get-fast-upsert-performance-in-apache-hudi"},"nextItem":{"title":"Lakehouse at Fortune 1 Scale","permalink":"/blog/2023/05/03/lakehouse-at-fortune-1-scale"}}')},56417:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(28436),n=t(74848),s=t(28453),r=t(9230);const o={title:"Delta, Hudi, Iceberg \u2014 Which is most popular?",excerpt:"Popular Lakehoue Project",author:"Kyle Weller",category:"blog",image:"/assets/images/blog/2023-08-25-Delta-Hudi-Iceberg-Which-is-most-popular.png",tags:["blog","apache hudi","delta lake","iceberg","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@kywe665/delta-hudi-iceberg-which-is-most-popular-29ca56767199",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},56500:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(32817),n=t(74848),s=t(28453),r=t(9230);const o={title:"Navigating the Future: The Evolutionary Journey of Upstox\u2019s Data Platform",author:"Manish Gaurav",category:"blog",image:"/assets/images/blog/2024-03-10-navigating-the-future-the-evolutionary-journey-of-upstoxs-data-platform.png",tags:["use-case","apache hudi","upstox-engineering"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/upstox-engineering/navigating-the-future-the-evolutionary-journey-of-upstoxs-data-platform-92dc10ff22ae",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},56533:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(65326),n=t(74848),s=t(28453),r=t(9230);const o={title:"Ingesting data to Apache Hudi using Spark sql",authors:[{name:"Sivabalan Narayanan"}],category:"blog",tags:["how-to","spark-sql","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@simpsons/ingesting-data-to-apache-hudi-using-spark-sql-36d9815423b3",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},56542:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/timeline-server-based-marker-mechanism-11d616800a7a241382c8a4ed647515a6.png"},56841:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide6-dfbac2ecb760185c7c401305c4796192.png"},56973:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(99645),n=t(74848),s=t(28453);const r={title:"Partition Stats: Enhancing Column Stats in Hudi 1.0",excerpt:"",author:"Aditya Goenka and Shiyan Xu",category:"blog",image:"/assets/images/blog/2025-10-22-Partition_Stats_Enhancing_Column_Stats_in_Hudi_1.0/fig1.jpg",tags:["hudi","indexing","data lakehouse","data skipping"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Multimodal Indexing",id:"multimodal-indexing",level:2},{value:"Example: US Shipping Addresses",id:"example-us-shipping-addresses",level:2},{value:"Results: the Data Skipping Effect",id:"results-the-data-skipping-effect",level:2},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const a={a:"a",code:"code",em:"em",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(a.p,{children:["For those tracking Apache Hudi's performance enhancements, the introduction of the column stats index was a significant development, as ",(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/blog/hudis-column-stats-index-and-data-skipping-feature-help-speed-up-queries-by-an-orders-of-magnitude",children:"detailed in this blog"}),". It represented a major advancement for query optimization by implementing a straightforward yet highly effective concept: storing lightweight, file-level statistics (such as min/max values and null counts) for specific columns. This provided Hudi's query engine a substantial performance improvement."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"cover",src:t(34508).A+"",width:"1944",height:"1654"})}),"\n",(0,n.jsxs)(a.p,{children:["Instead of blindly scanning every single file for a query, the engine could first peek at the index entries\u2014which is far more efficient than reading all the Parquet footers\u2014to determine which files ",(0,n.jsx)(a.em,{children:"couldn't"})," possibly contain the relevant data. This data-skipping capability meant engines could bypass large amounts of irrelevant data, slashing query latency. But that skipping process is conducted at the file level\u2014what if we could apply a similar skipping logic at the partition level? Since a single physical partition can contain thousands of data files, applying this logic at the partition level can further amplify the performance gains by only considering files in the relevant partitions. This is precisely the capability that Hudi 1.0\u2019s partition stats index introduces."]}),"\n",(0,n.jsx)(a.h2,{id:"multimodal-indexing",children:"Multimodal Indexing"}),"\n",(0,n.jsxs)(a.p,{children:["Hudi\u2019s ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/indexes#multi-modal-indexing",children:"multimodal indexing subsystem"})," enhances both read and write performance in data lakehouses by supporting versatile index types optimized for different workloads. This subsystem is built on a scalable, internal metadata table that ensures ACID-compliant updates and efficient lookups, which in turn reduces full data scans. It houses various indexes\u2014such as the files, column stats, and partition stats\u2014which work together to improve efficiency in reads, writes, and upserts, providing scalable, low-latency query performance for large datasets in the lakehouse."]}),"\n",(0,n.jsxs)(a.p,{children:["The partition stats index is built on top of the column stats index by aggregating its file-level statistics up to the partition level. As we've covered, the column stats index tracks statistics (min, max, null counts) for ",(0,n.jsx)(a.em,{children:"individual files"}),", enabling fine-grained file pruning. The partition stats index, in contrast, summarizes these same statistics across ",(0,n.jsx)(a.em,{children:"all files"})," within a single partition."]}),"\n",(0,n.jsx)(a.p,{children:"This partition-level aggregation allows Hudi to efficiently prune entire physical partitions before even examining file-level indexes, leading to faster query planning and execution by skipping large chunks of irrelevant data early in the process. In other words, the partition stats index provides a coarse-grained, high-level pruning layer on top of the fine-grained, file-level pruning enabled by the column stats index."}),"\n",(0,n.jsx)(a.p,{children:"Because partition-level pruning happens first, it narrows down the scope of files that the column stats index needs to inspect, improving overall query performance and reducing overhead on large datasets. The diagram below illustrates the file pruning process:"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"file pruning process",src:t(97283).A+"",width:"981",height:"706"})}),"\n",(0,n.jsx)(a.p,{children:"During query planning, the Hudi integration for the query engine takes the predicates parsed from user queries and queries the indexes within the metadata table."}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"The files index is queried first to return an initial list of all partitions in the table."}),"\n",(0,n.jsxs)(a.li,{children:["The partition stats index then filters this partition list by checking if each partition\u2019s min/max values for the indexed columns fall within the predicate's range. For example, with a predicate of ",(0,n.jsx)(a.code,{children:"A = 100"}),", the index skips any partition whose ",(0,n.jsx)(a.code,{children:"min(A)"})," is greater than 100 or whose ",(0,n.jsx)(a.code,{children:"max(A)"})," is less than 100."]}),"\n",(0,n.jsxs)(a.li,{children:["The files index is queried again to retrieve a list of all files ",(0,n.jsx)(a.em,{children:"within"})," these pruned partitions."]}),"\n",(0,n.jsx)(a.li,{children:"This file list is then passed to the column stats index, which performs the final, fine-grained pruning by applying the query predicates to the file-level statistics."}),"\n",(0,n.jsx)(a.li,{children:"Finally, this pruned list of files is returned to the query engine to complete query planning."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"This dual-layer pruning strategy is especially impactful in production systems managing large amounts of data. By complementing the fine-grained column stats index with this coarse-grained partition skipping, Hudi\u2019s metadata table significantly reduces I/O, computation, and cost. For end-users, this translates directly into a better experience, turning queries that once took minutes into operations that complete in seconds."}),"\n",(0,n.jsx)(a.h2,{id:"example-us-shipping-addresses",children:"Example: US Shipping Addresses"}),"\n",(0,n.jsxs)(a.p,{children:["To understand the impact, let's use the example table below, which stores US shipping addresses for online orders and is partitioned by ",(0,n.jsx)(a.code,{children:"state"}),". This table could contain billions of records, and we want to run a query filtering on the ",(0,n.jsx)(a.code,{children:"zip_code"})," column."]}),"\n",(0,n.jsx)(a.p,{children:"By default, the files, column stats, and partition stats indexes are all enabled in Hudi 1.0. You can create the Hudi table using Spark SQL, for example, without needing additional configs to enable column stats and partition stats:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"CREATE TABLE shipping_address (\n    order_id STRING,\n    state STRING,\n    zip_code STRING,\n    ...\n) USING HUDI\nTBLPROPERTIES (\n    primaryKey ='order_id',\n    hoodie.metadata.index.column.stats.column.list = 'zip_code'\n)\nPARTITIONED BY (state);\n"})}),"\n",(0,n.jsxs)(a.p,{children:["Note that, in practice, you would most likely want to use ",(0,n.jsx)(a.code,{children:"hoodie.metadata.index.column.stats.column.list"})," to indicate which column(s) to index according to your business use case, otherwise, the first 32 columns in the table schema will be indexed by default, which probably won\u2019t be optimal. The specified columns apply to both the column stats and partition stats indexes."]}),"\n",(0,n.jsxs)(a.p,{children:["Without the column and partition stats indexes, a query for a specific ZIP code (e.g., ",(0,n.jsx)(a.code,{children:"zip_code = '90001'"}),") would force the query engine to perform a full table scan. This is highly inefficient, leading to high query latency and excessive resource consumption."]}),"\n",(0,n.jsx)(a.p,{children:"With the indexes enabled, the process is drastically different."}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsxs)(a.li,{children:["During write operations, the Hudi writer tracks statistics for the ",(0,n.jsx)(a.code,{children:"zip_code"})," column. The column stats index stores min/max values for each data file, and the partition stats index aggregates and stores the min/max ",(0,n.jsx)(a.code,{children:"zip_code"})," for each ",(0,n.jsx)(a.code,{children:"state"}),"."]}),"\n",(0,n.jsxs)(a.li,{children:['At query time, suppose the partition stats index shows that the "California" partition contains ZIP codes from "90000" to "96199", while the "New York" partition contains ZIP codes from "10000" to "14999". When the query for ',(0,n.jsx)(a.code,{children:"zip_code = '90001'"}),' is executed, the query planner first consults the partition stats index. It sees that "90001" falls within the "California" partition\'s range but outside the "New York" partition\'s range.']}),"\n",(0,n.jsx)(a.li,{children:'The engine can therefore skip the entire "New York" partition (and any other partition like "Texas" or "Florida" whose ZIP code range doesn\'t include "90001"). The query proceeds by only reading data from the "California" partition\u2014the only one that could possibly contain the data.'}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"This ability to prune entire partitions before reading any files is what provides such a significant performance gain."}),"\n",(0,n.jsx)(a.h2,{id:"results-the-data-skipping-effect",children:"Results: the Data Skipping Effect"}),"\n",(0,n.jsxs)(a.p,{children:["We conducted a focused benchmarking exercise using a synthetic dataset generated by the open-source tool ",(0,n.jsx)(a.a,{href:"https://github.com/onehouseinc/lake-loader",children:"lake_loader"}),". Specifically, we created a 1 TB table for the US shipping addresses example and built both the column stats and partition stats indexes on this dataset."]}),"\n",(0,n.jsx)(a.p,{children:"The benchmarking objective was to evaluate the performance impact from the two indexes for data skipping. To do this, we executed the following query in two scenarios:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"select count(1) from shipping_address where zip_code = '10001'\n"})}),"\n",(0,n.jsx)(a.p,{children:"One with the column and partition stats indexes enabled (default), and one with both indexes disabled for reads, which forced a full table scan."}),"\n",(0,n.jsx)(a.p,{children:"The Spark job was configured with:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Executor cores = 4"}),"\n",(0,n.jsx)(a.li,{children:"Executor memory = 10g"}),"\n",(0,n.jsx)(a.li,{children:"Number of executors = 60"}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"The Spark DAGs for the two scenarios show the file pruning effect:"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Spark DAGs comparison",src:t(75706).A+"",width:"3456",height:"1992"})}),"\n",(0,n.jsx)(a.p,{children:"With both column stats and partition stats indexes enabled (the left-side DAG), the number of files read was 19,304. In contrast, the disabled setup (the right-side DAG) resulted in reading 393,360 files\u2014about 20 times more."}),"\n",(0,n.jsx)(a.p,{children:"The runtime comparison chart below shows the query time difference (shorter is better):"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"perf run time chart",src:t(74469).A+"",width:"2428",height:"1720"})}),"\n",(0,n.jsx)(a.p,{children:"Enabling data skipping with both the column stats and partition stats indexes for the Hudi table delivers approximately a 93% reduction in query runtime compared to the full scan (no data skipping)."}),"\n",(0,n.jsx)(a.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,n.jsxs)(a.p,{children:["The new partition stats index is a powerful addition to Hudi's multimodal indexing subsystem, directly addressing the challenge of query performance on large-scale partitioned tables. By working in concert with the existing column stats index, it provides a crucial layer of coarse-grained pruning, allowing the query engine to eliminate entire partitions from consideration ",(0,n.jsx)(a.em,{children:"before"})," inspecting individual files. As our benchmark showed, this two-level pruning strategy\u2014first by partition, then by file\u2014is not just a minor tweak. It results in a dramatic reduction in I/O, slashing query runtimes by over 93% and enabling near-interactive query speeds. This feature solidifies Hudi's data-skipping capabilities, making it even more efficient to run demanding analytical queries directly on the data lakehouse, saving both time and computation costs."]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},57006:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(57590),n=t(74848),s=t(28453);const r={title:"Apache Hudi meets Apache Flink",excerpt:"The design and latest progress of the integration of Apache Hudi and Apache Flink.",author:"wangxianghu",category:"blog",image:"/assets/images/blog/2020-10-15-apache-hudi-meets-apache-flink.png",tags:["blog","apache flink","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"1. Why decouple",id:"1-why-decouple",level:2},{value:"2. Challenges",id:"2-challenges",level:2},{value:"3. Decoupling Spark",id:"3-decoupling-spark",level:2},{value:"Decoupling principle",id:"decoupling-principle",level:3},{value:"4. Flink integration design",id:"4-flink-integration-design",level:2},{value:"4.1 Index design based on Flink State",id:"41-index-design-based-on-flink-state",level:3},{value:"5. Implementation examples",id:"5-implementation-examples",level:2},{value:"1) HoodieTable",id:"1-hoodietable",level:3},{value:"2) HoodieEngineContext",id:"2-hoodieenginecontext",level:3},{value:"6. Current progress and follow-up plan",id:"6-current-progress-and-follow-up-plan",level:2},{value:"6.1 Working time axis",id:"61-working-time-axis",level:3},{value:"6.2 Follow-up plan",id:"62-follow-up-plan",level:3},{value:"1) Promote the integration of Hudi and Flink",id:"1-promote-the-integration-of-hudi-and-flink",level:4},{value:"2) Performance optimization",id:"2-performance-optimization",level:4},{value:"3) flink-connector-hudi like third-party package development",id:"3-flink-connector-hudi-like-third-party-package-development",level:4}];function c(e){const a={a:"a",code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.p,{children:"Apache Hudi (Hudi for short) is a data lake framework created at Uber. Hudi joined the Apache incubator for incubation in January 2019, and was promoted to the top Apache project in May 2020. It is one of the most popular data lake frameworks."}),"\n",(0,n.jsx)(a.h2,{id:"1-why-decouple",children:"1. Why decouple"}),"\n",(0,n.jsx)(a.p,{children:"Hudi has been using Spark as its data processing engine since its birth. If users want to use Hudi as their data lake framework, they must introduce Spark into their platform technology stack.\nA few years ago, using Spark as a big data processing engine can be said to be very common or even natural. Since Spark can either perform batch processing or use micro-batch to simulate streaming, one engine solves both streaming and batch problems.\nHowever, in recent years, with the development of big data technology, Flink, which is also a big data processing engine, has gradually entered people's vision and has occupied a certain market in the field of computing engines.\nIn the big data technology community, forums and other territories, the voice of whether Hudi supports Flink has gradually appeared and has become more frequent. Therefore, it is a valuable thing to make Hudi support the Flink engine, and the first step of integrating the Flink engine is that Hudi and Spark are decoupled."}),"\n",(0,n.jsx)(a.p,{children:"In addition, looking at the mature, active, and viable frameworks in the big data, all frameworks are elegant in design and can be integrated with other frameworks and leverage each other's expertise.\nTherefore, decoupling Hudi from Spark and turning it into an engine-independent data lake framework will undoubtedly create more possibilities for the integration of Hudi and other components, allowing Hudi to better integrate into the big data ecosystem."}),"\n",(0,n.jsx)(a.h2,{id:"2-challenges",children:"2. Challenges"}),"\n",(0,n.jsx)(a.p,{children:"Hudi's internal use of Spark API is as common as our usual development and use of List. Since the data source reads the data, and finally writes the data to the table, Spark RDD is used as the main data structure everywhere, and even ordinary tools are implemented using the Spark API.\nIt can be said that Hudi is a universal data lake framework implemented by Spark. Hudi also leverages deep Spark functionality like custom partitioning, in-memory caching to implement indexing and file sizing using workload heuristics.\nFor some of these, Flink offers better out-of-box support (e.g using Flink\u2019s state store for indexing) and can in fact, make Hudi approach real-time latencies more and more."}),"\n",(0,n.jsx)(a.p,{children:"In addition, the primary engine integrated after this decoupling is Flink. Flink and Spark differ greatly in core abstraction. Spark believes that data is bounded, and its core abstraction is a limited set of data.\nFlink believes that the essence of data is a stream, and its core abstract DataStream contains various operations on data. Hudi has a streaming first design (record level updates, record level streams), that arguably fit the Flink model more naturally.\nAt the same time, there are multiple RDDs operating at the same time in Hudi, and the processing result of one RDD is combined with another RDD.\nThis difference in abstraction and the reuse of intermediate results during implementation make it difficult for Hudi to use a unified API to operate both RDD and DataStream in terms of decoupling abstraction."}),"\n",(0,n.jsx)(a.h2,{id:"3-decoupling-spark",children:"3. Decoupling Spark"}),"\n",(0,n.jsx)(a.p,{children:"In theory, Hudi uses Spark as its computing engine to use Spark's distributed computing power and RDD's rich operator capabilities. Apart from distributed computing power, Hudi uses RDD more as a data structure, and RDD is essentially a bounded data set.\nTherefore, it is theoretically feasible to replace RDD with List (of course, it may sacrifice performance/scale). In order to ensure the performance and stability of the Hudi Spark version as much as possible. We can keep setting the bounded data set as the basic operation unit.\nHudi's main operation API remains unchanged, and RDD is extracted as a generic type. The Spark engine implementation still uses RDD, and other engines use List or other bounded  data set according to the actual situation."}),"\n",(0,n.jsx)(a.h3,{id:"decoupling-principle",children:"Decoupling principle"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:["Unified generics. The input records ",(0,n.jsx)(a.code,{children:"JavaRDD<HoodieRecord>"}),", key of input records ",(0,n.jsx)(a.code,{children:"JavaRDD<HoodieKey>"}),", and result of write operations ",(0,n.jsx)(a.code,{children:"JavaRDD<WriteStatus>"})," used by the Spark API use generic ",(0,n.jsx)(a.code,{children:"I,K,O"})," instead;"]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"De-sparkization. All APIs of the abstraction layer must have nothing to do with Spark. Involving specific operations that are difficult to implement in the abstract layer, rewrite them as abstract methods and introduce Spark subclasses."}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["For example: Hudi uses the ",(0,n.jsx)(a.code,{children:"JavaSparkContext#map()"})," method in many places. To de-spark, you need to hide the ",(0,n.jsx)(a.code,{children:"JavaSparkContext"}),". For this problem, we introduced the ",(0,n.jsx)(a.code,{children:"HoodieEngineContext#map()"})," method, which will block the specific implementation details of ",(0,n.jsx)(a.code,{children:"map"}),", so as to achieve de-sparkization in abstraction."]}),"\n",(0,n.jsxs)(a.ol,{start:"3",children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"Minimize changes to the abstraction layer to ensure the original function and performance of Hudi;"}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:["Replace the ",(0,n.jsx)(a.code,{children:"JavaSparkContext"})," with the ",(0,n.jsx)(a.code,{children:"HoodieEngineContext"})," abstract class to provide the running environment context."]}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["In addition, some of the core algorithms in Hudi, like ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/pull/1756",children:"rollback"}),", has been redone without the need for computing a workload profile ahead of time, which used to rely on Spark caching."]}),"\n",(0,n.jsx)(a.h2,{id:"4-flink-integration-design",children:"4. Flink integration design"}),"\n",(0,n.jsxs)(a.p,{children:["Hudi's write operation is batch processing in nature, and the continuous mode of ",(0,n.jsx)(a.code,{children:"DeltaStreamer"})," is realized by looping batch processing. In order to use a unified API, when Hudi integrates Flink, we choose to collect a batch of data before processing, and finally submit it in a unified manner (here we use List to collect data in Flink).\nIn Hudi terminology, we will stream data for a given commit, but only publish the commits every so often, making it practical to scale storage on cloud storage and also tunable."]}),"\n",(0,n.jsx)(a.p,{children:"The easiest way to think of batch operation is to use a time window. However, when using a window, when there is no data flowing in a window, there will be no output data, and it is difficult for the Flink sink to judge whether all the data from a given batch has been processed.\nTherefore, we use Flink's checkpoint mechanism to collect batches. The data between every two barriers is a batch. When there is no data in a subtask, the mock result data is made up.\nIn this way, on the sink side, when each subtask has result data issued, it can be considered that a batch of data has been processed and the commit can be executed."}),"\n",(0,n.jsx)(a.p,{children:"The DAG is as follows:"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"dualism",src:t(31882).A+"",width:"1204",height:"932"})}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Source:"})," receives Kafka data and converts it into ",(0,n.jsx)(a.code,{children:"List<HoodieRecord>"}),";"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"InstantGeneratorOperator:"})," generates a globally unique instant. When the previous instant is not completed or the current batch has no data, no new instant is created;"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"KeyBy partitionPath:"})," partitions according to ",(0,n.jsx)(a.code,{children:"partitionPath"})," to avoid multiple subtasks from writing the same partition;"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"WriteProcessOperator:"})," performs a write operation. When there is no data in the current partition, it sends empty result data to the downstream to make up the number;"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"CommitSink:"})," receives the calculation results of the upstream task. When receiving the parallelism results, it is considered that all the upstream subtasks are completed and the commit is executed."]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["Note:\n",(0,n.jsx)(a.code,{children:"InstantGeneratorOperator"})," and ",(0,n.jsx)(a.code,{children:"WriteProcessOperator"})," are both custom Flink operators. ",(0,n.jsx)(a.code,{children:"InstantGeneratorOperator"})," will block checking the state of the previous instant to ensure that there is only one instant in the global (or requested) state.\n",(0,n.jsx)(a.code,{children:"WriteProcessOperator"})," is the actual execution Where a write operation is performed, the write operation is triggered at checkpoint."]}),"\n",(0,n.jsx)(a.h3,{id:"41-index-design-based-on-flink-state",children:"4.1 Index design based on Flink State"}),"\n",(0,n.jsxs)(a.p,{children:["Stateful computing is one of the highlights of the Flink engine. Compared with using external storage, using Flink's built-in ",(0,n.jsx)(a.code,{children:"State"})," can significantly improve the performance of Flink applications.\nTherefore, it would be a good choice to implement a Hudi index based on Flink's State."]}),"\n",(0,n.jsxs)(a.p,{children:["The core of the Hudi index is to maintain the mapping of the Hudi key ",(0,n.jsx)(a.code,{children:"HoodieKey"})," and the location of the Hudi data ",(0,n.jsx)(a.code,{children:"HoodieRecordLocation"}),".\nTherefore, based on the current design, we can simply maintain a ",(0,n.jsx)(a.code,{children:"MapState<HoodieKey, HoodieRecordLocation>"})," in Flink UDF to map the ",(0,n.jsx)(a.code,{children:"HoodieKey"})," and ",(0,n.jsx)(a.code,{children:"HoodieRecordLocation"}),", and leave the fault tolerance and persistence of State to the Flink framework."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"dualism",src:t(80865).A+"",width:"1024",height:"624"})}),"\n",(0,n.jsx)(a.h2,{id:"5-implementation-examples",children:"5. Implementation examples"}),"\n",(0,n.jsx)(a.h3,{id:"1-hoodietable",children:"1) HoodieTable"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:"/**\n  * Abstract implementation of a HoodieTable.\n  *\n  * @param <T> Sub type of HoodieRecordPayload\n  * @param <I> Type of inputs\n  * @param <K> Type of keys\n  * @param <O> Type of outputs\n  */\npublic abstract class HoodieTable<T extends HoodieRecordPayload, I, K, O> implements Serializable {\n\n   protected final HoodieWriteConfig config;\n   protected final HoodieTableMetaClient metaClient;\n   protected final HoodieIndex<T, I, K, O> index;\n\n   public abstract HoodieWriteMetadata<O> upsert(HoodieEngineContext context, String instantTime,\n       I records);\n\n   public abstract HoodieWriteMetadata<O> insert(HoodieEngineContext context, String instantTime,\n       I records);\n\n   public abstract HoodieWriteMetadata<O> bulkInsert(HoodieEngineContext context, String instantTime,\n       I records, Option<BulkInsertPartitioner<I>> bulkInsertPartitioner);\n\n   ...\n}\n"})}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.code,{children:"HoodieTable"})," is one of the core abstractions of Hudi, which defines operations such as ",(0,n.jsx)(a.code,{children:"insert"}),", ",(0,n.jsx)(a.code,{children:"upsert"}),", and ",(0,n.jsx)(a.code,{children:"bulkInsert"})," supported by the table.\nTake ",(0,n.jsx)(a.code,{children:"upsert"})," as an example, the input data is changed from the original ",(0,n.jsx)(a.code,{children:"JavaRDD<HoodieRecord> inputRdds"})," to ",(0,n.jsx)(a.code,{children:"I records"}),", and the runtime ",(0,n.jsx)(a.code,{children:"JavaSparkContext jsc"})," is changed to ",(0,n.jsx)(a.code,{children:"HoodieEngineContext context"}),"."]}),"\n",(0,n.jsxs)(a.p,{children:["From the class annotations, we can see that ",(0,n.jsx)(a.code,{children:"T, I, K, O"})," represents the load data type, input data type, primary key type and output data type of Hudi operation respectively.\nThese generics will run through the entire abstraction layer."]}),"\n",(0,n.jsx)(a.h3,{id:"2-hoodieenginecontext",children:"2) HoodieEngineContext"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:"/**\n * Base class contains the context information needed by the engine at runtime. It will be extended by different\n * engine implementation if needed.\n */\npublic abstract class HoodieEngineContext {\n\n  public abstract <I, O> List<O> map(List<I> data, SerializableFunction<I, O> func, int parallelism);\n\n  public abstract <I, O> List<O> flatMap(List<I> data, SerializableFunction<I, Stream<O>> func, int parallelism);\n\n  public abstract <I> void foreach(List<I> data, SerializableConsumer<I> consumer, int parallelism);\n\n  ......\n}\n"})}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.code,{children:"HoodieEngineContext"})," plays the role of ",(0,n.jsx)(a.code,{children:"JavaSparkContext"}),", it not only provides all the information that ",(0,n.jsx)(a.code,{children:"JavaSparkContext"})," can provide,\nbut also encapsulates many methods such as ",(0,n.jsx)(a.code,{children:"map"}),", ",(0,n.jsx)(a.code,{children:"flatMap"}),", ",(0,n.jsx)(a.code,{children:"foreach"}),", and hides The specific implementation of ",(0,n.jsx)(a.code,{children:"JavaSparkContext#map()"}),",",(0,n.jsx)(a.code,{children:"JavaSparkContext#flatMap()"}),", ",(0,n.jsx)(a.code,{children:"JavaSparkContext#foreach()"})," and other methods."]}),"\n",(0,n.jsxs)(a.p,{children:["Take the ",(0,n.jsx)(a.code,{children:"map"})," method as an example. In the Spark implementation class ",(0,n.jsx)(a.code,{children:"HoodieSparkEngineContext"}),", the ",(0,n.jsx)(a.code,{children:"map"})," method is as follows:"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:"  @Override\n  public <I, O> List<O> map(List<I> data, SerializableFunction<I, O> func, int parallelism) {\n    return javaSparkContext.parallelize(data, parallelism).map(func::apply).collect();\n  }\n"})}),"\n",(0,n.jsxs)(a.p,{children:["In the engine that operates List, the implementation can be as follows (different methods need to pay attention to thread-safety issues, use ",(0,n.jsx)(a.code,{children:"parallel()"})," with caution):"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:"  @Override\n  public <I, O> List<O> map(List<I> data, SerializableFunction<I, O> func, int parallelism) {\n    return data.stream().parallel().map(func::apply).collect(Collectors.toList());\n  }\n"})}),"\n",(0,n.jsxs)(a.p,{children:["Note:\nThe exception thrown in the map function can be solved by wrapping ",(0,n.jsx)(a.code,{children:"SerializableFunction<I, O> func"}),"."]}),"\n",(0,n.jsxs)(a.p,{children:["Here is a brief introduction to ",(0,n.jsx)(a.code,{children:"SerializableFunction"}),":"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:"@FunctionalInterface\npublic interface SerializableFunction<I, O> extends Serializable {\n  O apply(I v1) throws Exception;\n}\n"})}),"\n",(0,n.jsxs)(a.p,{children:["This method is actually a variant of ",(0,n.jsx)(a.code,{children:"java.util.function.Function"}),". The difference from ",(0,n.jsx)(a.code,{children:"java.util.function.Function"})," is that ",(0,n.jsx)(a.code,{children:"SerializableFunction"})," can be serialized and can throw exceptions.\nThis function is introduced because the input parameters that the ",(0,n.jsx)(a.code,{children:"JavaSparkContext#map()"})," function can receive must be serializable.\nAt the same time, there are many exceptions that need to be thrown in the logic of Hudi, and the code for ",(0,n.jsx)(a.code,{children:"try-catch"})," in the Lambda expression will be omitted It is bloated and not very elegant."]}),"\n",(0,n.jsx)(a.h2,{id:"6-current-progress-and-follow-up-plan",children:"6. Current progress and follow-up plan"}),"\n",(0,n.jsx)(a.h3,{id:"61-working-time-axis",children:"6.1 Working time axis"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"dualism",src:t(96248).A+"",width:"1204",height:"600"})}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.a,{href:"https://www.t3go.cn/",children:"T3go"}),"\n",(0,n.jsx)(a.a,{href:"https://cn.aliyun.com/",children:"Aliyun"}),"\n",(0,n.jsx)(a.a,{href:"https://www.sf-express.com/cn/sc/",children:"SF-express"})]}),"\n",(0,n.jsx)(a.h3,{id:"62-follow-up-plan",children:"6.2 Follow-up plan"}),"\n",(0,n.jsx)(a.h4,{id:"1-promote-the-integration-of-hudi-and-flink",children:"1) Promote the integration of Hudi and Flink"}),"\n",(0,n.jsx)(a.p,{children:"Push the integration of Flink and Hudi to the community as soon as possible. In the initial stage, this feature may only support Kafka data sources."}),"\n",(0,n.jsx)(a.h4,{id:"2-performance-optimization",children:"2) Performance optimization"}),"\n",(0,n.jsx)(a.p,{children:"In order to ensure the stability and performance of the Hudi-Spark version, the decoupling did not take too much into consideration the possible performance problems of the Flink version."}),"\n",(0,n.jsx)(a.h4,{id:"3-flink-connector-hudi-like-third-party-package-development",children:"3) flink-connector-hudi like third-party package development"}),"\n",(0,n.jsx)(a.p,{children:"Make the binding of Hudi-Flink into a third-party package. Users can this third-party package to read/write from/to Hudi with Flink."})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},57012:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/07/01/monitoring-table-size-stats","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-07-01-monitoring-table-size-stats.mdx","source":"@site/blog/2023-07-01-monitoring-table-size-stats.mdx","title":"Monitoring Table Size stats","description":"Redirecting... please wait!!","date":"2023-07-01T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"table size stats","permalink":"/blog/tags/table-size-stats"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.1,"hasTruncateMarker":false,"authors":[{"name":"Sivabalan Narayanan","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Monitoring Table Size stats","authors":[{"name":"Sivabalan Narayanan"}],"category":"blog","image":"/assets/images/blog/2023-07-01-monitoring-table-size-stats.png","tags":["blog","table size stats","medium"]},"unlisted":false,"prevItem":{"title":"Hudi Best Practices: Handling Failed Inserts/Upserts with Error Tables","permalink":"/blog/2023/07/02/Hudi-Best-Practices-Handling-Failed-Inserts-Upserts-with-Error-Tables"},"nextItem":{"title":"What about Apache Hudi, Apache Iceberg, and Delta Lake?","permalink":"/blog/2023/06/30/What-about-Apache-Hudi-Apache-Iceberg-and-Delta-Lake"}}')},57014:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide1-4395683e8b063979208436c3ecdecfbd.png"},57144:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/08/29/building-a-rag-based-ai-recommender-2","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-08-29-building-a-rag-based-ai-recommender-2.mdx","source":"@site/blog/2025-08-29-building-a-rag-based-ai-recommender-2.mdx","title":"Building a RAG-based AI Recommender (2/2)","description":"Redirecting... please wait!!","date":"2025-08-29T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"AI","permalink":"/blog/tags/ai"},{"inline":true,"label":"RAG","permalink":"/blog/tags/rag"},{"inline":true,"label":"Artificial Intelligence","permalink":"/blog/tags/artificial-intelligence"},{"inline":true,"label":"data lakehouse","permalink":"/blog/tags/data-lakehouse"},{"inline":true,"label":"Lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"datumagic","permalink":"/blog/tags/datumagic"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Shiyan Xu","key":null,"page":null}],"frontMatter":{"title":"Building a RAG-based AI Recommender (2/2)","author":"Shiyan Xu","category":"blog","image":"/assets/images/blog/2025-08-29-building-a-rag-based-ai-recommender-2.jpg","tags":["blog","Apache Hudi","AI","RAG","Artificial Intelligence","data lakehouse","Lakehouse","use-case","datumagic"]},"unlisted":false,"prevItem":{"title":"Automatic Record Key Generation in Apache Hudi","permalink":"/blog/2025/09/17/hudi-auto-gen-keys"},"nextItem":{"title":"A Deep Dive on Merge-on-Read (MoR) in Lakehouse Table Formats","permalink":"/blog/2025/07/21/mor-comparison"}}')},57356:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(18908),n=t(74848),s=t(28453);const r={title:"21 Unique Reasons Why Apache Hudi Should Be Your Next Data Lakehouse",excerpt:"Unique Differentiators of Apache Hudi, that stand out from other projects",author:"Vinoth Chandar",category:"blog",image:"/assets/images/blog/2025-03-05-21-reasons-why.png",tags:["Data Lake","Data Lakehouse","Apache Hudi","Apache Iceberg","Delta Lake","Table Format"]},o=void 0,l={authorsImageUrls:[void 0]},d=[];function c(e){const a={a:"a",em:"em",p:"p",strong:"strong",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(a.p,{children:["Apache Hudi is continuously ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0",children:"redefining"})," the data lakehouse, pushing the technical boundaries and offering cutting-edge features to handle data quickly and efficiently. If you have ever wondered how Apache Hudi has sustained its position over the years as the most comprehensive, open, high-performance data lakehouse project, this blog aims to give you some concise answers. Below, we shine a light on some unique capabilities in Hudi, that go beyond the lowest-common-denominator across the different projects in the space."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"1. Well-Balanced Storage Format"})}),"\n",(0,n.jsxs)(a.p,{children:["Hudi\u2019s ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/storage_layouts",children:"storage format"})," ",(0,n.jsx)(a.em,{children:"perfectly balances write speed"})," (record-level changes) and ",(0,n.jsx)(a.em,{children:"query performance"})," (scan+lookup optimized), at the cost of additional storage space to track indexes. In contrast, Apache Iceberg/Delta Lake formats produce storage layouts aimed at vanilla scans, focus more on metadata to help scale/prune the scans. Recent effots that adopt LSM tree structures to improve write performance, inevitably sacrifice query performance. See ",(0,n.jsx)(a.a,{href:"https://www.codementor.io/@arpitbhayani/the-rum-conjecture-16z2ckqte9",children:"RUM conjecture"}),"."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"2. Database-like Secondary Indexes"})}),"\n",(0,n.jsxs)(a.p,{children:["In a long line of unique technical contributions to the lakehouse tech, Hudi recently added ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/indexes#multi-modal-indexing",children:"secondary indexes"})," (record level, bloom filters, \u2026), with support for even creating indexes on expressions on columns. Features heavily inspired by relational databases like Postgres, that can ",(0,n.jsx)(a.em,{children:"unlock completely new use-cases"})," on the data lakehouse like ",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Hybrid_transactional/analytical_processing",children:"HTAP"})," or ",(0,n.jsx)(a.a,{href:"https://planetscale.com/learn/courses/mysql-for-developers/queries/indexing-joins",children:"index-joins"}),"."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"3. Efficient Merge-on-Read (MoR) Design"})}),"\n",(0,n.jsxs)(a.p,{children:["Hudi\u2019s ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/table_types#merge-on-read-table",children:"optimized MoR design"})," ",(0,n.jsx)(a.em,{children:"minimizes read/write amplification"}),", by a range of techniques like file grouping and partial updates. Grouping helps cut down the amount of update blocks/deletion blocks/vectors to be scanned to serve snapshot queries. It also helps ",(0,n.jsx)(a.em,{children:"preserve temporal locality"})," of data that dramatically improves time-based access for e.g building dashboards based on time - last hour, last day, last week, \u2026 - that are table stakes for warehouse/lakehouse users."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"4. Scalable Metadata for Large-Scale Datasets"})}),"\n",(0,n.jsxs)(a.p,{children:["Hudi\u2019s ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/metadata",children:"metadata table"})," efficiently handles ",(0,n.jsx)(a.em,{children:"millions of files"}),", by storing them ",(0,n.jsx)(a.em,{children:"efficiently"})," in an indexed ",(0,n.jsx)(a.a,{href:"https://www.scylladb.com/glossary/sstable",children:"SSTable"})," based file format. Similarly, Hudi also indexes other metadata like column statistics, such that query planning scales linearly with ",(0,n.jsx)(a.em,{children:"O(number_of_columns_in_query)"}),", as opposed to flat-file storage like avro that scales poorly with size of tables, large number of files or wide-columns."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"5. Built-In Table Services"})}),"\n",(0,n.jsxs)(a.p,{children:["Hudi comes ",(0,n.jsxs)(a.em,{children:["loaded with automated ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/write_operations#write-path",children:"table services"})]})," like compaction, clustering, indexer, de-duplication, archiver, TTL enforcement and cleaning, that are scheduled, executed, retried, automatically with every write without requiring any external orchestration or manual SQL commands for table maintenance. Hudi\u2019s ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/markers/",children:"marker mechanism"})," efficiently cleans up uncomitted/orphaned files during writes without requiring full-listing of cloud storage to identify such files (can take hours or even timeout forever)."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"6. Data Management Smarts"})}),"\n",(0,n.jsxs)(a.p,{children:["Stepping in level deeper, Hudi fully manages everything around storage : ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/overview",children:"file sizes, partitions and metadata maintenance"})," automatically on each write, to provide consistent, dependable read/write performance. Further more,  Hudi provides ",(0,n.jsxs)(a.em,{children:["advanced ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/clustering",children:"sorting/clustering"})," capabilities"]}),", that can be ",(0,n.jsx)(a.em,{children:"incrementally"})," run with new writes, to keep tables optimized."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"7. Concurrency Control Purpose-built For the Lake"})}),"\n",(0,n.jsxs)(a.p,{children:["Hudi\u2019s ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2025/01/28/concurrency-control",children:"concurrency control"})," is carefully designed to deliver high throughput for data lakehouse workloads, without blindly rehashing approaches that work for OLTP databases. Hudi brings novel MVCC based approaches and ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/concurrency_control#non-blocking-concurrency-control",children:"non-blocking concurrency control"}),". Data pipelines/SQL ETLs and table services won\u2019t fail/livelock each other eliminating wastage of compute cycles, improving data freshness and reducing cloud bills. Even on optimistic concurrency control model (L.C.D across projects), Hudi provides ",(0,n.jsx)(a.em,{children:"early conflict detection"})," to pre-emptively abort writes that will eventually fail due to conflicts, saving countless compute hours."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"8. Performance at Scale"})}),"\n",(0,n.jsxs)(a.p,{children:["Hudi stands out on the ",(0,n.jsx)(a.em,{children:"toughest workloads"})," you should be testing first before deciding your lakehouse stack : CDC ingest, expensive SQL merges or TB-PB scale streaming data. Hudi provides about ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/indexes#additional-writer-side-indexes",children:"half a dozen writer side indexes"})," including advanced record level indexes, range indexes built on interval trees or consistent-hashed bucket indexes to scale writes for such workloads. Hudi is the ",(0,n.jsx)(a.em,{children:"only lakehouse project"}),", that can rapidly ingest/write and handle small-file compaction without blocking those writes."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"9. Out-of-box CDC/Streaming Ingestion"})}),"\n",(0,n.jsxs)(a.p,{children:["Hudi provides ",(0,n.jsx)(a.em,{children:"powerful, fully-production ready  ingestion"})," ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/hoodie_streaming_ingestion",children:"tools"})," for both Spark/Flink/Kafka users, that help users build data lakehouses from their data, with a single-command. In fact, many many Hudi users blissfully use these tools, unaware of all the underlying machinery balancing write/read performance or table maintenance. This way, Hudi provides a self-managing runtime environment, for your data lakehouse pipelines, without having to pay for closed-services from vendors. Hudi ingest tools natively support popular CDC formats like Debezium/AWS DMS/Mongo and sources like S3, GCS, Kafka, Pulsar and the like."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"10. First-Class Support for Keys"})}),"\n",(0,n.jsxs)(a.p,{children:["Hudi treats record ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/key_generation",children:"keys"})," as first-class citizen, used everywhere from indexing, de-duplication, clustering, compaction to consistently track/control movement of records within a table, across files. Additionally, Hudi also tracks ",(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/blog/hudi-metafields-demystified",children:"necessary record-level metadata"})," that help implement powerful features like incremental queries, in conjunction with queries. Ingest tools seamlessly map source primary keys to Hudi primary keys or auto-generate ",(0,n.jsx)(a.em,{children:"highly-compressible"})," keys to aid these capabilities."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"11. Streaming-First Design"})}),"\n",(0,n.jsxs)(a.p,{children:["Hudi was born out of a need to bridge the gap between batch processing and stream processing models. Thus, naturally, Hudi offers ",(0,n.jsx)(a.em,{children:"best-in-class and unique capabilities"})," around handling streaming data. Hudi supports ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/record_merger#event_time_ordering",children:"event time ordering"})," and late data handling natively in storage where MoR is employed heavily. RecordPayload/RecordMerger APIs let you merge updates in the database LSN order compared to other approaches, avoiding cases like tables going back in (event) time, if the input is out-of-order/late-arriving (which is more the norm/nor an exception)."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"12. Efficient Incremental Processing"})}),"\n",(0,n.jsxs)(a.p,{children:["All roads in Hudi, lead to efficiency in storage and compute. Storage by ",(0,n.jsx)(a.em,{children:"reducing"})," the amount of ",(0,n.jsx)(a.em,{children:"data stored/accessed"}),", compute by reducing the ",(0,n.jsx)(a.em,{children:"time needed write/read"}),". Hudi supports unique ",(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/blog/getting-started-incrementally-process-data-with-apache-hudi",children:"incremental queries"}),", along with CDC queries to allow downstream data consumers to quickly obtain changes to a table, between two time intervals. Owing to scalable metadata design, a LSM-tree backed timeline history and record-level change tracking, Hudi is able to support near infinite retention for such streams, provide very useful when dealing with transactional data/logs."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"13. Powerful Apache Spark Implementation"})}),"\n",(0,n.jsxs)(a.p,{children:["Hudi comes with a very feature-rich, advanced integration with Apache Spark - across SQL, DataSource, RDD APIs, Structured Streaming and Spark Streaming. When combined together, ",(0,n.jsx)(a.em,{children:"Hudi + Spark"})," almost gives users a ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/rfc/rfc-69/rfc-69.md",children:"database"})," - with built-in data management, ingestion, streaming/batch APIs, ANSI SQL and programmatic access from Python/JVM. Much like a database, the write/read implementation paths automatically pick the right storage layout to optimize storage at rest or do necessary index pruning to speed up queries."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"14. Next-Gen Flink Writer for Streaming Pipelines"})}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/blog/intro-to-hudi-and-flink",children:"Hudi and Flink"})," have the best impedance match when it comes to handling streaming data. Hudi Flink sink is built on a ",(0,n.jsx)(a.em,{children:"deep integration"})," between the two project capabilities, by leveraging Flink\u2019s state backends as an writer side index in Hudi. With the combination of non-blocking concurrency and partial updates, Hudi is the only lakehouse storage sink for Flink, that can allow ",(0,n.jsx)(a.em,{children:"multiple streaming writers"})," concurrently write a table (without having to fail one). Just like Spark, Flink writer comes with built-in table services, akin to a \u201cstreaming database\u201d for the lakehouse."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"15. Avoid Compute Lockins"})}),"\n",(0,n.jsxs)(a.p,{children:["Don\u2019t let the noise fool you. Hudi is ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/ecosystem",children:(0,n.jsx)(a.em,{children:"widely supported"})})," across cloud warehouses (Redshift, BigQuery), open-source query/processing engines (Spark, Presto, Trino, Flink, Hive, Clickhouse, Starrocks, Doris) and also hosted offering of those open-source engines (AWS Athena, EMR, DataProc, Databricks). This means, you have the power to fully control ",(0,n.jsx)(a.em,{children:"not just the open format"})," you store data in, but also the end-end ingestion, transformation and optimizations of your tables, avoiding any \u201ccompute lockin\u201d with these engines."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"16. Seamless Interop Iceberg/Delta Lake and Catalog Syncs"})}),"\n",(0,n.jsxs)(a.p,{children:["To make the point above really easy, Hudi also ships with a ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/syncing_aws_glue_data_catalog",children:"catalog sync"})," mechanism, that supports about ",(0,n.jsx)(a.em,{children:"6 different data catalogs"})," to keep your table definitions in sync over time. Hudi tables can be readily queried as external tables on cloud data warehouses. And, with the ",(0,n.jsx)(a.a,{href:"https://github.com/apache/xtable",children:"Apache XTable"})," (Incubating) catalog sync, Hudi enables interoperability with Iceberg and Delta Lake table format, without the need to duplicate data storage or processing. Thus, Hudi offers the most open way to manage your data on the cloud."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"17. Truly Open and Community-Driven"})}),"\n",(0,n.jsxs)(a.p,{children:["Apache Hudi is an ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/community",children:"open-source project"}),", actively developed by a diverse global ",(0,n.jsx)(a.a,{href:"https://ossinsight.io/analyze/apache/hudi#contributors",children:"community"}),". In fact, the grass-roots nature of the project and its community have been the crucial reason for the lasting success Hudi has had in the industry, inspite 100-1000x bigger vendor teams marketing/selling users in other directions. Project has an established track record of truly, collaborative way of software development, the ",(0,n.jsx)(a.a,{href:"https://www.apache.org/theapacheway/",children:"apache way"}),"."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"18. Massive Adoption Across Industries"})}),"\n",(0,n.jsxs)(a.p,{children:["For system/infrastructure software like Hudi, it\u2019s very important to gain/prove maturity by clocking massive amounts of server hours. Hudi is used at massive scale at much of the Fortune 100s and large organizations like  ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/powered-by",children:"Uber, AWS, ByteDance, Peloton, Huawei, Alibaba, and more"}),", adding immense value in terms of a steady stream of  high-quality bug reports and feature asks shaping the projects roadmap. This way, Hudi users get highly capable lakehouse software, that can address a diverse range of use-cases."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"19. Proven Reliability in High-Pressure Workloads"})}),"\n",(0,n.jsxs)(a.p,{children:["Hudi has been pressure-tested at some of the most demanding worloads there is, on the data lakehouse. From ",(0,n.jsx)(a.a,{href:"https://www.uber.com/blog/uber-big-data-platform/",children:"minute-level latency"})," on petabytes to storing ingesting > 100GB/s or just very ",(0,n.jsx)(a.a,{href:"https://aws.amazon.com/blogs/big-data/how-amazon-transportation-service-enabled-near-real-time-event-analytics-at-petabyte-scale-using-aws-glue-with-apache-hudi/",children:"tough random write"})," workloads, that test even the best OLTP databases out there. Hudi has been deployed industry-wide for very critical data processing needs like financial clearing jobs, ride-sharing payments or transactional reconciliation."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"20. Cloud-Native and Lakehouse-Ready"})}),"\n",(0,n.jsxs)(a.p,{children:["Don\u2019t let the origins from a Hadoop mislead you either. Hudi has long evolved past HDFS and works seamlessly with ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/cloud",children:"S3, GCS, Azure, Alibaba, Huawei and many other cloud storage"})," systems. Together with the ",(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/blog/apache-hudi-native-aws-integrations",children:"cloud-native"})," integrations or just via ",(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/blog/apache-hudi-on-microsoft-azure",children:"easy integrations"})," outside of Cloud-native services, Hudi provides a very portable (cross-engine, format, cloud) way for building cloud data lakehouses."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"21. Future-Proof and Actively Evolving"})}),"\n",(0,n.jsxs)(a.p,{children:["Hudi\u2019s community boasts about 40-50 monthly active developers, which is growing even more with efforts like ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi-rs",children:"hudi-rs"}),". Hudi\u2019s ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi",children:"rapid development"})," ensures constant improvements and cutting-edge features on one hand, while the openness of the community to truly work across the entire cloud data ecosystem on the other, ensure your data stays as open as possible."]}),"\n",(0,n.jsx)(a.p,{children:"In summary, there is no secret sauce. The answer to the original question is simply how these design and implementation differences have compounded over time into unmatched technical capabilities that data engineers across the industry widely recognize. These have resulted from 6+ years of evolution, hardening and iteration from an OSS community. And, it's always a moving target, given the amount of innovation that is still ahead of us, in the data lakehouse space. By the time some of these differences make it to other projects, the community might have innovated 21 more reasons."}),"\n",(0,n.jsxs)(a.p,{children:["Apache Hudi is the ",(0,n.jsx)(a.strong,{children:"best-in-class open-source data lakehouse platform"})," \u2014powerful, efficient, and future-proof. Start exploring it today! \ud83d\ude80"]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},57590:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2020/10/15/apache-hudi-meets-apache-flink","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-10-15-apache-hudi-meets-apache-flink.md","source":"@site/blog/2020-10-15-apache-hudi-meets-apache-flink.md","title":"Apache Hudi meets Apache Flink","description":"Apache Hudi (Hudi for short) is a data lake framework created at Uber. Hudi joined the Apache incubator for incubation in January 2019, and was promoted to the top Apache project in May 2020. It is one of the most popular data lake frameworks.","date":"2020-10-15T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache flink","permalink":"/blog/tags/apache-flink"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":10.05,"hasTruncateMarker":true,"authors":[{"name":"wangxianghu","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi meets Apache Flink","excerpt":"The design and latest progress of the integration of Apache Hudi and Apache Flink.","author":"wangxianghu","category":"blog","image":"/assets/images/blog/2020-10-15-apache-hudi-meets-apache-flink.png","tags":["blog","apache flink","apache hudi"]},"unlisted":false,"prevItem":{"title":"Origins of Data Lake at Grofers","permalink":"/blog/2020/10/19/Origins-of-Data-Lake-at-Grofers"},"nextItem":{"title":"How nClouds Helps Accelerate Data Delivery with Apache Hudi on Amazon EMR","permalink":"/blog/2020/10/06/cdc-solution-using-hudi-by-nclouds"}}')},57738:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/09/12/Lakehouse-or-Warehouse-Part-2-of-2","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-09-12-Lakehouse-or-Warehouse-Part-2-of-2.mdx","source":"@site/blog/2023-09-12-Lakehouse-or-Warehouse-Part-2-of-2.mdx","title":"Lakehouse or Warehouse? Part 2 of 2","description":"Redirecting... please wait!!","date":"2023-09-12T00:00:00.000Z","tags":[{"inline":true,"label":"data warehouse","permalink":"/blog/tags/data-warehouse"},{"inline":true,"label":"data lakehouse","permalink":"/blog/tags/data-lakehouse"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"onehouse","permalink":"/blog/tags/onehouse"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Floyd Smith","key":null,"page":null}],"frontMatter":{"title":"Lakehouse or Warehouse? Part 2 of 2","excerpt":"Lakehouse or Warehouse? Part 2 of 2","author":"Floyd Smith","category":"blog","image":"/assets/images/blog/2023-09-12-Lakehouse-or-Warehouse-Part-2-of-2.png","tags":["data warehouse","data lakehouse","apache hudi","onehouse","blog"]},"unlisted":false,"prevItem":{"title":"Simplify operational data processing in data lakes using AWS Glue and Apache Hudi","permalink":"/blog/2023/09/13/Simplify-operational-data-processing-in-data-lakes-using-AWS-Glue-and-Apache-Hudi"},"nextItem":{"title":"Demystifying Copy-on-Write in Apache Hudi: Understanding Read and Write Operations","permalink":"/blog/2023/09/10/Demystifying-Copy-on-Write-in-Apache-Hudi-Understanding-Read-and-Write-Operations"}}')},58043:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(61684),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi: How Uber gets data a ride to its destination",authors:[{name:"Joe McKendrick"}],category:"blog",tags:["blog","rtinsights"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.rtinsights.com/apache-hudi-how-uber-gets-data-a-ride-to-its-destination/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},58113:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/06/20/How-to-query-data-in-Apache-Hudi-using-StarRocks","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-06-20-How-to-query-data-in-Apache-Hudi-using-StarRocks.mdx","source":"@site/blog/2023-06-20-How-to-query-data-in-Apache-Hudi-using-StarRocks.mdx","title":"How to query data in Apache Hudi using StarRocks","description":"Redirecting... please wait!!","date":"2023-06-20T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"starrocks","permalink":"/blog/tags/starrocks"},{"inline":true,"label":"queries","permalink":"/blog/tags/queries"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Albert Wong","socials":{},"key":null,"page":null}],"frontMatter":{"title":"How to query data in Apache Hudi using StarRocks","authors":[{"name":"Albert Wong"}],"category":"blog","image":"/assets/images/blog/2023-06-20-How-to-query-data-in-Apache-Hudi-using-StarRocks.png","tags":["blog","starrocks","queries","medium"]},"unlisted":false,"prevItem":{"title":"Multi-writer support with Apache Hudi","permalink":"/blog/2023/06/24/multi-writer-support-in-apache-hudi"},"nextItem":{"title":"Timeline Server in Apache Hudi","permalink":"/blog/2023/06/20/timeline-server-in-apache-hudi"}}')},58262:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(52816),n=t(74848),s=t(28453);const r={title:"What is a Data Lakehouse & How does it Work?",excerpt:"Explains the concept of the lakehouse architecture",author:"Dipankar Mazumdar",category:"blog",image:"/assets/images/blog/dlh_1200.png",tags:["data lakehouse","Apache Hudi","Apache Iceberg","Delta Lake","Open Architecture"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"The Evolution of Data Storage Solutions: How did we go from Warehouses to Lakes to Lakehouses?",id:"the-evolution-of-data-storage-solutions-how-did-we-go-from-warehouses-to-lakes-to-lakehouses",level:2},{value:"Introducing: Data Lakehouses",id:"introducing-data-lakehouses",level:2},{value:"Advantages of Data Lakehouses",id:"advantages-of-data-lakehouses",level:2},{value:"Implementing a Data Lakehouse",id:"implementing-a-data-lakehouse",level:2},{value:"Data Ingestion",id:"data-ingestion",level:3},{value:"Metadata &amp; Transactional Layer",id:"metadata--transactional-layer",level:3},{value:"Processing Layer",id:"processing-layer",level:3},{value:"Catalog Layer",id:"catalog-layer",level:3},{value:"Use Cases",id:"use-cases",level:2},{value:"Unified Batch &amp; Streaming",id:"unified-batch--streaming",level:3},{value:"Diverse Analytical Workloads",id:"diverse-analytical-workloads",level:3},{value:"Cost-Effective Data Management",id:"cost-effective-data-management",level:3},{value:"Real World Examples",id:"real-world-examples",level:2},{value:"Key Data Lakehouse Technologies",id:"key-data-lakehouse-technologies",level:2},{value:"Open Source Solutions",id:"open-source-solutions",level:3},{value:"Apache Hudi",id:"apache-hudi",level:4},{value:"Apache Iceberg",id:"apache-iceberg",level:4},{value:"Delta Lake",id:"delta-lake",level:4},{value:"Vendor Lakehouse Platforms",id:"vendor-lakehouse-platforms",level:3},{value:"Onehouse",id:"onehouse",level:4},{value:"Databricks",id:"databricks",level:4},{value:"Snowflake",id:"snowflake",level:4},{value:"The Future of Data Lakehouses",id:"the-future-of-data-lakehouses",level:2},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const a={a:"a",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.p,{children:"A data lakehouse is a hybrid data architecture that combines the best attributes of data warehouses and data lakes to address their respective limitations. This innovative approach to data management brings the transactional capabilities of data warehouses to cloud-based data lakes, offering scalability at lower costs."}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{alt:"/assets/images/blog/dlh_new.png",src:t(95584).A+"",width:"2323",height:"1259"}),"\n",(0,n.jsx)("p",{align:"center",children:"Figure: Data Lakehouse Architecture"})]}),"\n",(0,n.jsx)(a.p,{children:"The lakehouse architecture supports the management of various data types, such as structured, semi-structured, and unstructured, and caters to a wide range of use cases, including business intelligence, machine learning, and real-time streaming. This flexibility enables businesses to move away from the traditional two-tier architecture\u2014using warehouses for relational workloads and data lakes for machine learning and advanced analytics. As a result, organizations can reduce operational costs and streamline their data strategies by working on a single data store."}),"\n",(0,n.jsx)(a.h2,{id:"the-evolution-of-data-storage-solutions-how-did-we-go-from-warehouses-to-lakes-to-lakehouses",children:"The Evolution of Data Storage Solutions: How did we go from Warehouses to Lakes to Lakehouses?"}),"\n",(0,n.jsx)(a.p,{children:"Historically, organizations have been investing in building centralized and scalable data architectures to enable more data access and to support different types of analytical workloads. As demand for these workloads has grown, data architectures have evolved to address the complex needs of modern data processing and storage."}),"\n",(0,n.jsx)(a.p,{children:"Data warehouses were among the first to serve as centralized repositories for structured workloads, allowing organizations to derive historical insights from disparate data sources. However, they also introduce challenges, including proprietary storage formats that can result in lock-in issues, and limited support for analytical workloads, particularly with unstructured data like machine learning."}),"\n",(0,n.jsxs)(a.p,{children:["Data lakes emerged as the next generation of analytics architectures, enabling organizations to scale storage and compute independently, thereby optimizing resources and enhancing cost efficiency. They support storing all types of data\u2014structured, semi-structured, and unstructured\u2014in low-cost storage systems using open file formats like ",(0,n.jsx)(a.a,{href:"https://parquet.apache.org",children:"Apache Parquet"})," and ",(0,n.jsx)(a.a,{href:"https://orc.apache.org",children:"Apache ORC"}),". Although data lakes offer flexibility with their schema-on-read approach, they lack transactional capabilities (ACID characteristics) and often face challenges related to data quality and governance."]}),"\n",(0,n.jsx)(a.p,{children:"The challenges presented by these two data management approaches led to the development of a new architecture called data lakehouse. A lakehouse brings the transactional capabilities of database management systems (DBMS) to scalable data lakes, enabling running various types of workloads on open storage formats."}),"\n",(0,n.jsx)(a.h2,{id:"introducing-data-lakehouses",children:"Introducing: Data Lakehouses"}),"\n",(0,n.jsx)(a.p,{children:"A data lakehouse combines the reliability and performance of data warehouses with the scalability and cost-effectiveness of data lakes. This combined approach enables features such as time-travel, indexing, schema evolution, and performance optimization capabilities on openly accessible formats."}),"\n",(0,n.jsx)(a.p,{children:"Specifically, a lakehouse architecture is characterized by the following attributes."}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Open Data Architecture"}),": A lakehouse stores data in open storage formats. This allows various analytical workloads to be run by different engines (from multiple vendors) on the same data, preventing lock-in to proprietary formats."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Support for Varied Data Types & Workloads"}),": Data lakehouses accommodate a diverse range of data types\u2014including structured, semi-structured, and unstructured\u2014and are therefore equipped to handle various analytical workloads, such as business intelligence, machine learning, and real-time analytics."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Transactional support"}),": Data lakehouses enhance reliability and consistency by providing ACID guarantees in transactions, such as INSERT or UPDATE, akin to those in an RDBMS-OLAP system. This ensures safe, concurrent reads and writes."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Less data copies"}),": A data lakehouse minimizes data duplication since the compute engine can directly access data from open storage formats."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Schema management"}),": Data lakehouses ensure that a specific schema is adhered to when writing new data into the storage. They also facilitate schema evolution over time without the need to rewrite the entire table, thus reducing storage and operational costs."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Data Quality and Governance"}),": Lakehouses ensure data integrity and incorporate robust governance and auditing mechanisms. These features uphold high data quality standards, facilitate regulatory compliance (such as GDPR), and enable secure data management practices."]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"A data lakehouse architecture consists of six technical components that are modular, offering the flexibility to select and combine the best technologies based on specific requirements."}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Lake Storage"}),": The storage is where files from various operational systems land after ingestion through ETL/ELT processes. Cloud object stores such as Amazon S3, Azure Blob, and Google Cloud Storage support any type of data and can scale to virtually unlimited volumes. Their cost-effectiveness is a significant advantage over traditional data warehouse storage costs."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"File Format: File Format"}),": In a lakehouse architecture, file formats like Apache Parquet or ORC store the actual raw data on object storage. These open formats enable multiple engines to consume the data for various workloads. Being typically column-oriented, they offer significant advantages in data reading."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Table Format"}),": A key component of a lakehouse architecture is the table format, which is open in nature and acts as a metadata layer above file formats like Apache Parquet. This layer abstracts the complexity of the physical data structure by defining a schema on top of immutable data files. It allows different engines to concurrently read and write on the same dataset, supporting ACID-based transactions. Table formats like ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org",children:"Apache Hudi"}),", ",(0,n.jsx)(a.a,{href:"https://iceberg.apache.org",children:"Apache Iceberg"}),", and ",(0,n.jsx)(a.a,{href:"https://delta.io",children:"Delta Lake"})," bring essential features such as schema evolution, partitioning, and time travel."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Storage Engine"}),": The storage engine in a lakehouse orchestrates essential data management tasks including clustering, compaction, cleaning, and indexing to streamline data organization in cloud object storages for improved query performance. Both open source and proprietary lakehouse platforms are equipped with native storage engines that enhance these capabilities, optimizing the storage layout effectively."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Catalog"}),": Often referred to as a metastore, the catalog is a crucial component of the lakehouse architecture that facilitates efficient search and discovery by tracking all tables and their metadata. It records table names, schemas (column names and types), and references to each table's specific metadata (table format)."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Compute Engine"}),": The compute engine in a lakehouse processes data and ensures efficient read and write performance. It interacts with data using read and write APIs provided by table formats. Compute engines are tailored to specific workloads, with options such as ",(0,n.jsx)(a.a,{href:"https://trino.io",children:"Trino"})," and ",(0,n.jsx)(a.a,{href:"https://prestodb.io",children:"Presto"})," for low-latency ad hoc SQL, ",(0,n.jsx)(a.a,{href:"https://flink.apache.org",children:"Apache Flink"})," for streaming, and ",(0,n.jsx)(a.a,{href:"https://spark.apache.org",children:"Apache Spark"})," for machine learning tasks."]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"advantages-of-data-lakehouses",children:"Advantages of Data Lakehouses"}),"\n",(0,n.jsx)(a.p,{children:"A lakehouse architecture, characterized by its open data storage formats and cost-effective options, offers numerous advantages. Here are some key benefits:"}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{children:"Attributes"}),(0,n.jsx)(a.th,{children:"Description"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"Open Data Foundation"}),(0,n.jsx)(a.td,{children:"Data in a lakehouse is stored in open file formats like Apache Parquet and table formats such as Apache Hudi, Iceberg, or Delta Lake. This allows various engines to concurrently work on the same data, enhancing accessibility and compatibility."})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"Unified Data Platform"}),(0,n.jsx)(a.td,{children:"Lakehouses combine the functionalities of data warehouses and lakes into a single platform, supporting both types of workloads efficiently. This integration simplifies data management and accelerates analytics processes."})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"Centralized Data Repository for Diverse Data Types"}),(0,n.jsx)(a.td,{children:"A lakehouse architecture can store and manage structured, semi-structured, and unstructured data, serving different types of analytical workloads."})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"Cost Efficiency"}),(0,n.jsx)(a.td,{children:"Using low-cost cloud storage options and reducing the need for managing multiple systems significantly lowers overall engineering and ETL costs."})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"Performance and Scalability"}),(0,n.jsx)(a.td,{children:"Lakehouses allow independent scaling of storage and compute resources, which can be adjusted based on demand, ensuring high concurrency and cost-effective scalability."})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"Enhanced Query Performance"}),(0,n.jsx)(a.td,{children:"Lakehouse\u2019s storage engine component optimizes data layout in formats like Parquet and ORC to offer high performance comparable to traditional data warehouses, on large datasets."})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"Data Governance and Management"}),(0,n.jsx)(a.td,{children:"Lakehouses centralize data storage and management, streamlining the deployment of governance policies and security measures. This consolidation makes it easier to monitor, control, and secure data."})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"Improved Data Quality and Consistency"}),(0,n.jsx)(a.td,{children:"Lakehouses enforce strict schema adherence and provide transactional consistency, which minimizes write job failures and ensures data reliability."})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"Support for various Compute Engines"}),(0,n.jsx)(a.td,{children:"A lakehouse architecture supports SQL-based engines, ML tools, and streaming engines, making it versatile for handling diverse analytical demands on a single data store."})]})]})]}),"\n",(0,n.jsx)(a.h2,{id:"implementing-a-data-lakehouse",children:"Implementing a Data Lakehouse"}),"\n",(0,n.jsx)(a.p,{children:"The modular and open design of data lakehouse architecture allows for selection of best-of-breed engines and tools according to specific requirements. Therefore, the implementation of a lakehouse can vary based on the use case. This section outlines common considerations for implementing a lakehouse architecture. Given the variability in complexity (workloads, security, etc.) and tool stack, large-scale implementations may require tailored approaches."}),"\n",(0,n.jsx)(a.h3,{id:"data-ingestion",children:"Data Ingestion"}),"\n",(0,n.jsxs)(a.p,{children:["The first phase in a lakehouse architecture involves extracting and loading data into a cloud-based low cost data lake such as ",(0,n.jsx)(a.a,{href:"https://aws.amazon.com/s3/",children:"Amazon S3"}),', where it lands in its raw format (Parquet files). This approach utilizes the "schema-on-read" method, which means there\'s no need to process data immediately upon arrival. Once the data is in place, transformation logic can be applied to shift towards a "schema-on-write" setup, which organizes the data for specific analytical workloads such as ad hoc SQL queries or machine learning.']}),"\n",(0,n.jsx)(a.h3,{id:"metadata--transactional-layer",children:"Metadata & Transactional Layer"}),"\n",(0,n.jsx)(a.p,{children:"To enable transactional capabilities, Apache Hudi, Apache Iceberg or Delta Lake can be chosen as the table format. They provide a robust metadata layer with a table-like schema atop the physical data files in the object store. Together with the storage engine, they bring in data optimization strategies to maintain fast and efficient query performance. The metadata layer also facilitates capabilities such as time-travel querying, version rollbacks, and schema evolution akin to a traditional data warehouse."}),"\n",(0,n.jsx)(a.h3,{id:"processing-layer",children:"Processing Layer"}),"\n",(0,n.jsx)(a.p,{children:"The compute engine is a crucial component in a lakehouse architecture that processes the data files managed by the table format. Depending on the specific workload, SQL-based distributed query engines like Presto or Trino can be used for ad-hoc interactive analytics, or Apache Spark for distributed ETL tasks. Lakehouse table formats provide several optimizations such as indexes, and statistics, along with data layout optimizations including clustering, compaction, and Z-ordering. These enable the compute engines to achieve performance comparable to traditional data warehouses."}),"\n",(0,n.jsx)(a.h3,{id:"catalog-layer",children:"Catalog Layer"}),"\n",(0,n.jsx)(a.p,{children:"The catalog layer in a lakehouse architecture is responsible for tracking all tables and maintaining essential metadata. It ensures that data is easily accessible to query engines, supporting efficient data management, accessibility, and governance. Options for catalog implementation include Unity Catalog, AWS Glue, Hive Metastore, and file system-based ones. This layer plays a key role in upholding data quality and governance standards by establishing policies for data validation, security measures, and compliance protocols."}),"\n",(0,n.jsx)(a.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,n.jsx)(a.p,{children:"A Lakehouse architecture is used for a multitude of use cases. Here are some prominent examples."}),"\n",(0,n.jsx)(a.h3,{id:"unified-batch--streaming",children:"Unified Batch & Streaming"}),"\n",(0,n.jsx)(a.p,{children:"Traditional analytics architectures often separate real-time and batch storage, using specialized data stores for real-time insights and data lakes for delayed batch processing. Lakehouse platforms bridge this divide by introducing streaming capabilities to data lakes, allowing data ingestion within minutes and the creation of faster incremental pipelines. This integration reduces data freshness issues and eliminates the need for significant upfront infrastructure investments, making it a scalable and cost-effective solution for complex analytics."}),"\n",(0,n.jsx)(a.h3,{id:"diverse-analytical-workloads",children:"Diverse Analytical Workloads"}),"\n",(0,n.jsx)(a.p,{children:"Lakehouse architecture supports various data types\u2014structured, semi-structured, and unstructured\u2014enabling users to run both BI and ML workloads on the same dataset without the need for costly data duplication or movement. This unified approach allows data scientists and analysts to easily access and manipulate data for training ML models, deploying AI algorithms, and conducting in-depth BI analysis. By eliminating the need to create and maintain separate BI extracts and cubes, lakehouses reduce both storage and compute costs while maintaining a simple, self-service model for end-users. As a result, organizations can streamline their data operations and enhance analytical flexibility, making it easier to derive insights across different domains."}),"\n",(0,n.jsx)(a.h3,{id:"cost-effective-data-management",children:"Cost-Effective Data Management"}),"\n",(0,n.jsx)(a.p,{children:"Lakehouses leverage the low-cost storage of cloud-based data lakes while providing sophisticated data management and querying capabilities similar to data warehouses. This dual advantage makes it an economical choice for startups and enterprises alike that need to manage costs without compromising on analytics capabilities. Additionally, the open, unified architecture of a lakehouse eliminates non-monetary costs, such as running and maintaining ETL pipelines and creating multiple data copies, further streamlining operations."}),"\n",(0,n.jsx)(a.h2,{id:"real-world-examples",children:"Real World Examples"}),"\n",(0,n.jsxs)(a.p,{children:["ByteDance, Notion and Halodoc are some of the examples of how lakehouse architecture is being adopted in the industry. ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2021/09/01/building-eb-level-data-lake-using-hudi-at-bytedance/",children:"ByteDance"})," has built an exabyte-level data lakehouse using Apache Hudi to enhance their recommendation systems. The implementation of Hudi's Merge-on-read (MOR) tables, indexing, and Multi-Version Concurrency Control (MVCC) features allow ByteDance to provide real-time machine learning capabilities, providing instant and relevant recommendations."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.a,{href:"https://www.notion.so/blog/building-and-scaling-notions-data-lake",children:"Notion"})," scaled its data infrastructure by building an in-house lakehouse to handle rapid data growth and meet product demands, especially for Notion AI. The architecture uses S3 for storage, Kafka and Debezium for data ingestion, and Apache Hudi for efficient data management. This setup resulted in significant cost savings, faster data ingestion, and enhanced capabilities for analytics and product development."]}),"\n",(0,n.jsxs)(a.p,{children:["Similarly, ",(0,n.jsx)(a.a,{href:"https://blogs.halodoc.io/lake-house-architecture-halodoc-data-platform-2-0/",children:"Halodoc's"})," adoption of a lakehouse architecture allows them to enhance healthcare services by enabling real-time processing and analytics. This architecture helps Halodoc tackle challenges associated with managing vast healthcare data volumes, thus improving patient care through faster, more accurate decision-making and supporting both batch and stream processing crucial for timely health interventions."]}),"\n",(0,n.jsx)(a.h2,{id:"key-data-lakehouse-technologies",children:"Key Data Lakehouse Technologies"}),"\n",(0,n.jsx)(a.h3,{id:"open-source-solutions",children:"Open Source Solutions"}),"\n",(0,n.jsx)(a.h4,{id:"apache-hudi",children:"Apache Hudi"}),"\n",(0,n.jsxs)(a.p,{children:["Apache Hudi is an open source ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/hudi_stack",children:"transactional data lakehouse platform"})," built around a database kernel. It provides table-level abstractions over open file formats like Apache Parquet and ORC thereby delivering core warehouse and database functionalities directly in the data lake and supporting transactional capabilities such as updates and deletes."]}),"\n",(0,n.jsx)(a.p,{children:"Hudi also incorporates critical table services tightly integrated with its database kernel. These services can be run automatically, managing aspects like table bookkeeping, metadata, and storage layouts across both ingested and derived data. These capabilities, combined with specific platform services (ingestion, catalog sync tool, admin CLI, etc.) in Hudi, elevates its role from merely a table format to a comprehensive and robust data lakehouse platform. Apache Hudi has a broad support for various data sources and query engines, such as Apache Spark, Apache Flink, AWS Athena, Presto, Trino and StarRocks."}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{alt:"Hudi Stack",src:t(49938).A+"",width:"2076",height:"1400"}),"\n",(0,n.jsx)("p",{align:"center",children:"Figure: Apache Hudi Architectural stack"})]}),"\n",(0,n.jsx)(a.p,{children:"Below are some of the key features of Hudi\u2019s lakehouse platform."}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Mutability Support"}),": Hudi enables quick updates and deletions through an efficient, pluggable ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/indexing/",children:"indexing"})," mechanism supporting workloads such as streaming, out-of-order data, and data deduplication."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Incremental Processing"}),": Hudi optimizes for efficiency by enabling ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2020/08/18/hudi-incremental-processing-on-data-lakes/",children:"incremental processing"})," of new data. This feature allows you to replace traditional batch processing pipelines with more dynamic, incremental streaming, enhancing data ingestion and reducing processing times for analytical workloads."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"ACID Transactions"}),": Hudi brings ACID transactional guarantees to data lakes, offering consistent and atomic writes along with different ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/concurrency_control",children:"concurrency control"})," techniques essential for managing longer-running transactions."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Time Travel"}),": Hudi includes capabilities for ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/sql_queries#time-travel-query",children:"querying"})," historical data, allowing users to roll back to previous versions of tables to debug or audit changes."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Comprehensive Table Management"}),": Hudi brings automated table services that continuously orchestrate ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/clustering",children:"clustering"}),", ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/compaction",children:"compaction"}),", ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/hoodie_cleaner",children:"cleaning"}),", and indexing, ensuring high performance for analytical queries."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Query Performance Optimization"}),": Hudi introduces a novel ",(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/blog/introducing-multi-modal-index-for-the-lakehouse-in-apache-hudi",children:"multi-modal indexing"})," subsystem that speeds up write transactions and enhances query performance, especially in large or wide tables."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Schema Evolution and Enforcement"}),": With Hudi, you can ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/schema_evolution",children:"adapt the schema"})," of your tables as your data evolves, enhancing pipeline resilience by quickly identifying and preventing potential data integrity issues."]}),"\n"]}),"\n",(0,n.jsx)(a.h4,{id:"apache-iceberg",children:"Apache Iceberg"}),"\n",(0,n.jsx)(a.p,{children:"Apache Iceberg is a table format designed for managing large-scale analytical datasets in cloud data lakes, facilitating a lakehouse architecture. Technically, Iceberg serves as a table format specification, providing APIs and libraries that enable compute engines to interact with tables according to this specification. It introduces features essential for data lake workloads, including schema evolution, hidden partitioning, ACID-compliant transactions, and time travel capabilities. These features ensure robust data management, akin to that found in traditional data warehouses."}),"\n",(0,n.jsx)(a.h4,{id:"delta-lake",children:"Delta Lake"}),"\n",(0,n.jsx)(a.p,{children:"Delta Lake is another open source table format that enables building a lakehouse architecture on top of cloud data lakes. By offering an ACID-compliant layer that operates over cloud object stores, Delta Lake addresses the typical performance and consistency issues associated with data lakes. It enables features like schema enforcement and evolution, time travel, efficient metadata handling and DML operations, which are crucial for handling large-scale workloads on data lakes effectively."}),"\n",(0,n.jsx)(a.h3,{id:"vendor-lakehouse-platforms",children:"Vendor Lakehouse Platforms"}),"\n",(0,n.jsx)(a.h4,{id:"onehouse",children:"Onehouse"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/product",children:"Onehouse"})," offers a universal data platform that streamlines data ingestion and transformation into a lakehouse architecture. It eliminates lakehouse table format friction by working seamlessly with Apache Hudi, Apache Iceberg and Delta Lake tables (thanks to ",(0,n.jsx)(a.a,{href:"https://xtable.apache.org",children:"Apache XTable"}),"). The platform supports continuous data ingestion from diverse sources, including events streams such as Kafka, databases and cloud storage, enabling real-time data updates while ensuring data integrity through automated table optimizations and rigorous data quality measures. Onehouse provides a fully managed ingestion pipeline with serverless autoscaling and cost-efficient infrastructure. With its flexible querying capabilities across multiple engines and formats, Onehouse empowers organizations to efficiently manage and utilize their data."]}),"\n",(0,n.jsx)(a.h4,{id:"databricks",children:"Databricks"}),"\n",(0,n.jsxs)(a.p,{children:["The ",(0,n.jsx)(a.a,{href:"https://www.databricks.com/product/data-intelligence-platform",children:"Databricks"})," Lakehouse Platform unifies data engineering, machine learning, and analytics on a single platform. It combines the reliability, governance, and performance of data warehouses with the scalability, flexibility, and low cost of data lakes. By offering Delta Lake as its foundational storage layer and ",(0,n.jsx)(a.a,{href:"https://docs.delta.io/latest/delta-uniform.html",children:"UniFormat"})," for interoperability between Apache Iceberg and Apache Hudi, the platform supports ACID transactions, scalable metadata handling, and unifies batch and streaming data processing."]}),"\n",(0,n.jsx)(a.h4,{id:"snowflake",children:"Snowflake"}),"\n",(0,n.jsxs)(a.p,{children:["The ",(0,n.jsx)(a.a,{href:"https://www.snowflake.com/en/data-cloud/platform/",children:"Snowflake"})," Data Cloud provides a unified, fully managed platform for seamless data management and advanced analytics capabilities. It offers near-infinite scalability, robust security, and native support for diverse data types and SQL workloads. Snowflake currently supports Apache Iceberg as the open table format to facilitate a lakehouse architecture. This integration allows users to leverage Iceberg's rich table metadata and Parquet file storage within Snowflake's ecosystem, enabling seamless data handling, multi-table transactions, dynamic data masking, and row-level security, all while using customer-supplied cloud storage."]}),"\n",(0,n.jsx)(a.h2,{id:"the-future-of-data-lakehouses",children:"The Future of Data Lakehouses"}),"\n",(0,n.jsx)(a.p,{children:"The future of data lakehouses is shaped by their truly open data architecture, which meets the ongoing need for flexible, scalable, and cost-effective data management solutions. They offer a unified platform capable of efficiently handling both streaming and batch workloads, supporting a wide array of analytical workloads including BI and ML. With the rapid advancement of artificial intelligence, including generative AI, there is an increasing demand for robust platforms that provide the foundation for building and deploying powerful models. Lakehouse architecture rises to this challenge, offering a solid base for the evolving demands of modern data analytics."}),"\n",(0,n.jsx)(a.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,n.jsx)(a.p,{children:"The data lakehouse architecture utilizes an open data foundation to blend the best features of data lakes and warehouses, establishing a versatile platform that effectively handles a range of analytical workloads. This architecture marries cost-effective data management with robust performance, offering a cohesive system for both batch and streaming data processes. By enabling organizations to work on a single data store, this approach not only simplifies management but also equips businesses to swiftly integrate new technologies and adapt to evolving market demands. Additionally, by supporting diverse data types and analytical workloads, the lakehouse framework eliminates the need for a two-tier architecture, which helps save costs and enhances the efficiency of data teams."})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},58328:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(54422),n=t(74848),s=t(28453),r=t(9230);const o={title:"Hudi\u2019s Column Stats Index and Data Skipping feature help speed up queries by an orders of magnitude!",authors:[{name:"Alexey Kudinkin"}],category:"blog",image:"/assets/images/blog/2022-06-09-col-stats-and-data-skipping.png",tags:["design","indexing","data skipping","onehouse"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.onehouse.ai/blog/hudis-column-stats-index-and-data-skipping-feature-help-speed-up-queries-by-an-orders-of-magnitude",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},58378:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(64343),n=t(74848),s=t(28453),r=t(9230);const o={title:"UPSERT Performance Evaluation of Hudi 0.14 and Spark 3.4.1: Record Level Index vs. Global Bloom & Global Simple Indexes",excerpt:"Record Level Index Performance",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2023-10-29-UPSERT-Performance-Evaluation-of-Hudi-0-14-and-Spark-3-4-1-Record-Level-Index-Global-Bloom-Global-Simple-Indexes.png",tags:["linkedin","apache hudi","querying","indexing","performance"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.linkedin.com/pulse/upsert-performance-evaluation-hudi-014-spark-341-record-soumil-shah-oupre/?utm_source=share&utm_medium=member_ios&utm_campaign=share_via",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},58428:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/jdpost-image6-87ec3eaac37a810679ae6090c8953f9f.jpg"},58588:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/07/16/Query-apache-hudi-dataset-in-an-amazon-S3-data-lake-with-amazon-athena-Read-optimized-queries","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-07-16-Query-apache-hudi-dataset-in-an-amazon-S3-data-lake-with-amazon-athena-Read-optimized-queries.mdx","source":"@site/blog/2021-07-16-Query-apache-hudi-dataset-in-an-amazon-S3-data-lake-with-amazon-athena-Read-optimized-queries.mdx","title":"Part1: Query apache hudi dataset in an amazon S3 data lake with amazon athena : Read optimized queries","description":"Redirecting... please wait!!","date":"2021-07-16T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"read optimized query","permalink":"/blog/tags/read-optimized-query"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.2,"hasTruncateMarker":false,"authors":[{"name":"Dhiraj Thakur","socials":{},"key":null,"page":null},{"name":"Sameer Goel","socials":{},"key":null,"page":null},{"name":"Imtiaz Sayed","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Part1: Query apache hudi dataset in an amazon S3 data lake with amazon athena : Read optimized queries","authors":[{"name":"Dhiraj Thakur"},{"name":"Sameer Goel"},{"name":"Imtiaz Sayed"}],"category":"blog","image":"/assets/images/blog/2021-07-16-query-hudi-using-athena-ro-queries.png","tags":["how-to","read optimized query","amazon"]},"unlisted":false,"prevItem":{"title":"Amazon Athena expands Apache Hudi support","permalink":"/blog/2021/07/16/Amazon-Athena-expands-Apache-Hudi-support"},"nextItem":{"title":"Employing correct configurations for Hudi\'s cleaner table service","permalink":"/blog/2021/06/10/employing-right-configurations-for-hudi-cleaner"}}')},58646:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2020/10/19/hudi-meets-aws-emr-and-aws-dms","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-10-19-hudi-meets-aws-emr-and-aws-dms.md","source":"@site/blog/2020-10-19-hudi-meets-aws-emr-and-aws-dms.md","title":"Apply record level changes from relational databases to Amazon S3 data lake using Apache Hudi on Amazon EMR and AWS Database Migration Service","description":"This blog published by AWS shows how to build a CDC pipeline that captures data from an Amazon Relational Database Service (Amazon RDS) for MySQL database using AWS Database Migration Service (AWS DMS) and applies those changes to a dataset in Amazon S3 using Apache Hudi on Amazon EMR.","date":"2020-10-19T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":0.39,"hasTruncateMarker":false,"authors":[{"name":"aws","key":null,"page":null}],"frontMatter":{"title":"Apply record level changes from relational databases to Amazon S3 data lake using Apache Hudi on Amazon EMR and AWS Database Migration Service","excerpt":"AWS blog showing how to build a CDC pipeline that captures data from an Amazon RDS for MySQL database using AWS DMS and applies those changes to an Amazon S3 dataset using Apache Hudi on Amazon EMR.","author":"aws","category":"blog","image":"/assets/images/blog/2020-10-19-hudi-meets-aws-emr-and-aws-dms.jpeg","tags":["blog","apache hudi"]},"unlisted":false,"prevItem":{"title":"Data Lake Change Capture using Apache Hudi & Amazon AMS/EMR","permalink":"/blog/2020/10/21/Data-Lake-Change-Capture-using-Apache-Hudi-and-Amazon-AMS-EMR"},"nextItem":{"title":"Origins of Data Lake at Grofers","permalink":"/blog/2020/10/19/Origins-of-Data-Lake-at-Grofers"}}')},58683:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/08/23/async-clustering","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-08-23-async-clustering.md","source":"@site/blog/2021-08-23-async-clustering.md","title":"Asynchronous Clustering using Hudi","description":"In one of the previous blog posts, we introduced a new","date":"2021-08-23T00:00:00.000Z","tags":[{"inline":true,"label":"design","permalink":"/blog/tags/design"},{"inline":true,"label":"clustering","permalink":"/blog/tags/clustering"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":6.42,"hasTruncateMarker":true,"authors":[{"name":"codope","key":null,"page":null}],"frontMatter":{"title":"Asynchronous Clustering using Hudi","excerpt":"How to setup Hudi for asynchronous clustering","author":"codope","category":"blog","image":"/assets/images/blog/clustering/example_perf_improvement.png","tags":["design","clustering","apache hudi"]},"unlisted":false,"prevItem":{"title":"Building an ExaByte-level Data Lake Using Apache Hudi at ByteDance","permalink":"/blog/2021/09/01/building-eb-level-data-lake-using-hudi-at-bytedance"},"nextItem":{"title":"Reliable ingestion from AWS S3 using Hudi","permalink":"/blog/2021/08/23/s3-events-source"}}')},58705:(e,a,t)=>{var i={"./2016-08-04-The-Case-for-incremental-processing-on-Hadoop.mdx":39291,"./2016-12-30-strata-talk-2017.md":38501,"./2017-03-12-Hoodie-Uber-Engineerings-Incremental-Processing-Framework-on-Hadoop.mdx":44615,"./2019-01-18-asf-incubation.md":83578,"./2019-03-07-batch-vs-incremental.md":70484,"./2019-05-14-registering-dataset-to-hive.md":42638,"./2019-09-09-ingesting-database-changes.md":26515,"./2019-10-22-Hudi-On-Hops.mdx":56305,"./2019-11-15-New-Insert-Update-Delete-Data-on-S3-with-Amazon-EMR-and-Apache-Hudi.mdx":22606,"./2020-01-15-delete-support-in-hudi.md":30204,"./2020-01-20-change-capture-using-aws.md":55036,"./2020-03-22-exporting-hudi-datasets.md":17721,"./2020-04-27-apache-hudi-apache-zepplin.md":26825,"./2020-05-28-monitoring-hudi-metrics-with-datadog.md":30005,"./2020-06-04-The-Apache-Software-Foundation-Announces-Apache-Hudi-as-a-Top-Level-Project.mdx":27078,"./2020-06-09-Building-a-Large-scale-Transactional-Data-Lake-at-Uber-Using-Apache-Hudi.mdx":39106,"./2020-06-16-Apache-Hudi-grows-cloud-data-lake-maturity.mdx":68829,"./2020-08-04-PrestoDB-and-Apache-Hudi.mdx":4054,"./2020-08-18-hudi-incremental-processing-on-data-lakes.md":80921,"./2020-08-20-efficient-migration-of-large-parquet-tables.md":12022,"./2020-08-21-async-compaction-deployment-model.md":37728,"./2020-08-22-ingest-multiple-tables-using-hudi.md":17198,"./2020-10-06-cdc-solution-using-hudi-by-nclouds.md":49951,"./2020-10-15-apache-hudi-meets-apache-flink.md":57006,"./2020-10-19-Origins-of-Data-Lake-at-Grofers.mdx":75893,"./2020-10-19-hudi-meets-aws-emr-and-aws-dms.md":19481,"./2020-10-21-Architecting-Data-Lakes-for-the-Modern-Enterprise-at-Data-Summit-Connect-Fall-2020.mdx":193,"./2020-10-21-Data-Lake-Change-Capture-using-Apache-Hudi-and-Amazon-AMS-EMR.mdx":72110,"./2020-11-11-hudi-indexing-mechanisms.md":14023,"./2020-11-29-Can-Big-Data-Solutions-Be-Affordable.mdx":76364,"./2020-12-01-high-perf-data-lake-with-hudi-and-alluxio-t3go.md":54437,"./2021-01-27-hudi-clustering-intro.md":23860,"./2021-02-13-hudi-key-generators.md":38865,"./2021-02-24-Time-travel-operations-in-Hopsworks-Feature-Store.mdx":18773,"./2021-03-01-Data-Lakehouse-Building-the-Next-Generation-of-Data-Lakes-using-Apache-Hudi.mdx":14949,"./2021-03-01-hudi-file-sizing.md":78564,"./2021-03-04-Build-a-data-lake-using-amazon-kinesis-data-stream-for-amazon-dynamodb-and-apache-hudi.mdx":95737,"./2021-03-11-New-features-from-Apache-hudi-in-Amazon-EMR.mdx":36810,"./2021-04-12-Build-Slowly-Changing-Dimensions-Type-2-SCD2-with-Apache-Spark-and-Apache-Hudi-on-Amazon-EMR.mdx":36425,"./2021-05-12-Experts-primer-on-Apache-Hudi.mdx":28075,"./2021-06-04-Apache-Hudi-How-Uber-gets-data-a-ride-to-its-destination.mdx":58043,"./2021-06-10-employing-right-configurations-for-hudi-cleaner.md":26620,"./2021-07-16-Amazon-Athena-expands-Apache-Hudi-support.mdx":7550,"./2021-07-16-Query-apache-hudi-dataset-in-an-amazon-S3-data-lake-with-amazon-athena-Read-optimized-queries.mdx":29564,"./2021-07-21-streaming-data-lake-platform.md":17211,"./2021-07-26-Baixin-banksreal-time-data-lake-evolution-scheme-based-on-Apache-Hudi.mdx":48209,"./2021-08-03-MLOps-Wars-Versioned-Feature-Data-with-a-Lakehouse.mdx":12881,"./2021-08-11-Cost-Efficient-Open-Source-Big-Data-Platform-at-Uber.mdx":77089,"./2021-08-16-kafka-custom-deserializer.md":47513,"./2021-08-18-improving-marker-mechanism.md":69973,"./2021-08-18-virtual-keys.md":31007,"./2021-08-23-async-clustering.md":28866,"./2021-08-23-s3-events-source.md":25245,"./2021-09-01-building-eb-level-data-lake-using-hudi-at-bytedance.md":79862,"./2021-10-05-Data-Platform-2.0-Part-I.mdx":81755,"./2021-10-14-How-Amazon-Transportation-Service-enabled-near-real-time-event-analytics-at-petabyte-scale-using-AWS-Glue-with-Apache-Hudi.mdx":79682,"./2021-10-21-Practice-of-Apache-Hudi-in-building-real-time-data-lake-at-station-B.mdx":38968,"./2021-11-16-How-GE-Aviation-built-cloud-native-data-pipelines-at-enterprise-scale-using-the-AWS-platform.mdx":49834,"./2021-11-22-Apache-Hudi-Architecture-Tools-and-Best-Practices.mdx":2089,"./2021-12-16-lakehouse-concurrency-control-are-we-too-optimistic.mdx":81012,"./2021-12-20-New-features-from-Apache-Hudi-0.7.0-and-0.8.0-available-on-Amazon-EMR.mdx":92023,"./2021-12-29-hudi-zorder-and-hilbert-space-filling-curves.md":69240,"./2021-12-31-The-Art-of-Building-Open-Data-Lakes-with-Apache-Hudi-Kafka-Hive-and-Debezium.mdx":9933,"./2022-01-06-apache-hudi-2021-a-year-in-review.mdx":7128,"./2022-01-14-change-data-capture-with-debezium-and-apache-hudi.mdx":698,"./2022-01-18-Why-and-How-I-Integrated-Airbyte-and-Apache-Hudi.mdx":27751,"./2022-01-20-Hudi-powering-data-lake-efforts-at-Walmart-and-Disney-Hotstar.mdx":54512,"./2022-01-25-Cost-Efficiency-Scale-in-Big-Data-File-Format.mdx":33582,"./2022-02-02-Onehouse-Commitment-to-Openness.mdx":8751,"./2022-02-03-Onehouse-brings-a-fully-managed-lakehouse-to-Apache-Hudi.mdx":12147,"./2022-02-09-ACID-transformations-on-Distributed-file-system.mdx":79003,"./2022-02-12-Open-Source-Data-Lake-Table-Formats-Evaluating-Current-Interest-and-Rate-of-Adoption.mdx":88789,"./2022-02-17-Fresher-Data-Lake-on-AWS-S3.mdx":20990,"./2022-02-20-Understanding-its-core-concepts-from-hudi-persistence-files.mdx":42396,"./2022-03-01-Create-a-low-latency-source-to-data-lake-pipeline-using-Amazon-MSK-Connect-Apache-Flink-and-Apache-Hudi.mdx":18422,"./2022-03-09-Build-a-serverless-pipeline-to-analyze-streaming-data-using-AWS-Glue-Apache-Hudi-and-Amazon-S3.mdx":30597,"./2022-03-24-Zendesk-Insights-for-CTOs-Part-3-Growing-your-business-with-modern-data-capabilities.mdx":76050,"./2022-04-04-Key-Learnings-on-Using-Apache-HUDI-in-building-Lakehouse-Architecture-at-Halodoc.mdx":34159,"./2022-04-04-New-features-from-Apache-Hudi-0.9.0-on-Amazon-EMR.mdx":70606,"./2022-04-19-Corrections-in-data-lakehouse-table-format-comparisons.mdx":22415,"./2022-05-17-Introducing-Multi-Modal-Index-for-the-Lakehouse-in-Apache-Hudi.mdx":80105,"./2022-05-25-Record-by-record-deletable-data-lake-using-Apache-Hudi.mdx":21499,"./2022-06-04-Asynchronous-Indexing-Using-Hudi.mdx":93666,"./2022-06-09-Singificant-queries-speedup-from-Hudi-Column-Stats-Index-and-Data-Skipping-features.mdx":58328,"./2022-06-29-Apache-Hudi-vs-Delta-Lake-transparent-tpc-ds-lakehouse-performance-benchmarks.mdx":32759,"./2022-07-11-build-open-lakehouse-using-apache-hudi-and-dbt.md":46171,"./2022-08-09-How-NerdWallet-uses-AWS-and-Apache-Hudi-to-build-a-serverless-real-time-analytics-platform.mdx":74474,"./2022-08-12-Use-Flink-Hudi-to-Build-a-Streaming-Data-Lake-Platform.mdx":13680,"./2022-08-24-Implementation-of-SCD-2-with-Apache-Hudi-and-Spark.mdx":44952,"./2022-08-25-Data-Lake-Lakehouse-Guide-Powered-by-Data-Lake-Table-Formats-Delta-Lake-Iceberg-Hudi.mdx":46871,"./2022-09-20-Building-Streaming-Data-Lakes-with-Hudi-and-MinIO.mdx":3664,"./2022-09-28-Data-processing-with-Spark-time-traveling.mdx":80812,"./2022-10-06-Ingest-streaming-data-to-Apache-Hudi-using-AWS-Glue-and-DeltaStreamer.mdx":72605,"./2022-10-08-what-why-and-how-apache-hudis-bloom-index.mdx":80242,"./2022-10-17-Get-started-with-Apache-Hudi-using-AWS.mdx":9318,"./2022-11-10-How-Hudl-built-a-cost-optimized-AWS-Glue-pipeline-with-Apache-Hudi-datasets.mdx":99613,"./2022-11-22-Build-your-Apache-Hudi-data-lake-on-AWS-using-Amazon-EMR-Part-1.mdx":327,"./2022-12-01-Run-apache-hudi-at-scale-on-aws.mdx":25881,"./2022-12-19-Build-Your-First-Hudi-Lakehouse-with-AWS-Glue-and-AWS-S3.md":7718,"./2022-12-29-Apache-Hudi-2022-A-Year-In-Review.mdx":87248,"./2023-01-11-Apache-Hudi-vs-Delta-Lake-vs-Apache-Iceberg-Lakehouse-Feature-Comparison.mdx":17087,"./2023-01-27-Introducing-native-support-for-Apache-Hudi-Delta-Lake-Apache-Iceberg-on-AWS-Glue-for-Apache-Spark.mdx":59425,"./2023-02-07-automate-schema-evolution-at-scale-with-apache-hudi-in-aws-glue.mdx":91987,"./2023-02-12-table-service-deployment-models-in-apache-hudi.mdx":17586,"./2023-02-19-bulk-insert-sort-modes-with-apache-hudi.mdx":24375,"./2023-02-22-Getting-Started-Manage-your-Hudi-tables-with-the-admin-Hudi-CLI-tool.mdx":23483,"./2023-03-16-Setting-Uber-Transactional-Data-Lake-in-Motion-with-Incremental-ETL-Using-Apache-Hudi.mdx":87230,"./2023-03-17-introduction-to-apache-hudi.mdx":22153,"./2023-03-20-Introducing-native-support-for-Apache Hudi-Delta-Lake-and-Apache-Iceberg-on-AWS-Glue-for-Apache-Spark-Part-2-AWS-Glue-Studio-Visual-Editor.mdx":77936,"./2023-03-23-Spark-ETL-Chapter-8-with-Lakehouse-Apache-HUDI.mdx":44691,"./2023-04-02-global-vs-non-global-index-in-apache-hudi.mdx":74244,"./2023-04-07-Speed-up-your-write-latencies-using-Bucket-Index-in-Apache-Hudi.mdx":30938,"./2023-04-18-getting-started-incrementally-process-data-with-apache-hudi.mdx":17149,"./2023-04-26-the-lakehouse-trifecta.mdx":77036,"./2023-04-29-can-you-concurrently-write-data-to-apache-hudi-w-o-any-lock-provider.mdx":67198,"./2023-05-02-intro-to-hudi-and-flink.mdx":82939,"./2023-05-03-lakehouse-at-fortune-1-scale.mdx":15624,"./2023-05-09-amazon-athena-apache-hudi.mdx":89564,"./2023-05-10-top-3-things-you-can-do-to-get-fast-upsert-performance-in-apache-hudi.mdx":62433,"./2023-05-12-ingesting-data-to-apache-hudi-using-spark-sql.mdx":56533,"./2023-05-16-how-zoom-implemented-streaming-log-ingestion-and-efficient-gdpr-deletes-using-apache-hudi-on-amazon-emr.mdx":64878,"./2023-05-19-hudi-metafields-demystified.mdx":46294,"./2023-05-29-different-query-types-with-apache-hudi.mdx":14661,"./2023-06-03-text-based-search-from-elastic-search-to-vector-search.mdx":40748,"./2023-06-11-cleaner-and-archival-in-apache-hudi.mdx":36093,"./2023-06-16-Exploring-New-Frontiers-How-Apache-Flink-Apache-Hudi-and-Presto-Power-New-Insights-at-Scale.mdx":40686,"./2023-06-20-How-to-query-data-in-Apache-Hudi-using-StarRocks.mdx":54262,"./2023-06-20-timeline-server-in-apache-hudi.mdx":17165,"./2023-06-24-multi-writer-support-in-apache-hudi.mdx":81055,"./2023-06-26-Unlimited-Big-Data-Exchange-A-Wonderful-Review-of-Apache-DolphinScheduler-and-Hudi-Hangzhou-Meetup.mdx":54425,"./2023-06-30-What-about-Apache-Hudi-Apache-Iceberg-and-Delta-Lake.mdx":72089,"./2023-07-01-monitoring-table-size-stats.mdx":46265,"./2023-07-02-Hudi-Best-Practices-Handling-Failed-Inserts-Upserts-with-Error-Tables.mdx":12915,"./2023-07-07-Skip-rocks-and-files-Turbocharge-Trino-queries-with-Hudi-multi-modal-indexing-subsystem.mdx":75064,"./2023-07-08-Quickly-start-using-Apache-Hudi-on-AWS-EMR.mdx":63462,"./2023-07-09-Hoodie-Timeline-Foundational-pillar-for-ACID-transactions.mdx":74518,"./2023-07-20-Backfilling-Apache-Hudi-Tables-in-Production-Techniques-and-Approaches-Using-AWS-Glue-by-Job-Target-LLC.mdx":70163,"./2023-07-21-AWS-Glue-Crawlers-now-supports-Apache-Hudi-Tables.mdx":46017,"./2023-07-27-Apache-Hudi-Revolutionizing-Big-Data-Management-for-Real-Time-Analytics.mdx":52400,"./2023-08-03-Apache-Hudi-on-AWS-Glue-A-Step-by-Step-Guide.mdx":45731,"./2023-08-03-Create-an-Apache-Hudi-based-near-real-time-transactional-data lake-using-AWS-DMS-Amazon-Kinesis-AWS-Glue-streaming-ETL-and-data-visualization-using-Amazon-QuickSight.mdx":1365,"./2023-08-03-Data-lake-Table-formats-Apache-Iceberg-vs-Apache-Hudi-vs-Delta-lake.mdx":73719,"./2023-08-05-Data-Lakehouse-Architecture-for-Big-Data-with-Apache-Hudi.mdx":11466,"./2023-08-09-Lakehouse-Trifecta-Delta-Lake-Apache-Iceberg-and-Apache-Hudi.mdx":43019,"./2023-08-22-Exploring-various-storage-types-in-Apache-Hudi.mdx":75312,"./2023-08-25-Delta-Hudi-Iceberg-Which-is-most-popular.mdx":56417,"./2023-08-28-Apache-Hudi-From-Zero-To-One.mdx":85950,"./2023-08-28-Delta-Hudi-Iceberg-A-Benchmark-Compilation.mdx":78027,"./2023-08-31-Incremental-Queries-with-Apache-Hudi-and-Apache-Flink.mdx":12888,"./2023-09-06-Apache-Hudi-From-Zero-To-One-blog-2.mdx":50895,"./2023-09-06-Lakehouse-or-Warehouse-Part-1-of-2.mdx":96749,"./2023-09-10-Demystifying-Copy-on-Write-in-Apache-Hudi-Understanding-Read-and-Write-Operations.mdx":87431,"./2023-09-12-Lakehouse-or-Warehouse-Part-2-of-2.mdx":78821,"./2023-09-13-Simplify-operational-data-processing-in-data-lakes-using-AWS-Glue-and-Apache-Hudi.mdx":4671,"./2023-09-15-Apache-Hudi-From-Zero-To-One-blog-3.mdx":93598,"./2023-09-19-A-Beginners-Guide-to-Apache-Hudi-with-PySpark-Part-1-of-2.mdx":43160,"./2023-09-22-Exploring-the-Architecture-of-Apache-Iceberg-Delta-Lake-and-Apache-Hudi.mdx":36179,"./2023-09-27-Apache-Hudi-From-Zero-To-One-blog-4.mdx":82294,"./2023-10-06-Apache-Hudi-Copy-on-Write-CoW-Table.mdx":27189,"./2023-10-11-starrocks-query-performance-with-apache-hudi-and-onehouse.mdx":10891,"./2023-10-17-Get-started-with-Apache-Hudi-using-AWS-Glue-by-implementing-key-design-concepts-Part-1.mdx":41970,"./2023-10-18-Apache-Hudi-From-Zero-To-One-blog-5.mdx":59687,"./2023-10-19-load-data-incrementally-from-transactional-data-lakes-to-data-warehouses.mdx":94555,"./2023-10-20-Its-Time-for-the-Universal-Data-Lakehouse.mdx":38156,"./2023-10-22-Tipico-Facilitates-Faster-Data-Access-with-a-Modern-Data-Strategy-on-AWS.mdx":36927,"./2023-10-29-UPSERT-Performance-Evaluation-of-Hudi-0-14-and-Spark-3-4-1-Record-Level-Index-Global-Bloom-Global-Simple-Indexes.mdx":58378,"./2023-11-01-record-level-index.md":62420,"./2023-11-13-Apache-Hudi-From-Zero-To-One-blog-6.mdx":87388,"./2023-11-19-Hudi-Streamer-DeltaStreamer-Hands-On-Guide-Local-Ingestion-from-Parquet-Source.mdx":91409,"./2023-11-22-Introducing-Apache-Hudi-support-with-AWS-Glue-crawlers.mdx":28955,"./2023-11-26-Real-Time-Data-Processing-with-Postgres-Debezium-Kafka-Schema-Registry-and-DeltaStreamer-Guide-for-Begineers.mdx":26161,"./2023-11-28-Apache-Hudi-Part-1-History-Getting-Started.mdx":72385,"./2023-11-30-Mastering-Data-Lakes-A-Deep-Dive-into-MINIO-Hudi-and-Delta-Streamer.mdx":44501,"./2023-12-01-Getting-started-with-Apache-Hudi.mdx":36411,"./2023-12-06-Apache-Hudi-From-Zero-To-One-blog-7.mdx":77586,"./2023-12-09-Getting-started-with-Apache-Hudi.mdx":43267,"./2023-12-13-what-is-apache-hudi.mdx":81310,"./2023-12-28-apache-hudi-2023-a-year-in-review.mdx":67899,"./2024-01-01-From-Data-lake-to-Microservices-Unleashing-the-Power-of-Apache-Hudi-Record-Level-Index-with-FastAPI-and-Spark-Connect.mdx":67853,"./2024-01-02-Build-a-federated-query-solution-with-Apache-Doris-Apache-Flink-and-Apache-Hudi.mdx":14520,"./2024-01-05-Apache-Hudi-From-Zero-To-One-blog-8.mdx":21873,"./2024-01-05-Small-Talk-about-Apache-Hudi.mdx":16187,"./2024-01-09-introduction-to-apache-hudi.mdx":33221,"./2024-01-11-In-House-Data-Lake-with-CDC-Processing-Hudi-Docker.mdx":64175,"./2024-01-17-Enforce-fine-grained-access-control-on-Open-Table-Formats-via-Amazon-EMR-integrated-with-AWS-Lake-Formation.mdx":37950,"./2024-01-18-Deleting-Items-from-Apache-Hudi-using-Delta-Streamer-in-UPSERT-Mode-with-Kafka-Avro-Messages.mdx":30205,"./2024-01-20-Data-Engineering-Bootstrapping-Data-lake-with-Apache-Hudi.mdx":71457,"./2024-01-20-Learn-How-to-Move-Data-From-MongoDB-to-Apache-Hudi-Using-PySpark.mdx":55204,"./2024-01-24-Use-Amazon-Athena-with-Spark-SQL-for-your-open-source-transactional-table-formats.mdx":41515,"./2024-01-30-Leverage-Partition-Paths-of-your-data-lake-tables-to-Optimize-Data-Retrieval-Costs-on-the-cloud.mdx":79200,"./2024-02-04-Apache-Hudi-Managing-Partition-on-a-petabyte-scale-table.mdx":80931,"./2024-02-06-Building-an-Open-Source-Data-Lake-House-with-Hudi-Postgres-Hive-Metastore-Minio-and-StarRocks.mdx":46486,"./2024-02-06-Combine-Transactional-Integrity-and-Data-Lake-Operations-with-YugabyteDB-and-Apache-Hudi.mdx":68936,"./2024-02-12-How-a-POC-became-a-production-ready-Hudi-data-lakehouse-through-close-team-collaboration.mdx":39110,"./2024-02-23-Enabling-near-real-time-data-analytics-on-the-data-lake.mdx":15205,"./2024-02-27-Building-Data-Lakes-on-AWS-with-Kafka-Connect-Debezium-Apicurio-Registry-and-Apache-Hudi.mdx":79872,"./2024-02-27-empowering-data-driven-excellence-how-the-bluestone-data-platform-embraced-data-mesh-for-success.mdx":5375,"./2024-03-05-Apache-Hudi-From-Zero-To-One-blog-9.mdx":14522,"./2024-03-10-navigating-the-future-the-evolutionary-journey-of-upstoxs-data-platform.mdx":56500,"./2024-03-14-Modern-Datalakes-with-Hudi--MinIO--and-HMS.mdx":21967,"./2024-03-16-Open-Table-Formats-part-1-Apache-Hudi-Hadoop-Upserts-Deletes-and-Incrementals.mdx":35681,"./2024-03-22-data-lake-cost-optimisation-strategies.mdx":62265,"./2024-03-23-options-on-kafka-sink-to-open-table-formats-apache-iceberg-and-apache-hudi.mdx":64102,"./2024-03-30-record-level-indexing-apache-hudi-delivers-70-faster-point.mdx":76952,"./2024-04-03-hands-on-guide-reading-data-from-hudi-tables-joining-delta.mdx":11313,"./2024-04-13-Apache-Hudi-From-Zero-To-One-blog-10.mdx":4564,"./2024-04-21-build-real-time-streaming-pipeline-with-kinesis-apache-flink-and-apache-hudi.mdx":2847,"./2024-04-24-understanding-apache-hudi-consistency-model-part-1.mdx":99996,"./2024-04-24-understanding-apache-hudi-consistency-model-part-2.mdx":31427,"./2024-04-24-understanding-apache-hudi-consistency-model-part-3.mdx":59366,"./2024-04-25-apache-hudi-vs-apache-iceberg-a-comprehensive-comparison.mdx":91861,"./2024-05-02-how-query-apache-hudi-tables-python-using-daft-spark-free.mdx":47058,"./2024-05-07-learn-how-read-hudi-data-aws-glue-ray-using-daft-spark.mdx":50051,"./2024-05-10-building-analytical-apps-on-the-lakehouse-using-apache-hudi-daft-streamlit.mdx":63282,"./2024-05-19-apache-hudi-on-aws-glue.mdx":12231,"./2024-05-22-use-aws-data-exchange-to-seamlessly-share-apache-hudi-datasets.mdx":99697,"./2024-05-27-apache-hudi-vs-delta-lake-choosing-the-right-tool-for-your-data-lake-on-aws.mdx":16837,"./2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.mdx":41905,"./2024-06-18-how-to-use-apache-hudi-with-databricks.mdx":23816,"./2024-07-11-what-is-a-data-lakehouse.md":58262,"./2024-07-30-data-lake-cdc.md":10234,"./2024-07-31-hudi-file-formats.md":45672,"./2024-09-04-developer-guide-how-to-submit-hudi-pyspark-python-jobs-to-emr-serverless.mdx":71167,"./2024-09-09-use-apache-hudi-tables-in-athena-for-spark.mdx":74912,"./2024-09-11-comparing-apache-hudi-apache-iceberg-and-delta-lake.mdx":75046,"./2024-09-14-Ubers-Big-Data-Revolution-From-MySQL-to-Hadoop-and-Beyond.mdx":80400,"./2024-09-17-how-apache-hudi-transformed-yuno-s-data-lake.mdx":80867,"./2024-09-22-hands-on-with-apache-hudi-and-spark.mdx":51931,"./2024-09-24-hudi-iceberg-and-delta-lake-data-lake-table-formats-compared.mdx":5564,"./2024-09-30-change-query-support-in-apache-hudi-0-15.mdx":19484,"./2024-10-02-apache-hudi-spark-and-minio-hands-on-lab-in-docker.mdx":78026,"./2024-10-07-iceberg-vs-delta-lake-vs-hudi-a-comparative-look-at-lakehouse-architectures.mdx":98557,"./2024-10-07-mastering-slowly-changing-dimensions-with-apache-hudi-and-spark-sql.mdx":16580,"./2024-10-14-streaming-dynamodb-data-into-a-hudi-table-aws-glue-in-action.mdx":52371,"./2024-10-22-exploring-time-travel-queries-in-apache-hudi.mdx":5478,"./2024-10-23-Using-Apache-Hudi-with-Apache-Flink.mdx":35388,"./2024-10-23-mastering-open-table-formats-a-guide-to-apache-iceberg-hudi-and-delta-lake.mdx":33750,"./2024-10-26-moving-large-tables-from-snowflake-to-s3-using-the-copy-into-command-and-hudi.mdx":50506,"./2024-10-27-I-spent-5-hours-exploring-the-story-behind-Apache-Hudi.mdx":43537,"./2024-11-12-record-level-indexing-in-apache-hudi.mdx":48713,"./2024-11-12-storing-200-billion-entities-notions.mdx":20952,"./2024-11-12-understanding-cow-and-mor-in-apache-hudi.mdx":547,"./2024-11-19-automated-small-file-handling.md":98698,"./2024-12-03-apache-iceberg-vs-apache-hudi.mdx":28791,"./2024-12-04-use-open-table-format-libraries-on-aws-glue-5-0-for-apache-spark.mdx":41733,"./2024-12-06-non-blocking-concurrency-control.md":71749,"./2024-12-16-announcing-hudi-1-0-0.mdx":7320,"./2024-12-28-how-lakehouse-handles-concurrent-read-and-writes.mdx":94870,"./2024-12-29-apache-hudi-2024-a-year-in-review.mdx":7480,"./2024-12-31-indexing-in-apache-hudi.mdx":83270,"./2024-12-31-the-architects-guide-to-open-table-formats-and-object-storage.mdx":14792,"./2025-01-05-how-use-new-hudi-streamer-100-emr-serverless-750-hands-on.mdx":99220,"./2025-01-08-the-future-of-data-lakehouses-a-fireside.mdx":53632,"./2025-01-09-apache-iceberg-vs-delta-lake-vs-apache-hudi.mdx":79657,"./2025-01-15-outofbox-key-generators-in-hudi.mdx":60387,"./2025-01-18-apache-hudi-1-0-now-generally-available.mdx":73605,"./2025-01-28-concurrency-control.md":40584,"./2025-01-30-an-intro-to-hudi-with-minio.mdx":8193,"./2025-02-23-curious-engineering-facts-lakehouse-apache-hudi-daft-positional-argument.mdx":13062,"./2025-02-24-building-a-lakehouse-architecture-on-aws-with-terraform.mdx":94134,"./2025-02-25-curious-engineering-facts-trace-agents-hudi-daft-1.mdx":42679,"./2025-03-03-record-mergers-in-hudi.mdx":87504,"./2025-03-05-hudi-21-unique-differentiators.mdx":57356,"./2025-03-13-hudi-on-dbr.mdx":23652,"./2025-03-13-lightning-fast-analytics.mdx":8367,"./2025-03-26-acid-transactions.mdx":43898,"./2025-03-26-clustering.mdx":47999,"./2025-03-26-dedupe.mdx":85668,"./2025-03-26-uptycs.mdx":7913,"./2025-03-31-amazon-hudi.md":74548,"./2025-04-02-secondary-index.md":60181,"./2025-04-03-integrate-apache-doris-hudi-data-querying-migration.mdx":36095,"./2025-04-06-from-swamp-to-stream-how-apache-hudi-transforms-the-modern-data-lake.mdx":2350,"./2025-04-09-why-walmart-chose-apache-hudi-for-their-lakehouse.mdx":95446,"./2025-04-14-doris-hudi-making-impossible-possible.mdx":38175,"./2025-05-29-lsm-timeline.md":27098,"./2025-06-13-Optimizing-Apache-Hudi-Workflows-Automation-for-Clustering-Resizing-Concurrency.mdx":70864,"./2025-06-16-Apache-Hudi-does-XYZ-110.mdx":85591,"./2025-06-30-uber-hudi.md":99554,"./2025-07-02-Lakehouse-Architecture-apache-hudi-and-apache-iceberg.mdx":87459,"./2025-07-03-why-uber-built-hudi-the-strategic-decision-behind-a-custom-table-format.mdx":83823,"./2025-07-07-how-stifel-built-a-modern-data-platform-using-aws-glue-and-an-event-driven-domain-architecture.mdx":2722,"./2025-07-10-building-a-rag-based-ai-recommender.mdx":14234,"./2025-07-15-PayU-built-a-secure-enterprise-AI-assistant.mdx":10251,"./2025-07-15-modernizing-datainfra-peloton-hudi.md":81825,"./2025-07-21-mor-comparison.md":3952,"./2025-08-29-building-a-rag-based-ai-recommender-2.mdx":90874,"./2025-09-17-hudi-auto-gen-keys.mdx":41893,"./2025-10-02-Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph.mdx":64239,"./2025-10-16-Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless.md":19439,"./2025-10-22-Partition_Stats_Enhancing_Column_Stats_in_Hudi_1.0.md":56973,"./2025-10-29-deep-dive-into-hudis-indexing-subsystem-part-1-of-2.md":20086,"./2025-11-07-how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse.mdx":25678,"./2025-11-12-deep-dive-into-hudis-indexing-subsystem-part-2-of-2.md":72402,"./2025-11-25-apache-hudi-release-1-1-announcement.md":48840,"./2025-12-01-apache-hudi-JD-meetup-asia-2025-recap.md":47340};function n(e){var a=s(e);return t(a)}function s(e){if(!t.o(i,e)){var a=new Error("Cannot find module '"+e+"'");throw a.code="MODULE_NOT_FOUND",a}return i[e]}n.keys=function(){return Object.keys(i)},n.resolve=s,e.exports=n,n.id=58705},59025:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/07/30/data-lake-cdc","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-07-30-data-lake-cdc.md","source":"@site/blog/2024-07-30-data-lake-cdc.md","title":"Understanding Data Lake Change Data Capture","description":"Introduction","date":"2024-07-30T00:00:00.000Z","tags":[{"inline":true,"label":"Data Lake","permalink":"/blog/tags/data-lake"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"Change Data Capture","permalink":"/blog/tags/change-data-capture"},{"inline":true,"label":"CDC","permalink":"/blog/tags/cdc"}],"readingTime":8.7,"hasTruncateMarker":false,"authors":[{"name":"Sagar Lakshmipathy","key":null,"page":null}],"frontMatter":{"title":"Understanding Data Lake Change Data Capture","excerpt":"Explains the concept of CDC in data lakes","author":"Sagar Lakshmipathy","category":"blog","image":"/assets/images/blog/data-lake-cdc/hudi-cdc.jpg","tags":["Data Lake","Apache Hudi","Change Data Capture","CDC"]},"unlisted":false,"prevItem":{"title":"Column File Formats: How Hudi Leverages Parquet and ORC ","permalink":"/blog/2024/07/31/hudi-file-formats"},"nextItem":{"title":"What is a Data Lakehouse & How does it Work?","permalink":"/blog/2024/07/11/what-is-a-data-lakehouse"}}')},59331:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/Dimension20tables-6bbe96fbe9102541487b1819532f6bd4.gif"},59366:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(49462),n=t(74848),s=t(28453),r=t(9230);const o={title:"Understanding Apache Hudi's Consistency Model Part 3",author:"Jack Vanlightly",category:"blog",image:"/assets/images/blog/2024-04-24-understanding-apache-hudi-consistency-model-part-3.png",tags:["blog","apache hudi","tla+ specification","consistency","concurrency control","multi writer","monotonic timestamp","jack-vanlightly"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://jack-vanlightly.com/analyses/2024/4/25/understanding-apache-hudi-consistency-model-part-3",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},59425:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(88671),n=t(74848),s=t(28453),r=t(9230);const o={title:"Introducing native support for Apache Hudi, Delta Lake, and Apache Iceberg on AWS Glue for Apache Spark, Part 1: Getting Started",authors:[{name:"Akira Ajisaka, Noritaka Sekiyama and Savio Dsouza"}],category:"blog",image:"/assets/images/blog/0127-introducing-native-support-hudi-aws-glue.png",tags:["blog","amazon"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/part-1-getting-started-introducing-native-support-for-apache-hudi-delta-lake-and-apache-iceberg-on-aws-glue-for-apache-spark/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},59499:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/03/26/acid-transactions","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-03-26-acid-transactions.mdx","source":"@site/blog/2025-03-26-acid-transactions.mdx","title":"ACID Transactions in an Open Data Lakehouse","description":"Redirecting... please wait!!","date":"2025-03-26T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"Apache Iceberg","permalink":"/blog/tags/apache-iceberg"},{"inline":true,"label":"Delta Lake","permalink":"/blog/tags/delta-lake"},{"inline":true,"label":"ACID","permalink":"/blog/tags/acid"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Dipankar Mazumdar","key":null,"page":null}],"frontMatter":{"title":"ACID Transactions in an Open Data Lakehouse","author":"Dipankar Mazumdar","category":"blog","image":"/assets/images/blog/acid.png","tags":["blog","Apache Hudi","Apache Iceberg","Delta Lake","ACID"]},"unlisted":false,"prevItem":{"title":"Powering Amazon Unit Economics at Scale Using Apache Hudi","permalink":"/blog/2025/03/31/amazon-hudi"},"nextItem":{"title":"What is Clustering in an Open Data Lakehouse?","permalink":"/blog/2025/03/26/clustering"}}')},59687:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(97247),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi: From Zero To One (5/10)",excerpt:"Introduce table services: compaction, cleaning, and indexing",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2023-10-18-Apache-Hudi-From-Zero-To-One-blog-5.png",tags:["hudi","spark","blog","course","tutorial","datumagic","data lake","lakehouse","apache hudi","apache spark"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blog.datumagic.ai/p/apache-hudi-from-zero-to-one-510",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},59731:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/3-binary-copy-d58175c8f9cbd5817d1ace637a617a18.png"},60181:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(35845),n=t(74848),s=t(28453);const r={title:"Introducing Secondary Index in Apache Hudi Lakehouse Platform",excerpt:"What's & How's of Secondary indexes in Hudi 1.0",author:"Dipankar Mazumdar, Aditya Goenka",category:"blog",image:"/assets/images/blog/sec-thumb.jpg",tags:["Apache Hudi","Indexing","Performance"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Apache Hudi&#39;s Multi-Modal Indexing System",id:"apache-hudis-multi-modal-indexing-system",level:2},{value:"Introducing Secondary Index",id:"introducing-secondary-index",level:2},{value:"Creating a Secondary Index in Hudi",id:"creating-a-secondary-index-in-hudi",level:3},{value:"Asynchronous Indexing in Hudi",id:"asynchronous-indexing-in-hudi",level:3},{value:"Benchmarking",id:"benchmarking",level:2},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const a={a:"a",admonition:"admonition",br:"br",code:"code",em:"em",h2:"h2",h3:"h3",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.admonition,{title:"TL;DR",type:"tip",children:(0,n.jsx)(a.p,{children:"Apache Hudi 1.0 introduces Secondary Indexes, enabling faster queries on non-primary key fields. This improves data retrieval in Lakehouse architectures by reducing data scans. Hudi also offers asynchronous indexing for scalability and efficient index maintenance without disrupting data ingestion. By the end of this blog, you'll understand how these features enhance Hudi's capabilities as a high-performance lakehouse platform."})}),"\n",(0,n.jsxs)(a.p,{children:["Indexes are a fundamental data structure that enables efficient data retrieval by eliminating the need to scan the entire dataset for every query. In the context of a Lakehouse, where records are written as immutable data files (such as Parquet) at scale, indexing becomes crucial in reducing lookup times. Otherwise, a lot of time will be spent by the compute engine on finding out where exactly a particular record exists amongst thousands of files in the data lake storage, which is computationally expensive at scale. Indexing is not only important for ",(0,n.jsx)(a.em,{children:"reads"})," in a lakehouse architecture, but also for ",(0,n.jsx)(a.em,{children:"writes"}),", such as upserts and deletes, as you need to know where the record is to update it."]}),"\n",(0,n.jsxs)(a.p,{children:["One of the standout design choices in Apache Hudi that separates it from other lakehouse formats is its ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/next/indexes/",children:"indexing"})," capability, which has been central to its architecture from the beginning. Hudi is heavily optimized to handle mutable change streams with varying write patterns, and indexing plays a pivotal role in making upserts and deletes efficient."]}),"\n",(0,n.jsx)(a.p,{children:"Hudi's indexing mechanism is designed to efficiently manage record lookups and updates by maintaining a structured mapping between records and file groups. Here's how it works:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:["The first time a record is ingested into Hudi, it is assigned to a ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/tech-specs/#file-layout-hierarchy",children:"File Group"})," - a logical grouping of files. This assignment typically remains unchanged throughout the record's lifecycle. However, in cases such as clustering or cross-partition updates, the record may be remapped to a different file group. Even in such scenarios, Hudi ensures that a given record key is associated with exactly one file group at any completed instant on the timeline"]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:["Hudi maintains a mapping between the incoming ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/key_generation",children:"record\u2019s key"})," (unique identifier) and the File Group where it resides."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"The index is responsible for quickly locating records based on this File Group mapping, eliminating the need for full dataset scans."}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"This strategy allows Hudi to determine whether a record exists and pinpoint its exact location, enabling faster upserts and deletes."}),"\n",(0,n.jsx)(a.h2,{id:"apache-hudis-multi-modal-indexing-system",children:"Apache Hudi's Multi-Modal Indexing System"}),"\n",(0,n.jsxs)(a.p,{children:["While Hudi\u2019s indexes have set a benchmark for fast writes, bringing those advantages to queries was equally important. This led to the design of a generalized indexing subsystem that enhances performance in the lakehouse. Hudi\u2019s ",(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/blog/introducing-multi-modal-index-for-the-lakehouse-in-apache-hudi",children:"multi-modal indexing"})," redefines indexing in data lakes by employing multiple index types, each optimized for different workloads and query patterns. It is built on scalable metadata that supports multiple index types without extra overhead, ACID-compliant updates to keep indexes in sync with the data table, and optimized lookups that minimize full scans for low-latency queries on large datasets."]}),"\n",(0,n.jsxs)(a.p,{children:["At the core of Hudi\u2019s indexing design is its ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/metadata",children:"metadata table"}),", a specialized Merge-on-Read table that houses multiple index types as separate partitions. These indexes serve various purposes, improving the efficiency of reads, writes, and upserts."]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/hudi-stack-indexes.png",alt:"index",width:"800",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"Some key indexes within Hudi\u2019s metadata table include:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"File Index - Stores a compact listing of files, reducing the overhead of expensive file system operations."}),"\n",(0,n.jsx)(a.li,{children:"Column Stats Index - Tracks min/max statistics for each column, enabling more effective data pruning."}),"\n",(0,n.jsx)(a.li,{children:"Bloom Filter Index - Stores precomputed bloom filters for all data files, optimizing record lookups."}),"\n",(0,n.jsx)(a.li,{children:"Partition Stats Index - Stores aggregated partition-related information which helps in efficient partition pruning by skipping entire folders very quickly."}),"\n",(0,n.jsx)(a.li,{children:"Record-Level Index - Maintains direct mappings to individual records, facilitating faster upserts and deletes."}),"\n",(0,n.jsx)(a.li,{children:"Secondary Index - Allow users to create indexes on columns that are not part of record key columns in Hudi tables."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"By structuring these indexes as individual partitions within the metadata table, Hudi ensures efficient retrieval, quick lookups, and scalability, even as the data volume grows. In this blog, we will focus on secondary indexes and understand how it can help accelerate query performance in a lakehouse."}),"\n",(0,n.jsx)(a.h2,{id:"introducing-secondary-index",children:"Introducing Secondary Index"}),"\n",(0,n.jsx)(a.p,{children:"A secondary index is an indexing mechanism commonly used in database systems to provide efficient access to records based on non-primary key attributes. Unlike primary indexes, which enforce uniqueness and define the main data layout, secondary indexes serve as auxiliary data structures that accelerate lookups on fields that are frequently queried but are not the primary key."}),"\n",(0,n.jsxs)(a.p,{children:["For example, in an OLTP (Online Transaction Processing) database, a primary index might be defined on a unique ",(0,n.jsx)(a.code,{children:"order_id"}),", whereas a secondary index could be created on ",(0,n.jsx)(a.code,{children:"customer_id"})," to quickly fetch all orders placed by a specific customer. Secondary indexes enhance query performance by reducing the need for full table scans, especially in analytical workloads that involve complex filtering or joins."]}),"\n",(0,n.jsxs)(a.p,{children:["With ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0/",children:"Hudi 1.0"}),", Apache Hudi introduces ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/next/indexes#secondary-index",children:"secondary indexes"}),", bringing database-style indexing capabilities to the Lakehouse. Secondary indexes allow queries to scan significantly fewer files, reducing query latency and compute costs. This is especially beneficial for cloud-based query engines (such as AWS Athena), where pricing is based on the amount of data scanned. A secondary index in Hudi allows users to index any column beyond the record key (primary key), making queries on non-primary key fields much faster. This extends Hudi\u2019s existing ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2023/11/01/record-level-index/",children:"record-level index"}),", which optimizes writes and reads based on the record key."]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/secondary_index.png",alt:"sec_index",width:"800",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"Here is how the secondary index works in Hudi."}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Indexes Non-Primary Key Columns: Unlike the record-level index, which tracks record keys, secondary indexes help accelerate queries on fields outside the primary key."}),"\n",(0,n.jsx)(a.li,{children:"Stores Mappings Between Secondary and Primary Keys: Hudi maintains a mapping between secondary keys (e.g., city, driver) and record keys, enabling fast lookups for non-primary key queries."}),"\n",(0,n.jsx)(a.li,{children:"Minimizes Data Scans via Index-Aware Query Execution: During query execution, the secondary index enables data skipping, allowing Hudi to prune unnecessary files before scanning."}),"\n",(0,n.jsx)(a.li,{children:"SQL-Based Index Management: Users can create, drop, and manage indexes using SQL, making secondary indexes easily accessible."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Hudi supports hash-based secondary indexes, which are horizontally scalable by distributing keys across shards for fast writes and lookups."}),"\n",(0,n.jsxs)(a.p,{children:["If you are interested in the implementation details of secondary indexes, you can read more ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/tech-specs-1point0/#secondary-index",children:"here"}),"."]}),"\n",(0,n.jsx)(a.h3,{id:"creating-a-secondary-index-in-hudi",children:"Creating a Secondary Index in Hudi"}),"\n",(0,n.jsx)(a.p,{children:"In Hudi 1.0, secondary indexes are supported currently in Apache Spark, with future support planned for Flink, Presto, and Trino in Hudi 1.1."}),"\n",(0,n.jsx)(a.p,{children:"Let\u2019s see an example of creating a Hudi table with a secondary index."}),"\n",(0,n.jsxs)(a.p,{children:["First, let\u2019s create a table with a record index enabled. The record index maintains mappings of record keys (",(0,n.jsx)(a.code,{children:"id"}),") to file groups, enabling fast updates, deletes, and lookups."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"DROP TABLE IF EXISTS hudi_table;\nCREATE TABLE hudi_table (\n    ts BIGINT,\n    id STRING,\n    rider STRING,\n    driver STRING,\n    fare DOUBLE,\n    city STRING,\n    state STRING\n) USING hudi\nOPTIONS (\n    primaryKey = 'id',\n    hoodie.metadata.record.index.enable = 'true',  -- Enable record index\n    hoodie.write.record.merge.mode = \"COMMIT_TIME_ORDERING\" -- Only Required for 1.0.0 version\n)\nPARTITIONED BY (city, state)\nLOCATION 'file:///tmp/hudi_test_table';\n"})}),"\n",(0,n.jsxs)(a.p,{children:["Now we can create a secondary index on the ",(0,n.jsx)(a.code,{children:"city"})," field to optimize queries filtering on this column."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"CREATE INDEX idx_city ON hudi_table(city);\n"})}),"\n",(0,n.jsx)(a.p,{children:"Now, when executing a query such as:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"SELECT rider FROM hudi_table WHERE city = 'SFO';\n"})}),"\n",(0,n.jsxs)(a.p,{children:["\u2705 Hudi first checks the secondary index to determine which records match the filter condition.",(0,n.jsx)(a.br,{}),"\n","\u2705 It then uses the record index to locate the exact file group for retrieval.",(0,n.jsx)(a.br,{}),"\n","\u2705 Data skipping is applied, reducing the number of files read from cloud storage."]}),"\n",(0,n.jsx)(a.p,{children:"Users can also create secondary indexes using the Spark DataSource API by setting the following configurations:"}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Config Name"}),(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Default"}),(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Description"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:(0,n.jsx)(a.code,{children:"hoodie.metadata.index.secondary.enable"})}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"true"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"Enables secondary index maintenance. When true, Hudi writers automatically maintain all secondary indexes within the metadata table. When disabled, secondary indexes must be created manually using SQL."})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:(0,n.jsx)(a.code,{children:"hoodie.datasource.write.secondarykey.column"})}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"(N/A)"}),(0,n.jsxs)(a.td,{style:{textAlign:"left"},children:["Specifies the columns to be used as secondary keys. Supports dot notation for nested fields (e.g., ",(0,n.jsx)(a.code,{children:"customer.region"}),")."]})]})]})]}),"\n",(0,n.jsx)(a.h3,{id:"asynchronous-indexing-in-hudi",children:"Asynchronous Indexing in Hudi"}),"\n",(0,n.jsxs)(a.p,{children:["A notable thing about Hudi\u2019s indexing system is that it offers ",(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/blog/asynchronous-indexing-using-hudi",children:"asynchronous indexing"})," as a service. Traditional indexing approaches often introduce performance bottlenecks, as index maintenance needs to be performed synchronously with writes. Hudi\u2019s asynchronous indexing service eliminates the performance bottlenecks of traditional indexing by decoupling index maintenance from ingestion. Instead of requiring synchronous updates that slow down writes, Hudi builds indexes in the background, ensuring ingestion remains uninterrupted."]}),"\n",(0,n.jsxs)(a.p,{children:["A key aspect of this design is timeline-consistent indexing, where a new indexing action is introduced in Hudi\u2019s transactional ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/timeline",children:"timeline"}),". The indexer service schedules indexing by adding an ",(0,n.jsx)(a.code,{children:"indexing.requested"})," instant, moves it to ",(0,n.jsx)(a.code,{children:"inflight"})," during execution, and finally marks it ",(0,n.jsx)(a.code,{children:"completed"})," once indexing is done, without locking index file writes. This enables a scalable indexing framework, allowing indexes to be dynamically added or removed without downtime as datasets grow. Async indexing also supports multiple index types, including secondary indexes."]}),"\n",(0,n.jsx)(a.h2,{id:"benchmarking",children:"Benchmarking"}),"\n",(0,n.jsxs)(a.p,{children:["We ran a simple benchmark using the TPCDS 1TB dataset, created the index on one of the fact table ",(0,n.jsx)(a.code,{children:"web_sales"})," and ran a complex join query with lookup on customer id."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"Setup:"})}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Uses 1TB TPCDS public dataset."}),"\n",(0,n.jsx)(a.li,{children:"Apache Spark version -  3.5.5 installed on EMR cluster"}),"\n",(0,n.jsx)(a.li,{children:"Apache Hudi version - 1.0.1"}),"\n",(0,n.jsxs)(a.li,{children:["Table on which secondary index is created - ",(0,n.jsx)(a.code,{children:"web_sales"})]}),"\n",(0,n.jsxs)(a.li,{children:["Column on which Secondary Index is created - ",(0,n.jsx)(a.code,{children:"ws_ship_customer_sk"})]}),"\n",(0,n.jsxs)(a.li,{children:["Cluster Configurations","\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Nodes: m5.xlarge (10 executors)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["To evaluate performance, we executed the same query multiple times within the same Spark session. The table below demonstrates an approximately ",(0,n.jsx)(a.strong,{children:"33%"})," improvement in the first run and a ",(0,n.jsx)(a.strong,{children:"58%"})," improvement in the second run. Additionally, the amount of data scanned was reduced by ",(0,n.jsx)(a.strong,{children:"90%"})," when using the secondary index."]}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{style:{textAlign:"left"}}),(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Run 1"}),(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Run 2"}),(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Files Read"}),(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"File Size Read"}),(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Rows Scanned"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"Without Secondary index"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"32 sec"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"14 sec"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"5000"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"67 GB"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"719 M"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"With Secondary Index"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"22 sec"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"6 sec"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"521"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"7 GB"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"75 M"})]})]})]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"Read Query used for Benchmarking:"})}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"SELECT\n   ws.ws_order_number,\n   ws.ws_item_sk,\n   ws.ws_quantity,\n   ws.ws_sales_price,\n   c.c_customer_id,\n   c.c_first_name,\n   c.c_last_name,\n   d.d_date,\n   wp.wp_web_page_id\nFROM\n   web_sales ws\nJOIN\n   tpcds_hudi_1tb.customer c ON ws.ws_ship_customer_sk = c.c_customer_sk\nJOIN\n   tpcds_hudi_1tb.date_dim d ON ws.ws_ship_date_sk = d.d_date_sk\nJOIN\n   tpcds_hudi_1tb.web_page wp ON ws.ws_web_page_sk = wp.wp_web_page_sk\nWHERE\n   ws.ws_ship_customer_sk = '647632'\nORDER BY\n   ws.ws_order_number\n"})}),"\n",(0,n.jsx)(a.p,{children:"As shown in the DAG below, there is a significant difference in the amount of data scanned and other metrics (see the highlighted part) for the websales table, both with and without the secondary index."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"Spark SQL Stats  with Secondary index"})}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/sec_index_spark1.png",alt:"orch",width:"600",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"Spark SQL Stats  without Secondary index"})}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/sec_index_spark2.png",alt:"orch",width:"600",align:"middle"}),"\n",(0,n.jsx)(a.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,n.jsxs)(a.p,{children:["Indexing has been a core component of Apache Hudi since its inception, enabling efficient upserts and deletes at scale. With Hudi 1.0, the introduction of secondary indexing expands these capabilities by allowing queries to efficiently filter and retrieve records based on ",(0,n.jsx)(a.em,{children:"non-primary key"})," fields, significantly reducing data scans and improving query performance. Looking ahead, secondary indexing in Hudi opens new possibilities for further optimizations, such as accelerating complex joins and MERGE INTO operations."]}),"\n",(0,n.jsxs)(a.p,{children:["Additionally, to ensure that index maintenance does not introduce bottlenecks, Hudi\u2019s ",(0,n.jsx)(a.em,{children:"asynchronous indexing"})," service decouples index updates from ingestion, enabling seamless scaling while keeping indexes timeline-consistent and ACID-compliant. These advancements further solidify Hudi\u2019s role as a high-performance lakehouse platform, making data structures such as secondary indexes more accessible."]}),"\n",(0,n.jsx)(a.hr,{})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},60387:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(3987),n=t(74848),s=t(28453);const r={title:"Out of the box Key Generators in Apache Hudi",excerpt:"Explain need for key gerators and out of box key generators in Apache Hudi",author:"Aditya Goenka",category:"blog",image:"/assets/images/blog/2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.png",tags:["Data Lake","Data Lakehouse","Apache Hudi","Key Generators","partition"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Introduction",id:"introduction",level:2},{value:"Challenge",id:"challenge",level:2},{value:"Approaches to Handling this in Data Pipelines",id:"approaches-to-handling-this-in-data-pipelines",level:2},{value:"What are Key Generators in Apache Hudi",id:"what-are-key-generators-in-apache-hudi",level:2},{value:"Out of the Box Key Generators",id:"out-of-the-box-key-generators",level:2},{value:"SimpleKeyGenerator",id:"simplekeygenerator",level:3},{value:"NonpartitionedKeyGenerator",id:"nonpartitionedkeygenerator",level:3},{value:"ComplexKeyGenerator",id:"complexkeygenerator",level:3},{value:"TimestampBasedKeygenerator",id:"timestampbasedkeygenerator",level:3},{value:"Common Use Cases",id:"common-use-cases",level:4},{value:"CustomKeyGenerator",id:"customkeygenerator",level:3},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const a={a:"a",code:"code",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.h2,{id:"introduction",children:"Introduction"}),"\n",(0,n.jsxs)(a.p,{children:["The goal of Apache Hudi is to bring database-like features to data lakes. This addresses the main shortcoming of traditional data lakes: the inability to easily perform row-level updates or deletions.By integrating database-like management capabilities into data lakes, Hudi revolutionizes how it handles and processes large volumes of data, enabling out-of-the-box upserts and deletes that facilitate efficient record level updating and deletion.\nOne of Hudi's key innovations is the ability for users to explicitly define a Record Key, similar to a unique key in traditional databases, along with a Partition Key that aligns with the data lake paradigm. These two keys make the ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieKey.java",children:"HoodieKey"})," that aligns with the data lake paradigm. These two keys make the HoodieKey which is similar to the primary key which uniquely defines each row. This enables hudi to do the upsert based on Hoodiekey. The upsert operation works by utilizing the HoodieKey to locate the exact file group where the data associated with that key resides.  When a new record is ingested into the Hudi table, the system first derives  the HoodieKey of the incoming record based on the unique key and partitioning schema configured. This key is used to determine which file group (a logical grouping of files) the record should be associated with which is usually achieved via an ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/indexes",children:"indexing"})," mechanism.\nIn this blog, we will explore the concept of Key Generators in Apache Hudi, how they enhance data management, and their role in enabling efficient data operations in modern data lakes."]}),"\n",(0,n.jsx)(a.h2,{id:"challenge",children:"Challenge"}),"\n",(0,n.jsx)(a.p,{children:"The biggest challenge in defining the record key and partition key on a table is  the columns in input data does not naturally lend itself to being used as a primary key or partition key directly. In the realm of databases, we often have below cases -"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Need to have multiple fields that serve as primary key commonly known as composite keys in the database."}),"\n",(0,n.jsx)(a.li,{children:"It is necessary to preprocess the data to derive a specific field that can serve as a primary key before loading it into the database."}),"\n",(0,n.jsx)(a.li,{children:"Sometimes we have to generate unique ids also. Common use case is surrogate key."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Similarly, for partition columns also in datalakes, most of the time the raw field can\u2019t be used as a partition key."}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Partition columns often have time grain like month level or year level partition but input data mostly contain timestamp and date."}),"\n",(0,n.jsx)(a.li,{children:"Nested primary keys are very common, and necessitates multiple partition columns."}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"approaches-to-handling-this-in-data-pipelines",children:"Approaches to Handling this in Data Pipelines"}),"\n",(0,n.jsx)(a.p,{children:"Data Lake and Lakehouse technologies typically address such scenarios by preprocessing the data. For example, if date-based partitioning is required and a timestamp column is available, the data must be processed using Spark SQL date functions to extract relevant components (e.g., year, month, day). These derived columns are then used for partitioning. However, this process can become cumbersome at scale, especially when multiple data streams are writing to the same Hudi table. The same extraction logic needs to be applied to all streams, and any table maintenance activities (such as bootstrapping or backfilling) also require this logic to be reapplied. This repetition is error-prone and can lead to data consistency issues if the logic is incorrectly applied.\nHudi addresses these challenges with a built-in solution: key generators. These can be configured at the table level, eliminating the need to repeatedly apply the same logic. With key generators, Hudi automatically handles the conversion process every time, ensuring consistency and reducing the risk of errors."}),"\n",(0,n.jsx)(a.h2,{id:"what-are-key-generators-in-apache-hudi",children:"What are Key Generators in Apache Hudi"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/key_generation",children:"Key generators"})," in Apache Hudi are essential components responsible for creating record keys and partition keys for records within a dataset. Hudi uses key generators to extract the Hudi record key, which is a combination of the record key and the partition key, from the incoming record fields. This process allows Hudi to efficiently prepare the hoodie key on which updates can occur. During upserts, Hudi identifies the file group that contains the specified hoodie key using an index and updates the corresponding file group accordingly.\nHudi offers several built-in key generator implementations that cover common use cases, such as generating record keys based on fields from the input data. However, to provide flexibility and support for more complex use cases, Hudi also offers a pluggable interface. This allows users to implement custom key generators tailored to their specific requirements.\nTo create a custom key generator, you can extend the ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/keygen/BaseKeyGenerator.java",children:"BaseKeyGenerator"})," class which itself extends the ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/keygen/KeyGenerator.java",children:"KeyGenerator"}),"  class and implement methods such as getRecordKey and getPartitionKey. This enables you to define the specific logic required for calculating record and partition keys tailored to your dataset's requirements. Additionally, Hudi includes a variety of built-in key generators that address many common scenarios discussed in the previous section, streamlining the process of key generation for users.\nThe key generator is configured at the table level and stored in the hoodie.properties file, which resides within the .hoodie directory. This file contains all the table-level configurations, including the key generation settings. Once a table is created with a particular key generator we can\u2019t change it. It can be set using the configuration hoodie.datasource.write.keygenerator.class"]}),"\n",(0,n.jsx)(a.h2,{id:"out-of-the-box-key-generators",children:"Out of the Box Key Generators"}),"\n",(0,n.jsx)(a.h3,{id:"simplekeygenerator",children:"SimpleKeyGenerator"}),"\n",(0,n.jsx)(a.p,{children:"The SimpleKeyGenerator is a basic key generator used in Apache Hudi when direct fields from the input dataset can serve as both the record key and partition key. It maps a specific column in the DataFrame to the record key and another column to the partition path. This widely-used generator interprets values as-is from the DataFrame and converts them to strings, making it ideal for straightforward data structures.\nPlease note that this is the default key generator for the partitioned datasets."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-shell",children:'{\n  "hoodie.datasource.write.recordkey.field": "id",\n  "hoodie.datasource.write.partitionpath.field": "date",\n  "hoodie.datasource.write.keygenerator.class": "org.apache.hudi.keygen.SimpleKeyGenerator"\n}\n'})}),"\n",(0,n.jsx)(a.h3,{id:"nonpartitionedkeygenerator",children:"NonpartitionedKeyGenerator"}),"\n",(0,n.jsx)(a.p,{children:"The NonpartitionedKeyGenerator is a key generator in Apache Hudi designed specifically for non-partitioned datasets. Unlike the SimpleKeyGenerator, which uses a field to determine the partition path for the data, the NonpartitionedKeyGenerator does not assign a partition key to the records. Instead, it returns an empty string as the partition key for all records. This is because the dataset is non-partitioned, meaning all records are stored in a single partition."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-shell",children:'{\n  "hoodie.datasource.write.recordkey.field": "id",\n  "hoodie.datasource.write.keygenerator.class": "org.apache.hudi.keygen.NonpartitionedKeyGenerator"\n}\n'})}),"\n",(0,n.jsx)(a.h3,{id:"complexkeygenerator",children:"ComplexKeyGenerator"}),"\n",(0,n.jsxs)(a.p,{children:["This key generator is used when multiple fields are used to create the record key or partition key. We can provide the comma separated list of the columns. In the output, the hoodie record key is generated using the format key1",":value1",",key2",":value2",". If any one of the partition key or record key contains multiple fields, then we have to use ComplexKeyGenerator."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-shell",children:'{\n  "hoodie.datasource.write.keygenerator.class" : "org.apache.hudi.keygen.ComplexKeyGenerator",\n  "hoodie.datasource.write.recordkey.field" = "key1,key2",\n  "hoodie.datasource.write.partitionpath.field" = "country,state,city"\n}\n'})}),"\n",(0,n.jsx)(a.h3,{id:"timestampbasedkeygenerator",children:"TimestampBasedKeygenerator"}),"\n",(0,n.jsx)(a.p,{children:"The TimestampBasedKeyGenerator allows you to generate partition keys based on timestamp fields in your data. This is especially useful when you want to partition your data by date, month, or year, depending on your use case. The key generator can transform timestamps into different formats, enabling you to create partitions that suit your analytical needs."}),"\n",(0,n.jsx)(a.p,{children:"Relevant Configurations"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"hoodie.datasource.write.keygenerator.class"}),"\nTo use this key generator, The key gen class should be ",(0,n.jsx)(a.code,{children:"org.apache.hudi.keygen.TimestampBasedKeyGenerator"})]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"hoodie.deltastreamer.keygen.timebased.timestamp.type"}),"\nThis config determines the nature of the value of input. Below can be the possible values for this -\n",(0,n.jsx)(a.strong,{children:"DATE_STRING"}),": Use this when the input value is in string format."]}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"MIXED: This option allows for a combination of formats."}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"UNIX_TIMESTAMP: Select this when the input value is in epoch timestamp format (long type) measured in seconds."}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"EPOCHMILLISECONDS: Use this when the input value is in epoch timestamp format (long type) measured in milliseconds."}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"SCALAR: This option is for epoch timestamp values (long type) where you can specify any time unit."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"hoodie.deltastreamer.keygen.timebased.timestamp.scalar.time.unit"}),"\nWhen using the SCALAR timestamp type, you can define the unit of the epoch time. Valid options include NANOSECONDS, MICROSECONDS, MILLISECONDS, SECONDS, MINUTES, HOURS, DAYS"]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"hoodie.keygen.timebased.input.dateformat"}),"\nWhen the timestamp type is DATE_STRING or MIXED, this config can be defined to specify the date format in which the field is coming in input."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"hoodie.keygen.timebased.output.dateformat"}),"\nWhen the timestamp type is set to DATE_STRING or MIXED, this configuration defines the desired date format for the output field. It allows you to specify how the date should be formatted when it is generated or output."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"hoodie.deltastreamer.keygen.timebased.input.timezone"}),"\nThis setting specifies the timezone for the input date field derived from the raw data. The default value is UTC."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"hoodie.deltastreamer.keygen.timebased.output.timezone"}),"\nThis setting defines the timezone for the output date field that will be used to populate the partition column. The default value is UTC."]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.h4,{id:"common-use-cases",children:"Common Use Cases"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Data Contains Timestamp Field and We Want Date Level Partitions\nIn this scenario, you have a dataset with a timestamp field, and you want to partition the data by the date (i.e., year-month-day)."}),"\n"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-shell",children:'{\n  "hoodie.datasource.write.keygenerator.class":     "org.apache.hudi.keygen.TimestampBasedKeyGenerator",\n  "hoodie.deltastreamer.keygen.timebased.timestamp.type": "DATE_STRING",\n  "hoodie.keygen.timebased.input.dateformat":"yyyy-MM-dd\'T\'HH:mm:ss.SSSSSSZ",\n  "hoodie.keygen.timebased.output.dateformat":"yyyy-MM-dd",\n  "hoodie.datasource.write.partitionpath.field": "event_time"\n}\n'})}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Data Contains Date Field but We Want to Have Month or Year Level Partitions\nHere, you have a dataset with a date field, but you want to create partitions at a higher granularity, such as by month or year."}),"\n"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-shell",children:'{\n  "hoodie.datasource.write.keygenerator.class":     "org.apache.hudi.keygen.TimestampBasedKeyGenerator",\n  "hoodie.deltastreamer.keygen.timebased.timestamp.type": "DATE_STRING",\n  "hoodie.keygen.timebased.input.dateformat":"yyyy-MM-dd",\n  "hoodie.keygen.timebased.output.dateformat":"yyyyMM",\n  "hoodie.datasource.write.partitionpath.field": "event_date"\n}\n'})}),"\n",(0,n.jsx)(a.p,{children:"In the example above, if we have an input with a date column named event_date in the format 'yyyy-MM-dd', the configurations will convert this format to a monthly level in the format 'yyyyMM' and use it as the partition column."}),"\n",(0,n.jsxs)(a.p,{children:["We can refer ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/0.10.0/key_generation/#timestampbasedkeygenerator",children:"TimestampBasedKeyGenerator"})," for more examples"]}),"\n",(0,n.jsx)(a.h3,{id:"customkeygenerator",children:"CustomKeyGenerator"}),"\n",(0,n.jsxs)(a.p,{children:["In typical use cases, using the same key generator for both the record key and the partition key often does not meet the requirements. For such scenarios, a Custom Key Generator is particularly useful, as it allows for the use of different key generators for different fields.\nA common use case arises when the partition key consists of multiple fields, and you also need to extract date or month-level partitions from a timestamp field. In these situations, it is essential to utilize both the TimestampBasedKeyGenerator and the ComplexKeyGenerator. However, since you cannot specify two different key generator classes simultaneously, the CustomKeyGenerator serves as an effective solution. We can configure it as list of comma separated fields with the key generator separated by colon. Example - key1",":Timestamp",",key2",":SIMPLE",",key3",":SIMPLE","\nWhen we pass the partition column, we can also provide which key generator to use. The configurations below enable you to use SimpleKeyGenerator to extract the country field and TimestampBasedKeygenerator to transform the event_date field to use only month level partitions."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-shell",children:'{\n  "hoodie.datasource.write.keygenerator.class":     "org.apache.hudi.keygen.TimestampBasedKeyGenerator",\n  "hoodie.deltastreamer.keygen.timebased.timestamp.type": "DATE_STRING",\n  "hoodie.keygen.timebased.input.dateformat":"yyyy-MM-dd",\n  "hoodie.keygen.timebased.output.dateformat":"yyyyMM",\n  "hoodie.datasource.write.partitionpath.field": "country:SIMPLE,event_date:TIMESTAMP"\n}\n'})}),"\n",(0,n.jsx)(a.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,n.jsx)(a.p,{children:"Key generators in Hudi are vital components that enable efficient record identification, partitioning, and data operations in large datasets. Whether you're performing upserts, deletes, or managing time-series data, choosing the right key generator ensures that Hudi can handle the data efficiently, while aligning with your business logic. By addressing challenges like composite keys, timestamp-based partitioning, and complex use cases, Apache Hudi revolutionizes how data lakes handle evolving data, providing database-like management capabilities that are scalable and flexible."})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},60441:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/01/18/apache-hudi-1-0-now-generally-available","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-01-18-apache-hudi-1-0-now-generally-available.mdx","source":"@site/blog/2025-01-18-apache-hudi-1-0-now-generally-available.mdx","title":"Apache Hudi 1.0 Now Generally Available","description":"Redirecting... please wait!!","date":"2025-01-18T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"hudi 1.0.0","permalink":"/blog/tags/hudi-1-0-0"},{"inline":true,"label":"infoq","permalink":"/blog/tags/infoq"}],"readingTime":0.1,"hasTruncateMarker":false,"authors":[{"name":"Renato Losio","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi 1.0 Now Generally Available","author":"Renato Losio","category":"blog","image":"/assets/images/blog/2025-01-18-apache-hudi-1-0-now-generally-available.jpeg","tags":["blog","apache hudi","hudi 1.0.0","infoq"]},"unlisted":false,"prevItem":{"title":"Concurrency Control in Open Data Lakehouse","permalink":"/blog/2025/01/28/concurrency-control"},"nextItem":{"title":"Out of the box Key Generators in Apache Hudi","permalink":"/blog/2025/01/15/outofbox-key-generators-in-hudi"}}')},60725:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/02/13/hudi-key-generators","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-02-13-hudi-key-generators.md","source":"@site/blog/2021-02-13-hudi-key-generators.md","title":"Apache Hudi Key Generators","description":"Every record in Hudi is uniquely identified by a primary key, which is a pair of record key and partition path where","date":"2021-02-13T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"key generators","permalink":"/blog/tags/key-generators"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":6.26,"hasTruncateMarker":true,"authors":[{"name":"shivnarayan","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi Key Generators","excerpt":"Different key generators available with Apache Hudi","author":"shivnarayan","category":"blog","tags":["blog","key generators","apache hudi"]},"unlisted":false,"prevItem":{"title":"Time travel operations in Hopsworks Feature Store","permalink":"/blog/2021/02/24/Time-travel-operations-in-Hopsworks-Feature-Store"},"nextItem":{"title":"Optimize Data lake layout using Clustering in Apache Hudi","permalink":"/blog/2021/01/27/hudi-clustering-intro"}}')},61536:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/11/26/Real-Time-Data-Processing-with-Postgres-Debezium-Kafka-Schema-Registry-and-DeltaStreamer-Guide-for-Begineers","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-11-26-Real-Time-Data-Processing-with-Postgres-Debezium-Kafka-Schema-Registry-and-DeltaStreamer-Guide-for-Begineers.mdx","source":"@site/blog/2023-11-26-Real-Time-Data-Processing-with-Postgres-Debezium-Kafka-Schema-Registry-and-DeltaStreamer-Guide-for-Begineers.mdx","title":"Real-Time Data Processing with Postgres, Debezium, Kafka, Schema Registry, and Delta Streamer Guide for Begineers","description":"Redirecting... please wait!!","date":"2023-11-26T00:00:00.000Z","tags":[{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"postgres","permalink":"/blog/tags/postgres"},{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"debezium","permalink":"/blog/tags/debezium"},{"inline":true,"label":"apache kafka","permalink":"/blog/tags/apache-kafka"},{"inline":true,"label":"deltastreamer","permalink":"/blog/tags/deltastreamer"},{"inline":true,"label":"linkedin","permalink":"/blog/tags/linkedin"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"Soumil Shah","key":null,"page":null}],"frontMatter":{"title":"Real-Time Data Processing with Postgres, Debezium, Kafka, Schema Registry, and Delta Streamer Guide for Begineers","excerpt":"Real-Time Data Processing with Postgres, Debezium, Kafka, Schema Registry, and Delta Streamer Guide for Begineers","author":"Soumil Shah","category":"blog","image":"/assets/images/blog/2023-11-26-Real-Time-Data-Processing-with-Postgres-Debezium-Kafka-Schema-Registry-and-DeltaStreamer-Guide-for-Begineers.png","tags":["apache hudi","postgres","how-to","debezium","apache kafka","deltastreamer","linkedin"]},"unlisted":false,"prevItem":{"title":"Apache Hudi (Part 1): History, Getting Started","permalink":"/blog/2023/11/28/Apache-Hudi-Part-1-History-Getting-Started"},"nextItem":{"title":"Introducing Apache Hudi support with AWS Glue crawlers","permalink":"/blog/2023/11/22/Introducing-Apache-Hudi-support-with-AWS-Glue-crawlers"}}')},61616:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/12/01/Run-apache-hudi-at-scale-on-aws","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-12-01-Run-apache-hudi-at-scale-on-aws.mdx","source":"@site/blog/2022-12-01-Run-apache-hudi-at-scale-on-aws.mdx","title":"Run Apache Hudi at scale on AWS","description":"Redirecting... please wait!!","date":"2022-12-01T00:00:00.000Z","tags":[{"inline":true,"label":"aws","permalink":"/blog/tags/aws"},{"inline":true,"label":"guide","permalink":"/blog/tags/guide"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":0.19,"hasTruncateMarker":false,"authors":[{"name":"Imtiaz Sayed,","socials":{},"key":null,"page":null},{"name":"Shana Schipers","socials":{},"key":null,"page":null},{"name":"Dylan Qu","socials":{},"key":null,"page":null},{"name":"Carlos Rodrigues","socials":{},"key":null,"page":null},{"name":"Arun A K","socials":{},"key":null,"page":null},{"name":"Francisco Morillo","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Run Apache Hudi at scale on AWS","authors":[{"name":"Imtiaz Sayed,"},{"name":"Shana Schipers"},{"name":"Dylan Qu"},{"name":"Carlos Rodrigues"},{"name":"Arun A K"},{"name":"Francisco Morillo"}],"category":"technical guide","image":"/assets/images/blog/run-hudi-at-scale-on-aws.png","tags":["aws","guide","apache hudi"]},"unlisted":false,"prevItem":{"title":"Build Your First Hudi Lakehouse with AWS S3 and AWS Glue","permalink":"/blog/2022/12/19/Build-Your-First-Hudi-Lakehouse-with-AWS-Glue-and-AWS-S3"},"nextItem":{"title":"Build your Apache Hudi data lake on AWS using Amazon EMR \u2013 Part 1","permalink":"/blog/2022/11/22/Build-your-Apache-Hudi-data-lake-on-AWS-using-Amazon-EMR-Part-1"}}')},61684:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/06/04/Apache-Hudi-How-Uber-gets-data-a-ride-to-its-destination","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-06-04-Apache-Hudi-How-Uber-gets-data-a-ride-to-its-destination.mdx","source":"@site/blog/2021-06-04-Apache-Hudi-How-Uber-gets-data-a-ride-to-its-destination.mdx","title":"Apache Hudi: How Uber gets data a ride to its destination","description":"Redirecting... please wait!!","date":"2021-06-04T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"rtinsights","permalink":"/blog/tags/rtinsights"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Joe McKendrick","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Apache Hudi: How Uber gets data a ride to its destination","authors":[{"name":"Joe McKendrick"}],"category":"blog","tags":["blog","rtinsights"]},"unlisted":false,"prevItem":{"title":"Employing correct configurations for Hudi\'s cleaner table service","permalink":"/blog/2021/06/10/employing-right-configurations-for-hudi-cleaner"},"nextItem":{"title":"Experts primer on Apache Hudi","permalink":"/blog/2021/05/12/Experts-primer-on-Apache-Hudi"}}')},61815:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/11/07/how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-11-07-how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse.mdx","source":"@site/blog/2025-11-07-how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse.mdx","title":"How FreeWheel Uses Apache Hudi to Power Its Data Lakehouse","description":"Talk title slide","date":"2025-11-07T00:00:00.000Z","tags":[{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"case-study","permalink":"/blog/tags/case-study"},{"inline":true,"label":"freewheel","permalink":"/blog/tags/freewheel"}],"readingTime":6.54,"hasTruncateMarker":false,"authors":[{"name":"The Hudi Community","key":null,"page":null}],"frontMatter":{"title":"How FreeWheel Uses Apache Hudi to Power Its Data Lakehouse","excerpt":"How FreeWheel unified batch and streaming with an Apache Hudi\u2013powered lakehouse to improve freshness, simplify operations, and scale analytics.","author":"The Hudi Community","category":"blog","image":"/assets/images/blog/2025-11-07-how-freewheel-uses-apache-hudi-to-power-its-data-lakehouse/image1.png","tags":["hudi","lakehouse","case-study","freewheel"]},"unlisted":false,"prevItem":{"title":"Deep Dive Into Hudi\'s Indexing Subsystem (Part 2 of 2)","permalink":"/blog/2025/11/12/deep-dive-into-hudis-indexing-subsystem-part-2-of-2"},"nextItem":{"title":"Deep Dive Into Hudi\u2019s Indexing Subsystem (Part 1 of 2)","permalink":"/blog/2025/10/29/deep-dive-into-hudis-indexing-subsystem-part-1-of-2"}}')},61972:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/10/07/iceberg-vs-delta-lake-vs-hudi-a-comparative-look-at-lakehouse-architectures","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-10-07-iceberg-vs-delta-lake-vs-hudi-a-comparative-look-at-lakehouse-architectures.mdx","source":"@site/blog/2024-10-07-iceberg-vs-delta-lake-vs-hudi-a-comparative-look-at-lakehouse-architectures.mdx","title":"Iceberg vs. Delta Lake vs. Hudi: A Comparative Look at Lakehouse Architectures","description":"Redirecting... please wait!!","date":"2024-10-07T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"Apache Iceberg","permalink":"/blog/tags/apache-iceberg"},{"inline":true,"label":"Delta Lake","permalink":"/blog/tags/delta-lake"},{"inline":true,"label":"comparison","permalink":"/blog/tags/comparison"},{"inline":true,"label":"forefathers","permalink":"/blog/tags/forefathers"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Abdelkbir Armel","key":null,"page":null}],"frontMatter":{"title":"Iceberg vs. Delta Lake vs. Hudi: A Comparative Look at Lakehouse Architectures","author":"Abdelkbir Armel","category":"blog","image":"/assets/images/blog/2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.png","tags":["blog","Apache Hudi","Apache Iceberg","Delta Lake","comparison","forefathers"]},"unlisted":false,"prevItem":{"title":"Streaming DynamoDB Data into a Hudi Table: AWS Glue in Action","permalink":"/blog/2024/10/14/streaming-dynamodb-data-into-a-hudi-table-aws-glue-in-action"},"nextItem":{"title":"Mastering Slowly Changing Dimensions with Apache Hudi & Spark SQL","permalink":"/blog/2024/10/07/mastering-slowly-changing-dimensions-with-apache-hudi-and-spark-sql"}}')},62265:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(44692),n=t(74848),s=t(28453),r=t(9230);const o={title:"Cost Optimization Strategies for scalable Data Lakehouse",author:"Suresh Hasundi",category:"blog",image:"/assets/images/blog/2024-03-22-data-lake-cost-optimisation-strategies.png",tags:["blog","apache hudi","amazon s3","amazon emr","apcache spark","lakehouse","cost optimization","halodoc"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blogs.halodoc.io/data-lake-cost-optimisation-strategies/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},62420:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(69777),n=t(74848),s=t(28453);const r={title:"Record Level Index: Hudi's blazing fast indexing for large-scale datasets",excerpt:"Announcing the Record Level Index in Apache Hudi",author:"Shiyan Xu and Sivabalan Narayanan",category:"blog",image:"/assets/images/blog/record-level-index/03.RLI_bulkinsert.png",tags:["design","indexing","metadata","apache hudi","blog"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Introduction",id:"introduction",level:2},{value:"Metadata table",id:"metadata-table",level:2},{value:"Record Level Index",id:"record-level-index",level:2},{value:"Initialization",id:"initialization",level:3},{value:"Updating RLI upon data table writes",id:"updating-rli-upon-data-table-writes",level:3},{value:"Writer Indexing",id:"writer-indexing",level:3},{value:"Read Flow",id:"read-flow",level:3},{value:"Storage",id:"storage",level:3},{value:"Performance",id:"performance",level:3},{value:"Write latency",id:"write-latency",level:4},{value:"Index look-up latency",id:"index-look-up-latency",level:4},{value:"Data shuffling",id:"data-shuffling",level:4},{value:"Query latency",id:"query-latency",level:4},{value:"When to Use",id:"when-to-use",level:3},{value:"Future Work",id:"future-work",level:2}];function c(e){const a={a:"a",code:"code",h2:"h2",h3:"h3",h4:"h4",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.h2,{id:"introduction",children:"Introduction"}),"\n",(0,n.jsx)(a.p,{children:"Index is a critical component that facilitates quick updates and deletes for Hudi writers, and it plays a pivotal\nrole in boosting query executions as well. Hudi provides several index types, including the Bloom and Simple indexes with global\nvariations, the HBase Index that leverages a HBase server, the hash-based Bucket index, and the multi-modal index\nrealized through the metadata table. The choice of an index depends on factors such as table sizes, partition data distributions,\nor traffic patterns, where a specific index may be more suitable for simpler operation or better performance[^1].\nUsers often face trade-offs when selecting index types for different tables, since there hasn't been\na generally performant index capable of facilitating both writes and reads with minimal operational overhead."}),"\n",(0,n.jsxs)(a.p,{children:["Starting from ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-0.14.0",children:"Hudi 0.14.0"}),", we are thrilled to announce a\ngeneral purpose index for Apache Hudi - the Record Level Index (RLI). This innovation not only dramatically boosts\nwrite efficiency but also improves read efficiency for relevant queries. Integrated seamlessly within the table storage layer,\nRLI can easily work without any additional operational efforts."]}),"\n",(0,n.jsx)(a.p,{children:"In the subsequent sections of this blog, we will give a brief introduction to Hudi's metadata table, a pre-requisite for discussing RLI.\nFollowing that, we will delve into the design and workflows of RLI, and then show performance analysis and index type comparisons. The blog\nwill conclude with insights into future work for RLI."}),"\n",(0,n.jsx)(a.h2,{id:"metadata-table",children:"Metadata table"}),"\n",(0,n.jsxs)(a.p,{children:["A ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/metadata",children:"Hudi metadata table"})," is a Merge-on-Read (MoR) table within the ",(0,n.jsx)(a.code,{children:".hoodie/metadata/"})," directory. It contains various\nmetadata pertaining to records, seamlessly integrated into both the writer and reader paths to improve indexing efficiency.\nThe metadata is segregated into four partitions: ",(0,n.jsx)(a.code,{children:"files"}),", ",(0,n.jsx)(a.code,{children:"column stats"}),", ",(0,n.jsx)(a.code,{children:"bloom filters"}),", and ",(0,n.jsx)(a.code,{children:"record level index"}),"."]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/record-level-index/01.metadatatable_layout.png",alt:"Hudi metadata table layout",width:"800",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"The metadata table is updated synchronously with each commit action on the Timeline, in other words, the commits to the\nmetadata table are part of the transactions to the Hudi data table. With four partitions containing different types of\nmetadata, this layout serves the purpose of a multi-modal index:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.code,{children:"files"})," partition keeps track of the Hudi data table\u2019s partitions, and data files of each partition"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.code,{children:"column stats"})," partition records statistics about each column of the data table"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.code,{children:"bloom filter"})," partition stores serialized bloom filters for base files"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.code,{children:"record level index"})," partition contains mappings of individual record key and the corresponding file group id"]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["Users can activate the metadata table by setting ",(0,n.jsx)(a.code,{children:"hoodie.metadata.enable=true"}),". Once activated, the ",(0,n.jsx)(a.code,{children:"files"})," partition\nwill always be enabled. Other partitions can be enabled and configured individually to harness additional indexing\ncapabilities."]}),"\n",(0,n.jsx)(a.h2,{id:"record-level-index",children:"Record Level Index"}),"\n",(0,n.jsxs)(a.p,{children:["Starting from release 0.14.0, the Record Level Index (RLI) can be activated by setting ",(0,n.jsx)(a.code,{children:"hoodie.metadata.record.index.enable=true"}),"\nand ",(0,n.jsx)(a.code,{children:"hoodie.index.type=RECORD_INDEX"}),'. The core concept behind RLI is the ability to determine the location of records, thus\nreducing the number of files that need to be scanned to extract the desired data. This process is usually referred to as "index look-up".\nHudi employs a primary-key model, requiring each record to be associated with a key\nto satisfy the uniqueness constraint. Consequently, we can establish one-to-one mappings between record keys and file groups,\nprecisely the data we intend to store within the ',(0,n.jsx)(a.code,{children:"record level index"})," partition."]}),"\n",(0,n.jsxs)(a.p,{children:["Performance is paramount when it comes to indexes. The metadata table, which includes the RLI partition, chooses ",(0,n.jsx)(a.a,{href:"https://hbase.apache.org/book.html#_hfile_format_2",children:"HFile"}),"[^2],\nHBase\u2019s file format that utilizes B+ tree-like structures for fast look-up, as the file format. Real-world benchmarking\nhas shown that an HFile containing 1 million RLI mappings can look up a batch of 100k records in just 600 ms.\nWe will cover the performance topic in a later section with detailed analysis."]}),"\n",(0,n.jsx)(a.h3,{id:"initialization",children:"Initialization"}),"\n",(0,n.jsx)(a.p,{children:"Initializing the RLI partition for an existing Hudi table can be a laborious and time-consuming task, contingent on the number\nof records. Just like with a typical database, building indexes takes time, but the investment ultimately pays off by speeding up\nnumerous queries in the future."}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/record-level-index/02.RLI_init_flow.png",alt:"RLI init flow",width:"800",align:"middle"}),"\n",(0,n.jsxs)(a.p,{children:["The diagram above shows the high-level steps of RLI initialization. Since these jobs are all parallelizable, users can\nscale the cluster and configure relevant parallelism settings (e.g., ",(0,n.jsx)(a.code,{children:"hoodie.metadata.max.init.parallelism"}),") accordingly\nto meet their time requirement."]}),"\n",(0,n.jsx)(a.p,{children:'Focusing on the final step, "Bulk insert to RLI partition," the metadata table writer employs a hash function to\npartition the RLI records, ensuring that the number of resulting file groups aligns with the number of partitions.\nThis guarantees consistent record key look-ups.'}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/record-level-index/03.RLI_bulkinsert.png",alt:"RLI bulkinsert",width:"800",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"It\u2019s important to note that the current implementation fixes the number of file groups in the RLI partition once it\u2019s initialized.\nTherefore, users should lean towards over-provisioning the file groups and adjust these configurations accordingly."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:"hoodie.metadata.record.index.max.filegroup.count\nhoodie.metadata.record.index.min.filegroup.count\nhoodie.metadata.record.index.max.filegroup.size\nhoodie.metadata.record.index.growth.factor\n"})}),"\n",(0,n.jsx)(a.p,{children:"In future development iterations, RLI should be able to overcome this limitation by dynamically rebalancing file groups to\naccommodate the ever-increasing number of records."}),"\n",(0,n.jsx)(a.h3,{id:"updating-rli-upon-data-table-writes",children:"Updating RLI upon data table writes"}),"\n",(0,n.jsx)(a.p,{children:"During regular writes, the RLI partition will be updated as part of the transactions. Metadata records will be generated\nusing the incoming record keys with their corresponding location info. Given that the RLI partition contains the exact\nmappings of record keys and locations, upserts to the data table will result in upsertion of the corresponding keys to the\nRLI partition, The hash function employed will guarantee that identical keys are routed to the same file group."}),"\n",(0,n.jsx)(a.h3,{id:"writer-indexing",children:"Writer Indexing"}),"\n",(0,n.jsx)(a.p,{children:"Being part of the write flow, RLI follows the high-level indexing flow, similar to any other global index: for a given\nset of records, it tags each record with location information if the index finds them present in any existing file group.\nThe key distinction lies in the source of truth for the existence test\u2014the RLI partition. The diagram below illustrates\nthe tagging flow with detailed steps."}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/record-level-index/04.RLI_tagging.png",alt:"RLI tagging",width:"800",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"The tagged records will be passed to Hudi write handles and will undergo write operations to their respective file groups.\nThe indexing process is a critical step in applying updates to the table, as its efficiency directly influences the write\nlatency. In a later section, we will demonstrate the Record Level Index performance using benchmarking results."}),"\n",(0,n.jsx)(a.h3,{id:"read-flow",children:"Read Flow"}),"\n",(0,n.jsx)(a.p,{children:"The Record Level Index is also integrated on the query side[^3]. In queries that involve equality check (e.g., EqualTo or IN)\nagainst the record key column, Hudi\u2019s file index implementation optimizes the file pruning process. This optimization is\nachieved by leveraging RLI to precisely locate the file groups that need to be read for completing the queries."}),"\n",(0,n.jsx)(a.h3,{id:"storage",children:"Storage"}),"\n",(0,n.jsx)(a.p,{children:"Storage efficiency is another vital aspect of the design. Each RLI mapping entry must include some necessary information\nto precisely locate files, such as record key, partition path, file group id, etc. To optimize the storage, RLI adopts\nsome compression techniques such as encoding file group id (in the form of UUID) into 2 Longs to represent the high and\nlow bits. Using Gzip compression and a 4MB block size, an individual RLI record averages only 48 bytes in size. To\nillustrate this more practically, let\u2019s assume we have a table of 100TB data with about 1 billion records (average record size = 100Kb).\nThe storage space required by the RLI partition will be approximately 48 Gb, which is less than 0.05% of the total data size.\nSince RLI contains the same number of entries as the data table, storage optimization is crucial to make RLI practical,\nespecially for tables of petabyte size and beyond."}),"\n",(0,n.jsx)(a.p,{children:"RLI exploits the low cost of storage to enable the rapid look-up process similar to the HBase index, while avoiding the\noperational overhead of running an extra server. In the next section, we will review some benchmarking results to demonstrate\nits performance advantages."}),"\n",(0,n.jsx)(a.h3,{id:"performance",children:"Performance"}),"\n",(0,n.jsx)(a.p,{children:"We conducted a comprehensive benchmarking analysis of the Record Level Index evaluating aspects such write latency,\nindex look-up latency, and data shuffling in comparison to existing indexing mechanisms in Hudi. In addition to the\nbenchmarks for write operations, we will also showcase the reduction in query latencies for point look-ups. Hudi 0.14.0\nand Spark 3.2.1 were used throughout the experiments."}),"\n",(0,n.jsx)(a.p,{children:"In comparison to the Global Simple Index (GSI) in Hudi, Record Level Index (RLI) is crafted for significant performance\nadvantages stemming from a greatly reduced scan space and minimized data shuffling. GSI conducts join operations between\nincoming records and existing data across all partitions of the data table, resulting in substantial data shuffling and\ncomputational overhead to pinpoint the records. On the other hand, RLI efficiently extracts location info through a\nhash function, leading to a considerably smaller amount of data shuffling by only loading the file groups of interest\nfrom the metadata table."}),"\n",(0,n.jsx)(a.h4,{id:"write-latency",children:"Write latency"}),"\n",(0,n.jsxs)(a.p,{children:["In the first set of experiments, we established two pipelines: one configured using GSI, and the other configured with RLI.\nEach pipeline was executed on an EMR cluster of 10 m5.4xlarge core instances, and was set to ingest batches of 200Mb data\ninto a 1TB dataset of 2 billion records. The RLI partition was configured with 1000 file groups. For N batches of ingestion,\n",(0,n.jsx)(a.strong,{children:"the average write latency using RLI showed a remarkable 72% improvement over GSI"}),"."]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/record-level-index/write-latency.png",alt:"metadata-rli",width:"600",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"Note: Between Global Simple Index and Global Bloom Index in Hudi, the former yielded better results due to the randomness\nof record keys. Therefore, we omitted the presentation of the Global Bloom Index in the chart."}),"\n",(0,n.jsx)(a.h4,{id:"index-look-up-latency",children:"Index look-up latency"}),"\n",(0,n.jsxs)(a.p,{children:["We also isolated the index look-up step using HoodieReadClient to accurately gauge indexing efficiency. Through\nexperiments involving the look-up of 400,000 records (0.02%) in a 1TB dataset of 2 billion records, ",(0,n.jsx)(a.strong,{children:"RLI showcased a\n72% improvement over GSI, consistent with the end-to-end write latency results"}),"."]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/record-level-index/index-latency.png",alt:"index-latency",width:"600",align:"middle"}),"\n",(0,n.jsx)(a.h4,{id:"data-shuffling",children:"Data shuffling"}),"\n",(0,n.jsxs)(a.p,{children:["In the index look-up experiments, we observed that around 85Gb of data was shuffled for GSI, whereas only 700Mb was shuffled\nfor RLI. ",(0,n.jsx)(a.strong,{children:"This reflects an impressive 92% reduction in data shuffling when using RLI compared to GSI"}),"."]}),"\n",(0,n.jsx)(a.h4,{id:"query-latency",children:"Query latency"}),"\n",(0,n.jsxs)(a.p,{children:["The Record Level Index will greatly boost Spark queries with \u201cEqualTo\u201d and \u201cIN\u201d predicates on record key columns.\nWe created a 400GB Hudi table comprising 20,000 file groups. When we executed a query predicated on a single record key,\nwe observed a significant improvement in query time. ",(0,n.jsx)(a.strong,{children:"With RLI enabled, the query time decreased from 977 seconds to just\n12 seconds, representing an impressive 98% reduction in latency"}),"[^4]."]}),"\n",(0,n.jsx)(a.h3,{id:"when-to-use",children:"When to Use"}),"\n",(0,n.jsx)(a.p,{children:"RLI demonstrates outstanding performance in general, elevating update and delete efficiency to a new level and\nfast-tracking reads when executing key-matching queries. Enabling RLI is also as simple as setting some configuration flags.\nBelow, we have summarized a comparison table highlighting these important characteristics of RLI in contrast to other common Hudi index types."}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{}),(0,n.jsx)(a.th,{children:"Record Level Index"}),(0,n.jsx)(a.th,{children:"Global Simple Index"}),(0,n.jsx)(a.th,{children:"Global Bloom Index"}),(0,n.jsx)(a.th,{children:"HBase Index"}),(0,n.jsx)(a.th,{children:"Bucket Index"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"Performant look-up in general"}),(0,n.jsx)(a.td,{children:"Yes"}),(0,n.jsx)(a.td,{children:"No"}),(0,n.jsx)(a.td,{children:"No"}),(0,n.jsx)(a.td,{children:"Yes, with possible throttling issues"}),(0,n.jsx)(a.td,{children:"Yes"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"Boost both writes and reads"}),(0,n.jsx)(a.td,{children:"Yes"}),(0,n.jsx)(a.td,{children:"No, write-only"}),(0,n.jsx)(a.td,{children:"No, write-only"}),(0,n.jsx)(a.td,{children:"No, write-only"}),(0,n.jsx)(a.td,{children:"No, write-only"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"Easy to enable"}),(0,n.jsx)(a.td,{children:"Yes"}),(0,n.jsx)(a.td,{children:"Yes"}),(0,n.jsx)(a.td,{children:"Yes"}),(0,n.jsx)(a.td,{children:"No, require HBase server"}),(0,n.jsx)(a.td,{children:"Yes"})]})]})]}),"\n",(0,n.jsx)(a.p,{children:"Many real-world applications will significantly benefit from using RLI. A common example is fulfilling the GDPR requirements.\nTypically, when users make requests, a set of IDs will be provided to identify the to-be-deleted records,\nwhich will either be updated (columns being nullified) or permanently removed.\nBy enabling RLI, offline jobs performing such changes will become notably more efficient, resulting in cost savings.\nOn the read side, analysts or engineers collecting historical events through certain tracing IDs will also\nexperience blazing fast responses from the key-matching queries."}),"\n",(0,n.jsx)(a.p,{children:"While RLI holds the above-mentioned advantages over all other index types, it is important to consider certain\naspects when using it. Similar to any other global index, RLI requires record-key uniqueness across all partitions in a table.\nAs RLI keeps track of all record keys and locations, the initialization process may take time for large tables.\nIn scenarios with extremely skewed large workloads, RLI might not achieve the desired performance due to limitations in the current design."}),"\n",(0,n.jsx)(a.h2,{id:"future-work",children:"Future Work"}),"\n",(0,n.jsx)(a.p,{children:'In this initial version of the Record Level Index, certain limitations are acknowledged. As mentioned in the\n"Initialization" section, the number of file groups must be predetermined during the creation of the RLI partition.\nHudi does use some heuristics and a growth factor for an existing table, but for a new table, it is recommended to\nset appropriate file group configs for RLI. As the data volume increases, the RLI partition requires re-bootstrapping\nwhen additional file groups are needed for scaling out. To address the need for rebalancing, a consistent hashing\ntechnique could be employed.'}),"\n",(0,n.jsx)(a.p,{children:"Another valuable enhancement would involve supporting the indexing of secondary columns alongside the record key\nfields, thus catering to a broader range of queries. On the reader side, there is a plan to integrate more query\nengines, such as Presto and Trino, with the Record Level Index to fully leverage the performance benefits offered\nby Hudi metadata tables."}),"\n",(0,n.jsx)(a.hr,{}),"\n",(0,n.jsxs)(a.p,{children:["[^1] ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2020/11/11/hudi-indexing-mechanisms/",children:"This blog"})," well-explained some best practices regarding index selection and configuration."]}),"\n",(0,n.jsx)(a.p,{children:"[^2] Other formats like Parquet can also be supported in the future."}),"\n",(0,n.jsx)(a.p,{children:"[^3] As of now, query engine integration is only available for Spark, with plans to support additional engines in the future."}),"\n",(0,n.jsx)(a.p,{children:"[^4] The query improvement is specific to record-key-matching queries and does not reflect a general reduction in latency by enabling RLI. In the case of the single record-key query, 99.995% of file groups (19999 out of 20000) were pruned during query execution."})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},62433:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(67152),n=t(74848),s=t(28453),r=t(9230);const o={title:"Top 3 Things You Can Do to Get Fast Upsert Performance in Apache Hudi",authors:[{name:"Nadine Farah"}],category:"blog",image:"/assets/images/blog/2023-05-10-top-3-things-you-can-do-to-get-fast-upsert-performance-in-apache-hudi.png",tags:["how-to","performance","onehouse"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.onehouse.ai/blog/top-3-things-you-can-do-to-get-fast-upsert-performance-in-apache-hudi",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},62766:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/02/19/bulk-insert-sort-modes-with-apache-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-02-19-bulk-insert-sort-modes-with-apache-hudi.mdx","source":"@site/blog/2023-02-19-bulk-insert-sort-modes-with-apache-hudi.mdx","title":"Bulk Insert Sort Modes with Apache Hudi","description":"Redirecting... please wait!!","date":"2023-02-19T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"bulk-insert","permalink":"/blog/tags/bulk-insert"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Sivabalan Narayanan","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Bulk Insert Sort Modes with Apache Hudi","authors":[{"name":"Sivabalan Narayanan"}],"category":"blog","tags":["blog","bulk-insert","medium"]},"unlisted":false,"prevItem":{"title":"Getting Started: Manage your Hudi tables with the admin Hudi-CLI tool","permalink":"/blog/2023/02/22/Getting-Started-Manage-your-Hudi-tables-with-the-admin-Hudi-CLI-tool"},"nextItem":{"title":"Table service deployment models in Apache Hudi","permalink":"/blog/2023/02/12/table-service-deployment-models-in-apache-hudi"}}')},62784:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/06/13/Optimizing-Apache-Hudi-Workflows-Automation-for-Clustering-Resizing-Concurrency","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-06-13-Optimizing-Apache-Hudi-Workflows-Automation-for-Clustering-Resizing-Concurrency.mdx","source":"@site/blog/2025-06-13-Optimizing-Apache-Hudi-Workflows-Automation-for-Clustering-Resizing-Concurrency.mdx","title":"Optimizing Apache Hudi Workflows: Automation for Clustering, Resizing & Concurrency","description":"Redirecting... please wait!!","date":"2025-06-13T00:00:00.000Z","tags":[{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"halodoc","permalink":"/blog/tags/halodoc"},{"inline":true,"label":"data lake","permalink":"/blog/tags/data-lake"},{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Halodoc, Apache Hudi","key":null,"page":null}],"frontMatter":{"title":"Optimizing Apache Hudi Workflows: Automation for Clustering, Resizing & Concurrency","author":"Halodoc, Apache Hudi","category":"blog","image":"/assets/images/blog/2025-06-13-Optimizing-Apache-Hudi-Workflows-Automation-for-Clustering-Resizing-Concurrency.png","tags":["hudi","blog","halodoc","data lake","lakehouse","apache hudi"]},"unlisted":false,"prevItem":{"title":"Apache Hudi does XYZ (1/10): File pruning with multi-modal index","permalink":"/blog/2025/06/16/Apache-Hudi-does-XYZ-110"},"nextItem":{"title":"Exploring Apache Hudi\u2019s New Log-Structured Merge (LSM) Timeline","permalink":"/blog/2025/05/29/lsm-timeline"}}')},62795:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/s3-migration-task-2-797ea4b89d2a3be41d476785040e2886.png"},63176:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/10/23/mastering-open-table-formats-a-guide-to-apache-iceberg-hudi-and-delta-lake","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-10-23-mastering-open-table-formats-a-guide-to-apache-iceberg-hudi-and-delta-lake.mdx","source":"@site/blog/2024-10-23-mastering-open-table-formats-a-guide-to-apache-iceberg-hudi-and-delta-lake.mdx","title":"Mastering Open Table Formats: A Guide to Apache Iceberg, Hudi, and Delta Lake","description":"Redirecting... please wait!!","date":"2024-10-23T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"Apache Iceberg","permalink":"/blog/tags/apache-iceberg"},{"inline":true,"label":"Delta Lake","permalink":"/blog/tags/delta-lake"},{"inline":true,"label":"comparison","permalink":"/blog/tags/comparison"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Naresh Dulam","key":null,"page":null}],"frontMatter":{"title":"Mastering Open Table Formats: A Guide to Apache Iceberg, Hudi, and Delta Lake","author":"Naresh Dulam","category":"blog","image":"/assets/images/blog/2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.png","tags":["blog","Apache Hudi","Apache Iceberg","Delta Lake","comparison","medium"]},"unlisted":false,"prevItem":{"title":"Using Apache Hudi with Apache Flink","permalink":"/blog/2024/10/23/Using-Apache-Hudi-with-Apache-Flink"},"nextItem":{"title":"Exploring Time Travel Queries in Apache Hudi","permalink":"/blog/2024/10/22/exploring-time-travel-queries-in-apache-hudi"}}')},63282:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(64236),n=t(74848),s=t(28453),r=t(9230);const o={title:"Building Analytical Apps on the Lakehouse using Apache Hudi, Daft & Streamlit",author:"Dipankar Mazumdar",category:"blog",image:"/assets/images/blog/2024-05-10-building-analytical-apps-on-the-lakehouse-using-apache-hudi-daft-streamlit.png",tags:["blog","apache hudi","python","daft","streamlit","lakehouse","apache-hudi-blogs"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/apache-hudi-blogs/building-analytical-apps-on-the-lakehouse-using-apache-hudi-daft-streamlit-3224766fe58a",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},63462:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(96211),n=t(74848),s=t(28453),r=t(9230);const o={title:"Quickly start using Apache Hudi on AWS EMR",authors:[{name:"Ritik Kaushik"}],category:"blog",tags:["blog","aws emr","cow","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@ritik20023/quickly-start-using-apache-hudi-on-aws-emr-de432c01e488",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},63661:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/08/22/Exploring-various-storage-types-in-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-08-22-Exploring-various-storage-types-in-Apache-Hudi.mdx","source":"@site/blog/2023-08-22-Exploring-various-storage-types-in-Apache-Hudi.mdx","title":"Exploring various storage types in Apache Hudi","description":"Redirecting... please wait!!","date":"2023-08-22T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"storage types","permalink":"/blog/tags/storage-types"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Arun Kumar Nagaraj","key":null,"page":null}],"frontMatter":{"title":"Exploring various storage types in Apache Hudi","excerpt":"Hudi Storage Format Overview","author":"Arun Kumar Nagaraj","category":"blog","image":"/assets/images/blog/2023-08-22-Exploring-various-storage-types-in-Apache-Hudi.png","tags":["blog","apache hudi","storage types","medium"]},"unlisted":false,"prevItem":{"title":"Delta, Hudi, Iceberg \u2014 Which is most popular?","permalink":"/blog/2023/08/25/Delta-Hudi-Iceberg-Which-is-most-popular"},"nextItem":{"title":"Lakehouse Trifecta \u2014 Delta Lake, Apache Iceberg & Apache Hudi","permalink":"/blog/2023/08/09/Lakehouse-Trifecta-Delta-Lake-Apache-Iceberg-and-Apache-Hudi"}}')},63680:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/06/16/Apache-Hudi-does-XYZ-110","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-06-16-Apache-Hudi-does-XYZ-110.mdx","source":"@site/blog/2025-06-16-Apache-Hudi-does-XYZ-110.mdx","title":"Apache Hudi does XYZ (1/10): File pruning with multi-modal index","description":"Redirecting... please wait!!","date":"2025-06-16T00:00:00.000Z","tags":[{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"spark","permalink":"/blog/tags/spark"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"course","permalink":"/blog/tags/course"},{"inline":true,"label":"tutorial","permalink":"/blog/tags/tutorial"},{"inline":true,"label":"datumagic","permalink":"/blog/tags/datumagic"},{"inline":true,"label":"data lake","permalink":"/blog/tags/data-lake"},{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"apache spark","permalink":"/blog/tags/apache-spark"}],"readingTime":0.11,"hasTruncateMarker":false,"authors":[{"name":"Shiyan Xu","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi does XYZ (1/10): File pruning with multi-modal index","excerpt":"File pruning with multi-modal index","author":"Shiyan Xu","category":"blog","image":"/assets/images/blog/2025-06-16-Apache-Hudi-does-XYZ-110-cover.jpg","tags":["hudi","spark","blog","course","tutorial","datumagic","data lake","lakehouse","apache hudi","apache spark"]},"unlisted":false,"prevItem":{"title":"Scaling Complex Data Workflows at Uber Using Apache Hudi","permalink":"/blog/2025/06/30/uber-hudi"},"nextItem":{"title":"Optimizing Apache Hudi Workflows: Automation for Clustering, Resizing & Concurrency","permalink":"/blog/2025/06/13/Optimizing-Apache-Hudi-Workflows-Automation-for-Clustering-Resizing-Concurrency"}}')},63928:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/03/24/Zendesk-Insights-for-CTOs-Part-3-Growing-your-business-with-modern-data-capabilities","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-03-24-Zendesk-Insights-for-CTOs-Part-3-Growing-your-business-with-modern-data-capabilities.mdx","source":"@site/blog/2022-03-24-Zendesk-Insights-for-CTOs-Part-3-Growing-your-business-with-modern-data-capabilities.mdx","title":"Zendesk - Insights for CTOs: Part 3 \u2013 Growing your business with modern data capabilities","description":"Redirecting... please wait!!","date":"2022-03-24T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"modern data architecture","permalink":"/blog/tags/modern-data-architecture"},{"inline":true,"label":"near real-time analytics","permalink":"/blog/tags/near-real-time-analytics"},{"inline":true,"label":"gdpr deletion","permalink":"/blog/tags/gdpr-deletion"},{"inline":true,"label":"streaming ingestion","permalink":"/blog/tags/streaming-ingestion"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Syed Jaffry","socials":{},"key":null,"page":null},{"name":"Johnathan Hwang","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Zendesk - Insights for CTOs: Part 3 \u2013 Growing your business with modern data capabilities","authors":[{"name":"Syed Jaffry"},{"name":"Johnathan Hwang"}],"category":"blog","image":"/assets/images/blog/2022-03-24-insights-for-ctos-part-3.png","tags":["blog","modern data architecture","near real-time analytics","gdpr deletion","streaming ingestion","amazon"]},"unlisted":false,"prevItem":{"title":"New features from Apache Hudi 0.9.0 on Amazon EMR","permalink":"/blog/2022/04/04/New-features-from-Apache-Hudi-0.9.0-on-Amazon-EMR"},"nextItem":{"title":"Build a serverless pipeline to analyze streaming data using AWS Glue, Apache Hudi, and Amazon S3","permalink":"/blog/2022/03/09/Build-a-serverless-pipeline-to-analyze-streaming-data-using-AWS-Glue-Apache-Hudi-and-Amazon-S3"}}')},64035:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/01/08/the-future-of-data-lakehouses-a-fireside","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-01-08-the-future-of-data-lakehouses-a-fireside.mdx","source":"@site/blog/2025-01-08-the-future-of-data-lakehouses-a-fireside.mdx","title":"The Future of Data Lakehouses: A Fireside Chat with Vinoth Chandar - Founder CEO Onehouse & PMC Chair of Apache Hudi","description":"Redirecting... please wait!!","date":"2025-01-08T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"data lakehouse","permalink":"/blog/tags/data-lakehouse"},{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"dataengineeringweekly","permalink":"/blog/tags/dataengineeringweekly"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Ananth Packkildurai","key":null,"page":null}],"frontMatter":{"title":"The Future of Data Lakehouses: A Fireside Chat with Vinoth Chandar - Founder CEO Onehouse & PMC Chair of Apache Hudi","author":"Ananth Packkildurai","category":"blog","image":"/assets/images/blog/2025-01-08-the-future-of-data-lakehouses-a-fireside.jpg","tags":["blog","apache hudi","data lakehouse","lakehouse","dataengineeringweekly"]},"unlisted":false,"prevItem":{"title":"Apache Iceberg vs Delta Lake vs Apache Hudi","permalink":"/blog/2025/01/09/apache-iceberg-vs-delta-lake-vs-apache-hudi"},"nextItem":{"title":"How to Use the New Hudi Streamer with Hudi 1.0.0 on EMR Serverless 7.5.0 | Hands-on Labs","permalink":"/blog/2025/01/05/how-use-new-hudi-streamer-100-emr-serverless-750-hands-on"}}')},64102:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(13135),n=t(74848),s=t(28453),r=t(9230);const o={title:"Options on Kafka sink to open table Formats: Apache Iceberg and Apache Hudi",author:"Albert Wong",category:"blog",image:"/assets/images/blog/2024-03-23-options-on-kafka-sink-to-open-table-formats-apache-iceberg-and-apache-hudi.png",tags:["blog","apache hudi","apache iceberg","apache Kafka","kafka connect","starrocks","devgenius"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blog.devgenius.io/options-on-kafka-sink-to-open-table-formats-apache-iceberg-and-apache-hudi-f6839ddad978",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},64113:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/03/20/Introducing-native-support-for-Apache Hudi-Delta-Lake-and-Apache-Iceberg-on-AWS-Glue-for-Apache-Spark-Part-2-AWS-Glue-Studio-Visual-Editor","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-03-20-Introducing-native-support-for-Apache Hudi-Delta-Lake-and-Apache-Iceberg-on-AWS-Glue-for-Apache-Spark-Part-2-AWS-Glue-Studio-Visual-Editor.mdx","source":"@site/blog/2023-03-20-Introducing-native-support-for-Apache Hudi-Delta-Lake-and-Apache-Iceberg-on-AWS-Glue-for-Apache-Spark-Part-2-AWS-Glue-Studio-Visual-Editor.mdx","title":"Introducing native support for Apache Hudi, Delta Lake, and Apache Iceberg on AWS Glue for Apache Spark, Part 2: AWS Glue Studio Visual Editor","description":"Redirecting... please wait!!","date":"2023-03-20T00:00:00.000Z","tags":[{"inline":true,"label":"aws glue","permalink":"/blog/tags/aws-glue"},{"inline":true,"label":"glue studio","permalink":"/blog/tags/glue-studio"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.21,"hasTruncateMarker":false,"authors":[{"name":"Noritaka Sekiyama","socials":{},"key":null,"page":null},{"name":"Scott Long","socials":{},"key":null,"page":null},{"name":"Sean Ma","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Introducing native support for Apache Hudi, Delta Lake, and Apache Iceberg on AWS Glue for Apache Spark, Part 2: AWS Glue Studio Visual Editor","authors":[{"name":"Noritaka Sekiyama"},{"name":"Scott Long"},{"name":"Sean Ma"}],"category":"blog","image":"/assets/images/blog/native-support-hudi-for-glue-studio.png","tags":["aws glue","glue studio","blog","amazon"]},"unlisted":false,"prevItem":{"title":"Spark ETL Chapter 8 with Lakehouse | Apache HUDI","permalink":"/blog/2023/03/23/Spark-ETL-Chapter-8-with-Lakehouse-Apache-HUDI"},"nextItem":{"title":"Introduction to Apache Hudi","permalink":"/blog/2023/03/17/introduction-to-apache-hudi"}}')},64175:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(23176),n=t(74848),s=t(28453),r=t(9230);const o={title:"In-House Data Lake with CDC Processing, Hudi, Docker",excerpt:"In-House Data Lake with CDC Processing, Hudi, Docker",author:"Rahul",category:"blog",image:"/assets/images/blog/2024-01-11-In-House-Data-Lake-with-CDC-Processing-Hudi-Docker.png",tags:["blog","apache hudi","medium","intermediate","docker","cdc","apache kafka","debezium","apache spark","aws s3"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@rhlkmr089/in-house-data-lake-with-cdc-processing-hudi-docker-878cee483ca0",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},64236:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/05/10/building-analytical-apps-on-the-lakehouse-using-apache-hudi-daft-streamlit","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-05-10-building-analytical-apps-on-the-lakehouse-using-apache-hudi-daft-streamlit.mdx","source":"@site/blog/2024-05-10-building-analytical-apps-on-the-lakehouse-using-apache-hudi-daft-streamlit.mdx","title":"Building Analytical Apps on the Lakehouse using Apache Hudi, Daft & Streamlit","description":"Redirecting... please wait!!","date":"2024-05-10T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"python","permalink":"/blog/tags/python"},{"inline":true,"label":"daft","permalink":"/blog/tags/daft"},{"inline":true,"label":"streamlit","permalink":"/blog/tags/streamlit"},{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"apache-hudi-blogs","permalink":"/blog/tags/apache-hudi-blogs"}],"readingTime":0.15,"hasTruncateMarker":false,"authors":[{"name":"Dipankar Mazumdar","key":null,"page":null}],"frontMatter":{"title":"Building Analytical Apps on the Lakehouse using Apache Hudi, Daft & Streamlit","author":"Dipankar Mazumdar","category":"blog","image":"/assets/images/blog/2024-05-10-building-analytical-apps-on-the-lakehouse-using-apache-hudi-daft-streamlit.png","tags":["blog","apache hudi","python","daft","streamlit","lakehouse","apache-hudi-blogs"]},"unlisted":false,"prevItem":{"title":"Apache Hudi on AWS Glue","permalink":"/blog/2024/05/19/apache-hudi-on-aws-glue"},"nextItem":{"title":"Learn how to read Hudi data with AWS Glue Ray using Daft (No Spark)","permalink":"/blog/2024/05/07/learn-how-read-hudi-data-aws-glue-ray-using-daft-spark"}}')},64239:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(18949),n=t(74848),s=t(28453);const r={title:"Real-Time Cloud Security Graphs with Apache Hudi and PuppyGraph",excerpt:"Hudi tables support fast upserts and incremental processing. PuppyGraph queries relationships in place using openCypher or Gremlin. In this blog, we explore how to get started with real-time security graph analytics at scale using the data already stored in your Hudi lakehouse tables.",author:"Jaz Samantha Ku, in collaboration with Shiyan Xu",category:"blog",image:"/assets/images/blog/2025-10-02-Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph/fig-4-Sample-Architecture-of-PuppyGraph-Hudi.png",tags:["Apache Hudi","PuppyGraph","security"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Why Apache Hudi for Cybersecurity Data?",id:"why-apache-hudi-for-cybersecurity-data",level:2},{value:"Why PuppyGraph for Cybersecurity Data?",id:"why-puppygraph-for-cybersecurity-data",level:2},{value:"Traditional Analytics on the Lake",id:"traditional-analytics-on-the-lake",level:3},{value:"Dedicated Graph Databases",id:"dedicated-graph-databases",level:3},{value:"How PuppyGraph Helps",id:"how-puppygraph-helps",level:3},{value:"Real-World Use Case",id:"real-world-use-case",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Data Preparation",id:"data-preparation",level:4},{value:"Loading Data",id:"loading-data",level:4},{value:"Modeling the Graph",id:"modeling-the-graph",level:4},{value:"Sample Queries",id:"sample-queries",level:3},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const a={a:"a",code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.a,{href:"https://www.crowdstrike.com/en-us/global-threat-report/",children:"CrowdStrike\u2019s 2025 Global Threat Report"})," puts average eCrime breakout time at 48 minutes, with the fastest at 51 seconds. This means that by the time security teams are even alerted about the potential breach, attackers have already long infiltrated the system. And that\u2019s assuming they even get alerted. Cloud environments generate massive amounts of access logs, configuration changes, alerts, and telemetry. Reviewing these events in isolation rarely surfaces patterns like lateral movement or privilege escalation."]}),"\n",(0,n.jsx)(a.p,{children:"Security tools such as SIEM, CSPM, and cloud workload protection need relationship-based analysis. It is not only a login attempt or a policy change, but also who acted, which systems were touched, what privileges were active, and what happened next. Event-centric methods struggle to answer those questions at scale. Graph analysis fits better because it captures paths and context across entities."}),"\n",(0,n.jsx)(a.p,{children:"To keep up, the data pipeline must support:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Continuous upserts with low lag so detections run on the latest state"}),"\n",(0,n.jsx)(a.li,{children:"Incremental consumption so analytics read only \u201cwhat changed since T\u201d"}),"\n",(0,n.jsx)(a.li,{children:"A rewindable timeline so responders can review state during investigations"}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"With Apache Hudi and PuppyGraph, this becomes straightforward. Hudi tables support fast upserts and incremental processing. PuppyGraph queries relationships in place using openCypher or Gremlin. In this blog, we explore how to get started with real-time security graph analytics at scale using the data already stored in your Hudi lakehouse tables."}),"\n",(0,n.jsx)(a.h2,{id:"why-apache-hudi-for-cybersecurity-data",children:"Why Apache Hudi for Cybersecurity Data?"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.a,{href:"https://hudi.apache.org/",children:"Apache Hudi"})," is an open data lakehouse platform that brings ACID transaction guarantees to data lakes. It enables efficient, record-level updates and deletes on massive datasets, which makes it a strong foundation for storing and analyzing cybersecurity data such as logs, telemetry, and threat intelligence. Its combination of performance, flexibility, and broad ecosystem integration is well-suited for threat detection, forensic investigation, and compliance work."]}),"\n",(0,n.jsxs)(a.p,{children:["Hudi speeds up large-scale security analytics through features that keep tables both current and query-efficient. Hudi writers excel at handling continuous, mutable workloads without requiring costly full rewrites. Hudi\u2019s ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/metadata",children:"multi-modal indexing subsystem"}),", backed by its internal metadata table, offers efficient lookups and data skipping, dramatically accelerating queries that scan massive log sets to isolate suspicious activity. Hudi keeps tables updatable and queryable as they change, with time-travel and incremental reads for point-in-time forensic analysis."]}),"\n",(0,n.jsxs)(a.p,{children:["Even as data volumes grow, operations remain manageable. Hudi tracks every commit on a timeline, enabling powerful time-travel queries for historical investigations. Asynchronous table services like ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/compaction",children:"compaction"}),", ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/clustering",children:"clustering"}),", ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/cleaning",children:"cleaning"}),", and ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/metadata_indexing",children:"indexing"})," run in the background to maintain peak performance and storage health while minimizing disruption to ingestion pipelines. Furthermore, its consistent commit and delete semantics support the creation of reliable audit trails, simplify data retention policies, and help meet privacy requirements."]}),"\n",(0,n.jsx)("figure",{children:(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{src:t(84548).A+"",width:"1333",height:"978"}),"\n",(0,n.jsxs)("figcaption",{children:["The ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/hudi_stack",children:"Apache Hudi Stack"})]})]})}),"\n",(0,n.jsx)(a.p,{children:"Hudi also integrates seamlessly with the tools security teams already use. You can stream data from Apache Kafka or Debezium CDC into Hudi, register tables in Hive Metastore or AWS Glue Catalog, and query them from popular query engines like Apache Spark, Apache Flink, Presto, Trino, or Amazon Athena. PuppyGraph connects to the same Hudi tables and runs openCypher or Gremlin queries directly on them via the user access layer, so you get real-time graph analytics on the lake with no ETL and no data duplication."}),"\n",(0,n.jsx)(a.h2,{id:"why-puppygraph-for-cybersecurity-data",children:"Why PuppyGraph for Cybersecurity Data?"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.a,{href:"https://puppygraph.com/",children:"PuppyGraph"})," is the first real-time, zero-ETL graph query engine. It lets data teams query existing relational stores as a single graph and get up and running in under 10 minutes, avoiding the cost, latency, and maintenance of a separate graph database. To understand why this is so important, let\u2019s take a look at the status quo."]}),"\n",(0,n.jsx)(a.h3,{id:"traditional-analytics-on-the-lake",children:"Traditional Analytics on the Lake"}),"\n",(0,n.jsx)(a.p,{children:"Security teams already store logs, configs, and alerts in a lakehouse. SQL engines are great for counts, filters, rollups, and point lookups. They struggle when questions depend on relationships. Lateral movement, privilege escalation, and blast radius span many tables and time windows. Each new join adds complexity, pushes latency up, and breaks easily when schemas evolve or events arrive late. You can stitch context with views and pipelines, but it is fragile and slow to adapt."}),"\n",(0,n.jsx)(a.h3,{id:"dedicated-graph-databases",children:"Dedicated Graph Databases"}),"\n",(0,n.jsx)(a.p,{children:"Graphs make paths and neighborhoods first class. Graph queries let you answer \u201cwhat connects to what\u201d in a way that makes sense, without the need for confusing data joins. The tradeoff is operations and freshness. Most graph databases want their own storage. That means ETL, a second copy, and lag between source and graph. Continuous upserts are heavy because every change can touch nodes, edges, and multiple indexes. Running a separate cluster adds backups, upgrades, sizing, and vendor-specific tuning. During an incident, that overhead shows up as stale data and slower investigations."}),"\n",(0,n.jsx)(a.h3,{id:"how-puppygraph-helps",children:"How PuppyGraph Helps"}),"\n",(0,n.jsx)(a.p,{children:'PuppyGraph is not a traditional graph database but a graph query engine designed to run directly on top of your existing data infrastructure without costly and complex ETL (Extract, Transform, Load) processes. This "zero-ETL" approach is its core differentiator, allowing you to query relational data in data warehouses, data lakes, and databases as a unified graph model in minutes.'}),"\n",(0,n.jsxs)(a.p,{children:["Instead of migrating data into a specialized store, PuppyGraph connects to sources including ",(0,n.jsx)(a.a,{href:"https://www.puppygraph.com/blog/postgresql-graph-database",children:"PostgreSQL"}),", ",(0,n.jsx)(a.a,{href:"https://docs.puppygraph.com/connecting/connecting-to-iceberg/?h=ice",children:"Apache Iceberg"}),", ",(0,n.jsx)(a.a,{href:"https://docs.puppygraph.com/connecting/connecting-to-apache-hudi/",children:"Apache Hudi"}),", ",(0,n.jsx)(a.a,{href:"https://docs.puppygraph.com/connecting/connecting-to-bigquery/?h=bigq",children:"BigQuery"}),", and others, then builds a virtual graph layer over them. Graph models are defined through simple JSON schema files, making it easy to update, version, or switch graph views without touching the underlying data. From there, you can quickly begin exploring your data with graph queries written in Gremlin or openCypher."]}),"\n",(0,n.jsx)("figure",{children:(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{src:t(52977).A+"",width:"1313",height:"745"}),"\n",(0,n.jsx)("figcaption",{children:"PuppyGraph Supported Data Sources"})]})}),"\n",(0,n.jsx)("figure",{children:(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{src:t(22188).A+"",width:"1497",height:"843"}),"\n",(0,n.jsx)("figcaption",{children:"Architecture with Graph Database vs. with PuppyGraph"})]})}),"\n",(0,n.jsx)(a.p,{children:"This approach aligns with the broader shift in modern data stacks to separate compute from storage. You keep data where it belongs and scale query power independently, which supports petabyte-level workloads without duplicating data or managing fragile pipelines."}),"\n",(0,n.jsx)(a.h2,{id:"real-world-use-case",children:"Real-World Use Case"}),"\n",(0,n.jsx)(a.p,{children:"We have shown why cloud security benefits from a relationship-first view of identities, resources, and events. In this demo, we\u2019ll show how easy it is to begin querying your cloud security data as a graph. Apache Hudi keeps those tables current with streaming upserts and an investigation-friendly timeline. PuppyGraph lets you query your existing lake tables as a graph. Together they give you real-time security graph analytics on the data you already store."}),"\n",(0,n.jsx)(a.p,{children:"Getting started is straightforward. You will deploy the stack, load security data into Hudi, connect PuppyGraph to your catalog, define a graph view, and run a few queries. All in a matter of minutes."}),"\n",(0,n.jsx)("figure",{children:(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{src:t(12274).A+"",width:"858",height:"1100"}),"\n",(0,n.jsx)("figcaption",{children:"Sample Architecture of PuppyGraph + Hudi"})]})}),"\n",(0,n.jsx)(a.p,{children:"The components of this demo project include:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Storage: MinIO/S3 \u2013 Object store for Hudi data"}),"\n",(0,n.jsx)(a.li,{children:"Data Lakehouse: Apache Hudi \u2013 Brings database functionality to your data lakes"}),"\n",(0,n.jsx)(a.li,{children:"Catalog: Hive Metastore \u2013 Backed by Postgres"}),"\n",(0,n.jsxs)(a.li,{children:["Compute engines:","\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Spark \u2013 Initial table writes"}),"\n",(0,n.jsx)(a.li,{children:"PuppyGraph \u2013 Graph query engine for complex, multi-hop graph queries"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,n.jsx)(a.p,{children:"This tutorial assumes that you have the following:"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Docker"})," and ",(0,n.jsx)(a.strong,{children:"Docker"})," ",(0,n.jsx)(a.strong,{children:"Compose"})," (for setting up the Docker container)"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Python 3"})," (for managing dependencies)"]}),"\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.a,{href:"https://github.com/puppygraph/puppygraph-getting-started/tree/main/integration-demos/hudi-demo",children:"PuppyGraph-Hudi Demo Repository"})}),"\n"]}),"\n",(0,n.jsx)(a.h4,{id:"data-preparation",children:"Data Preparation"}),"\n",(0,n.jsx)(a.p,{children:"Before we can load our data into our Hudi tables, we need to make sure they\u2019re in the correct file format. Hudi currently supports Parquet and ORC for base files, and we\u2019ll be going with Parquet for this demo:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-shell",children:"python3 -m venv demo\nsource demo/bin/activate\npip install -r requirements.txt\npython3 CsvToParquet.py ./csv_data ./parquet_data\n"})}),"\n",(0,n.jsx)(a.p,{children:"Since we\u2019ll be connecting to the Hudi Catalog via the Hive Metastore (HMS), we also have to install the following dependencies:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-shell",children:"mkdir -p lib\ncurl -L -o lib/postgresql-42.5.1.jar \\\nhttps://repo1.maven.org/maven2/org/postgresql/postgresql/42.5.1/postgresql-42.5.1.jar\n\ncurl -L -o lib/hadoop-aws-3.3.4.jar \\\nhttps://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar\n\ncurl -L -o lib/aws-java-sdk-bundle-1.12.262.jar \\\nhttps://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar\n"})}),"\n",(0,n.jsx)(a.h4,{id:"loading-data",children:"Loading Data"}),"\n",(0,n.jsx)(a.p,{children:"With all our dependencies installed and data prepared, we can launch the required services:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-shell",children:"docker compose up -d\n"})}),"\n",(0,n.jsx)(a.p,{children:"Once everything is up and running, we can finally populate the tables with our data:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-shell",children:"docker compose exec spark /opt/spark/bin/spark-sql -f /init.sql\n"})}),"\n",(0,n.jsx)(a.h4,{id:"modeling-the-graph",children:"Modeling the Graph"}),"\n",(0,n.jsxs)(a.p,{children:["Now that our data is loaded in, we can log into the PuppyGraph Web UI at ",(0,n.jsx)(a.a,{href:"http://localhost:8081",children:"http://localhost:8081"})," with the default credentials (username: puppygraph, password: puppygraph123)"]}),"\n",(0,n.jsx)("figure",{children:(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{src:t(37815).A+"",width:"3024",height:"1710"}),"\n",(0,n.jsx)("figcaption",{children:"PuppyGraph Login Page"})]})}),"\n",(0,n.jsx)(a.p,{children:"To model your data as a graph, you can simply select the file `schema.json` in the Upload Graph Schema JSON section and click on Upload."}),"\n",(0,n.jsx)("figure",{children:(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{src:t(23155).A+"",width:"1600",height:"907"}),"\n",(0,n.jsx)("figcaption",{children:"Schema Page in PuppyGraph UI"})]})}),"\n",(0,n.jsx)(a.p,{children:"Once you see your graph schema loaded in, you\u2019re ready to start querying your data as a graph."}),"\n",(0,n.jsx)("figure",{children:(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{src:t(72222).A+"",width:"1600",height:"907"}),"\n",(0,n.jsx)("figcaption",{children:"Loaded Schema in PuppyGraph UI"})]})}),"\n",(0,n.jsx)(a.h3,{id:"sample-queries",children:"Sample Queries"}),"\n",(0,n.jsx)(a.p,{children:"By modeling the network infrastructure as a graph, users can identify potential security risks, such as:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Public IP addresses exposed to the internet"}),"\n",(0,n.jsx)(a.li,{children:"Network interfaces not protected by any security group"}),"\n",(0,n.jsx)(a.li,{children:"Roles granted excessive access permissions"}),"\n",(0,n.jsx)(a.li,{children:"Security groups with overly permissive ingress rules"}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Listed below are some sample queries you can try running to explore the data:"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsx)(a.li,{children:"Tracing Admin Access Paths from Users to Internet Gateways"}),"\n"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-javascript",children:"g.V().hasLabel('User').as('user')\n .outE('ACCESS').has('access_level', 'admin').as('edge')\n .inV()\n .path()\n"})}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{src:t(38681).A+"",width:"1600",height:"907"})}),"\n",(0,n.jsxs)(a.ol,{start:"2",children:["\n",(0,n.jsx)(a.li,{children:"Find all public IP addresses exposed to the internet, along with their associated virtual machine instances, security groups, subnets, VPCs, internet gateways, and users, displaying all these entities in the traversal path."}),"\n"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-javascript",children:" g.V().hasLabel('PublicIP').as('ip')\n .in('HAS_PUBLIC_IP').as('ni')\n .in('PROTECTS').hasLabel('SecurityGroup').as('sg')\n .out('HAS_RULE').hasLabel('IngressRule').as('rule')\n .where(\n   __.out('ALLOWS_TRAFFIC_FROM').hasLabel('InternetGateway')\n )\n .select('ni')\n   .out('ATTACHED_TO').hasLabel('VMInstance').as('vm')\n .select('ni')\n   .in('HOSTS_INTERFACE').hasLabel('Subnet').as('subnet')\n   .in('CONTAINS').hasLabel('VPC').as('vpc')\n   .in('GATEWAY_TO').hasLabel('InternetGateway').as('igw')\n   .in('ACCESS').hasLabel('User').as('user')\n .path()\n .limit(1000)\n"})}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{src:t(82601).A+"",width:"1600",height:"907"})}),"\n",(0,n.jsxs)(a.ol,{start:"3",children:["\n",(0,n.jsx)(a.li,{children:"Find roles that have been granted excessive access permissions, along with their associated virtual machine instances."}),"\n"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-javascript",children:"g.V().hasLabel('Role').as('role')\n.where(\n  __.out('ALLOWS_ACCESS_TO').count().is(gt(4L))\n)\n.out('ALLOWS_ACCESS_TO').hasLabel('Resource').as('resource')\n.select('role')\n.in('ASSIGNED_ROLE').hasLabel('VMInstance').as('vm')\n.path()\n"})}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{src:t(4226).A+"",width:"1600",height:"907"})}),"\n",(0,n.jsxs)(a.ol,{start:"4",children:["\n",(0,n.jsx)(a.li,{children:"Find security groups that have ingress rules permitting traffic from any IP address (0.0.0.0/0) to sensitive ports (22 or 3389), and retrieve the associated ingress rules, network interfaces, and virtual machine instances in the traversal path."}),"\n"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-javascript",children:"g.V().hasLabel('SecurityGroup').as('sg')\n .out('HAS_RULE')\n   .has('source', '0.0.0.0/0')\n   .has('port_range', P.within('22', '3389'))\n   .hasLabel('IngressRule').as('rule')\n .in('HAS_RULE').as('sg')\n .out('PROTECTS').hasLabel('NetworkInterface').as('ni')\n .out('ATTACHED_TO').hasLabel('VMInstance').as('vm')\n .path()\n"})}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{src:t(16738).A+"",width:"1600",height:"907"})}),"\n",(0,n.jsx)(a.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,n.jsx)(a.p,{children:"Real-time security work comes down to two needs: fresh tables and connected questions. Apache Hudi keeps lakehouse data current with streaming upserts, incremental reads, and a rewindable timeline. PuppyGraph reads those same tables in place and runs multi-hop graph queries with openCypher or Gremlin. One data copy. No ETL."}),"\n",(0,n.jsx)(a.p,{children:"The result is faster investigations and clearer decisions. You can trace attack paths, size blast radius, and correlate alerts to recent changes while keeping governance and access controls in a single lake. When you need to look back, time travel gives you point-in-time views without rebuilding pipelines."})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},64341:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/12/16/announcing-hudi-1-0-0","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-12-16-announcing-hudi-1-0-0.mdx","source":"@site/blog/2024-12-16-announcing-hudi-1-0-0.mdx","title":"Announcing Apache Hudi 1.0 and the Next Generation of Data Lakehouses","description":"Overview","date":"2024-12-16T00:00:00.000Z","tags":[{"inline":true,"label":"timeline","permalink":"/blog/tags/timeline"},{"inline":true,"label":"design","permalink":"/blog/tags/design"},{"inline":true,"label":"release","permalink":"/blog/tags/release"},{"inline":true,"label":"streaming ingestion","permalink":"/blog/tags/streaming-ingestion"},{"inline":true,"label":"multi-writer","permalink":"/blog/tags/multi-writer"},{"inline":true,"label":"concurrency-control","permalink":"/blog/tags/concurrency-control"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"}],"readingTime":61.49,"hasTruncateMarker":false,"authors":[{"name":"Vinoth Chandar","key":null,"page":null}],"frontMatter":{"title":"Announcing Apache Hudi 1.0 and the Next Generation of Data Lakehouses","excerpt":"game-changing major release, that reimagines Hudi and Data Lakehouses.","author":"Vinoth Chandar","category":"blog","image":"/assets/images/blog/dlms-hierarchy.png","tags":["timeline","design","release","streaming ingestion","multi-writer","concurrency-control","blog"]},"unlisted":false,"prevItem":{"title":"How lakehouse handles concurrent Read and Writes","permalink":"/blog/2024/12/28/how-lakehouse-handles-concurrent-read-and-writes"},"nextItem":{"title":"Introducing Hudi\'s Non-blocking Concurrency Control for streaming, high-frequency writes","permalink":"/blog/2024/12/06/non-blocking-concurrency-control"}}')},64343:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/10/29/UPSERT-Performance-Evaluation-of-Hudi-0-14-and-Spark-3-4-1-Record-Level-Index-Global-Bloom-Global-Simple-Indexes","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-10-29-UPSERT-Performance-Evaluation-of-Hudi-0-14-and-Spark-3-4-1-Record-Level-Index-Global-Bloom-Global-Simple-Indexes.mdx","source":"@site/blog/2023-10-29-UPSERT-Performance-Evaluation-of-Hudi-0-14-and-Spark-3-4-1-Record-Level-Index-Global-Bloom-Global-Simple-Indexes.mdx","title":"UPSERT Performance Evaluation of Hudi 0.14 and Spark 3.4.1: Record Level Index vs. Global Bloom & Global Simple Indexes","description":"Redirecting... please wait!!","date":"2023-10-29T00:00:00.000Z","tags":[{"inline":true,"label":"linkedin","permalink":"/blog/tags/linkedin"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"querying","permalink":"/blog/tags/querying"},{"inline":true,"label":"indexing","permalink":"/blog/tags/indexing"},{"inline":true,"label":"performance","permalink":"/blog/tags/performance"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"Soumil Shah","key":null,"page":null}],"frontMatter":{"title":"UPSERT Performance Evaluation of Hudi 0.14 and Spark 3.4.1: Record Level Index vs. Global Bloom & Global Simple Indexes","excerpt":"Record Level Index Performance","author":"Soumil Shah","category":"blog","image":"/assets/images/blog/2023-10-29-UPSERT-Performance-Evaluation-of-Hudi-0-14-and-Spark-3-4-1-Record-Level-Index-Global-Bloom-Global-Simple-Indexes.png","tags":["linkedin","apache hudi","querying","indexing","performance"]},"unlisted":false,"prevItem":{"title":"Record Level Index: Hudi\'s blazing fast indexing for large-scale datasets","permalink":"/blog/2023/11/01/record-level-index"},"nextItem":{"title":"Tipico Facilitates Faster Data Access with a Modern Data Strategy on AWS","permalink":"/blog/2023/10/22/Tipico-Facilitates-Faster-Data-Access-with-a-Modern-Data-Strategy-on-AWS"}}')},64456:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/04/24/understanding-apache-hudi-consistency-model-part-1","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-04-24-understanding-apache-hudi-consistency-model-part-1.mdx","source":"@site/blog/2024-04-24-understanding-apache-hudi-consistency-model-part-1.mdx","title":"Understanding Apache Hudi\'s Consistency Model Part 1","description":"Redirecting... please wait!!","date":"2024-04-24T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"table formats","permalink":"/blog/tags/table-formats"},{"inline":true,"label":"ACID","permalink":"/blog/tags/acid"},{"inline":true,"label":"consistency","permalink":"/blog/tags/consistency"},{"inline":true,"label":"cow","permalink":"/blog/tags/cow"},{"inline":true,"label":"concurrency control","permalink":"/blog/tags/concurrency-control"},{"inline":true,"label":"multi writer","permalink":"/blog/tags/multi-writer"},{"inline":true,"label":"tla+ specification","permalink":"/blog/tags/tla-specification"},{"inline":true,"label":"jack-vanlightly","permalink":"/blog/tags/jack-vanlightly"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Jack Vanlightly","key":null,"page":null}],"frontMatter":{"title":"Understanding Apache Hudi\'s Consistency Model Part 1","author":"Jack Vanlightly","category":"blog","image":"/assets/images/blog/2024-04-24-understanding-apache-hudi-consistency-model-part-1.png","tags":["blog","apache hudi","table formats","ACID","consistency","cow","concurrency control","multi writer","tla+ specification","jack-vanlightly"]},"unlisted":false,"prevItem":{"title":"Apache Hudi vs Apache Iceberg: A Comprehensive Comparison","permalink":"/blog/2024/04/25/apache-hudi-vs-apache-iceberg-a-comprehensive-comparison"},"nextItem":{"title":"Understanding Apache Hudi\'s Consistency Model Part 2","permalink":"/blog/2024/04/24/understanding-apache-hudi-consistency-model-part-2"}}')},64878:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(20828),n=t(74848),s=t(28453),r=t(9230);const o={title:"How Zoom implemented streaming log ingestion and efficient GDPR deletes using Apache Hudi on Amazon EMR",authors:[{name:"Sekar Srinivasan"},{name:"Amit Kumar Agrawal"},{name:"Chandra Dhandapani"},{name:"Viral Shah"}],category:"blog",image:"/assets/images/blog/2023-05-16-how-zoom-implemented-streaming-log-ingestion-and-efficient-gdpr-deletes-using-apache-hudi-on-amazon-emr.png",tags:["use-case","streaming ingestion","gdpr deletion","deletes","amazon"]},l=void 0,d={authorsImageUrls:[void 0,void 0,void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/how-zoom-implemented-streaming-log-ingestion-and-efficient-gdpr-deletes-using-apache-hudi-on-amazon-emr/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},64933:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/05/19/hudi-metafields-demystified","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-05-19-hudi-metafields-demystified.mdx","source":"@site/blog/2023-05-19-hudi-metafields-demystified.mdx","title":"Hudi Metafields demystified","description":"Redirecting... please wait!!","date":"2023-05-19T00:00:00.000Z","tags":[{"inline":true,"label":"design","permalink":"/blog/tags/design"},{"inline":true,"label":"metadata","permalink":"/blog/tags/metadata"},{"inline":true,"label":"metafields","permalink":"/blog/tags/metafields"},{"inline":true,"label":"onehouse","permalink":"/blog/tags/onehouse"}],"readingTime":0.1,"hasTruncateMarker":false,"authors":[{"name":"Bhavani Sudha Saktheeswaran","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Hudi Metafields demystified","authors":[{"name":"Bhavani Sudha Saktheeswaran"}],"category":"blog","image":"/assets/images/blog/2023-05-19-Hudi-Metafields-demystified.png","tags":["design","metadata","metafields","onehouse"]},"unlisted":false,"prevItem":{"title":"Different Query types with Apache Hudi","permalink":"/blog/2023/05/29/different-query-types-with-apache-hudi"},"nextItem":{"title":"How Zoom implemented streaming log ingestion and efficient GDPR deletes using Apache Hudi on Amazon EMR","permalink":"/blog/2023/05/16/how-zoom-implemented-streaming-log-ingestion-and-efficient-gdpr-deletes-using-apache-hudi-on-amazon-emr"}}')},65326:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/05/12/ingesting-data-to-apache-hudi-using-spark-sql","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-05-12-ingesting-data-to-apache-hudi-using-spark-sql.mdx","source":"@site/blog/2023-05-12-ingesting-data-to-apache-hudi-using-spark-sql.mdx","title":"Ingesting data to Apache Hudi using Spark sql","description":"Redirecting... please wait!!","date":"2023-05-12T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"spark-sql","permalink":"/blog/tags/spark-sql"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Sivabalan Narayanan","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Ingesting data to Apache Hudi using Spark sql","authors":[{"name":"Sivabalan Narayanan"}],"category":"blog","tags":["how-to","spark-sql","medium"]},"unlisted":false,"prevItem":{"title":"How Zoom implemented streaming log ingestion and efficient GDPR deletes using Apache Hudi on Amazon EMR","permalink":"/blog/2023/05/16/how-zoom-implemented-streaming-log-ingestion-and-efficient-gdpr-deletes-using-apache-hudi-on-amazon-emr"},"nextItem":{"title":"Top 3 Things You Can Do to Get Fast Upsert Performance in Apache Hudi","permalink":"/blog/2023/05/10/top-3-things-you-can-do-to-get-fast-upsert-performance-in-apache-hudi"}}')},65417:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/12/31/The-Art-of-Building-Open-Data-Lakes-with-Apache-Hudi-Kafka-Hive-and-Debezium","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-12-31-The-Art-of-Building-Open-Data-Lakes-with-Apache-Hudi-Kafka-Hive-and-Debezium.mdx","source":"@site/blog/2021-12-31-The-Art-of-Building-Open-Data-Lakes-with-Apache-Hudi-Kafka-Hive-and-Debezium.mdx","title":"The Art of Building Open Data Lakes with Apache Hudi, Kafka, Hive, and Debezium","description":"Redirecting... please wait!!","date":"2021-12-31T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"datalake","permalink":"/blog/tags/datalake"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.15,"hasTruncateMarker":false,"authors":[{"name":"Gary Stafford","socials":{},"key":null,"page":null}],"frontMatter":{"title":"The Art of Building Open Data Lakes with Apache Hudi, Kafka, Hive, and Debezium","authors":[{"name":"Gary Stafford"}],"category":"blog","image":"/assets/images/blog/2021-12-31-open-source-data-lakes-on-aws.png","tags":["how-to","datalake","medium"]},"unlisted":false,"prevItem":{"title":"Apache Hudi - 2021 a Year in Review","permalink":"/blog/2022/01/06/apache-hudi-2021-a-year-in-review"},"nextItem":{"title":"Hudi Z-Order and Hilbert Space Filling Curves","permalink":"/blog/2021/12/29/hudi-zorder-and-hilbert-space-filling-curves"}}')},65481:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/10/02/apache-hudi-spark-and-minio-hands-on-lab-in-docker","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-10-02-apache-hudi-spark-and-minio-hands-on-lab-in-docker.mdx","source":"@site/blog/2024-10-02-apache-hudi-spark-and-minio-hands-on-lab-in-docker.mdx","title":"Apache Hudi, Spark and Minio: Hands-on Lab in Docker","description":"Redirecting... please wait!!","date":"2024-10-02T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"Apache Spark","permalink":"/blog/tags/apache-spark"},{"inline":true,"label":"Minio","permalink":"/blog/tags/minio"},{"inline":true,"label":"docker","permalink":"/blog/tags/docker"},{"inline":true,"label":"devgenius","permalink":"/blog/tags/devgenius"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Sanjeet Shukla","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi, Spark and Minio: Hands-on Lab in Docker","author":"Sanjeet Shukla","category":"blog","image":"/assets/images/blog/2024-10-02-apache-hudi-spark-and-minio-hands-on-lab-in-docker.jpeg","tags":["how-to","Apache Hudi","Apache Spark","Minio","docker","devgenius"]},"unlisted":false,"prevItem":{"title":"Mastering Slowly Changing Dimensions with Apache Hudi & Spark SQL","permalink":"/blog/2024/10/07/mastering-slowly-changing-dimensions-with-apache-hudi-and-spark-sql"},"nextItem":{"title":"Change query support in Apache Hudi (0.15)","permalink":"/blog/2024/09/30/change-query-support-in-apache-hudi-0-15"}}')},65493:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide19-787cf360bbd5e0f4cdc06cbf6df16016.png"},65662:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/12/31/the-architects-guide-to-open-table-formats-and-object-storage","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-12-31-the-architects-guide-to-open-table-formats-and-object-storage.mdx","source":"@site/blog/2024-12-31-the-architects-guide-to-open-table-formats-and-object-storage.mdx","title":"The Architect\u2019s Guide to Open Table Formats and Object Storage","description":"Redirecting... please wait!!","date":"2024-12-31T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"apache iceberg","permalink":"/blog/tags/apache-iceberg"},{"inline":true,"label":"delta lake","permalink":"/blog/tags/delta-lake"},{"inline":true,"label":"data lakehouse","permalink":"/blog/tags/data-lakehouse"},{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"table formats","permalink":"/blog/tags/table-formats"},{"inline":true,"label":"thenewstack","permalink":"/blog/tags/thenewstack"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Brenna Buuck","key":null,"page":null}],"frontMatter":{"title":"The Architect\u2019s Guide to Open Table Formats and Object Storage","author":"Brenna Buuck","category":"blog","image":"/assets/images/blog/2025-01-08-the-future-of-data-lakehouses-a-fireside.jpg","tags":["blog","apache hudi","apache iceberg","delta lake","data lakehouse","lakehouse","table formats","thenewstack"]},"unlisted":false,"prevItem":{"title":"Indexing in Apache Hudi","permalink":"/blog/2024/12/31/indexing-in-apache-hudi"},"nextItem":{"title":"Apache Hudi 2024: A Year In Review","permalink":"/blog/2024/12/29/apache-hudi-2024-a-year-in-review"}}')},65833:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/05/03/lakehouse-at-fortune-1-scale","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-05-03-lakehouse-at-fortune-1-scale.mdx","source":"@site/blog/2023-05-03-lakehouse-at-fortune-1-scale.mdx","title":"Lakehouse at Fortune 1 Scale","description":"Redirecting... please wait!!","date":"2023-05-03T00:00:00.000Z","tags":[{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"comparison","permalink":"/blog/tags/comparison"},{"inline":true,"label":"performance","permalink":"/blog/tags/performance"},{"inline":true,"label":"walmartglobaltech","permalink":"/blog/tags/walmartglobaltech"}],"readingTime":0.11,"hasTruncateMarker":false,"authors":[{"name":"Samuel Guleff","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Lakehouse at Fortune 1 Scale","authors":[{"name":"Samuel Guleff"}],"category":"blog","image":"/assets/images/blog/2023-05-03-lakehouse-at-fortune-1-scale.jpeg","tags":["use-case","comparison","performance","walmartglobaltech"]},"unlisted":false,"prevItem":{"title":"Amazon Athena now supports Apache Hudi 0.12.2","permalink":"/blog/2023/05/09/amazon-athena-apache-hudi"},"nextItem":{"title":"An Introduction to the Hudi and Flink Integration","permalink":"/blog/2023/05/02/intro-to-hudi-and-flink"}}')},65938:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/09/11/comparing-apache-hudi-apache-iceberg-and-delta-lake","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-09-11-comparing-apache-hudi-apache-iceberg-and-delta-lake.mdx","source":"@site/blog/2024-09-11-comparing-apache-hudi-apache-iceberg-and-delta-lake.mdx","title":"Comparing Apache Hudi, Apache Iceberg, and Delta Lake","description":"Redirecting... please wait!!","date":"2024-09-11T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"apache iceberg","permalink":"/blog/tags/apache-iceberg"},{"inline":true,"label":"delta lake","permalink":"/blog/tags/delta-lake"},{"inline":true,"label":"comparison","permalink":"/blog/tags/comparison"},{"inline":true,"label":"cloudthat","permalink":"/blog/tags/cloudthat"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Vasanth Kumar R","key":null,"page":null}],"frontMatter":{"title":"Comparing Apache Hudi, Apache Iceberg, and Delta Lake","author":"Vasanth Kumar R","category":"blog","image":"/assets/images/blog/2024-09-11-comparing-apache-hudi-apache-iceberg-and-delta-lake.jpeg","tags":["blog","apache hudi","apache iceberg","delta lake","comparison","cloudthat"]},"unlisted":false,"prevItem":{"title":"Uber\u2019s Big Data Revolution: From MySQL to Hadoop and Beyond","permalink":"/blog/2024/09/14/Ubers-Big-Data-Revolution-From-MySQL-to-Hadoop-and-Beyond"},"nextItem":{"title":"Use Apache Hudi tables in Athena for Spark","permalink":"/blog/2024/09/09/use-apache-hudi-tables-in-athena-for-spark"}}')},66130:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide5-63a50a959a8ca1fd6371bafd1765bd0a.png"},66151:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image4-e0b30b97adaffc9f1a9cf2eb8f0a9c52.jpg"},66515:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2020/08/04/PrestoDB-and-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-08-04-PrestoDB-and-Apache-Hudi.mdx","source":"@site/blog/2020-08-04-PrestoDB-and-Apache-Hudi.mdx","title":"PrestoDB and Apache Hudi","description":"Redirecting... please wait!!","date":"2020-08-04T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"prestodb","permalink":"/blog/tags/prestodb"}],"readingTime":0.11,"hasTruncateMarker":false,"authors":[{"name":"Bhavani Sudha Saktheeswaran","socials":{},"key":null,"page":null},{"name":"Brandon Scheller","socials":{},"key":null,"page":null}],"frontMatter":{"title":"PrestoDB and Apache Hudi","authors":[{"name":"Bhavani Sudha Saktheeswaran"},{"name":"Brandon Scheller"}],"category":"blog","image":"/assets/images/blog/2020-08-04-PrestoDB-and-Apache-Hudi.png","tags":["blog","prestodb"]},"unlisted":false,"prevItem":{"title":"Incremental Processing on the Data Lake","permalink":"/blog/2020/08/18/hudi-incremental-processing-on-data-lakes"},"nextItem":{"title":"Apache Hudi grows cloud data lake maturity","permalink":"/blog/2020/06/16/Apache-Hudi-grows-cloud-data-lake-maturity"}}')},66524:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/10/23/Using-Apache-Hudi-with-Apache-Flink","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-10-23-Using-Apache-Hudi-with-Apache-Flink.mdx","source":"@site/blog/2024-10-23-Using-Apache-Hudi-with-Apache-Flink.mdx","title":"Using Apache Hudi with Apache Flink","description":"Redirecting... please wait!!","date":"2024-10-23T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"apache flink","permalink":"/blog/tags/apache-flink"},{"inline":true,"label":"beginner","permalink":"/blog/tags/beginner"},{"inline":true,"label":"aws","permalink":"/blog/tags/aws"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"amazon","key":null,"page":null}],"frontMatter":{"title":"Using Apache Hudi with Apache Flink","author":"amazon","category":"blog","image":"/assets/images/blog/2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.png","tags":["blog","apache hudi","apache flink","beginner","aws","amazon"]},"unlisted":false,"prevItem":{"title":"Moving Large Tables from Snowflake to S3 Using the COPY INTO Command and Hudi Bootstrapping to Build Data Lakes | Hands-On Labs","permalink":"/blog/2024/10/26/moving-large-tables-from-snowflake-to-s3-using-the-copy-into-command-and-hudi"},"nextItem":{"title":"Mastering Open Table Formats: A Guide to Apache Iceberg, Hudi, and Delta Lake","permalink":"/blog/2024/10/23/mastering-open-table-formats-a-guide-to-apache-iceberg-hudi-and-delta-lake"}}')},66671:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/07/09/Hoodie-Timeline-Foundational-pillar-for-ACID-transactions","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-07-09-Hoodie-Timeline-Foundational-pillar-for-ACID-transactions.mdx","source":"@site/blog/2023-07-09-Hoodie-Timeline-Foundational-pillar-for-ACID-transactions.mdx","title":"Hoodie Timeline: Foundational pillar for ACID transactions","description":"Redirecting... please wait!!","date":"2023-07-09T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"ACID","permalink":"/blog/tags/acid"},{"inline":true,"label":"transactions","permalink":"/blog/tags/transactions"},{"inline":true,"label":"commits","permalink":"/blog/tags/commits"},{"inline":true,"label":"timeline","permalink":"/blog/tags/timeline"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Sivabalan Narayanan","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Hoodie Timeline: Foundational pillar for ACID transactions","authors":[{"name":"Sivabalan Narayanan"}],"category":"blog","image":"/assets/images/hudi_timeline.png","tags":["blog","ACID","transactions","commits","timeline","medium"]},"unlisted":false,"prevItem":{"title":"Backfilling Apache Hudi Tables in Production: Techniques & Approaches Using AWS Glue by Job Target LLC","permalink":"/blog/2023/07/20/Backfilling-Apache-Hudi-Tables-in-Production-Techniques-and-Approaches-Using-AWS-Glue-by-Job-Target-LLC"},"nextItem":{"title":"Quickly start using Apache Hudi on AWS EMR","permalink":"/blog/2023/07/08/Quickly-start-using-Apache-Hudi-on-AWS-EMR"}}')},66847:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/10/21/Practice-of-Apache-Hudi-in-building-real-time-data-lake-at-station-B","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-10-21-Practice-of-Apache-Hudi-in-building-real-time-data-lake-at-station-B.mdx","source":"@site/blog/2021-10-21-Practice-of-Apache-Hudi-in-building-real-time-data-lake-at-station-B.mdx","title":"Practice of Apache Hudi in building real-time data lake at station B","description":"Redirecting... please wait!!","date":"2021-10-21T00:00:00.000Z","tags":[{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"real-time datalake","permalink":"/blog/tags/real-time-datalake"},{"inline":true,"label":"developpaper","permalink":"/blog/tags/developpaper"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Yu Zhaojing","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Practice of Apache Hudi in building real-time data lake at station B","authors":[{"name":"Yu Zhaojing"}],"category":"blog","image":"/assets/images/blog/2021-10-21-station-b-real-time-data-lake-using-hudi.png","tags":["use-case","real-time datalake","developpaper"]},"unlisted":false,"prevItem":{"title":"How GE Aviation built cloud-native data pipelines at enterprise scale using the AWS platform","permalink":"/blog/2021/11/16/How-GE-Aviation-built-cloud-native-data-pipelines-at-enterprise-scale-using-the-AWS-platform"},"nextItem":{"title":"How Amazon Transportation Service enabled near-real-time event analytics at petabyte scale using AWS Glue with Apache Hudi","permalink":"/blog/2021/10/14/How-Amazon-Transportation-Service-enabled-near-real-time-event-analytics-at-petabyte-scale-using-AWS-Glue-with-Apache-Hudi"}}')},66871:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/04/14/doris-hudi-making-impossible-possible","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-04-14-doris-hudi-making-impossible-possible.mdx","source":"@site/blog/2025-04-14-doris-hudi-making-impossible-possible.mdx","title":"How Doris + Hudi Turned the Impossible Into the Everyday","description":"Redirecting... please wait!!","date":"2025-04-14T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"Apache Doris","permalink":"/blog/tags/apache-doris"},{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"federated querying","permalink":"/blog/tags/federated-querying"},{"inline":true,"label":"dzone","permalink":"/blog/tags/dzone"}],"readingTime":0.11,"hasTruncateMarker":false,"authors":[{"name":"Zen Hua","key":null,"page":null}],"frontMatter":{"title":"How Doris + Hudi Turned the Impossible Into the Everyday","author":"Zen Hua","category":"blog","image":"/assets/images/blog/2025-04-03-integrate-apache-doris-hudi-data-querying-migration.png","tags":["blog","Apache Hudi","Apache Doris","use-case","federated querying","dzone"]},"unlisted":false,"prevItem":{"title":"Exploring Apache Hudi\u2019s New Log-Structured Merge (LSM) Timeline","permalink":"/blog/2025/05/29/lsm-timeline"},"nextItem":{"title":"Why Walmart Chose Apache Hudi for Their Lakehouse","permalink":"/blog/2025/04/09/why-walmart-chose-apache-hudi-for-their-lakehouse"}}')},67152:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/05/10/top-3-things-you-can-do-to-get-fast-upsert-performance-in-apache-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-05-10-top-3-things-you-can-do-to-get-fast-upsert-performance-in-apache-hudi.mdx","source":"@site/blog/2023-05-10-top-3-things-you-can-do-to-get-fast-upsert-performance-in-apache-hudi.mdx","title":"Top 3 Things You Can Do to Get Fast Upsert Performance in Apache Hudi","description":"Redirecting... please wait!!","date":"2023-05-10T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"performance","permalink":"/blog/tags/performance"},{"inline":true,"label":"onehouse","permalink":"/blog/tags/onehouse"}],"readingTime":0.15,"hasTruncateMarker":false,"authors":[{"name":"Nadine Farah","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Top 3 Things You Can Do to Get Fast Upsert Performance in Apache Hudi","authors":[{"name":"Nadine Farah"}],"category":"blog","image":"/assets/images/blog/2023-05-10-top-3-things-you-can-do-to-get-fast-upsert-performance-in-apache-hudi.png","tags":["how-to","performance","onehouse"]},"unlisted":false,"prevItem":{"title":"Ingesting data to Apache Hudi using Spark sql","permalink":"/blog/2023/05/12/ingesting-data-to-apache-hudi-using-spark-sql"},"nextItem":{"title":"Amazon Athena now supports Apache Hudi 0.12.2","permalink":"/blog/2023/05/09/amazon-athena-apache-hudi"}}')},67198:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(11913),n=t(74848),s=t(28453),r=t(9230);const o={title:"Can you concurrently write data to Apache Hudi w/o any lock provider?",authors:[{name:"Sivabalan Narayanan"}],category:"blog",image:"/assets/images/blog/2023-04-29-can-you-concurrently-write-data-to-apache-hudi-w-o-any-lock-provider.gif",tags:["how-to","concurrency","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@simpsons/can-you-concurrently-write-data-to-apache-hudi-w-o-any-lock-provider-51ea55bf2dd6",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},67411:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/05/25/Record-by-record-deletable-data-lake-using-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-05-25-Record-by-record-deletable-data-lake-using-Apache-Hudi.mdx","source":"@site/blog/2022-05-25-Record-by-record-deletable-data-lake-using-Apache-Hudi.mdx","title":"The story of building a data lake that can be deleted on a record-by-record basis using Apache Hudi","description":"Redirecting... please wait!!","date":"2022-05-25T00:00:00.000Z","tags":[{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"gdpr deletion","permalink":"/blog/tags/gdpr-deletion"},{"inline":true,"label":"yahoo","permalink":"/blog/tags/yahoo"}],"readingTime":0.09,"hasTruncateMarker":false,"authors":[{"name":"Shota Ejima","socials":{},"key":null,"page":null}],"frontMatter":{"title":"The story of building a data lake that can be deleted on a record-by-record basis using Apache Hudi","authors":[{"name":"Shota Ejima"}],"category":"blog","image":"/assets/images/blog/2022-05-25-data-lake-at-yahoo-advertising-at-yahoo-japan.png","tags":["use-case","gdpr deletion","yahoo"]},"unlisted":false,"prevItem":{"title":"Asynchronous Indexing using Hudi","permalink":"/blog/2022/06/04/Asynchronous-Indexing-Using-Hudi"},"nextItem":{"title":"Multi-Modal Index for the Lakehouse in Apache Hudi","permalink":"/blog/2022/05/17/Introducing-Multi-Modal-Index-for-the-Lakehouse-in-Apache-Hudi"}}')},67441:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/dms-task-cf605b4a3c85bea264a16a20a1645608.png"},67834:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/08/16/kafka-custom-deserializer","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-08-16-kafka-custom-deserializer.md","source":"@site/blog/2021-08-16-kafka-custom-deserializer.md","title":"Schema evolution with DeltaStreamer using KafkaSource","description":"The schema used for data exchange between services can change rapidly with new business requirements.","date":"2021-08-16T00:00:00.000Z","tags":[{"inline":true,"label":"design","permalink":"/blog/tags/design"},{"inline":true,"label":"deltastreamer","permalink":"/blog/tags/deltastreamer"},{"inline":true,"label":"schema","permalink":"/blog/tags/schema"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"apache kafka","permalink":"/blog/tags/apache-kafka"}],"readingTime":3.41,"hasTruncateMarker":true,"authors":[{"name":"sbernauer","key":null,"page":null}],"frontMatter":{"title":"Schema evolution with DeltaStreamer using KafkaSource","excerpt":"Evolve schema used in Kafkasource of DeltaStreamer to keep data up to date with business","author":"sbernauer","category":"blog","image":"/assets/images/blog/hudi_schemaevolution.png","tags":["design","deltastreamer","schema","apache hudi","apache kafka"]},"unlisted":false,"prevItem":{"title":"Adding support for Virtual Keys in Hudi","permalink":"/blog/2021/08/18/virtual-keys"},"nextItem":{"title":"Cost-Efficient Open Source Big Data Platform at Uber","permalink":"/blog/2021/08/11/Cost-Efficient-Open-Source-Big-Data-Platform-at-Uber"}}')},67853:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(54258),n=t(74848),s=t(28453),r=t(9230);const o={title:"From Data lake to Microservices: Unleashing the Power of Apache Hudi's Record Level Index with FastAPI and Spark Connect",excerpt:"From Data lake to Microservices: Unleashing the Power of Apache Hudi's Record Level Index with FastAPI and Spark Connect",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2024-01-01-From-Data-lake-to-Microservices-Unleashing-the-Power-of-Apache-Hudi-Record-Level-Index-with-FastAPI-and-Spark-Connect.png",tags:["blog","apache hudi","linkedin","beginner","apache spark","record level index","pyspark","upserts","FastAPI"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.linkedin.com/pulse/from-datalake-microservices-unleashing-power-apache-hudis-soumil-shah-ylkoe/?trk=article-ssr-frontend-pulse_more-articles_related-content-card",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},67899:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(79556),n=t(74848),s=t(28453),r=t(82915);const o={title:"Apache Hudi 2023: A Year In Review",excerpt:"Reflect on and celebrate the myriad of exciting developments and accomplishments that have defined the year 2023 for the Hudi community.",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2023-12-28-a-year-in-review-2023/00.cover.png",tags:["apache hudi","community"]},l=void 0,d={authorsImageUrls:[void 0]},c=[{value:"Development Highlights",id:"development-highlights",level:2},{value:"Indexing has elevated to a whole new level",id:"indexing-has-elevated-to-a-whole-new-level",level:3},{value:"Write throughput achieves remarkable advancement",id:"write-throughput-achieves-remarkable-advancement",level:3},{value:"Programming APIs have a brand-new look",id:"programming-apis-have-a-brand-new-look",level:3},{value:"Usability receives significant attention",id:"usability-receives-significant-attention",level:3},{value:"Platform capabilities are substantially enhanced",id:"platform-capabilities-are-substantially-enhanced",level:3},{value:"Ecosystem integrations undergo notable expansions",id:"ecosystem-integrations-undergo-notable-expansions",level:3},{value:"Interoperability is the key",id:"interoperability-is-the-key",level:3},{value:"Stay tuned for 2024",id:"stay-tuned-for-2024",level:3},{value:"Content Spotlight",id:"content-spotlight",level:2},{value:"Engage with the Community",id:"engage-with-the-community",level:2}];function h(e){const a={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)("img",{src:"/assets/images/blog/2023-12-28-a-year-in-review-2023/00.cover.png",alt:"drawing",style:{width:"80%",display:"block",marginLeft:"auto",marginRight:"auto",marginTop:"18pt",marginBottom:"18pt"}}),"\n",(0,n.jsx)(a.p,{children:"In the warm glow of the holiday season, I am delighted to convey a message of deep appreciation on behalf of the\nHudi Project Management Committee (PMC) to all the contributors and users in the community who made 2023 an\nextraordinary year for Hudi."}),"\n",(0,n.jsxs)(a.p,{children:["In 2023, the Hudi community continued strong engagement and activities, evident in the\n",(0,n.jsx)(a.a,{href:"https://ossinsight.io/analyze/apache/hudi#pull-requests",children:"1,832 pull requests created"}),",\nwith a significant 1,363 of these being merged. We proudly welcomed 2 new PMC members and 3 new Committers.\nOur community ",(0,n.jsx)(a.a,{href:"https://apache-hudi.slack.com/join/shared_invite/zt-20r833rxh-627NWYDUyR8jRtMa2mZ~gg#/",children:"Slack channel"}),"\nwitnessed a remarkable 44% increase in users, with numbers exceeding 3,800.\nOur presence on social media platforms has grown impressively, with our ",(0,n.jsx)(a.a,{href:"https://x.com/apachehudi",children:"X (Twitter) account"}),"\ngarnering 2,274 followers, and our newly established ",(0,n.jsx)(a.a,{href:"https://www.linkedin.com/company/apache-hudi/",children:"LinkedIn page"}),"\nrapidly gaining 2,245 followers in just three months. Let\u2019s take a moment to reflect on and celebrate the myriad of\nexciting developments and accomplishments that have defined the year 2023 for the Hudi community."]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2023-12-28-a-year-in-review-2023/01.PR_histogram.png",alt:"drawing",style:{width:"80%",display:"block",marginLeft:"auto",marginRight:"auto",marginTop:"18pt",marginBottom:"18pt"}}),"\n",(0,n.jsx)(a.h2,{id:"development-highlights",children:"Development Highlights"}),"\n",(0,n.jsxs)(a.p,{children:["The year 2023 has been exceptionally productive for Hudi, marked by significant advancements and innovations.\nThere have been three major releases: ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-0.13.0",children:"0.13.0"}),",\n",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-0.14.0",children:"0.14.0"}),", and the trailblazing\n",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-1.0.0-beta1",children:"1.0.0-beta1"})," that have collectively reshaped the\ndatabase experience for Hudi data lakehouses. Here are some brief summaries highlighting key features introduced:"]}),"\n",(0,n.jsx)(a.h3,{id:"indexing-has-elevated-to-a-whole-new-level",children:"Indexing has elevated to a whole new level"}),"\n",(0,n.jsxs)(a.p,{children:["Hudi's new ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-0.14.0#record-level-index",children:"Record Level Index"}),"\nis a game-changing feature that boosts write performance for large tables. It achieves this by efficiently\nstoring per-record locations, enabling rapid retrieval during index look-ups. Benchmarks indicate a 72%\nimprovement in write latency compared to the Global Simple Index, alongside notable reductions in query latency\nfor equality-matching queries. The new ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-0.14.0#consistent-hashing-index-support",children:"Consistent Hash Index"}),"\ndynamically scales the buckets for hash-based indexing schemes. By addressing data skew issues inherent in bucket\nindex, it can achieve blazing fast look-up similar to the Record Level Index during the write process.\n",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-1.0.0-beta1#functional-index",children:"Functional Index"}),"\nenables the creation and deletion of indexes on specific columns, providing users with additional means to\nspeed up queries and adjust partitioning."]}),"\n",(0,n.jsx)(a.h3,{id:"write-throughput-achieves-remarkable-advancement",children:"Write throughput achieves remarkable advancement"}),"\n",(0,n.jsxs)(a.p,{children:["A common reason why developers choose Apache Hudi is for its ",(0,n.jsx)(a.a,{href:"https://medium.com/@kywe665/delta-hudi-iceberg-a-benchmark-compilation-a5630c69cffc",children:"industry leading write throughput and performance"}),".\nThe community has continued innovations on write performance including\n",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-0.13.0#early-conflict-detection-for-multi-writer",children:"Early-conflict detection for OCC"}),"\nwhich proactively validates concurrent writes before they are written to disk, avoiding significant resource wastage\nand enhancing throughput. Up-leveling this, the\n",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-1.0.0-beta1#concurrency-control",children:"Non-Blocking Concurrency Control"}),"\nintroduced in 1.0 further optimizes multi-writer throughput by allowing conflicts to be resolved later in query\nor via compaction. Responding to popular community requests,\n",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-0.13.0#support-for-partial-payload-update",children:"partial update capability"}),"\nwas implemented to allow updates to be applied only to changed fields, particularly benefiting the dimension\ntables that are usually super wide."]}),"\n",(0,n.jsx)(a.h3,{id:"programming-apis-have-a-brand-new-look",children:"Programming APIs have a brand-new look"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-0.13.0#optimizing-record-payload-handling",children:"HoodieRecordMerger"}),"\nis a new abstraction that unifies the merging semantics and makes use of the engine-native representation for\nrecords in the process. Benchmark shows a ballpark of 10-20% boost for upsert performance.\n",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-1.0.0-beta1#new-filegroup-reader",children:"File Group Reader"}),"\nis another API that standardizes File Group access, reducing MoR tables' read latencies by approximately 20%.\nEnabling position-based merging and page-skipping can further accelerate snapshot queries by 5.7 times."]}),"\n",(0,n.jsx)(a.h3,{id:"usability-receives-significant-attention",children:"Usability receives significant attention"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsxs)(a.a,{href:"https://hudi.apache.org/releases/release-0.14.0#table-valued-function-named-hudi_table_changes-designed-for-incremental-reading-through-spark-sql",children:["Table-valued function ",(0,n.jsx)(a.code,{children:"hudi_table_changes"})]}),"\nsimplifies performing incremental queries via SQLs.\n",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-0.14.0#support-for-hudi-tables-with-autogenerated-keys",children:"Auto-generated keys"}),"\nallows users to omit providing a record key field, especially useful for append-only tables. Among many other\nuser-friendly updates, two more notable ones are the addition of a\n",(0,n.jsxs)(a.a,{href:"https://hudi.apache.org/releases/release-0.13.0#hudi-cli-bundle",children:[(0,n.jsx)(a.code,{children:"hudi-cli-bundle"})," jar"]}),"\nand a revamped ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/basic_configurations",children:"configuration page"}),"."]}),"\n",(0,n.jsx)(a.h3,{id:"platform-capabilities-are-substantially-enhanced",children:"Platform capabilities are substantially enhanced"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-0.13.0#change-data-capture",children:"Changed Data Capture"}),"\nwas supported by logging additional information alongside writers. The changed data, including ",(0,n.jsx)(a.code,{children:"before"}),"\nand ",(0,n.jsx)(a.code,{children:"after"})," images, can be served through incremental queries, offering rich analytical insights.\n",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-0.13.0#metaserver",children:"Metaserver"}),"\noffers centralized management services for operating numerous tables in lakehouse projects, signifying a major\nstep in Hudi's platform features.\n",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-0.14.0#hoodiedeltastreamer-renamed-to-hoodiestreamer",children:(0,n.jsx)(a.code,{children:"HoodieStreamer"})}),"\n(formerly ",(0,n.jsx)(a.code,{children:"HoodieDeltaStreamer"}),") remains a highly popular tool for data ingestion:\n",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-0.13.0#new-source-support-in-deltastreamer",children:"new sources"}),"\nsuch as Protobuf Kafka source, GCS incremental source, and Pulsar source were added, further expanding\nthe integration capabilities."]}),"\n",(0,n.jsx)(a.h3,{id:"ecosystem-integrations-undergo-notable-expansions",children:"Ecosystem integrations undergo notable expansions"}),"\n",(0,n.jsxs)(a.p,{children:["On AWS,\n",(0,n.jsx)(a.a,{href:"https://aws.amazon.com/about-aws/whats-new/2023/05/amazon-athena-apache-hudi/",children:"Athena supported Hudi 0.12.2 and Hudi's metadata table"}),",\nelevating query performance.\n",(0,n.jsx)(a.a,{href:"https://aws.amazon.com/blogs/big-data/introducing-apache-hudi-support-with-aws-glue-crawlers/",children:"AWS Glue crawlers added Hudi support"}),"\nwith Glue 4.0 working with Hudi 0.12.1, and AWS EMR extended the\n",(0,n.jsx)(a.a,{href:"https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-app-versions-6.x.html",children:"support matrix"}),"\nto cover Hudi 0.13 and 0.14. GCP improved\n",(0,n.jsx)(a.a,{href:"https://cloud.google.com/blog/products/data-analytics/bigquery-manifest-file-support-for-open-table-format-queries",children:"Hudi integration in BigQuery"}),"\nwith a new manifest file integration for improved performance.\n",(0,n.jsx)(a.a,{href:"https://docs.starburst.io/latest/connector/hudi.html",children:"Starburst also added a Hudi connector"}),".\nExecution engine support has also been extended to newer versions, including Spark 3.4 and 3.5,\nas well as Flink 1.16, 1.17, and 1.18."]}),"\n",(0,n.jsx)(a.h3,{id:"interoperability-is-the-key",children:"Interoperability is the key"}),"\n",(0,n.jsxs)(a.p,{children:["While Apache Hudi continues its strong growth momentum, some members of the community also decided it is time to\nstart building interoperability bridges across Lakehouse table formats with Delta Lake and Iceberg. The\n",(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/blog/onetable-is-now-open-source",children:"recent announcement about OneTable becoming open source"}),"\nmarks a big leap forward for all developers looking to build a ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2024/07/11/what-is-a-data-lakehouse/",children:"data lakehouse"})," architecture. This development not\nonly emphasizes Hudi's commitment to openness but also enables a wider range of users to experience the\ntechnological advantages offered by Hudi."]}),"\n",(0,n.jsx)(a.h3,{id:"stay-tuned-for-2024",children:"Stay tuned for 2024"}),"\n",(0,n.jsxs)(a.p,{children:["The File Group Reader APIs are poised for widespread adoption, promising benefits for numerous query\nengines. We also anticipate broad adoption for Non-Blocking Concurrency Control. And there's more on\nthe horizon, including innovations like infinite timeline, secondary indexes, multi-table transactions,\nand the support for unstructured data. For the latest updates and detailed insights, I encourage you to\nvisit the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/roadmap",children:"roadmap page"}),"."]}),"\n",(0,n.jsx)(a.h2,{id:"content-spotlight",children:"Content Spotlight"}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2023-12-28-a-year-in-review-2023/02.contentspotlight.png",alt:"drawing",style:{width:"80%",display:"block",marginLeft:"auto",marginRight:"auto",marginTop:"18pt",marginBottom:"18pt"}}),"\n",(0,n.jsx)(a.p,{children:"Below is a curated list highlighting noteworthy pieces of content from the diverse Hudi community in 2023:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://aws.amazon.com/blogs/big-data/create-an-apache-hudi-based-near-real-time-transactional-data-lake-using-aws-dms-amazon-kinesis-aws-glue-streaming-etl-and-data-visualization-using-amazon-quicksight/",children:"Create Hudi-based near-real-time transactional data lake"})," - AWS"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://aws.amazon.com/blogs/big-data/automate-schema-evolution-at-scale-with-apache-hudi-in-aws-glue/",children:"Automate schema evolution at scale with Apache Hudi"})," - AWS"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://aws.amazon.com/blogs/big-data/how-zoom-implemented-streaming-log-ingestion-and-efficient-gdpr-deletes-using-apache-hudi-on-amazon-emr/",children:"Zoom implemented streaming log ingestion and efficient GDPR deletes using Hudi"})," - AWS"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://medium.com/walmartglobaltech/lakehouse-at-fortune-1-scale-480bcb10391b",children:"Lakehouse at Fortune 1 scale"})," - Walmart"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://www.uber.com/blog/ubers-lakehouse-architecture/",children:"Setting Uber\u2019s Transactional Data Lake in Motion"})," - Uber"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://youtu.be/dZbXC4mlNck",children:"Notion\u2019s journey: transition from Snowflake to Hudi"})," - Notion"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://opensourcedatasummit.com/robinhoods-data-lakehouse/",children:"Scaling and governing Robinhood's data lakehouse"})," - Robinhood"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/blog/apache-hudi-vs-delta-lake-vs-apache-iceberg-lakehouse-feature-comparison",children:"Feature comparison: Hudi vs Delta vs Iceberg"})," - Kyle Weller, Onehouse"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://opensourcedatasummit.com/apache-hudi-1-preview/",children:"Apache Hudi 1.0 preview: A database experience on the data lake"})," - Sagar Sumit & Bhavani Sudha Saktheeswaran, Hudi PMC"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/blog/hudi-metafields-demystified",children:"Hudi Metafields demystified"})," and ",(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/blog/knowing-your-data-partitioning-vices-on-the-data-lakehouse",children:"Knowing your data partitioning vices"})," - Bhavani Sudha Saktheeswaran, Hudi PMC"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2023/11/01/record-level-index/",children:"Record Level Index: blazing fast indexing for large-scale datasets"})," - Shiyan Xu & Sivabalan Narayanan, Hudi PMC"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://blog.datumagic.com/p/apache-hudi-from-zero-to-one-110",children:"Apache Hudi from zero to one: a 10-post blog series"})," - Shiyan Xu, Hudi PMC"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://youtu.be/YgmOASLum7g",children:"Hudi Workshop: Build a ride-share lakehouse platform on AWS"})," - Soumil Shah, Jaganath Achari, Nadine Farah"]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["Additionally, the official Hudi website is a treasure trove of valuable learning materials. Begin your\njourney on ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/overview",children:"the documentation page"}),", and then explore a wealth of\n",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/talks",children:"talks"}),", ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/videos",children:"videos"}),",\nand ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog",children:"blogs"})," to deepen your understanding and knowledge of Hudi."]}),"\n",(0,n.jsx)(a.h2,{id:"engage-with-the-community",children:"Engage with the Community"}),"\n",(0,n.jsxs)(a.p,{children:["Throughout 2023, the Hudi community played an important role in the data industry altogether, gathering and\nfeaturing in many virtual syncs, live events, meet-ups, and conference presentations. We marked our presence\nat a variety of events, listed here in no particular order: Re",":Invent",", PrestoCon, Trino Fest, Current,\nthe Data & AI Summit, Flink Forward, the Open-source Data Summit, ApacheCon, AI.dev, QCon, OSA Con, DEWCon,\nand Data Council."]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2023-12-28-a-year-in-review-2023/03.events.png",alt:"drawing",style:{width:"80%",display:"block",marginLeft:"auto",marginRight:"auto",marginTop:"18pt",marginBottom:"18pt"}}),"\n",(0,n.jsxs)(a.p,{children:["As we reflect on an eventful 2023, the Hudi community continues to thrive and welcomes diverse forms\nof engagement. For those looking to connect, our ",(0,n.jsx)(r.A,{title:"Slack group"}),"\nis an excellent place for general inquiries, being watched out by Hudi experts and an LLM-backed\nquestion bot. You can also participate in our\n",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/community/office_hours",children:"weekly office hours"}),"\nand ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/community/syncs",children:"monthly community syncs"}),"\nto stay updated and involved. To keep abreast of the latest developments, follow Hudi's\n",(0,n.jsx)(a.a,{href:"https://www.linkedin.com/company/apache-hudi/",children:"LinkedIn page"}),",\n",(0,n.jsx)(a.a,{href:"https://twitter.com/apachehudi",children:"X (Twitter) account"}),",\nand ",(0,n.jsx)(a.a,{href:"https://www.youtube.com/@apachehudi",children:"YouTube channel"}),"."]}),"\n",(0,n.jsxs)(a.p,{children:["If you encounter any issues or have feature requests, we encourage you to file them through\n",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/issues",children:"GitHub issues"})," or\n",(0,n.jsx)(a.a,{href:"https://issues.apache.org/jira/projects/HUDI/summary",children:"JIRA tickets"}),".\nFor more in-depth discussions and contributions to the ongoing development of Hudi,\nsubscribing (by sending an empty email) to\n",(0,n.jsx)(a.a,{href:"mailto:dev-subscribe@hudi.apache.org",children:"our dev mailing list"})," is a great option."]}),"\n",(0,n.jsxs)(a.p,{children:["And for those inspired to contribute directly to the project,\n",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/contribute/how-to-contribute",children:"our contribution guide"})," is your\nstarting point. Your involvement, whether it's by contributing code, sharing ideas, or simply giving\n",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/",children:"our GitHub repository"})," a star, is greatly valued. Together,\nlet's continue to shape Hudi's future and drive innovation in the open-source community.\nHere's to an even more vibrant and successful 2024 ahead!"]})]})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h(e)}},67946:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image7-a67a3aa2b20416cf15c3229553c626ce.png"},68234:(e,a,t)=>{"use strict";t.d(a,{A:()=>s});t(96540);var i=t(28774),n=t(74848);const s=({authors:e=[],className:a,withLink:t=!0})=>{const s=e=>(0,n.jsx)("span",{className:a,itemProp:"name",children:e.name});return(0,n.jsx)(n.Fragment,{children:e.map((a,r)=>(0,n.jsx)("div",{children:(0,n.jsx)("div",{children:a.name&&(0,n.jsxs)("div",{children:[0!==r?r!==e.length-1?",":"and":"",t?(0,n.jsx)(i.A,{href:a.url,itemProp:"url",children:s(a)}):s(a)]})})},r))})}},68284:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image1-352adf99bd976cab9ea093daab842339.png"},68445:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/01/14/change-data-capture-with-debezium-and-apache-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-01-14-change-data-capture-with-debezium-and-apache-hudi.mdx","source":"@site/blog/2022-01-14-change-data-capture-with-debezium-and-apache-hudi.mdx","title":"Change Data Capture with Debezium and Apache Hudi","description":"As of Hudi v0.10.0, we are excited to announce the availability of Debezium sources for Deltastreamer that provide the ingestion of change capture data (CDC) from Postgres and Mysql databases to your data lake. For more details, please refer to the original RFC.","date":"2022-01-14T00:00:00.000Z","tags":[{"inline":true,"label":"design","permalink":"/blog/tags/design"},{"inline":true,"label":"deltastreamer","permalink":"/blog/tags/deltastreamer"},{"inline":true,"label":"cdc","permalink":"/blog/tags/cdc"},{"inline":true,"label":"change data capture","permalink":"/blog/tags/change-data-capture"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":9.01,"hasTruncateMarker":true,"authors":[{"name":"Rajesh Mahindra","key":null,"page":null}],"frontMatter":{"title":"Change Data Capture with Debezium and Apache Hudi","excerpt":"A review of new Debezium source connector for Apache Hudi","author":"Rajesh Mahindra","category":"blog","image":"/assets/images/blog/debezium.png","tags":["design","deltastreamer","cdc","change data capture","apache hudi"]},"unlisted":false,"prevItem":{"title":"Why and How I Integrated Airbyte and Apache Hudi","permalink":"/blog/2022/01/18/Why-and-How-I-Integrated-Airbyte-and-Apache-Hudi"},"nextItem":{"title":"Apache Hudi - 2021 a Year in Review","permalink":"/blog/2022/01/06/apache-hudi-2021-a-year-in-review"}}')},68660:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/03/26/clustering","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-03-26-clustering.mdx","source":"@site/blog/2025-03-26-clustering.mdx","title":"What is Clustering in an Open Data Lakehouse?","description":"Redirecting... please wait!!","date":"2025-03-26T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"Apache Iceberg","permalink":"/blog/tags/apache-iceberg"},{"inline":true,"label":"Delta Lake","permalink":"/blog/tags/delta-lake"},{"inline":true,"label":"Clustering","permalink":"/blog/tags/clustering"},{"inline":true,"label":"Z-order","permalink":"/blog/tags/z-order"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Dipankar Mazumdar","key":null,"page":null}],"frontMatter":{"title":"What is Clustering in an Open Data Lakehouse?","author":"Dipankar Mazumdar","category":"blog","image":"/assets/images/blog/cluster.png","tags":["blog","Apache Hudi","Apache Iceberg","Delta Lake","Clustering","Z-order"]},"unlisted":false,"prevItem":{"title":"ACID Transactions in an Open Data Lakehouse","permalink":"/blog/2025/03/26/acid-transactions"},"nextItem":{"title":"Data Deduplication Strategies in an Open Lakehouse Architecture","permalink":"/blog/2025/03/26/dedupe"}}')},68692:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/03/16/Setting-Uber-Transactional-Data-Lake-in-Motion-with-Incremental-ETL-Using-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-03-16-Setting-Uber-Transactional-Data-Lake-in-Motion-with-Incremental-ETL-Using-Apache-Hudi.mdx","source":"@site/blog/2023-03-16-Setting-Uber-Transactional-Data-Lake-in-Motion-with-Incremental-ETL-Using-Apache-Hudi.mdx","title":"Setting Uber\u2019s Transactional Data Lake in Motion with Incremental ETL Using Apache Hudi","description":"Redirecting... please wait!!","date":"2023-03-16T00:00:00.000Z","tags":[{"inline":true,"label":"incremental processing","permalink":"/blog/tags/incremental-processing"},{"inline":true,"label":"datalake","permalink":"/blog/tags/datalake"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"medallion architecture","permalink":"/blog/tags/medallion-architecture"},{"inline":true,"label":"uber","permalink":"/blog/tags/uber"}],"readingTime":0.1,"hasTruncateMarker":false,"authors":[{"name":"Vinoth Govindarajan","socials":{},"key":null,"page":null},{"name":"Saketh Chintapalli","socials":{},"key":null,"page":null},{"name":"Yogesh Saswade","socials":{},"key":null,"page":null},{"name":"Aayush Bareja","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Setting Uber\u2019s Transactional Data Lake in Motion with Incremental ETL Using Apache Hudi","authors":[{"name":"Vinoth Govindarajan"},{"name":"Saketh Chintapalli"},{"name":"Yogesh Saswade"},{"name":"Aayush Bareja"}],"category":"blog","image":"/assets/images/blog/hudi-lakehouse-architecture-uber.png","tags":["incremental processing","datalake","apache hudi","medallion architecture","uber"]},"unlisted":false,"prevItem":{"title":"Introduction to Apache Hudi","permalink":"/blog/2023/03/17/introduction-to-apache-hudi"},"nextItem":{"title":"Getting Started: Manage your Hudi tables with the admin Hudi-CLI tool","permalink":"/blog/2023/02/22/Getting-Started-Manage-your-Hudi-tables-with-the-admin-Hudi-CLI-tool"}}')},68820:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/05/27/apache-hudi-vs-delta-lake-choosing-the-right-tool-for-your-data-lake-on-aws","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-05-27-apache-hudi-vs-delta-lake-choosing-the-right-tool-for-your-data-lake-on-aws.mdx","source":"@site/blog/2024-05-27-apache-hudi-vs-delta-lake-choosing-the-right-tool-for-your-data-lake-on-aws.mdx","title":"Apache Hudi vs. Delta Lake: Choosing the Right Tool for Your Data Lake on AWS","description":"Redirecting... please wait!!","date":"2024-05-27T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"delta lake","permalink":"/blog/tags/delta-lake"},{"inline":true,"label":"comparison","permalink":"/blog/tags/comparison"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.16,"hasTruncateMarker":false,"authors":[{"name":"Siladitya Ghosh","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi vs. Delta Lake: Choosing the Right Tool for Your Data Lake on AWS","author":"Siladitya Ghosh","category":"blog","image":"/assets/images/blog/2024-05-27-apache-hudi-vs-delta-lake-choosing-the-right-tool-for-your-data-lake-on-aws.png","tags":["blog","apache hudi","delta lake","comparison","medium"]},"unlisted":false,"prevItem":{"title":"Apache Hudi: A Deep Dive with Python Code Examples","permalink":"/blog/2024/06/07/apache-hudi-a-deep-dive-with-python-code-examples"},"nextItem":{"title":"Use AWS Data Exchange to seamlessly share Apache Hudi datasets","permalink":"/blog/2024/05/22/use-aws-data-exchange-to-seamlessly-share-apache-hudi-datasets"}}')},68829:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(49076),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi grows cloud data lake maturity",authors:[{name:"Sean Michael Kerner"}],category:"blog",image:"/assets/images/blog/2020-06-16-Apache-Hudi-grows-cloud-data-lake-maturity.jpeg",tags:["blog","techtarget"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://searchdatamanagement.techtarget.com/news/252484740/Apache-Hudi-grows-cloud-data-lake-maturity",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},68900:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide10-698c9a0980df4f61806ae141521a12b0.png"},68936:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(54388),n=t(74848),s=t(28453),r=t(9230);const o={title:"Combine Transactional Integrity and Data Lake Operations with YugabyteDB and Apache Hudi",author:"Balachandar Seetharaman",category:"blog",image:"/assets/images/blog/2024-02-06-Combine-Transactional-Integrity-and-Data-Lake-Operations-with-YugabyteDB-and-Apache-Hudi.png",tags:["blog","apache hudi","ACID","transactions","real-time datalake","cdc","etl","yugabyte"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.yugabyte.com/blog/apache-hudi-data-lakehouse-integration/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},69003:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/07/16/Amazon-Athena-expands-Apache-Hudi-support","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-07-16-Amazon-Athena-expands-Apache-Hudi-support.mdx","source":"@site/blog/2021-07-16-Amazon-Athena-expands-Apache-Hudi-support.mdx","title":"Amazon Athena expands Apache Hudi support","description":"Redirecting... please wait!!","date":"2021-07-16T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Amazon Athena expands Apache Hudi support","category":"blog","image":"/assets/images/blog/aws.jpg","tags":["blog","amazon"]},"unlisted":false,"prevItem":{"title":"Apache Hudi - The Data Lake Platform","permalink":"/blog/2021/07/21/streaming-data-lake-platform"},"nextItem":{"title":"Part1: Query apache hudi dataset in an amazon S3 data lake with amazon athena : Read optimized queries","permalink":"/blog/2021/07/16/Query-apache-hudi-dataset-in-an-amazon-S3-data-lake-with-amazon-athena-Read-optimized-queries"}}')},69085:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig1-86b4d7f2e4b5c535b2ec0036d223065c.png"},69127:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/04/06/from-swamp-to-stream-how-apache-hudi-transforms-the-modern-data-lake","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-04-06-from-swamp-to-stream-how-apache-hudi-transforms-the-modern-data-lake.mdx","source":"@site/blog/2025-04-06-from-swamp-to-stream-how-apache-hudi-transforms-the-modern-data-lake.mdx","title":" From Swamp to Stream: How Apache Hudi Transforms the Modern Data Lake","description":"Redirecting... please wait!!","date":"2025-04-06T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"real-time datalake","permalink":"/blog/tags/real-time-datalake"},{"inline":true,"label":"incremental processing","permalink":"/blog/tags/incremental-processing"},{"inline":true,"label":"upserts","permalink":"/blog/tags/upserts"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Everton Gomede","key":null,"page":null}],"frontMatter":{"title":" From Swamp to Stream: How Apache Hudi Transforms the Modern Data Lake","author":"Everton Gomede","category":"blog","tags":["blog","Apache Hudi","real-time datalake","incremental processing","upserts","medium"]},"unlisted":false,"prevItem":{"title":"Why Walmart Chose Apache Hudi for Their Lakehouse","permalink":"/blog/2025/04/09/why-walmart-chose-apache-hudi-for-their-lakehouse"},"nextItem":{"title":"Integrating Apache Doris and Hudi for Data Querying and Migration","permalink":"/blog/2025/04/03/integrate-apache-doris-hudi-data-querying-migration"}}')},69240:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(71611),n=t(74848),s=t(28453);const r={title:"Hudi Z-Order and Hilbert Space Filling Curves",excerpt:"Explore the benefits of new Apache Hudi Z-Order and Hilbert Curves",author:"Alexey Kudinkin and Tao Meng",category:"blog",image:"/assets/images/zordercurve.png",tags:["design","clustering","data skipping","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Background",id:"background",level:3},{value:"Setup",id:"setup",level:3},{value:"Testing",id:"testing",level:3},{value:"Results",id:"results",level:3},{value:"Epilogue",id:"epilogue",level:3}];function c(e){const a={a:"a",code:"code",em:"em",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(a.p,{children:["As of Hudi v0.10.0, we are excited to introduce support for an advanced Data Layout Optimization technique known in the database realm as ",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Z-order_curve",children:"Z-order"})," and ",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Hilbert_curve",children:"Hilbert"})," space filling curves."]}),"\n",(0,n.jsx)(a.h3,{id:"background",children:"Background"}),"\n",(0,n.jsxs)(a.p,{children:["Amazon EMR team recently published a ",(0,n.jsx)(a.a,{href:"https://aws.amazon.com/blogs/big-data/new-features-from-apache-hudi-0-7-0-and-0-8-0-available-on-amazon-emr/",children:"great article"})," show-casing how ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/clustering",children:"clustering"})," your data can improve your ",(0,n.jsx)(a.em,{children:"query performance"}),"."]}),"\n",(0,n.jsx)(a.p,{children:"To better understand what's going on and how it's related to space-filling curves, let's zoom in to the setup in that article:"}),"\n",(0,n.jsxs)(a.p,{children:["In the article, 2 ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/overview",children:"Apache Hudi"})," tables are compared (both ingested from the well-known ",(0,n.jsx)(a.a,{href:"https://s3.amazonaws.com/amazon-reviews-pds/tsv/index.txt",children:"Amazon Reviews"})," dataset):"]}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.code,{children:"amazon_reviews"})," table which is not clustered (ie the data has not been re-ordered by any particular key)"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.code,{children:"amazon_reviews_clustered"})," table which is clustered. When data is clustered by Apache Hudi the data is ",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Lexicographic_order",children:(0,n.jsx)(a.strong,{children:"lexicographically ordered"})})," (hereon we will be referring to this kind of ordering as ",(0,n.jsx)(a.strong,{children:(0,n.jsx)(a.em,{children:"linear ordering"})}),") by 2 columns: ",(0,n.jsx)(a.code,{children:"star_rating"}),", ",(0,n.jsx)(a.code,{children:"total_votes"})," (see screenshot below)"]}),"\n"]}),"\n",(0,n.jsx)("img",{src:"/assets/images/hudiconfigz.png",alt:"drawing",width:"800"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.em,{children:"Screenshot of the Hudi configuration (from Amazon EMR team article)"})}),"\n",(0,n.jsx)(a.p,{children:"To showcase the improvement in querying performance, the following queries are executed against both of these tables:"}),"\n",(0,n.jsx)("img",{src:"/assets/images/table1.png",alt:"drawing",width:"800"}),"\n",(0,n.jsx)("img",{src:"/assets/images/table2.png",alt:"drawing",width:"800"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.em,{children:"Screenshots of the queries run against the previously setup tables (from Amazon EMR team article)"})}),"\n",(0,n.jsxs)(a.p,{children:["The important consideration to point out here is that the queries were specifying ",(0,n.jsx)(a.strong,{children:"both of the columns"})," latter table is ordered by (both ",(0,n.jsx)(a.code,{children:"star_rating"})," and ",(0,n.jsx)(a.code,{children:"total_votes"}),")."]}),"\n",(0,n.jsx)(a.p,{children:"And this is unfortunately a crucial limitation of the linear/lexicographic ordering, the value of the ordering diminishes very quickly as you add more columns. It's not hard to see why:"}),"\n",(0,n.jsx)("img",{src:"/assets/images/lexicographicorder.png",alt:"drawing",width:"250"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.em,{children:"Courtesy of Wikipedia,"})," ",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Lexicographic_order",children:(0,n.jsx)(a.em,{children:"Lexicographic Order article"})})]}),"\n",(0,n.jsxs)(a.p,{children:["From this image you can see that with lexicographically ordered 3-tuples of integers, only the first column is able to feature crucial property of ",(0,n.jsx)(a.strong,{children:"locality"}),' for all of the records having the same value: for ex, all of the records wit values starting with "1", "2", "3" (in the first columns) are nicely clumped together. However if you try to find all the values that have "5" as the value in their third column you\'d see that those are now dispersed all over the place, not being localized at all.']}),"\n",(0,n.jsx)(a.p,{children:"The crucial property that improves query performance is locality: it enables queries to substantially reduce the search space and the number of files that need to be scanned, parsed, etc."}),"\n",(0,n.jsx)(a.p,{children:"But... does this mean that our queries are doomed to do a full-scan if we're filtering by anything other than the first (or more accurate would be to say prefix) of the list of columns the table is ordered by?"}),"\n",(0,n.jsx)(a.p,{children:"Not exactly: luckily, locality is also a property that space-filling curves enable while enumerating multi-dimensional spaces (records in our table could be represented as points in N-dimensional space, where N is the number of columns in our table)"}),"\n",(0,n.jsx)(a.p,{children:"How does it work?"}),"\n",(0,n.jsx)(a.p,{children:"Let's take Z-curve as an example: Z-order curves fitting a 2-dimensional plane would look like the following:"}),"\n",(0,n.jsx)("img",{src:"/assets/images/zordercurve.png",alt:"drawing",width:"400"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.em,{children:"Courtesy of Wikipedia,"})," ",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Z-order_curve",children:(0,n.jsx)(a.em,{children:"Z-order curve article"})})]}),"\n",(0,n.jsxs)(a.p,{children:['As you can see following its path, instead of simply ordering by one coordinate ("x") first, following with the other, it\'s actually ordering them as if the bits of those coordinates have been ',(0,n.jsx)(a.em,{children:"interwoven"})," into a single value:"]}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{children:"Coordinate"}),(0,n.jsx)(a.th,{children:"X (binary)"}),(0,n.jsx)(a.th,{children:"Y (binary)"}),(0,n.jsx)(a.th,{children:"Z-values (ordered)"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"(0, 0)"}),(0,n.jsx)(a.td,{children:"000"}),(0,n.jsx)(a.td,{children:"000"}),(0,n.jsx)(a.td,{children:"000000"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"(1, 0)"}),(0,n.jsx)(a.td,{children:"001"}),(0,n.jsx)(a.td,{children:"000"}),(0,n.jsx)(a.td,{children:"000001"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"(0, 1)"}),(0,n.jsx)(a.td,{children:"000"}),(0,n.jsx)(a.td,{children:"001"}),(0,n.jsx)(a.td,{children:"000010"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"(1, 1)"}),(0,n.jsx)(a.td,{children:"001"}),(0,n.jsx)(a.td,{children:"001"}),(0,n.jsx)(a.td,{children:"000011"})]})]})]}),"\n",(0,n.jsx)(a.p,{children:'This allows for that crucial property of locality (even though a slightly "stretched" one) to be carried over to all columns as compared to just the first one in case of linear ordering.'}),"\n",(0,n.jsxs)(a.p,{children:["In a similar fashion, Hilbert curves also allow you to map points in a N-dimensional space (rows in our table) onto 1-dimensional curve, essentially ",(0,n.jsx)(a.em,{children:"ordering"})," them, while still preserving the crucial property of locality. Read more details about Hilbert Curves ",(0,n.jsx)(a.a,{href:"https://drum.lib.umd.edu/handle/1903/804",children:"here"}),". Our personal experiments so far show that ordering data along a Hilbert curve leads to better clustering and performance outcomes."]}),"\n",(0,n.jsx)(a.p,{children:"Now, let's check it out in action!"}),"\n",(0,n.jsx)(a.h3,{id:"setup",children:"Setup"}),"\n",(0,n.jsxs)(a.p,{children:["We will use the ",(0,n.jsx)(a.a,{href:"https://s3.amazonaws.com/amazon-reviews-pds/readme.html",children:"Amazon Reviews"})," dataset again, but this time we will use Hudi to Z-Order by ",(0,n.jsx)(a.code,{children:"product_id"}),", ",(0,n.jsx)(a.code,{children:"customer_id"})," columns tuple instead of Clustering or ",(0,n.jsx)(a.em,{children:"linear ordering"}),"."]}),"\n",(0,n.jsxs)(a.p,{children:["No special preparations are required for the dataset, you can simply download it from ",(0,n.jsx)(a.a,{href:"https://s3.amazonaws.com/amazon-reviews-pds/readme.html",children:"S3"})," in Parquet format and use it directly as an input for Spark ingesting it into Hudi table."]}),"\n",(0,n.jsx)(a.p,{children:"Launch Spark Shell"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:"./bin/spark-shell --master 'local[4]' --driver-memory 8G --executor-memory 8G \\\n  --jars ../../packaging/hudi-spark-bundle/target/hudi-spark3-bundle_2.12-0.10.0.jar \\\n  --packages org.apache.spark:spark-avro_2.12:2.4.4 \\\n  --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer'\n"})}),"\n",(0,n.jsx)(a.p,{children:"Paste"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-scala",children:'import org.apache.hadoop.fs.{FileStatus, Path}\nimport scala.collection.JavaConversions._\nimport org.apache.spark.sql.SaveMode._\nimport org.apache.hudi.{DataSourceReadOptions, DataSourceWriteOptions}\nimport org.apache.hudi.DataSourceWriteOptions._\nimport org.apache.hudi.common.fs.FSUtils\nimport org.apache.hudi.common.table.HoodieTableMetaClient\nimport org.apache.hudi.common.util.ClusteringUtils\nimport org.apache.hudi.config.HoodieClusteringConfig\nimport org.apache.hudi.config.HoodieWriteConfig._\nimport org.apache.spark.sql.DataFrame\n\nimport java.util.stream.Collectors\n\nval layoutOptStrategy = "z-order"; // OR "hilbert"\n\nval inputPath = s"file:///${System.getProperty("user.home")}/datasets/amazon_reviews_parquet"\nval tableName = s"amazon_reviews_${layoutOptStrategy}"\nval outputPath = s"file:///tmp/hudi/$tableName"\n\n\ndef safeTableName(s: String) = s.replace(\'-\', \'_\')\n\nval commonOpts =\n  Map(\n    "hoodie.compact.inline" -> "false",\n    "hoodie.bulk_insert.shuffle.parallelism" -> "10"\n  )\n\n\n////////////////////////////////////////////////////////////////\n// Writing to Hudi\n////////////////////////////////////////////////////////////////\n\nval df = spark.read.parquet(inputPath)\n\ndf.write.format("hudi")\n  .option(DataSourceWriteOptions.TABLE_TYPE.key(), COW_TABLE_TYPE_OPT_VAL)\n  .option("hoodie.table.name", tableName)\n  .option(PRECOMBINE_FIELD.key(), "review_id")\n  .option(RECORDKEY_FIELD.key(), "review_id")\n  .option(DataSourceWriteOptions.PARTITIONPATH_FIELD.key(), "product_category")\n  .option("hoodie.clustering.inline", "true")\n  .option("hoodie.clustering.inline.max.commits", "1")\n  // NOTE: Small file limit is intentionally kept _ABOVE_ target file-size max threshold for Clustering,\n  // to force re-clustering\n  .option("hoodie.clustering.plan.strategy.small.file.limit", String.valueOf(1024 * 1024 * 1024)) // 1Gb\n  .option("hoodie.clustering.plan.strategy.target.file.max.bytes", String.valueOf(128 * 1024 * 1024)) // 128Mb\n  // NOTE: We\'re increasing cap on number of file-groups produced as part of the Clustering run to be able to accommodate for the \n  // whole dataset (~33Gb)\n  .option("hoodie.clustering.plan.strategy.max.num.groups", String.valueOf(4096))\n  .option(HoodieClusteringConfig.LAYOUT_OPTIMIZE_ENABLE.key, "true")\n  .option(HoodieClusteringConfig.LAYOUT_OPTIMIZE_STRATEGY.key, layoutOptStrategy)\n  .option(HoodieClusteringConfig.PLAN_STRATEGY_SORT_COLUMNS.key, "product_id,customer_id")\n  .option(DataSourceWriteOptions.OPERATION.key(), DataSourceWriteOptions.BULK_INSERT_OPERATION_OPT_VAL)\n  .option(BULK_INSERT_SORT_MODE.key(), "NONE")\n  .options(commonOpts)\n  .mode(ErrorIfExists)\n  \n'})}),"\n",(0,n.jsx)(a.h3,{id:"testing",children:"Testing"}),"\n",(0,n.jsx)(a.p,{children:"Please keep in mind, that each individual test is run in a separate spark-shell to avoid caching getting in the way of our measurements."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-scala",children:'////////////////////////////////////////////////////////////////\n// Reading\n///////////////////////////////////////////////////////////////\n\n// Temp Table w/ Data Skipping DISABLED\nval readDf: DataFrame =\n  spark.read.option(DataSourceReadOptions.ENABLE_DATA_SKIPPING.key(), "false").format("hudi").load(outputPath)\n\nval rawSnapshotTableName = safeTableName(s"${tableName}_sql_snapshot")\n\nreadDf.createOrReplaceTempView(rawSnapshotTableName)\n\n\n// Temp Table w/ Data Skipping ENABLED\nval readDfSkip: DataFrame =\n  spark.read.option(DataSourceReadOptions.ENABLE_DATA_SKIPPING.key(), "true").format("hudi").load(outputPath)\n\nval dataSkippingSnapshotTableName = safeTableName(s"${tableName}_sql_snapshot_skipping")\n\nreadDfSkip.createOrReplaceTempView(dataSkippingSnapshotTableName)\n\n// Query 1: Total votes by product_category, for 6 months\ndef runQuery1(tableName: String) = {\n  // Query 1: Total votes by product_category, for 6 months\n  spark.sql(s"SELECT sum(total_votes), product_category FROM $tableName WHERE review_date > \'2013-12-15\' AND review_date < \'2014-06-01\' GROUP BY product_category").show()\n}\n\n// Query 2: Average star rating by product_id, for some product\ndef runQuery2(tableName: String) = {\n  spark.sql(s"SELECT avg(star_rating), product_id FROM $tableName WHERE product_id in (\'B0184XC75U\') GROUP BY product_id").show()\n}\n\n// Query 3: Count number of reviews by customer_id for some 5 customers\ndef runQuery3(tableName: String) = {\n  spark.sql(s"SELECT count(*) as num_reviews, customer_id FROM $tableName WHERE customer_id in (\'53096570\',\'10046284\',\'53096576\',\'10000196\',\'21700145\') GROUP BY customer_id").show()\n}\n\n//\n// Query 1: Is a "wide" query and hence it\'s expected to touch a lot of files\n//\nscala> runQuery1(rawSnapshotTableName)\n+----------------+--------------------+\n|sum(total_votes)|    product_category|\n+----------------+--------------------+\n|         1050944|                  PC|\n|          867794|             Kitchen|\n|         1167489|                Home|\n|          927531|            Wireless|\n|            6861|               Video|\n|           39602| Digital_Video_Games|\n|          954924|Digital_Video_Dow...|\n|           81876|             Luggage|\n|          320536|         Video_Games|\n|          817679|              Sports|\n|           11451|  Mobile_Electronics|\n|          228739|  Home_Entertainment|\n|         3769269|Digital_Ebook_Pur...|\n|          252273|                Baby|\n|          735042|             Apparel|\n|           49101|    Major_Appliances|\n|          484732|             Grocery|\n|          285682|               Tools|\n|          459980|         Electronics|\n|          454258|            Outdoors|\n+----------------+--------------------+\nonly showing top 20 rows\n\nscala> runQuery1(dataSkippingSnapshotTableName)\n+----------------+--------------------+\n|sum(total_votes)|    product_category|\n+----------------+--------------------+\n|         1050944|                  PC|\n|          867794|             Kitchen|\n|         1167489|                Home|\n|          927531|            Wireless|\n|            6861|               Video|\n|           39602| Digital_Video_Games|\n|          954924|Digital_Video_Dow...|\n|           81876|             Luggage|\n|          320536|         Video_Games|\n|          817679|              Sports|\n|           11451|  Mobile_Electronics|\n|          228739|  Home_Entertainment|\n|         3769269|Digital_Ebook_Pur...|\n|          252273|                Baby|\n|          735042|             Apparel|\n|           49101|    Major_Appliances|\n|          484732|             Grocery|\n|          285682|               Tools|\n|          459980|         Electronics|\n|          454258|            Outdoors|\n+----------------+--------------------+\nonly showing top 20 rows\n\n//\n// Query 2: Is a "pointwise" query and hence it\'s expected that data-skipping should substantially reduce number \n// of files scanned (as compared to Baseline)\n//\n// NOTE: That Linear Ordering (as compared to Space-curve based on) will have similar effect on performance reducing\n// total # of Parquet files scanned, since we\'re querying on the prefix of the ordering key\n//\nscala> runQuery2(rawSnapshotTableName)\n+----------------+----------+\n|avg(star_rating)|product_id|\n+----------------+----------+\n|             1.0|B0184XC75U|\n+----------------+----------+\n\n\nscala> runQuery2(dataSkippingSnapshotTableName)\n+----------------+----------+\n|avg(star_rating)|product_id|\n+----------------+----------+\n|             1.0|B0184XC75U|\n+----------------+----------+\n\n//\n// Query 3: Similar to Q2, is a "pointwise" query, but querying other part of the ordering-key (product_id, customer_id)\n// and hence it\'s expected that data-skipping should substantially reduce number of files scanned (as compared to Baseline, Linear Ordering).\n//\n// NOTE: That Linear Ordering (as compared to Space-curve based on) will _NOT_ have similar effect on performance reducing\n// total # of Parquet files scanned, since we\'re NOT querying on the prefix of the ordering key\n//\nscala> runQuery3(rawSnapshotTableName)\n+-----------+-----------+\n|num_reviews|customer_id|\n+-----------+-----------+\n|         50|   53096570|\n|          3|   53096576|\n|         25|   10046284|\n|          1|   10000196|\n|         14|   21700145|\n+-----------+-----------+\n\nscala> runQuery3(dataSkippingSnapshotTableName)\n+-----------+-----------+\n|num_reviews|customer_id|\n+-----------+-----------+\n|         50|   53096570|\n|          3|   53096576|\n|         25|   10046284|\n|          1|   10000196|\n|         14|   21700145|\n+-----------+-----------+\n'})}),"\n",(0,n.jsx)(a.h3,{id:"results",children:"Results"}),"\n",(0,n.jsx)(a.p,{children:"We've summarized the measured performance metrics below:"}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{children:(0,n.jsx)(a.strong,{children:"Query"})}),(0,n.jsxs)(a.th,{children:[(0,n.jsx)(a.strong,{children:"Baseline (B)"})," duration (files scanned / size)"]}),(0,n.jsx)(a.th,{children:(0,n.jsx)(a.strong,{children:"Linear Sorting (S)"})}),(0,n.jsxs)(a.th,{children:[(0,n.jsx)(a.strong,{children:"Z-order (Z)"})," duration (scanned)"]}),(0,n.jsxs)(a.th,{children:[(0,n.jsx)(a.strong,{children:"Hilbert (H)"})," duration (scanned)"]})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"Q1"}),(0,n.jsx)(a.td,{children:"14s (543 / 31.4Gb)"}),(0,n.jsx)(a.td,{children:"15s (533 / 28.8Gb)"}),(0,n.jsx)(a.td,{children:"15s (543 / 31.4Gb)"}),(0,n.jsx)(a.td,{children:"14s (541 / 31.3Gb)"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"Q2"}),(0,n.jsx)(a.td,{children:"21s (543 / 31.4Gb)"}),(0,n.jsx)(a.td,{children:"10s (533 / 28.8Gb)"}),(0,n.jsxs)(a.td,{children:[(0,n.jsx)(a.strong,{children:"8s"})," ",(0,n.jsx)(a.strong,{children:"(243 / 14.4Gb)"})]}),(0,n.jsxs)(a.td,{children:[(0,n.jsx)(a.strong,{children:"7s"})," ",(0,n.jsx)(a.strong,{children:"(237 / 13.9Gb)"})]})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"Q3"}),(0,n.jsx)(a.td,{children:"17s (543 / 31.4Gb)"}),(0,n.jsx)(a.td,{children:"15s (533 / 28.8Gb)"}),(0,n.jsxs)(a.td,{children:[(0,n.jsx)(a.strong,{children:"6s"})," ",(0,n.jsx)(a.strong,{children:"(224 / 12.4Gb)"})]}),(0,n.jsxs)(a.td,{children:[(0,n.jsx)(a.strong,{children:"6s"})," ",(0,n.jsx)(a.strong,{children:"(219 / 11.9Gb)"})]})]})]})]}),"\n",(0,n.jsx)(a.p,{children:"As you can see multi-column linear ordering is not very effective for the queries that do filtering by columns other than the first one (Q2, Q3)."}),"\n",(0,n.jsxs)(a.p,{children:["Which is a very clear contrast with space-filling curves (both Z-order and Hilbert) that allow to speed up query time by up to ",(0,n.jsx)(a.strong,{children:"3x!"})]}),"\n",(0,n.jsxs)(a.p,{children:["It's worth noting that the performance gains are heavily dependent on your underlying data and queries. In benchmarks on our internal data we were able to achieve queries performance improvements of more than ",(0,n.jsx)(a.strong,{children:"11x!"})]}),"\n",(0,n.jsx)(a.h3,{id:"epilogue",children:"Epilogue"}),"\n",(0,n.jsx)(a.p,{children:"Apache Hudi v0.10 brings new layout optimization capabilities Z-order and Hilbert to open source. Using these industry leading layout optimization techniques can bring substantial performance improvement and cost savings to your queries!"})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},69342:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/08/23/s3-events-source","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-08-23-s3-events-source.md","source":"@site/blog/2021-08-23-s3-events-source.md","title":"Reliable ingestion from AWS S3 using Hudi","description":"In this post we will talk about a new deltastreamer source which reliably and efficiently processes new data files as they arrive in AWS S3.","date":"2021-08-23T00:00:00.000Z","tags":[{"inline":true,"label":"design","permalink":"/blog/tags/design"},{"inline":true,"label":"deltastreamer","permalink":"/blog/tags/deltastreamer"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":6.7,"hasTruncateMarker":true,"authors":[{"name":"codope","key":null,"page":null}],"frontMatter":{"title":"Reliable ingestion from AWS S3 using Hudi","excerpt":"From listing to log-based approach, a reliable way of ingesting data from AWS S3 into Hudi.","author":"codope","category":"blog","image":"/assets/images/blog/s3_events_source_design.png","tags":["design","deltastreamer","apache hudi"]},"unlisted":false,"prevItem":{"title":"Asynchronous Clustering using Hudi","permalink":"/blog/2021/08/23/async-clustering"},"nextItem":{"title":"Improving Marker Mechanism in Apache Hudi","permalink":"/blog/2021/08/18/improving-marker-mechanism"}}')},69411:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/10/05/Data-Platform-2.0-Part-I","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-10-05-Data-Platform-2.0-Part-I.mdx","source":"@site/blog/2021-10-05-Data-Platform-2.0-Part-I.mdx","title":"Data Platform 2.0 - Part I","description":"Redirecting... please wait!!","date":"2021-10-05T00:00:00.000Z","tags":[{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"halodoc","permalink":"/blog/tags/halodoc"},{"inline":true,"label":"datalake","permalink":"/blog/tags/datalake"},{"inline":true,"label":"datalake platform","permalink":"/blog/tags/datalake-platform"}],"readingTime":0.11,"hasTruncateMarker":false,"authors":[{"name":"Jitendra Shah","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Data Platform 2.0 - Part I","authors":[{"name":"Jitendra Shah"}],"category":"blog","image":"/assets/images/blog/2021-10-05-data-platform-2-0-part-1.png","tags":["use-case","halodoc","datalake","datalake platform"]},"unlisted":false,"prevItem":{"title":"How Amazon Transportation Service enabled near-real-time event analytics at petabyte scale using AWS Glue with Apache Hudi","permalink":"/blog/2021/10/14/How-Amazon-Transportation-Service-enabled-near-real-time-event-analytics-at-petabyte-scale-using-AWS-Glue-with-Apache-Hudi"},"nextItem":{"title":"Building an ExaByte-level Data Lake Using Apache Hudi at ByteDance","permalink":"/blog/2021/09/01/building-eb-level-data-lake-using-hudi-at-bytedance"}}')},69509:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image6-3f3cbe07ee5f79cce9b6f18241058961.png"},69691:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/08/11/Cost-Efficient-Open-Source-Big-Data-Platform-at-Uber","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-08-11-Cost-Efficient-Open-Source-Big-Data-Platform-at-Uber.mdx","source":"@site/blog/2021-08-11-Cost-Efficient-Open-Source-Big-Data-Platform-at-Uber.mdx","title":"Cost-Efficient Open Source Big Data Platform at Uber","description":"Redirecting... please wait!!","date":"2021-08-11T00:00:00.000Z","tags":[{"inline":true,"label":"cost efficiency","permalink":"/blog/tags/cost-efficiency"},{"inline":true,"label":"optimization","permalink":"/blog/tags/optimization"},{"inline":true,"label":"bigdata","permalink":"/blog/tags/bigdata"},{"inline":true,"label":"data platform","permalink":"/blog/tags/data-platform"},{"inline":true,"label":"incremental processing","permalink":"/blog/tags/incremental-processing"},{"inline":true,"label":"uber","permalink":"/blog/tags/uber"}],"readingTime":0.1,"hasTruncateMarker":false,"authors":[{"name":"Zheng Shao","socials":{},"key":null,"page":null},{"name":"Mohammad Islam","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Cost-Efficient Open Source Big Data Platform at Uber","authors":[{"name":"Zheng Shao"},{"name":"Mohammad Islam"}],"category":"blog","image":"/assets/images/blog/2021-08-11-cost-efficient-open-source-big-data-platform-at-uber.png","tags":["cost efficiency","optimization","bigdata","data platform","incremental processing","uber"]},"unlisted":false,"prevItem":{"title":"Schema evolution with DeltaStreamer using KafkaSource","permalink":"/blog/2021/08/16/kafka-custom-deserializer"},"nextItem":{"title":"MLOps Wars: Versioned Feature Data with a Lakehouse","permalink":"/blog/2021/08/03/MLOps-Wars-Versioned-Feature-Data-with-a-Lakehouse"}}')},69777:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/11/01/record-level-index","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-11-01-record-level-index.md","source":"@site/blog/2023-11-01-record-level-index.md","title":"Record Level Index: Hudi\'s blazing fast indexing for large-scale datasets","description":"Introduction","date":"2023-11-01T00:00:00.000Z","tags":[{"inline":true,"label":"design","permalink":"/blog/tags/design"},{"inline":true,"label":"indexing","permalink":"/blog/tags/indexing"},{"inline":true,"label":"metadata","permalink":"/blog/tags/metadata"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"}],"readingTime":11.82,"hasTruncateMarker":false,"authors":[{"name":"Shiyan Xu and Sivabalan Narayanan","key":null,"page":null}],"frontMatter":{"title":"Record Level Index: Hudi\'s blazing fast indexing for large-scale datasets","excerpt":"Announcing the Record Level Index in Apache Hudi","author":"Shiyan Xu and Sivabalan Narayanan","category":"blog","image":"/assets/images/blog/record-level-index/03.RLI_bulkinsert.png","tags":["design","indexing","metadata","apache hudi","blog"]},"unlisted":false,"prevItem":{"title":"Apache Hudi: From Zero To One (6/10)","permalink":"/blog/2023/11/13/Apache-Hudi-From-Zero-To-One-blog-6"},"nextItem":{"title":"UPSERT Performance Evaluation of Hudi 0.14 and Spark 3.4.1: Record Level Index vs. Global Bloom & Global Simple Indexes","permalink":"/blog/2023/10/29/UPSERT-Performance-Evaluation-of-Hudi-0-14-and-Spark-3-4-1-Record-Level-Index-Global-Bloom-Global-Simple-Indexes"}}')},69800:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/01/02/Build-a-federated-query-solution-with-Apache-Doris-Apache-Flink-and-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-01-02-Build-a-federated-query-solution-with-Apache-Doris-Apache-Flink-and-Apache-Hudi.mdx","source":"@site/blog/2024-01-02-Build-a-federated-query-solution-with-Apache-Doris-Apache-Flink-and-Apache-Hudi.mdx","title":"Build a federated query solution with Apache Doris, Apache Flink, and Apache Hudi","description":"Redirecting... please wait!!","date":"2024-01-02T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"dev to","permalink":"/blog/tags/dev-to"},{"inline":true,"label":"beginner","permalink":"/blog/tags/beginner"},{"inline":true,"label":"apache doris","permalink":"/blog/tags/apache-doris"},{"inline":true,"label":"apache flink","permalink":"/blog/tags/apache-flink"}],"readingTime":0.15,"hasTruncateMarker":false,"authors":[{"name":"Apache Doris","key":null,"page":null}],"frontMatter":{"title":"Build a federated query solution with Apache Doris, Apache Flink, and Apache Hudi","excerpt":"Build a federated query solution with Apache Doris, Apache Flink, and Apache Hudi","author":"Apache Doris","category":"blog","image":"/assets/images/blog/2024-01-02-Build-a-federated-query-solution-with-Apache-Doris-Apache-Flink-and-Apache-Hudi.png","tags":["blog","apache hudi","dev to","beginner","apache doris","apache flink"]},"unlisted":false,"prevItem":{"title":"Small Talk about Apache Hudi","permalink":"/blog/2024/01/05/Small-Talk-about-Apache-Hudi"},"nextItem":{"title":"From Data lake to Microservices: Unleashing the Power of Apache Hudi\'s Record Level Index with FastAPI and Spark Connect","permalink":"/blog/2024/01/01/From-Data-lake-to-Microservices-Unleashing-the-Power-of-Apache-Hudi-Record-Level-Index-with-FastAPI-and-Spark-Connect"}}')},69856:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2019/10/22/Hudi-On-Hops","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2019-10-22-Hudi-On-Hops.mdx","source":"@site/blog/2019-10-22-Hudi-On-Hops.mdx","title":"Hudi On Hops","description":"Redirecting... please wait!!","date":"2019-10-22T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"diva-portal","permalink":"/blog/tags/diva-portal"}],"readingTime":0.11,"hasTruncateMarker":false,"authors":[{"name":"NETSANET GEBRETSADKAN KIDANE","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Hudi On Hops","authors":[{"name":"NETSANET GEBRETSADKAN KIDANE"}],"category":"blog","tags":["blog","diva-portal"]},"unlisted":false,"prevItem":{"title":"New \u2013 Insert, Update, Delete Data on S3 with Amazon EMR and Apache Hudi","permalink":"/blog/2019/11/15/New-Insert-Update-Delete-Data-on-S3-with-Amazon-EMR-and-Apache-Hudi"},"nextItem":{"title":"Ingesting Database changes via Sqoop/Hudi","permalink":"/blog/2019/09/09/ingesting-database-changes"}}')},69973:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(47354),n=t(74848),s=t(28453);const r={title:"Improving Marker Mechanism in Apache Hudi",excerpt:"We introduce a new marker mechanism leveraging the timeline server to address performance bottlenecks due to rate-limiting on cloud storage like AWS S3.",author:"yihua",category:"blog",image:"/assets/images/blog/marker-mechanism/timeline-server-based-marker-mechanism.png",tags:["design","timeline-server","markers","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Need for Markers during Write Operations",id:"need-for-markers-during-write-operations",level:2},{value:"Existing Direct Marker Mechanism and its limitations",id:"existing-direct-marker-mechanism-and-its-limitations",level:2},{value:"Timeline-server-based marker mechanism improving write performance",id:"timeline-server-based-marker-mechanism-improving-write-performance",level:2},{value:"Marker-related write options",id:"marker-related-write-options",level:2},{value:"Performance",id:"performance",level:2},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const a={a:"a",code:"code",h2:"h2",img:"img",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.p,{children:"Hudi supports fully automatic cleanup of uncommitted data on storage during its write operations. Write operations in an Apache Hudi table use markers to efficiently track the data files written to storage. In this blog, we dive into the design of the existing direct marker file mechanism and explain its performance problems on cloud storage like AWS S3 for\nvery large writes. We demonstrate how we improve write performance with introduction of timeline-server-based markers."}),"\n",(0,n.jsx)(a.h2,{id:"need-for-markers-during-write-operations",children:"Need for Markers during Write Operations"}),"\n",(0,n.jsxs)(a.p,{children:["A ",(0,n.jsx)(a.strong,{children:"marker"})," in Hudi, such as a marker file with a unique filename, is a label to indicate that a corresponding data file exists in storage, which then Hudi\nuses to automatically clean up uncommitted data during failure and rollback scenarios. Each marker entry is composed of three parts, the data file name,\nthe marker extension (",(0,n.jsx)(a.code,{children:".marker"}),"), and the I/O operation created the file (",(0,n.jsx)(a.code,{children:"CREATE"})," - inserts, ",(0,n.jsx)(a.code,{children:"MERGE"})," - updates/deletes, or ",(0,n.jsx)(a.code,{children:"APPEND"})," - either). For example, the marker ",(0,n.jsx)(a.code,{children:"91245ce3-bb82-4f9f-969e-343364159174-0_140-579-0_20210820173605.parquet.marker.CREATE"})," indicates\nthat the corresponding data file is ",(0,n.jsx)(a.code,{children:"91245ce3-bb82-4f9f-969e-343364159174-0_140-579-0_20210820173605.parquet"})," and the I/O type is ",(0,n.jsx)(a.code,{children:"CREATE"}),". Hudi creates a marker before creating the corresponding data file in the file system and deletes all markers pertaining to a commit when it succeeds."]}),"\n",(0,n.jsx)(a.p,{children:"The markers are useful for efficiently carrying out different operations by the write client.  Markers serve as a way to track data files of interest rather than scanning the whole Hudi table by listing all files in the table.  Two important operations use markers which come in handy to find uncommitted data files of interest efficiently:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Removing duplicate/partial data files"}),": in Spark, the Hudi write client delegates the data file writing to multiple executors.  One executor can fail the task, leaving partial data files written, and Spark retries the task in this case until it succeeds. When speculative execution is enabled, there can also be multiple successful attempts at writing out the same data into different files, only one of which is finally handed to the Spark driver process for committing. The markers help efficiently identify the partial data files written, which contain duplicate data compared to the data files written by the successful trial later, and these duplicate data files are cleaned up when the commit is finalized.  If there are no such marker to track the per-commit data files, we have to list all files in the file system, correlate that with the files seen in timeline and then delete the ones that belong to partial write failures.  As you could imagine, this would be very costly in a very large installation of a datalake."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Rolling back failed commits"}),": the write operation can fail in the middle, leaving some data files written in storage.  In this case, the marker entries stay in storage as the commit is failed.  In the next write operation, the write client rolls back the failed commit before proceeding with the new write. The rollback is done with the help of markers to identify the data files written as part of the failed commit."]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Next, we dive into the existing marker mechanism, explain its performance problem, and demonstrate the new timeline-server-based marker mechanism to address the problem."}),"\n",(0,n.jsx)(a.h2,{id:"existing-direct-marker-mechanism-and-its-limitations",children:"Existing Direct Marker Mechanism and its limitations"}),"\n",(0,n.jsxs)(a.p,{children:["The ",(0,n.jsx)(a.strong,{children:"existing marker mechanism"})," simply creates a new marker file corresponding to each data file, with the marker filename as described above.  The marker file does not have any content, i.e., empty.  Each marker file is written to storage in the same directory hierarchy, i.e., commit instant and partition path, under a temporary folder ",(0,n.jsx)(a.code,{children:".hoodie/.temp"})," under the base path of the Hudi table.  For example, the figure below shows one example of the marker files created and the corresponding data files when writing data to the Hudi table.  When getting or deleting all the marker file paths, the mechanism first lists all the paths under the temporary folder, ",(0,n.jsx)(a.code,{children:".hoodie/.temp/<commit_instant>"}),", and then does the operation."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"An example of marker and data files in direct marker file mechanism",src:t(36995).A+"",width:"3440",height:"1444"})}),"\n",(0,n.jsxs)(a.p,{children:["While it's much efficient over scanning the entire table for uncommitted data files, as the number of data files to write increases, so does the number of marker files to create.  For large writes which need to write significant number of data files, e.g., 10K or more, this can create performance bottlenecks for cloud storage such as AWS S3.  In AWS S3, each file create and delete call triggers an HTTP request and there is ",(0,n.jsx)(a.a,{href:"https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html",children:"rate-limiting"})," on how many requests can be processed per second per prefix in a bucket.  When the number of data files to write concurrently and the number of marker files is huge, the marker file operations could take up non-trivial time during the write operation, sometimes on the order of a few minutes or more.  Users may barely notice this on a storage like HDFS, where the file system metadata is efficiently cached in memory."]}),"\n",(0,n.jsx)(a.h2,{id:"timeline-server-based-marker-mechanism-improving-write-performance",children:"Timeline-server-based marker mechanism improving write performance"}),"\n",(0,n.jsxs)(a.p,{children:["To address the performance bottleneck due to rate-limiting of AWS S3 explained above, we introduce a ",(0,n.jsx)(a.strong,{children:"new marker mechanism leveraging the timeline server"}),", which optimizes the marker-related latency for storage with non-trivial file I/O latency.  The ",(0,n.jsx)(a.strong,{children:"timeline server"})," in Hudi serves as a centralized place for providing the file system and timeline views. As shown below, the new timeline-server-based marker mechanism delegates the marker creation and other marker-related operations from individual executors to the timeline server for centralized processing.  The timeline server batches the marker creation requests and writes the markers to a bounded set of files in the file system at regular intervals.  In such a way, the number of actual file operations and latency related to markers can be significantly reduced even with a huge number of data files, thus improving the performance of the writes."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Timeline-server-based marker mechanism",src:t(56542).A+"",width:"1200",height:"432"})}),"\n",(0,n.jsx)(a.p,{children:"To improve the efficiency of processing marker creation requests, we design the batched handling of marker requests at the timeline server. Each marker creation request is handled asynchronously in the Javalin timeline server and queued before processing. For every batch interval, e.g., 20ms, the timeline server pulls the pending marker creation requests from the queue and writes all markers to the next file in a round robin fashion.  Inside the timeline server, such batch processing is multi-threaded, designed and implemented to guarantee consistency and correctness.  Both the batch interval and the batch concurrency can be configured through the write options."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Batched processing of marker creation requests",src:t(78660).A+"",width:"3184",height:"1168"})}),"\n",(0,n.jsx)(a.p,{children:"Note that the worker thread always checks whether the marker has already been created by comparing the marker name from the request with the memory copy of all markers maintained at the timeline server. The underlying files storing the markers are only read upon the first marker request (lazy loading).  The responses of requests are only sent back once the new markers are flushed to the files, so that in the case of the timeline server failure, the timeline server can recover the already created markers. These ensure consistency between storage and the in-memory copy, and improve the performance of processing marker requests."}),"\n",(0,n.jsx)(a.h2,{id:"marker-related-write-options",children:"Marker-related write options"}),"\n",(0,n.jsxs)(a.p,{children:["We introduce the following new marker-related write options in ",(0,n.jsx)(a.code,{children:"0.9.0"})," release, to configure the marker mechanism.  Note that the timeline-server-based marker mechanism is not yet supported for HDFS in ",(0,n.jsx)(a.code,{children:"0.9.0"})," release, and we plan to support the timeline-server-based marker mechanism for HDFS in the future."]}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{children:"Property Name"}),(0,n.jsx)(a.th,{children:"Default"}),(0,n.jsx)(a.th,{style:{textAlign:"center"},children:"Meaning"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.write.markers.type"})}),(0,n.jsx)(a.td,{children:"direct"}),(0,n.jsxs)(a.td,{style:{textAlign:"center"},children:["Marker type to use.  Two modes are supported: (1) ",(0,n.jsx)(a.code,{children:"direct"}),": individual marker file corresponding to each data file is directly created by the executor; (2) ",(0,n.jsx)(a.code,{children:"timeline_server_based"}),": marker operations are all handled at the timeline service which serves as a proxy.  New marker entries are batch processed and stored in a limited number of underlying files for efficiency."]})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.markers.timeline_server_based.batch.num_threads"})}),(0,n.jsx)(a.td,{children:"20"}),(0,n.jsx)(a.td,{style:{textAlign:"center"},children:"Number of threads to use for batch processing marker creation requests at the timeline server."})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:(0,n.jsx)(a.code,{children:"hoodie.markers.timeline_server_based.batch.interval_ms"})}),(0,n.jsx)(a.td,{children:"50"}),(0,n.jsx)(a.td,{style:{textAlign:"center"},children:"The batch interval in milliseconds for marker creation batch processing."})]})]})]}),"\n",(0,n.jsx)(a.h2,{id:"performance",children:"Performance"}),"\n",(0,n.jsx)(a.p,{children:"We evaluate the write performance over both direct and timeline-server-based marker mechanisms by bulk-inserting a large dataset using Amazon EMR with Spark and S3. The input data is around 100GB.  We configure the write operation to generate a large number of data files concurrently by setting the max parquet file size to be 1MB and parallelism to be 240.  Note that it is unlikely to set max parquet file size to 1MB in production and such a setup is only to evaluate the performance regarding the marker mechanisms. As we noted before, while the latency of direct marker mechanism is acceptable for incremental writes with smaller number of data files written, it increases dramatically for large bulk inserts/writes which produce much more data files."}),"\n",(0,n.jsx)(a.p,{children:"As shown below, direct marker mechanism works really well, when a part of the table is written, e.g., 1K out of 165K data files.  However, the time of direct marker operations is non-trivial when we need to write significant number of data files. Compared to the direct marker mechanism, the timeline-server-based marker mechanism generates much fewer files storing markers because of the batch processing, leading to much less time on marker-related I/O operations, thus achieving 31% lower write completion time compared to the direct marker file mechanism."}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{children:"Marker Type"}),(0,n.jsx)(a.th,{children:"Total Files"}),(0,n.jsx)(a.th,{style:{textAlign:"center"},children:"Num data files written"}),(0,n.jsx)(a.th,{style:{textAlign:"center"},children:"Files created for markers"}),(0,n.jsx)(a.th,{style:{textAlign:"center"},children:"Marker deletion time"}),(0,n.jsx)(a.th,{style:{textAlign:"center"},children:"Bulk Insert Time (including marker deletion)"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"Direct"}),(0,n.jsx)(a.td,{children:"165k"}),(0,n.jsx)(a.td,{style:{textAlign:"center"},children:"1k"}),(0,n.jsx)(a.td,{style:{textAlign:"center"},children:"1k"}),(0,n.jsx)(a.td,{style:{textAlign:"center"},children:"5.4secs"}),(0,n.jsx)(a.td,{style:{textAlign:"center"},children:"-"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"Direct"}),(0,n.jsx)(a.td,{children:"165k"}),(0,n.jsx)(a.td,{style:{textAlign:"center"},children:"165k"}),(0,n.jsx)(a.td,{style:{textAlign:"center"},children:"165k"}),(0,n.jsx)(a.td,{style:{textAlign:"center"},children:"15min"}),(0,n.jsx)(a.td,{style:{textAlign:"center"},children:"55min"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"Timeline-server-based"}),(0,n.jsx)(a.td,{children:"165k"}),(0,n.jsx)(a.td,{style:{textAlign:"center"},children:"165k"}),(0,n.jsx)(a.td,{style:{textAlign:"center"},children:"20"}),(0,n.jsx)(a.td,{style:{textAlign:"center"},children:"~3s"}),(0,n.jsx)(a.td,{style:{textAlign:"center"},children:"38min"})]})]})]}),"\n",(0,n.jsx)(a.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,n.jsx)(a.p,{children:"We identify that for large writes which need to write significant number of data files, the existing direct marker file mechanism can incur performance bottlenecks due to the rate-limiting of file create and delete calls on cloud storage like AWS S3.  To address this issue, we introduce a new marker mechanism leveraging the timeline server, which delegates the marker creation and other marker-related operations from individual executors to the timeline server and uses batch processing to improve performance.  Performance evaluations on Amazon EMR with Spark and S3 show that the marker-related I/O latency and overall write time are reduced."})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},70142:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig6-29897046b22cae13ecde0bad971f9544.png"},70163:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(7805),n=t(74848),s=t(28453),r=t(9230);const o={title:"Backfilling Apache Hudi Tables in Production: Techniques & Approaches Using AWS Glue by Job Target LLC",authors:[{name:"Soumil Shah"}],category:"blog",image:"/assets/images/blog/2023-07-20-Backfilling-Apache-Hudi-Tables-in-Production-Techniques-and-Approaches-Using-AWS-Glue-by-Job-Target-LLC.png",tags:["blog","backfilling","hudi","aws glue","code sample"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.linkedin.com/pulse/backfilling-apache-hudi-tables-production-techniques-approaches-shah",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},70413:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/08/05/Data-Lakehouse-Architecture-for-Big-Data-with-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-08-05-Data-Lakehouse-Architecture-for-Big-Data-with-Apache-Hudi.mdx","source":"@site/blog/2023-08-05-Data-Lakehouse-Architecture-for-Big-Data-with-Apache-Hudi.mdx","title":"Data Lakehouse Architecture for Big Data with Apache Hudi","description":"Redirecting... please wait!!","date":"2023-08-05T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"data lakehouse","permalink":"/blog/tags/data-lakehouse"},{"inline":true,"label":"big data","permalink":"/blog/tags/big-data"},{"inline":true,"label":"google scholar","permalink":"/blog/tags/google-scholar"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Tauno Treier","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Data Lakehouse Architecture for Big Data with Apache Hudi","authors":[{"name":"Tauno Treier"}],"category":"blog","image":"/assets/images/blog/2023-08-05-Data-Lakehouse-Architecture-for-Big-Data-with-Apache-Hudi.png","tags":["blog","apache hudi","data lakehouse","big data","google scholar"]},"unlisted":false,"prevItem":{"title":"Lakehouse Trifecta \u2014 Delta Lake, Apache Iceberg & Apache Hudi","permalink":"/blog/2023/08/09/Lakehouse-Trifecta-Delta-Lake-Apache-Iceberg-and-Apache-Hudi"},"nextItem":{"title":"Apache Hudi on AWS Glue: A Step-by-Step Guide","permalink":"/blog/2023/08/03/Apache-Hudi-on-AWS-Glue-A-Step-by-Step-Guide"}}')},70484:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(97117),n=t(74848),s=t(28453);const r={title:"Big Batch vs Incremental Processing",author:"vinoth",category:"blog",image:"/assets/images/blog/batch_vs_incremental.png"},o=void 0,l={authorsImageUrls:[void 0]},d=[];function c(e){const a={img:"img",p:"p",...(0,s.R)(),...e.components};return(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{src:t(84876).A+"",width:"1200",height:"376"})})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},70606:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(6056),n=t(74848),s=t(28453),r=t(9230);const o={title:"New features from Apache Hudi 0.9.0 on Amazon EMR",authors:[{name:"Kunal Gautam"},{name:"Gabriele Cacciola"},{name:"Udit Mehrotra"}],category:"blog",image:"/assets/images/blog/aws.jpg",tags:["blog","amazon"]},l=void 0,d={authorsImageUrls:[void 0,void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/new-features-from-apache-hudi-0-9-0-on-amazon-emr/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},70755:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/02/23/curious-engineering-facts-lakehouse-apache-hudi-daft-positional-argument","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-02-23-curious-engineering-facts-lakehouse-apache-hudi-daft-positional-argument.mdx","source":"@site/blog/2025-02-23-curious-engineering-facts-lakehouse-apache-hudi-daft-positional-argument.mdx","title":"Curious Engineering Facts (Lakehouse | Apache Hudi | Daft |Positional argument|) : March Release 19 : 25","description":"Redirecting... please wait!!","date":"2025-02-23T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"daft","permalink":"/blog/tags/daft"},{"inline":true,"label":"streamlit","permalink":"/blog/tags/streamlit"},{"inline":true,"label":"cow","permalink":"/blog/tags/cow"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Gayan Sanjeewa","key":null,"page":null}],"frontMatter":{"title":"Curious Engineering Facts (Lakehouse | Apache Hudi | Daft |Positional argument|) : March Release 19 : 25","author":"Gayan Sanjeewa","category":"blog","image":"/assets/images/blog/2025-02-23-curious-engineering-facts-lakehouse-apache-hudi-daft-positional-argument.jpeg","tags":["blog","apache hudi","daft","streamlit","cow","medium"]},"unlisted":false,"prevItem":{"title":"Building a Lakehouse Architecture on AWS with Terraform","permalink":"/blog/2025/02/24/building-a-lakehouse-architecture-on-aws-with-terraform"},"nextItem":{"title":"An intro to Hudi with MinIO","permalink":"/blog/2025/01/30/an-intro-to-hudi-with-minio"}}')},70864:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(62784),n=t(74848),s=t(28453),r=t(9230);const o={title:"Optimizing Apache Hudi Workflows: Automation for Clustering, Resizing & Concurrency",author:"Halodoc, Apache Hudi",category:"blog",image:"/assets/images/blog/2025-06-13-Optimizing-Apache-Hudi-Workflows-Automation-for-Clustering-Resizing-Concurrency.png",tags:["hudi","blog","halodoc","data lake","lakehouse","apache hudi"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blogs.halodoc.io/optimizing-apache-hudi-workflows-automation-for-clustering-resizing-concurrency/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},71085:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/06/24/multi-writer-support-in-apache-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-06-24-multi-writer-support-in-apache-hudi.mdx","source":"@site/blog/2023-06-24-multi-writer-support-in-apache-hudi.mdx","title":"Multi-writer support with Apache Hudi","description":"Redirecting... please wait!!","date":"2023-06-24T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"concurrency control","permalink":"/blog/tags/concurrency-control"},{"inline":true,"label":"lock provider","permalink":"/blog/tags/lock-provider"},{"inline":true,"label":"multi writer","permalink":"/blog/tags/multi-writer"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Sivabalan Narayanan","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Multi-writer support with Apache Hudi","authors":[{"name":"Sivabalan Narayanan"}],"category":"blog","image":"/assets/images/blog/2023-06-24-multi-writer-support-in-apache-hudi.png","tags":["blog","concurrency control","lock provider","multi writer","medium"]},"unlisted":false,"prevItem":{"title":"Unlimited Big Data Exchange: A Wonderful Review of Apache DolphinScheduler & Hudi Hangzhou Meetup","permalink":"/blog/2023/06/26/Unlimited-Big-Data-Exchange-A-Wonderful-Review-of-Apache-DolphinScheduler-and-Hudi-Hangzhou-Meetup"},"nextItem":{"title":"How to query data in Apache Hudi using StarRocks","permalink":"/blog/2023/06/20/How-to-query-data-in-Apache-Hudi-using-StarRocks"}}')},71167:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(80219),n=t(74848),s=t(28453),r=t(9230);const o={title:"Developer Guide: How to Submit Hudi PySpark(Python) Jobs to EMR Serverless (7.1.0) with AWS Glue Hive MetaStore",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2024-09-04-developer-guide-how-to-submit-hudi-pyspark-python-jobs-to-emr-serverless.png",tags:["blog","apache hudi","pyspark","python","amazon emr","aws glue","linkedin"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.linkedin.com/pulse/developer-guide-how-submit-hudi-pysparkpython-jobs-emr-soumil-shah-bdfne/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},71457:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(14054),n=t(74848),s=t(28453),r=t(9230);const o={title:"Data Engineering: Bootstrapping Data lake with Apache Hudi",excerpt:"Data Engineering: Bootstrapping Data lake with Apache Hudi",author:"Krishna Prasad",category:"blog",image:"/assets/images/blog/2024-01-20-Data-Engineering-Bootstrapping-Data-lake-with-Apache-Hudi.png",tags:["blog","apache hudi","medium","beginner","ETL","aws glue","apache spark","aws s3"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@krishnaiitd/data-engineering-bootstrapping-data-lake-with-apache-hudi-b3323dff65fa",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},71611:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/12/29/hudi-zorder-and-hilbert-space-filling-curves","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-12-29-hudi-zorder-and-hilbert-space-filling-curves.md","source":"@site/blog/2021-12-29-hudi-zorder-and-hilbert-space-filling-curves.md","title":"Hudi Z-Order and Hilbert Space Filling Curves","description":"As of Hudi v0.10.0, we are excited to introduce support for an advanced Data Layout Optimization technique known in the database realm as Z-order and Hilbert space filling curves.","date":"2021-12-29T00:00:00.000Z","tags":[{"inline":true,"label":"design","permalink":"/blog/tags/design"},{"inline":true,"label":"clustering","permalink":"/blog/tags/clustering"},{"inline":true,"label":"data skipping","permalink":"/blog/tags/data-skipping"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":8.63,"hasTruncateMarker":true,"authors":[{"name":"Alexey Kudinkin and Tao Meng","key":null,"page":null}],"frontMatter":{"title":"Hudi Z-Order and Hilbert Space Filling Curves","excerpt":"Explore the benefits of new Apache Hudi Z-Order and Hilbert Curves","author":"Alexey Kudinkin and Tao Meng","category":"blog","image":"/assets/images/zordercurve.png","tags":["design","clustering","data skipping","apache hudi"]},"unlisted":false,"prevItem":{"title":"The Art of Building Open Data Lakes with Apache Hudi, Kafka, Hive, and Debezium","permalink":"/blog/2021/12/31/The-Art-of-Building-Open-Data-Lakes-with-Apache-Hudi-Kafka-Hive-and-Debezium"},"nextItem":{"title":"New features from Apache Hudi 0.7.0 and 0.8.0 available on Amazon EMR","permalink":"/blog/2021/12/20/New-features-from-Apache-Hudi-0.7.0-and-0.8.0-available-on-Amazon-EMR"}}')},71749:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(32753),n=t(74848),s=t(28453);const r={title:"Introducing Hudi's Non-blocking Concurrency Control for streaming, high-frequency writes",excerpt:"Announcing the Non-blocking Concurrency Control in Apache Hudi",author:"Danny Chan",category:"blog",image:"/assets/images/blog/non-blocking-concurrency-control/lsm_archive_timeline.png",tags:["design","streaming ingestion","multi-writer","concurrency-control","blog"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Introduction",id:"introduction",level:2},{value:"Older Design",id:"older-design",level:2},{value:"NBCC Design",id:"nbcc-design",level:2},{value:"True Time API",id:"true-time-api",level:3},{value:"LSM timeline",id:"lsm-timeline",level:3},{value:"Flink SQL demo",id:"flink-sql-demo",level:2},{value:"Future Roadmap",id:"future-roadmap",level:2}];function c(e){const a={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",hr:"hr",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.h2,{id:"introduction",children:"Introduction"}),"\n",(0,n.jsx)(a.p,{children:"In streaming ingestion scenarios, there are plenty of use cases that require concurrent ingestion from multiple streaming sources.\nThe user can union all the upstream source inputs into one downstream table to collect the records for unified access across federated queries.\nAnother very common scenario is multiple stream sources joined together to supplement dimensions of the records to build a wide-dimension table where each source\nstream is taking records with partial table schema fields. Common and strong demand for multi-stream concurrent ingestion has always been there.\nThe Hudi community has collected so many feedbacks from users ever since the day Hudi supported streaming ingestion and processing."}),"\n",(0,n.jsxs)(a.p,{children:["Starting from ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-1.0.0",children:"Hudi 1.0.0"}),", we are thrilled to announce a new general-purpose\nconcurrency model for Apache Hudi - the Non-blocking Concurrency Control (NBCC)- aimed at the stream processing or high-contention/frequent writing scenarios.\nIn contrast to ",(0,n.jsx)(a.a,{href:"/blog/2021/12/16/lakehouse-concurrency-control-are-we-too-optimistic/",children:"Optimistic Concurrency Control"}),", where writers abort the transaction\nif there is a hint of contention, this innovation allows multiple streaming writes to the same Hudi table without any overhead of conflict resolution, while\nkeeping the semantics of ",(0,n.jsx)(a.a,{href:"https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/",children:"event-time ordering"})," found in streaming systems, along with\nasynchronous table service such as compaction, archiving and cleaning."]}),"\n",(0,n.jsx)(a.p,{children:"NBCC works seamlessly without any new infrastructure or operational overhead. In the subsequent sections of this blog, we will give a brief introduction to Hudi's internals\nabout the data file layout and TrueTime semantics for time generation, a pre-requisite for discussing NBCC. Following that, we will delve into the design and workflows of NBCC,\nand then a simple SQL demo to show the NBCC related config options. The blog will conclude with insights into future work for NBCC."}),"\n",(0,n.jsx)(a.h2,{id:"older-design",children:"Older Design"}),"\n",(0,n.jsxs)(a.p,{children:["It's important to understand the Hudi ",(0,n.jsx)(a.a,{href:"/docs/next/storage_layouts",children:"storage layout"})," and it evolves/manages data versions. In older release before 1.0.0,\nHudi organizes the data files with units as ",(0,n.jsx)(a.code,{children:"FileGroup"}),". Each file group contains multiple ",(0,n.jsx)(a.code,{children:"FileSlice"}),"s. Every compaction on this file group generates a new file slice.\nEach file slice may comprise an optional base file(columnar file format like Apache Parquet or ORC) and multiple log files(row file format in Apache Avro or Parquet)."]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/non-blocking-concurrency-control/legacy_file_layout.png",alt:"Legacy file layout",width:"800",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:'The timestamp in the base file name is the instant time of the compaction that writes it, it is also called as "requested instant time" in Hudi\'s notion.\nThe timestamp in the log file name is the same timestamp as the current file slice base instant time. Data files with the same instant time belong to one file slice.\nIn effect, a file group represented a linear ordered sequence of base files (checkpoints) followed by logs files (deltas), followed by base files (checkpoints).'}),"\n",(0,n.jsx)(a.p,{children:"The instant time naming convention in log files becomes a hash limitation in concurrency mode. Each log file contains incremental changes from\nmultiple commits. Each writer needs to query the file layout to get the base instant time and figure out the full file name before flushing the records.\nA more severe problem is the base instant time can be variable with the async compaction pushing forward. In order to make the base instant time deterministic for the log writers, Hudi\nforces the schedule sequence between a write commit and compaction scheduling: a compaction can be scheduled only if there is no ongoing ingestion into the Hudi table. Without this, a log file\ncan be written with a wrong base instant time which could introduce data loss. This means a compaction scheduling could block all the writers in concurrency mode."}),"\n",(0,n.jsx)(a.h2,{id:"nbcc-design",children:"NBCC Design"}),"\n",(0,n.jsxs)(a.p,{children:["In order to resolve these pains, since 1.0.0, Hudi introduces a new storage layout based on both requested and completion times of actions, viewing them as an interval.\nEach commit in 1.x Hudi has two ",(0,n.jsx)(a.a,{href:"/docs/next/timeline",children:"important notions of time"}),": instant time(or requested time) and completion time.\nAll the generated timestamp are globally monotonically increasing. Instead of putting the base instant time in the log file name, Hudi now just uses the requested instant time\nof the write. During file slicing, Hudi queries the completion time for each log file with the instant time, and we have a new rule for file slicing:"]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.em,{children:"A log file belongs to the file slice with the maximum base requested time smaller than(or equals with) it's completion time."}),"[^1]"]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/non-blocking-concurrency-control/new_file_layout.png",alt:"New file layout",width:"800",align:"middle"}),"\n",(0,n.jsxs)(a.p,{children:["With the flexibility of the new file layout, the overhead of querying base instant time is eliminated for log writers and a compaction can be scheduled anywhere with any instant time.\nSee ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/rfc/rfc-66/rfc-66.md",children:"RFC-66"})," for more."]}),"\n",(0,n.jsx)(a.h3,{id:"true-time-api",children:"True Time API"}),"\n",(0,n.jsxs)(a.p,{children:['In order to ensure the monotonicity of timestamp generation, Hudi introduces the "',(0,n.jsx)(a.a,{href:"/docs/next/timeline#timeline-components",children:"TrueTime API"}),'" since 1.x release.\nBasically there are two ways to make the time generation monotonically increasing, inline with TrueTime semantics:']}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"A global lock to guard the time generation with mutex, along with a wait for an estimated max allowed clock skew on distributed hosts;"}),"\n",(0,n.jsx)(a.li,{children:"Globally synchronized time generation service, e.g. Google Spanner Time Service, the service itself can ensure the monotonicity."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:'Hudi now implements the "TrueTime" semantics with the first solution, a configurable max waiting time is supported.'}),"\n",(0,n.jsx)(a.h3,{id:"lsm-timeline",children:"LSM timeline"}),"\n",(0,n.jsxs)(a.p,{children:["The new file layout requires efficient queries from instant time to get the completion time. Hudi re-implements the archived timeline since 1.x, the\nnew archived timeline data files are organized as ",(0,n.jsx)(a.a,{href:"/docs/next/timeline#lsm-timeline-history",children:"an LSM tree"})," to support fast time range filtering queries with instant time data-skipping on it."]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/non-blocking-concurrency-control/lsm_archive_timeline.png",alt:"LSM archive timeline",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"With the powerful new file layout, it is quite straight-forward to implement non-blocking concurrency control. The function is implemented with the simple bucket index on MOR table for Flink.\nThe bucket index ensures fixed record key to file group mappings for multiple workloads. The log writer writes the records into avro logs and the compaction table service would take care of\nthe conflict resolution. Because each log file name contains the instant time and each record contains the event time ordering field, Hudi reader can merge the records either\nwith natural order(processing time sequence) or event time order."}),"\n",(0,n.jsxs)(a.p,{children:["The concurrency mode should be configured as ",(0,n.jsx)(a.code,{children:"NON_BLOCKING_CONCURRENCY_CONTROL"}),", you can enable the table services on one job and disable it for the others."]}),"\n",(0,n.jsx)(a.h2,{id:"flink-sql-demo",children:"Flink SQL demo"}),"\n",(0,n.jsx)(a.p,{children:"Here is a demo to show 2 pipelines that ingest into the same downstream table, the two sink table views share the same table path."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"-- NB-CC demo\n\n-- The source table\nCREATE TABLE sourceT (\n  uuid varchar(20),\n  name varchar(10),\n  age int,\n  ts timestamp(3),\n  `partition` as 'par1'\n) WITH (\n  'connector' = 'datagen',\n  'rows-per-second' = '200'\n);\n\n-- table view for writer1\ncreate table t1(\n  uuid varchar(20),\n  name varchar(10),\n  age int,\n  ts timestamp(3),\n  `partition` varchar(20)\n)\nwith (\n  'connector' = 'hudi',\n  'path' = '/Users/chenyuzhao/workspace/hudi-demo/t1',\n  'table.type' = 'MERGE_ON_READ',\n  'index.type' = 'BUCKET',\n  'hoodie.write.concurrency.mode' = 'NON_BLOCKING_CONCURRENCY_CONTROL',\n  'write.tasks' = '2'\n);\n\ninsert into t1/*+options('metadata.enabled'='true')*/ select * from sourceT;\n\n-- table view for writer2\n-- compaction and cleaning are disabled because writer1 has taken care of it.\ncreate table t1_2(\n  uuid varchar(20),\n  name varchar(10),\n  age int,\n  ts timestamp(3),\n  `partition` varchar(20)\n)\nwith (\n  'connector' = 'hudi',\n  'path' = '/Users/chenyuzhao/workspace/hudi-demo/t1',\n  'table.type' = 'MERGE_ON_READ',\n  'index.type' = 'BUCKET',\n  'hoodie.write.concurrency.mode' = 'NON_BLOCKING_CONCURRENCY_CONTROL',\n  'write.tasks' = '2',\n  'compaction.schedule.enabled' = 'false',\n  'compaction.async.enabled' = 'false',\n  'clean.async.enabled' = 'false'\n);\n\n-- executes the ingestion workloads\ninsert into t1 select * from sourceT;\ninsert into t1_2 select * from sourceT;\n"})}),"\n",(0,n.jsx)(a.h2,{id:"future-roadmap",children:"Future Roadmap"}),"\n",(0,n.jsx)(a.p,{children:"While non-blocking concurrency control is a very powerful feature for streaming users, it is a general solution for multiple writer conflict resolution,\nhere are some plans that improve the Hudi core features:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"NBCC support for metadata table"}),"\n",(0,n.jsx)(a.li,{children:"NBCC for clustering"}),"\n",(0,n.jsx)(a.li,{children:"NBCC for other index type"}),"\n"]}),"\n",(0,n.jsx)(a.hr,{}),"\n",(0,n.jsxs)(a.p,{children:["[^1] ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/rfc/rfc-66/rfc-66.md",children:"RFC-66"})," well-explained the completion time based file slicing with a pseudocode."]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},72089:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(54781),n=t(74848),s=t(28453),r=t(9230);const o={title:"What about Apache Hudi, Apache Iceberg, and Delta Lake?",authors:[{name:"Martin Jurado Pedroza"}],category:"blog",image:"/assets/images/blog/2023-06-30-What-about-Apache-Hudi-Apache-Iceberg-and-Delta-Lake.png",tags:["blog","vector search","comparison","apache hudi","delta lake","iceberg","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@martin.jurado.p/what-about-apache-hudi-apache-iceberg-and-delta-lake-3cae0eecd148",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},72110:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(29273),n=t(74848),s=t(28453),r=t(9230);const o={title:"Data Lake Change Capture using Apache Hudi & Amazon AMS/EMR",authors:[{name:"Manoj Kukreja"}],category:"blog",image:"/assets/images/blog/2020-10-21-Data-Lake-Change-Capture-using-Apache-Hudi-and-Amazon-AMS-EMR.jpeg",tags:["how-to","change data capture","cdc","towardsdatascience"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://towardsdatascience.com/data-lake-change-data-capture-cdc-using-apache-hudi-on-amazon-emr-part-2-process-65e4662d7b4b",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},72153:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/jdpost-image7-361b48a09e72bed4c19f8be9bf616ed5.jpg"},72222:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig-7-Loaded-Schema-in-PuppyGraph-UI-b32e7eb53b88d6290b5f463308445bfa.png"},72298:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/12/13/what-is-apache-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-12-13-what-is-apache-hudi.mdx","source":"@site/blog/2023-12-13-what-is-apache-hudi.mdx","title":"What is Apache Hudi","description":"Redirecting... please wait!!","date":"2023-12-13T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"},{"inline":true,"label":"beginner","permalink":"/blog/tags/beginner"},{"inline":true,"label":"apache spark","permalink":"/blog/tags/apache-spark"}],"readingTime":0.11,"hasTruncateMarker":false,"authors":[{"name":"Karim Faiz","key":null,"page":null}],"frontMatter":{"title":"What is Apache Hudi","excerpt":"What is Apache Hudi","author":"Karim Faiz","category":"blog","image":"/assets/images/blog/2023-12-13-what-is-apache-hudi.png","tags":["blog","apache hudi","medium","beginner","apache spark"]},"unlisted":false,"prevItem":{"title":"Apache Hudi 2023: A Year In Review","permalink":"/blog/2023/12/28/apache-hudi-2023-a-year-in-review"},"nextItem":{"title":"Getting started with Apache Hudi","permalink":"/blog/2023/12/09/Getting-started-with-Apache-Hudi"}}')},72338:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2016/08/04/The-Case-for-incremental-processing-on-Hadoop","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2016-08-04-The-Case-for-incremental-processing-on-Hadoop.mdx","source":"@site/blog/2016-08-04-The-Case-for-incremental-processing-on-Hadoop.mdx","title":"The Case for incremental processing on Hadoop","description":"Redirecting... please wait!!","date":"2016-08-04T00:00:00.000Z","tags":[{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"incremental processing","permalink":"/blog/tags/incremental-processing"},{"inline":true,"label":"oreilly","permalink":"/blog/tags/oreilly"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Vinoth Chandar","socials":{},"key":null,"page":null}],"frontMatter":{"title":"The Case for incremental processing on Hadoop","authors":[{"name":"Vinoth Chandar"}],"category":"blog","image":"/assets/images/blog/2016-08-04-The-Case-for-incremental-processing-on-Hadoop.png","tags":["use-case","incremental processing","oreilly"]},"unlisted":false,"prevItem":{"title":"Connect with us at Strata San Jose March 2017","permalink":"/blog/2016/12/30/strata-talk-2017"}}')},72385:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(75114),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi (Part 1): History, Getting Started",excerpt:"Apache Hudi (Part 1): History, Getting Started",author:"Dipankar Mazumdar",category:"blog",image:"/assets/images/blog/2023-11-28-Apache-Hudi-Part-1-History-Getting-Started.png",tags:["apache hudi","blog","getting started","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://dipankar-tnt.medium.com/apache-hudi-part-1-history-getting-started-95030b003759",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},72402:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(47645),n=t(74848),s=t(28453);const r={title:"Deep Dive Into Hudi's Indexing Subsystem (Part 2 of 2)",excerpt:"Explore advanced indexing in Apache Hudi: record and secondary indexes for fast point lookups, expression indexes for transformed predicates, and async indexing for building indexes without blocking writes.",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2025-11-12-deep-dive-into-hudis-indexing-subsystem-part-2-of-2/fig1.png",tags:["hudi","indexing","data lakehouse","data skipping"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Equality Matching with Record and Secondary Indexes",id:"equality-matching-with-record-and-secondary-indexes",level:2},{value:"The lookup process",id:"the-lookup-process",level:3},{value:"SQL examples",id:"sql-examples",level:3},{value:"Expression Index",id:"expression-index",level:2},{value:"SQL examples",id:"sql-examples-1",level:3},{value:"Building Indexes Efficiently with the Async Indexer",id:"building-indexes-efficiently-with-the-async-indexer",level:2},{value:"Summary",id:"summary",level:2}];function c(e){const a={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(a.p,{children:["In ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2025/10/29/deep-dive-into-hudis-indexing-subsystem-part-1-of-2/",children:"part 1"}),", we explored how Hudi's metadata table functions as a self-managed, multimodal indexing subsystem. We covered its internal architecture\u2014a partitioned Hudi Merge-on-Read (MOR) table using HFile format for efficient key lookups\u2014and how the files, column stats, and partition stats indexes work together to implement powerful data skipping. These indexes dramatically reduce I/O by pruning partitions and files that don't contain the data your query needs."]}),"\n",(0,n.jsxs)(a.p,{children:["Now in part 2, we'll dive into more specialized indexes that handle different query patterns. We'll look at the record and secondary indexes, which provide exact file locations for equality-matching predicates rather than just skipping irrelevant files. We'll explore expression indexes that optimize queries with inline transformations like ",(0,n.jsx)(a.code,{children:"from_unixtime()"})," or ",(0,n.jsx)(a.code,{children:"substring()"}),". Finally, we'll cover async indexing, which lets you build resource-intensive indexes in the background without blocking your active read and write operations."]}),"\n",(0,n.jsx)(a.h2,{id:"equality-matching-with-record-and-secondary-indexes",children:"Equality Matching with Record and Secondary Indexes"}),"\n",(0,n.jsxs)(a.p,{children:["Queries may contain equality-matching predicates like ",(0,n.jsx)(a.code,{children:"A = X"})," or ",(0,n.jsx)(a.code,{children:"B IN (X, Y, Z)"}),". While data skipping indexes such as column stats and partition stats help here too, record-level indexing goes further by pinpointing the exact data files containing those values."]}),"\n",(0,n.jsxs)(a.p,{children:["Hudi\u2019s multimodal indexing subsystem implements the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2023/11/01/record-level-index/",children:(0,n.jsx)(a.em,{children:"record index"})})," and ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2025/04/02/secondary-index/",children:(0,n.jsx)(a.em,{children:"secondary index"})})," to meet this need:"]}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Record index"}),": Stores mappings between record keys and the file locations that contain them."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Secondary index"}),": Stores mappings between non-record-key column values and their corresponding record keys to support mapping to file locations."]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["Note that the record index is located at the ",(0,n.jsx)(a.code,{children:"record_index/"})," partition of the metadata table. You can create multiple secondary indexes, each for a chosen column, stored under a dedicated partition (prefixed with ",(0,n.jsx)(a.code,{children:"secondary_index_"}),") in the metadata table."]}),"\n",(0,n.jsxs)(a.p,{children:["The record index is a high-performance, general-purpose index that works on both the writer and reader sides. As described in ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2023/11/01/record-level-index/",children:"this blog"}),", its direct record-location lookup allows Hudi writers to efficiently route updates and deletes to their corresponding file groups in a Hudi table. The secondary index leverages the record index to look up non-record-key columns efficiently. The remainder of this section focuses on the reader side to show how these two indexes optimize equality-matching predicates."]}),"\n",(0,n.jsx)(a.h3,{id:"the-lookup-process",children:"The lookup process"}),"\n",(0,n.jsx)(a.p,{children:"Similar to the data skipping process, the query engine parses equality-matching predicates and pushes them down to the Hudi integration component. This component then performs the index lookup and returns the file locations to scan."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Record and secondary index lookup process",src:t(69085).A+"",width:"974",height:"769"})}),"\n",(0,n.jsxs)(a.p,{children:["First, let's consider the record index. When a query with an equality filter like ",(0,n.jsx)(a.code,{children:"id = '001'"})," runs against a Hudi table where ",(0,n.jsx)(a.code,{children:"id"})," is the record key, the engine uses the record index to find the exact file locations for that key. The index returns these locations to the query engine, which then plans the read execution."]}),"\n",(0,n.jsx)(a.p,{children:"This direct lookup dramatically optimizes the query by ensuring only the relevant file locations are scanned. For example, on a 400 GB synthetic Hudi table with 20,000 file groups, a query filtering on a single record key saw its execution time drop from 977 seconds to just 12 seconds\u2014a 98% reduction\u2014when using the record index."}),"\n",(0,n.jsxs)(a.p,{children:["Now, let's consider the case when the equality filter is ",(0,n.jsx)(a.code,{children:"name = 'foo'"})," where ",(0,n.jsx)(a.code,{children:"name"})," is not a record key field. A secondary index built for the column ",(0,n.jsx)(a.code,{children:"name"})," will be used for the lookup process. Entries in the secondary index contain mappings of all ",(0,n.jsx)(a.code,{children:"name"})," values and their corresponding record keys. Because multiple distinct records can have the same ",(0,n.jsx)(a.code,{children:"name"})," value, the lookup may return multiple record keys. The next step is to look up these returned record keys in the record index to find the enclosing file locations for scanning. As you can tell, the record index must be enabled for using the secondary index."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2025/04/02/secondary-index/#benchmarking",children:"A recent TPCDS benchmarking"})," shows that, by using the secondary index, query performance improved by about 45% on average, and the amount of data scanned was reduced by 90%."]}),"\n",(0,n.jsx)(a.h3,{id:"sql-examples",children:"SQL examples"}),"\n",(0,n.jsxs)(a.p,{children:["You can specify ",(0,n.jsx)(a.code,{children:"hoodie.metadata.record.index.enable"})," during table creation to enable the record index for the table:"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"CREATE TABLE trips (\n    ts BIGINT,\n    id STRING,\n    rider STRING,\n    driver STRING,\n    fare DOUBLE,\n    city STRING,\n    state STRING\n) USING hudi\n OPTIONS(\n    primaryKey = 'id',\n    hoodie.metadata.record.index.enable = 'true' -- enable record index\n)\nPARTITIONED BY (city, state);\n"})}),"\n",(0,n.jsxs)(a.p,{children:["To create a secondary index on a specific column, you can use ",(0,n.jsx)(a.code,{children:"CREATE INDEX"})," like this:"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"CREATE INDEX driver_idx ON trips (driver); -- enable secondary index on column `driver`\n"})}),"\n",(0,n.jsxs)(a.p,{children:["When you write data to the example table, index data gets written to the record index and secondary index partitions in the metadata table, which then accelerates query execution during reads. Check out the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/sql_ddl",children:"SQL DDL page"})," for more examples."]}),"\n",(0,n.jsx)(a.h2,{id:"expression-index",children:"Expression Index"}),"\n",(0,n.jsxs)(a.p,{children:["Query predicates often contain expressions that perform inline transformations on columns, such as ",(0,n.jsx)(a.code,{children:"from_unixtime()"})," or ",(0,n.jsx)(a.code,{children:"substring()"}),". These expressions prevent a direct match with standard column indexes like column stats or partition stats. To optimize such queries, Hudi provides the ",(0,n.jsx)(a.em,{children:"expression index"})," that operates on transformed column values. A full list of supported expressions is available in the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/sql_ddl/#create-expression-index",children:"documentation"}),"."]}),"\n",(0,n.jsx)(a.p,{children:"Hudi currently supports two types of expression indexes:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Column stats type"}),": Stores file-level statistics (min, max, null count, value count) for the transformed values after applying the expression."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Bloom filter type"}),": Stores a file-level bloom filter built from the transformed values after applying the expression."]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["Each expression index\u2014defined by its type, the expression used, and the target column\u2014occupies a dedicated partition within the metadata table, identified by an ",(0,n.jsx)(a.code,{children:"expr_index_"})," prefix in its partition path."]}),"\n",(0,n.jsxs)(a.p,{children:["The column stats expression index functions similarly to a standard column stats index and is effective for data skipping. As the diagram below illustrates, a predicate containing a ",(0,n.jsx)(a.code,{children:"from_unixtime()"})," expression is processed for lookup, and the corresponding expression index prunes the file list for the query engine."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Expression index lookup process",src:t(87878).A+"",width:"860",height:"620"})}),"\n",(0,n.jsx)(a.p,{children:"The bloom filter expression index is designed for equality-matching predicates. Unlike the record and secondary indexes, which provide exact file locations, this index uses a bloom filter\u2014a space-efficient data structure for quick presence checks\u2014to prune files. The query planner can skip a file if the bloom filter indicates a target value is definitively not present."}),"\n",(0,n.jsxs)(a.p,{children:['The bloom filter expression index is most effective for high-cardinality columns, where the probability of a "not present" result is higher, allowing more files to be skipped. For low-cardinality columns, the proposed ',(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/rfc/rfc-92/rfc-92.md",children:"bitmap index"})," would be more efficient and represents a valuable future extension to Hudi's indexing subsystem."]}),"\n",(0,n.jsx)(a.h3,{id:"sql-examples-1",children:"SQL examples"}),"\n",(0,n.jsx)(a.p,{children:"Similar to creating a secondary index, you can create an expression index (column stats type) like this:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"CREATE INDEX ts_date ON trips\n  USING column_stats(ts) \n  OPTIONS(expr='from_unixtime', format='yyyy-MM-dd');\n"})}),"\n",(0,n.jsxs)(a.p,{children:["This example creates a column stats expression index on the column ",(0,n.jsx)(a.code,{children:"ts"})," with the expression ",(0,n.jsx)(a.code,{children:"from_unixtime"})," that transforms an epoch timestamp into a date string, allowing effective data skipping based on dates."]}),"\n",(0,n.jsx)(a.p,{children:"You can create a bloom filter expression index similarly:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"CREATE INDEX bloom_idx_rider ON trips\n  USING bloom_filters(rider)\n  OPTIONS(expr='lower');\n"})}),"\n",(0,n.jsxs)(a.p,{children:["This example builds a bloom filter expression index using the lowercase values of column ",(0,n.jsx)(a.code,{children:"rider"}),", optimizing for predicates that match lowercase rider names. Check out the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/sql_ddl",children:"SQL DDL page"})," for more examples."]}),"\n",(0,n.jsx)(a.h2,{id:"building-indexes-efficiently-with-the-async-indexer",children:"Building Indexes Efficiently with the Async Indexer"}),"\n",(0,n.jsxs)(a.p,{children:["Hudi provides flexible mechanisms for managing indexes. You can use SQL DDL commands\u2014such as ",(0,n.jsx)(a.code,{children:"CREATE INDEX"}),", ",(0,n.jsx)(a.code,{children:"DROP INDEX"}),", and ",(0,n.jsx)(a.code,{children:"SHOW INDEXES"}),"\u2014or programmatically set writer configurations via the Spark DataSource and Flink DataStream APIs. For example, setting ",(0,n.jsx)(a.code,{children:"hoodie.metadata.index.partition.stats.enable=false"})," during a write operation drops the partition stats index. This action deletes the corresponding partition from the metadata table and skips indexing computations for subsequent writes until the configuration is re-enabled."]}),"\n",(0,n.jsx)(a.p,{children:"Creating a new index can be a resource-intensive operation, particularly for large tables and for indexes with high space complexity. For instance, the space complexity of the column stats index is O(columns \xd7 files), while the record index requires O(records) space. When adding such an index to a large table via DDL or a writer configuration, the time-consuming index initialization process must not block ongoing read and write operations."}),"\n",(0,n.jsxs)(a.p,{children:["To address this challenge, Hudi's index management is designed with two key goals: index creation should not block concurrent reads and writes, and once built, an index must serve consistent data up to the latest table commit. Hudi meets these requirements with its ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/metadata_indexing/#setup-async-indexing",children:"async indexing"})," (illustrated below), which builds indexes in the background without interrupting active writers and readers."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Async indexing process",src:t(53071).A+"",width:"1112",height:"814"})}),"\n",(0,n.jsx)(a.p,{children:"The async indexing process consists of two phases: scheduling and execution. First, the scheduler creates an indexing plan that covers data up to the latest data table commit. Next, the executor reads the required file groups from the data table and writes the corresponding index data to the metadata table. While this process runs, concurrent writers can continue ingesting data. The async indexing executor writes index data to base files in the target index partitions in the metadata table, while the ongoing writer append new index data to log files in those partitions. Hudi uses a conflict resolution mechanism to determine if an indexing operation needs to be retried due to concurrent write conflicts."}),"\n",(0,n.jsxs)(a.p,{children:["To manage this concurrency, a lock provider must be configured for both the indexer and the data writers. Upon successful completion, the operation is marked by a completed indexing commit in the Hudi table\u2019s timeline. For future improvements, the metadata table will employ non-blocking concurrency control to gracefully absorb conflicting updates from both indexing and write operations, thus avoiding wasteful retries. You can find configuration examples in the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/metadata_indexing/#setup-async-indexing",children:"documentation"}),"."]}),"\n",(0,n.jsx)(a.h2,{id:"summary",children:"Summary"}),"\n",(0,n.jsxs)(a.p,{children:["Throughout this two-part series, we've explored how Hudi's indexing subsystem brings database-grade performance to the data lakehouse. In ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2025/10/29/deep-dive-into-hudis-indexing-subsystem-part-1-of-2/",children:"part 1"}),", we examined the metadata table's architecture and how files, column stats, and partition stats indexes work together to skip irrelevant data. In part 2, we covered specialized indexes\u2014record, secondary, and expression indexes\u2014that provide exact file locations for equality matching and handle transformed predicates. We also looked at async indexing, which lets you add resource-intensive indexes without blocking ongoing operations."]}),"\n",(0,n.jsx)(a.p,{children:"Here's a quick guide for choosing the right indexes for your workload:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Files"}),": Always enabled in the metadata table\u2014provides partition and file lists in the table to facilitate common indexing processes"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Column stats and partition stats"}),": Enable by default and configure ",(0,n.jsx)(a.code,{children:"hoodie.metadata.index.column.stats.column.list"})," to include only the columns you frequently filter on. These indexes are essential for range predicates and data skipping"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Record index"}),": Enable when you have frequent point lookups on record keys or when you need secondary indexes. The record index also optimizes Hudi's write path by efficiently routing updates and deletes"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Secondary index"}),": Create secondary indexes for non-record-key columns that appear in equality predicates. Each secondary index adds maintenance overhead, so focus on high-value columns"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Expression index"}),": Use expression indexes when queries contain predicates with inline transformations. Choose column stats type for range queries on transformed values, or bloom filter type for equality matching on high-cardinality columns"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Async indexing"}),": Use async indexing when adding indexes to large tables. The async indexer builds indexes in the background, keeping your writers and readers unblocked"]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"All indexes are maintained transactionally alongside data writes, ensuring consistency without sacrificing performance. The metadata table uses HFile format for fast point lookups and periodic compaction to keep reads efficient. This design makes Hudi's indexing subsystem both powerful and practical\u2014ready to handle lakehouse-scale data while remaining simple to configure and operate."}),"\n",(0,n.jsx)(a.p,{children:"As Hudi continues to evolve, the indexing subsystem is designed for extensibility. Upcoming features like the bitmap index for low-cardinality columns and vector search index for AI workloads will further expand its capabilities. By understanding these indexing patterns and following the configuration guidelines in this series, you can build lakehouse tables that deliver the query performance your analytics and data pipelines demand."})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},72605:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(47153),n=t(74848),s=t(28453),r=t(9230);const o={title:"Ingest streaming data to Apache Hudi tables using AWS Glue and Apache Hudi DeltaStreamer",authors:[{name:"Vishal Pathak"},{name:"Anand Prakash"},{name:"Noritaka Sekiyama"}],category:"blog",image:"/assets/images/blog/2022-10-06_Ingest_streaming_data_to_Apache_Hudi_tables_using_AWS_Glue_and_DeltaStreamer.png",tags:["how-to","streaming ingestion","deltastreamer","amazon"]},l=void 0,d={authorsImageUrls:[void 0,void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/ingest-streaming-data-to-apache-hudi-tables-using-aws-glue-and-apache-hudi-deltastreamer/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},72806:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/5-storage-based-lp-8528a28ca24084f9c01e550a1c312a36.png"},72901:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/11/19/Hudi-Streamer-DeltaStreamer-Hands-On-Guide-Local-Ingestion-from-Parquet-Source","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-11-19-Hudi-Streamer-DeltaStreamer-Hands-On-Guide-Local-Ingestion-from-Parquet-Source.mdx","source":"@site/blog/2023-11-19-Hudi-Streamer-DeltaStreamer-Hands-On-Guide-Local-Ingestion-from-Parquet-Source.mdx","title":"Hudi Streamer (Delta Streamer) Hands-On Guide: Local Ingestion from Parquet Source","description":"Redirecting... please wait!!","date":"2023-11-19T00:00:00.000Z","tags":[{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"hudi streamer","permalink":"/blog/tags/hudi-streamer"},{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"apache parquet","permalink":"/blog/tags/apache-parquet"},{"inline":true,"label":"linkedin","permalink":"/blog/tags/linkedin"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"Soumil Shah","key":null,"page":null}],"frontMatter":{"title":"Hudi Streamer (Delta Streamer) Hands-On Guide: Local Ingestion from Parquet Source","excerpt":"Hudi Streamer (Delta Streamer) Hands-On Guide: Local Ingestion from Parquet Source","author":"Soumil Shah","category":"blog","image":"/assets/images/blog/2023-11-19-Hudi-Streamer-DeltaStreamer-Hands-On-Guide-Local-Ingestion-from-Parquet-Source.png","tags":["apache hudi","hudi streamer","how-to","apache parquet","linkedin"]},"unlisted":false,"prevItem":{"title":"Introducing Apache Hudi support with AWS Glue crawlers","permalink":"/blog/2023/11/22/Introducing-Apache-Hudi-support-with-AWS-Glue-crawlers"},"nextItem":{"title":"Apache Hudi: From Zero To One (6/10)","permalink":"/blog/2023/11/13/Apache-Hudi-From-Zero-To-One-blog-6"}}')},72906:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/01/28/concurrency-control","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-01-28-concurrency-control.md","source":"@site/blog/2025-01-28-concurrency-control.md","title":"Concurrency Control in Open Data Lakehouse","description":"Introduction","date":"2025-01-28T00:00:00.000Z","tags":[{"inline":true,"label":"multi-writer","permalink":"/blog/tags/multi-writer"},{"inline":true,"label":"concurrency","permalink":"/blog/tags/concurrency"},{"inline":true,"label":"concurrency-control","permalink":"/blog/tags/concurrency-control"},{"inline":true,"label":"non-blocking concurrency-control","permalink":"/blog/tags/non-blocking-concurrency-control"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"Apache Iceberg","permalink":"/blog/tags/apache-iceberg"},{"inline":true,"label":"Delta Lake","permalink":"/blog/tags/delta-lake"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"design","permalink":"/blog/tags/design"}],"readingTime":18.95,"hasTruncateMarker":false,"authors":[{"name":"Dipankar Mazumdar","key":null,"page":null}],"frontMatter":{"title":"Concurrency Control in Open Data Lakehouse","excerpt":"How various concurrency control techniques works in Apache Hudi, Apache Iceberg & Delta Lake","author":"Dipankar Mazumdar","category":"blog","image":"/assets/images/blog/concurrency_control/concurrency_blog_thumb.jpg","tags":["multi-writer","concurrency","concurrency-control","non-blocking concurrency-control","Apache Hudi","Apache Iceberg","Delta Lake","blog","design"]},"unlisted":false,"prevItem":{"title":"An intro to Hudi with MinIO","permalink":"/blog/2025/01/30/an-intro-to-hudi-with-minio"},"nextItem":{"title":"Apache Hudi 1.0 Now Generally Available","permalink":"/blog/2025/01/18/apache-hudi-1-0-now-generally-available"}}')},73081:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/02/02/Onehouse-Commitment-to-Openness","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-02-02-Onehouse-Commitment-to-Openness.mdx","source":"@site/blog/2022-02-02-Onehouse-Commitment-to-Openness.mdx","title":"Onehouse Commitment to Openness","description":"Redirecting... please wait!!","date":"2022-02-02T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"community","permalink":"/blog/tags/community"},{"inline":true,"label":"onehouse","permalink":"/blog/tags/onehouse"}],"readingTime":0.1,"hasTruncateMarker":false,"authors":[{"name":"Vinoth Chandar","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Onehouse Commitment to Openness","authors":[{"name":"Vinoth Chandar"}],"category":"blog","image":"/assets/images/blog/2022-02-02-onehouse-commitment-to-openness.jpeg","tags":["blog","community","onehouse"]},"unlisted":false,"prevItem":{"title":"Onehouse brings a fully-managed lakehouse to Apache Hudi","permalink":"/blog/2022/02/03/Onehouse-brings-a-fully-managed-lakehouse-to-Apache-Hudi"},"nextItem":{"title":"Cost Efficiency @ Scale in Big Data File Format","permalink":"/blog/2022/01/25/Cost-Efficiency-Scale-in-Big-Data-File-Format"}}')},73239:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/4-binary-copy-chart-f7366c95b72eb12b3ce39cc1b83bfcff.png"},73605:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(60441),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi 1.0 Now Generally Available",author:"Renato Losio",category:"blog",image:"/assets/images/blog/2025-01-18-apache-hudi-1-0-now-generally-available.jpeg",tags:["blog","apache hudi","hudi 1.0.0","infoq"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.infoq.com/news/2025/01/apache-hudi/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},73719:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(77461),n=t(74848),s=t(28453),r=t(9230);const o={title:"Data lake Table formats: Apache Iceberg vs Apache Hudi vs Delta lake",authors:[{name:"Shashwat Pandey"}],category:"blog",image:"/assets/images/blog/2023-08-03-Data-lake-Table-formats-Apache-Iceberg-vs-Apache-Hudi-vs-Delta-lake.png",tags:["blog","hudi","iceberg","delta lake","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://shashwat-pandey.medium.com/data-lake-table-formats-apache-iceberg-vs-apache-hudi-vs-delta-lake-10b67a1d587",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},74059:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/04/03/hands-on-guide-reading-data-from-hudi-tables-joining-delta","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-04-03-hands-on-guide-reading-data-from-hudi-tables-joining-delta.mdx","source":"@site/blog/2024-04-03-hands-on-guide-reading-data-from-hudi-tables-joining-delta.mdx","title":"Hands-On Guide: Reading Data from Hudi Tables Incrementally, Joining with Delta Tables using HudiStreamer and SQL-Based Transformer","description":"Redirecting... please wait!!","date":"2024-04-03T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"deltastreamer","permalink":"/blog/tags/deltastreamer"},{"inline":true,"label":"hudi streamer","permalink":"/blog/tags/hudi-streamer"},{"inline":true,"label":"delta","permalink":"/blog/tags/delta"},{"inline":true,"label":"sql transformer","permalink":"/blog/tags/sql-transformer"},{"inline":true,"label":"linkedin","permalink":"/blog/tags/linkedin"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"Soumil Shah","key":null,"page":null}],"frontMatter":{"title":"Hands-On Guide: Reading Data from Hudi Tables Incrementally, Joining with Delta Tables using HudiStreamer and SQL-Based Transformer","author":"Soumil Shah","category":"blog","image":"/assets/images/blog/2024-04-03-hands-on-guide-reading-data-from-hudi-tables-joining-delta.png","tags":["blog","apache hudi","deltastreamer","hudi streamer","delta","sql transformer","linkedin"]},"unlisted":false,"prevItem":{"title":"Apache Hudi: From Zero To One (10/10)","permalink":"/blog/2024/04/13/Apache-Hudi-From-Zero-To-One-blog-10"},"nextItem":{"title":"Record Level Indexing in Apache Hudi Delivers 70% Faster Point Lookups","permalink":"/blog/2024/03/30/record-level-indexing-apache-hudi-delivers-70-faster-point"}}')},74244:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(78292),n=t(74848),s=t(28453),r=t(9230);const o={title:"Global vs Non-global index in Apache Hudi",authors:[{name:"Sivabalan Narayanan"}],category:"blog",tags:["how-to","indexing","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@simpsons/global-vs-non-global-index-in-apache-hudi-ac880b031cbc",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},74276:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/s3-endpoint-configuration-1-6246a9d09772ac527a13f5b26a6fb38e.png"},74358:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/DataCouncil-04dfc7a9001968f04689ba9fda4dbaab.jpg"},74469:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig4-8df00f82c190ca1663131f7dbb6ddf8d.jpg"},74474:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(32350),n=t(74848),s=t(28453),r=t(9230);const o={title:"How NerdWallet uses AWS and Apache Hudi to build a serverless, real-time analytics platform",authors:[{name:"Kevin Chun"},{name:"Dylan Qu"}],category:"blog",image:"/assets/images/blog/2022-08-09-How-NerdWallet-uses-AWS-and-Apache-Hudi-to-build-a-serverless-real-time-analytics-platform.png",tags:["use-case","near real-time analytics","incremental processing","amazon"]},l=void 0,d={authorsImageUrls:[void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/how-nerdwallet-uses-aws-and-apache-hudi-to-build-a-serverless-real-time-analytics-platform/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},74518:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(66671),n=t(74848),s=t(28453),r=t(9230);const o={title:"Hoodie Timeline: Foundational pillar for ACID transactions",authors:[{name:"Sivabalan Narayanan"}],category:"blog",image:"/assets/images/hudi_timeline.png",tags:["blog","ACID","transactions","commits","timeline","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@simpsons/hoodie-timeline-foundational-pillar-for-acid-transactions-be871399cbae",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},74548:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(46601),n=t(74848),s=t(28453);const r={title:"Powering Amazon Unit Economics at Scale Using Apache Hudi",excerpt:"How Amazon's Profit Intelligence team uses Apache Hudi to power hundreds of pipelines",author:"Jason, Abhishek, Sethu in collaboration with Dipankar",category:"blog",image:"/assets/images/blog/amz-1200x600.jpg",tags:["Apache Hudi","Amazon","Community"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"The Business Need: Profit Intelligence and Unit Economics",id:"the-business-need-profit-intelligence-and-unit-economics",level:2},{value:"Amazon\u2019s Data Lakehouse Journey",id:"amazons-data-lakehouse-journey",level:2},{value:"Early Phase",id:"early-phase",level:3},{value:"Intermediate Phase",id:"intermediate-phase",level:3},{value:"Current State: Nexus + Apache Hudi",id:"current-state-nexus--apache-hudi",level:3},{value:"Key Modules of Nexus",id:"key-modules-of-nexus",level:4},{value:"Why Apache Hudi?",id:"why-apache-hudi",level:2},{value:"Key Learnings from Operating Hudi at Amazon Scale",id:"key-learnings-from-operating-hudi-at-amazon-scale",level:2},{value:"1. Concurrency Control",id:"1-concurrency-control",level:3},{value:"2. Metadata Table Management: Async vs Sync Trade-Offs",id:"2-metadata-table-management-async-vs-sync-trade-offs",level:3},{value:"3. Cost Management",id:"3-cost-management",level:3},{value:"Operational Scale: Nexus by the Numbers",id:"operational-scale-nexus-by-the-numbers",level:2}];function c(e){const a={a:"a",admonition:"admonition",br:"br",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",hr:"hr",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.admonition,{title:"TL;DR",type:"tip",children:(0,n.jsx)(a.p,{children:"Amazon\u2019s Profit Intelligence team built Nexus, a configuration-driven platform powered by Apache Hudi, to scale unit economics across thousands of retail use cases. Nexus manages over 1,200 tables, processes hundreds of billions of rows daily, and handles ~1 petabyte of data churn each month. This blog dives into their data lakehouse journey, Nexus architecture, Hudi integration, and key operational learnings."})}),"\n",(0,n.jsx)(a.p,{children:"Understanding and improving unit-level profitability at Amazon's scale is a massive challenge - one that requires flexibility, precision, and operational efficiency. In this blog, we walk through how Amazon\u2019s Profit Intelligence team built a scalable, configuration-driven platform called Nexus, and how Apache Hudi became the cornerstone of its data lake architecture."}),"\n",(0,n.jsx)(a.p,{children:"By combining declarative configuration with Hudi's advanced table management capabilities, the team has enabled thousands of retail business use cases to run seamlessly, allowing finance and pricing teams to self-serve insights on cost and profitability, without constantly relying on engineering intervention."}),"\n",(0,n.jsx)(a.h2,{id:"the-business-need-profit-intelligence-and-unit-economics",children:"The Business Need: Profit Intelligence and Unit Economics"}),"\n",(0,n.jsxs)(a.p,{children:["Within Amazon\u2019s Worldwide Stores, the Selling Partner Services (SPS) team supports seller-facing operations. A key part of this effort is computing ",(0,n.jsx)(a.strong,{children:"Contribution Profit"})," - a granular metric that captures revenue, costs, and profitability at the unit level, such as ",(0,n.jsx)(a.em,{children:"a shipped item to the customer"}),"."]}),"\n",(0,n.jsx)(a.p,{children:"Contribution Profit powers decision-making for a range of downstream teams including:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Pricing"}),"\n",(0,n.jsx)(a.li,{children:"Forecasting"}),"\n",(0,n.jsx)(a.li,{children:"Finance"}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"The challenge? Supporting the scale and diversity of retail use cases across Amazon's global business, while maintaining a data platform that's both extensible and maintainable."}),"\n",(0,n.jsx)(a.h2,{id:"amazons-data-lakehouse-journey",children:"Amazon\u2019s Data Lakehouse Journey"}),"\n",(0,n.jsx)(a.p,{children:"Over the past decade, the architecture behind Contribution Profit has gone through several phases of evolution, driven by the need to better support Amazon\u2019s growing and diverse retail business use cases."}),"\n",(0,n.jsx)(a.h3,{id:"early-phase",children:"Early Phase"}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/fig1_amz.png",alt:"redshift",width:"800",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"Initial implementations relied on ETL pipelines that published data to Redshift, often with unstructured job flows. Business logic could exist at various layers of the ETL and was written entirely in SQL, making it difficult to track, maintain, or modify. These pipelines lacked systematic enforcement of patterns, which led to fragmentation and technical debt."}),"\n",(0,n.jsx)(a.h3,{id:"intermediate-phase",children:"Intermediate Phase"}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/fig2_amz.png",alt:"flink",width:"800",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"To improve scalability and support streaming workloads, the team transitioned to a setup involving Apache Flink and a custom-built data lake. Although this introduced broader data processing flexibility, it still had major drawbacks:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Redshift-based ETLs remained in use."}),"\n",(0,n.jsx)(a.li,{children:"Business logic and schema changes required engineering involvement."}),"\n",(0,n.jsx)(a.li,{children:"There were ongoing scalability and maintainability issues with the custom data lake."}),"\n",(0,n.jsx)(a.li,{children:"Flink introduced operational challenges of its own, such as handling version upgrades through AWS Managed Flink and providing done signal in batch operation."}),"\n"]}),"\n",(0,n.jsx)(a.h3,{id:"current-state-nexus--apache-hudi",children:"Current State: Nexus + Apache Hudi"}),"\n",(0,n.jsx)(a.p,{children:"Each of the prior approaches came with tradeoffs, especially around business logic being tightly coupled with code, making it hard for non-engineers to simulate or change metrics for a specific retail business."}),"\n",(0,n.jsxs)(a.p,{children:["Recognizing the need for better abstraction and operational maturity, the team built Nexus - a configuration-driven platform for defining and orchestrating data pipelines. All lake interactions including ingestion, transformation, schema evolution, and table management now go through Nexus. Nexus is powered by ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org",children:(0,n.jsx)(a.strong,{children:"Apache Hudi"})}),", which provides the foundation for scalable ingestion, efficient upserts, schema evolution, and transactional guarantees on Amazon S3."]}),"\n",(0,n.jsx)(a.p,{children:"This new architecture enabled the team to decouple business logic from engineering code, allowing business teams to define logic declaratively. It also introduced standardization across workloads, eliminated redundant pipelines, and laid the groundwork for scaling unit economics calculations across thousands of use cases."}),"\n",(0,n.jsx)(a.h4,{id:"key-modules-of-nexus",children:"Key Modules of Nexus"}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/fig3_amz.png",alt:"nexus",width:"800",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"Nexus consists of four core components:"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"Configuration Layer"})}),"\n",(0,n.jsx)(a.p,{children:"The topmost layer where users define their business logic in a declarative format. These configurations are typically generated and enriched with metadata by internal systems."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"NexusFlow (Orchestration)"})}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/fig4_amz.png",alt:"orch",width:"1000",align:"middle"}),"\n",(0,n.jsx)("p",{align:"center",children:(0,n.jsx)("em",{children:"Figure: Sample NexusFlow Config"})}),"\n",(0,n.jsx)(a.p,{children:"Responsible for generating and executing workflows. It operates on two levels:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Logical Layer: Comprising NexusETL jobs and other tasks."}),"\n",(0,n.jsx)(a.li,{children:"Physical Layer: Implemented via AWS Step Functions to orchestrate EMR jobs and related dependencies. NexusFlow supports extensibility through a federated model and can execute diverse task types like Spark jobs, Redshift queries, wait conditions, and legacy ETLs."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"NexusETL (Execution)"})}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/fig5_amz.png",alt:"etl",width:"1000",align:"middle"}),"\n",(0,n.jsx)("p",{align:"center",children:(0,n.jsx)("em",{children:"Figure: Sample NexusETL Config"})}),"\n",(0,n.jsx)(a.p,{children:"Executes Spark-based data transformation jobs. Jobs are defined entirely in configuration, with support for:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Built-in transforms like joins and filters"}),"\n",(0,n.jsx)(a.li,{children:"Custom UDFs"}),"\n",(0,n.jsx)(a.li,{children:"Source/Sink/Transform operators: It operates at the job abstraction level and is typically invoked by NexusFlow during orchestration."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"NexusDataLake (Storage)"})}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/fig5_amz.png",alt:"datalake",width:"1000",align:"middle"}),"\n",(0,n.jsx)("p",{align:"center",children:(0,n.jsx)("em",{children:"Figure: Sample NexusDataLake Config"})}),"\n",(0,n.jsx)(a.p,{children:"A storage abstraction layer built on Apache Hudi. NexusDataLake manages:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Table creation"}),"\n",(0,n.jsx)(a.li,{children:"Schema inference and evolution"}),"\n",(0,n.jsx)(a.li,{children:"Catalog integration: All interactions with Hudi, such as inserts, upserts, table schema changes, and metadata syncs are funneled through NexusETL and NexusFlow, maintaining consistency across the platform."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"By standardizing how data is defined, processed, and stored, Nexus has enabled a scalable, maintainable, and extensible architecture. Every data lake interaction - from ingestion to table maintenance, is performed through this configuration-first model, which now powers hundreds of use cases across Amazon retail."}),"\n",(0,n.jsx)(a.h2,{id:"why-apache-hudi",children:"Why Apache Hudi?"}),"\n",(0,n.jsx)(a.p,{children:"Apache Hudi has been central to Nexus\u2019 success, providing the core data lake storage layer for scalable ingestion, updates, and metadata management. It enables fast, incremental updates at massive scale while maintaining transactional guarantees on top of Amazon S3."}),"\n",(0,n.jsx)(a.p,{children:"In Amazon\u2019s current architecture:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Copy-on-Write (COW) table type is used for all Hudi tables."}),"\n",(0,n.jsx)(a.li,{children:"Workloads generate hundreds of billions of row updates daily, with write patterns spanning concentrated single-partition updates and wide-range backfills across up to 90 partitions."}),"\n",(0,n.jsx)(a.li,{children:"All Hudi interactions, including inserts, schema changes, and metadata syncs, are managed through Nexus."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"Key Capabilities used with Apache Hudi"})}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Efficient Upserts"}),(0,n.jsx)(a.br,{}),"\n","Hudi\u2019s design primitives such as ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/indexes",children:"indexes"})," for Copy-on-Write (CoW) tables enable high-throughput update patterns by avoiding the need to join against the entire dataset to determine which files to rewrite, which is particularly critical for our daily workloads."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Incremental Processing"}),(0,n.jsx)(a.br,{}),"\n","By using Hudi\u2019s native ",(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/blog/getting-started-incrementally-process-data-with-apache-hudi",children:"incremental pull"})," capabilities, downstream systems are able to consume only the changes between commits. This is essential for efficiently updating Contribution Profit metrics that power business decision-making."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Metadata Table"}),(0,n.jsx)(a.br,{}),"\n","Enabling the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/metadata",children:"metadata table"})," (",(0,n.jsx)(a.code,{children:"hoodie.metadata.enable=true"}),") significantly reduced job runtimes by avoiding expensive file listings on S3. This is an important optimization given the scale at which we process updates across more than 1200 Hudi tables."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Schema Evolution"}),(0,n.jsx)(a.br,{}),"\n","Table creation and evolution are fully managed through configuration in Nexus. Hudi\u2019s built-in support for ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/schema_evolution",children:"schema evolution"})," has allowed the team to onboard new use cases and adapt to changing schemas without requiring expensive rewrites or manual interventions."]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"key-learnings-from-operating-hudi-at-amazon-scale",children:"Key Learnings from Operating Hudi at Amazon Scale"}),"\n",(0,n.jsx)(a.p,{children:"Operating Apache Hudi at the scale and velocity required by Amazon\u2019s Profit Intelligence workloads surfaced a set of hard-earned lessons, especially around concurrency, metadata handling, and cost optimization. These learnings reflect both architectural refinements and operational trade-offs that others adopting Hudi at large scale may find useful."}),"\n",(0,n.jsx)(a.h3,{id:"1-concurrency-control",children:"1. Concurrency Control"}),"\n",(0,n.jsx)(a.p,{children:"At Amazon\u2019s ingestion scale - hundreds of billions of rows per day and thousands of concurrent table updates, multi-writer concurrency is a reality, not an edge case."}),"\n",(0,n.jsx)(a.p,{children:"The team initially used Optimistic Concurrency Control (OCC), which works well in environments with low write conflicts. OCC assumes that concurrent writers rarely overlap, and when they do, the job retries after detecting a conflict. However, in high-contention scenarios, like multiple jobs writing to the same partition within a short time window, this led to frequent retries and job failures."}),"\n",(0,n.jsx)(a.p,{children:"To resolve this, the team pivoted to a new table structure designed to minimize concurrent insertions. This change helped reduce contention by lowering the likelihood of multiple writers operating on overlapping partitions simultaneously. The updated design enabled using OCC while avoiding the excessive retries and failures we had initially encountered."}),"\n",(0,n.jsx)(a.h3,{id:"2-metadata-table-management-async-vs-sync-trade-offs",children:"2. Metadata Table Management: Async vs Sync Trade-Offs"}),"\n",(0,n.jsxs)(a.p,{children:["Apache Hudi\u2019s metadata table dramatically improves performance by avoiding expensive file listings on cloud object stores like S3. It maintains a persistent ",(0,n.jsx)(a.em,{children:"index"})," ",(0,n.jsx)(a.em,{children:"of files"}),", enabling faster operations such as file pruning, and data skipping."]}),"\n",(0,n.jsxs)(a.p,{children:["The team enabled Hudi\u2019s metadata table early (",(0,n.jsx)(a.code,{children:"hoodie.metadata.enable=true"}),") and started off with synchronous cleaning but switched to asynchronous cleaning to reduce job runtime. However, we ran into an issue when experimenting with asynchronous cleaning. Due to a ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/issues/11535",children:"known issue (#11535)"}),", async cleaning wasn\u2019t properly cleaning up metadata entries."]}),"\n",(0,n.jsx)(a.p,{children:"To ensure the metadata tables were properly cleaned, we switched all of  our Hudi workloads back to synchronous cleaning."}),"\n",(0,n.jsx)(a.h3,{id:"3-cost-management",children:"3. Cost Management"}),"\n",(0,n.jsx)(a.p,{children:"While Apache Hudi helped Amazon reduce data duplication and improve ingestion efficiency, we quickly realized that operational costs were not driven by storage - but by the API interaction patterns with S3."}),"\n",(0,n.jsx)(a.p,{children:"Breakdown of the cost profile:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"70% of total cost"})," came from ",(0,n.jsx)(a.code,{children:"PUT"})," requests (writes)"]}),"\n",(0,n.jsxs)(a.li,{children:["Combined ",(0,n.jsx)(a.code,{children:"PUT + GET"})," operations accounted for ",(0,n.jsx)(a.strong,{children:"80%"})," of the bill"]}),"\n",(0,n.jsx)(a.li,{children:"Storage cost remained a small fraction, even with 3+ PB of total data under management"}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Their data ingestion patterns contributed to this:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Daily workloads: Heavy concentration (99%) of updates into a single partition"}),"\n",(0,n.jsx)(a.li,{children:"Backfill workloads: Spread evenly across 30\u201390 partitions"}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"To manage this:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["We moved to ",(0,n.jsx)(a.strong,{children:"S3 Intelligent-Tiering"})," to reduce unused data storage costs"]}),"\n",(0,n.jsxs)(a.li,{children:["Enabled ",(0,n.jsx)(a.strong,{children:"EMR cluster auto-scaling"})," to dynamically adjust compute resources"]}),"\n",(0,n.jsxs)(a.li,{children:["Batched writes and carefully tuned Hudi configurations (e.g., ",(0,n.jsx)(a.code,{children:"write.batch.size"}),", ",(0,n.jsx)(a.code,{children:"compaction.small.file.limit"}),") to reduce unnecessary file churn"]}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"operational-scale-nexus-by-the-numbers",children:"Operational Scale: Nexus by the Numbers"}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Metric"}),(0,n.jsx)(a.th,{style:{textAlign:"left"},children:"Value"})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"Tables Managed"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"1200+ (5\u201315 updates/day per table)"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"Legacy SQL Deprecated"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"300,000+ lines"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"Total Data Managed"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"~3 Petabytes"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"Monthly Data Changes"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"~1 Petabyte added/deleted"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"Daily Record Updates"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"Hundreds of billions"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"Developer Time Saved"}),(0,n.jsx)(a.td,{style:{textAlign:"left"},children:"300+ days"})]})]})]}),"\n",(0,n.jsx)(a.p,{children:"Nexus with Apache Hudi as the foundation has significantly improved the scale, modularity, and maintainability of the data lake operations at Amazon. As the business use cases scale, the team is also focused on managing the increasing complexity of the data lake, while ensuring that both technical and non-technical stakeholders can interact with Nexus effectively."}),"\n",(0,n.jsxs)(a.p,{children:["This blog is based on Amazon\u2019s presentation at the Hudi Community Sync. If you are interested in watching the recorded version of the video, you can find it ",(0,n.jsx)(a.a,{href:"https://www.youtube.com/watch?v=rMXhlb7Uci8",children:"here"}),"."]}),"\n",(0,n.jsx)(a.hr,{})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},74857:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i="data:image/png;base64,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"},74912:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(52369),n=t(74848),s=t(28453),r=t(9230);const o={title:"Use Apache Hudi tables in Athena for Spark",author:"Amazon",category:"blog",image:"/assets/images/blog/2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.png",tags:["blog","apache hudi","athena","amazon spark","amazon"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://docs.aws.amazon.com/athena/latest/ug/notebooks-spark-table-formats-apache-hudi.html",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},75046:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(65938),n=t(74848),s=t(28453),r=t(9230);const o={title:"Comparing Apache Hudi, Apache Iceberg, and Delta Lake",author:"Vasanth Kumar R",category:"blog",image:"/assets/images/blog/2024-09-11-comparing-apache-hudi-apache-iceberg-and-delta-lake.jpeg",tags:["blog","apache hudi","apache iceberg","delta lake","comparison","cloudthat"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.cloudthat.com/resources/blog/comparing-apache-hudi-apache-iceberg-and-delta-lake",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},75064:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(15924),n=t(74848),s=t(28453),r=t(9230);const o={title:"Skip rocks and files: Turbocharge Trino queries with Hudi\u2019s multi-modal indexing subsystem",authors:[{name:"Nadine Farah"},{name:"Sagar Sumit"},{name:"Cole Bowden"}],category:"blog",image:"/assets/images/blog/2023-07-07-Skip-rocks-and-files-Turbocharge-Trino-queries-with-Hudi-multi-modal-indexing-subsystem.png",tags:["blog","conference","trino","apache hudi","multi modal indexing","queries"]},l=void 0,d={authorsImageUrls:[void 0,void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://trino.io/blog/2023/07/07/trino-fest-2023-onehouse-recap.html",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},75077:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/normal_operation-5bf358ee14c1ee57978939d66f0ccc3e.png"},75114:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/11/28/Apache-Hudi-Part-1-History-Getting-Started","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-11-28-Apache-Hudi-Part-1-History-Getting-Started.mdx","source":"@site/blog/2023-11-28-Apache-Hudi-Part-1-History-Getting-Started.mdx","title":"Apache Hudi (Part 1): History, Getting Started","description":"Redirecting... please wait!!","date":"2023-11-28T00:00:00.000Z","tags":[{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"getting started","permalink":"/blog/tags/getting-started"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Dipankar Mazumdar","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi (Part 1): History, Getting Started","excerpt":"Apache Hudi (Part 1): History, Getting Started","author":"Dipankar Mazumdar","category":"blog","image":"/assets/images/blog/2023-11-28-Apache-Hudi-Part-1-History-Getting-Started.png","tags":["apache hudi","blog","getting started","medium"]},"unlisted":false,"prevItem":{"title":"Mastering Data Lakes: A Deep Dive into MINIO, Hudi, and Delta Streamer","permalink":"/blog/2023/11/30/Mastering-Data-Lakes-A-Deep-Dive-into-MINIO-Hudi-and-Delta-Streamer"},"nextItem":{"title":"Real-Time Data Processing with Postgres, Debezium, Kafka, Schema Registry, and Delta Streamer Guide for Begineers","permalink":"/blog/2023/11/26/Real-Time-Data-Processing-with-Postgres-Debezium-Kafka-Schema-Registry-and-DeltaStreamer-Guide-for-Begineers"}}')},75171:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/06/20/timeline-server-in-apache-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-06-20-timeline-server-in-apache-hudi.mdx","source":"@site/blog/2023-06-20-timeline-server-in-apache-hudi.mdx","title":"Timeline Server in Apache Hudi","description":"Redirecting... please wait!!","date":"2023-06-20T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"timeline Server","permalink":"/blog/tags/timeline-server"},{"inline":true,"label":"FileSystemView","permalink":"/blog/tags/file-system-view"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.11,"hasTruncateMarker":false,"authors":[{"name":"Sivabalan Narayanan","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Timeline Server in Apache Hudi","authors":[{"name":"Sivabalan Narayanan"}],"category":"blog","image":"/assets/images/blog/2023-06-20-timeline-server-in-apache-hudi.png","tags":["blog","timeline Server","FileSystemView","medium"]},"unlisted":false,"prevItem":{"title":"How to query data in Apache Hudi using StarRocks","permalink":"/blog/2023/06/20/How-to-query-data-in-Apache-Hudi-using-StarRocks"},"nextItem":{"title":"Exploring New Frontiers: How Apache Flink, Apache Hudi and Presto Power New Insights at Scale","permalink":"/blog/2023/06/16/Exploring-New-Frontiers-How-Apache-Flink-Apache-Hudi-and-Presto-Power-New-Insights-at-Scale"}}')},75312:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(63661),n=t(74848),s=t(28453),r=t(9230);const o={title:"Exploring various storage types in Apache Hudi",excerpt:"Hudi Storage Format Overview",author:"Arun Kumar Nagaraj",category:"blog",image:"/assets/images/blog/2023-08-22-Exploring-various-storage-types-in-Apache-Hudi.png",tags:["blog","apache hudi","storage types","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@aruun1995/exploring-various-storage-types-in-apache-hudi-a3f0ab394a95",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},75341:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/09/19/A-Beginners-Guide-to-Apache-Hudi-with-PySpark-Part-1-of-2","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-09-19-A-Beginners-Guide-to-Apache-Hudi-with-PySpark-Part-1-of-2.mdx","source":"@site/blog/2023-09-19-A-Beginners-Guide-to-Apache-Hudi-with-PySpark-Part-1-of-2.mdx","title":"A Beginner\u2019s Guide to Apache Hudi with PySpark \u2014 Part 1 of 2","description":"Redirecting... please wait!!","date":"2023-09-19T00:00:00.000Z","tags":[{"inline":true,"label":"pyspark","permalink":"/blog/tags/pyspark"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Sagar Lakshmipathy","key":null,"page":null}],"frontMatter":{"title":"A Beginner\u2019s Guide to Apache Hudi with PySpark \u2014 Part 1 of 2","author":"Sagar Lakshmipathy","category":"blog","image":"/assets/images/blog/2023-09-19-A-Beginners-Guide-to-Apache-Hudi-with-PySpark-Part-1-of-2.png","tags":["pyspark","apache hudi","how-to","medium"]},"unlisted":false,"prevItem":{"title":"Exploring the Architecture of Apache Iceberg, Delta Lake, and Apache Hudi","permalink":"/blog/2023/09/22/Exploring-the-Architecture-of-Apache-Iceberg-Delta-Lake-and-Apache-Hudi"},"nextItem":{"title":"Apache Hudi: From Zero To One (3/10)","permalink":"/blog/2023/09/15/Apache-Hudi-From-Zero-To-One-blog-3"}}')},75367:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/2020-12-01-t3go-architecture-93f0c75faf35e99a62d7ab952a12ff74.png"},75525:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/07/07/how-stifel-built-a-modern-data-platform-using-aws-glue-and-an-event-driven-domain-architecture","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-07-07-how-stifel-built-a-modern-data-platform-using-aws-glue-and-an-event-driven-domain-architecture.mdx","source":"@site/blog/2025-07-07-how-stifel-built-a-modern-data-platform-using-aws-glue-and-an-event-driven-domain-architecture.mdx","title":"How Stifel built a modern data platform using AWS Glue and an event-driven domain architecture","description":"Redirecting... please wait!!","date":"2025-07-07T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"aws","permalink":"/blog/tags/aws"},{"inline":true,"label":"AWS Glue","permalink":"/blog/tags/aws-glue"},{"inline":true,"label":"AWS Blogs","permalink":"/blog/tags/aws-blogs"},{"inline":true,"label":"Amazon EMR","permalink":"/blog/tags/amazon-emr"},{"inline":true,"label":"AWS Lake Formation","permalink":"/blog/tags/aws-lake-formation"},{"inline":true,"label":"Data Governance","permalink":"/blog/tags/data-governance"},{"inline":true,"label":"Lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"det","permalink":"/blog/tags/det"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"Amit Maindola and Srinivas Kandi, Hossein Johari, Ahmad Rawashdeh, Lei Meng","key":null,"page":null}],"frontMatter":{"title":"How Stifel built a modern data platform using AWS Glue and an event-driven domain architecture","author":"Amit Maindola and Srinivas Kandi, Hossein Johari, Ahmad Rawashdeh, Lei Meng","category":"blog","image":"/assets/images/blog/2025-07-07-how-stifel-built-a-modern-data-platform-using-aws-glue-and-an-event-driven-domain-architecture.png","tags":["blog","Apache Hudi","aws","AWS Glue","AWS Blogs","Amazon EMR","AWS Lake Formation","Data Governance","Lakehouse","use-case","det"]},"unlisted":false,"prevItem":{"title":"Building a RAG-based AI Recommender (Part 1/2)","permalink":"/blog/2025/07/10/building-a-rag-based-ai-recommender"},"nextItem":{"title":"Why Uber Built Hudi: The Strategic Decision Behind a Custom Table Format","permalink":"/blog/2025/07/03/why-uber-built-hudi-the-strategic-decision-behind-a-custom-table-format"}}')},75706:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig3-2a993e3d03e054e6e2697772a56e673f.png"},75801:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/07/11/build-open-lakehouse-using-apache-hudi-and-dbt","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-07-11-build-open-lakehouse-using-apache-hudi-and-dbt.md","source":"@site/blog/2022-07-11-build-open-lakehouse-using-apache-hudi-and-dbt.md","title":"Build Open Lakehouse using Apache Hudi & dbt","description":"The focus of this blog is to show you how to build an open lakehouse leveraging incremental data processing and performing field-level updates. We are excited to announce that you can now use Apache Hudi + dbt for building open data lakehouses.","date":"2022-07-11T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"deltastreamer","permalink":"/blog/tags/deltastreamer"},{"inline":true,"label":"incremental processing","permalink":"/blog/tags/incremental-processing"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":6.79,"hasTruncateMarker":false,"authors":[{"name":"Vinoth Govindarajan","key":null,"page":null}],"frontMatter":{"title":"Build Open Lakehouse using Apache Hudi & dbt","excerpt":"How to style blog focused projects on teaching how to build an open Lakehouse using Apache Hudi & dbt","author":"Vinoth Govindarajan","category":"blog","image":"/assets/images/blog/hudi_dbt_lakehouse.png","tags":["how-to","deltastreamer","incremental processing","apache hudi"]},"unlisted":false,"prevItem":{"title":"How NerdWallet uses AWS and Apache Hudi to build a serverless, real-time analytics platform","permalink":"/blog/2022/08/09/How-NerdWallet-uses-AWS-and-Apache-Hudi-to-build-a-serverless-real-time-analytics-platform"},"nextItem":{"title":"Apache Hudi vs Delta Lake - Transparent TPC-DS Lakehouse Performance Benchmarks","permalink":"/blog/2022/06/29/Apache-Hudi-vs-Delta-Lake-transparent-tpc-ds-lakehouse-performance-benchmarks"}}')},75893:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(84348),n=t(74848),s=t(28453),r=t(9230);const o={title:"Origins of Data Lake at Grofers",authors:[{name:"Akshay Agarwal"}],category:"blog",image:"/assets/images/blog/2020-10-19-Origins-of-Data-Lake-at-Grofers.gif",tags:["use-case","datalake","change data capture","cdc","grofers"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://lambda.grofers.com/origins-of-data-lake-at-grofers-6c011f94b86c",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},76011:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/06/04/Asynchronous-Indexing-Using-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-06-04-Asynchronous-Indexing-Using-Hudi.mdx","source":"@site/blog/2022-06-04-Asynchronous-Indexing-Using-Hudi.mdx","title":"Asynchronous Indexing using Hudi","description":"Redirecting... please wait!!","date":"2022-06-04T00:00:00.000Z","tags":[{"inline":true,"label":"design","permalink":"/blog/tags/design"},{"inline":true,"label":"multi modal indexing","permalink":"/blog/tags/multi-modal-indexing"},{"inline":true,"label":"onehouse","permalink":"/blog/tags/onehouse"},{"inline":true,"label":"async indexing","permalink":"/blog/tags/async-indexing"}],"readingTime":0.1,"hasTruncateMarker":false,"authors":[{"name":"Sagar Sumit","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Asynchronous Indexing using Hudi","authors":[{"name":"Sagar Sumit"}],"category":"blog","image":"/assets/images/blog/2022-06-04-async-index.png","tags":["design","multi modal indexing","onehouse","async indexing"]},"unlisted":false,"prevItem":{"title":"Hudi\u2019s Column Stats Index and Data Skipping feature help speed up queries by an orders of magnitude!","permalink":"/blog/2022/06/09/Singificant-queries-speedup-from-Hudi-Column-Stats-Index-and-Data-Skipping-features"},"nextItem":{"title":"The story of building a data lake that can be deleted on a record-by-record basis using Apache Hudi","permalink":"/blog/2022/05/25/Record-by-record-deletable-data-lake-using-Apache-Hudi"}}')},76050:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(63928),n=t(74848),s=t(28453),r=t(9230);const o={title:"Zendesk - Insights for CTOs: Part 3 \u2013 Growing your business with modern data capabilities",authors:[{name:"Syed Jaffry"},{name:"Johnathan Hwang"}],category:"blog",image:"/assets/images/blog/2022-03-24-insights-for-ctos-part-3.png",tags:["blog","modern data architecture","near real-time analytics","gdpr deletion","streaming ingestion","amazon"]},l=void 0,d={authorsImageUrls:[void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/architecture/insights-for-ctos-part-3-growing-your-business-with-modern-data-capabilities/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},76114:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/07/03/why-uber-built-hudi-the-strategic-decision-behind-a-custom-table-format","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-07-03-why-uber-built-hudi-the-strategic-decision-behind-a-custom-table-format.mdx","source":"@site/blog/2025-07-03-why-uber-built-hudi-the-strategic-decision-behind-a-custom-table-format.mdx","title":"Why Uber Built Hudi: The Strategic Decision Behind a Custom Table Format","description":"Redirecting... please wait!!","date":"2025-07-03T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"Apache Iceberg","permalink":"/blog/tags/apache-iceberg"},{"inline":true,"label":"Lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"Uber","permalink":"/blog/tags/uber"},{"inline":true,"label":"det","permalink":"/blog/tags/det"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"ThamizhElango Natarajan","key":null,"page":null}],"frontMatter":{"title":"Why Uber Built Hudi: The Strategic Decision Behind a Custom Table Format","author":"ThamizhElango Natarajan","category":"blog","image":"/assets/images/blog/2025-07-03-why-uber-built-hudi-the-strategic-decision-behind-a-custom-table-format.jpg","tags":["blog","Apache Hudi","Apache Iceberg","Lakehouse","use-case","Uber","det"]},"unlisted":false,"prevItem":{"title":"How Stifel built a modern data platform using AWS Glue and an event-driven domain architecture","permalink":"/blog/2025/07/07/how-stifel-built-a-modern-data-platform-using-aws-glue-and-an-event-driven-domain-architecture"},"nextItem":{"title":"Lakehouse Architecture - Apache Hudi and Apache Iceberg","permalink":"/blog/2025/07/02/Lakehouse-Architecture-apache-hudi-and-apache-iceberg"}}')},76226:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/SingleWriterInline-d18346421aa3f1d11a3247164389e1ce.gif"},76364:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(48279),n=t(74848),s=t(28453),r=t(9230);const o={title:"Can Big Data Solutions Be Affordable?",category:"blog",image:"/assets/images/blog/2020-11-29-Can-Big-Data-Solutions-Be-Affordable.jpg",tags:["blog","big-data","near real-time analytics","analyticsinsight"]},l=void 0,d={authorsImageUrls:[]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.analyticsinsight.net/can-big-data-solutions-be-affordable/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},76679:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/04/04/Key-Learnings-on-Using-Apache-HUDI-in-building-Lakehouse-Architecture-at-Halodoc","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-04-04-Key-Learnings-on-Using-Apache-HUDI-in-building-Lakehouse-Architecture-at-Halodoc.mdx","source":"@site/blog/2022-04-04-Key-Learnings-on-Using-Apache-HUDI-in-building-Lakehouse-Architecture-at-Halodoc.mdx","title":"Key Learnings on Using Apache HUDI in building Lakehouse Architecture @ Halodoc","description":"Redirecting... please wait!!","date":"2022-04-04T00:00:00.000Z","tags":[{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"incremental processing","permalink":"/blog/tags/incremental-processing"},{"inline":true,"label":"halodoc","permalink":"/blog/tags/halodoc"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Jitendra Shah","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Key Learnings on Using Apache HUDI in building Lakehouse Architecture @ Halodoc","authors":[{"name":"Jitendra Shah"}],"category":"blog","image":"/assets/images/blog/2022-04-04-halodoc-lakehouse-architecture.png","tags":["use-case","lakehouse","incremental processing","halodoc"]},"unlisted":false,"prevItem":{"title":"Corrections in data lakehouse table format comparisons","permalink":"/blog/2022/04/19/Corrections-in-data-lakehouse-table-format-comparisons"},"nextItem":{"title":"New features from Apache Hudi 0.9.0 on Amazon EMR","permalink":"/blog/2022/04/04/New-features-from-Apache-Hudi-0.9.0-on-Amazon-EMR"}}')},76952:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(32929),n=t(74848),s=t(28453),r=t(9230);const o={title:"Record Level Indexing in Apache Hudi Delivers 70% Faster Point Lookups",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2024-03-30-record-level-indexing-apache-hudi-delivers-70-faster-point.png",tags:["blog","apache hudi","record level index","performance","linkedin"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.linkedin.com/pulse/record-level-indexing-apache-hudi-delivers-70-faster-point-shah-hlite/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},77036:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(87115),n=t(74848),s=t(28453),r=t(9230);const o={title:"Delta, Hudi, and Iceberg: The Data Lakehouse Trifecta",authors:[{name:"Andrey Gusarov"}],category:"blog",image:"/assets/images/blog/0426-lakehouse-trifecta.png",tags:["lakehouse","delta lake","apache hudi","apache iceberg","comparison","dzone"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://dzone.com/articles/delta-hudi-and-iceberg-the-data-lakehouse-trifecta",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},77089:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(69691),n=t(74848),s=t(28453),r=t(9230);const o={title:"Cost-Efficient Open Source Big Data Platform at Uber",authors:[{name:"Zheng Shao"},{name:"Mohammad Islam"}],category:"blog",image:"/assets/images/blog/2021-08-11-cost-efficient-open-source-big-data-platform-at-uber.png",tags:["cost efficiency","optimization","bigdata","data platform","incremental processing","uber"]},l=void 0,d={authorsImageUrls:[void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://eng.uber.com/cost-efficient-big-data-platform/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},77386:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i={container:"container_HUZa",grid:"grid_XV9t",card:"card_lCc2",link:"link_Q4Fi",imageWrapper:"imageWrapper_NOzp",image:"image_U_FG",content:"content__2a1",meta:"meta_cGkM",date:"date_A2Cp",title:"title_K4Jj",description:"description_a9rF",pagination:"pagination_R8bK",paginationButton:"paginationButton_mWSk",paginationNumbers:"paginationNumbers_HIVB",paginationNumber:"paginationNumber_Q4aD",active:"active_NzRk",paginationInfo:"paginationInfo_o7Ou",authorName:"authorName_Sbdz",faqCard:"faqCard_IMmB",faqLink:"faqLink_jLxp",faqContent:"faqContent_DVVp",faqDescription:"faqDescription_kWFy",faqAction:"faqAction_vi6g",faqLinkText:"faqLinkText_M4rA"}},77400:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image5-61c7ba9d9a0b31e28ce22ae67eb12d08.png"},77461:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/08/03/Data-lake-Table-formats-Apache-Iceberg-vs-Apache-Hudi-vs-Delta-lake","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-08-03-Data-lake-Table-formats-Apache-Iceberg-vs-Apache-Hudi-vs-Delta-lake.mdx","source":"@site/blog/2023-08-03-Data-lake-Table-formats-Apache-Iceberg-vs-Apache-Hudi-vs-Delta-lake.mdx","title":"Data lake Table formats: Apache Iceberg vs Apache Hudi vs Delta lake","description":"Redirecting... please wait!!","date":"2023-08-03T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"iceberg","permalink":"/blog/tags/iceberg"},{"inline":true,"label":"delta lake","permalink":"/blog/tags/delta-lake"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Shashwat Pandey","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Data lake Table formats: Apache Iceberg vs Apache Hudi vs Delta lake","authors":[{"name":"Shashwat Pandey"}],"category":"blog","image":"/assets/images/blog/2023-08-03-Data-lake-Table-formats-Apache-Iceberg-vs-Apache-Hudi-vs-Delta-lake.png","tags":["blog","hudi","iceberg","delta lake","medium"]},"unlisted":false,"prevItem":{"title":"Create an Apache Hudi-based-near-real-time transactional data lake using AWS DMS, Amazon Kinesis, AWS Glue streaming ETL, and data visualization using Amazon QuickSight","permalink":"/blog/2023/08/03/Create-an-Apache-Hudi-based-near-real-time-transactional-data lake-using-AWS-DMS-Amazon-Kinesis-AWS-Glue-streaming-ETL-and-data-visualization-using-Amazon-QuickSight"},"nextItem":{"title":"Apache Hudi: Revolutionizing Big Data Management for Real-Time Analytics","permalink":"/blog/2023/07/27/Apache-Hudi-Revolutionizing-Big-Data-Management-for-Real-Time-Analytics"}}')},77586:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(2128),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi: From Zero To One (7/10)",excerpt:"Concurrently run writers and table services",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2023-12-06-Apache-Hudi-From-Zero-To-One-blog-7.png",tags:["hudi","spark","blog","course","tutorial","datumagic","data lake","lakehouse","apache hudi","apache spark"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blog.datumagic.ai/p/apache-hudi-from-zero-to-one-710",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},77731:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/03/09/Build-a-serverless-pipeline-to-analyze-streaming-data-using-AWS-Glue-Apache-Hudi-and-Amazon-S3","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-03-09-Build-a-serverless-pipeline-to-analyze-streaming-data-using-AWS-Glue-Apache-Hudi-and-Amazon-S3.mdx","source":"@site/blog/2022-03-09-Build-a-serverless-pipeline-to-analyze-streaming-data-using-AWS-Glue-Apache-Hudi-and-Amazon-S3.mdx","title":"Build a serverless pipeline to analyze streaming data using AWS Glue, Apache Hudi, and Amazon S3","description":"Redirecting... please wait!!","date":"2022-03-09T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"streaming ingestion","permalink":"/blog/tags/streaming-ingestion"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"Nikhil Khokhar","socials":{},"key":null,"page":null},{"name":"Dipta Bhattacharya","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Build a serverless pipeline to analyze streaming data using AWS Glue, Apache Hudi, and Amazon S3","authors":[{"name":"Nikhil Khokhar"},{"name":"Dipta Bhattacharya"}],"category":"blog","image":"/assets/images/blog/2022-03-09-serverless-pipeline-using-glue-hudi-s3.png","tags":["how-to","streaming ingestion","amazon"]},"unlisted":false,"prevItem":{"title":"Zendesk - Insights for CTOs: Part 3 \u2013 Growing your business with modern data capabilities","permalink":"/blog/2022/03/24/Zendesk-Insights-for-CTOs-Part-3-Growing-your-business-with-modern-data-capabilities"},"nextItem":{"title":"Create a low-latency source-to-data lake pipeline using Amazon MSK Connect, Apache Flink, and Apache Hudi","permalink":"/blog/2022/03/01/Create-a-low-latency-source-to-data-lake-pipeline-using-Amazon-MSK-Connect-Apache-Flink-and-Apache-Hudi"}}')},77936:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(64113),n=t(74848),s=t(28453),r=t(9230);const o={title:"Introducing native support for Apache Hudi, Delta Lake, and Apache Iceberg on AWS Glue for Apache Spark, Part 2: AWS Glue Studio Visual Editor",authors:[{name:"Noritaka Sekiyama"},{name:"Scott Long"},{name:"Sean Ma"}],category:"blog",image:"/assets/images/blog/native-support-hudi-for-glue-studio.png",tags:["aws glue","glue studio","blog","amazon"]},l=void 0,d={authorsImageUrls:[void 0,void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/part-2-glue-studio-visual-editor-introducing-native-support-for-apache-hudi-delta-lake-and-apache-iceberg-on-aws-glue-for-apache-spark/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},77969:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/05/29/different-query-types-with-apache-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-05-29-different-query-types-with-apache-hudi.mdx","source":"@site/blog/2023-05-29-different-query-types-with-apache-hudi.mdx","title":"Different Query types with Apache Hudi","description":"Redirecting... please wait!!","date":"2023-05-29T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"snapshot query","permalink":"/blog/tags/snapshot-query"},{"inline":true,"label":"real-time query","permalink":"/blog/tags/real-time-query"},{"inline":true,"label":"time travel query","permalink":"/blog/tags/time-travel-query"},{"inline":true,"label":"timestamp as of query","permalink":"/blog/tags/timestamp-as-of-query"},{"inline":true,"label":"read optimized query","permalink":"/blog/tags/read-optimized-query"},{"inline":true,"label":"incremental query","permalink":"/blog/tags/incremental-query"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Sivabalan Narayanan","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Different Query types with Apache Hudi","authors":[{"name":"Sivabalan Narayanan"}],"category":"blog","tags":["blog","snapshot query","real-time query","time travel query","timestamp as of query","read optimized query","incremental query","medium"]},"unlisted":false,"prevItem":{"title":"Text-Based Search: From Elastic Search to Vector Search","permalink":"/blog/2023/06/03/text-based-search-from-elastic-search-to-vector-search"},"nextItem":{"title":"Hudi Metafields demystified","permalink":"/blog/2023/05/19/hudi-metafields-demystified"}}')},78026:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(65481),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi, Spark and Minio: Hands-on Lab in Docker",author:"Sanjeet Shukla",category:"blog",image:"/assets/images/blog/2024-10-02-apache-hudi-spark-and-minio-hands-on-lab-in-docker.jpeg",tags:["how-to","Apache Hudi","Apache Spark","Minio","docker","devgenius"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blog.devgenius.io/apache-hudi-spark-and-minio-hands-on-lab-in-docker-f1daa099ccd0",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},78027:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(92379),n=t(74848),s=t(28453),r=t(9230);const o={title:"Delta, Hudi, Iceberg \u2014 A Benchmark Compilation",excerpt:"Benchmark Compilation",author:"Kyle Weller",category:"blog",image:"/assets/images/blog/2023-08-28-Delta-Hudi-Iceberg-A-Benchmark-Compilation.png",tags:["performance","apache hudi","delta lake","iceberg","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@kywe665/delta-hudi-iceberg-a-benchmark-compilation-a5630c69cffc",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},78038:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/01/30/an-intro-to-hudi-with-minio","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-01-30-an-intro-to-hudi-with-minio.mdx","source":"@site/blog/2025-01-30-an-intro-to-hudi-with-minio.mdx","title":"An intro to Hudi with MinIO","description":"Redirecting... please wait!!","date":"2025-01-30T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"minio","permalink":"/blog/tags/minio"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Simbu","key":null,"page":null}],"frontMatter":{"title":"An intro to Hudi with MinIO","author":"Simbu","category":"blog","image":"/assets/images/blog/2025-01-30-an-intro-to-hudi-with-minio.jpeg","tags":["blog","apache hudi","minio","medium"]},"unlisted":false,"prevItem":{"title":"Curious Engineering Facts (Lakehouse | Apache Hudi | Daft |Positional argument|) : March Release 19 : 25","permalink":"/blog/2025/02/23/curious-engineering-facts-lakehouse-apache-hudi-daft-positional-argument"},"nextItem":{"title":"Concurrency Control in Open Data Lakehouse","permalink":"/blog/2025/01/28/concurrency-control"}}')},78219:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/02/22/Getting-Started-Manage-your-Hudi-tables-with-the-admin-Hudi-CLI-tool","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-02-22-Getting-Started-Manage-your-Hudi-tables-with-the-admin-Hudi-CLI-tool.mdx","source":"@site/blog/2023-02-22-Getting-Started-Manage-your-Hudi-tables-with-the-admin-Hudi-CLI-tool.mdx","title":"Getting Started: Manage your Hudi tables with the admin Hudi-CLI tool","description":"Redirecting... please wait!!","date":"2023-02-22T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"hudi cli","permalink":"/blog/tags/hudi-cli"},{"inline":true,"label":"onehouse","permalink":"/blog/tags/onehouse"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Sivabalan Narayanan","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Getting Started: Manage your Hudi tables with the admin Hudi-CLI tool","authors":[{"name":"Sivabalan Narayanan"}],"category":"blog","image":"/assets/images/blog/2023-02-22-Getting-Started-Manage-your-Hudi-tables-with-the-admin-Hudi-CLI-tool.png","tags":["how-to","hudi cli","onehouse"]},"unlisted":false,"prevItem":{"title":"Setting Uber\u2019s Transactional Data Lake in Motion with Incremental ETL Using Apache Hudi","permalink":"/blog/2023/03/16/Setting-Uber-Transactional-Data-Lake-in-Motion-with-Incremental-ETL-Using-Apache-Hudi"},"nextItem":{"title":"Bulk Insert Sort Modes with Apache Hudi","permalink":"/blog/2023/02/19/bulk-insert-sort-modes-with-apache-hudi"}}')},78292:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/04/02/global-vs-non-global-index-in-apache-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-04-02-global-vs-non-global-index-in-apache-hudi.mdx","source":"@site/blog/2023-04-02-global-vs-non-global-index-in-apache-hudi.mdx","title":"Global vs Non-global index in Apache Hudi","description":"Redirecting... please wait!!","date":"2023-04-02T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"indexing","permalink":"/blog/tags/indexing"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Sivabalan Narayanan","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Global vs Non-global index in Apache Hudi","authors":[{"name":"Sivabalan Narayanan"}],"category":"blog","tags":["how-to","indexing","medium"]},"unlisted":false,"prevItem":{"title":"Speed up your write latencies using Bucket Index in Apache Hudi","permalink":"/blog/2023/04/07/Speed-up-your-write-latencies-using-Bucket-Index-in-Apache-Hudi"},"nextItem":{"title":"Spark ETL Chapter 8 with Lakehouse | Apache HUDI","permalink":"/blog/2023/03/23/Spark-ETL-Chapter-8-with-Lakehouse-Apache-HUDI"}}')},78564:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(86840),n=t(74848),s=t(28453);const r={title:"Streaming Responsibly - How Apache Hudi maintains optimum sized files",excerpt:"Maintaining well-sized files can improve query performance significantly",author:"shivnarayan",category:"blog",image:"/assets/images/blog/2021-03-01-hudi-file-sizing.png",tags:["design","file sizing","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"During Write vs After Write",id:"during-write-vs-after-write",level:2},{value:"Configs",id:"configs",level:3},{value:"Example",id:"example",level:3}];function c(e){const a={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.p,{children:"Apache Hudi is a data lake platform technology that provides several functionalities needed to build and manage data lakes.\nOne such key feature that hudi provides is self-managing file sizing so that users don\u2019t need to worry about\nmanual table maintenance. Having a lot of small files will make it harder to achieve good query performance, due to query engines\nhaving to open/read/close files way too many times, to plan and execute queries. But for streaming data lake use-cases,\ninherently ingests are going to end up having smaller volume of writes, which might result in lot of small files if no special handling is done."}),"\n",(0,n.jsx)(a.h2,{id:"during-write-vs-after-write",children:"During Write vs After Write"}),"\n",(0,n.jsxs)(a.p,{children:["Common approaches to writing very small files and then later stitching them together solve for system scalability issues posed\nby small files but might violate query SLA's by exposing small files to them. In fact, you can easily do so on a Hudi table,\nby running a clustering operation, as detailed in a ",(0,n.jsx)(a.a,{href:"/blog/2021/01/27/hudi-clustering-intro",children:"previous blog"}),"."]}),"\n",(0,n.jsx)(a.p,{children:"In this blog, we discuss file sizing optimizations in Hudi, during the initial write time, so we don't have to effectively\nre-write all data again, just for file sizing. If you want to have both (a) self managed file sizing and\n(b) Avoid exposing small files to queries, automatic file sizing feature saves the day."}),"\n",(0,n.jsxs)(a.p,{children:["Hudi has the ability to maintain a configured target file size, when performing inserts/upsert operations.\n(Note: bulk_insert operation does not provide this functionality and is designed as a simpler replacement for\nnormal ",(0,n.jsx)(a.code,{children:"spark.write.parquet"}),")."]}),"\n",(0,n.jsx)(a.h3,{id:"configs",children:"Configs"}),"\n",(0,n.jsx)(a.p,{children:"For illustration purposes, we are going to consider only COPY_ON_WRITE table."}),"\n",(0,n.jsx)(a.p,{children:"Configs of interest before we dive into the algorithm:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"/docs/configurations#hoodieparquetmaxfilesize",children:"Max file size"}),": Max size for a given data file. Hudi will try to maintain file sizes to this configured value ",(0,n.jsx)("br",{})]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"/docs/configurations#hoodieparquetsmallfilelimit",children:"Soft file limit"}),": Max file size below which a given data file is considered to a small file ",(0,n.jsx)("br",{})]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"/docs/configurations#hoodiecopyonwriteinsertsplitsize",children:"Insert split size"}),": Number of inserts grouped for a single partition. This value should match\nthe number of records in a single file (you can determine based on max file size and per record size)"]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"For instance, if your first config value is 120MB and 2nd config value is set to 100MB, any file whose size is < 100MB\nwould be considered a small file."}),"\n",(0,n.jsx)(a.p,{children:"If you wish to turn off this feature, set the config value for soft file limit to 0."}),"\n",(0,n.jsx)(a.h3,{id:"example",children:"Example"}),"\n",(0,n.jsx)(a.p,{children:"Let\u2019s say this is the layout of data files for a given partition."}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{alt:"Initial layout",src:t(94519).A+"",width:"886",height:"426"}),"\n",(0,n.jsx)(a.em,{children:"Figure: Initial data file sizes for a given partition of interest"})]}),"\n",(0,n.jsx)(a.p,{children:"Let\u2019s assume the configured values for max file size and small file size limit are 120MB and 100MB. File_1\u2019s current\nsize is 40MB, File_2\u2019s size is 80MB, File_3\u2019s size is 90MB, File_4\u2019s size is 130MB and File_5\u2019s size is 105MB. Let\u2019s see\nwhat happens when a new write happens."}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Step 1:"})," Assigning updates to files. In this step, We look up the index to find the tagged location and records are\nassigned to respective files. Note that we assume updates are only going to increase the file size and that would simply result\nin a much bigger file. When updates lower the file size (by say, nulling out lot of fields), then a subsequent write will deem\nit a small file."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Step 2:"}),"  Determine small files for each partition path. The soft file limit config value will be leveraged here\nto determine eligible small files. In our example, given the config value is set to 100MB, the small files are File_1(40MB)\nand File_2(80MB) and file_3\u2019s (90MB)."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Step 3:"})," Once small files are determined, incoming inserts are assigned to them so that they reach their max capacity of\n120MB. File_1 will be ingested with 80MB worth of inserts, file_2 will be ingested with 40MB worth of inserts and\nFile_3 will be ingested with 30MB worth of inserts."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{alt:"Bin packing small files",src:t(7604).A+"",width:"1280",height:"625"}),"\n",(0,n.jsx)(a.em,{children:"Figure: Incoming records are bin packed to existing small files"})]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Step 4:"})," Once all small files are bin packed to its max capacity and if there are pending inserts unassigned, new file\ngroups/data files are created and inserts are assigned to them. Number of records per new data file is determined from insert split\nsize config. Assuming the insert split size is configured to 120k records, if there are 300k remaining records, 3 new\nfiles will be created in which 2 of them (File_6 and File_7) will be filled with 120k records and the last one (File_8)\nwill be filled with 60k records (assuming each record is 1000 bytes). In future ingestions, 3rd new file will be\nconsidered as a small file to be packed with more data."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{alt:"Assigning to new files",src:t(86214).A+"",width:"1300",height:"963"}),"\n",(0,n.jsx)(a.em,{children:"Figure: Remaining records are assigned to new files"})]}),"\n",(0,n.jsx)(a.p,{children:"Hudi leverages mechanisms such as custom partitioning for optimized record distribution to different files, executing\nthe algorithm above. After this round of ingestion is complete, all files except File_8 are nicely sized to the optimum size.\nThis process is followed during every ingestion to ensure there are no small files in your Hudi tables."}),"\n",(0,n.jsx)(a.p,{children:"Hopefully the blog gave you an overview into how hudi manages small files and assists in boosting your query performance."})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},78633:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide15-4340ab9068964e510a89f2bb70ef4adb.png"},78660:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/batched-marker-creation-e8455c544f3b11ceed810b663df59f7f.png"},78685:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/spark_real_time_view-1b76ee7a1e9e884439da562f46d95f57.png"},78821:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(57738),n=t(74848),s=t(28453),r=t(9230);const o={title:"Lakehouse or Warehouse? Part 2 of 2",excerpt:"Lakehouse or Warehouse? Part 2 of 2",author:"Floyd Smith",category:"blog",image:"/assets/images/blog/2023-09-12-Lakehouse-or-Warehouse-Part-2-of-2.png",tags:["data warehouse","data lakehouse","apache hudi","onehouse","blog"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.onehouse.ai/blog/lakehouse-or-warehouse-part-2-of-2",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},78843:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide17-ae7bfe343dd1d0cd170f2f5d00094dea.png"},79003:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(19635),n=t(74848),s=t(28453),r=t(9230);const o={title:"ACID transformations on Distributed file system",authors:[{name:"Rajasekhar"}],category:"blog",image:"/assets/images/blog/2022-02-09-acid-transformations-on-distributed-files-systems.png",tags:["blog","walmartglobaltech"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/walmartglobaltech/acid-transformations-on-distributed-file-system-fdec5301c1b1",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},79013:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/dms-demo-files-2c926cf6a9fb12b5e9bc44a65df8e2b7.png"},79200:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(40789),n=t(74848),s=t(28453),r=t(9230);const o={title:"Leverage Partition Paths of your data lake tables to Optimize Data Retrieval Costs on the cloud",excerpt:"Leverage Partition Paths of your data lake tables to Optimize Data Retrieval Costs on the cloud",author:"Krishna Prasad",category:"blog",image:"/assets/images/blog/2024-01-30-Leverage-Partition-Paths-of-your-data-lake-tables-to-Optimize-Data-Retrieval-Costs-on-the-cloud.png",tags:["blog","apache hudi","medium","intermediate","aws glue","cost","apache spark","partition"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@krishnaiitd/leverage-partition-paths-of-your-data-lake-tables-to-optimize-data-retrieval-costs-on-the-cloud-6f4cced24398",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},79373:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/01/06/apache-hudi-2021-a-year-in-review","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-01-06-apache-hudi-2021-a-year-in-review.mdx","source":"@site/blog/2022-01-06-apache-hudi-2021-a-year-in-review.mdx","title":"Apache Hudi - 2021 a Year in Review","description":"As the year came to end, I took some time to reflect on where we are and what we accomplished in 2021. I am humbled by how strong our community is and how regardless of it being another tough pandemic year, that people from around the globe leaned in together and made this the best year yet for Apache Hudi. In this blog I want to recap some of the 2021 highlights.","date":"2022-01-06T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"community","permalink":"/blog/tags/community"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":4.32,"hasTruncateMarker":true,"authors":[{"name":"vinoth","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi - 2021 a Year in Review","excerpt":"A reflection on the growth and momentum of Apache Hudi in 2021","author":"vinoth","category":"blog","image":"/assets/images/Hudi_community.png","tags":["blog","community","apache hudi"]},"unlisted":false,"prevItem":{"title":"Change Data Capture with Debezium and Apache Hudi","permalink":"/blog/2022/01/14/change-data-capture-with-debezium-and-apache-hudi"},"nextItem":{"title":"The Art of Building Open Data Lakes with Apache Hudi, Kafka, Hive, and Debezium","permalink":"/blog/2021/12/31/The-Art-of-Building-Open-Data-Lakes-with-Apache-Hudi-Kafka-Hive-and-Debezium"}}')},79556:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/12/28/apache-hudi-2023-a-year-in-review","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-12-28-apache-hudi-2023-a-year-in-review.mdx","source":"@site/blog/2023-12-28-apache-hudi-2023-a-year-in-review.mdx","title":"Apache Hudi 2023: A Year In Review","description":"In the warm glow of the holiday season, I am delighted to convey a message of deep appreciation on behalf of the","date":"2023-12-28T00:00:00.000Z","tags":[{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"community","permalink":"/blog/tags/community"}],"readingTime":9.89,"hasTruncateMarker":false,"authors":[{"name":"Shiyan Xu","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi 2023: A Year In Review","excerpt":"Reflect on and celebrate the myriad of exciting developments and accomplishments that have defined the year 2023 for the Hudi community.","author":"Shiyan Xu","category":"blog","image":"/assets/images/blog/2023-12-28-a-year-in-review-2023/00.cover.png","tags":["apache hudi","community"]},"unlisted":false,"prevItem":{"title":"From Data lake to Microservices: Unleashing the Power of Apache Hudi\'s Record Level Index with FastAPI and Spark Connect","permalink":"/blog/2024/01/01/From-Data-lake-to-Microservices-Unleashing-the-Power-of-Apache-Hudi-Record-Level-Index-with-FastAPI-and-Spark-Connect"},"nextItem":{"title":"What is Apache Hudi","permalink":"/blog/2023/12/13/what-is-apache-hudi"}}')},79657:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(88710),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Iceberg vs Delta Lake vs Apache Hudi",author:"AlgoDays",category:"blog",image:"/assets/images/blog/2025-01-09-apache-iceberg-vs-delta-lake-vs-apache-hudi.jpeg",tags:["blog","apache hudi","apache iceberg","delta lake","comparison","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@algodaysindia/apache-iceberg-vs-delta-lake-vs-apache-hudi-f987fee8dbe1",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},79663:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/12/09/Getting-started-with-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-12-09-Getting-started-with-Apache-Hudi.mdx","source":"@site/blog/2023-12-09-Getting-started-with-Apache-Hudi.mdx","title":"Getting started with Apache Hudi","description":"Redirecting... please wait!!","date":"2023-12-09T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"},{"inline":true,"label":"beginner","permalink":"/blog/tags/beginner"}],"readingTime":0.11,"hasTruncateMarker":false,"authors":[{"name":"DataCouch","key":null,"page":null}],"frontMatter":{"title":"Getting started with Apache Hudi","excerpt":"Getting started with Apache Hudi","author":"DataCouch","category":"blog","image":"/assets/images/blog/2023-12-09-Getting-started-with-Apache-Hudi.png","tags":["blog","apache hudi","medium","beginner"]},"unlisted":false,"prevItem":{"title":"What is Apache Hudi","permalink":"/blog/2023/12/13/what-is-apache-hudi"},"nextItem":{"title":"Apache Hudi: From Zero To One (7/10)","permalink":"/blog/2023/12/06/Apache-Hudi-From-Zero-To-One-blog-7"}}')},79682:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(35752),n=t(74848),s=t(28453),r=t(9230);const o={title:"How Amazon Transportation Service enabled near-real-time event analytics at petabyte scale using AWS Glue with Apache Hudi",authors:[{name:"Madhavan Sriram"},{name:"Diego Menin"},{name:"Gabriele Cacciola"},{name:"Kunal Gautam"}],category:"blog",image:"/assets/images/blog/2021-10-14-near-real-time-analytics-at-amazon-transportation-service.png",tags:["use-case","near real-time analytics","analytics at scale","amazon"]},l=void 0,d={authorsImageUrls:[void 0,void 0,void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/how-amazon-transportation-service-enabled-near-real-time-event-analytics-at-petabyte-scale-using-aws-glue-with-apache-hudi/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},79802:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/09/30/change-query-support-in-apache-hudi-0-15","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-09-30-change-query-support-in-apache-hudi-0-15.mdx","source":"@site/blog/2024-09-30-change-query-support-in-apache-hudi-0-15.mdx","title":"Change query support in Apache Hudi (0.15)","description":"Redirecting... please wait!!","date":"2024-09-30T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"CDC","permalink":"/blog/tags/cdc"},{"inline":true,"label":"Change Data Capture","permalink":"/blog/tags/change-data-capture"},{"inline":true,"label":"jack-vanlightly","permalink":"/blog/tags/jack-vanlightly"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Jack Vanlightly","key":null,"page":null}],"frontMatter":{"title":"Change query support in Apache Hudi (0.15)","author":"Jack Vanlightly","category":"blog","image":"/assets/images/blog/2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.png","tags":["blog","apache hudi","CDC","Change Data Capture","jack-vanlightly"]},"unlisted":false,"prevItem":{"title":"Apache Hudi, Spark and Minio: Hands-on Lab in Docker","permalink":"/blog/2024/10/02/apache-hudi-spark-and-minio-hands-on-lab-in-docker"},"nextItem":{"title":"Hudi, Iceberg and Delta Lake: Data Lake Table Formats Compared","permalink":"/blog/2024/09/24/hudi-iceberg-and-delta-lake-data-lake-table-formats-compared"}}')},79862:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(12557),n=t(74848),s=t(28453);const r={title:"Building an ExaByte-level Data Lake Using Apache Hudi at ByteDance",excerpt:"Ziyue Guan from Bytedance shares the production experience of building an ExaByte-level data lake using Apache Hudi and how it is used in the recommendation system at Bytedance.",author:"Ziyue Guan, translated to English by yihua",category:"blog",image:"/assets/images/blog/bytedance_hudi.png",tags:["use-case","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Scenario Requirements",id:"scenario-requirements",level:2},{value:"Design Decisions",id:"design-decisions",level:2},{value:"Functionality Support",id:"functionality-support",level:2},{value:"Performance Tuning",id:"performance-tuning",level:2},{value:"Future Work",id:"future-work",level:2}];function c(e){const a={a:"a",code:"code",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.p,{children:"Ziyue Guan from Bytedance shares the experience of building an ExaByte(EB)-level data lake using Apache Hudi at Bytedance."}),"\n",(0,n.jsxs)(a.p,{children:["This blog is a translated version of ",(0,n.jsx)(a.a,{href:"https://mp.weixin.qq.com/s/oZz_2HzPCWgzxwZO0nuDUQ",children:"the same blog originally in Chinese/\u4e2d\u6587"}),".  Here are the ",(0,n.jsx)(a.a,{target:"_blank","data-noBrokenLinkCheck":!0,href:t(346).A+"",children:"original slides in Chinese/\u4e2d\u6587"})," and ",(0,n.jsx)(a.a,{target:"_blank","data-noBrokenLinkCheck":!0,href:t(8903).A+"",children:"the translated slides in English"}),"."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"slide1 title",src:t(57014).A+"",width:"2048",height:"1153"})}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"slide2 agenda",src:t(38637).A+"",width:"2048",height:"1153"})}),"\n",(0,n.jsx)(a.p,{children:"Next, I will explain how we use Hudi in Bytedance\u2019s recommendation system in five parts: scenario requirements, design decisions, functionality support, performance tuning, and future work."}),"\n",(0,n.jsx)(a.h2,{id:"scenario-requirements",children:"Scenario Requirements"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{alt:"slide3 scenario requirements",src:t(98692).A+"",width:"2048",height:"1153"}),"\n",(0,n.jsx)(a.img,{alt:"slide4 scenario diagram",src:t(80347).A+"",width:"2048",height:"1153"}),"\n",(0,n.jsx)(a.img,{alt:"slide5 scenario details",src:t(66130).A+"",width:"2048",height:"1153"})]}),"\n",(0,n.jsx)(a.p,{children:"In the recommendation system, we use the data lake in the following two scenarios:"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"We use BigTable as the data storage for the near real-time processing in the entire system. There is an internally developed component TBase, which provides the semantics of BigTable and the abstraction of some requirements in the search advertisement recommendation scenarios, and shields the differences in underlying storage. For a better understanding, it can be directly regarded as an HBase. In this process, in order to serve offline data analysis and mining needs, the data needs to be exported to offline storage. In the past, users either use MR/Spark to directly access the storage, or obtain data by scanning the database, which do not meet the data access characteristics in the OLAP scenario. Therefore, we build BigTable's CDC based on the data lake to improve data timeliness, reduce the access pressure of the near real-time system, and provide efficient OLAP access and user-friendly SQL consumption methods."}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"In addition, we also use data lakes in the scenarios of feature engineering and model training. We obtain two types of real-time data streams from internal and external sources. One is the instances returned from the system, which includes the features obtained when the recommendation system is serving. The other is the feedback from event tracking at vantage points and a variety of complex external data sources. This type of data is used as labels and forms a complete machine learning data sample with the previously mentioned features. For this scenario, we need to implement a merging operation based on the primary key to merge the instance and label together.  The time window range may be as long as tens of days, with the volume at the order of hundreds of billions of rows. The system needs to support efficient column selection and predicate pushdown. At the same time, it also needs to support concurrent updates and other related capabilities."}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"These two scenarios pose the following challenges:"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"The data is very irregular."})," Compared with Binlog, WAL cannot obtain all the information of a row, and the data size changes significantly."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"The throughput is relatively large."}),"  The throughput of a single table exceeds ",(0,n.jsx)(a.strong,{children:"100 GB/s"}),", and the single table needs ",(0,n.jsx)(a.strong,{children:"PB-level"})," storage."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"The data schema is complex."})," The data is highly dimensional and sparse.  The number of table columns ranges from 1000 to 10000+. And there are a lot of complex data types."]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"design-decisions",children:"Design Decisions"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{alt:"slide6 design decisions",src:t(56841).A+"",width:"2048",height:"1153"}),"\n",(0,n.jsx)(a.img,{alt:"slide7 design details",src:t(2016).A+"",width:"2048",height:"1153"})]}),"\n",(0,n.jsxs)(a.p,{children:["When making the decision on the engine, we examine three of the most popular data lake engines, ",(0,n.jsx)(a.strong,{children:"Hudi"}),", ",(0,n.jsx)(a.strong,{children:"Iceberg"}),", and ",(0,n.jsx)(a.strong,{children:"DeltaLake"}),". These three have their own advantages and disadvantages in our scenarios. Finally, ",(0,n.jsx)(a.strong,{children:"Hudi"})," is selected as the storage engine based on Hudi's openness to the upstream and downstream ecosystems, support for the global index, and customized development interfaces for certain storage logic."]}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"For real-time writing, MOR with better timeliness is selected."}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"We examine the index type.  First of all, because WAL can't get the partition of the data each time, it must use a global index. Among several global index implementations, in order to achieve high-performance writing, HBase is the only choice. The other two implementations have major performance gaps from HBase."}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"Regarding the computing engine and API, Hudi's support for Flink was not perfect at the time, so we choose Spark which has more mature support. In order to flexibly implement some customized functionality and logic, and because the DataFrame API has more semantic restrictions, we choose the lower-level RDD API."}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"functionality-support",children:"Functionality Support"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"slide8 functionality support",src:t(46247).A+"",width:"2048",height:"1153"})}),"\n",(0,n.jsx)(a.p,{children:"Functionality support includes MVCC and Schema registration systems that store semantics."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"slide9 mvcc",src:t(27934).A+"",width:"2048",height:"1153"})}),"\n",(0,n.jsx)(a.p,{children:"First of all, in order to support WAL write, we implement the payload for MVCC, and based on Avro, we customized a set of data structure implementation with timestamp. This logic is hidden from users through view access. In addition, we also implement the HBase append semantics, which realizes the appending to the List type instead of overwriting."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"slide10 schema",src:t(68900).A+"",width:"2048",height:"1153"})}),"\n",(0,n.jsx)(a.p,{children:"Because Hudi obtains the schema from write data, it is not convenient for working with other systems.  We also need some extensions based on the schema, so we build a metadata center to provide metadata-related operations."}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"First of all, we realized atomic changes and multi-site high availability based on the semantics provided by internal storage. Users can atomically trigger schema changes through the interface and get the results immediately."}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"Achieves versioning of the Schema by adding the version number. After having the version number, we can easily use the schema instead of passing JSON object back and forth. With multiple versions, schema evolution can also be flexibly achieved."}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"We also support additional information encoding at the column level to help the business achieve special extended functionality in some scenarios. We replace column names with IDs to save the cost in the storage process."}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"When the Spark job with Hudi is running, it builds a local cache at the JVM level and syncs the data with the metadata center through the pull method, to achieve rapid access to the schema and singleton instance of the in-process schema."}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"performance-tuning",children:"Performance Tuning"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"slide11 performance tuning",src:t(95053).A+"",width:"2048",height:"1153"})}),"\n",(0,n.jsxs)(a.p,{children:["In our scenario, the performance challenges are huge. ",(0,n.jsx)(a.strong,{children:"The maximum data volume of a single table reaches 400PB+, the daily volume increase is PB level, and the total data volume reaches EB level."})," Therefore, we have done some work to improve performance based on the performance and data characteristics."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"slide12 serialization",src:t(4374).A+"",width:"2048",height:"1153"})}),"\n",(0,n.jsx)(a.p,{children:"Serialization includes the following optimizations:"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"Schema: the cost of data serialization using Avro is very expensive which consumes a lot of compute resources. To address this problem, we first use the singleton schema instance in JVM to avoid CPU-consuming comparison operations during the serialization process."}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"By optimizing the payload logic, the number of times of running serialization is reduced."}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"With the help of a third-party Avro serialization implementation, the serialization process is compiled into bytecode to improve the speed of SerDe and reduce memory usage. The serialization process has been modified to ensure that our complex schema can also be compiled properly."}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"slide13 compaction",src:t(82463).A+"",width:"2048",height:"1153"})}),"\n",(0,n.jsx)(a.p,{children:"The optimization of the compaction process is as follows."}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"In addition to the default Inline/Async compaction options, Hudi also supports flexible deployment of compaction. The characteristics of the compaction job are quite different from the ingestion job. In the same Spark application, it not only is impossible to set targeted settings but also has the problem of insufficient resource flexibility. We first build an independently deployed script so that the compaction job can be triggered and run independently. A low-cost mixed queue is used for resource scheduling for the compaction plan. In addition, we have also developed a compaction strategy based on rules and heuristics. The user's requirement is usually to guarantee a day-level or hour-level SLA, and targeted compression of data in certain partitions, so targeted compression capabilities are provided."}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"In order to shorten the time of critical compaction, we usually do compaction in advance to avoid all work being completed in a single compaction job. However, if a FileGroup compacted has a new update, it has to be compacted again. In order to optimize the overall efficiency, we made a heuristic scheduling of when a FileGroup should be compacted based on business logic to reduce additional compaction costs.  The actual benefits of this feature are still being evaluated."}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"Finally, we made some process optimizations for the compaction, such as not using WriteStatus's Cache and so on."}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"slide14 hdfs sla",src:t(23808).A+"",width:"2048",height:"1153"})}),"\n",(0,n.jsx)(a.p,{children:"As storage designed for throughput, HDFS has serious real-time write glitches when the cluster usage level is relatively high. Through communication and cooperation with the HDFS team, some improvements have been done."}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"First, we replace the original data HSync operation with HFlush to avoid disk I/O write amplification caused by distributed updates."}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"We make aggressive pipeline switching settings based on the scenario tuning, and the HDFS team has developed a flexible API that can control the pipeline to achieve flexible configurations in this scenario."}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(a.p,{children:"Finally, the timeliness of real-time writing is ensured through independent I/O isolation of log files."}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"slide15 process optimization",src:t(78633).A+"",width:"2048",height:"1153"})}),"\n",(0,n.jsx)(a.p,{children:"There are also some small performance improvements, process modifications, and bug fixes. If you are interested, feel free to discuss that with me."}),"\n",(0,n.jsx)(a.h2,{id:"future-work",children:"Future Work"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.img,{alt:"slide16 future work",src:t(13490).A+"",width:"2048",height:"1153"}),"\n",(0,n.jsx)(a.img,{alt:"slide17 future work details",src:t(78843).A+"",width:"2048",height:"1153"})]}),"\n",(0,n.jsx)(a.p,{children:"In the future, we will continue to iterate in the following aspects."}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Productization issues"}),": The current way of using APIs and tuning parameters are highly demanding for the users, especially for the tuning, operation, and maintenance, which requires a deep understanding of Hudi principles to complete.  This hinders the promotion of that to users."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Support issues for ecosystems"}),": In our scenario, the technology stack is mainly on Flink, and the use of Flink will be explored in the future. In addition, the applications and environments used in upstream and downstream are complex, which requires cross-language and universal interface implementation. The current binding with Spark is cumbersome."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Cost and performance issues"}),": a common topic, since our scenario is relatively broad, the benefits from optimization are highly considerable."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Storage semantics"}),": We use Hudi as storage rather than a table format. Therefore, in the future, we plan to expand scenarios using Hudi, and need richer storage semantics.  We'll do more work in this area."]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"slide19 hiring",src:t(65493).A+"",width:"2048",height:"1153"})}),"\n",(0,n.jsxs)(a.p,{children:["Finally, an advertisement, our recommendation architecture team is responsible for the recommendation architecture design and development for products such as Douyin, Toutiao, and Xigua Video. The challenges are big and the growth is fast. Now we are hiring people and the working locations include: Beijing/Shanghai/Hangzhou/Singapore/Mountain View.  If you are  interested, you are welcomed to add WeChat ",(0,n.jsx)(a.code,{children:"qinglingcannotfly"})," or send your resume to the email: ",(0,n.jsx)(a.code,{children:"guanziyue.gzy@bytedance.com"}),"."]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},79872:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(47400),n=t(74848),s=t(28453),r=t(9230);const o={title:"Building Data Lakes on AWS with Kafka Connect, Debezium, Apicurio Registry, and Apache Hudi",excerpt:"Building Data Lakes on AWS with Kafka Connect, Debezium, Apicurio Registry, and Apache Hudi",author:"Gary A. Stafford",category:"blog",image:"/assets/images/blog/2024-02-27-Building-Data-Lakes-on-AWS-with-Kafka-Connect-Debezium-Apicurio-Registry-and-Apache-Hudi.png",tags:["blog","apache hudi","itnext","beginner","apache kafka","kafka connect","debezium","apicurio registry","aws","apache spark","deltastreamer","hudi streamer","amazon rds","amazon mks","amazon eks","aws glue","amazon emr"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/itnext/building-data-lakes-on-aws-with-kafka-connect-debezium-apicurio-registry-and-apache-hudi-b4da0268dce",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},79875:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig3-468ec18846bf7194631d838fd9824bcf.png"},80105:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(17022),n=t(74848),s=t(28453),r=t(9230);const o={title:"Multi-Modal Index for the Lakehouse in Apache Hudi",authors:[{name:"Sivabalan Narayanan"},{name:"Ethan Guo"}],category:"blog",image:"/assets/images/blog/2022-05-17-multimodal-index.gif",tags:["design","multi modal indexing","lakehouse","onehouse"]},l=void 0,d={authorsImageUrls:[void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.onehouse.ai/blog/introducing-multi-modal-index-for-the-lakehouse-in-apache-hudi",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},80219:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/09/04/developer-guide-how-to-submit-hudi-pyspark-python-jobs-to-emr-serverless","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-09-04-developer-guide-how-to-submit-hudi-pyspark-python-jobs-to-emr-serverless.mdx","source":"@site/blog/2024-09-04-developer-guide-how-to-submit-hudi-pyspark-python-jobs-to-emr-serverless.mdx","title":"Developer Guide: How to Submit Hudi PySpark(Python) Jobs to EMR Serverless (7.1.0) with AWS Glue Hive MetaStore","description":"Redirecting... please wait!!","date":"2024-09-04T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"pyspark","permalink":"/blog/tags/pyspark"},{"inline":true,"label":"python","permalink":"/blog/tags/python"},{"inline":true,"label":"amazon emr","permalink":"/blog/tags/amazon-emr"},{"inline":true,"label":"aws glue","permalink":"/blog/tags/aws-glue"},{"inline":true,"label":"linkedin","permalink":"/blog/tags/linkedin"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Soumil Shah","key":null,"page":null}],"frontMatter":{"title":"Developer Guide: How to Submit Hudi PySpark(Python) Jobs to EMR Serverless (7.1.0) with AWS Glue Hive MetaStore","author":"Soumil Shah","category":"blog","image":"/assets/images/blog/2024-09-04-developer-guide-how-to-submit-hudi-pyspark-python-jobs-to-emr-serverless.png","tags":["blog","apache hudi","pyspark","python","amazon emr","aws glue","linkedin"]},"unlisted":false,"prevItem":{"title":"Use Apache Hudi tables in Athena for Spark","permalink":"/blog/2024/09/09/use-apache-hudi-tables-in-athena-for-spark"},"nextItem":{"title":"Column File Formats: How Hudi Leverages Parquet and ORC ","permalink":"/blog/2024/07/31/hudi-file-formats"}}')},80242:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(85711),n=t(74848),s=t(28453),r=t(9230);const o={title:"What, Why and How : Apache Hudi\u2019s Bloom Index",authors:[{name:"Sivabalan Narayanan"}],category:"blog",image:"/assets/images/blog/2022-10-08-what-why-and-how-apache-hudis-bloom-index.png",tags:["how-to","design","bloom","indexing","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@simpsons/what-why-and-how-apache-hudis-bloom-index-8646747520c1",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},80332:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/01/05/how-use-new-hudi-streamer-100-emr-serverless-750-hands-on","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-01-05-how-use-new-hudi-streamer-100-emr-serverless-750-hands-on.mdx","source":"@site/blog/2025-01-05-how-use-new-hudi-streamer-100-emr-serverless-750-hands-on.mdx","title":"How to Use the New Hudi Streamer with Hudi 1.0.0 on EMR Serverless 7.5.0 | Hands-on Labs","description":"Redirecting... please wait!!","date":"2025-01-05T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"hudi 1.0.0","permalink":"/blog/tags/hudi-1-0-0"},{"inline":true,"label":"hudi streamer","permalink":"/blog/tags/hudi-streamer"},{"inline":true,"label":"amazon emr","permalink":"/blog/tags/amazon-emr"},{"inline":true,"label":"linkedin","permalink":"/blog/tags/linkedin"}],"readingTime":0.15,"hasTruncateMarker":false,"authors":[{"name":"Soumil Shah","key":null,"page":null}],"frontMatter":{"title":"How to Use the New Hudi Streamer with Hudi 1.0.0 on EMR Serverless 7.5.0 | Hands-on Labs","author":"Soumil Shah","category":"blog","image":"/assets/images/blog/2025-01-05-how-use-new-hudi-streamer-100-emr-serverless-750-hands-on.png","tags":["blog","how-to","apache hudi","hudi 1.0.0","hudi streamer","amazon emr","linkedin"]},"unlisted":false,"prevItem":{"title":"The Future of Data Lakehouses: A Fireside Chat with Vinoth Chandar - Founder CEO Onehouse & PMC Chair of Apache Hudi","permalink":"/blog/2025/01/08/the-future-of-data-lakehouses-a-fireside"},"nextItem":{"title":"Indexing in Apache Hudi","permalink":"/blog/2024/12/31/indexing-in-apache-hudi"}}')},80347:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide4-4d2f3854977f63b75788213b21518b62.png"},80400:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(53825),n=t(74848),s=t(28453),r=t(9230);const o={title:"Uber\u2019s Big Data Revolution: From MySQL to Hadoop and Beyond",author:"Vu Trinh",category:"blog",image:"/assets/images/blog/2024-09-14-Ubers-Big-Data-Revolution-From-MySQL-to-Hadoop-and-Beyond.png",tags:["blog","apache hudi","use-case","substack"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://vutr.substack.com/p/ubers-big-data-revolution-from-mysql",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},80431:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/03/26/dedupe","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-03-26-dedupe.mdx","source":"@site/blog/2025-03-26-dedupe.mdx","title":"Data Deduplication Strategies in an Open Lakehouse Architecture","description":"Redirecting... please wait!!","date":"2025-03-26T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"Apache Iceberg","permalink":"/blog/tags/apache-iceberg"},{"inline":true,"label":"Delta Lake","permalink":"/blog/tags/delta-lake"},{"inline":true,"label":"Deduplication","permalink":"/blog/tags/deduplication"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Dipankar Mazumdar, Aditya Goenka","key":null,"page":null}],"frontMatter":{"title":"Data Deduplication Strategies in an Open Lakehouse Architecture","author":"Dipankar Mazumdar, Aditya Goenka","category":"blog","image":"/assets/images/blog/dedupe.png","tags":["blog","Apache Hudi","Apache Iceberg","Delta Lake","Deduplication"]},"unlisted":false,"prevItem":{"title":"What is Clustering in an Open Data Lakehouse?","permalink":"/blog/2025/03/26/clustering"},"nextItem":{"title":"From Transactional Bottlenecks to Lightning-Fast Analytics","permalink":"/blog/2025/03/26/uptycs"}}')},80437:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/08/28/Apache-Hudi-From-Zero-To-One","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-08-28-Apache-Hudi-From-Zero-To-One.mdx","source":"@site/blog/2023-08-28-Apache-Hudi-From-Zero-To-One.mdx","title":"Apache Hudi: From Zero To One (1/10)","description":"Redirecting... please wait!!","date":"2023-08-28T00:00:00.000Z","tags":[{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"spark","permalink":"/blog/tags/spark"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"course","permalink":"/blog/tags/course"},{"inline":true,"label":"tutorial","permalink":"/blog/tags/tutorial"},{"inline":true,"label":"datumagic","permalink":"/blog/tags/datumagic"},{"inline":true,"label":"data lake","permalink":"/blog/tags/data-lake"},{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"apache spark","permalink":"/blog/tags/apache-spark"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Shiyan Xu","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi: From Zero To One (1/10)","excerpt":"A first glance at Hudi\'s storage format","author":"Shiyan Xu","category":"blog","image":"/assets/images/blog/2023-08-28-Apache-Hudi-From-Zero-To-One-blog-1.png","tags":["hudi","spark","blog","course","tutorial","datumagic","data lake","lakehouse","apache hudi","apache spark"]},"unlisted":false,"prevItem":{"title":"Incremental Queries with Apache Hudi and Apache Flink","permalink":"/blog/2023/08/31/Incremental-Queries-with-Apache-Hudi-and-Apache-Flink"},"nextItem":{"title":"Delta, Hudi, Iceberg \u2014 A Benchmark Compilation","permalink":"/blog/2023/08/28/Delta-Hudi-Iceberg-A-Benchmark-Compilation"}}')},80812:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(92777),n=t(74848),s=t(28453),r=t(9230);const o={title:"Data processing with Spark: time traveling",authors:[{name:"Petrica Leuca"}],category:"blog",image:"/assets/images/blog/2022-09-28_Data_processing_with_Spark_time_traveling.png",tags:["how-to","time travel query","devgenius"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blog.devgenius.io/data-processing-with-spark-time-traveling-55905f765694",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},80814:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image3-ecdbeac000b14fa4bf639c7e0daf8654.png"},80857:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/03/13/lightning-fast-analytics","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-03-13-lightning-fast-analytics.mdx","source":"@site/blog/2025-03-13-lightning-fast-analytics.mdx","title":"From Transactional Bottlenecks to Lightning-Fast Analytics","description":"Redirecting... please wait!!","date":"2025-03-13T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"kafka","permalink":"/blog/tags/kafka"},{"inline":true,"label":"debezium","permalink":"/blog/tags/debezium"},{"inline":true,"label":"S3","permalink":"/blog/tags/s-3"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Akash Sankritya","key":null,"page":null}],"frontMatter":{"title":"From Transactional Bottlenecks to Lightning-Fast Analytics","author":"Akash Sankritya","category":"blog","image":"/assets/images/blog/lightning-fast-analytics.jpeg","tags":["blog","apache hudi","kafka","debezium","S3"]},"unlisted":false,"prevItem":{"title":"Building an Amazon Sales Analytics Pipeline with Apache Hudi on Databricks","permalink":"/blog/2025/03/13/hudi-on-dbr"},"nextItem":{"title":"21 Unique Reasons Why Apache Hudi Should Be Your Next Data Lakehouse","permalink":"/blog/2025/03/05/hudi-21-unique-differentiators"}}')},80865:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image2-73e008bce9561bc07e7586c36a0cb745.png"},80867:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(94042),n=t(74848),s=t(28453),r=t(9230);const o={title:"How Apache Hudi transformed Yuno\u2019s data lake",author:"Nahuel Leandro Mazzitelli",category:"blog",image:"/assets/images/blog/2024-09-17-how-apache-hudi-transformed-yuno-s-data-lake.png",tags:["blog","apache hudi","cow","mor","record index","record level index","clustering","cleaning","bloom index","fiel sizing","y.uno"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.y.uno/post/how-apache-hudi-transformed-yunos-data-lake",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},80921:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(43983),n=t(74848),s=t(28453);const r={title:"Incremental Processing on the Data Lake",excerpt:"How Apache Hudi provides ability for incremental data processing.",author:"vinoyang",category:"blog",image:"/assets/images/blog/incr-processing/image7.png",tags:["blog","datalake","incremental processing","apache hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"NOTE: This article is a translation of the infoq.cn article, found here, with minor edits",id:"note-this-article-is-a-translation-of-the-infoqcn-article-found-here-with-minor-edits",level:3},{value:"Traditional data lakes lack the primitives for incremental processing",id:"traditional-data-lakes-lack-the-primitives-for-incremental-processing",level:2},{value:"The significance of incremental processing for the data lake",id:"the-significance-of-incremental-processing-for-the-data-lake",level:2},{value:"Streaming Semantics",id:"streaming-semantics",level:3},{value:"Warehousing needs Incremental Processing",id:"warehousing-needs-incremental-processing",level:3},{value:"Quasi-real-time scenarios, resource/efficiency trade-offs",id:"quasi-real-time-scenarios-resourceefficiency-trade-offs",level:3},{value:"Incremental processing facilitates unified data lake architecture",id:"incremental-processing-facilitates-unified-data-lake-architecture",level:3},{value:"Hudi&#39;s support for incremental processing",id:"hudis-support-for-incremental-processing",level:2},{value:"Summary",id:"summary",level:2}];function c(e){const a={a:"a",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(a.h3,{id:"note-this-article-is-a-translation-of-the-infoqcn-article-found-here-with-minor-edits",children:["NOTE: This article is a translation of the infoq.cn article, found ",(0,n.jsx)(a.a,{href:"https://www.infoq.cn/article/CAgIDpfJBVcJHKJLSbhe",children:"here"}),", with minor edits"]}),"\n",(0,n.jsxs)(a.p,{children:['Apache Hudi is a data lake framework which provides the ability to ingest, manage and query large analytical data sets on a distributed file system/cloud stores.\nHudi joined the Apache incubator for incubation in January 2019, and was promoted to the top Apache project in May 2020. This article mainly discusses the importance\nof Hudi to the data lake from the perspective of "incremental processing". More information about Apache Hudi\'s framework functions, features, usage scenarios, and\nlatest developments can be found at ',(0,n.jsx)(a.a,{href:"https://qconplus.infoq.cn/2020/shanghai/presentation/2646",children:"QCon Global Software Development Conference (Shanghai Station) 2020"}),"."]}),"\n",(0,n.jsx)(a.p,{children:"Throughout the development of big data technology, Hadoop has steadily seized the opportunities of this era and has become the de-facto standard for enterprises to build big data infrastructure.\nAmong them, the distributed file system HDFS that supports the Hadoop ecosystem almost naturally has become the standard interface for big data storage systems. In recent years, with the rise of\ncloud-native architectures, we have seen a wave of newer models embracing low-cost cloud storage emerging, a number of data lake frameworks compatible with HDFS interfaces\nembracing cloud vendor storage have emerged in the industry as well."}),"\n",(0,n.jsx)(a.p,{children:'However, we are still processing data pretty much in the same way we did 10 years ago. This article will try to talk about its importance to the data lake from the perspective of "incremental processing".'}),"\n",(0,n.jsx)(a.h2,{id:"traditional-data-lakes-lack-the-primitives-for-incremental-processing",children:"Traditional data lakes lack the primitives for incremental processing"}),"\n",(0,n.jsxs)(a.p,{children:["In the era of mobile Internet and Internet of Things, delayed arrival of data is very common.\nHere we are involved in the definition of two time semantics: ",(0,n.jsx)(a.a,{href:"https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/",children:"event time and processing time"}),"."]}),"\n",(0,n.jsx)(a.p,{children:"As the name suggests:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Event time:"})," the time when the event actually occurred;"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Processing time:"})," the time when an event is observed (processed) in the system;"]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:['Ideally, the event time and the processing time are the same, but in reality, they may have more or less deviation, which we often call "Time Skew".\nWhether for low-latency stream computing or common batch processing, the processing of event time and processing time and late data is a common and difficult problem.\nIn general, in order to ensure correctness, when we strictly follow the "event time" semantics, late data will trigger the\n',(0,n.jsx)(a.a,{href:"https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/stream/operators/windows#late-elements-considerations",children:"recalculation of the time window"}),'\n(usually Hive partitions for batch processing), although the results of these "windows" may have been calculated or even interacted with the end user.\nFor recalculation, the extensible key-value storage structure is usually used in streaming processing, which is processed incrementally at the record/event level and optimized\nbased on point queries and updates. However, in data lakes, recalculating usually means rewriting the entire (immutable) Hive partition (or simply a folder in DFS), and\nre-triggering the recalculation of cascading tasks that have consumed that Hive partition.']}),"\n",(0,n.jsx)(a.p,{children:"With data lakes supporting massive amounts of data, many long-tail businesses still have a strong demand for updating cold data. However, for a long time,\nthe data in a single partition in the data lake was designed to be non-updatable. If it needs to be updated, the entire partition needs to be rewritten.\nThis will seriously damage the efficiency of the entire ecosystem. From the perspective of latency and resource utilization, these operations on Hadoop will incur expensive overhead.\nBesides, this overhead is usually also cascaded to the entire Hadoop data processing pipeline, which ultimately leads to an increase in latency by several hours."}),"\n",(0,n.jsx)(a.p,{children:"In response to the two problems mentioned above, if the data lake supports fine-grained incremental processing, we can incorporate changes into existing Hive partitions\nmore effectively, and provide a way for downstream table consumers to obtain only the changed data. For effectively supporting incremental processing, we can decompose it into the\nfollowing two primitive operations:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Update insert (upsert):"})," Conceptually, rewriting the entire partition can be regarded as a very inefficient upsert operation, which will eventually write much more data than the\noriginal data itself. Therefore, support for (bulk) upsert is considered a very important feature. ",(0,n.jsx)(a.a,{href:"https://research.google/pubs/pub42851/",children:"Google's Mesa"})," (Google's data warehouse system) also\ntalks about several techniques that can be applied to rapid data ingestion scenarios."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Incremental consumption:"})," Although upsert can solve the problem of quickly releasing new data to a partition, downstream data consumers do not know\nwhich data has been changed from which time in the past. Usually, consumers can only know the changed data by scanning the entire partition/data table and\nrecalculating all the data, which requires considerable time and resources. Therefore, we also need a mechanism to more efficiently obtain data records that\nhave changed since the last time the partition was consumed."]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"With the above two primitive operations, you can upsert a data set, and then incrementally consume from it, and create another (also incremental) data set to solve the two problems\nwe mentioned above and support many common cases, so as to support end-to-end incremental processing and reduce end-to-end latency. These two primitives combine with each other,\nunlocking the ability of stream/incremental processing based on DFS abstraction."}),"\n",(0,n.jsx)(a.p,{children:"The storage scale of the data lake far exceeds that of the data warehouse. Although the two have different focuses on the definition of functions,\nthere is still a considerable intersection (of course, there are still disputes and deviations from definition and implementation.\nThis is not the topic this article tries to discuss). In any case, the data lake will support larger analytical data sets with cheaper storage,\nso incremental processing is also very important for it. Next let's discuss the significance of incremental processing for the data lake."}),"\n",(0,n.jsx)(a.h2,{id:"the-significance-of-incremental-processing-for-the-data-lake",children:"The significance of incremental processing for the data lake"}),"\n",(0,n.jsx)(a.h3,{id:"streaming-semantics",children:"Streaming Semantics"}),"\n",(0,n.jsxs)(a.p,{children:['It has long been stated that there is a "',(0,n.jsx)(a.a,{href:"https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying",children:"dualism"}),'"\nbetween the change log (that is, the "flow" in the conventional sense we understand) and the table.']}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"dualism",src:t(66151).A+"",width:"576",height:"576"})}),"\n",(0,n.jsx)(a.p,{children:'The core of this discussion is: if there is a change log, you can use these changes to generate a data table and get the current status. If you update a table,\nyou can record these changes and publish all "change logs" to the table\'s status information. This interchangeable nature is called "stream table duality" for short.'}),"\n",(0,n.jsx)(a.p,{children:'A more general understanding of "stream table duality": when the business system is modifying the data in the MySQL table, MySQL will reflect these changes as Binlog,\nif we publish these continuous Binlog (stream) to Kafka, and then let the downstream processing system subscribe to the Kafka, and use the state store to gradually\naccumulate the intermediate results. Then the current state of this intermediate result can reflects the current snapshot of the table.'}),"\n",(0,n.jsx)(a.p,{children:'If the two primitives mentioned above that support incremental processing can be introduced to the data lake, the above pipeline, which can reflect the\n"stream table duality", is also applicable on the data lake. Based on the first primitive, the data lake can also ingest the Binlog log streams in Kafka,\nand then store these Binlog log streams into "tables" on the data lake. Based on the second primitive, these tables recognize the changed records as "Binlog"\nstreams to support the incremental consumption of subsequent cascading tasks.'}),"\n",(0,n.jsx)(a.p,{children:"Of course, as the data in the data lake needs to be landed on the final file/object storage, considering the trade-off between throughput and write performance,\nBinlog on the data lake reacts to a small batch of change logs over a period of time on the stream. For example, the Apache Hudi community is further trying to\nprovide an incremental view similar to Binlog for different Commits (a Commit refers to a batch of data write commit),\nas shown in the following figure:"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"idu",src:t(2634).A+"",width:"939",height:"193"})}),"\n",(0,n.jsx)(a.p,{children:'Remarks in the "Flag" column:'}),"\n",(0,n.jsx)(a.p,{children:"I: Insert;\nD: Delete;\nU: After image of Update;\nX: Before image of Update;"}),"\n",(0,n.jsx)(a.p,{children:"Based on the above discussion, we can think that incremental processing and stream are naturally compatible, and we can naturally connect them on the data lake."}),"\n",(0,n.jsx)(a.h3,{id:"warehousing-needs-incremental-processing",children:"Warehousing needs Incremental Processing"}),"\n",(0,n.jsxs)(a.p,{children:["In the data warehouse, whether it is dimensional modeling or relational modeling theory, it is usually constructed based on the ",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Data_warehouse#Design_methods",children:"layered design ideas"}),".\nIn terms of technical implementation, multiple stages (steps) of a long pipeline are formed by connecting multiple levels of ETL tasks through a workflow scheduling engine,\nas shown in the following figure:"]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"image2",src:t(16865).A+"",width:"1999",height:"958"})}),"\n",(0,n.jsx)(a.p,{children:"As the main application of the data warehouse, in the OLAP field, for the conventional business scenarios(for no or few changes), there are already some frameworks in the industry\nthat focus on the scenarios where they are good at providing efficient analysis capabilities. However, in the Hadoop data warehouse/data lake ecosystem,\nthere is still no good solution for the analysis scenario of frequent changes of business data."}),"\n",(0,n.jsx)(a.p,{children:"For example, let\u2019s consider the scenario of updating the order status of a travel business. This scenario has a typical long-tail effect:\nyou cannot know whether an order will be billed tomorrow, one month later, or one year later. In this scenario, the order table is the main data table,\nbut usually we will derive other derived tables based on this table to support the modeling of various business scenarios.\nThe initial update takes place in the order table at the ODS level, but the derived tables need to be updated in cascade."}),"\n",(0,n.jsx)(a.p,{children:"For this scenario, in the past, once there is a change, people usually need to find the partition where the data to be updated is located in the Hive order\ntable of the ODS layer, and update the entire partition, besides, the partition of the relevant data of the derived table needs to be updated in cascade."}),"\n",(0,n.jsx)(a.p,{children:"Yes, someone will definitely think of that Kudu's support for Upsert can solve the problem of the old version of Hive missing the first incremental primitive.\nBut the Kudu storage engine has its own limitations:"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsx)(a.li,{children:"Performance: additional requirements for the hardware itself;"}),"\n",(0,n.jsx)(a.li,{children:"Ecologically: In terms of adapting to mainstream big data computing frameworks and machine learning frameworks, it is far less advantageous than Hive;"}),"\n",(0,n.jsx)(a.li,{children:"Cost: requires special maintenance costs and expenses;"}),"\n",(0,n.jsx)(a.li,{children:"Did not solve the second primitive of incremental processing mentioned above: the problem of incremental consumption."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"In summary, incremental processing has the following advantages on the data lake:"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Performance improvement:"})," Ingesting data usually needs to handle updates, deletes, and enforce unique key constraints. Since incremental primitives support record-level updates,\nit can bring orders of magnitude performance improvements to these operations."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Faster ETL/derived Pipelines:"})," An ubiquitous next step, once the data has been ingested from external sources is to build derived data pipelines using\nApache Spark/Apache Hive or any other data processing framework to ETL the ingested data for a variety of use-cases like data warehouse,\nmachine learning, or even just analytics. Typically, such processes again rely on batch processing jobs expressed in code or SQL. Such data pipelines can be speed up dramatically,\nby querying one or more input tables using an incremental query instead of a regular snapshot query, resulting in only processing the incremental changes from upstream tables and\nthen upsert or delete the target derived table.Similar to raw data ingestion, in order to reduce the data delay of the modelled table, the ETL job only needs to gradually extract the\nchanged data from the original table and update the previously derived output table instead of rebuilding the entire output table every few hours ."]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Unified storage:"})," Based on the above two advantages, faster and lighter processing on the existing data lake means that only for the purpose of accessing near real-time data,\nno special storage or data mart is needed."]}),"\n",(0,n.jsxs)(a.p,{children:["Next, we use two simple examples to illustrate how ",(0,n.jsx)(a.a,{href:"https://www.oreilly.com/content/ubers-case-for-incremental-processing-on-hadoop/",children:"incremental processing"})," can speed up the processing\nof pipelines in analytical scenarios. First of all, data projection is the most common and easy to understand case:"]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"image7",src:t(86620).A+"",width:"1200",height:"300"})}),"\n",(0,n.jsx)(a.p,{children:"This simple example shows that: by upserting new changes into table_1 and establishing a simple projected table (projected_table) through incremental consumption, we can\noperate simpler with lower latency more efficiently projection."}),"\n",(0,n.jsx)(a.p,{children:"Next, for a more complex scenario, we can use incremental processing to support the stream and batch connections supported by the stream computing framework,\nand stream-stream connections (just need to add some additional logic to align window) :"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"image6",src:t(69509).A+"",width:"939",height:"347"})}),"\n",(0,n.jsx)(a.p,{children:"The example in the figure above connects a fact table to multiple dimension tables to create a connected table. This case is one of the rare scenarios where we can save hardware\ncosts while significantly reducing latency."}),"\n",(0,n.jsx)(a.h3,{id:"quasi-real-time-scenarios-resourceefficiency-trade-offs",children:"Quasi-real-time scenarios, resource/efficiency trade-offs"}),"\n",(0,n.jsx)(a.p,{children:"Incremental processing of new data in mini batches can use resources more efficiently. Let's refer to a specific example. We have a Kafka event stream that is pouring in\nat a rate of 10,000 per second. We want to count the number of messages in some dimensions over the past 15 minutes. Many stream processing pipelines use an external/internal\nresult state store (such as RocksDB, Cassandra, ElasticSearch) to save the aggregated count results, and run the containers in resource managers such as YARN/Mesos continuously,\nwhich is very reasonable in less than a five-minute delay window scene. In fact, the YARN container itself has some startup overhead. In addition, in order to improve the\nperformance of writing to result storage system, we usually cache the results before performing batch updates. This kind of protocol requires the container to run continuously."}),"\n",(0,n.jsx)(a.p,{children:"However, in quasi-real-time processing scenarios, these options may not be optimal. To achieve the same effect, you can use short-life containers and optimize overall\nresource utilization. For example, a streaming processor may need to perform six million updates to the result storage system in 15 minutes. However, in the incremental\nbatch mode, we only need to perform an in-memory merge on the accumulated data and update the result storage system only once, then only use the resource container for\nfive minutes. Compared with the pure stream processing mode, the incremental batch processing mode has several times the CPU efficiency improvement, and there are several\norders of magnitude efficiency improvement in updating to the result storage. Basically, this processing method obtains resources on demand, instead of swallowing CPU and\nmemory while waiting for data to be calculated in real time."}),"\n",(0,n.jsx)(a.h3,{id:"incremental-processing-facilitates-unified-data-lake-architecture",children:"Incremental processing facilitates unified data lake architecture"}),"\n",(0,n.jsxs)(a.p,{children:["Whether in the data warehouse or in the data lake, data processing is an unavoidable problem. Data processing involves the selection of computing engines and\nthe design of architectures. There are currently two mainstream architectures in the industry: Lambda and Kappa architectures. Each architecture has its own\ncharacteristics and existing problems. Derivative versions of these architectures are also ",(0,n.jsx)(a.a,{href:"https://www.infoq.cn/article/Uo4pFswlMzBVhq*Y2tB9",children:"emerging endlessly"}),"."]}),"\n",(0,n.jsxs)(a.p,{children:["In reality, many enterprises still maintain the implementation of the ",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Lambda_architecture",children:"Lambda architecture"}),".\nThe typical Lambda architecture has two modules for the data processing part: the speed layer and the batch layer."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"image5",src:t(99150).A+"",width:"1999",height:"849"})}),"\n",(0,n.jsx)(a.p,{children:"They are usually two independent implementations (from code to infrastructure). For example, Flink (formerly Storm) is a popular option on the speed layer,\nwhile MapReduce/Spark can serve as a batch layer. In fact, people often rely on the speed layer to provide updated results (which may not be accurate), and\nonce the data is considered complete, the results of the speed layer are corrected at a later time through the batch layer. With incremental processing,\nwe have the opportunity to implement the Lambda architecture for batch processing and quasi-real-time processing at the code level and infrastructure level in\na unified manner. It typically looks like below:"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"image3",src:t(95736).A+"",width:"939",height:"443"})}),"\n",(0,n.jsx)(a.p,{children:'As we said, you can use SQL or a batch processing framework like Spark to consistently implement your processing logic. The result table is built incrementally,\nand SQL is executed on "new data" like streaming to produce a quick view of the results. The same SQL can be executed periodically on the full amount of data to\ncorrect any inaccurate results (remember, join operations are always tricky!) and produce a more "complete" view of the results. In both cases, we will use the\nsame infrastructure to perform calculations, which can reduce overall operating costs and complexity.'}),"\n",(0,n.jsxs)(a.p,{children:["Setting aside the Lambda architecture, even in the Kappa architecture, the first primitive of incremental processing (upsert) also plays an important role.\nUber ",(0,n.jsx)(a.a,{href:"https://www.slideshare.net/FlinkForward/flink-forward-san-francisco-2019-moving-from-lambda-and-kappa-architectures-to-kappa-at-uber-roshan-naik",children:"proposed"})," the Kappa + architecture\nbased on this. The Kappa architecture advocates a single stream computing layer sufficient to become a general solution\nfor data processing. Although the batch layer is removed in this model, there are still two problems in the service layer:"]}),"\n",(0,n.jsx)(a.p,{children:"Now days many stream processing engines support row-level data processing, which requires that our service layer should also support row-level updates;\nThe trade-offs between data ingestion delay, scanning performance and computing resources and operational complexity are unavoidable."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"image8",src:t(96107).A+"",width:"1999",height:"770"})}),"\n",(0,n.jsx)(a.p,{children:"However, if our business scenarios have low latency requirements, for example, we can accept a delay of about 10 minutes. And if we can quickly ingest and prepare data on DFS,\neffectively connect and propagate updates to the upper-level modeling data set, Speed Serving in the service layer is unnecessary. Then the service layer can be unified,\ngreatly reducing the overall complexity and resource consumption of the system."}),"\n",(0,n.jsx)(a.p,{children:"Above, we introduced the significance of incremental processing for the data lake. Next, we introduce the implementation and support of incremental processing.\nAmong the three open source data lake frameworks (Apache Hudi/Iceberg, Delta Lake), only Apache Hudi provides good support for incremental processing.\nThis is completely rooted in a framework developed by Uber at the time when it encountered the pain points of data analysis on the Hadoop data lake.\nSo, next, let's introduce how Hudi supports incremental processing."}),"\n",(0,n.jsx)(a.h2,{id:"hudis-support-for-incremental-processing",children:"Hudi's support for incremental processing"}),"\n",(0,n.jsx)(a.p,{children:"Apache Hudi (Hadoop Upserts Deletes and Incrementals) is a top-level project of the Apache Foundation. It allows you to process very large-scale data on\ntop of Hadoop-compatible storage, and it also provides two primitives that enable stream processing on the data lake in addition to classic batch processing."}),"\n",(0,n.jsx)(a.p,{children:'From the naming of the letter "I" denotes "Incremental Processing", we can see that it will support incremental processing as a first class citizen.\nThe two primitives we mentioned at the beginning of this article that support incremental processing are reflected in the following two aspects in Apache Hudi:'}),"\n",(0,n.jsxs)(a.p,{children:["Update/Delete operation",":Hudi"," provides support for updating/deleting records, using fine-grained file/record level indexes while providing transactional guarantees\nfor the write operation. Queries process the last such committed snapshot, to produce results.."]}),"\n",(0,n.jsx)(a.p,{children:"Change stream: Hudi also provides first-class support for obtaining an incremental stream of all the records that were updated/inserted/deleted in a given table, from a given point-in-time."}),"\n",(0,n.jsx)(a.p,{children:'The specific implementation of the change flow is "incremental view". Hudi is the only one of the three open source data lake frameworks that supports\nthe incremental query feature, with support for record level change streams. The following sample code snippet shows us how to query the incremental view:'}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-java",children:'// spark-shell\n// reload data\nspark.\n  read.\n  format("hudi").\n  load(basePath + "/*/*/*/*").\n  createOrReplaceTempView("hudi_trips_snapshot")\n\nval commits = spark.sql("select distinct(_hoodie_commit_time) as commitTime from  hudi_trips_snapshot order by commitTime").map(k => k.getString(0)).take(50)\nval beginTime = commits(commits.length - 2) // commit time we are interested in\n\n// incrementally query data\nval tripsIncrementalDF = spark.read.format("hudi").\n  option(QUERY_TYPE_OPT_KEY, QUERY_TYPE_INCREMENTAL_OPT_VAL).\n  option(BEGIN_INSTANTTIME_OPT_KEY, beginTime).\n  load(basePath)\ntripsIncrementalDF.createOrReplaceTempView("hudi_trips_incremental")\n\nspark.sql("select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from  hudi_trips_incremental where fare > 20.0").show()\n\n'})}),"\n",(0,n.jsx)(a.p,{children:'The code snippet above creates a Hudi trip increment table (hudi_trips_incremental), and then queries all the change records in the increment table after the "beginTime" submission time\nand the "cost"  is greater than 20.0. Based on this query, you can create incremental data pipelines on batch data.'}),"\n",(0,n.jsx)(a.h2,{id:"summary",children:"Summary"}),"\n",(0,n.jsx)(a.p,{children:"In this article, we first elaborated many problems caused by the lack of incremental processing primitives in the traditional Hadoop data warehouse due to the trade-off between data integrity\nand latency, and some long-tail applications that rely heavily on updates. Next, we argued that to support incremental processing, we must have at least two primitives: upsert and\nincremental consumption, and explained why these two primitives can solve the problems explained above."}),"\n",(0,n.jsx)(a.p,{children:'Then, we introduced why incremental processing is also important to the data lake. There are many common parts in data processing between the data lake and the data warehouse.\nIn the data warehouse, some "pain points" caused by the lack of incremental processing also exist in the data lake. We elaborated its significance to the data lake from four\naspects: incremental processing of semantics of natural fit flow, the need for analytical scenarios, quasi-real-time scene resource/efficiency trade-offs, and unified lake architecture.'}),"\n",(0,n.jsx)(a.p,{children:"Finally, we introduced the open source data lake storage framework Apache Hudi's support for incremental processing and simple cases."})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},80931:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(43900),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi: Managing Partition on a petabyte-scale table",excerpt:"Apache Hudi: Managing Partition on a petabyte-scale table",author:"Krishna Prasad",category:"blog",image:"/assets/images/blog/2024-02-04-Apache-Hudi-Managing-Partition-on-a-petabyte-scale-table.png",tags:["blog","apache hudi","medium","intermediate","partition","aws glue","apache spark","aws s3"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@krishnaiitd/partitioning-apache-hudi-data-lake-table-ffd0ac28aad4",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},81012:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(12102),n=t(74848),s=t(28453),r=t(82915);const o={title:"Lakehouse Concurrency Control: Are we too optimistic?",excerpt:"Vinoth Chandar, original creator of Apache Hudi, dives into concurrency control mechanisms",author:"vinoth",category:"blog",image:"/assets/images/blog/concurrency/MultiWriter.gif",tags:["blog","concurrency-control","apache hudi"]},l=void 0,d={authorsImageUrls:[void 0]},c=[{value:"Pitfalls in Lake Concurrency Control",id:"pitfalls-in-lake-concurrency-control",level:3},{value:"Model 1 : Single Writer, Inline Table Services",id:"model-1--single-writer-inline-table-services",level:3},{value:"Model 2 : Single Writer, Async Table Services",id:"model-2--single-writer-async-table-services",level:3},{value:"Model 3 : Multiple Writers",id:"model-3--multiple-writers",level:3}];function h(e){const a={a:"a",em:"em",h3:"h3",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(a.p,{children:["Transactions on data lakes are now considered a key characteristic of a ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2024/07/11/what-is-a-data-lakehouse/",children:"Lakehouse"})," these days. But what has actually been accomplished so far? What are the current approaches? How do they fare in real-world scenarios? These questions are the focus of this blog."]}),"\n",(0,n.jsxs)(a.p,{children:["Having had the good fortune of working on diverse database projects - an RDBMS (",(0,n.jsx)(a.a,{href:"https://www.oracle.com/database/",children:"Oracle"}),"), a NoSQL key-value store (",(0,n.jsx)(a.a,{href:"https://www.slideshare.net/vinothchandar/voldemort-prototype-to-production-nectar-edits",children:"Voldemort"}),"), a streaming database (",(0,n.jsx)(a.a,{href:"https://www.confluent.io/blog/ksqldb-pull-queries-high-availability/",children:"ksqlDB"}),"), a closed-source real-time datastore and of course, Apache Hudi, I can safely say that the nature of workloads deeply influence the concurrency control mechanisms adopted in different databases. This blog will also describe how we rethought concurrency control for the data lake in Apache Hudi."]}),"\n",(0,n.jsxs)(a.p,{children:["First, let's set the record straight. RDBMS databases offer the richest set of transactional capabilities and the widest array of concurrency control ",(0,n.jsx)(a.a,{href:"https://dev.mysql.com/doc/refman/5.7/en/innodb-locking-transaction-model.html",children:"mechanisms"}),". Different isolation levels, fine grained locking, deadlock detection/avoidance, and more are possible because they have to support row-level mutations and reads across many tables while enforcing ",(0,n.jsx)(a.a,{href:"https://dev.mysql.com/doc/refman/8.0/en/create-table-foreign-keys.html",children:"key constraints"})," and maintaining ",(0,n.jsx)(a.a,{href:"https://dev.mysql.com/doc/refman/8.0/en/create-table-secondary-indexes.html",children:"indexes"}),". NoSQL stores offer dramatically weaker guarantees like eventual-consistency and simple row level atomicity in exchange for greater scalability for simpler workloads. Drawing a similar parallel, traditional data warehouses offer more or less the full set of capabilities that you would find in an RDBMS, over columnar data, with locking and key constraints ",(0,n.jsx)(a.a,{href:"https://docs.teradata.com/r/a8IdS6iVHR77Z9RrIkmMGg/wFPZS4jwZgSG21GnOIpEsw",children:"enforced"})," whereas cloud data warehouses seem to have focused a lot more on separating the data and compute in architecture, while offering fewer isolation levels. As a surprising example, ",(0,n.jsx)(a.a,{href:"https://docs.snowflake.com/en/sql-reference/constraints-overview.html#supported-constraint-types",children:"no enforcement"})," of key constraints!"]}),"\n",(0,n.jsx)(a.h3,{id:"pitfalls-in-lake-concurrency-control",children:"Pitfalls in Lake Concurrency Control"}),"\n",(0,n.jsxs)(a.p,{children:["Historically, data lakes have been viewed as batch jobs reading/writing files on cloud storage and it's interesting to see how most new work extends this view and implements glorified file version control using some form of \"",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Optimistic_concurrency_control",children:(0,n.jsx)(a.strong,{children:"Optimistic concurrency control"})}),'" (OCC). With OCC jobs take a table level lock to check if they have impacted overlapping files and if a conflict exists, they abort their operations completely. Without naming names, the lock is sometimes even just a JVM level lock held on a single Apache Spark driver node. Once again, this may be okay for lightweight coordination of old school batch jobs that mostly append files to tables, but cannot be applied broadly to modern data lake workloads. Such approaches are built with immutable/append-only data models in mind, which are inadequate for incremental data processing or keyed updates/deletes. OCC is very optimistic that real contention never happens. Developer evangelism comparing OCC to the full fledged transactional capabilities of an RDBMS or a traditional data warehouse is rather misinformed. Quoting Wikipedia directly - "',(0,n.jsx)(a.em,{children:"if contention for data resources is frequent, the cost of repeatedly restarting transactions hurts performance significantly, in which case other"})," ",(0,n.jsx)(a.a,{href:"https://en.wikipedia.org/wiki/Concurrency_control",children:(0,n.jsx)(a.em,{children:"concurrency control"})})," ",(0,n.jsx)(a.em,{children:"methods may be better suited."}),' " When conflicts do occur, they can cause massive resource wastage since you have a batch job that fails after it ran for a few hours, during every attempt!']}),"\n",(0,n.jsx)(a.p,{children:"Imagine a real-life scenario of two writer processes : an ingest writer job producing new data every 30 minutes and a deletion writer job that is enforcing GDPR, taking 2 hours to issue deletes. It's very likely for these to overlap files with random deletes, and the deletion job is almost guaranteed to starve and fail to commit each time. In database speak, mixing long running transactions with optimism leads to disappointment, since the longer the transactions the higher the probability they will overlap."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"concurrency",src:t(84066).A+"",width:"1110",height:"881"})}),"\n",(0,n.jsxs)(a.p,{children:["So, what's the alternative? Locking? Wikipedia also says - \"",(0,n.jsx)(a.em,{children:'However, locking-based ("pessimistic") methods also can deliver poor performance because locking can drastically limit effective concurrency even when deadlocks are avoided.".'})," Here is where Hudi takes a different approach, that we believe is more apt for modern lake transactions which are typically long-running and even continuous. Data lake workloads share more characteristics with high throughput stream processing jobs, than they do to standard reads/writes from a database and this is where we borrow from. In stream processing events are serialized into a single ordered log, avoiding any locks/concurrency bottlenecks and you can continuously process millions of events/sec. Hudi implements a file level, log based concurrency control protocol on the Hudi ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/timeline",children:"timeline"}),", which in-turn relies on bare minimum atomic puts to cloud storage. By building on an event log as the central piece for inter process coordination, Hudi is able to offer a few flexible deployment models that offer greater concurrency over pure OCC approaches that just track table snapshots."]}),"\n",(0,n.jsx)(a.h3,{id:"model-1--single-writer-inline-table-services",children:"Model 1 : Single Writer, Inline Table Services"}),"\n",(0,n.jsx)(a.p,{children:"The simplest form of concurrency control is just no concurrency at all. A data lake table often has common services operating on it to ensure efficiency. Reclaiming storage space from older versions and logs, coalescing files (clustering in Hudi), merging deltas (compactions in Hudi), and more. Hudi can simply eliminate the need for concurrency control and maximizes throughput by supporting these table services out-of-box and running inline after every write to the table."}),"\n",(0,n.jsx)(a.p,{children:"Execution plans are idempotent, persisted to the timeline and auto-recover from failures. For most simple use-cases, this means just writing is sufficient to get a well-managed table that needs no concurrency control."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"concurrency-single-writer",src:t(76226).A+"",width:"1280",height:"720"})}),"\n",(0,n.jsx)(a.h3,{id:"model-2--single-writer-async-table-services",children:"Model 2 : Single Writer, Async Table Services"}),"\n",(0,n.jsxs)(a.p,{children:["Our delete/ingest example above is n't really that simple. While ingest/writer may just be updating the last N partitions on the table, delete may span across the entire table even. Mixing them in the same job, could slow down ingest latency by a lot. But, Hudi provides the option of running the table services in an async fashion, where most of the heavy lifting (e.g actually rewriting the columnar data by compaction service) is done asynchronously, eliminating any repeated wasteful retries whatsoever, while also optimizing the table using clustering techniques. Thus a single writer could consumes both regular updates and GDPR deletes and serialize them into a log. Given Hudi has record level indexing and avro log writes are much cheaper (as opposed to writing parquet, which can be 10x or more expensive), ingest latency can be sustained while enjoying great replayability. In fact, we were able to scale this model at ",(0,n.jsx)(a.a,{href:"https://eng.uber.com/uber-big-data-platform/",children:"Uber"}),", across 100s of petabytes, by sequencing all deletes & updates into the same source Apache Kafka topic. There's more to concurrency control than locking and Hudi accomplishes all this without needing any external locking."]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"concurrency-async",src:t(84003).A+"",width:"1280",height:"720"})}),"\n",(0,n.jsx)(a.h3,{id:"model-3--multiple-writers",children:"Model 3 : Multiple Writers"}),"\n",(0,n.jsx)(a.p,{children:"But it's not always possible to serialize the deletes into the same write stream or sql based deletes are required. With multiple distributed processes, some form of locking is inevitable, but like real databases Hudi's concurrency model is intelligent enough to differentiate actual writing to the table, from table services that manage or optimize the table. Hudi offers similar optimistic concurrency control across multiple writers, but table services can still execute completely lock-free and async. This means the delete job can merely encode deletes and the ingest job can log updates, while the compaction service again applies the updates/deletes to base files. Even though the delete job and ingest job can contend and starve each other like like we mentioned above, their run-times are much lower and the wastage is drastically lower, since the compaction does the heavy-lifting of parquet/columnar data writing."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"concurrency-multi",src:t(13202).A+"",width:"1200",height:"600"})}),"\n",(0,n.jsx)(a.p,{children:"All this said, there are still many ways we can improve upon this foundation."}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["For starters, Hudi has already implemented a ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2021/08/18/improving-marker-mechanism/",children:"marker mechanism"})," that tracks all the files that are part of an active write transaction and a heartbeat mechanism that can track active writers to a table. This can be directly used by other active transactions/writers to detect what other writers are doing and ",(0,n.jsx)(a.a,{href:"https://issues.apache.org/jira/browse/HUDI-1575",children:"abort early"})," if conflicts are detected, yielding the cluster resources back to other jobs sooner."]}),"\n",(0,n.jsxs)(a.li,{children:["While optimistic concurrency control is attractive when serializable snapshot isolation is desired, it's neither optimal nor the only method for dealing with concurrency between writers. We plan to implement a fully lock-free concurrency control using CRDTs and widely adopted stream processing concepts, over our log ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/bc8bf043d5512f7afbb9d94882c4e43ee61d6f06/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordPayload.java#L38",children:"merge API"}),", that has already been ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2021/09/01/building-eb-level-data-lake-using-hudi-at-bytedance/#functionality-support",children:"proven"})," to sustain enormous continuous write volumes for the data lake."]}),"\n",(0,n.jsxs)(a.li,{children:["Touching upon key constraints, Hudi is the only lake transactional layer that ensures unique ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/key_generation",children:"key"})," constraints today, but limited to the record key of the table. We will be looking to expand this capability in a more general form to non-primary key fields, with the said newer concurrency models."]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:['Finally, for data lakes to transform successfully into lakehouses, we must learn from the failing of the "hadoop warehouse" vision, which shared similar goals with the new "',(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2024/07/11/what-is-a-data-lakehouse/",children:"lakehouse"}),'" vision. Designers did not pay closer attention to the missing technology gaps against warehouses and created unrealistic expectations from the actual software. As transactions and database functionality finally goes mainstream on data lakes, we must apply these lessons and remain candid about the current shortcomings. If you are building a lakehouse, I hope this post encourages you to closely consider various operational and efficiency aspects around concurrency control. Join our fast growing community by trying out ',(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/overview",children:"Apache Hudi"})," or join us in conversations on ",(0,n.jsx)(r.A,{title:"Slack"}),"."]})]})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h(e)}},81055:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(71085),n=t(74848),s=t(28453),r=t(9230);const o={title:"Multi-writer support with Apache Hudi",authors:[{name:"Sivabalan Narayanan"}],category:"blog",image:"/assets/images/blog/2023-06-24-multi-writer-support-in-apache-hudi.png",tags:["blog","concurrency control","lock provider","multi writer","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@simpsons/multi-writer-support-with-apache-hudi-e1b75dca29e6",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},81310:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(72298),n=t(74848),s=t(28453),r=t(9230);const o={title:"What is Apache Hudi",excerpt:"What is Apache Hudi",author:"Karim Faiz",category:"blog",image:"/assets/images/blog/2023-12-13-what-is-apache-hudi.png",tags:["blog","apache hudi","medium","beginner","apache spark"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@karim.faiz/what-is-apache-hudi-e9363083830e",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},81579:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/02/24/building-a-lakehouse-architecture-on-aws-with-terraform","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-02-24-building-a-lakehouse-architecture-on-aws-with-terraform.mdx","source":"@site/blog/2025-02-24-building-a-lakehouse-architecture-on-aws-with-terraform.mdx","title":"Building a Lakehouse Architecture on AWS with Terraform","description":"Redirecting... please wait!!","date":"2025-02-24T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"aws","permalink":"/blog/tags/aws"},{"inline":true,"label":"terraform","permalink":"/blog/tags/terraform"},{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Juanfelipear","key":null,"page":null}],"frontMatter":{"title":"Building a Lakehouse Architecture on AWS with Terraform","author":"Juanfelipear","category":"blog","image":"/assets/images/blog/2025-02-24-building-a-lakehouse-architecture-on-aws-with-terraform.jpeg","tags":["blog","apache hudi","aws","terraform","lakehouse","medium"]},"unlisted":false,"prevItem":{"title":"Curious Engineering Facts ( Trace Agents | Hudi| Daft : 1) : March Release 18 : 25","permalink":"/blog/2025/02/25/curious-engineering-facts-trace-agents-hudi-daft-1"},"nextItem":{"title":"Curious Engineering Facts (Lakehouse | Apache Hudi | Daft |Positional argument|) : March Release 19 : 25","permalink":"/blog/2025/02/23/curious-engineering-facts-lakehouse-apache-hudi-daft-positional-argument"}}')},81755:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(69411),n=t(74848),s=t(28453),r=t(9230);const o={title:"Data Platform 2.0 - Part I",authors:[{name:"Jitendra Shah"}],category:"blog",image:"/assets/images/blog/2021-10-05-data-platform-2-0-part-1.png",tags:["use-case","halodoc","datalake","datalake platform"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blogs.halodoc.io/data-platform-2-0-part-1/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},81825:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(26311),n=t(74848),s=t(28453);const r={title:"Modernizing Data Infrastructure at Peloton Using Apache Hudi",excerpt:"How Peloton's Data Platform team scaled their data infrastructure using Hudi",author:"Amaresh Bingumalla, Thinh Kenny Vu, Gabriel Wang, Arun Vasudevan in collaboration with Dipankar Mazumdar",category:"blog",image:"/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/peloton-1200x600.jpg",tags:["Apache Hudi","Peloton","Community"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"The Challenge: Data Growth, Latency, and Operational Bottlenecks",id:"the-challenge-data-growth-latency-and-operational-bottlenecks",level:2},{value:"The Legacy Architecture",id:"the-legacy-architecture",level:2},{value:"Reimagining the Data Platform with Apache Hudi",id:"reimagining-the-data-platform-with-apache-hudi",level:2},{value:"Learnings from Running Hudi at Scale",id:"learnings-from-running-hudi-at-scale",level:2},{value:"CoW vs MoR: Performance Trade-offs",id:"cow-vs-mor-performance-trade-offs",level:3},{value:"Async vs Inline Table Services",id:"async-vs-inline-table-services",level:3},{value:"Glue Schema Version Limits",id:"glue-schema-version-limits",level:3},{value:"Debezium &amp; TOAST Handling",id:"debezium--toast-handling",level:3},{value:"Data Validation and Quality Enforcement",id:"data-validation-and-quality-enforcement",level:3},{value:"DynamoDB Ingestion and Schema Challenges",id:"dynamodb-ingestion-and-schema-challenges",level:3},{value:"Reducing Operational Costs",id:"reducing-operational-costs",level:3},{value:"Gains from Hudi Adoption",id:"gains-from-hudi-adoption",level:2}];function c(e){const a={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.admonition,{title:"TL;DR",type:"tip",children:(0,n.jsx)(a.p,{children:"Peloton re-architected its data platform using Apache Hudi to overcome snapshot delays, rigid service coupling, and high operational costs. By adopting CDC-based ingestion from PostgreSQL and DynamoDB, moving from CoW to MoR tables, and leveraging asynchronous services with fine-grained schema control, Peloton achieved 10-minute ingestion cycles, reduced compute/storage overhead, and enabled time travel and GDPR compliance."})}),"\n",(0,n.jsxs)(a.p,{children:["Peloton is a global interactive fitness platform that delivers connected, instructor-led fitness experiences to millions of members worldwide. Known for its immersive classes and cutting-edge equipment, Peloton combines software, hardware, and data to create personalized workout journeys. With a growing member base and increasing product diversity, data has become central to how Peloton delivers value. The ",(0,n.jsx)(a.em,{children:"Data Platform"})," team at Peloton is responsible for building and maintaining the core infrastructure that powers analytics, reporting, and real-time data applications. Their work ensures that data flows seamlessly from transactional systems to the data lake, enabling teams across the organization to make timely, data-driven decisions."]}),"\n",(0,n.jsx)(a.h2,{id:"the-challenge-data-growth-latency-and-operational-bottlenecks",children:"The Challenge: Data Growth, Latency, and Operational Bottlenecks"}),"\n",(0,n.jsx)(a.p,{children:"As Peloton evolved into a global interactive fitness platform, its data infrastructure was challenged by the growing need for timely insights, agile service migrations, and cost-effective analytics. Daily operations, recommendation systems, and compliance requirements demanded an architecture that could support near real-time access, high-frequency updates, and scalable service boundaries."}),"\n",(0,n.jsx)(a.p,{children:"However, the team faced persistent bottlenecks with the existing setup:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Reporting pipelines were gated by the completion of full snapshot jobs."}),"\n",(0,n.jsx)(a.li,{children:"Recommender systems could only function on daily refreshed datasets."}),"\n",(0,n.jsx)(a.li,{children:"The analytics platform was tightly coupled with operational systems."}),"\n",(0,n.jsx)(a.li,{children:"Microservice migrations were constrained to all-at-once shifts."}),"\n",(0,n.jsx)(a.li,{children:"Database read replicas incurred high infrastructure costs."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"These limitations made it difficult to meet SLA expectations, scale workloads efficiently, and adapt the platform to new user and product needs."}),"\n",(0,n.jsx)(a.h2,{id:"the-legacy-architecture",children:"The Legacy Architecture"}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig1.png",alt:"challenge",width:"1000",align:"middle"}),"\n",(0,n.jsxs)(a.p,{children:["Peloton's earlier architecture relied on daily snapshots from a monolithic ",(0,n.jsx)(a.strong,{children:"PostgreSQL"})," database. The analytics systems would consume these snapshots, often waiting hours for completion. This not only delayed reporting but also introduced downstream rigidity."]}),"\n",(0,n.jsx)(a.p,{children:"Because the same data platform supported both online and analytical workloads, any schema or service migration required significant planning and coordination. Database read replicas, used to scale reads, increased cost overhead. Moreover, recommendation systems that depended on data freshness were constrained by the snapshot interval, limiting personalization capabilities. This architecture struggled to support a fast-moving product roadmap, near real-time analytics, and the data agility needed to experiment and iterate."}),"\n",(0,n.jsx)(a.h2,{id:"reimagining-the-data-platform-with-apache-hudi",children:"Reimagining the Data Platform with Apache Hudi"}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig2.png",alt:"challenge",width:"1000",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"To address these challenges, the data platform team introduced Apache Hudi as the foundation of its modern data lake. The architecture was rebuilt to support Change Data Capture (CDC) ingestion from both PostgreSQL and DynamoDB using Debezium, with Kafka acting as the transport layer. A custom-built Hudi writer was developed to ingest CDC records into S3 using Apache Spark on EMR (version 6.12.0 with Hudi 0.13.1)."}),"\n",(0,n.jsx)(a.p,{children:"Peloton initially chose Copy-on-Write (CoW) table formats to support querying via Redshift Spectrum and simplify adoption. However, performance and cost bottlenecks prompted a transition to Merge-on-Read (MoR) tables with asynchronous table services for cleaning and compaction."}),"\n",(0,n.jsx)(a.p,{children:"Key architectural enhancements included:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Support for GDPR compliance"})," through structured delete propagation."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Time travel queries"})," for recommender model training and data recovery."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Phased migration support"})," for microservices via decoupled ingestion."]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Peloton's broader data platform tech stack supports this architecture with a range of tools for orchestration, analytics, and governance. This includes EMR for compute, Redshift for querying, DBT for data transformations, Looker for BI and visualization, Airflow for orchestration, and DataHub for metadata management. These components complement Apache Hudi in forming a modular and production-ready lakehouse stack."}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig3.png",alt:"challenge",width:"1000",align:"middle"}),"\n",(0,n.jsx)(a.h2,{id:"learnings-from-running-hudi-at-scale",children:"Learnings from Running Hudi at Scale"}),"\n",(0,n.jsx)(a.p,{children:"With Hudi now integrated into Peloton's data lake, the team began to observe and address new operational and architectural challenges that emerged at scale. This section outlines the major lessons learned while maintaining high-ingestion throughput, ensuring data reliability, and keeping infrastructure costs under control."}),"\n",(0,n.jsx)(a.h3,{id:"cow-vs-mor-performance-trade-offs",children:"CoW vs MoR: Performance Trade-offs"}),"\n",(0,n.jsx)(a.p,{children:"Initially, Copy-on-Write (CoW) tables were chosen to simplify deployment and ensure compatibility with Redshift Spectrum. However, as ingestion frequency increased and update volumes spanned hundreds of partitions, performance became a bottleneck. Some high-frequency tables with updates across 256 partitions took nearly an hour to process per run. Additionally, retaining 30 days of commits for training recommender models significantly inflated storage requirements, reaching into the hundreds of gigabytes."}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig4.png",alt:"challenge",width:"1000",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"To resolve this, the team migrated to Hudi\u2019s Merge-on-Read (MoR) tables and reduced commit retention to 7 days. With ingestion jobs now running every 10 minutes, latency dropped significantly, and storage and compute usage became more efficient."}),"\n",(0,n.jsx)(a.h3,{id:"async-vs-inline-table-services",children:"Async vs Inline Table Services"}),"\n",(0,n.jsx)(a.p,{children:"To improve write throughput and meet low-latency ingestion goals, the Peloton team initially configured Apache Hudi with asynchronous cleaner and compactor services. This approach worked well across most tables, allowing ingestion pipelines to run every 10 minutes with minimal blocking but introduced some operational edge cases. Some of the challenges encountered included:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Concurrent execution of writer and cleaner jobs, leading to conflicts. These were mitigated by introducing DynamoDB-based locks to serialize access."}),"\n",(0,n.jsxs)(a.li,{children:["Reader-cleaner race conditions, where time travel queries intermittently failed with ",(0,n.jsx)(a.code,{children:'"File Not Found"'})," errors - traced back to cleaners deleting files mid-read."]}),"\n",(0,n.jsx)(a.li,{children:"Compaction disruptions caused by EMR node terminations, which led to orphaned files when jobs failed mid-way."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"These edge cases were largely due to the operational complexity of managing concurrent workloads at Peloton\u2019s scale. After weighing reliability against latency, the team opted to switch to inline table services for compaction and cleaning, augmented with custom logic to control when these actions would run. This change improved system stability while maintaining acceptable latency trade-offs."}),"\n",(0,n.jsx)(a.h3,{id:"glue-schema-version-limits",children:"Glue Schema Version Limits"}),"\n",(0,n.jsxs)(a.p,{children:["As schema evolution continued, the team used Hudi's ",(0,n.jsx)(a.code,{children:"META_SYNC_ENABLED"})," to sync schema updates with AWS Glue. Over time, high-frequency schema updates pushed the number of ",(0,n.jsx)(a.code,{children:"TABLE_VERSION"})," resources in Glue beyond the ",(0,n.jsx)(a.em,{children:"1 million"})," limit. This caused jobs to fail in ways that were initially difficult to trace."]}),"\n",(0,n.jsx)(a.p,{children:"One such failure manifested as the following error:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:"ERROR Client: Application diagnostics message: User class threw exception:\njava.lang.NoSuchMethodError: 'org.apache.hudi.exception.HoodieException \norg.apache.hudi.sync.common.util.SyncUtilHelpers.getExceptionFromList(java.util.Collection)'\n"})}),"\n",(0,n.jsx)(a.p,{children:"After significant debugging, the issue was traced to AWS Glue limits. The team implemented a multi-step fix:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Worked with AWS to temporarily raise resource limits."}),"\n",(0,n.jsx)(a.li,{children:"Developed a Python service to identify and delete outdated table versions, removing over 1 million entries."}),"\n",(0,n.jsx)(a.li,{children:"Added an Airflow job to schedule weekly cleanup tasks."}),"\n",(0,n.jsx)(a.li,{children:"Improved schema sync logic to trigger only when the schema changed."}),"\n"]}),"\n",(0,n.jsx)(a.h3,{id:"debezium--toast-handling",children:"Debezium & TOAST Handling"}),"\n",(0,n.jsxs)(a.p,{children:["PostgreSQL CDC ingestion posed unique challenges due to the database\u2019s handling of large fields using TOAST (The Oversized-Attribute Storage Technique). When fields over 8KB were unchanged, Debezium emitted a placeholder value ",(0,n.jsx)(a.code,{children:"__debezium_unavailable_value"}),", making it impossible to determine whether the value had changed."]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig5.png",alt:"challenge",width:"1000",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"To address this, Peloton:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Populated initial data using PostgreSQL snapshots."}),"\n",(0,n.jsx)(a.li,{children:"Implemented self-joins between incoming CDC records and existing Hudi records to fill in missing values."}),"\n",(0,n.jsx)(a.li,{children:"Separated inserts, updates, and deletes within Spark batch processing."}),"\n",(0,n.jsxs)(a.li,{children:["Used the ",(0,n.jsx)(a.code,{children:"ts"})," field as the precombine key to ensure only the latest record state was retained."]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"A reconciliation pipeline was also developed to heal data inconsistencies caused by multiple operations on the same key within a batch (e.g., create-delete-create)."}),"\n",(0,n.jsx)(a.h3,{id:"data-validation-and-quality-enforcement",children:"Data Validation and Quality Enforcement"}),"\n",(0,n.jsx)(a.p,{children:"Data quality was critical to ensure trust in the newly established data lake. The team developed several internal libraries and checks:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["A Crypto Shredding Library to encrypt ",(0,n.jsx)(a.code,{children:"user_id"})," and other PII fields before storage."]}),"\n",(0,n.jsx)(a.li,{children:"A Data Validation Framework that compared records in the lake against snapshot data."}),"\n",(0,n.jsx)(a.li,{children:"A Data Quality Library that enforced column-level thresholds. These checks integrated with DataHub and were tied to Airflow sensors to halt downstream jobs on failures."}),"\n"]}),"\n",(0,n.jsx)(a.h3,{id:"dynamodb-ingestion-and-schema-challenges",children:"DynamoDB Ingestion and Schema Challenges"}),"\n",(0,n.jsx)(a.p,{children:"Some Peloton services relied on DynamoDB for operational workloads (NoSQL). To ingest these datasets into the lake, the team used DynamoDB Streams and a Kafka Connector, allowing reuse of the existing Kafka-based Hudi ingestion path."}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig6.png",alt:"challenge",width:"1000",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"However, the NoSQL nature of DynamoDB introduced schema management challenges. Two strategies were evaluated:"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsx)(a.li,{children:"Stakeholder-defined schemas, using SUPER-type fields."}),"\n",(0,n.jsx)(a.li,{children:"Dynamic schema inference, where incoming JSON records were parsed, and the evolving schema was inferred and reconciled."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"The team opted for dynamic inference despite increased processing time, as it enabled better support for exploratory workloads. Daily snapshots and reconciliation steps helped clean up inconsistent schema states."}),"\n",(0,n.jsx)(a.h3,{id:"reducing-operational-costs",children:"Reducing Operational Costs"}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/2025-07-15-modernizing-datainfra-peloton-hudi/pel_fig7.png",alt:"challenge",width:"1000",align:"middle"}),"\n",(0,n.jsxs)(a.p,{children:["As the system matured, cost optimization became a priority. The team used ",(0,n.jsx)(a.a,{href:"https://github.com/ganglia/",children:"Ganglia"})," to analyze job profiles and identify areas for improvement:"]}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"EMR resources were gradually right-sized based on CPU and memory usage."}),"\n",(0,n.jsx)(a.li,{children:"Conditional Hive syncing was introduced to avoid unnecessary sync operations during each run."}),"\n",(0,n.jsx)(a.li,{children:"A Spark-side inefficiency was discovered where archived timelines were unnecessarily loaded, causing jobs to take 4x longer. Fixing this reduced overall latency and compute resource usage."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"These operational refinements significantly reduced idle times and improved the cost-efficiency of the platform."}),"\n",(0,n.jsx)(a.h2,{id:"gains-from-hudi-adoption",children:"Gains from Hudi Adoption"}),"\n",(0,n.jsx)(a.p,{children:"Peloton's transition to Apache Hudi led to measurable performance, operational, and cost-related improvements across its modern data platform."}),"\n",(0,n.jsx)(a.p,{children:"Peloton's transition to Apache Hudi yielded several measurable improvements:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Ingestion frequency increased from once daily to every 10 minutes."}),"\n",(0,n.jsx)(a.li,{children:"Reduced snapshot job durations from an hour to under 15 minutes."}),"\n",(0,n.jsx)(a.li,{children:"Cost savings by eliminating read replicas and optimizing EMR cluster usage."}),"\n",(0,n.jsx)(a.li,{children:"Time travel support enabled retrospective analysis and model re-training."}),"\n",(0,n.jsx)(a.li,{children:"Improved compliance posture through structured deletes and encrypted PII."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"The modernization laid the groundwork for future evolution, including real-time streaming ingestion using Apache Flink and continued improvements in data freshness, latency, and governance."}),"\n",(0,n.jsxs)(a.p,{children:["This blog is based on Peloton\u2019s presentation at the Apache Hudi Community Sync. If you are interested in watching the recorded version of the video, you can find it ",(0,n.jsx)(a.a,{href:"https://youtu.be/-Pyid5K9dyU?feature=shared",children:"here"}),"."]}),"\n",(0,n.jsx)(a.hr,{})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},82294:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(83692),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi: From Zero To One (4/10)",excerpt:"All about writer indexes",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2023-09-27-Apache-Hudi-From-Zero-To-One-blog-4.png",tags:["hudi","spark","blog","course","tutorial","datumagic","data lake","lakehouse","apache hudi","apache spark"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blog.datumagic.ai/p/apache-hudi-from-zero-to-one-410",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},82357:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/2025-09-17-hudi-auto-gen-keys.fig1-fb5004b3f1cd1832795f39f6c7255411.jpg"},82463:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide13-998b520f6392c9d218febdcb0f87b59f.png"},82517:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig2-a2a9161b7ad75628a36b03514ae4a9c4.png"},82601:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig-9-PuppyGraph-Query-2-59da47f7532f00f65a89d7ac108865a8.png"},82759:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/2020-08-20-skeleton-dcb4339a37b8ac8ce291a4f119e326c9.png"},82915:(e,a,t)=>{"use strict";t.d(a,{A:()=>s});var i=t(44586),n=t(74848);const s=({title:e,isItalic:a})=>{const{siteConfig:t}=(0,i.A)(),{slackUrl:s}=t.customFields;return(0,n.jsx)("a",{href:s,style:{fontStyle:a?"italic":"normal"},target:"_blank",rel:"noopener noreferrer",children:e})}},82939:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(92296),n=t(74848),s=t(28453),r=t(9230);const o={title:"An Introduction to the Hudi and Flink Integration",authors:[{name:"Danny Chan"}],category:"blog",image:"/assets/images/blog/2023-05-02-intro-to-hudi-and-flink.png",tags:["blog","apache hudi","apache flink","onehouse"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.onehouse.ai/blog/intro-to-hudi-and-flink",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},83270:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(53135),n=t(74848),s=t(28453),r=t(9230);const o={title:"Indexing in Apache Hudi",author:"Sanjeet Shukla",category:"blog",image:"/assets/images/blog/2024-12-31-indexing-in-apache-hudi.jpeg",tags:["blog","apache hudi","indexing","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@sanjeets1900/indexing-in-apache-hudi-674f9481796e",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},83414:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/09/15/Apache-Hudi-From-Zero-To-One-blog-3","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-09-15-Apache-Hudi-From-Zero-To-One-blog-3.mdx","source":"@site/blog/2023-09-15-Apache-Hudi-From-Zero-To-One-blog-3.mdx","title":"Apache Hudi: From Zero To One (3/10)","description":"Redirecting... please wait!!","date":"2023-09-15T00:00:00.000Z","tags":[{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"spark","permalink":"/blog/tags/spark"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"course","permalink":"/blog/tags/course"},{"inline":true,"label":"tutorial","permalink":"/blog/tags/tutorial"},{"inline":true,"label":"datumagic","permalink":"/blog/tags/datumagic"},{"inline":true,"label":"data lake","permalink":"/blog/tags/data-lake"},{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"apache spark","permalink":"/blog/tags/apache-spark"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Shiyan Xu","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi: From Zero To One (3/10)","excerpt":"Understand write flows and operations","author":"Shiyan Xu","category":"blog","image":"/assets/images/blog/2023-09-15-Apache-Hudi-From-Zero-To-One-blog-3.png","tags":["hudi","spark","blog","course","tutorial","datumagic","data lake","lakehouse","apache hudi","apache spark"]},"unlisted":false,"prevItem":{"title":"A Beginner\u2019s Guide to Apache Hudi with PySpark \u2014 Part 1 of 2","permalink":"/blog/2023/09/19/A-Beginners-Guide-to-Apache-Hudi-with-PySpark-Part-1-of-2"},"nextItem":{"title":"Simplify operational data processing in data lakes using AWS Glue and Apache Hudi","permalink":"/blog/2023/09/13/Simplify-operational-data-processing-in-data-lakes-using-AWS-Glue-and-Apache-Hudi"}}')},83540:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2020/08/20/efficient-migration-of-large-parquet-tables","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-08-20-efficient-migration-of-large-parquet-tables.md","source":"@site/blog/2020-08-20-efficient-migration-of-large-parquet-tables.md","title":"Efficient Migration of Large Parquet Tables to Apache Hudi","description":"We will look at how to migrate a large parquet table to Hudi without having to rewrite the entire dataset.","date":"2020-08-20T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"migration","permalink":"/blog/tags/migration"},{"inline":true,"label":"bootstrap","permalink":"/blog/tags/bootstrap"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":5.05,"hasTruncateMarker":true,"authors":[{"name":"vbalaji","key":null,"page":null}],"frontMatter":{"title":"Efficient Migration of Large Parquet Tables to Apache Hudi","excerpt":"Migrating a large parquet table to Apache Hudi without having to rewrite the entire dataset.","author":"vbalaji","category":"blog","image":"/assets/images/blog/2020-08-20-skeleton.png","tags":["how-to","migration","bootstrap","apache hudi"]},"unlisted":false,"prevItem":{"title":"Async Compaction Deployment Models","permalink":"/blog/2020/08/21/async-compaction-deployment-model"},"nextItem":{"title":"Incremental Processing on the Data Lake","permalink":"/blog/2020/08/18/hudi-incremental-processing-on-data-lakes"}}')},83578:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(20402),n=t(74848),s=t(28453);const r={title:"Hudi entered Apache Incubator",author:"admin",date:new Date("2019-01-18T00:00:00.000Z"),category:"blog"},o=void 0,l={authorsImageUrls:[void 0]},d=[];function c(e){const a={p:"p",...(0,s.R)(),...e.components};return(0,n.jsx)(a.p,{children:"In the coming weeks, we will be moving in our new home on the Apache Incubator."})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},83692:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/09/27/Apache-Hudi-From-Zero-To-One-blog-4","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-09-27-Apache-Hudi-From-Zero-To-One-blog-4.mdx","source":"@site/blog/2023-09-27-Apache-Hudi-From-Zero-To-One-blog-4.mdx","title":"Apache Hudi: From Zero To One (4/10)","description":"Redirecting... please wait!!","date":"2023-09-27T00:00:00.000Z","tags":[{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"spark","permalink":"/blog/tags/spark"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"course","permalink":"/blog/tags/course"},{"inline":true,"label":"tutorial","permalink":"/blog/tags/tutorial"},{"inline":true,"label":"datumagic","permalink":"/blog/tags/datumagic"},{"inline":true,"label":"data lake","permalink":"/blog/tags/data-lake"},{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"apache spark","permalink":"/blog/tags/apache-spark"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Shiyan Xu","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi: From Zero To One (4/10)","excerpt":"All about writer indexes","author":"Shiyan Xu","category":"blog","image":"/assets/images/blog/2023-09-27-Apache-Hudi-From-Zero-To-One-blog-4.png","tags":["hudi","spark","blog","course","tutorial","datumagic","data lake","lakehouse","apache hudi","apache spark"]},"unlisted":false,"prevItem":{"title":"Apache Hudi: Copy on Write(CoW) Table","permalink":"/blog/2023/10/06/Apache-Hudi-Copy-on-Write-CoW-Table"},"nextItem":{"title":"Exploring the Architecture of Apache Iceberg, Delta Lake, and Apache Hudi","permalink":"/blog/2023/09/22/Exploring-the-Architecture-of-Apache-Iceberg-Delta-Lake-and-Apache-Hudi"}}')},83823:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(76114),n=t(74848),s=t(28453),r=t(9230);const o={title:"Why Uber Built Hudi: The Strategic Decision Behind a Custom Table Format",author:"ThamizhElango Natarajan",category:"blog",image:"/assets/images/blog/2025-07-03-why-uber-built-hudi-the-strategic-decision-behind-a-custom-table-format.jpg",tags:["blog","Apache Hudi","Apache Iceberg","Lakehouse","use-case","Uber","det"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://thamizhelango.medium.com/why-uber-built-hudi-the-strategic-decision-behind-a-custom-table-format-f57db68b0cb9",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},83846:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/12/29/apache-hudi-2024-a-year-in-review","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-12-29-apache-hudi-2024-a-year-in-review.mdx","source":"@site/blog/2024-12-29-apache-hudi-2024-a-year-in-review.mdx","title":"Apache Hudi 2024: A Year In Review","description":"As we wrap up another remarkable year for Apache Hudi, I am thrilled to reflect on the tremendous achievements and milestones that have defined 2024. This year has been particularly special as we achieved several significant milestones, including the landmark release of Hudi 1.0, the publication of comprehensive books, and the introduction of new tools that expand Hudi\'s ecosystem.","date":"2024-12-29T00:00:00.000Z","tags":[{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"community","permalink":"/blog/tags/community"}],"readingTime":6.79,"hasTruncateMarker":false,"authors":[{"name":"Shiyan Xu","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi 2024: A Year In Review","excerpt":"Reflect on and celebrate the myriad of exciting developments and accomplishments that have defined the year 2024 for the Hudi community.","author":"Shiyan Xu","category":"blog","image":"/assets/images/blog/2024-12-29-a-year-in-review-2024/cover.jpg","tags":["apache hudi","community"]},"unlisted":false,"prevItem":{"title":"The Architect\u2019s Guide to Open Table Formats and Object Storage","permalink":"/blog/2024/12/31/the-architects-guide-to-open-table-formats-and-object-storage"},"nextItem":{"title":"How lakehouse handles concurrent Read and Writes","permalink":"/blog/2024/12/28/how-lakehouse-handles-concurrent-read-and-writes"}}')},83980:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/11/30/Mastering-Data-Lakes-A-Deep-Dive-into-MINIO-Hudi-and-Delta-Streamer","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-11-30-Mastering-Data-Lakes-A-Deep-Dive-into-MINIO-Hudi-and-Delta-Streamer.mdx","source":"@site/blog/2023-11-30-Mastering-Data-Lakes-A-Deep-Dive-into-MINIO-Hudi-and-Delta-Streamer.mdx","title":"Mastering Data Lakes: A Deep Dive into MINIO, Hudi, and Delta Streamer","description":"Redirecting... please wait!!","date":"2023-11-30T00:00:00.000Z","tags":[{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"mino","permalink":"/blog/tags/mino"},{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"deltastreamer","permalink":"/blog/tags/deltastreamer"},{"inline":true,"label":"linkedin","permalink":"/blog/tags/linkedin"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"Soumil Shah","key":null,"page":null}],"frontMatter":{"title":"Mastering Data Lakes: A Deep Dive into MINIO, Hudi, and Delta Streamer","excerpt":"A Deep Dive into MINIO, Hudi, and Delta Streamer","author":"Soumil Shah","category":"blog","image":"/assets/images/blog/2023-11-30-Mastering-Data-Lakes-A-Deep-Dive-into-MINIO-Hudi-and-Delta-Streamer.png","tags":["apache hudi","mino","how-to","deltastreamer","linkedin"]},"unlisted":false,"prevItem":{"title":"Getting started with Apache Hudi","permalink":"/blog/2023/12/01/Getting-started-with-Apache-Hudi"},"nextItem":{"title":"Apache Hudi (Part 1): History, Getting Started","permalink":"/blog/2023/11/28/Apache-Hudi-Part-1-History-Getting-Started"}}')},84003:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/SingleWriterAsync-3d7ddf7312381eab7fdb91a7f2746376.gif"},84066:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/ConcurrencyControlConflicts-55bed17c500b3b29e4f8cdb42cf0f483.png"},84348:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2020/10/19/Origins-of-Data-Lake-at-Grofers","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-10-19-Origins-of-Data-Lake-at-Grofers.mdx","source":"@site/blog/2020-10-19-Origins-of-Data-Lake-at-Grofers.mdx","title":"Origins of Data Lake at Grofers","description":"Redirecting... please wait!!","date":"2020-10-19T00:00:00.000Z","tags":[{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"datalake","permalink":"/blog/tags/datalake"},{"inline":true,"label":"change data capture","permalink":"/blog/tags/change-data-capture"},{"inline":true,"label":"cdc","permalink":"/blog/tags/cdc"},{"inline":true,"label":"grofers","permalink":"/blog/tags/grofers"}],"readingTime":0.11,"hasTruncateMarker":false,"authors":[{"name":"Akshay Agarwal","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Origins of Data Lake at Grofers","authors":[{"name":"Akshay Agarwal"}],"category":"blog","image":"/assets/images/blog/2020-10-19-Origins-of-Data-Lake-at-Grofers.gif","tags":["use-case","datalake","change data capture","cdc","grofers"]},"unlisted":false,"prevItem":{"title":"Apply record level changes from relational databases to Amazon S3 data lake using Apache Hudi on Amazon EMR and AWS Database Migration Service","permalink":"/blog/2020/10/19/hudi-meets-aws-emr-and-aws-dms"},"nextItem":{"title":"Apache Hudi meets Apache Flink","permalink":"/blog/2020/10/15/apache-hudi-meets-apache-flink"}}')},84548:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi-stack-1-x-7ac3c524e79ef6771783245fef6ce062.png"},84876:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/batch_vs_incremental-ea2dd4a52745c47a6b2aa11b65c058c9.png"},85337:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/6-spark-upsert-write-time-chart-734ca6975e38a28de049809099a99caa.png"},85402:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi-design-diagrams_-_Page_6-3b292156302554ff2ad53e6f2847f56c.png"},85591:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(63680),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi does XYZ (1/10): File pruning with multi-modal index",excerpt:"File pruning with multi-modal index",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2025-06-16-Apache-Hudi-does-XYZ-110-cover.jpg",tags:["hudi","spark","blog","course","tutorial","datumagic","data lake","lakehouse","apache hudi","apache spark"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blog.datumagic.ai/p/apache-hudi-does-xyz-110",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},85600:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/trigger-based-cdc-51c20f90024a12e97cbc728cfc7c0ed4.png"},85668:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(80431),n=t(74848),s=t(28453),r=t(9230);const o={title:"Data Deduplication Strategies in an Open Lakehouse Architecture",author:"Dipankar Mazumdar, Aditya Goenka",category:"blog",image:"/assets/images/blog/dedupe.png",tags:["blog","Apache Hudi","Apache Iceberg","Delta Lake","Deduplication"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.onehouse.ai/blog/data-deduplication-strategies-in-an-open-lakehouse-architecture",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},85711:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/10/08/what-why-and-how-apache-hudis-bloom-index","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-10-08-what-why-and-how-apache-hudis-bloom-index.mdx","source":"@site/blog/2022-10-08-what-why-and-how-apache-hudis-bloom-index.mdx","title":"What, Why and How : Apache Hudi\u2019s Bloom Index","description":"Redirecting... please wait!!","date":"2022-10-08T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"design","permalink":"/blog/tags/design"},{"inline":true,"label":"bloom","permalink":"/blog/tags/bloom"},{"inline":true,"label":"indexing","permalink":"/blog/tags/indexing"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Sivabalan Narayanan","socials":{},"key":null,"page":null}],"frontMatter":{"title":"What, Why and How : Apache Hudi\u2019s Bloom Index","authors":[{"name":"Sivabalan Narayanan"}],"category":"blog","image":"/assets/images/blog/2022-10-08-what-why-and-how-apache-hudis-bloom-index.png","tags":["how-to","design","bloom","indexing","medium"]},"unlisted":false,"prevItem":{"title":"Get started with Apache Hudi using AWS Glue by implementing key design concepts \u2013 Part 1","permalink":"/blog/2022/10/17/Get-started-with-Apache-Hudi-using-AWS"},"nextItem":{"title":"Ingest streaming data to Apache Hudi tables using AWS Glue and Apache Hudi DeltaStreamer","permalink":"/blog/2022/10/06/Ingest-streaming-data-to-Apache-Hudi-using-AWS-Glue-and-DeltaStreamer"}}')},85782:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/02/17/Fresher-Data-Lake-on-AWS-S3","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-02-17-Fresher-Data-Lake-on-AWS-S3.mdx","source":"@site/blog/2022-02-17-Fresher-Data-Lake-on-AWS-S3.mdx","title":"Fresher Data Lake on AWS S3","description":"Redirecting... please wait!!","date":"2022-02-17T00:00:00.000Z","tags":[{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"incremental processing","permalink":"/blog/tags/incremental-processing"},{"inline":true,"label":"robinhood","permalink":"/blog/tags/robinhood"}],"readingTime":0.1,"hasTruncateMarker":false,"authors":[{"name":"Balaji Varadarajan","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Fresher Data Lake on AWS S3","authors":[{"name":"Balaji Varadarajan"}],"category":"blog","image":"/assets/images/blog/2022-02-17-fresher-data-lake-on-aws-s3.png","tags":["use-case","incremental processing","robinhood"]},"unlisted":false,"prevItem":{"title":"Understanding its core concepts from hudi persistence files","permalink":"/blog/2022/02/20/Understanding-its-core-concepts-from-hudi-persistence-files"},"nextItem":{"title":"Open Source Data Lake Table Formats: Evaluating Current Interest and Rate of Adoption","permalink":"/blog/2022/02/12/Open-Source-Data-Lake-Table-Formats-Evaluating-Current-Interest-and-Rate-of-Adoption"}}')},85950:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(80437),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi: From Zero To One (1/10)",excerpt:"A first glance at Hudi's storage format",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2023-08-28-Apache-Hudi-From-Zero-To-One-blog-1.png",tags:["hudi","spark","blog","course","tutorial","datumagic","data lake","lakehouse","apache hudi","apache spark"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blog.datumagic.ai/p/apache-hudi-from-zero-to-one-110",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},86214:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/adding_new_files-13e5a1cf0c213c07a412b09a29be4e3d.png"},86415:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/01/20/Learn-How-to-Move-Data-From-MongoDB-to-Apache-Hudi-Using-PySpark","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-01-20-Learn-How-to-Move-Data-From-MongoDB-to-Apache-Hudi-Using-PySpark.mdx","source":"@site/blog/2024-01-20-Learn-How-to-Move-Data-From-MongoDB-to-Apache-Hudi-Using-PySpark.mdx","title":"Learn How to Move Data From MongoDB to Apache Hudi Using PySpark","description":"Redirecting... please wait!!","date":"2024-01-20T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"linkedin","permalink":"/blog/tags/linkedin"},{"inline":true,"label":"beginner","permalink":"/blog/tags/beginner"},{"inline":true,"label":"mongodb","permalink":"/blog/tags/mongodb"},{"inline":true,"label":"apache spark","permalink":"/blog/tags/apache-spark"},{"inline":true,"label":"pyspark","permalink":"/blog/tags/pyspark"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"Soumil Shah","key":null,"page":null}],"frontMatter":{"title":"Learn How to Move Data From MongoDB to Apache Hudi Using PySpark","excerpt":"Learn How to Move Data From MongoDB to Apache Hudi Using PySpark","author":"Soumil Shah","category":"blog","image":"/assets/images/blog/2024-01-20-Learn-How-to-Move-Data-From-MongoDB-to-Apache-Hudi-Using-PySpark.png","tags":["blog","apache hudi","linkedin","beginner","mongodb","apache spark","pyspark"]},"unlisted":false,"prevItem":{"title":"Data Engineering: Bootstrapping Data lake with Apache Hudi","permalink":"/blog/2024/01/20/Data-Engineering-Bootstrapping-Data-lake-with-Apache-Hudi"},"nextItem":{"title":"Deleting Items from Apache Hudi using Delta Streamer in UPSERT Mode with Kafka Avro Messages","permalink":"/blog/2024/01/18/Deleting-Items-from-Apache-Hudi-using-Delta-Streamer-in-UPSERT-Mode-with-Kafka-Avro-Messages"}}')},86620:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image7-0a4cfd0fa02ba7efc07901ae75d1c188.png"},86672:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/09/20/Building-Streaming-Data-Lakes-with-Hudi-and-MinIO","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-09-20-Building-Streaming-Data-Lakes-with-Hudi-and-MinIO.mdx","source":"@site/blog/2022-09-20-Building-Streaming-Data-Lakes-with-Hudi-and-MinIO.mdx","title":"Building Streaming Data Lakes with Hudi and MinIO","description":"Redirecting... please wait!!","date":"2022-09-20T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"datalake","permalink":"/blog/tags/datalake"},{"inline":true,"label":"datalake platform","permalink":"/blog/tags/datalake-platform"},{"inline":true,"label":"streaming ingestion","permalink":"/blog/tags/streaming-ingestion"},{"inline":true,"label":"minio","permalink":"/blog/tags/minio"}],"readingTime":0.1,"hasTruncateMarker":false,"authors":[{"name":"Matt Sarrel","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Building Streaming Data Lakes with Hudi and MinIO","authors":[{"name":"Matt Sarrel"}],"category":"blog","image":"/assets/images/blog/2022-09-20_streaming_data_lakes_with_hudi_and_minio.png","tags":["how-to","datalake","datalake platform","streaming ingestion","minio"]},"unlisted":false,"prevItem":{"title":"Data processing with Spark: time traveling","permalink":"/blog/2022/09/28/Data-processing-with-Spark-time-traveling"},"nextItem":{"title":"Data Lake / Lakehouse Guide: Powered by Data Lake Table Formats (Delta Lake, Iceberg, Hudi)","permalink":"/blog/2022/08/25/Data-Lake-Lakehouse-Guide-Powered-by-Data-Lake-Table-Formats-Delta-Lake-Iceberg-Hudi"}}')},86738:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/hudi_dbt_lakehouse-14b1cb2c180ecb95dc78e3b4c44a6301.png"},86840:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/03/01/hudi-file-sizing","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-03-01-hudi-file-sizing.md","source":"@site/blog/2021-03-01-hudi-file-sizing.md","title":"Streaming Responsibly - How Apache Hudi maintains optimum sized files","description":"Apache Hudi is a data lake platform technology that provides several functionalities needed to build and manage data lakes.","date":"2021-03-01T00:00:00.000Z","tags":[{"inline":true,"label":"design","permalink":"/blog/tags/design"},{"inline":true,"label":"file sizing","permalink":"/blog/tags/file-sizing"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":4.56,"hasTruncateMarker":true,"authors":[{"name":"shivnarayan","key":null,"page":null}],"frontMatter":{"title":"Streaming Responsibly - How Apache Hudi maintains optimum sized files","excerpt":"Maintaining well-sized files can improve query performance significantly","author":"shivnarayan","category":"blog","image":"/assets/images/blog/2021-03-01-hudi-file-sizing.png","tags":["design","file sizing","apache hudi"]},"unlisted":false,"prevItem":{"title":"Build a data lake using amazon kinesis data stream for amazon dynamodb and apache hudi","permalink":"/blog/2021/03/04/Build-a-data-lake-using-amazon-kinesis-data-stream-for-amazon-dynamodb-and-apache-hudi"},"nextItem":{"title":"Data Lakehouse: Building the Next Generation of Data Lakes using Apache Hudi","permalink":"/blog/2021/03/01/Data-Lakehouse-Building-the-Next-Generation-of-Data-Lakes-using-Apache-Hudi"}}')},87115:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/04/26/the-lakehouse-trifecta","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-04-26-the-lakehouse-trifecta.mdx","source":"@site/blog/2023-04-26-the-lakehouse-trifecta.mdx","title":"Delta, Hudi, and Iceberg: The Data Lakehouse Trifecta","description":"Redirecting... please wait!!","date":"2023-04-26T00:00:00.000Z","tags":[{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"delta lake","permalink":"/blog/tags/delta-lake"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"apache iceberg","permalink":"/blog/tags/apache-iceberg"},{"inline":true,"label":"comparison","permalink":"/blog/tags/comparison"},{"inline":true,"label":"dzone","permalink":"/blog/tags/dzone"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Andrey Gusarov","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Delta, Hudi, and Iceberg: The Data Lakehouse Trifecta","authors":[{"name":"Andrey Gusarov"}],"category":"blog","image":"/assets/images/blog/0426-lakehouse-trifecta.png","tags":["lakehouse","delta lake","apache hudi","apache iceberg","comparison","dzone"]},"unlisted":false,"prevItem":{"title":"Can you concurrently write data to Apache Hudi w/o any lock provider?","permalink":"/blog/2023/04/29/can-you-concurrently-write-data-to-apache-hudi-w-o-any-lock-provider"},"nextItem":{"title":"Getting Started: Incrementally process data with Apache Hudi","permalink":"/blog/2023/04/18/getting-started-incrementally-process-data-with-apache-hudi"}}')},87190:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/07/21/mor-comparison","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-07-21-mor-comparison.md","source":"@site/blog/2025-07-21-mor-comparison.md","title":"A Deep Dive on Merge-on-Read (MoR) in Lakehouse Table Formats","description":"TL;DR","date":"2025-07-21T00:00:00.000Z","tags":[{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"Merge-on-Read (MoR)","permalink":"/blog/tags/merge-on-read-mo-r"},{"inline":true,"label":"Streaming","permalink":"/blog/tags/streaming"}],"readingTime":20.77,"hasTruncateMarker":false,"authors":[{"name":"Dipankar Mazumdar","key":null,"page":null}],"frontMatter":{"title":"A Deep Dive on Merge-on-Read (MoR) in Lakehouse Table Formats","excerpt":"How is MoR implemented in Hudi, Iceberg, Delta and how it impacts workloads","author":"Dipankar Mazumdar","category":"blog","image":"/assets/images/blog/2025-07-21-mor-comparison/mor-1200x600.jpg","tags":["Apache Hudi","Merge-on-Read (MoR)","Streaming"]},"unlisted":false,"prevItem":{"title":"Building a RAG-based AI Recommender (2/2)","permalink":"/blog/2025/08/29/building-a-rag-based-ai-recommender-2"},"nextItem":{"title":"Modernizing Data Infrastructure at Peloton Using Apache Hudi","permalink":"/blog/2025/07/15/modernizing-datainfra-peloton-hudi"}}')},87230:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(68692),n=t(74848),s=t(28453),r=t(9230);const o={title:"Setting Uber\u2019s Transactional Data Lake in Motion with Incremental ETL Using Apache Hudi",authors:[{name:"Vinoth Govindarajan"},{name:"Saketh Chintapalli"},{name:"Yogesh Saswade"},{name:"Aayush Bareja"}],category:"blog",image:"/assets/images/blog/hudi-lakehouse-architecture-uber.png",tags:["incremental processing","datalake","apache hudi","medallion architecture","uber"]},l=void 0,d={authorsImageUrls:[void 0,void 0,void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.uber.com/blog/ubers-lakehouse-architecture/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},87248:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(2074),n=t(74848),s=t(28453),r=t(82915);const o={title:"Apache Hudi 2022 - A year in Review",excerpt:"2022 was the best year for Apache Hudi yet! Huge thank you to everyone who contributed!",author:"Sivabalan Narayanan",category:"blog",image:"/assets/images/blog/Apache-Hudi-2022-Review.png",tags:["apache hudi","community"]},l=void 0,d={authorsImageUrls:[void 0]},c=[{value:"Apache Hudi Momentum",id:"apache-hudi-momentum",level:2},{value:"Key Releases in 2022",id:"key-releases-in-2022",level:2},{value:"Community Events",id:"community-events",level:2},{value:"Community Content",id:"community-content",level:2},{value:"What to look for in 2023",id:"what-to-look-for-in-2023",level:2}];function h(e){const a={a:"a",h2:"h2",li:"li",ol:"ol",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)("img",{src:"/assets/images/blog/Apache-Hudi-2022-Review.png",alt:"drawing",style:{width:"80%",display:"block",marginLeft:"auto",marginRight:"auto"}}),"\n",(0,n.jsx)(a.h2,{id:"apache-hudi-momentum",children:"Apache Hudi Momentum"}),"\n",(0,n.jsxs)(a.p,{children:["As we wrap up 2022 I want to take the opportunity to reflect on and highlight the incredible progress of the Apache Hudi\nproject and most importantly, the community. First and foremost, I want to thank all of the contributors who have made\n2022 the best year for the project ever. There were ",(0,n.jsx)(a.a,{href:"https://ossinsight.io/analyze/apache/hudi#pull-requests",children:"over 2,200 PRs"}),"\ncreated (+38% YoY) and over 600+ users engaged on ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/",children:"Github"}),". The Apache Hudi community\n",(0,n.jsx)(r.A,{title:"slack channel"})," has grown to more\nthan 2,600 users (+100% YoY growth) averaging nearly 200 messages per month! The most impressive stat is that with this\nvolume growth, the median response time to questions is ~3h. ",(0,n.jsx)(r.A,{title:"Come join the community"}),"\nwhere people are sharing and helping each other!"]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/Apache-Hudi-Pull-Request-History.png",alt:"drawing",style:{width:"80%",display:"block",marginLeft:"auto",marginRight:"auto"}}),"\n",(0,n.jsx)(a.h2,{id:"key-releases-in-2022",children:"Key Releases in 2022"}),"\n",(0,n.jsxs)(a.p,{children:["2022 has been a year jam packed with exciting new features for Apache Hudi across 0.11.0 and 0.12.0 releases. In addition to new features, vendor/ecosystem partnerships and relationships have been strengthened across many in the community. ",(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/blog/apache-hudi-native-aws-integrations",children:"AWS continues to double down"})," on Apache Hudi, upgrading versions in ",(0,n.jsx)(a.a,{href:"https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi.html",children:"EMR"}),", ",(0,n.jsx)(a.a,{href:"https://docs.aws.amazon.com/athena/latest/ug/querying-hudi.html",children:"Athena"}),", ",(0,n.jsx)(a.a,{href:"https://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-external-tables.html",children:"Redshift"}),", and announcing a new ",(0,n.jsx)(a.a,{href:"https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-hudi.html",children:"native connector inside Glue"}),". ",(0,n.jsx)(a.a,{href:"https://prestodb.io/docs/current/connector/hudi.html",children:"Presto"})," and ",(0,n.jsx)(a.a,{href:"https://trino.io/docs/current/connector/hudi.html",children:"Trino"})," merged native Hudi connectors for interactive analytics. ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2022/07/11/build-open-lakehouse-using-apache-hudi-and-dbt/",children:"DBT"}),", ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/tree/master/hudi-kafka-connect",children:"Confluent"}),", ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/syncing_datahub",children:"Datahub"}),", and several others have added support for Hudi tables. While Google has supported Hudi for a while in ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/gcp_bigquery/",children:"BigQuery"})," and ",(0,n.jsx)(a.a,{href:"https://cloud.google.com/blog/products/data-analytics/getting-started-with-new-table-formats-on-dataproc",children:"Dataproc"}),", it also announced plans to add Hudi in ",(0,n.jsx)(a.a,{href:"https://cloud.google.com/blog/products/data-analytics/building-most-open-data-cloud-all-data-all-source-any-platform",children:"BigLake"}),". The first tutorial for ",(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/blog/apache-hudi-on-microsoft-azure",children:"Hudi on Azure Synapse Analytics"})," was published."]}),"\n",(0,n.jsx)(a.p,{children:"While there are too many features added in 2022 to list them all, take a look at some of the exciting highlights:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2022/05/17/Introducing-Multi-Modal-Index-for-the-Lakehouse-in-Apache-Hudi",children:"Multi-Modal Index"})," is a first-of-its-kind high-performance indexing subsystem for the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2024/07/11/what-is-a-data-lakehouse/",children:"Lakehouse"}),". It improves metadata lookup performance by up to 100x and reduces overall query latency by up to 30x. Two new indices were added to the metadata table - Bloom filter index that enables faster upsert performance and",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2022/06/09/Singificant-queries-speedup-from-Hudi-Column-Stats-Index-and-Data-Skipping-features",children:"  column stats index along with Data skipping"}),"  helps speed up queries dramatically."]}),"\n",(0,n.jsxs)(a.li,{children:["Hudi added support for ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-0.11.0/#async-indexer",children:"asynchronous indexing"})," to assist building such indices without blocking ingestion so that regular writers don't need to scale up resources for such one off spikes."]}),"\n",(0,n.jsx)(a.li,{children:"A new type of index called Bucket Index was introduced this year. This could be game changing for deterministic workloads with partitioned datasets. It is very light-weight and allows the distribution of records to buckets using a hash function."}),"\n",(0,n.jsxs)(a.li,{children:["Filesystem based Lock Provider - This implementation avoids the need of external systems and leverages the abilities of underlying filesystem to support lock provider needed for optimistic concurrency control in case of multiple writers. Please check the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/configurations#Locks-Configurations",children:"lock configuration"})," for details."]}),"\n",(0,n.jsx)(a.li,{children:"Deltastreamer Graceful Completion - Users can now configure a post-write completion strategy with deltastreamer continuous mode for graceful shutdown."}),"\n",(0,n.jsxs)(a.li,{children:["Schema on read is supported as an experimental feature since 0.11.0, allowing users to leverage Spark SQL DDL\xa0 support for ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/schema_evolution",children:"evolving data schema"})," needs(drop, rename etc).\xa0 Added support for a lot of ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/procedures/",children:"CALL commands"})," to invoke an array of actions on Hudi tables."]}),"\n",(0,n.jsxs)(a.li,{children:["It is now feasible to ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/encryption/",children:"encrypt"})," your data that you store with Apache Hudi."]}),"\n",(0,n.jsx)(a.li,{children:"Pulsar Write Commit Callback - On new events to the Hudi table, users can get notified via Pulsar."}),"\n",(0,n.jsxs)(a.li,{children:["Flink Enhancements: We added metadata table support, async clustering, data skipping, and bucket index for write paths. We also extended flink support to versions 1.13.x, 1.14.x and",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-0.12.0/#bundle-updates",children:"  1.15.x"}),"."]}),"\n",(0,n.jsxs)(a.li,{children:["Presto Hudi integration: In addition to the hive connector we have had for a long time, we added ",(0,n.jsx)(a.a,{href:"https://prestodb.io/docs/current/connector/hudi.html",children:"native Presto Hudi connector"}),". This enables users to get access to advanced features of Hudi faster. Users can now leverage metadata table to reduce file listing cost. We also added support for accessing clustered datasets this year."]}),"\n",(0,n.jsxs)(a.li,{children:["Trino Hudi integration: We also added ",(0,n.jsx)(a.a,{href:"https://trino.io/docs/current/connector/hudi.html",children:"native Trino Hudi connector"})," to assist in querying Hudi tables via Trino Engine. Users can now leverage metadata table to make their queries performant."]}),"\n",(0,n.jsxs)(a.li,{children:["Performance enhancements: Many performance optimizations were landed by the community throughout the year to keep Hudi on par with competition or better. Check out this ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2022/06/29/Apache-Hudi-vs-Delta-Lake-transparent-tpc-ds-lakehouse-performance-benchmarks",children:"TPC-DS benchmark"})," comparing Hudi vs Delta Lake."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://hudi.apache.org/releases/release-0.12.3#long-term-support",children:"Long Term Support"}),": We start to maintain 0.12 as the Long Term Support releases for users to migrate to and stay for a longer duration. In lieu of that, we have made 0.12.1\xa0 and 0.12.2 releases to assist users with stable release that comes packed with a lot of stability and bug fixes."]}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"community-events",children:"Community Events"}),"\n",(0,n.jsxs)(a.p,{children:["Apache Hudi is a global community and thankfully we live in a world today that empowers virtual collaboration and productivity. In addition to connecting virtually this year we have seen the Apache Hudi community gather at many events in person. Re",":Invent",", Data+AI Summit, Flink Forward, Alluxio Day, Data Council, PrestoCon, Confluent Current, DBT Coalesce, Cinco de Trino, Data Platform Summit, and many more."]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/Apache-Hudi-Conferences.png",alt:"drawing",style:{width:"80%",display:"block",marginLeft:"auto",marginRight:"auto"}}),"\n",(0,n.jsx)(a.p,{children:"You don\u2019t have to travel far to meet and collaborate with the Hudi community. We hold monthly virtual meetups, weekly office hours, and there are plenty of friendly faces on Hudi Slack who like to talk shop. Join us via Zoom for the next Hudi meetup!"}),"\n",(0,n.jsx)(a.h2,{id:"community-content",children:"Community Content"}),"\n",(0,n.jsx)(a.p,{children:"A wide diversity of organizations around the globe use Apache Hudi as the foundation of their production data platforms. Over 800+ organizations have engaged with Hudi (up 60% YoY) Here are a few highlights of content written by the community sharing their experiences, designs, and best practices:"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://aws.amazon.com/blogs/big-data/part-1-build-your-apache-hudi-data-lake-on-aws-using-amazon-emr/",children:"Build your Hudi data lake on AWS"})," - Suthan Phillips and Dylan Qu from AWS"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://www.youtube.com/playlist?list=PLL2hlSFBmWwwbMpcyMjYuRn8cN99gFSY6",children:"Soumil Shah Hudi Youtube Playlist"})," - Soumil Shah from JobTarget"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://medium.com/walmartglobaltech/implementation-of-scd-2-slowly-changing-dimension-with-apache-hudi-465e0eb94a5",children:"SCD-2 with Apache Hudi"})," - Jayasheel Kalgal from Walmart"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/blog/apache-hudi-vs-delta-lake-vs-apache-iceberg-lakehouse-feature-comparison",children:"Hudi vs Delta vs Iceberg comparisons"})," - Kyle Weller from Onehouse"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://aws.amazon.com/blogs/big-data/how-nerdwallet-uses-aws-and-apache-hudi-to-build-a-serverless-real-time-analytics-platform/",children:"Serverless, real-time analytics platform"})," - Kevin Chun from NerdWallet"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2022/07/11/build-open-lakehouse-using-apache-hudi-and-dbt/",children:"DBT and Hudi to Build Open Lakehouse"})," - Vinoth Govindarajan from Apple"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://www.onehouse.ai/blog/apache-hudi-vs-delta-lake-transparent-tpc-ds-lakehouse-performance-benchmarks",children:"TPC-DS Benchmarks Hudi vs Delta Lake"})," - Alexey Kudinkin from Onehouse"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://blogs.halodoc.io/key-learnings-on-using-apache-hudi-in-building-lakehouse-architecture-halodoc/",children:"Key Learnings Using Hudi building a Lakehouse"})," - Jitendra Shah from Halodoc"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://aws.amazon.com/blogs/architecture/insights-for-ctos-part-3-growing-your-business-with-modern-data-capabilities/",children:"Growing your business with modern data capabilities"})," - Jonathan Hwang from Zendesk"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://aws.amazon.com/blogs/big-data/create-a-low-latency-source-to-data-lake-pipeline-using-amazon-msk-connect-apache-flink-and-apache-hudi/",children:"Low-latency data lake using MSK, Flink, and Hudi"})," - Ali Alemi from AWS"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://robinhood.engineering/author-balaji-varadarajan-e3f496815ebf",children:"Fresher data lakes on AWS S3"})," - Balaji Varadarajan from Robinhood"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://www.youtube.com/watch?v=ZamXiT9aqs8",children:"Experiences with Hudi from Uber meetup"})," - Sam Guleff from Walmart and Vinay Patil from Disney+ Hotstar"]}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"what-to-look-for-in-2023",children:"What to look for in 2023"}),"\n",(0,n.jsxs)(a.p,{children:["Thanks to the strength of the community, Apache Hudi has a bright future for 2023. Check out ",(0,n.jsx)(a.a,{href:"https://youtu.be/9LPSdd-AS8E?t=2090",children:"this recording"})," from our Re",":Invent"," meetup where Vinoth Chandar talks about exciting new features to expect in 2023."]}),"\n",(0,n.jsx)(a.p,{children:"0.13.0 will be the next major release, with a package of exciting new features. Here are a few highlights:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://cwiki.apache.org/confluence/display/HUDI/RFC-08++Record+level+indexing+mechanisms+for+Hudi+datasets",children:"Record-key-based index"})," to speed up the lookup of records for UUID-based updates and deletes, well tested with 10+ TB index data for hundreds of billions of records at Uber;"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/rfc/rfc-42/rfc-42.md",children:"Consistent Hashing Index"})," with dynamically-sized buckets to achieve fast upsert performance with no data skew among file groups compared to existing ",(0,n.jsx)(a.a,{href:"https://cwiki.apache.org/confluence/display/HUDI/RFC+-+29%3A+Hash+Index",children:"Bucket Index"}),";"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/rfc/rfc-51/rfc-51.md",children:"New CDC format"})," with Debezium-like database change logs to provide before and after image and operation field for streaming changes from Hudi tables, friendly to engines like Flink;"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/rfc/rfc-46/rfc-46.md",children:"New Record Merge API"})," to support engine-specific record representation for more efficient writes;"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/rfc/rfc-56/rfc-56.md",children:"Early detection of conflicts"})," among concurrent writers to give back compute resources proactively."]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["The long-term vision of Apache Hudi is to make streaming data lake the mainstream, achieving sub-minute commit SLAs with stellar query performance and incremental ETLs.\xa0 We plan to harden the indexing subsystem with ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/pull/7080",children:"Table APIs"})," for easy integration with query engines and access to Hudi metadata and indexes, ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/pull/7235",children:"Indexing Functions"})," and ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/rfc/rfc-60/rfc-60.md",children:"a Federated Storage Layer"})," to eliminate the notion of partitions and reduce I/O, and new ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/pull/5370",children:"secondary indexes"}),".\xa0 To realize fast queries, we will provide an option of a standalone ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/pull/4718",children:"MetaServer"})," serving Hudi metadata to plan queries in milliseconds and a ",(0,n.jsx)(a.a,{href:"https://docs.google.com/presentation/d/1QBgLw11TM2Qf1KUESofGrQDb63EuggNCpPaxc82Kldo/edit#slide=id.gf7e0551254_0_5",children:"Hudi-aware lake cache"})," that speeds up the read performance of MOR tables along with fast writes for updates.\xa0 Incremental and streaming SQL will be enhanced in Spark and Flink.\xa0 For Hudi on Flink, we plan to make the multi-modal indexing production-ready, bring read and write compatibility between Flink and Spark engines, and harden the streaming capabilities, including CDC, streaming ETL semantics, pre-aggregation models and materialized views."]}),"\n",(0,n.jsxs)(a.p,{children:["Check out ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/roadmap",children:"Hudi Roadmap"})," for more to come in 2023!"]}),"\n",(0,n.jsx)(a.p,{children:"If you haven't tried Apache Hudi yet, 2023 is your year! Here are a few useful links to help you get started:"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/overview",children:"Apache Hudi Docs"})}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsx)(r.A,{title:"Hudi Slack Channel"}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.a,{href:"https://hudi.apache.org/community/office_hours",children:"Hudi Weekly Office Hours"})," and ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/community/syncs#monthly-community-call",children:"Monthly Meetup"})]}),"\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.a,{href:"https://hudi.apache.org/contribute/how-to-contribute",children:"Contributor Guide"})}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["If you enjoyed Hudi in 2022 don't forget to give it a little star on ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/",children:"Github"})," \u2b50"]})]})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h(e)}},87388:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(39464),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi: From Zero To One (6/10)",excerpt:"Demystify clustering and space-filling curves",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2023-11-13-Apache-Hudi-From-Zero-To-One-blog-6.png",tags:["hudi","spark","blog","course","tutorial","datumagic","data lake","lakehouse","apache hudi","apache spark"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blog.datumagic.ai/p/apache-hudi-from-zero-to-one-610",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},87431:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(15188),n=t(74848),s=t(28453),r=t(9230);const o={title:"Demystifying Copy-on-Write in Apache Hudi: Understanding Read and Write Operations",excerpt:"COW Overview",author:"Eswaramoorthy P",category:"blog",image:"/assets/images/blog/2023-09-10-Demystifying-Copy-on-Write-in-Apache-Hudi-Understanding-Read-and-Write-Operations.png",tags:["reads","medium","blog","apache hudi","writes","cow"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/walmartglobaltech/demystifying-copy-on-write-in-apache-hudi-understanding-read-and-write-operations-3aa274017884",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},87459:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(37183),n=t(74848),s=t(28453),r=t(9230);const o={title:"Lakehouse Architecture - Apache Hudi and Apache Iceberg",author:"beCloudReady",category:"blog",image:"/assets/images/blog/2025-07-02-Lakehouse-Architecture-apache-hudi-and-apache-iceberg.png",tags:["blog","Apache Hudi","Apache Iceberg","Lakehouse","use-case","det"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.linkedin.com/pulse/lakehouse-architecture-apache-hudi-iceberg-becloudready-4b1ac/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},87504:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(15799),n=t(74848),s=t(28453);const r={title:"Record Mergers in Apache Hudi",excerpt:"Explain need for record mergers in Apache Hudi and implemenation details",author:"Aditya Goenka",category:"blog",image:"/assets/images/blog/2025-03-03-record-mergers-in-apache-hudi.png",tags:["Data Lake","Data Lakehouse","Apache Hudi","Record Mergers","Record payloads","Late Arriving Data"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"The Challenge of Unordered Streams of Events",id:"the-challenge-of-unordered-streams-of-events",level:2},{value:"Data Integrity",id:"data-integrity",level:4},{value:"Complexity in Processing",id:"complexity-in-processing",level:4},{value:"What are Record Mergers",id:"what-are-record-mergers",level:2},{value:"1. Merging input data before writing : Combining Change Records During Writes",id:"1-merging-input-data-before-writing--combining-change-records-during-writes",level:4},{value:"2. Merging Final Change Record in CoW (Copy-on-Write) Tables: Applying Changes to Existing Records",id:"2-merging-final-change-record-in-cow-copy-on-write-tables-applying-changes-to-existing-records",level:4},{value:"3. Compaction Merge in MoR (Merge-on-Read) Tables : Merging Log Files with Base Files",id:"3-compaction-merge-in-mor-merge-on-read-tables--merging-log-files-with-base-files",level:4},{value:"4. Query-Time Merge: Merging Log Files with Base Files in MoR (Merge-on-Read) Tables",id:"4-query-time-merge-merging-log-files-with-base-files-in-mor-merge-on-read-tables",level:4},{value:"Implementation",id:"implementation",level:2},{value:"1. COMMIT_TIME_ORDERING",id:"1-commit_time_ordering",level:4},{value:"2. EVENT_TIME_ORDERING (DEFAULT)",id:"2-event_time_ordering-default",level:4},{value:"3. CUSTOM",id:"3-custom",level:4},{value:"Record Payloads",id:"record-payloads",level:2},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const a={a:"a",code:"code",h2:"h2",h4:"h4",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.h2,{id:"the-challenge-of-unordered-streams-of-events",children:"The Challenge of Unordered Streams of Events"}),"\n",(0,n.jsx)(a.p,{children:"One of the primary challenges associated with streaming workloads is the unordered nature of incoming events. In a typical streaming scenario, events can arrive out of sequence due to network latency, processing delays, or other factors. With the increasing volume and velocity of data being ingested from various sources\u2014especially in mobile applications and IoT platforms\u2014data processing frameworks must be equipped to handle mutations (i.e., changes to records) and out-of-order events.\nTraditional data storage systems and file formats, such as those optimized for batch processing, often struggle to manage these scenarios effectively. Hudi steps in with features specifically designed to handle such challenges.\nWhen events or changes to a record arrive at different times, they may not be in the same order in which they were originally generated. For example, in a smart city traffic monitoring system, sensors may report vehicle speeds at various intersections in real-time. However, due to network issues or delays, some sensor data might arrive later than others, possibly out of order. To handle this, the system needs to merge the new incoming data with existing records efficiently. Just like how Hudi\u2019s merge modes control the merging of records with the same key in a storage system, ensuring consistency and accuracy, it ensures that the final traffic data reflects the correct event times, even when some data arrives with a delay. These merge modes help maintain a consistent, deterministic result under heavy load, making sure that late data updates the right records without causing inconsistencies.\nThis can lead to several issues:"}),"\n",(0,n.jsx)(a.h4,{id:"data-integrity",children:"Data Integrity"}),"\n",(0,n.jsx)(a.p,{children:"When events are processed out of order, it can result in incorrect or inconsistent data states. For example, if an event representing a transaction is processed before the event that indicates the account balance, the resulting data may not accurately reflect the true state of the system."}),"\n",(0,n.jsx)(a.h4,{id:"complexity-in-processing",children:"Complexity in Processing"}),"\n",(0,n.jsx)(a.p,{children:"Handling unordered events often requires additional logic to ensure that data is processed in the correct sequence. This can complicate the data pipeline and increase the likelihood of errors."}),"\n",(0,n.jsx)(a.h2,{id:"what-are-record-mergers",children:"What are Record Mergers"}),"\n",(0,n.jsx)(a.p,{children:"With the new api introduced with version 1.0.0, Hudi supports three primary merge modes, each suited to different stages of data processing: writing, compaction, and querying.\n4 places/points of data processing [Subheader]"}),"\n",(0,n.jsx)(a.h4,{id:"1-merging-input-data-before-writing--combining-change-records-during-writes",children:"1. Merging input data before writing : Combining Change Records During Writes"}),"\n",(0,n.jsx)(a.p,{children:"When new data arrives for an existing record, Hudi performs deduplication on the input dataset. This process involves combining multiple change records for the same record key before the write phase. This is an optimization that also helps reduce the number of records written to the log files (in case of MOR). By merging changes upfront, Hudi reduces unnecessary records, improving the efficiency of both query and write operations.\nThis step is crucial for handling stream data in real-time, where changes may arrive rapidly, and ensuring that only the final version of the record is written into the system. Normally these out of order events come together commonly in the same batch,  With processing engines like spark, which deals with micro-batches, merging the input changes helps in reduces the number of records which needs to be written."}),"\n",(0,n.jsx)(a.h4,{id:"2-merging-final-change-record-in-cow-copy-on-write-tables-applying-changes-to-existing-records",children:"2. Merging Final Change Record in CoW (Copy-on-Write) Tables: Applying Changes to Existing Records"}),"\n",(0,n.jsx)(a.p,{children:"In Copy-on-Write (CoW) tables, changes are applied by creating new file versions for the records. When an update, partial update, or delete operation occurs, Hudi will merge this final change with the existing record in the storage. The merge mode controls how these updates are applied, ensuring that only the most recent changes are reflected and the table\u2019s data remains consistent.\nThis is especially important in CoW tables, as they preserve immutability of historical data by writing new versions of the records instead of overwriting the existing data. The merge mode ensures that the new version of the record is consistent with all previous changes."}),"\n",(0,n.jsx)(a.h4,{id:"3-compaction-merge-in-mor-merge-on-read-tables--merging-log-files-with-base-files",children:"3. Compaction Merge in MoR (Merge-on-Read) Tables : Merging Log Files with Base Files"}),"\n",(0,n.jsx)(a.p,{children:"Hudi uses a concept of log files (delta logs) and base files (original data). As changes to records accumulate over time, Hudi\u2019s compaction service merges the change records stored in the log files with the base files to keep the data consistent and query-optimized. The merge mode defines how these log records are merged with base files during the compaction process.\nCompaction helps maintain storage efficiency and ensures that queries run faster by reducing the number of small log files that might need to be read."}),"\n",(0,n.jsx)(a.h4,{id:"4-query-time-merge-merging-log-files-with-base-files-in-mor-merge-on-read-tables",children:"4. Query-Time Merge: Merging Log Files with Base Files in MoR (Merge-on-Read) Tables"}),"\n",(0,n.jsx)(a.p,{children:"In Merge-on-Read (MoR) tables, the data is stored in both log files and base files. When a query is executed, Hudi merges the change records in the log files with the base files based on the merge mode. The merge operation occurs at query time to provide the final, consistent view of the data.\nBy merging records at query time, Hudi ensures that queries reflect the most recent changes while maintaining query performance."}),"\n",(0,n.jsx)(a.h2,{id:"implementation",children:"Implementation"}),"\n",(0,n.jsx)(a.p,{children:"In common scenarios, the input data contains a field that can be used to identify the latest record. Typically, tables have fields like updated_at or other ordering columns. If no such column is present in the input, we are limited to relying on the incoming order."}),"\n",(0,n.jsxs)(a.p,{children:["After the release of Hudi 1.0.0, a new configuration, ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/configurations/#hoodierecordmergemode",children:"hoodie.record.merge.mode"})," was introduced to define the merge modes responsible for handling record updates. These merge modes dictate how records with the same key are processed at different stages of the pipeline, from data ingestion to query results.\nIt can have the following three values:"]}),"\n",(0,n.jsx)(a.h4,{id:"1-commit_time_ordering",children:"1. COMMIT_TIME_ORDERING"}),"\n",(0,n.jsx)(a.p,{children:"This merge mode is used when no field is available in the input data to explicitly determine which record is the latest. The system will rely on the order of ingestion (commit time) to determine the order of records. Hudi expects records to arrive in strict order of their commits. So, the most recent record (in terms of ingestion time) is assumed to be the latest version of the record. This mode is typically used when there is no dedicated column like updated_at, timestamp, or versioning field that can indicate the order of the records.\nThe merging logic here simply picks the latest write based on the ingestion order (commit time). In a way, it's equivalent to overwriting semantics where only the most recent record is considered.\nExample -"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"SET hoodie.spark.sql.insert.into.operation=upsert;\nCREATE TABLE hudi_table (\n    ts BIGINT,\n    uuid STRING,\n    rider STRING,\n    driver STRING,\n    fare DOUBLE,\n    city STRING\n) USING HUDI TBLPROPERTIES (primaryKey = 'uuid', hoodie.record.merge.mode='COMMIT_TIME_ORDERING');\n\nINSERT INTO hudi_table\nVALUES\n(3,'334e26e9-8355-45cc-97c6-c31daf0df330','rider-A','driver-K',19.10,'san_francisco'),\n(2,'334e26e9-8355-45cc-97c6-c31daf0df330','rider-C','driver-M',27.70 ,'san_francisco');\n\nselect * from hudi_table;\n-- Result - 20250106162911278\t20250106162911278_0_0\t334e26e9-8355-45cc-97c6-c31daf0df330\t\t08218473-f72a-480d-90e6-c6764f062e5c-0_0-43-47_20250106162911278.parquet\t1695091554788\t334e26e9-8355-45cc-97c6-c31daf0df330\trider-C\tdriver-M\t27.7\tsan_francisco\n\nINSERT INTO hudi_table\nVALUES\n(1,'334e26e9-8355-45cc-97c6-c31daf0df330','rider-D','driver-K',19.10,'san_francisco');\n\nselect * from hudi_table;\n-- Result - 20250106163449812\t20250106163449812_0_0\t334e26e9-8355-45cc-97c6-c31daf0df330\t\t08218473-f72a-480d-90e6-c6764f062e5c-0_0-71-68_20250106163449812.parquet\t1\t334e26e9-8355-45cc-97c6-c31daf0df330\trider-D\tdriver-K\t19.1\tsan_francisco\n"})}),"\n",(0,n.jsx)(a.p,{children:"In the example above, we created the table using the COMMIT_TIME_ORDERING merge mode. When using this mode, there is no need to specify a precombine or ordering field.\nDuring the first insert, two records with the same record key are provided. The system will deduplicate them and keep the record that is processed later.\nIn the second insert, a new record with the same record key is inserted. As expected, the table is updated with the new record because it is committed later, regardless of the values in any of the fields."}),"\n",(0,n.jsx)(a.h4,{id:"2-event_time_ordering-default",children:"2. EVENT_TIME_ORDERING (DEFAULT)"}),"\n",(0,n.jsx)(a.p,{children:'This merge mode is used when you do have a field in the input data that can be used to determine the order of events (such as a timestamp field like updated_at or a version number). If your records contain a field that can be used to track when the record was last updated (e.g., updated_at, last_modified, or a sequence number), Hudi will use this field to determine which record is the latest.\nIn this case, Hudi does not rely on the ingestion order but instead uses the value of the ordering field (updated_at, for example) to decide the correct record.\nThis approach is ideal when you have temporal or event-driven data, and you want to maintain the "latest" record according to an event timestamp.\nExample -'}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"DROP TABLE hudi_table;\nSET hoodie.spark.sql.insert.into.operation=upsert;\n\nCREATE TABLE hudi_table (\n    ts BIGINT,\n    uuid STRING,\n    rider STRING,\n    driver STRING,\n    fare DOUBLE,\n    city STRING\n) USING HUDI TBLPROPERTIES (primaryKey = 'uuid',preCombineField = 'ts', hoodie.record.merge.mode='EVENT_TIME_ORDERING');\n\nINSERT INTO hudi_table\nVALUES\n(3,'334e26e9-8355-45cc-97c6-c31daf0df330','rider-A','driver-K',19.10,'san_francisco'),\n(2,'334e26e9-8355-45cc-97c6-c31daf0df330','rider-C','driver-M',27.70 ,'san_francisco');\n\nselect * from hudi_table;\n-- Result - 20250106165902806\t20250106165902806_0_0\t334e26e9-8355-45cc-97c6-c31daf0df330\t\t568ce7bc-9b71-4e15-b557-cbaeb5b4d2ea-0_0-56-57_20250106165902806.parquet\t3\t334e26e9-8355-45cc-97c6-c31daf0df330\trider-A\tdriver-K\t19.1\tsan_francisco\n\nINSERT INTO hudi_table\nVALUES\n(1,'334e26e9-8355-45cc-97c6-c31daf0df330','rider-D','driver-K',18.00,'san_francisco');\n\nselect * from hudi_table;\n-- Result - 20250106165902806\t20250106165902806_0_0\t334e26e9-8355-45cc-97c6-c31daf0df330\t\t568ce7bc-9b71-4e15-b557-cbaeb5b4d2ea-0_0-84-78_20250106165918731.parquet\t3\t334e26e9-8355-45cc-97c6-c31daf0df330\trider-A\tdriver-K\t19.1\tsan_francisco\n"})}),"\n",(0,n.jsx)(a.p,{children:"In the example above, we created the table using the EVENT_TIME_ORDERING merge mode. When using this mode, we need to specify the precombineField. In this case we are specifying ts as the precombineField.\nDuring the first insert, two records with the same record key are provided. The system will deduplicate them and keep the record that is processed later.\nIn the second insert, a new record with the same record key is inserted. As expected, the table is updated with the new record because it is committed later, regardless of the values in any of the fields."}),"\n",(0,n.jsx)(a.h4,{id:"3-custom",children:"3. CUSTOM"}),"\n",(0,n.jsxs)(a.p,{children:["For more complex use-case sometimes prior discussed merging modes won\u2019t work. We may need to implement a use-case specific merging logic.\nThe details for the implementation is provided here  - ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/record_merger/#custom",children:"https://hudi.apache.org/docs/record_merger/#custom"})]}),"\n",(0,n.jsx)(a.h2,{id:"record-payloads",children:"Record Payloads"}),"\n",(0,n.jsxs)(a.p,{children:["Pre 1.0.0, Hudi uses the legacy Record Payload API, Please refer to the ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/record_merger/#record-payloads",children:"Record Payloads"})," section to know about the implementation and some of the existing record payloads."]}),"\n",(0,n.jsxs)(a.p,{children:["Along with the existing payloads, Hudi provides flexibility to implement the custom record payload by implementing the ",(0,n.jsx)(a.a,{href:"https://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordPayload.java",children:"HoodieRecordPayload"})," interface"]}),"\n",(0,n.jsx)(a.p,{children:"The following example demonstrates the use of Record Payload, which achieves a similar outcome to what EVENT_TIME_ORDERING does. We\u2019ve used the same example as above to illustrate how this functionality works."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-sql",children:"DROP TABLE hudi_table;\nSET hoodie.spark.sql.insert.into.operation=upsert;\n\nCREATE TABLE hudi_table (\n    ts BIGINT,\n    uuid STRING,\n    rider STRING,\n    driver STRING,\n    fare DOUBLE,\n    city STRING\n) USING HUDI TBLPROPERTIES (primaryKey = 'uuid',preCombineField = 'ts', hoodie.datasource.write.payload.class='org.apache.hudi.common.model.DefaultHoodieRecordPayload');\n\nINSERT INTO hudi_table\nVALUES\n(3,'334e26e9-8355-45cc-97c6-c31daf0df330','rider-A','driver-K',19.10,'san_francisco'),\n(2,'334e26e9-8355-45cc-97c6-c31daf0df330','rider-C','driver-M',27.70 ,'san_francisco');\n\nselect * from hudi_table;\n-- Result - 20250203164444124\t20250203164444124_0_0\t334e26e9-8355-45cc-97c6-c31daf0df330\t\t4549ed8e-0346-4d59-8878-9e047fb6c651-0_0-14-17_20250203164444124.parquet\t3\t334e26e9-8355-45cc-97c6-c31daf0df330\trider-A\tdriver-K\t19.1\tsan_francisco\n\n\nINSERT INTO hudi_table\nVALUES\n(1,'334e26e9-8355-45cc-97c6-c31daf0df330','rider-D','driver-K',18.00,'san_francisco');\n\nselect * from hudi_table;\n-- Result - 20250203164444124\t20250203164444124_0_0\t334e26e9-8355-45cc-97c6-c31daf0df330\t\t4549ed8e-0346-4d59-8878-9e047fb6c651-0_0-53-51_20250203164537068.parquet\t3\t334e26e9-8355-45cc-97c6-c31daf0df330\trider-A\tdriver-K\t19.1\tsan_francisco\n"})}),"\n",(0,n.jsx)(a.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,n.jsx)(a.p,{children:"In conclusion, managing late-arriving and out-of-order data is a critical challenge in modern data processing systems, especially when dealing with large-scale, real-time data pipelines. Tools like Hudi provide powerful merge modes that ensure data consistency, accuracy, and efficiency by handling record updates intelligently across different stages of the pipeline. Whether you're working with streaming data, IoT sensors, or social media posts, understanding how to configure and use these merge modes can greatly improve the performance and reliability of your data storage and query processes. By leveraging the right merge strategy, you can ensure that your system remains robust, even under heavy load and with delayed data, ultimately enabling better decision-making and insights from your data."})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},87878:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig2-7177a573864028788685add3456491ec.png"},88671:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/01/27/Introducing-native-support-for-Apache-Hudi-Delta-Lake-Apache-Iceberg-on-AWS-Glue-for-Apache-Spark","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-01-27-Introducing-native-support-for-Apache-Hudi-Delta-Lake-Apache-Iceberg-on-AWS-Glue-for-Apache-Spark.mdx","source":"@site/blog/2023-01-27-Introducing-native-support-for-Apache-Hudi-Delta-Lake-Apache-Iceberg-on-AWS-Glue-for-Apache-Spark.mdx","title":"Introducing native support for Apache Hudi, Delta Lake, and Apache Iceberg on AWS Glue for Apache Spark, Part 1: Getting Started","description":"Redirecting... please wait!!","date":"2023-01-27T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.2,"hasTruncateMarker":false,"authors":[{"name":"Akira Ajisaka, Noritaka Sekiyama and Savio Dsouza","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Introducing native support for Apache Hudi, Delta Lake, and Apache Iceberg on AWS Glue for Apache Spark, Part 1: Getting Started","authors":[{"name":"Akira Ajisaka, Noritaka Sekiyama and Savio Dsouza"}],"category":"blog","image":"/assets/images/blog/0127-introducing-native-support-hudi-aws-glue.png","tags":["blog","amazon"]},"unlisted":false,"prevItem":{"title":"Automate schema evolution at scale with Apache Hudi in AWS Glue | Amazon Web Services","permalink":"/blog/2023/02/07/automate-schema-evolution-at-scale-with-apache-hudi-in-aws-glue"},"nextItem":{"title":"Apache Hudi vs Delta Lake vs Apache Iceberg - Lakehouse Feature Comparison","permalink":"/blog/2023/01/11/Apache-Hudi-vs-Delta-Lake-vs-Apache-Iceberg-Lakehouse-Feature-Comparison"}}')},88710:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/01/09/apache-iceberg-vs-delta-lake-vs-apache-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-01-09-apache-iceberg-vs-delta-lake-vs-apache-hudi.mdx","source":"@site/blog/2025-01-09-apache-iceberg-vs-delta-lake-vs-apache-hudi.mdx","title":"Apache Iceberg vs Delta Lake vs Apache Hudi","description":"Redirecting... please wait!!","date":"2025-01-09T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"apache iceberg","permalink":"/blog/tags/apache-iceberg"},{"inline":true,"label":"delta lake","permalink":"/blog/tags/delta-lake"},{"inline":true,"label":"comparison","permalink":"/blog/tags/comparison"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"AlgoDays","key":null,"page":null}],"frontMatter":{"title":"Apache Iceberg vs Delta Lake vs Apache Hudi","author":"AlgoDays","category":"blog","image":"/assets/images/blog/2025-01-09-apache-iceberg-vs-delta-lake-vs-apache-hudi.jpeg","tags":["blog","apache hudi","apache iceberg","delta lake","comparison","medium"]},"unlisted":false,"prevItem":{"title":"Out of the box Key Generators in Apache Hudi","permalink":"/blog/2025/01/15/outofbox-key-generators-in-hudi"},"nextItem":{"title":"The Future of Data Lakehouses: A Fireside Chat with Vinoth Chandar - Founder CEO Onehouse & PMC Chair of Apache Hudi","permalink":"/blog/2025/01/08/the-future-of-data-lakehouses-a-fireside"}}')},88789:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(29287),n=t(74848),s=t(28453),r=t(9230);const o={title:"Open Source Data Lake Table Formats: Evaluating Current Interest and Rate of Adoption",authors:[{name:"Gary Stafford"}],category:"blog",image:"/assets/images/blog/2022-02-12-open-source-data-lake-formats.png",tags:["blog","datalake","comparison","community","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://garystafford.medium.com/data-lake-table-formats-interest-and-adoption-rate-40817b87be9e",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},89355:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/real_time_view-101973743bcb2dccb6e7cd4aa411aa73.png"},89366:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/05/07/learn-how-read-hudi-data-aws-glue-ray-using-daft-spark","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-05-07-learn-how-read-hudi-data-aws-glue-ray-using-daft-spark.mdx","source":"@site/blog/2024-05-07-learn-how-read-hudi-data-aws-glue-ray-using-daft-spark.mdx","title":"Learn how to read Hudi data with AWS Glue Ray using Daft (No Spark)","description":"Redirecting... please wait!!","date":"2024-05-07T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"aws glue","permalink":"/blog/tags/aws-glue"},{"inline":true,"label":"ray","permalink":"/blog/tags/ray"},{"inline":true,"label":"daft","permalink":"/blog/tags/daft"},{"inline":true,"label":"linkedin","permalink":"/blog/tags/linkedin"}],"readingTime":0.15,"hasTruncateMarker":false,"authors":[{"name":"Soumil Shah","key":null,"page":null}],"frontMatter":{"title":"Learn how to read Hudi data with AWS Glue Ray using Daft (No Spark)","author":"Soumil Shah","category":"blog","image":"/assets/images/blog/2024-05-07-learn-how-read-hudi-data-aws-glue-ray-using-daft-spark.png","tags":["blog","apache hudi","aws glue","ray","daft","linkedin"]},"unlisted":false,"prevItem":{"title":"Building Analytical Apps on the Lakehouse using Apache Hudi, Daft & Streamlit","permalink":"/blog/2024/05/10/building-analytical-apps-on-the-lakehouse-using-apache-hudi-daft-streamlit"},"nextItem":{"title":"How to Query Apache Hudi Tables with Python Using Daft: A Spark-Free Approach","permalink":"/blog/2024/05/02/how-query-apache-hudi-tables-python-using-daft-spark-free"}}')},89564:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(56338),n=t(74848),s=t(28453),r=t(9230);const o={title:"Amazon Athena now supports Apache Hudi 0.12.2",category:"blog",image:"/assets/images/blog/aws.jpg",tags:["blog","amazon"]},l=void 0,d={authorsImageUrls:[]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/about-aws/whats-new/2023/05/amazon-athena-apache-hudi/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},89746:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/03/05/Apache-Hudi-From-Zero-To-One-blog-9","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-03-05-Apache-Hudi-From-Zero-To-One-blog-9.mdx","source":"@site/blog/2024-03-05-Apache-Hudi-From-Zero-To-One-blog-9.mdx","title":"Apache Hudi: From Zero To One (9/10)","description":"Redirecting... please wait!!","date":"2024-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"spark","permalink":"/blog/tags/spark"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"course","permalink":"/blog/tags/course"},{"inline":true,"label":"tutorial","permalink":"/blog/tags/tutorial"},{"inline":true,"label":"datumagic","permalink":"/blog/tags/datumagic"},{"inline":true,"label":"data lake","permalink":"/blog/tags/data-lake"},{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"apache spark","permalink":"/blog/tags/apache-spark"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Shiyan Xu","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi: From Zero To One (9/10)","excerpt":"Hudi Streamer - a Swiss Army knife for ingestion","author":"Shiyan Xu","category":"blog","image":"/assets/images/blog/2024-03-05-Apache-Hudi-From-Zero-To-One-blog-9.png","tags":["hudi","spark","blog","course","tutorial","datumagic","data lake","lakehouse","apache hudi","apache spark"]},"unlisted":false,"prevItem":{"title":"Navigating the Future: The Evolutionary Journey of Upstox\u2019s Data Platform","permalink":"/blog/2024/03/10/navigating-the-future-the-evolutionary-journey-of-upstoxs-data-platform"},"nextItem":{"title":"Building Data Lakes on AWS with Kafka Connect, Debezium, Apicurio Registry, and Apache Hudi","permalink":"/blog/2024/02/27/Building-Data-Lakes-on-AWS-with-Kafka-Connect-Debezium-Apicurio-Registry-and-Apache-Hudi"}}')},89941:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/09/17/hudi-auto-gen-keys","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-09-17-hudi-auto-gen-keys.mdx","source":"@site/blog/2025-09-17-hudi-auto-gen-keys.mdx","title":"Automatic Record Key Generation in Apache Hudi","description":"In database systems, the primary key is a foundational design principle for managing data at the record level. Its function is to provide each record with a unique and stable logical identifier, which decouples the record\'s identity from its physical location on storage. While using direct physical address pointers (e.g., position inside a file being used as a key) can be convenient, the physical address can change when records are moved around within the table for things like clustering or z-ordering (called out here).","date":"2025-09-17T00:00:00.000Z","tags":[{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"record key generation","permalink":"/blog/tags/record-key-generation"},{"inline":true,"label":"database","permalink":"/blog/tags/database"},{"inline":true,"label":"data lakehouse","permalink":"/blog/tags/data-lakehouse"}],"readingTime":7.6,"hasTruncateMarker":false,"authors":[{"name":"Shiyan Xu","key":null,"page":null}],"frontMatter":{"title":"Automatic Record Key Generation in Apache Hudi","excerpt":"","author":"Shiyan Xu","category":"blog","image":"/assets/images/blog/2025-09-17-hudi-auto-gen-keys/2025-09-17-hudi-auto-gen-keys.fig2.jpg","tags":["hudi","record key generation","database","data lakehouse"]},"unlisted":false,"prevItem":{"title":"Real-Time Cloud Security Graphs with Apache Hudi and PuppyGraph","permalink":"/blog/2025/10/02/Real-Time-Cloud-Security-Graphs-Hudi+PuppyGraph"},"nextItem":{"title":"Building a RAG-based AI Recommender (2/2)","permalink":"/blog/2025/08/29/building-a-rag-based-ai-recommender-2"}}')},89988:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/01/09/introduction-to-apache-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-01-09-introduction-to-apache-hudi.mdx","source":"@site/blog/2024-01-09-introduction-to-apache-hudi.mdx","title":"Introduction to Apache Hudi","description":"Redirecting... please wait!!","date":"2024-01-09T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"},{"inline":true,"label":"beginner","permalink":"/blog/tags/beginner"},{"inline":true,"label":"apache spark","permalink":"/blog/tags/apache-spark"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Andrew Savchyns","key":null,"page":null}],"frontMatter":{"title":"Introduction to Apache Hudi","excerpt":"Introduction to Apache Hudi","author":"Andrew Savchyns","category":"blog","image":"/assets/images/blog/2024-01-09-introduction-to-apache-hudi.png","tags":["blog","apache hudi","medium","beginner","apache spark"]},"unlisted":false,"prevItem":{"title":"In-House Data Lake with CDC Processing, Hudi, Docker","permalink":"/blog/2024/01/11/In-House-Data-Lake-with-CDC-Processing-Hudi-Docker"},"nextItem":{"title":"Apache Hudi: From Zero To One (8/10)","permalink":"/blog/2024/01/05/Apache-Hudi-From-Zero-To-One-blog-8"}}')},90097:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2019/11/15/New-Insert-Update-Delete-Data-on-S3-with-Amazon-EMR-and-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2019-11-15-New-Insert-Update-Delete-Data-on-S3-with-Amazon-EMR-and-Apache-Hudi.mdx","source":"@site/blog/2019-11-15-New-Insert-Update-Delete-Data-on-S3-with-Amazon-EMR-and-Apache-Hudi.mdx","title":"New \u2013 Insert, Update, Delete Data on S3 with Amazon EMR and Apache Hudi","description":"Redirecting... please wait!!","date":"2019-11-15T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.15,"hasTruncateMarker":false,"authors":[{"name":"Danilo Poccia","socials":{},"key":null,"page":null}],"frontMatter":{"title":"New \u2013 Insert, Update, Delete Data on S3 with Amazon EMR and Apache Hudi","authors":[{"name":"Danilo Poccia"}],"category":"blog","image":"/assets/images/blog/aws.jpg","tags":["blog","amazon"]},"unlisted":false,"prevItem":{"title":"Delete support in Hudi","permalink":"/blog/2020/01/15/delete-support-in-hudi"},"nextItem":{"title":"Hudi On Hops","permalink":"/blog/2019/10/22/Hudi-On-Hops"}}')},90105:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/2024-11-19-automated-small-file-handling-process-676b9be484af36088162dfaf6a219a1f.png"},90167:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/07/10/building-a-rag-based-ai-recommender","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-07-10-building-a-rag-based-ai-recommender.mdx","source":"@site/blog/2025-07-10-building-a-rag-based-ai-recommender.mdx","title":"Building a RAG-based AI Recommender (Part 1/2)","description":"Redirecting... please wait!!","date":"2025-07-10T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"AI","permalink":"/blog/tags/ai"},{"inline":true,"label":"RAG","permalink":"/blog/tags/rag"},{"inline":true,"label":"Artificial Intelligence","permalink":"/blog/tags/artificial-intelligence"},{"inline":true,"label":"data lakehouse","permalink":"/blog/tags/data-lakehouse"},{"inline":true,"label":"Lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"datumagic","permalink":"/blog/tags/datumagic"}],"readingTime":0.11,"hasTruncateMarker":false,"authors":[{"name":"Shiyan Xu","key":null,"page":null}],"frontMatter":{"title":"Building a RAG-based AI Recommender (Part 1/2)","author":"Shiyan Xu","category":"blog","image":"/assets/images/blog/2025-07-10-building-a-rag-based-ai-recommender.png","tags":["blog","Apache Hudi","AI","RAG","Artificial Intelligence","data lakehouse","Lakehouse","use-case","datumagic"]},"unlisted":false,"prevItem":{"title":"How PayU built a secure enterprise AI assistant using Amazon Bedrock","permalink":"/blog/2025/07/15/PayU-built-a-secure-enterprise-AI-assistant"},"nextItem":{"title":"How Stifel built a modern data platform using AWS Glue and an event-driven domain architecture","permalink":"/blog/2025/07/07/how-stifel-built-a-modern-data-platform-using-aws-glue-and-an-event-driven-domain-architecture"}}')},90204:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2019/05/14/registering-dataset-to-hive","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2019-05-14-registering-dataset-to-hive.md","source":"@site/blog/2019-05-14-registering-dataset-to-hive.md","title":"Registering sample dataset to Hive via beeline","description":"Hudi hive sync tool typically handles registration of the dataset into Hive metastore. In case, there are issues with quickstart around this, following page shows commands that can be used to do this manually via beeline.","date":"2019-05-14T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":1.82,"hasTruncateMarker":true,"authors":[{"name":"vinoth","key":null,"page":null}],"frontMatter":{"title":"Registering sample dataset to Hive via beeline","excerpt":"How to manually register HUDI dataset into Hive using beeline","author":"vinoth","category":"blog","tags":["how-to","apache hudi"]},"unlisted":false,"prevItem":{"title":"Ingesting Database changes via Sqoop/Hudi","permalink":"/blog/2019/09/09/ingesting-database-changes"},"nextItem":{"title":"Big Batch vs Incremental Processing","permalink":"/blog/2019/03/07/batch-vs-incremental"}}')},90212:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/11/10/How-Hudl-built-a-cost-optimized-AWS-Glue-pipeline-with-Apache-Hudi-datasets","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-11-10-How-Hudl-built-a-cost-optimized-AWS-Glue-pipeline-with-Apache-Hudi-datasets.mdx","source":"@site/blog/2022-11-10-How-Hudl-built-a-cost-optimized-AWS-Glue-pipeline-with-Apache-Hudi-datasets.mdx","title":"How Hudl built a cost-optimized AWS Glue pipeline with Apache Hudi datasets","description":"Redirecting... please wait!!","date":"2022-11-10T00:00:00.000Z","tags":[{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"cost efficiency","permalink":"/blog/tags/cost-efficiency"},{"inline":true,"label":"incremental processing","permalink":"/blog/tags/incremental-processing"},{"inline":true,"label":"near real-time analytics","permalink":"/blog/tags/near-real-time-analytics"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.16,"hasTruncateMarker":false,"authors":[{"name":"Indira Balakrishnan","socials":{},"key":null,"page":null},{"name":"Ramzi Yassine","socials":{},"key":null,"page":null},{"name":"Swagat Kulkarni","socials":{},"key":null,"page":null}],"frontMatter":{"title":"How Hudl built a cost-optimized AWS Glue pipeline with Apache Hudi datasets","authors":[{"name":"Indira Balakrishnan"},{"name":"Ramzi Yassine"},{"name":"Swagat Kulkarni"}],"category":"blog","image":"/assets/images/blog/2022-11-10_How_to_build_a_cost_optimized_glue_pipeline_with_apache_hudi.png","tags":["use-case","cost efficiency","incremental processing","near real-time analytics","amazon"]},"unlisted":false,"prevItem":{"title":"Build your Apache Hudi data lake on AWS using Amazon EMR \u2013 Part 1","permalink":"/blog/2022/11/22/Build-your-Apache-Hudi-data-lake-on-AWS-using-Amazon-EMR-Part-1"},"nextItem":{"title":"Get started with Apache Hudi using AWS Glue by implementing key design concepts \u2013 Part 1","permalink":"/blog/2022/10/17/Get-started-with-Apache-Hudi-using-AWS"}}')},90282:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/2025-09-17-hudi-auto-gen-keys.fig2-760500605f2a1ecfa253caffaa013c4a.jpg"},90534:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/event-time-ordering-merge-mode-c8164e035840388bf4290fa81ac6262a.png"},90575:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>f,contentTitle:()=>b,default:()=>k,frontMatter:()=>m,metadata:()=>i,toc:()=>y});const i=JSON.parse('{"id":"blog","title":"Blogs","description":"Welcome to Apache Hudi blogs! Here you\'ll find the latest articles, tutorials, and updates from the Hudi community.","source":"@site/learn/blog.md","sourceDirName":".","slug":"/blog","permalink":"/learn/blog","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Blogs"},"sidebar":"learn","previous":{"title":"Tutorial Series","permalink":"/learn/tutorial-series"},"next":{"title":"Talks","permalink":"/learn/talks"}}');var n=t(74848),s=t(28453),r=t(96540),o=t(28774),l=t(86025),d=t(68234),c=t(77386);var h;const u=(h=t(58705)).keys().reduce((e,a,t)=>{try{const t=h(a);if(t&&(t.frontMatter||t.metadata))return[...e,{frontMatter:{...t.frontMatter||{}},metadata:{...t.metadata||{}},assets:{...t.assets||{}}}]}catch(i){console.warn("Error loading blog post:",a,i)}return e},[]).filter(e=>e.metadata&&e.metadata.title&&e.metadata.permalink).sort((e,a)=>{const t=e.metadata?.date?new Date(e.metadata.date).getTime():0;return(a.metadata?.date?new Date(a.metadata.date).getTime():0)-t}),p=10;function g(){const{withBaseUrl:e}=(0,l.hH)(),[a,t]=(0,r.useState)(1),i=Math.ceil(u.length/p),s=(a-1)*p,h=s+p,g=u.slice(s,h),m=e=>{t(e),window.scrollTo({top:0,behavior:"smooth"})};return(0,n.jsxs)("div",{className:c.A.container,children:[(0,n.jsx)("h2",{children:"All Blog Posts"}),(0,n.jsx)("div",{className:c.A.grid,children:g.map((a,t)=>{const{frontMatter:i,assets:s,metadata:r}=a,{date:l,title:h,authors:u,permalink:p,description:g}=r||{},m=s?.image??i?.image??"/assets/images/hudi.png";if(!h||!p)return null;const b=l?new Date(l):null,f=b?b.toLocaleDateString("en-US",{year:"numeric",month:"long",day:"numeric"}):"";return(0,n.jsx)("article",{className:c.A.card,children:(0,n.jsxs)(o.A,{to:p,className:c.A.link,target:"_blank",rel:"noopener noreferrer",children:[(0,n.jsx)("div",{className:c.A.imageWrapper,children:(0,n.jsx)("img",{src:e(m,{absolute:!0}),alt:h,className:c.A.image})}),(0,n.jsxs)("div",{className:c.A.content,children:[(0,n.jsxs)("div",{className:c.A.meta,children:[(0,n.jsx)(d.A,{authors:u,className:c.A.authorName,withLink:!1}),(0,n.jsx)("span",{className:c.A.date,children:f})]}),(0,n.jsx)("h3",{className:c.A.title,children:h})]})]})},t)})}),i>1&&(0,n.jsxs)("nav",{className:c.A.pagination,"aria-label":"Blog pagination",children:[(0,n.jsx)("button",{className:c.A.paginationButton,onClick:()=>m(a-1),disabled:1===a,"aria-label":"Previous page",children:"Previous"}),(0,n.jsx)("div",{className:c.A.paginationNumbers,children:(()=>{const e=[];let t=Math.max(1,a-Math.floor(2.5)),n=Math.min(i,t+5-1);n-t<4&&(t=Math.max(1,n-5+1));for(let a=t;a<=n;a++)e.push(a);return e})().map(e=>(0,n.jsx)("button",{className:`${c.A.paginationNumber} ${a===e?c.A.active:""}`,onClick:()=>m(e),"aria-label":`Page ${e}`,"aria-current":a===e?"page":void 0,children:e},e))}),(0,n.jsx)("button",{className:c.A.paginationButton,onClick:()=>m(a+1),disabled:a===i,"aria-label":"Next page",children:"Next"})]}),(0,n.jsxs)("div",{className:c.A.paginationInfo,children:["Showing ",s+1,"-",Math.min(h,u.length)," of ",u.length," posts"]})]})}const m={title:"Blogs"},b="Blogs",f={},y=[];function x(e){const a={h1:"h1",header:"header",p:"p",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.header,{children:(0,n.jsx)(a.h1,{id:"blogs",children:"Blogs"})}),"\n",(0,n.jsx)(a.p,{children:"Welcome to Apache Hudi blogs! Here you'll find the latest articles, tutorials, and updates from the Hudi community."}),"\n",(0,n.jsx)(g,{})]})}function k(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(x,{...e})}):x(e)}},90874:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(57144),n=t(74848),s=t(28453),r=t(9230);const o={title:"Building a RAG-based AI Recommender (2/2)",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2025-08-29-building-a-rag-based-ai-recommender-2.jpg",tags:["blog","Apache Hudi","AI","RAG","Artificial Intelligence","data lakehouse","Lakehouse","use-case","datumagic"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blog.datumagic.ai/p/building-a-rag-based-ai-recommender-147",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},91112:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/06/30/uber-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-06-30-uber-hudi.md","source":"@site/blog/2025-06-30-uber-hudi.md","title":"Scaling Complex Data Workflows at Uber Using Apache Hudi","description":"Uber\u2019s trip and order collection pipelines grew highly complex, with long runtimes, massive DAGs, and rigid SQL logic that hampered scalability and maintainability. By adopting Apache Hudi, Uber re-architected these pipelines to enable incremental processing, custom merge behavior, and rule-based functional transformations. This reduced runtime from 20 hours to 4 hours, improved test coverage to 95%, cut costs by 60%, and delivered a composable, explainable, and scalable data workflow architecture.","date":"2025-06-30T00:00:00.000Z","tags":[{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"Uber","permalink":"/blog/tags/uber"},{"inline":true,"label":"Community","permalink":"/blog/tags/community"}],"readingTime":6.78,"hasTruncateMarker":false,"authors":[{"name":"Ankit Shrivastava in collaboration with Dipankar","key":null,"page":null}],"frontMatter":{"title":"Scaling Complex Data Workflows at Uber Using Apache Hudi","excerpt":"How Uber\'s Core Services Data Engineering team supports a wide range of use cases with Apache Hudi","author":"Ankit Shrivastava in collaboration with Dipankar","category":"blog","image":"/assets/images/blog/uber1200x600.jpg","tags":["Apache Hudi","Uber","Community"]},"unlisted":false,"prevItem":{"title":"Lakehouse Architecture - Apache Hudi and Apache Iceberg","permalink":"/blog/2025/07/02/Lakehouse-Architecture-apache-hudi-and-apache-iceberg"},"nextItem":{"title":"Apache Hudi does XYZ (1/10): File pruning with multi-modal index","permalink":"/blog/2025/06/16/Apache-Hudi-does-XYZ-110"}}')},91242:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/03/17/introduction-to-apache-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-03-17-introduction-to-apache-hudi.mdx","source":"@site/blog/2023-03-17-introduction-to-apache-hudi.mdx","title":"Introduction to Apache Hudi","description":"Redirecting... please wait!!","date":"2023-03-17T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"guide","permalink":"/blog/tags/guide"},{"inline":true,"label":"introduction","permalink":"/blog/tags/introduction"}],"readingTime":0.11,"hasTruncateMarker":false,"authors":[{"name":"Itamar Syn-Hershko","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Introduction to Apache Hudi","authors":[{"name":"Itamar Syn-Hershko"}],"category":"blog","image":"/assets/images/blog/2023-03-17-introduction-to-apache-hudi.png","tags":["how-to","guide","introduction"]},"unlisted":false,"prevItem":{"title":"Introducing native support for Apache Hudi, Delta Lake, and Apache Iceberg on AWS Glue for Apache Spark, Part 2: AWS Glue Studio Visual Editor","permalink":"/blog/2023/03/20/Introducing-native-support-for-Apache Hudi-Delta-Lake-and-Apache-Iceberg-on-AWS-Glue-for-Apache-Spark-Part-2-AWS-Glue-Studio-Visual-Editor"},"nextItem":{"title":"Setting Uber\u2019s Transactional Data Lake in Motion with Incremental ETL Using Apache Hudi","permalink":"/blog/2023/03/16/Setting-Uber-Transactional-Data-Lake-in-Motion-with-Incremental-ETL-Using-Apache-Hudi"}}')},91251:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/07/21/streaming-data-lake-platform","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-07-21-streaming-data-lake-platform.md","source":"@site/blog/2021-07-21-streaming-data-lake-platform.md","title":"Apache Hudi - The Data Lake Platform","description":"As early as 2016, we set out a bold, new vision reimagining batch data processing through a new \u201cincremental\u201d data processing stack - alongside the existing batch and streaming stacks.","date":"2021-07-21T00:00:00.000Z","tags":[{"inline":true,"label":"datalake platform","permalink":"/blog/tags/datalake-platform"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":32.76,"hasTruncateMarker":true,"authors":[{"name":"vinoth","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi - The Data Lake Platform","excerpt":"It\'s been called many things. But, we have always been building a data lake platform","author":"vinoth","category":"blog","image":"/assets/images/blog/hudi_streaming.png","tags":["datalake platform","blog","apache hudi"]},"unlisted":false,"prevItem":{"title":"Baixin bank\u2019s real-time data lake evolution scheme based on Apache Hudi","permalink":"/blog/2021/07/26/Baixin-banksreal-time-data-lake-evolution-scheme-based-on-Apache-Hudi"},"nextItem":{"title":"Amazon Athena expands Apache Hudi support","permalink":"/blog/2021/07/16/Amazon-Athena-expands-Apache-Hudi-support"}}')},91409:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(72901),n=t(74848),s=t(28453),r=t(9230);const o={title:"Hudi Streamer (Delta Streamer) Hands-On Guide: Local Ingestion from Parquet Source",excerpt:"Hudi Streamer (Delta Streamer) Hands-On Guide: Local Ingestion from Parquet Source",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2023-11-19-Hudi-Streamer-DeltaStreamer-Hands-On-Guide-Local-Ingestion-from-Parquet-Source.png",tags:["apache hudi","hudi streamer","how-to","apache parquet","linkedin"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.linkedin.com/pulse/hudi-streamer-delta-hands-on-guide-local-ingestion-from-soumil-shah-jssse/?utm_source=share&utm_medium=member_ios&utm_campaign=share_via",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},91623:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/07/26/Baixin-banksreal-time-data-lake-evolution-scheme-based-on-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-07-26-Baixin-banksreal-time-data-lake-evolution-scheme-based-on-Apache-Hudi.mdx","source":"@site/blog/2021-07-26-Baixin-banksreal-time-data-lake-evolution-scheme-based-on-Apache-Hudi.mdx","title":"Baixin bank\u2019s real-time data lake evolution scheme based on Apache Hudi","description":"Redirecting... please wait!!","date":"2021-07-26T00:00:00.000Z","tags":[{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"real-time datalake","permalink":"/blog/tags/real-time-datalake"},{"inline":true,"label":"incremental processing","permalink":"/blog/tags/incremental-processing"},{"inline":true,"label":"developpaper","permalink":"/blog/tags/developpaper"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Baixin bank\u2019s real-time data lake evolution scheme based on Apache Hudi","category":"blog","image":"/assets/images/blog/2021-07-26-baixin-bank-real-time-data-lake.png","tags":["use-case","real-time datalake","incremental processing","developpaper"]},"unlisted":false,"prevItem":{"title":"MLOps Wars: Versioned Feature Data with a Lakehouse","permalink":"/blog/2021/08/03/MLOps-Wars-Versioned-Feature-Data-with-a-Lakehouse"},"nextItem":{"title":"Apache Hudi - The Data Lake Platform","permalink":"/blog/2021/07/21/streaming-data-lake-platform"}}')},91861:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(17539),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi vs Apache Iceberg: A Comprehensive Comparison",author:"RisingWave marketing team",category:"blog",image:"/assets/images/blog/2024-04-25-apache-hudi-vs-apache-iceberg-a-comprehensive-comparison.png",tags:["blog","apache hudi","apache iceberg","comparison","risingwave"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://risingwave.com/blog/apache-hudi-vs-apache-iceberg-a-comprehensive-comparison/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},91987:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(38229),n=t(74848),s=t(28453),r=t(9230);const o={title:"Automate schema evolution at scale with Apache Hudi in AWS Glue | Amazon Web Services",authors:[{name:"Subhro Bose"},{name:"Eva Fang"},{name:"Ketan Karalkar"}],category:"blog",image:"/assets/images/blog/automate-schema-evolution-at-scale-with-apache-hudi-in-aws-glue.png",tags:["how-to","schema evolution","amazon"]},l=void 0,d={authorsImageUrls:[void 0,void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/automate-schema-evolution-at-scale-with-apache-hudi-in-aws-glue/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},92023:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(99112),n=t(74848),s=t(28453),r=t(9230);const o={title:"New features from Apache Hudi 0.7.0 and 0.8.0 available on Amazon EMR",authors:[{name:"Udit Mehrotra"},{name:"Gagan Brahmi"}],category:"blog",image:"/assets/images/blog/aws.jpg",tags:["blog","amazon"]},l=void 0,d={authorsImageUrls:[void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/new-features-from-apache-hudi-0-7-0-and-0-8-0-available-on-amazon-emr/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},92296:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/05/02/intro-to-hudi-and-flink","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-05-02-intro-to-hudi-and-flink.mdx","source":"@site/blog/2023-05-02-intro-to-hudi-and-flink.mdx","title":"An Introduction to the Hudi and Flink Integration","description":"Redirecting... please wait!!","date":"2023-05-02T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"apache flink","permalink":"/blog/tags/apache-flink"},{"inline":true,"label":"onehouse","permalink":"/blog/tags/onehouse"}],"readingTime":0.11,"hasTruncateMarker":false,"authors":[{"name":"Danny Chan","socials":{},"key":null,"page":null}],"frontMatter":{"title":"An Introduction to the Hudi and Flink Integration","authors":[{"name":"Danny Chan"}],"category":"blog","image":"/assets/images/blog/2023-05-02-intro-to-hudi-and-flink.png","tags":["blog","apache hudi","apache flink","onehouse"]},"unlisted":false,"prevItem":{"title":"Lakehouse at Fortune 1 Scale","permalink":"/blog/2023/05/03/lakehouse-at-fortune-1-scale"},"nextItem":{"title":"Can you concurrently write data to Apache Hudi w/o any lock provider?","permalink":"/blog/2023/04/29/can-you-concurrently-write-data-to-apache-hudi-w-o-any-lock-provider"}}')},92379:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/08/28/Delta-Hudi-Iceberg-A-Benchmark-Compilation","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-08-28-Delta-Hudi-Iceberg-A-Benchmark-Compilation.mdx","source":"@site/blog/2023-08-28-Delta-Hudi-Iceberg-A-Benchmark-Compilation.mdx","title":"Delta, Hudi, Iceberg \u2014 A Benchmark Compilation","description":"Redirecting... please wait!!","date":"2023-08-28T00:00:00.000Z","tags":[{"inline":true,"label":"performance","permalink":"/blog/tags/performance"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"delta lake","permalink":"/blog/tags/delta-lake"},{"inline":true,"label":"iceberg","permalink":"/blog/tags/iceberg"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Kyle Weller","key":null,"page":null}],"frontMatter":{"title":"Delta, Hudi, Iceberg \u2014 A Benchmark Compilation","excerpt":"Benchmark Compilation","author":"Kyle Weller","category":"blog","image":"/assets/images/blog/2023-08-28-Delta-Hudi-Iceberg-A-Benchmark-Compilation.png","tags":["performance","apache hudi","delta lake","iceberg","medium"]},"unlisted":false,"prevItem":{"title":"Apache Hudi: From Zero To One (1/10)","permalink":"/blog/2023/08/28/Apache-Hudi-From-Zero-To-One"},"nextItem":{"title":"Delta, Hudi, Iceberg \u2014 Which is most popular?","permalink":"/blog/2023/08/25/Delta-Hudi-Iceberg-Which-is-most-popular"}}')},92548:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/10/22/exploring-time-travel-queries-in-apache-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-10-22-exploring-time-travel-queries-in-apache-hudi.mdx","source":"@site/blog/2024-10-22-exploring-time-travel-queries-in-apache-hudi.mdx","title":"Exploring Time Travel Queries in Apache Hudi","description":"Redirecting... please wait!!","date":"2024-10-22T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"time travel query","permalink":"/blog/tags/time-travel-query"},{"inline":true,"label":"opstree","permalink":"/blog/tags/opstree"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Ramneek Kaur","key":null,"page":null}],"frontMatter":{"title":"Exploring Time Travel Queries in Apache Hudi","author":"Ramneek Kaur","category":"blog","image":"/assets/images/blog/2024-10-22-exploring-time-travel-queries-in-apache-hudi.jpeg","tags":["blog","Apache Hudi","time travel query","opstree"]},"unlisted":false,"prevItem":{"title":"Mastering Open Table Formats: A Guide to Apache Iceberg, Hudi, and Delta Lake","permalink":"/blog/2024/10/23/mastering-open-table-formats-a-guide-to-apache-iceberg-hudi-and-delta-lake"},"nextItem":{"title":"Streaming DynamoDB Data into a Hudi Table: AWS Glue in Action","permalink":"/blog/2024/10/14/streaming-dynamodb-data-into-a-hudi-table-aws-glue-in-action"}}')},92580:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/03/14/Modern-Datalakes-with-Hudi--MinIO--and-HMS","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-03-14-Modern-Datalakes-with-Hudi--MinIO--and-HMS.mdx","source":"@site/blog/2024-03-14-Modern-Datalakes-with-Hudi--MinIO--and-HMS.mdx","title":"Modern Datalakes with Hudi, MinIO, and HMS","description":"Redirecting... please wait!!","date":"2024-03-14T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"minio","permalink":"/blog/tags/minio"},{"inline":true,"label":"hms","permalink":"/blog/tags/hms"},{"inline":true,"label":"hive metastore","permalink":"/blog/tags/hive-metastore"},{"inline":true,"label":"min","permalink":"/blog/tags/min"}],"readingTime":0.1,"hasTruncateMarker":false,"authors":[{"name":"Brenna Buuck","key":null,"page":null}],"frontMatter":{"title":"Modern Datalakes with Hudi, MinIO, and HMS","author":"Brenna Buuck","category":"blog","image":"/assets/images/blog/2024-03-14-Modern-Datalakes-with-Hudi--MinIO--and-HMS.jpg","tags":["blog","apache hudi","minio","hms","hive metastore","min"]},"unlisted":false,"prevItem":{"title":"Open Table Formats (part-1): Apache Hudi (Hadoop Upserts Deletes and Incrementals)","permalink":"/blog/2024/03/16/Open-Table-Formats-part-1-Apache-Hudi-Hadoop-Upserts-Deletes-and-Incrementals"},"nextItem":{"title":"Navigating the Future: The Evolutionary Journey of Upstox\u2019s Data Platform","permalink":"/blog/2024/03/10/navigating-the-future-the-evolutionary-journey-of-upstoxs-data-platform"}}')},92777:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/09/28/Data-processing-with-Spark-time-traveling","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-09-28-Data-processing-with-Spark-time-traveling.mdx","source":"@site/blog/2022-09-28-Data-processing-with-Spark-time-traveling.mdx","title":"Data processing with Spark: time traveling","description":"Redirecting... please wait!!","date":"2022-09-28T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"time travel query","permalink":"/blog/tags/time-travel-query"},{"inline":true,"label":"devgenius","permalink":"/blog/tags/devgenius"}],"readingTime":0.11,"hasTruncateMarker":false,"authors":[{"name":"Petrica Leuca","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Data processing with Spark: time traveling","authors":[{"name":"Petrica Leuca"}],"category":"blog","image":"/assets/images/blog/2022-09-28_Data_processing_with_Spark_time_traveling.png","tags":["how-to","time travel query","devgenius"]},"unlisted":false,"prevItem":{"title":"Ingest streaming data to Apache Hudi tables using AWS Glue and Apache Hudi DeltaStreamer","permalink":"/blog/2022/10/06/Ingest-streaming-data-to-Apache-Hudi-using-AWS-Glue-and-DeltaStreamer"},"nextItem":{"title":"Building Streaming Data Lakes with Hudi and MinIO","permalink":"/blog/2022/09/20/Building-Streaming-Data-Lakes-with-Hudi-and-MinIO"}}')},92916:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/03/23/Spark-ETL-Chapter-8-with-Lakehouse-Apache-HUDI","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-03-23-Spark-ETL-Chapter-8-with-Lakehouse-Apache-HUDI.mdx","source":"@site/blog/2023-03-23-Spark-ETL-Chapter-8-with-Lakehouse-Apache-HUDI.mdx","title":"Spark ETL Chapter 8 with Lakehouse | Apache HUDI","description":"Redirecting... please wait!!","date":"2023-03-23T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"guide","permalink":"/blog/tags/guide"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Kalpan Shah","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Spark ETL Chapter 8 with Lakehouse | Apache HUDI","authors":[{"name":"Kalpan Shah"}],"category":"blog","image":"/assets/images/blog/2023-03-23-Spark-ETL-Chapter-8-with-Lakehouse-Apache-HUDI.png","tags":["how-to","guide","medium"]},"unlisted":false,"prevItem":{"title":"Global vs Non-global index in Apache Hudi","permalink":"/blog/2023/04/02/global-vs-non-global-index-in-apache-hudi"},"nextItem":{"title":"Introducing native support for Apache Hudi, Delta Lake, and Apache Iceberg on AWS Glue for Apache Spark, Part 2: AWS Glue Studio Visual Editor","permalink":"/blog/2023/03/20/Introducing-native-support-for-Apache Hudi-Delta-Lake-and-Apache-Iceberg-on-AWS-Glue-for-Apache-Spark-Part-2-AWS-Glue-Studio-Visual-Editor"}}')},93598:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(83414),n=t(74848),s=t(28453),r=t(9230);const o={title:"Apache Hudi: From Zero To One (3/10)",excerpt:"Understand write flows and operations",author:"Shiyan Xu",category:"blog",image:"/assets/images/blog/2023-09-15-Apache-Hudi-From-Zero-To-One-blog-3.png",tags:["hudi","spark","blog","course","tutorial","datumagic","data lake","lakehouse","apache hudi","apache spark"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blog.datumagic.ai/p/apache-hudi-from-zero-to-one-310",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},93666:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(76011),n=t(74848),s=t(28453),r=t(9230);const o={title:"Asynchronous Indexing using Hudi",authors:[{name:"Sagar Sumit"}],category:"blog",image:"/assets/images/blog/2022-06-04-async-index.png",tags:["design","multi modal indexing","onehouse","async indexing"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.onehouse.ai/blog/asynchronous-indexing-using-hudi",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},93717:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/07/31/hudi-file-formats","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-07-31-hudi-file-formats.md","source":"@site/blog/2024-07-31-hudi-file-formats.md","title":"Column File Formats: How Hudi Leverages Parquet and ORC ","description":"Introduction","date":"2024-07-31T00:00:00.000Z","tags":[{"inline":true,"label":"Data Lake","permalink":"/blog/tags/data-lake"},{"inline":true,"label":"Apache Hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"Apache Parquet","permalink":"/blog/tags/apache-parquet"},{"inline":true,"label":"Apache ORC","permalink":"/blog/tags/apache-orc"}],"readingTime":4.11,"hasTruncateMarker":false,"authors":[{"name":"Albert Wong","key":null,"page":null}],"frontMatter":{"title":"Column File Formats: How Hudi Leverages Parquet and ORC ","excerpt":"Explains how Hudi uses Parquet and ORC","author":"Albert Wong","category":"blog","image":"/assets/images/blog/hudi-parquet-orc.png","tags":["Data Lake","Apache Hudi","Apache Parquet","Apache ORC"]},"unlisted":false,"prevItem":{"title":"Developer Guide: How to Submit Hudi PySpark(Python) Jobs to EMR Serverless (7.1.0) with AWS Glue Hive MetaStore","permalink":"/blog/2024/09/04/developer-guide-how-to-submit-hudi-pyspark-python-jobs-to-emr-serverless"},"nextItem":{"title":"Understanding Data Lake Change Data Capture","permalink":"/blog/2024/07/30/data-lake-cdc"}}')},93902:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/06/29/Apache-Hudi-vs-Delta-Lake-transparent-tpc-ds-lakehouse-performance-benchmarks","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-06-29-Apache-Hudi-vs-Delta-Lake-transparent-tpc-ds-lakehouse-performance-benchmarks.mdx","source":"@site/blog/2022-06-29-Apache-Hudi-vs-Delta-Lake-transparent-tpc-ds-lakehouse-performance-benchmarks.mdx","title":"Apache Hudi vs Delta Lake - Transparent TPC-DS Lakehouse Performance Benchmarks","description":"Redirecting... please wait!!","date":"2022-06-29T00:00:00.000Z","tags":[{"inline":true,"label":"performance","permalink":"/blog/tags/performance"},{"inline":true,"label":"datalake","permalink":"/blog/tags/datalake"},{"inline":true,"label":"comparison","permalink":"/blog/tags/comparison"},{"inline":true,"label":"onehouse","permalink":"/blog/tags/onehouse"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Alexey Kudinkin","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Apache Hudi vs Delta Lake - Transparent TPC-DS Lakehouse Performance Benchmarks","authors":[{"name":"Alexey Kudinkin"}],"category":"blog","image":"/assets/images/blog/2022-06-29-apache_hudi_vs_delta_lake_tpc_ds_benchmarks.png","tags":["performance","datalake","comparison","onehouse"]},"unlisted":false,"prevItem":{"title":"Build Open Lakehouse using Apache Hudi & dbt","permalink":"/blog/2022/07/11/build-open-lakehouse-using-apache-hudi-and-dbt"},"nextItem":{"title":"Hudi\u2019s Column Stats Index and Data Skipping feature help speed up queries by an orders of magnitude!","permalink":"/blog/2022/06/09/Singificant-queries-speedup-from-Hudi-Column-Stats-Index-and-Data-Skipping-features"}}')},94042:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/09/17/how-apache-hudi-transformed-yuno-s-data-lake","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-09-17-how-apache-hudi-transformed-yuno-s-data-lake.mdx","source":"@site/blog/2024-09-17-how-apache-hudi-transformed-yuno-s-data-lake.mdx","title":"How Apache Hudi transformed Yuno\u2019s data lake","description":"Redirecting... please wait!!","date":"2024-09-17T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"cow","permalink":"/blog/tags/cow"},{"inline":true,"label":"mor","permalink":"/blog/tags/mor"},{"inline":true,"label":"record index","permalink":"/blog/tags/record-index"},{"inline":true,"label":"record level index","permalink":"/blog/tags/record-level-index"},{"inline":true,"label":"clustering","permalink":"/blog/tags/clustering"},{"inline":true,"label":"cleaning","permalink":"/blog/tags/cleaning"},{"inline":true,"label":"bloom index","permalink":"/blog/tags/bloom-index"},{"inline":true,"label":"fiel sizing","permalink":"/blog/tags/fiel-sizing"},{"inline":true,"label":"y.uno","permalink":"/blog/tags/y-uno"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Nahuel Leandro Mazzitelli","key":null,"page":null}],"frontMatter":{"title":"How Apache Hudi transformed Yuno\u2019s data lake","author":"Nahuel Leandro Mazzitelli","category":"blog","image":"/assets/images/blog/2024-09-17-how-apache-hudi-transformed-yuno-s-data-lake.png","tags":["blog","apache hudi","cow","mor","record index","record level index","clustering","cleaning","bloom index","fiel sizing","y.uno"]},"unlisted":false,"prevItem":{"title":"Hands-on with Apache Hudi and Spark","permalink":"/blog/2024/09/22/hands-on-with-apache-hudi-and-spark"},"nextItem":{"title":"Uber\u2019s Big Data Revolution: From MySQL to Hadoop and Beyond","permalink":"/blog/2024/09/14/Ubers-Big-Data-Revolution-From-MySQL-to-Hadoop-and-Beyond"}}')},94134:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(81579),n=t(74848),s=t(28453),r=t(9230);const o={title:"Building a Lakehouse Architecture on AWS with Terraform",author:"Juanfelipear",category:"blog",image:"/assets/images/blog/2025-02-24-building-a-lakehouse-architecture-on-aws-with-terraform.jpeg",tags:["blog","apache hudi","aws","terraform","lakehouse","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@juanfelipear97/building-a-lakehouse-architecture-on-aws-with-terraform-139c079ec385",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},94183:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/10/27/I-spent-5-hours-exploring-the-story-behind-Apache-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-10-27-I-spent-5-hours-exploring-the-story-behind-Apache-Hudi.mdx","source":"@site/blog/2024-10-27-I-spent-5-hours-exploring-the-story-behind-Apache-Hudi.mdx","title":"I spent 5 hours exploring the story behind Apache Hudi.","description":"Redirecting... please wait!!","date":"2024-10-27T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"beginner","permalink":"/blog/tags/beginner"},{"inline":true,"label":"det","permalink":"/blog/tags/det"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Vu Trinh","key":null,"page":null}],"frontMatter":{"title":"I spent 5 hours exploring the story behind Apache Hudi.","author":"Vu Trinh","category":"blog","image":"/assets/images/blog/2024-10-27-I-spent-5-hours-exploring-the-story-behind-Apache-Hudi.jpeg","tags":["blog","apache hudi","beginner","det"]},"unlisted":false,"prevItem":{"title":"Understanding COW and MOR in Apache Hudi: Choosing the Right Storage Strategy","permalink":"/blog/2024/11/12/understanding-cow-and-mor-in-apache-hudi"},"nextItem":{"title":"Moving Large Tables from Snowflake to S3 Using the COPY INTO Command and Hudi Bootstrapping to Build Data Lakes | Hands-On Labs","permalink":"/blog/2024/10/26/moving-large-tables-from-snowflake-to-s3-using-the-copy-into-command-and-hudi"}}')},94367:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/example_perf_improvement-acd223093d7c84fb6f0a896dcb571737.png"},94519:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/initial_layout-ba5e4c454e6d2328f74dfc5e9fa2966a.png"},94555:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(27392),n=t(74848),s=t(28453),r=t(9230);const o={title:"Load data incrementally from transactional data lakes to data warehouses",excerpt:"Load data incrementally from Apache Hudi table to Amazon Redshift using a Hudi incremental query",author:"Noritaka Sekiyama",category:"blog",image:"/assets/images/blog/2023-10-19-load-data-incrementally-from-transactional-data-lakes-to-data-warehouses.png",tags:["incremental updates","amazon","how to","querying","aws","amazon redshift","apache hudi"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/load-data-incrementally-from-transactional-data-lakes-to-data-warehouses/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},94852:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2022/08/25/Data-Lake-Lakehouse-Guide-Powered-by-Data-Lake-Table-Formats-Delta-Lake-Iceberg-Hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2022-08-25-Data-Lake-Lakehouse-Guide-Powered-by-Data-Lake-Table-Formats-Delta-Lake-Iceberg-Hudi.mdx","source":"@site/blog/2022-08-25-Data-Lake-Lakehouse-Guide-Powered-by-Data-Lake-Table-Formats-Delta-Lake-Iceberg-Hudi.mdx","title":"Data Lake / Lakehouse Guide: Powered by Data Lake Table Formats (Delta Lake, Iceberg, Hudi)","description":"Redirecting... please wait!!","date":"2022-08-25T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"datalake","permalink":"/blog/tags/datalake"},{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"comparison","permalink":"/blog/tags/comparison"},{"inline":true,"label":"airbyte","permalink":"/blog/tags/airbyte"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[{"name":"Simon Sp\xe4ti","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Data Lake / Lakehouse Guide: Powered by Data Lake Table Formats (Delta Lake, Iceberg, Hudi)","authors":[{"name":"Simon Sp\xe4ti"}],"category":"blog","image":"/assets/images/blog/2022-08-25-Data-Lake-Lakehouse-Guide-Powered-by-Data-Lake-Table-Formats-Delta-Lake-Iceberg-Hudi.png","tags":["blog","datalake","lakehouse","comparison","airbyte"]},"unlisted":false,"prevItem":{"title":"Building Streaming Data Lakes with Hudi and MinIO","permalink":"/blog/2022/09/20/Building-Streaming-Data-Lakes-with-Hudi-and-MinIO"},"nextItem":{"title":"Implementation of SCD-2 (Slowly Changing Dimension) with Apache Hudi & Spark","permalink":"/blog/2022/08/24/Implementation-of-SCD-2-with-Apache-Hudi-and-Spark"}}')},94870:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(14912),n=t(74848),s=t(28453),r=t(9230);const o={title:"How lakehouse handles concurrent Read and Writes",author:"Sanjeet Shukla",category:"blog",image:"/assets/images/blog/2024-12-28-how-lakehouse-handles-concurrent-read-and-writes.jpeg",tags:["blog","apache hudi","concurrency","concurrency-control","non-blocking concurrency-control","nbcc","medium"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://medium.com/@sanjeets1900/how-lakehouse-handles-concurrent-read-and-writes-b4423fecfe81",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},95053:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide11-e6a0a852c7c72494dae31e8a9f95ba74.png"},95446:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(36632),n=t(74848),s=t(28453),r=t(9230);const o={title:"Why Walmart Chose Apache Hudi for Their Lakehouse",author:"Vu Trinh",category:"blog",image:"/assets/images/blog/2025-04-09-why-walmart-chose-apache-hudi-for-their-lakehouse.jpg",tags:["blog","Apache Hudi","use-case","det"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blog.det.life/why-walmart-chose-apache-hudi-for-their-lakehouse-c0a3574db0ba",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},95508:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2020/05/28/monitoring-hudi-metrics-with-datadog","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2020-05-28-monitoring-hudi-metrics-with-datadog.md","source":"@site/blog/2020-05-28-monitoring-hudi-metrics-with-datadog.md","title":"Monitor Hudi metrics with Datadog","description":"Availability","date":"2020-05-28T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"metrics","permalink":"/blog/tags/metrics"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":1.49,"hasTruncateMarker":true,"authors":[{"name":"rxu","key":null,"page":null}],"frontMatter":{"title":"Monitor Hudi metrics with Datadog","excerpt":"Introducing the feature of reporting Hudi metrics via Datadog HTTP API","author":"rxu","category":"blog","tags":["how-to","metrics","apache hudi"]},"unlisted":false,"prevItem":{"title":"The Apache Software Foundation Announces Apache\xae Hudi\u2122 as a Top-Level Project","permalink":"/blog/2020/06/04/The-Apache-Software-Foundation-Announces-Apache-Hudi-as-a-Top-Level-Project"},"nextItem":{"title":"Apache Hudi Support on Apache Zeppelin","permalink":"/blog/2020/04/27/apache-hudi-apache-zepplin"}}')},95584:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/dlh_new-ae34f6d692de93292db9eb4e19690670.png"},95736:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image3-8f35b19f5afc8d6b571f4479eb024189.png"},95737:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(98029),n=t(74848),s=t(28453),r=t(9230);const o={title:"Build a data lake using amazon kinesis data stream for amazon dynamodb and apache hudi",authors:[{name:"Dhiraj Thakur"},{name:"Dylan Qu"},{name:"Saurabh Shrivastava"}],category:"blog",image:"/assets/images/blog/2021-03-04-build-data-lake-using-amazon-kinesis-for-amazon-dynamodb-and-apache-hudi.jpeg",tags:["how-to","streaming ingestion","amazon"]},l=void 0,d={authorsImageUrls:[void 0,void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/build-a-data-lake-using-amazon-kinesis-data-streams-for-amazon-dynamodb-and-apache-hudi/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},95878:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/10/17/Get-started-with-Apache-Hudi-using-AWS-Glue-by-implementing-key-design-concepts-Part-1","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-10-17-Get-started-with-Apache-Hudi-using-AWS-Glue-by-implementing-key-design-concepts-Part-1.mdx","source":"@site/blog/2023-10-17-Get-started-with-Apache-Hudi-using-AWS-Glue-by-implementing-key-design-concepts-Part-1.mdx","title":"Get started with Apache Hudi using AWS Glue by implementing key design concepts \u2013 Part 1","description":"Redirecting... please wait!!","date":"2023-10-17T00:00:00.000Z","tags":[{"inline":true,"label":"aws glue","permalink":"/blog/tags/aws-glue"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"},{"inline":true,"label":"design","permalink":"/blog/tags/design"},{"inline":true,"label":"aws glue","permalink":"/blog/tags/aws-glue"},{"inline":true,"label":"upserts","permalink":"/blog/tags/upserts"},{"inline":true,"label":"bulk insert","permalink":"/blog/tags/bulk-insert"},{"inline":true,"label":"indexing","permalink":"/blog/tags/indexing"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"Srinivas Kandi","socials":{},"key":null,"page":null},{"name":"Ravi Itha","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Get started with Apache Hudi using AWS Glue by implementing key design concepts \u2013 Part 1","excerpt":"Get started with Apache Hudi using AWS Glue by implementing key design concepts \u2013 Part 1","authors":[{"name":"Srinivas Kandi"},{"name":"Ravi Itha"}],"category":"blog","image":"/assets/images/blog/2023-10-17-Get-started-with-Apache-Hudi-using-AWS-Glue-by-implementing-key-design-concepts-Part-1.png","tags":["aws glue","apache hudi","how-to","amazon","design","aws glue","upserts","bulk insert","indexing"]},"unlisted":false,"prevItem":{"title":"Apache Hudi: From Zero To One (5/10)","permalink":"/blog/2023/10/18/Apache-Hudi-From-Zero-To-One-blog-5"},"nextItem":{"title":"StarRocks query performance with Apache Hudi and Onehouse","permalink":"/blog/2023/10/11/starrocks-query-performance-with-apache-hudi-and-onehouse"}}')},96107:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image8-953168c35d108f42143d2942a7197941.png"},96211:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/07/08/Quickly-start-using-Apache-Hudi-on-AWS-EMR","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-07-08-Quickly-start-using-Apache-Hudi-on-AWS-EMR.mdx","source":"@site/blog/2023-07-08-Quickly-start-using-Apache-Hudi-on-AWS-EMR.mdx","title":"Quickly start using Apache Hudi on AWS EMR","description":"Redirecting... please wait!!","date":"2023-07-08T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"aws emr","permalink":"/blog/tags/aws-emr"},{"inline":true,"label":"cow","permalink":"/blog/tags/cow"},{"inline":true,"label":"medium","permalink":"/blog/tags/medium"}],"readingTime":0.13,"hasTruncateMarker":false,"authors":[{"name":"Ritik Kaushik","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Quickly start using Apache Hudi on AWS EMR","authors":[{"name":"Ritik Kaushik"}],"category":"blog","tags":["blog","aws emr","cow","medium"]},"unlisted":false,"prevItem":{"title":"Hoodie Timeline: Foundational pillar for ACID transactions","permalink":"/blog/2023/07/09/Hoodie-Timeline-Foundational-pillar-for-ACID-transactions"},"nextItem":{"title":"Skip rocks and files: Turbocharge Trino queries with Hudi\u2019s multi-modal indexing subsystem","permalink":"/blog/2023/07/07/Skip-rocks-and-files-Turbocharge-Trino-queries-with-Hudi-multi-modal-indexing-subsystem"}}')},96248:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image3-570b43b2c0cec6865c17b50ddef9a3f4.png"},96749:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(20880),n=t(74848),s=t(28453),r=t(9230);const o={title:"Lakehouse or Warehouse? Part 1 of 2",excerpt:"Lakehouse or Warehouse? Part 1 of 2",author:"Floyd Smith",category:"blog",image:"/assets/images/blog/2023-09-06-Lakehouse-or-Warehouse-Part-1-of-2.png",tags:["blog","onehouse","data lakehouse","data warehouse","apache hudi"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.onehouse.ai/blog/lakehouse-or-warehouse-part-1-of-2",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},96766:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig1-3baa485e75ef728786f15b45d2d97d6b.png"},97047:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/Event20tables-8998b57588a66cb2f5d3e9233dfb6d0f.gif"},97117:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2019/03/07/batch-vs-incremental","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2019-03-07-batch-vs-incremental.md","source":"@site/blog/2019-03-07-batch-vs-incremental.md","title":"Big Batch vs Incremental Processing","description":"","date":"2019-03-07T00:00:00.000Z","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[{"name":"vinoth","key":null,"page":null}],"frontMatter":{"title":"Big Batch vs Incremental Processing","author":"vinoth","category":"blog","image":"/assets/images/blog/batch_vs_incremental.png"},"unlisted":false,"prevItem":{"title":"Registering sample dataset to Hive via beeline","permalink":"/blog/2019/05/14/registering-dataset-to-hive"},"nextItem":{"title":"Hudi entered Apache Incubator","permalink":"/blog/2019/01/18/asf-incubation"}}')},97183:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/10/06/Apache-Hudi-Copy-on-Write-CoW-Table","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-10-06-Apache-Hudi-Copy-on-Write-CoW-Table.mdx","source":"@site/blog/2023-10-06-Apache-Hudi-Copy-on-Write-CoW-Table.mdx","title":"Apache Hudi: Copy on Write(CoW) Table","description":"Redirecting... please wait!!","date":"2023-10-06T00:00:00.000Z","tags":[{"inline":true,"label":"medium","permalink":"/blog/tags/medium"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"cow","permalink":"/blog/tags/cow"},{"inline":true,"label":"deep dive","permalink":"/blog/tags/deep-dive"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Ankur Ranjan","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Apache Hudi: Copy on Write(CoW) Table","excerpt":"Apache Hudi: Copy on Write(CoW) Table","authors":[{"name":"Ankur Ranjan"}],"category":"blog","image":"/assets/images/blog/2023-10-06-Apache-Hudi-Copy-on-Write-CoW-Table.png","tags":["medium","blog","cow","deep dive","apache hudi"]},"unlisted":false,"prevItem":{"title":"StarRocks query performance with Apache Hudi and Onehouse","permalink":"/blog/2023/10/11/starrocks-query-performance-with-apache-hudi-and-onehouse"},"nextItem":{"title":"Apache Hudi: From Zero To One (4/10)","permalink":"/blog/2023/09/27/Apache-Hudi-From-Zero-To-One-blog-4"}}')},97247:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2023/10/18/Apache-Hudi-From-Zero-To-One-blog-5","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2023-10-18-Apache-Hudi-From-Zero-To-One-blog-5.mdx","source":"@site/blog/2023-10-18-Apache-Hudi-From-Zero-To-One-blog-5.mdx","title":"Apache Hudi: From Zero To One (5/10)","description":"Redirecting... please wait!!","date":"2023-10-18T00:00:00.000Z","tags":[{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"spark","permalink":"/blog/tags/spark"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"course","permalink":"/blog/tags/course"},{"inline":true,"label":"tutorial","permalink":"/blog/tags/tutorial"},{"inline":true,"label":"datumagic","permalink":"/blog/tags/datumagic"},{"inline":true,"label":"data lake","permalink":"/blog/tags/data-lake"},{"inline":true,"label":"lakehouse","permalink":"/blog/tags/lakehouse"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"apache spark","permalink":"/blog/tags/apache-spark"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Shiyan Xu","key":null,"page":null}],"frontMatter":{"title":"Apache Hudi: From Zero To One (5/10)","excerpt":"Introduce table services: compaction, cleaning, and indexing","author":"Shiyan Xu","category":"blog","image":"/assets/images/blog/2023-10-18-Apache-Hudi-From-Zero-To-One-blog-5.png","tags":["hudi","spark","blog","course","tutorial","datumagic","data lake","lakehouse","apache hudi","apache spark"]},"unlisted":false,"prevItem":{"title":"Load data incrementally from transactional data lakes to data warehouses","permalink":"/blog/2023/10/19/load-data-incrementally-from-transactional-data-lakes-to-data-warehouses"},"nextItem":{"title":"Get started with Apache Hudi using AWS Glue by implementing key design concepts \u2013 Part 1","permalink":"/blog/2023/10/17/Get-started-with-Apache-Hudi-using-AWS-Glue-by-implementing-key-design-concepts-Part-1"}}')},97283:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/fig2-468ec18846bf7194631d838fd9824bcf.png"},98029:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/03/04/Build-a-data-lake-using-amazon-kinesis-data-stream-for-amazon-dynamodb-and-apache-hudi","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-03-04-Build-a-data-lake-using-amazon-kinesis-data-stream-for-amazon-dynamodb-and-apache-hudi.mdx","source":"@site/blog/2021-03-04-Build-a-data-lake-using-amazon-kinesis-data-stream-for-amazon-dynamodb-and-apache-hudi.mdx","title":"Build a data lake using amazon kinesis data stream for amazon dynamodb and apache hudi","description":"Redirecting... please wait!!","date":"2021-03-04T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"streaming ingestion","permalink":"/blog/tags/streaming-ingestion"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"Dhiraj Thakur","socials":{},"key":null,"page":null},{"name":"Dylan Qu","socials":{},"key":null,"page":null},{"name":"Saurabh Shrivastava","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Build a data lake using amazon kinesis data stream for amazon dynamodb and apache hudi","authors":[{"name":"Dhiraj Thakur"},{"name":"Dylan Qu"},{"name":"Saurabh Shrivastava"}],"category":"blog","image":"/assets/images/blog/2021-03-04-build-data-lake-using-amazon-kinesis-for-amazon-dynamodb-and-apache-hudi.jpeg","tags":["how-to","streaming ingestion","amazon"]},"unlisted":false,"prevItem":{"title":"New features from Apache hudi in Amazon EMR","permalink":"/blog/2021/03/11/New-features-from-Apache-hudi-in-Amazon-EMR"},"nextItem":{"title":"Streaming Responsibly - How Apache Hudi maintains optimum sized files","permalink":"/blog/2021/03/01/hudi-file-sizing"}}')},98529:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image4-d88e573a6dba93c4a1edd9e87a09fa5f.png"},98557:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(61972),n=t(74848),s=t(28453),r=t(9230);const o={title:"Iceberg vs. Delta Lake vs. Hudi: A Comparative Look at Lakehouse Architectures",author:"Abdelkbir Armel",category:"blog",image:"/assets/images/blog/2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.png",tags:["blog","Apache Hudi","Apache Iceberg","Delta Lake","comparison","forefathers"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://blog.forefathers.io/iceberg-vs-delta-lake-vs-hudi-a-comparative-look-at-lakehouse-architectures-52eec62b29e8",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},98636:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2024/02/12/How-a-POC-became-a-production-ready-Hudi-data-lakehouse-through-close-team-collaboration","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2024-02-12-How-a-POC-became-a-production-ready-Hudi-data-lakehouse-through-close-team-collaboration.mdx","source":"@site/blog/2024-02-12-How-a-POC-became-a-production-ready-Hudi-data-lakehouse-through-close-team-collaboration.mdx","title":"How a POC became a production-ready Hudi data lakehouse through close team collaboration","description":"Redirecting... please wait!!","date":"2024-02-12T00:00:00.000Z","tags":[{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"apache hudi","permalink":"/blog/tags/apache-hudi"},{"inline":true,"label":"leboncoin-tech-blog","permalink":"/blog/tags/leboncoin-tech-blog"},{"inline":true,"label":"beginner","permalink":"/blog/tags/beginner"},{"inline":true,"label":"delete","permalink":"/blog/tags/delete"},{"inline":true,"label":"gdpr deletion","permalink":"/blog/tags/gdpr-deletion"},{"inline":true,"label":"upsert","permalink":"/blog/tags/upsert"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"Xiaoxiao Rey and Hussein Awala","key":null,"page":null}],"frontMatter":{"title":"How a POC became a production-ready Hudi data lakehouse through close team collaboration","excerpt":"How a POC became a production-ready Hudi data lakehouse through close team collaboration","author":"Xiaoxiao Rey and Hussein Awala","category":"blog","image":"/assets/images/blog/2024-02-12-How-a-POC-became-a-production-ready-Hudi-data-lakehouse-through-close-team-collaboration.png","tags":["use-case","apache hudi","leboncoin-tech-blog","beginner","delete","gdpr deletion","upsert"]},"unlisted":false,"prevItem":{"title":"Enabling near real-time data analytics on the data lake","permalink":"/blog/2024/02/23/Enabling-near-real-time-data-analytics-on-the-data-lake"},"nextItem":{"title":"Building an Open Source Data Lake House with Hudi, Postgres Hive Metastore, Minio, and StarRocks","permalink":"/blog/2024/02/06/Building-an-Open-Source-Data-Lake-House-with-Hudi-Postgres-Hive-Metastore-Minio-and-StarRocks"}}')},98692:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/slide3-717ff61fa0432142d84c418fc3a73200.png"},98698:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(42698),n=t(74848),s=t(28453);const r={title:"Hudi\u2019s Automatic File Sizing Delivers Unmatched Performance",excerpt:"Explains how Hudi handles small files during ingestion and its benefits",author:"Aditya Goenka",category:"blog",image:"/assets/images/blog/2024-06-07-apache-hudi-a-deep-dive-with-python-code-examples.png",tags:["Data Lake","Apache Hudi"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"Introduction",id:"introduction",level:2},{value:"Understanding Small File Challenges",id:"understanding-small-file-challenges",level:2},{value:"Impact of Small File",id:"impact-of-small-file",level:3},{value:"How table formats solve this problem",id:"how-table-formats-solve-this-problem",level:2},{value:"<strong>Ingesting Data As-Is and Optimizing Post-Ingestion</strong> :",id:"ingesting-data-as-is-and-optimizing-post-ingestion-",level:3},{value:"Pros:",id:"pros",level:4},{value:"Cons:",id:"cons",level:4},{value:"<strong>Managing Small Files During Ingestion Only</strong> :",id:"managing-small-files-during-ingestion-only-",level:3},{value:"How Hudi helps in small file handling during ingestion",id:"how-hudi-helps-in-small-file-handling-during-ingestion",level:2},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const a={a:"a",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.h2,{id:"introduction",children:"Introduction"}),"\n",(0,n.jsx)(a.p,{children:"In today\u2019s data-driven world, managing large volumes of data efficiently is crucial. One of the standout features of Apache Hudi is its ability to handle small files during data writes, which significantly optimizes both performance and cost. In this post, we\u2019ll explore how Hudi\u2019s auto file sizing, powered by a unique bin packing algorithm, can transform your data processing workflows."}),"\n",(0,n.jsx)(a.h2,{id:"understanding-small-file-challenges",children:"Understanding Small File Challenges"}),"\n",(0,n.jsx)(a.p,{children:"In big data environments, small files can pose a major challenge. Some major use-cases which can create lot of small files -"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Streaming Workloads"})," :\nWhen data is ingested in micro-batches, as is common in streaming workloads, the resulting files tend to be small. This can lead to a significant number of small files, especially for high-throughput streaming applications."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"High-Cardinality Partitioning"})," :\nExcessive partitioning, particularly on columns with high cardinality, can create a large number of small files. This can be especially problematic when dealing with large datasets and complex data schemas."]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"These small files can lead to several inefficiencies that can include increased metadata overhead, degraded read performance, and higher storage costs, particularly when using cloud storage solutions like Amazon S3."}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Increased Metadata Overhead"})," :\nMetadata is data about data, including information such as file names, sizes, creation dates, and other attributes that help systems manage and locate files. Each file, no matter how small, requires metadata to be tracked and managed. In environments where numerous small files are created, the amount of metadata generated can skyrocket. For instance, if a dataset consists of thousands of tiny files, the system must maintain metadata for each of these files. This can overwhelm metadata management systems, leading to longer lookup times and increased latency when accessing files."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Degraded Read Performance"})," :\nReading data from storage typically involves input/output (I/O) operations, which can be costly in terms of time and resources. When files are small, the number of I/O operations increases, as each small file needs to be accessed individually. This scenario can create bottlenecks, particularly in analytical workloads where speed is critical. Querying a large number of small files may result in significant delays, as the system spends more time opening and reading each file than processing the data itself."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Higher Cloud Costs"})," :\nMany cloud storage solutions, like Amazon S3, charge based on the total amount of data stored as well as the number of requests made. With numerous small files, not only does the total storage requirement increase, but the number of requests to access these files also grows. Each small file incurs additional costs due to the overhead associated with managing and accessing them. This can add up quickly, leading to unexpectedly high storage bills."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"High Query Load"})," :\nMultiple teams are querying these tables for various dashboards, ad-hoc analyses, and machine learning tasks. This leads to a high number of concurrent queries, including Spark jobs, which can significantly impact performance. All those queries/jobs will take a hit on both performance and cost."]}),"\n"]}),"\n",(0,n.jsx)(a.h3,{id:"impact-of-small-file",children:"Impact of Small File"}),"\n",(0,n.jsxs)(a.p,{children:["To demonstrate the impact of small files, we conducted a benchmarking using AWS EMR.\nDataset Used - TPC-DS 1 TB dataset ( ",(0,n.jsx)(a.a,{href:"https://www.tpc.org/tpcds/",children:"https://www.tpc.org/tpcds/"})," )\nCluster Configurations - 10 nodes (m5.4xlarge)\nSpark Configurations - Executors: 10 (16 cores 32 GB memory)\nDataset Generation - We generated two types of datasets in parquet format"]}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Optimized File Sizes which had ~100 MB sized files"}),"\n",(0,n.jsx)(a.li,{children:"Small File Sizes which had ~5-10 MB sized files\nExecution and Results"}),"\n",(0,n.jsx)(a.li,{children:"We executed 3 rounds of 99 standard TPC-DS queries on both datasets and measured the time taken by the queries."}),"\n",(0,n.jsx)(a.li,{children:"The results indicated that queries executed on small files were, on average, 30% slower compared to those executed on optimized file sizes."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"The following chart illustrates the average runtimes for the 99 queries across each round."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Impact of Small Files",src:t(23504).A+"",width:"3188",height:"1844"})}),"\n",(0,n.jsx)(a.h2,{id:"how-table-formats-solve-this-problem",children:"How table formats solve this problem"}),"\n",(0,n.jsx)(a.p,{children:"When it comes to managing small files in table formats, there are two primary strategies:"}),"\n",(0,n.jsxs)(a.h3,{id:"ingesting-data-as-is-and-optimizing-post-ingestion-",children:[(0,n.jsx)(a.strong,{children:"Ingesting Data As-Is and Optimizing Post-Ingestion"})," :"]}),"\n",(0,n.jsx)(a.p,{children:"In this approach, data, including small files, is initially ingested without immediate processing. After ingestion, various technologies provide functionalities to merge these small files into larger, more efficient partitions:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Hudi uses clustering to manage small files."}),"\n",(0,n.jsx)(a.li,{children:"Delta Lake utilizes the OPTIMIZE command."}),"\n",(0,n.jsx)(a.li,{children:"Iceberg offers the rewrite_data_files function."}),"\n"]}),"\n",(0,n.jsx)(a.h4,{id:"pros",children:"Pros:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Writing small files directly accelerates the ingestion process, enabling quick data availability\u2014especially beneficial for real-time or near-real-time applications."}),"\n",(0,n.jsx)(a.li,{children:"The initial write phase involves less data manipulation, as small files are simply appended. This streamlines workflows and eases the management of incoming data streams."}),"\n"]}),"\n",(0,n.jsx)(a.h4,{id:"cons",children:"Cons:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Until clustering or optimization is performed, small files may be exposed to readers, which can significantly slow down queries and potentially violate read SLAs."}),"\n",(0,n.jsx)(a.li,{children:"Just like with read performance, exposing small files to readers can lead to a high number of cloud storage API calls, which can increase cloud costs significantly."}),"\n",(0,n.jsx)(a.li,{children:"Managing table service jobs can become cumbersome. These jobs often can't run in parallel with ingestion tasks, leading to potential delays and resource contention."}),"\n"]}),"\n",(0,n.jsxs)(a.h3,{id:"managing-small-files-during-ingestion-only-",children:[(0,n.jsx)(a.strong,{children:"Managing Small Files During Ingestion Only"})," :"]}),"\n",(0,n.jsx)(a.p,{children:"Hudi offers a unique functionality that can handle small files during the ingestion only, ensuring that only larger files are stored in the table. This not only optimizes read performance but also significantly reduces storage costs.\nBy eliminating small files from the lake, Hudi addresses key challenges associated with data management, providing a streamlined solution that enhances both performance and cost efficiency."}),"\n",(0,n.jsx)(a.h2,{id:"how-hudi-helps-in-small-file-handling-during-ingestion",children:"How Hudi helps in small file handling during ingestion"}),"\n",(0,n.jsx)(a.p,{children:'Hudi automatically manages file sizing during insert and upsert operations. It employs a bin packing algorithm to handle small files effectively. A bin packing algorithm is a technique used to optimize file storage by grouping files of varying sizes into fixed-size containers, often referred to as "bins." This strategy aims to minimize the number of bins required to store all files efficiently. When writing data, Hudi identifies file groups of small files and merges new data into the same  group, resulting in optimized file sizes.'}),"\n",(0,n.jsx)(a.p,{children:"The diagram above illustrates how Hudi employs a bin packing algorithm to manage small files while using default parameters: a small file limit of 100 MB and a maximum file size of 120 MB."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"  ",src:t(90105).A+"",width:"1350",height:"632"})}),"\n",(0,n.jsxs)(a.p,{children:["Initially, the table contains the following files: F1 (110 MB), F2 (60 MB), F3 (20 MB), and F4 (20 MB).\nAfter processing a batch-1 of 150 MB, F2, F3, and F4 will all be classified as small files since they each fall below the 100 MB threshold. The first 60 MB will be allocated to F2, increasing its size to 120 MB. The remaining 90 MB will be assigned to F3, bringing its total to 110 MB.\nAfter processing batch-2 of 150 MB, only F4 will be classified as a small file. F3, now at 110 MB, will not be considered a small file since it exceeds the 100 MB limit. Therefore, an additional 100 MB will be allocated to F4, increasing its size to 120 MB, while the remaining 50 MB will create a new file of 50 MB.\nWe can refer this blog for in-depth details of the functionality  - ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2021/03/01/hudi-file-sizing/",children:"https://hudi.apache.org/blog/2021/03/01/hudi-file-sizing/"})]}),"\n",(0,n.jsx)(a.p,{children:"We use following configs to configure this -"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"hoodie.parquet.max.file.size (Default 128 MB)"}),"\nThis setting specifies the target size, in bytes, for Parquet files generated during Hudi write phases. The writer will attempt to create files that approach this target size. For example, if an existing file is 80 MB, the writer will allocate only 40 MB to that particular file group."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"hoodie.parquet.small.file.limit (Default 100 MB)"}),"\nThis setting defines the maximum file size for a data file to be classified as a small file. Files below this threshold are considered small files, prompting the system to allocate additional records to their respective file groups in subsequent write phases."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"hoodie.copyonwrite.record.size.estimate (Default 1024)"}),"\nThis setting represents the estimated average size of a record. If not explicitly specified, Hudi will dynamically compute this estimate based on commit metadata. Accurate record size estimation is essential for determining insert parallelism and efficiently bin-packing inserts into smaller files."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"hoodie.copyonwrite.insert.split.size (Default 500000)"}),"\nThis setting determines the number of records inserted into each partition or bucket during a write operation. The default value is based on the assumption of 100MB files with at least 1KB records, resulting in approximately 100,000 records per file. To accommodate potential variations, we overprovision to 500,000 records. As long as auto-tuning of splits is turned on, this only affects the first write, where there is no history to learn record sizes from."]}),"\n"]}),"\n",(0,n.jsxs)(a.li,{children:["\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"hoodie.merge.small.file.group.candidates.limit (Default1)"}),"\nThis setting specifies the maximum number of file groups whose base files meet the small-file limit that can be considered for appending records during an upsert operation. This parameter is applicable only to Merge-On-Read (MOR) tables."]}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["We can refer this blog to understand internal functionality how it works -\n",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/blog/2021/03/01/hudi-file-sizing/#during-write-vs-after-write",children:"https://hudi.apache.org/blog/2021/03/01/hudi-file-sizing/#during-write-vs-after-write"})]}),"\n",(0,n.jsx)(a.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,n.jsx)(a.p,{children:"Hudi's innovative approach to managing small files during ingestion positions it as a compelling choice in the lakehouse landscape. By automatically merging small files at the time of ingestion, it optimizes storage costs and enhances read performance, and alleviates users from the operational burden of maintaining their tables in an optimized state."}),"\n",(0,n.jsxs)(a.p,{children:["Unleash the power of Apache Hudi for your big data challenges! Head over to ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/",children:"https://hudi.apache.org/"})," and dive into the quickstarts to get started. Want to learn more? Join our vibrant Hudi community! Attend the monthly Community Call or hop into the Apache Hudi Slack to ask questions and gain deeper insights."]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},98750:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/02/24/Time-travel-operations-in-Hopsworks-Feature-Store","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-02-24-Time-travel-operations-in-Hopsworks-Feature-Store.mdx","source":"@site/blog/2021-02-24-Time-travel-operations-in-Hopsworks-Feature-Store.mdx","title":"Time travel operations in Hopsworks Feature Store","description":"Redirecting... please wait!!","date":"2021-02-24T00:00:00.000Z","tags":[{"inline":true,"label":"use-case","permalink":"/blog/tags/use-case"},{"inline":true,"label":"incremental processing","permalink":"/blog/tags/incremental-processing"},{"inline":true,"label":"feature store","permalink":"/blog/tags/feature-store"},{"inline":true,"label":"time travel query","permalink":"/blog/tags/time-travel-query"},{"inline":true,"label":"hopsworks","permalink":"/blog/tags/hopsworks"}],"readingTime":0.1,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Time travel operations in Hopsworks Feature Store","category":"blog","image":"/assets/images/blog/2021-02-24-featurestore_incremental_pull.png","tags":["use-case","incremental processing","feature store","time travel query","hopsworks"]},"unlisted":false,"prevItem":{"title":"Data Lakehouse: Building the Next Generation of Data Lakes using Apache Hudi","permalink":"/blog/2021/03/01/Data-Lakehouse-Building-the-Next-Generation-of-Data-Lakes-using-Apache-Hudi"},"nextItem":{"title":"Apache Hudi Key Generators","permalink":"/blog/2021/02/13/hudi-key-generators"}}')},99019:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/spark_read_optimized_view-3aeb662ab165a9702e1d73ee495107ec.png"},99112:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2021/12/20/New-features-from-Apache-Hudi-0.7.0-and-0.8.0-available-on-Amazon-EMR","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2021-12-20-New-features-from-Apache-Hudi-0.7.0-and-0.8.0-available-on-Amazon-EMR.mdx","source":"@site/blog/2021-12-20-New-features-from-Apache-Hudi-0.7.0-and-0.8.0-available-on-Amazon-EMR.mdx","title":"New features from Apache Hudi 0.7.0 and 0.8.0 available on Amazon EMR","description":"Redirecting... please wait!!","date":"2021-12-20T00:00:00.000Z","tags":[{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"amazon","permalink":"/blog/tags/amazon"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"Udit Mehrotra","socials":{},"key":null,"page":null},{"name":"Gagan Brahmi","socials":{},"key":null,"page":null}],"frontMatter":{"title":"New features from Apache Hudi 0.7.0 and 0.8.0 available on Amazon EMR","authors":[{"name":"Udit Mehrotra"},{"name":"Gagan Brahmi"}],"category":"blog","image":"/assets/images/blog/aws.jpg","tags":["blog","amazon"]},"unlisted":false,"prevItem":{"title":"Hudi Z-Order and Hilbert Space Filling Curves","permalink":"/blog/2021/12/29/hudi-zorder-and-hilbert-space-filling-curves"},"nextItem":{"title":"Lakehouse Concurrency Control: Are we too optimistic?","permalink":"/blog/2021/12/16/lakehouse-concurrency-control-are-we-too-optimistic"}}')},99150:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/image5-58bbba66797e915ba7518ad7e61bcd56.png"},99220:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(80332),n=t(74848),s=t(28453),r=t(9230);const o={title:"How to Use the New Hudi Streamer with Hudi 1.0.0 on EMR Serverless 7.5.0 | Hands-on Labs",author:"Soumil Shah",category:"blog",image:"/assets/images/blog/2025-01-05-how-use-new-hudi-streamer-100-emr-serverless-750-hands-on.png",tags:["blog","how-to","apache hudi","hudi 1.0.0","hudi streamer","amazon emr","linkedin"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://www.linkedin.com/pulse/how-use-new-hudi-streamer-100-emr-serverless-750-hands-on-soumil-shah-fxrae/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},99554:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(91112),n=t(74848),s=t(28453);const r={title:"Scaling Complex Data Workflows at Uber Using Apache Hudi",excerpt:"How Uber's Core Services Data Engineering team supports a wide range of use cases with Apache Hudi",author:"Ankit Shrivastava in collaboration with Dipankar",category:"blog",image:"/assets/images/blog/uber1200x600.jpg",tags:["Apache Hudi","Uber","Community"]},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"The Challenge: Scale, Latency, and Complexity",id:"the-challenge-scale-latency-and-complexity",level:2},{value:"Rigid SQL and Tight Coupling",id:"rigid-sql-and-tight-coupling",level:2},{value:"How We Solved It?",id:"how-we-solved-it",level:2},{value:"Final Architecture",id:"final-architecture",level:2},{value:"The Wins with Hudi",id:"the-wins-with-hudi",level:2},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const a={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.admonition,{title:"TL;DR",type:"tip",children:(0,n.jsx)(a.p,{children:"Uber\u2019s trip and order collection pipelines grew highly complex, with long runtimes, massive DAGs, and rigid SQL logic that hampered scalability and maintainability. By adopting Apache Hudi, Uber re-architected these pipelines to enable incremental processing, custom merge behavior, and rule-based functional transformations. This reduced runtime from 20 hours to 4 hours, improved test coverage to 95%, cut costs by 60%, and delivered a composable, explainable, and scalable data workflow architecture."})}),"\n",(0,n.jsx)(a.p,{children:"At Uber, the Core Services Data Engineering team supports a wide range of use cases across products like Uber Mobility and Uber Eats. One critical use case is computing the collection - the net payable amount - from a trip or an order. While this sounds straightforward at first, it quickly becomes a complex data problem when you factor in real-world scenarios like refunds, tips, driver disputes, location updates, and settlement adjustments across multiple verticals."}),"\n",(0,n.jsxs)(a.p,{children:["To solve this problem at scale, Uber re-architected their pipelines using ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/",children:"Apache Hudi"})," to enable low-latency, incremental, and rule-based processing. This post outlines the challenges they faced, the architectural shifts they made, and the measurable outcomes they achieved in production."]}),"\n",(0,n.jsx)(a.h2,{id:"the-challenge-scale-latency-and-complexity",children:"The Challenge: Scale, Latency, and Complexity"}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/figure2_uber.png",alt:"challenge",width:"800",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"Our original data pipelines were processing nearly 90 million records a day, but the nature of updates made them inefficient. For instance, a trip taken three years ago could still be updated due to a late settlement. Our statistical analysis showed most updates occur within 180 days, so we designed the system to read and write a 180-day window every day - leading to severe read and write amplification."}),"\n",(0,n.jsx)(a.p,{children:"The pipeline itself was a massive DAG with over 50\u201360 tasks, taking close to 20 hours to complete. These long runtimes made recovery difficult and introduced operational risks. Making a change meant tracing the logic across this sprawling DAG, which affected developer productivity and increased the chances of regressions."}),"\n",(0,n.jsx)(a.p,{children:"Despite the large window, we still missed updates that fell outside the 180-day mark, leading to data quality issues. The long development cycles and heavy debugging effort further hindered our ability to iterate and maintain the system."}),"\n",(0,n.jsx)(a.h2,{id:"rigid-sql-and-tight-coupling",children:"Rigid SQL and Tight Coupling"}),"\n",(0,n.jsx)(a.p,{children:"Digging deeper, we identified multiple underlying causes. The pipeline relied heavily on SQL for all transformations. But expressing the evolving business rules for different Uber products in SQL was limiting. The logic had grown too complex to be managed effectively, and granular transformations led to a proliferation of intermediate stages. This made unit testing and debugging difficult, and the absence of structured logging made observability poor."}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/figure3_uber.png",alt:"redshift",width:"800",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"Additionally, data and logic were tightly coupled. The system often required joining tables at very fine granularities, introducing redundancy and making logic harder to reason about. Complex joins, table scans, and late-arriving data amplified processing costs. It was also difficult to trace how a specific row was transformed through the DAG, making explainability a real challenge."}),"\n",(0,n.jsx)(a.h2,{id:"how-we-solved-it",children:"How We Solved It?"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.strong,{children:"Solving Read Amplification"})}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["The first step in addressing inefficiencies was eliminating the brute-force strategy of scanning and processing a 180-day window of data on every pipeline run. With the help of Apache Hudi\u2019s ",(0,n.jsxs)(a.a,{href:"https://hudi.apache.org/docs/table_types#incremental-queries",children:[(0,n.jsx)(a.em,{children:"incremental"})," ",(0,n.jsx)(a.em,{children:"read"})]})," capabilities, we restructured the ingestion layer to read only the records that had mutated since the last checkpoint."]}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/fig4_uber.png",alt:"redshift",width:"800",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"We introduced an intermediate Hudi table that consolidated all related records for a trip or order into a single row, using complex data types such as structs, lists, and maps. This model allowed us to capture the complete state of a trip - including all updates, tips, disputes, and refunds in one place, without scattering information across multiple joins."}),"\n",(0,n.jsx)(a.p,{children:"By using this intermediate table as the foundation, all downstream logic could operate on change-driven inputs. The result was a pipeline that avoided unnecessary scans, improved correctness by processing all real changes (not just those in a time window), and reduced overall I/O dramatically."}),"\n",(0,n.jsxs)(a.ol,{start:"2",children:["\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.strong,{children:"Eliminating Self Joins with Custom Payloads"})}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Self joins - especially for reconciling updates to the same trip were one of the costliest operations in our original pipeline."}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/fig5_uber.png",alt:"redshift",width:"800",align:"middle"}),"\n",(0,n.jsxs)(a.p,{children:["To solve this, we implemented a custom Hudi payload class that allows us to control how updates are applied during the merge phase. This class overrides methods such as ",(0,n.jsx)(a.code,{children:"combineAndGetUpdateValue"})," and ",(0,n.jsx)(a.code,{children:"getInsertValue"}),", and executes the merge logic as part of the write path, eliminating the need for a full table scan or shuffle."]}),"\n",(0,n.jsx)(a.p,{children:"This approach helped us efficiently handle updates to complex, nested records in the intermediate Hudi table, and dramatically reduced the cost associated with self joins."}),"\n",(0,n.jsxs)(a.ol,{start:"3",children:["\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.strong,{children:"Simplifying Processing with a Rule-Based Framework"})}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"To move away from the rigidity of SQL, we designed a rule engine framework based on functional programming principles."}),"\n",(0,n.jsx)(a.p,{children:"Instead of expressing business logic as large, monolithic SQL queries, we cast each input row (from the intermediate table) into a strongly typed object (e.g., a Trip object). These objects were then passed through a series of declarative rules - each consisting of a condition and an action."}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/fig6_uber.png",alt:"redshift",width:"800",align:"middle"}),"\n",(0,n.jsxs)(a.p,{children:["This framework was implemented as a custom ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/hoodie_streaming_ingestion#transformers",children:(0,n.jsx)(a.em,{children:"transformer"})})," plugged into ",(0,n.jsx)(a.a,{href:"https://hudi.apache.org/docs/hoodie_streaming_ingestion",children:"HudiStreamer"}),". The transformer intercepts the ingested data, applies the rule engine logic, and emits the transformed object to the final Hudi output table. We also built in capabilities for:"]}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Logging and observability (for metrics and debugging)"}),"\n",(0,n.jsx)(a.li,{children:"Unreachable state detection (flagging invalid rows)"}),"\n",(0,n.jsx)(a.li,{children:"Unit testing support for each rule independently"}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"This architecture replaced the huge DAG with modular, testable, and composable rule definitions, dramatically improving developer productivity and data pipeline clarity."}),"\n",(0,n.jsx)(a.h2,{id:"final-architecture",children:"Final Architecture"}),"\n",(0,n.jsx)("img",{src:"/assets/images/blog/fig7_uber.png",alt:"redshift",width:"800",align:"middle"}),"\n",(0,n.jsx)(a.p,{children:"The redesigned system follows a clean, composable structure:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Incremental ingestion from the data lake is done using HudiStreamer, which writes to an intermediate Hudi table."}),"\n",(0,n.jsx)(a.li,{children:"The intermediate table consolidates all records for a trip using complex types, serving as the central input for downstream processing."}),"\n",(0,n.jsx)(a.li,{children:"A custom Transformer intercepts the records, casts them into typed domain objects, and passes them through a rule engine."}),"\n",(0,n.jsx)(a.li,{children:"The rule engine applies business logic declaratively and emits fully processed objects."}),"\n",(0,n.jsx)(a.li,{children:"The output is written to a final Hudi table that supports efficient, incremental consumption."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"This design eliminates redundant scans, reduces shuffle overhead, enables full test coverage, and offers detailed observability across all transformation stages."}),"\n",(0,n.jsx)(a.h2,{id:"the-wins-with-hudi",children:"The Wins with Hudi"}),"\n",(0,n.jsx)(a.p,{children:"The improvements were substantial and measurable:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Runtime reduced from ~20 hours to ~4 hours (~75% improvement)"}),"\n",(0,n.jsx)(a.li,{children:"Test coverage increased to 95% for transformation logic"}),"\n",(0,n.jsx)(a.li,{children:"Single run cost reduced by 60%"}),"\n",(0,n.jsx)(a.li,{children:"Improved data completeness, processing all updates\u2014not just those in a statistical window"}),"\n",(0,n.jsx)(a.li,{children:"Reusable and modular logic, reducing DAG complexity"}),"\n",(0,n.jsx)(a.li,{children:"Higher developer productivity, with isolated unit testing and simplified debugging"}),"\n",(0,n.jsx)(a.li,{children:"Improved self-join performance through custom payloads"}),"\n",(0,n.jsx)(a.li,{children:"A generic rule engine design, portable across Spark and Flink"}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Apache Hudi has been central to Nexus\u2019 success, providing the core data lake storage layer for scalable ingestion, updates, and metadata management. It enables fast, incremental updates at massive scale while maintaining transactional guarantees on top of Amazon S3."}),"\n",(0,n.jsx)(a.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,n.jsx)(a.p,{children:"By redesigning the system around Apache Hudi and adopting functional, rule-based processing, Uber was able to transform a brittle, long-running pipeline into a maintainable and efficient architecture. The changes allowed them to scale their data workflows to meet the needs of complex, multi-product use cases without compromising on performance, observability, or data quality."}),"\n",(0,n.jsx)(a.p,{children:"This work highlights the power of pairing the right storage format with a principled architectural approach. Apache Hudi was instrumental in helping achieve these outcomes and continues to play a key role in Uber\u2019s evolving data platform."}),"\n",(0,n.jsxs)(a.p,{children:["This blog is based on Uber\u2019s presentation at the Apache Hudi Community Sync. If you are interested in watching the recorded version of the video, you can find it ",(0,n.jsx)(a.a,{href:"https://www.youtube.com/watch?v=VpdimpH_nsI",children:"here"}),"."]}),"\n",(0,n.jsx)(a.hr,{})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},99613:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(90212),n=t(74848),s=t(28453),r=t(9230);const o={title:"How Hudl built a cost-optimized AWS Glue pipeline with Apache Hudi datasets",authors:[{name:"Indira Balakrishnan"},{name:"Ramzi Yassine"},{name:"Swagat Kulkarni"}],category:"blog",image:"/assets/images/blog/2022-11-10_How_to_build_a_cost_optimized_glue_pipeline_with_apache_hudi.png",tags:["use-case","cost efficiency","incremental processing","near real-time analytics","amazon"]},l=void 0,d={authorsImageUrls:[void 0,void 0,void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/how-hudl-built-a-cost-optimized-aws-glue-pipeline-with-apache-hudi-datasets/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},99645:e=>{"use strict";e.exports=JSON.parse('{"permalink":"/blog/2025/10/22/Partition_Stats_Enhancing_Column_Stats_in_Hudi_1.0","editUrl":"https://github.com/apache/hudi/edit/asf-site/website/blog/blog/2025-10-22-Partition_Stats_Enhancing_Column_Stats_in_Hudi_1.0.md","source":"@site/blog/2025-10-22-Partition_Stats_Enhancing_Column_Stats_in_Hudi_1.0.md","title":"Partition Stats: Enhancing Column Stats in Hudi 1.0","description":"For those tracking Apache Hudi\'s performance enhancements, the introduction of the column stats index was a significant development, as detailed in this blog. It represented a major advancement for query optimization by implementing a straightforward yet highly effective concept: storing lightweight, file-level statistics (such as min/max values and null counts) for specific columns. This provided Hudi\'s query engine a substantial performance improvement.","date":"2025-10-22T00:00:00.000Z","tags":[{"inline":true,"label":"hudi","permalink":"/blog/tags/hudi"},{"inline":true,"label":"indexing","permalink":"/blog/tags/indexing"},{"inline":true,"label":"data lakehouse","permalink":"/blog/tags/data-lakehouse"},{"inline":true,"label":"data skipping","permalink":"/blog/tags/data-skipping"}],"readingTime":7.35,"hasTruncateMarker":false,"authors":[{"name":"Aditya Goenka and Shiyan Xu","key":null,"page":null}],"frontMatter":{"title":"Partition Stats: Enhancing Column Stats in Hudi 1.0","excerpt":"","author":"Aditya Goenka and Shiyan Xu","category":"blog","image":"/assets/images/blog/2025-10-22-Partition_Stats_Enhancing_Column_Stats_in_Hudi_1.0/fig1.jpg","tags":["hudi","indexing","data lakehouse","data skipping"]},"unlisted":false,"prevItem":{"title":"Deep Dive Into Hudi\u2019s Indexing Subsystem (Part 1 of 2)","permalink":"/blog/2025/10/29/deep-dive-into-hudis-indexing-subsystem-part-1-of-2"},"nextItem":{"title":"Modernizing Upstox\'s Data Platform with Apache Hudi, dbt, and EMR Serverless","permalink":"/blog/2025/10/16/Modernizing-Upstox-Data-Platform-with-Apache-Hudi-DBT-and-EMR-Serverless"}}')},99662:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});const i=t.p+"assets/images/jdpost-image4-0bc644b6a57a3145ebccc064d497349e.jpg"},99697:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(3874),n=t(74848),s=t(28453),r=t(9230);const o={title:"Use AWS Data Exchange to seamlessly share Apache Hudi datasets",author:"Saurabh Bhutyani, Ankith Ede, and Chandra Krishnan",category:"blog",image:"/assets/images/blog/2024-05-22-use-aws-data-exchange-to-seamlessly-share-apache-hudi-datasets.png",tags:["blog","apache hudi","aws data exchange","amazon emr","amazon s3","amazon athena","data sahring","amazon"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://aws.amazon.com/blogs/big-data/use-aws-data-exchange-to-seamlessly-share-apache-hudi-datasets/",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}},99996:(e,a,t)=>{"use strict";t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var i=t(64456),n=t(74848),s=t(28453),r=t(9230);const o={title:"Understanding Apache Hudi's Consistency Model Part 1",author:"Jack Vanlightly",category:"blog",image:"/assets/images/blog/2024-04-24-understanding-apache-hudi-consistency-model-part-1.png",tags:["blog","apache hudi","table formats","ACID","consistency","cow","concurrency control","multi writer","tla+ specification","jack-vanlightly"]},l=void 0,d={authorsImageUrls:[void 0]},c=[];function h(e){return(0,n.jsx)(r.A,{url:"https://jack-vanlightly.com/analyses/2024/4/24/understanding-apache-hudi-consistency-model-part-1",children:"Redirecting... please wait!! "})}function u(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h()}}}]);