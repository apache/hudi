<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Querying Hudi Datasets - Apache Hudi</title>
<meta name="description" content="Conceptually, Hudi stores data physically once on DFS, while providing 3 logical views on top, as explained before. Once the dataset is synced to the Hive metastore, it provides external Hive tables backed by Hudi’s custom inputformats. Once the proper hudibundle has been provided, the dataset can be queried by popular query engines like Hive, Spark and Presto.">

<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Apache Hudi">
<meta property="og:title" content="Querying Hudi Datasets">
<meta property="og:url" content="https://hudi.apache.org/docs/querying_data.html">


  <meta property="og:description" content="Conceptually, Hudi stores data physically once on DFS, while providing 3 logical views on top, as explained before. Once the dataset is synced to the Hive metastore, it provides external Hive tables backed by Hudi’s custom inputformats. Once the proper hudibundle has been provided, the dataset can be queried by popular query engines like Hive, Spark and Presto.">





  <meta property="article:modified_time" content="2019-12-30T14:59:57-05:00">







<!-- end _includes/seo.html -->


<!--<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Apache Hudi Feed">-->

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



<link rel="icon" type="image/x-icon" href="/assets/images/favicon.ico">
<link rel="stylesheet" href="/assets/css/font-awesome.min.css">

  </head>

  <body class="layout--single">
    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap" id="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/hudi.png" alt=""></a>
        
        <a class="site-title" href="/">
          Apache Hudi
          <span class="site-subtitle">0.5.0-incubating(incubating)</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/docs/quick-start-guide.html" target="_self" >Documentation</a>
            </li><li class="masthead__menu-item">
              <a href="/community.html" target="_self" >Community</a>
            </li><li class="masthead__menu-item">
              <a href="/roadmap.html" target="_self" >Roadmap</a>
            </li><li class="masthead__menu-item">
              <a href="/activity.html" target="_self" >Activities</a>
            </li><li class="masthead__menu-item">
              <a href="https://cwiki.apache.org/confluence/display/HUDI/FAQ" target="_blank" >FAQ</a>
            </li><li class="masthead__menu-item">
              <a href="/releases.html" target="_self" >Releases</a>
            </li></ul>
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      <div id="main" role="main">
  
  <div class="sidebar sticky">

  

  
    
      
      
      
      
    


    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">Toggle menu</label>
  <ul class="nav__items">
    
      <li>
        
          <span class="nav__sub-title">Getting Started</span>
        

        
        <ul>
          
            
            

            
            

            
              <li><a href="/versions/0.5.0-incubating/docs/quick-start-guide.html" class="">Quick Start</a></li>
            

          
            
            

            
            

            
              <li><a href="/versions/0.5.0-incubating/docs/structure.html" class="">Structure</a></li>
            

          
            
            

            
            

            
              <li><a href="/versions/0.5.0-incubating/docs/use_cases.html" class="">User Cases</a></li>
            

          
            
            

            
            

            
              <li><a href="/versions/0.5.0-incubating/docs/powered_by.html" class="">Talks & Powered By</a></li>
            

          
            
            

            
            

            
              <li><a href="/versions/0.5.0-incubating/docs/comparison.html" class="">Comparison</a></li>
            

          
        </ul>
        
      </li>
    
      <li>
        
          <span class="nav__sub-title">Documentation</span>
        

        
        <ul>
          
            
            

            
            

            
              <li><a href="/versions/0.5.0-incubating/docs/concepts.html" class="">Concepts</a></li>
            

          
            
            

            
            

            
              <li><a href="/versions/0.5.0-incubating/docs/writing_data.html" class="">Writing Data</a></li>
            

          
            
            

            
            

            
              <li><a href="/versions/0.5.0-incubating/docs/querying_data.html" class="active">Querying Data</a></li>
            

          
            
            

            
            

            
              <li><a href="/versions/0.5.0-incubating/docs/configurations.html" class="">Configuration</a></li>
            

          
            
            

            
            

            
              <li><a href="/versions/0.5.0-incubating/docs/performance.html" class="">Performance</a></li>
            

          
            
            

            
            

            
              <li><a href="/versions/0.5.0-incubating/docs/admin_guide.html" class="">Administering</a></li>
            

          
        </ul>
        
      </li>
    
      <li>
        
          <span class="nav__sub-title">Meta Info</span>
        

        
        <ul>
          
            
            

            
            

            
              <li><a href="/versions/0.5.0-incubating/docs/docs-versions.html" class="">Docs Versions</a></li>
            

          
            
            

            
            

            
              <li><a href="/versions/0.5.0-incubating/docs/privacy.html" class="">Privacy Policy</a></li>
            

          
        </ul>
        
      </li>
    
  </ul>
</nav>
    
  
  </div>


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Querying Hudi Datasets
</h1>
        </header>
      

      <section class="page__content" itemprop="text">
        
        <aside class="sidebar__right sticky">
          <nav class="toc">
            <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> Section Nav</h4></header>
            <ul class="toc__menu">
  <li><a href="#hive">Hive</a>
    <ul>
      <li><a href="#read-optimized-table">Read Optimized table</a></li>
      <li><a href="#real-time-table">Real time table</a></li>
      <li><a href="#incremental-pulling">Incremental Pulling</a></li>
    </ul>
  </li>
  <li><a href="#spark">Spark</a>
    <ul>
      <li><a href="#read-optimized-table-1">Read Optimized table</a></li>
      <li><a href="#spark-rt-view">Real time table</a></li>
      <li><a href="#spark-incr-pull">Incremental Pulling</a></li>
    </ul>
  </li>
  <li><a href="#presto">Presto</a></li>
</ul>
          </nav>
        </aside>
        
        <p>Conceptually, Hudi stores data physically once on DFS, while providing 3 logical views on top, as explained <a href="concepts.html#views">before</a>. 
Once the dataset is synced to the Hive metastore, it provides external Hive tables backed by Hudi’s custom inputformats. Once the proper hudi
bundle has been provided, the dataset can be queried by popular query engines like Hive, Spark and Presto.</p>

<p>Specifically, there are two Hive tables named off <a href="configurations.html#TABLE_NAME_OPT_KEY">table name</a> passed during write. 
For e.g, if <code class="highlighter-rouge">table name = hudi_tbl</code>, then we get</p>

<ul>
  <li><code class="highlighter-rouge">hudi_tbl</code> realizes the read optimized view of the dataset backed by <code class="highlighter-rouge">HoodieParquetInputFormat</code>, exposing purely columnar data.</li>
  <li><code class="highlighter-rouge">hudi_tbl_rt</code> realizes the real time view of the dataset  backed by <code class="highlighter-rouge">HoodieParquetRealtimeInputFormat</code>, exposing merged view of base and log data.</li>
</ul>

<p>As discussed in the concepts section, the one key primitive needed for <a href="https://www.oreilly.com/ideas/ubers-case-for-incremental-processing-on-hadoop">incrementally processing</a>,
is <code class="highlighter-rouge">incremental pulls</code> (to obtain a change stream/log from a dataset). Hudi datasets can be pulled incrementally, which means you can get ALL and ONLY the updated &amp; new rows 
since a specified instant time. This, together with upserts, are particularly useful for building data pipelines where 1 or more source Hudi tables are incrementally pulled (streams/facts),
joined with other tables (datasets/dimensions), to <a href="writing_data.html">write out deltas</a> to a target Hudi dataset. Incremental view is realized by querying one of the tables above, 
with special configurations that indicates to query planning that only incremental data needs to be fetched out of the dataset.</p>

<p>In sections, below we will discuss in detail how to access all the 3 views on each query engine.</p>

<h2 id="hive">Hive</h2>

<p>In order for Hive to recognize Hudi datasets and query correctly, the HiveServer2 needs to be provided with the <code class="highlighter-rouge">hudi-hadoop-mr-bundle-x.y.z-SNAPSHOT.jar</code> 
in its <a href="https://www.cloudera.com/documentation/enterprise/5-6-x/topics/cm_mc_hive_udf.html#concept_nc3_mms_lr">aux jars path</a>. This will ensure the input format 
classes with its dependencies are available for query planning &amp; execution.</p>

<h3 id="read-optimized-table">Read Optimized table</h3>
<p>In addition to setup above, for beeline cli access, the <code class="highlighter-rouge">hive.input.format</code> variable needs to be set to the  fully qualified path name of the 
inputformat <code class="highlighter-rouge">org.apache.hudi.hadoop.HoodieParquetInputFormat</code>. For Tez, additionally the <code class="highlighter-rouge">hive.tez.input.format</code> needs to be set 
to <code class="highlighter-rouge">org.apache.hadoop.hive.ql.io.HiveInputFormat</code></p>

<h3 id="real-time-table">Real time table</h3>
<p>In addition to installing the hive bundle jar on the HiveServer2, it needs to be put on the hadoop/hive installation across the cluster, so that
queries can pick up the custom RecordReader as well.</p>

<h3 id="incremental-pulling">Incremental Pulling</h3>

<p><code class="highlighter-rouge">HiveIncrementalPuller</code> allows incrementally extracting changes from large fact/dimension tables via HiveQL, combining the benefits of Hive (reliably process complex SQL queries) and 
incremental primitives (speed up query by pulling tables incrementally instead of scanning fully). The tool uses Hive JDBC to run the hive query and saves its results in a temp table.
that can later be upserted. Upsert utility (<code class="highlighter-rouge">HoodieDeltaStreamer</code>) has all the state it needs from the directory structure to know what should be the commit time on the target table.
e.g: <code class="highlighter-rouge">/app/incremental-hql/intermediate/{source_table_name}_temp/{last_commit_included}</code>.The Delta Hive table registered will be of the form <code class="highlighter-rouge">{tmpdb}.{source_table}_{last_commit_included}</code>.</p>

<p>The following are the configuration options for HiveIncrementalPuller</p>

<table>
  <tbody>
    <tr>
      <td><strong>Config</strong></td>
      <td><strong>Description</strong></td>
      <td><strong>Default</strong></td>
    </tr>
    <tr>
      <td>hiveUrl</td>
      <td>Hive Server 2 URL to connect to</td>
      <td> </td>
    </tr>
    <tr>
      <td>hiveUser</td>
      <td>Hive Server 2 Username</td>
      <td> </td>
    </tr>
    <tr>
      <td>hivePass</td>
      <td>Hive Server 2 Password</td>
      <td> </td>
    </tr>
    <tr>
      <td>queue</td>
      <td>YARN Queue name</td>
      <td> </td>
    </tr>
    <tr>
      <td>tmp</td>
      <td>Directory where the temporary delta data is stored in DFS. The directory structure will follow conventions. Please see the below section.</td>
      <td> </td>
    </tr>
    <tr>
      <td>extractSQLFile</td>
      <td>The SQL to execute on the source table to extract the data. The data extracted will be all the rows that changed since a particular point in time.</td>
      <td> </td>
    </tr>
    <tr>
      <td>sourceTable</td>
      <td>Source Table Name. Needed to set hive environment properties.</td>
      <td> </td>
    </tr>
    <tr>
      <td>targetTable</td>
      <td>Target Table Name. Needed for the intermediate storage directory structure.</td>
      <td> </td>
    </tr>
    <tr>
      <td>sourceDataPath</td>
      <td>Source DFS Base Path. This is where the Hudi metadata will be read.</td>
      <td> </td>
    </tr>
    <tr>
      <td>targetDataPath</td>
      <td>Target DFS Base path. This is needed to compute the fromCommitTime. This is not needed if fromCommitTime is specified explicitly.</td>
      <td> </td>
    </tr>
    <tr>
      <td>tmpdb</td>
      <td>The database to which the intermediate temp delta table will be created</td>
      <td>hoodie_temp</td>
    </tr>
    <tr>
      <td>fromCommitTime</td>
      <td>This is the most important parameter. This is the point in time from which the changed records are pulled from.</td>
      <td> </td>
    </tr>
    <tr>
      <td>maxCommits</td>
      <td>Number of commits to include in the pull. Setting this to -1 will include all the commits from fromCommitTime. Setting this to a value &gt; 0, will include records that ONLY changed in the specified number of commits after fromCommitTime. This may be needed if you need to catch up say 2 commits at a time.</td>
      <td>3</td>
    </tr>
    <tr>
      <td>help</td>
      <td>Utility Help</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>Setting fromCommitTime=0 and maxCommits=-1 will pull in the entire source dataset and can be used to initiate backfills. If the target dataset is a Hudi dataset,
then the utility can determine if the target dataset has no commits or is behind more than 24 hour (this is configurable),
it will automatically use the backfill configuration, since applying the last 24 hours incrementally could take more time than doing a backfill. The current limitation of the tool
is the lack of support for self-joining the same table in mixed mode (normal and incremental modes).</p>

<p><strong>NOTE on Hive queries that are executed using Fetch task:</strong>
Since Fetch tasks invoke InputFormat.listStatus() per partition, Hoodie metadata can be listed in
every such listStatus() call. In order to avoid this, it might be useful to disable fetch tasks
using the hive session property for incremental queries: <code class="highlighter-rouge">set hive.fetch.task.conversion=none;</code> This
would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma
separated) and calls InputFormat.listStatus() only once with all those partitions.</p>

<h2 id="spark">Spark</h2>

<p>Spark provides much easier deployment &amp; management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi datasets in Spark.</p>

<ul>
  <li><strong>Hudi DataSource</strong> : Supports Read Optimized, Incremental Pulls similar to how standard datasources (e.g: <code class="highlighter-rouge">spark.read.parquet</code>) work.</li>
  <li><strong>Read as Hive tables</strong> : Supports all three views, including the real time view, relying on the custom Hudi input formats again like Hive.</li>
</ul>

<p>In general, your spark job needs a dependency to <code class="highlighter-rouge">hudi-spark</code> or <code class="highlighter-rouge">hudi-spark-bundle-x.y.z.jar</code> needs to be on the class path of driver &amp; executors (hint: use <code class="highlighter-rouge">--jars</code> argument)</p>

<h3 id="read-optimized-table-1">Read Optimized table</h3>

<p>To read RO table as a Hive table using SparkSQL, simply push a path filter into sparkContext as follows. 
This method retains Spark built-in optimizations for reading Parquet files like vectorized reading on Hudi tables.</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">spark</span><span class="o">.</span><span class="py">sparkContext</span><span class="o">.</span><span class="py">hadoopConfiguration</span><span class="o">.</span><span class="py">setClass</span><span class="o">(</span><span class="s">"mapreduce.input.pathFilter.class"</span><span class="o">,</span> <span class="n">classOf</span><span class="o">[</span><span class="kt">org.apache.hudi.hadoop.HoodieROTablePathFilter</span><span class="o">],</span> <span class="n">classOf</span><span class="o">[</span><span class="kt">org.apache.hadoop.fs.PathFilter</span><span class="o">]);</span>
</code></pre></div></div>

<p>If you prefer to glob paths on DFS via the datasource, you can simply do something like below to get a Spark dataframe to work with.</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">Dataset</span><span class="o">&lt;</span><span class="nc">Row</span><span class="o">&gt;</span> <span class="n">hoodieROViewDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">read</span><span class="o">().</span><span class="na">format</span><span class="o">(</span><span class="s">"org.apache.hudi"</span><span class="o">)</span>
<span class="c1">// pass any path glob, can include hudi &amp; non-hudi datasets</span>
<span class="o">.</span><span class="na">load</span><span class="o">(</span><span class="s">"/glob/path/pattern"</span><span class="o">);</span>
</code></pre></div></div>

<h3 id="spark-rt-view">Real time table</h3>
<p>Currently, real time table can only be queried as a Hive table in Spark. In order to do this, set <code class="highlighter-rouge">spark.sql.hive.convertMetastoreParquet=false</code>, forcing Spark to fallback 
to using the Hive Serde to read the data (planning/executions is still Spark).</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">$</span> <span class="n">spark</span><span class="o">-</span><span class="n">shell</span> <span class="o">--</span><span class="n">jars</span> <span class="n">hudi</span><span class="o">-</span><span class="n">spark</span><span class="o">-</span><span class="n">bundle</span><span class="o">-</span><span class="n">x</span><span class="o">.</span><span class="na">y</span><span class="o">.</span><span class="na">z</span><span class="o">-</span><span class="no">SNAPSHOT</span><span class="o">.</span><span class="na">jar</span> <span class="o">--</span><span class="n">driver</span><span class="o">-</span><span class="kd">class</span><span class="err">-</span><span class="nc">path</span> <span class="o">/</span><span class="n">etc</span><span class="o">/</span><span class="n">hive</span><span class="o">/</span><span class="n">conf</span>  <span class="o">--</span><span class="n">packages</span> <span class="n">com</span><span class="o">.</span><span class="na">databricks</span><span class="o">:</span><span class="n">spark</span><span class="o">-</span><span class="n">avro_2</span><span class="o">.</span><span class="mi">11</span><span class="o">:</span><span class="mf">4.0</span><span class="o">.</span><span class="mi">0</span> <span class="o">--</span><span class="n">conf</span> <span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">.</span><span class="na">hive</span><span class="o">.</span><span class="na">convertMetastoreParquet</span><span class="o">=</span><span class="kc">false</span> <span class="o">--</span><span class="n">num</span><span class="o">-</span><span class="n">executors</span> <span class="mi">10</span> <span class="o">--</span><span class="n">driver</span><span class="o">-</span><span class="n">memory</span> <span class="mi">7</span><span class="n">g</span> <span class="o">--</span><span class="n">executor</span><span class="o">-</span><span class="n">memory</span> <span class="mi">2</span><span class="n">g</span>  <span class="o">--</span><span class="n">master</span> <span class="n">yarn</span><span class="o">-</span><span class="n">client</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="n">sqlContext</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">"select count(*) from hudi_rt where datestr = '2016-10-02'"</span><span class="o">).</span><span class="na">show</span><span class="o">()</span>
</code></pre></div></div>

<h3 id="spark-incr-pull">Incremental Pulling</h3>
<p>The <code class="highlighter-rouge">hudi-spark</code> module offers the DataSource API, a more elegant way to pull data from Hudi dataset and process it via Spark.
A sample incremental pull, that will obtain all records written since <code class="highlighter-rouge">beginInstantTime</code>, looks like below.</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="nc">Dataset</span><span class="o">&lt;</span><span class="nc">Row</span><span class="o">&gt;</span> <span class="n">hoodieIncViewDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">read</span><span class="o">()</span>
     <span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">"org.apache.hudi"</span><span class="o">)</span>
     <span class="o">.</span><span class="na">option</span><span class="o">(</span><span class="nc">DataSourceReadOptions</span><span class="o">.</span><span class="na">VIEW_TYPE_OPT_KEY</span><span class="o">(),</span>
             <span class="nc">DataSourceReadOptions</span><span class="o">.</span><span class="na">VIEW_TYPE_INCREMENTAL_OPT_VAL</span><span class="o">())</span>
     <span class="o">.</span><span class="na">option</span><span class="o">(</span><span class="nc">DataSourceReadOptions</span><span class="o">.</span><span class="na">BEGIN_INSTANTTIME_OPT_KEY</span><span class="o">(),</span>
            <span class="o">&lt;</span><span class="n">beginInstantTime</span><span class="o">&gt;)</span>
     <span class="o">.</span><span class="na">load</span><span class="o">(</span><span class="n">tablePath</span><span class="o">);</span> <span class="c1">// For incremental view, pass in the root/base path of dataset</span>
</code></pre></div></div>

<p>Please refer to <a href="configurations.html#spark-datasource">configurations</a> section, to view all datasource options.</p>

<p>Additionally, <code class="highlighter-rouge">HoodieReadClient</code> offers the following functionality using Hudi’s implicit indexing.</p>

<table>
  <tbody>
    <tr>
      <td><strong>API</strong></td>
      <td><strong>Description</strong></td>
    </tr>
    <tr>
      <td>read(keys)</td>
      <td>Read out the data corresponding to the keys as a DataFrame, using Hudi’s own index for faster lookup</td>
    </tr>
    <tr>
      <td>filterExists()</td>
      <td>Filter out already existing records from the provided RDD[HoodieRecord]. Useful for de-duplication</td>
    </tr>
    <tr>
      <td>checkExists(keys)</td>
      <td>Check if the provided keys exist in a Hudi dataset</td>
    </tr>
  </tbody>
</table>

<h2 id="presto">Presto</h2>

<p>Presto is a popular query engine, providing interactive query performance. Hudi RO tables can be queries seamlessly in Presto. 
This requires the <code class="highlighter-rouge">hudi-presto-bundle</code> jar to be placed into <code class="highlighter-rouge">&lt;presto_install&gt;/plugin/hive-hadoop2/</code>, across the installation.</p>

        
      </section>

      <a href="#masthead__inner-wrap" class="back-to-top">Back to top &uarr;</a>


      

    </div>

  </article>

</div>

    </div>

    <div class="page__footer">
      <footer>
        
<div class="row">
  <div class="col-lg-12 footer">
    <p>
      <a class="footer-link-img" href="https://apache.org">
        <img width="250px" src="/assets/images/asf_logo.svg" alt="The Apache Software Foundation">
      </a>
    </p>
    <p>
      Copyright &copy; <span id="copyright-year">2019</span> <a href="https://apache.org">The Apache Software Foundation</a>, Licensed under the Apache License, Version 2.0.
      Apache and the Apache feather logo are trademarks of The Apache Software Foundation. <a href="/docs/privacy">Privacy Policy</a>
      <br>
      Apache Hudi is an effort undergoing incubation at The Apache Software Foundation (ASF), sponsored by the <a href="http://incubator.apache.org/">Apache Incubator</a>.
      Incubation is required of all newly accepted projects until a further review indicates that the infrastructure, communications, and decision making process have
      stabilized in a manner consistent with other successful ASF projects. While incubation status is not necessarily a
      reflection of the completeness or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF.
    </p>
  </div>
</div>
      </footer>
    </div>

    
<script src="/assets/js/main.min.js"></script>


  </body>
</html>