<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Use Cases - Apache Hudi</title>
<meta name="description" content="Near Real-Time Ingestion">

<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Apache Hudi">
<meta property="og:title" content="Use Cases">
<meta property="og:url" content="https://hudi.apache.org/docs/use_cases.html">


  <meta property="og:description" content="Near Real-Time Ingestion">





  <meta property="article:modified_time" content="2019-12-30T14:59:57-05:00">







<!-- end _includes/seo.html -->


<!--<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Apache Hudi Feed">-->

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



<link rel="icon" type="image/x-icon" href="/assets/images/favicon.ico">
<link rel="stylesheet" href="/assets/css/font-awesome.min.css">

  </head>

  <body class="layout--single">
    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap" id="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/hudi.png" alt=""></a>
        
        <a class="site-title" href="/">
          Apache Hudi
          <span class="site-subtitle">0.5.1-SNAPSHOT(incubating)</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/docs/quick-start-guide.html" target="_self" >Documentation</a>
            </li><li class="masthead__menu-item">
              <a href="/community.html" target="_self" >Community</a>
            </li><li class="masthead__menu-item">
              <a href="/roadmap.html" target="_self" >Roadmap</a>
            </li><li class="masthead__menu-item">
              <a href="/activity.html" target="_self" >Activities</a>
            </li><li class="masthead__menu-item">
              <a href="https://cwiki.apache.org/confluence/display/HUDI/FAQ" target="_blank" >FAQ</a>
            </li><li class="masthead__menu-item">
              <a href="/releases.html" target="_self" >Releases</a>
            </li></ul>
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      <div id="main" role="main">
  
  <div class="sidebar sticky">

  

  
    
      
      
      
      
    


    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">Toggle menu</label>
  <ul class="nav__items">
    
      <li>
        
          <span class="nav__sub-title">Getting Started</span>
        

        
        <ul>
          
            
            

            
            

            
              <li><a href="/docs/quick-start-guide.html" class="">Quick Start</a></li>
            

          
            
            

            
            

            
              <li><a href="/docs/structure.html" class="">Structure</a></li>
            

          
            
            

            
            

            
              <li><a href="/docs/use_cases.html" class="active">User Cases</a></li>
            

          
            
            

            
            

            
              <li><a href="/docs/powered_by.html" class="">Talks & Powered By</a></li>
            

          
            
            

            
            

            
              <li><a href="/docs/comparison.html" class="">Comparison</a></li>
            

          
        </ul>
        
      </li>
    
      <li>
        
          <span class="nav__sub-title">Documentation</span>
        

        
        <ul>
          
            
            

            
            

            
              <li><a href="/docs/concepts.html" class="">Concepts</a></li>
            

          
            
            

            
            

            
              <li><a href="/docs/writing_data.html" class="">Writing Data</a></li>
            

          
            
            

            
            

            
              <li><a href="/docs/querying_data.html" class="">Querying Data</a></li>
            

          
            
            

            
            

            
              <li><a href="/docs/configurations.html" class="">Configuration</a></li>
            

          
            
            

            
            

            
              <li><a href="/docs/performance.html" class="">Performance</a></li>
            

          
            
            

            
            

            
              <li><a href="/docs/admin_guide.html" class="">Administering</a></li>
            

          
        </ul>
        
      </li>
    
      <li>
        
          <span class="nav__sub-title">Meta Info</span>
        

        
        <ul>
          
            
            

            
            

            
              <li><a href="/docs/docs-versions.html" class="">Docs Versions</a></li>
            

          
            
            

            
            

            
              <li><a href="/docs/privacy.html" class="">Privacy Policy</a></li>
            

          
        </ul>
        
      </li>
    
  </ul>
</nav>
    
  
  </div>


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Use Cases
</h1>
        </header>
      

      <section class="page__content" itemprop="text">
        
        <aside class="sidebar__right sticky">
          <nav class="toc">
            <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> Section Nav</h4></header>
            <ul class="toc__menu">
  <li><a href="#near-real-time-ingestion">Near Real-Time Ingestion</a></li>
  <li><a href="#near-real-time-analytics">Near Real-time Analytics</a></li>
  <li><a href="#incremental-processing-pipelines">Incremental Processing Pipelines</a></li>
  <li><a href="#data-dispersal-from-dfs">Data Dispersal From DFS</a></li>
</ul>
          </nav>
        </aside>
        
        <h2 id="near-real-time-ingestion">Near Real-Time Ingestion</h2>

<p>Ingesting data from external sources like (event logs, databases, external sources) into a <a href="http://martinfowler.com/bliki/DataLake.html">Hadoop Data Lake</a> is a well known problem.
In most (if not all) Hadoop deployments, it is unfortunately solved in a piecemeal fashion, using a medley of ingestion tools,
even though this data is arguably the most valuable for the entire organization.</p>

<p>For RDBMS ingestion, Hudi provides <strong>faster loads via Upserts</strong>, as opposed costly &amp; inefficient bulk loads. For e.g, you can read the MySQL BIN log or <a href="https://sqoop.apache.org/docs/1.4.2/SqoopUserGuide.html#_incremental_imports">Sqoop Incremental Import</a> and apply them to an
equivalent Hudi table on DFS. This would be much faster/efficient than a <a href="https://sqoop.apache.org/docs/1.4.0-incubating/SqoopUserGuide.html#id1770457">bulk merge job</a>
or <a href="http://hortonworks.com/blog/four-step-strategy-incremental-updates-hive/">complicated handcrafted merge workflows</a></p>

<p>For NoSQL datastores like <a href="http://cassandra.apache.org/">Cassandra</a> / <a href="http://www.project-voldemort.com/voldemort/">Voldemort</a> / <a href="https://hbase.apache.org/">HBase</a>, even moderately big installations store billions of rows.
It goes without saying that <strong>full bulk loads are simply infeasible</strong> and more efficient approaches are needed if ingestion is to keep up with the typically high update volumes.</p>

<p>Even for immutable data sources like <a href="kafka.apache.org">Kafka</a> , Hudi helps <strong>enforces a minimum file size on HDFS</strong>, which improves NameNode health by solving one of the <a href="https://blog.cloudera.com/blog/2009/02/the-small-files-problem/">age old problems in Hadoop land</a> in a holistic way. This is all the more important for event streams, since typically its higher volume (eg: click streams) and if not managed well, can cause serious damage to your Hadoop cluster.</p>

<p>Across all sources, Hudi adds the much needed ability to atomically publish new data to consumers via notion of commits, shielding them from partial ingestion failures</p>

<h2 id="near-real-time-analytics">Near Real-time Analytics</h2>

<p>Typically, real-time <a href="https://en.wikipedia.org/wiki/Data_mart">datamarts</a> are powered by specialized analytical stores such as <a href="http://druid.io/">Druid</a> or <a href="http://www.memsql.com/">Memsql</a> or <a href="http://opentsdb.net/">even OpenTSDB</a> .
This is absolutely perfect for lower scale (<a href="https://blog.twitter.com/2015/hadoop-filesystem-at-twitter">relative to Hadoop installations like this</a>) data, that needs sub-second query responses such as system monitoring or interactive real-time analysis.
But, typically these systems end up getting abused for less interactive queries also since data on Hadoop is intolerably stale. This leads to under utilization &amp; wasteful hardware/license costs.</p>

<p>On the other hand, interactive SQL solutions on Hadoop such as Presto &amp; SparkSQL excel in <strong>queries that finish within few seconds</strong>.
By bringing <strong>data freshness to a few minutes</strong>, Hudi can provide a much efficient alternative, as well unlock real-time analytics on <strong>several magnitudes larger datasets</strong> stored in DFS.
Also, Hudi has no external dependencies (like a dedicated HBase cluster, purely used for real-time analytics) and thus enables faster analytics on much fresher analytics, without increasing the operational overhead.</p>

<h2 id="incremental-processing-pipelines">Incremental Processing Pipelines</h2>

<p>One fundamental ability Hadoop provides is to build a chain of datasets derived from each other via DAGs expressed as workflows.
Workflows often depend on new data being output by multiple upstream workflows and traditionally, availability of new data is indicated by a new DFS Folder/Hive Partition.
Let’s take a concrete example to illustrate this. An upstream workflow <code class="highlighter-rouge">U</code> can create a Hive partition for every hour, with data for that hour (event_time) at the end of each hour (processing_time), providing effective freshness of 1 hour.
Then, a downstream workflow <code class="highlighter-rouge">D</code>, kicks off immediately after <code class="highlighter-rouge">U</code> finishes, and does its own processing for the next hour, increasing the effective latency to 2 hours.</p>

<p>The above paradigm simply ignores late arriving data i.e when <code class="highlighter-rouge">processing_time</code> and <code class="highlighter-rouge">event_time</code> drift apart.
Unfortunately, in today’s post-mobile &amp; pre-IoT world, <strong>late data from intermittently connected mobile devices &amp; sensors are the norm, not an anomaly</strong>.
In such cases, the only remedy to guarantee correctness is to <a href="https://falcon.apache.org/FalconDocumentation.html#Handling_late_input_data">reprocess the last few hours</a> worth of data,
over and over again each hour, which can significantly hurt the efficiency across the entire ecosystem. For e.g; imagine reprocessing TBs worth of data every hour across hundreds of workflows.</p>

<p>Hudi comes to the rescue again, by providing a way to consume new data (including late data) from an upsteam Hudi dataset <code class="highlighter-rouge">HU</code> at a record granularity (not folders/partitions),
apply the processing logic, and efficiently update/reconcile late data with a downstream Hudi dataset <code class="highlighter-rouge">HD</code>. Here, <code class="highlighter-rouge">HU</code> and <code class="highlighter-rouge">HD</code> can be continuously scheduled at a much more frequent schedule
like 15 mins, and providing an end-end latency of 30 mins at <code class="highlighter-rouge">HD</code>.</p>

<p>To achieve this, Hudi has embraced similar concepts from stream processing frameworks like <a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html#join-operations">Spark Streaming</a> , Pub/Sub systems like <a href="http://kafka.apache.org/documentation/#theconsumer">Kafka</a>
or database replication technologies like <a href="https://docs.oracle.com/cd/E11882_01/server.112/e16545/xstrm_cncpt.htm#XSTRM187">Oracle XStream</a>.
For the more curious, a more detailed explanation of the benefits of Incremental Processing (compared to Stream Processing &amp; Batch Processing) can be found <a href="https://www.oreilly.com/ideas/ubers-case-for-incremental-processing-on-hadoop">here</a></p>

<h2 id="data-dispersal-from-dfs">Data Dispersal From DFS</h2>

<p>A popular use-case for Hadoop, is to crunch data and then disperse it back to an online serving store, to be used by an application.
For e.g, a Spark Pipeline can <a href="https://eng.uber.com/telematics/">determine hard braking events on Hadoop</a> and load them into a serving store like ElasticSearch, to be used by the Uber application to increase safe driving. Typical architectures for this employ a <code class="highlighter-rouge">queue</code> between Hadoop and serving store, to prevent overwhelming the target serving store.
A popular choice for this queue is Kafka and this model often results in <strong>redundant storage of same data on DFS (for offline analysis on computed results) and Kafka (for dispersal)</strong></p>

<p>Once again Hudi can efficiently solve this problem, by having the Spark Pipeline upsert output from
each run into a Hudi dataset, which can then be incrementally tailed (just like a Kafka topic) for new data &amp; written into the serving store.</p>

        
      </section>

      <a href="#masthead__inner-wrap" class="back-to-top">Back to top &uarr;</a>


      

    </div>

  </article>

</div>

    </div>

    <div class="page__footer">
      <footer>
        
<div class="row">
  <div class="col-lg-12 footer">
    <p>
      <a class="footer-link-img" href="https://apache.org">
        <img width="250px" src="/assets/images/asf_logo.svg" alt="The Apache Software Foundation">
      </a>
    </p>
    <p>
      Copyright &copy; <span id="copyright-year">2019</span> <a href="https://apache.org">The Apache Software Foundation</a>, Licensed under the Apache License, Version 2.0.
      Apache and the Apache feather logo are trademarks of The Apache Software Foundation. <a href="/docs/privacy">Privacy Policy</a>
      <br>
      Apache Hudi is an effort undergoing incubation at The Apache Software Foundation (ASF), sponsored by the <a href="http://incubator.apache.org/">Apache Incubator</a>.
      Incubation is required of all newly accepted projects until a further review indicates that the infrastructure, communications, and decision making process have
      stabilized in a manner consistent with other successful ASF projects. While incubation status is not necessarily a
      reflection of the completeness or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF.
    </p>
  </div>
</div>
      </footer>
    </div>

    
<script src="/assets/js/main.min.js"></script>


  </body>
</html>