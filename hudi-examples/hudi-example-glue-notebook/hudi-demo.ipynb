{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "\n# Glue Studio Notebook\nYou are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n\n## Available Magics\n|          Magic              |   Type       |                                                                        Description                                                                        |\n|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n| %region                     |  String      |  Specify the AWS region in which to initialize a session.                                                                                                 |\n| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0 (eg: %glue_version 2.0).                               |\n| %security_config            |  String      |  Define a security configuration to be used with this session.                                                                                            |\n| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n| %etl                        |  String      |  Changes the session type to Glue ETL.                                                                                                                    |\n| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n| %stop_session               |              |  Stops the current session.                                                                                                                               |\n| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |\n| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X.                                                                           |\n| %spark_conf                 |  String      |  Specify custom spark configurations for your session. E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer.                      |",
			"metadata": {
				"editable": false,
				"deletable": false,
				"tags": [],
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "# %stop_session",
			"metadata": {
				"trusted": true
			},
			"execution_count": 12,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "# Step 1: Define your configurations",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "%connections hudi-connection\n%glue_version 3.0\n%region us-west-2\n%worker_type G.1X\n%number_of_workers 3\n%spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n%additional_python_modules Faker",
			"metadata": {
				"trusted": true
			},
			"execution_count": 31,
			"outputs": [
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 54019378-cddd-4a85-9f0d-0fb532ef92ba.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Connections to be included:\nhudi-connection\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 54019378-cddd-4a85-9f0d-0fb532ef92ba.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Setting Glue version to: 3.0\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 54019378-cddd-4a85-9f0d-0fb532ef92ba.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous region: us-west-2\nSetting new region to: us-west-2\nReauthenticating Glue client with new region: us-west-2\nIAM role has been set to arn:aws:iam::043916019468:role/Lab3. Reauthenticating.\nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::043916019468:role/Lab3\nAuthentication done.\nRegion is set to: us-west-2\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 54019378-cddd-4a85-9f0d-0fb532ef92ba.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous worker type: G.1X\nSetting new worker type to: G.1X\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 54019378-cddd-4a85-9f0d-0fb532ef92ba.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous number of workers: 3\nSetting new number of workers to: 3\nPrevious Spark configuration: spark.serializer=org.apache.spark.serializer.KryoSerializer\nSetting new Spark configuration to: spark.serializer=org.apache.spark.serializer.KryoSerializer\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 54019378-cddd-4a85-9f0d-0fb532ef92ba.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Additional python modules to be included:\nFaker\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 2: Define your Imports",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "try:\n    import sys\n    from awsglue.transforms import *\n    from awsglue.utils import getResolvedOptions\n    from pyspark.context import SparkContext\n    from awsglue.context import GlueContext\n    from awsglue.job import Job\n    from pyspark.sql.session import SparkSession\n    from awsglue.dynamicframe import DynamicFrame\n    from pyspark.sql.functions import col, to_timestamp, monotonically_increasing_id, to_date, when\n    from pyspark.sql.functions import *\n    from awsglue.utils import getResolvedOptions\n    from pyspark.sql.types import *\n    from datetime import datetime\n    import boto3\n    from functools import reduce\nexcept Exception as e:\n    pass",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 81,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 3:  Create Spark Session",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "spark = SparkSession.builder.config('spark.serializer','org.apache.spark.serializer.KryoSerializer').config('spark.sql.hive.convertMetastoreParquet','false').config('spark.sql.legacy.pathOptionBehavior.enabled', 'true').getOrCreate()\nsc = spark.sparkContext\nglueContext = GlueContext(sc)\njob = Job(glueContext)\nlogger = glueContext.get_logger()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 82,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 4: Define Data Generator Class ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from faker import Faker\nglobal faker\nfaker = Faker()\n\nclass DataGenerator(object):\n\n    @staticmethod\n    def get_data():\n        return [\n            (\n                x,\n                faker.name(),\n                faker.random_element(elements=('IT', 'HR', 'Sales', 'Marketing')),\n                faker.random_element(elements=('CA', 'NY', 'TX', 'FL', 'IL', 'RJ')),\n                faker.random_int(min=10000, max=150000),\n                faker.random_int(min=18, max=60),\n                faker.random_int(min=0, max=100000),\n                faker.unix_time()\n            ) for x in range(3)\n        ]\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 83,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 5: Create Spark Data frame",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "data = DataGenerator.get_data()\n\ncolumns = [\"emp_id\", \"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\", \"ts\"]\nspark_df = spark.createDataFrame(data=data, schema=columns)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 84,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark_df.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 85,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------+--------------+----------+-----+------+---+-----+----------+\n|emp_id| employee_name|department|state|salary|age|bonus|        ts|\n+------+--------------+----------+-----+------+---+-----+----------+\n|     0|Cassidy Hansen|        IT|   FL|106824| 51|25134| 631540494|\n|     1|   Sherri Long|     Sales|   FL|119716| 54|50529| 465602763|\n|     2|  Sandra Smith|        IT|   IL|101594| 38| 7895|1035622594|\n+------+--------------+----------+-----+------+---+-----+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark_df = ChangeSchemaApplyMapping.toDF()\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "NameError: name 'ChangeSchemaApplyMapping' is not defined\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark_df.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "# Step 7:  Define your HUDI Settings ",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "db_name = \"hudidb\"\ntable_name=\"hudi_table\"\n\nrecordkey = 'emp_id'\nprecombine = 'ts'\n\npath = \"s3://soumil-dms-learn/hudi/hudi_table/\"\nmethod = 'upsert'\ntable_type = \"COPY_ON_WRITE\"",
			"metadata": {
				"trusted": true
			},
			"execution_count": 86,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "connection_options={\n    \"path\": path,\n    \"connectionName\": \"hudi-connection\",\n\n    \"hoodie.datasource.write.storage.type\": table_type,\n    'className': 'org.apache.hudi',\n    'hoodie.table.name': table_name,\n    'hoodie.datasource.write.recordkey.field': recordkey,\n    'hoodie.datasource.write.table.name': table_name,\n    'hoodie.datasource.write.operation': method,\n    'hoodie.datasource.write.precombine.field': precombine,\n\n\n    'hoodie.datasource.hive_sync.enable': 'true',\n    \"hoodie.datasource.hive_sync.mode\":\"hms\",\n    'hoodie.datasource.hive_sync.sync_as_datasource': 'false',\n    'hoodie.datasource.hive_sync.database': db_name,\n    'hoodie.datasource.hive_sync.table': table_name,\n    'hoodie.datasource.hive_sync.use_jdbc': 'false',\n    'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n    'hoodie.datasource.write.hive_style_partitioning': 'true',\n}",
			"metadata": {
				"trusted": true
			},
			"execution_count": 87,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 8: Write to HUDI ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "ApacheHudiConnector0101forAWSGlue30_node1671045598524 = (\n    glueContext.write_dynamic_frame.from_options(\n        frame=DynamicFrame.fromDF(spark_df, glueContext,\"glue_df\"),\n        connection_type=\"marketplace.spark\",\n        connection_options=connection_options,\n        transformation_ctx=\"glue_df\",\n    )\n)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 88,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 9 : Read from HUDI Table",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "ReadGlueDF = (\n    glueContext.create_dynamic_frame.from_options(\n        connection_type=\"marketplace.spark\",\n        connection_options=connection_options,\n        transformation_ctx=\"ReadGlueDF\",\n    )\n)\nglue_to_spark_df = ReadGlueDF.toDF()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 89,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "glue_to_spark_df.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 90,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------+----------+-----+------+---+-----+----------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|emp_id| employee_name|department|state|salary|age|bonus|        ts|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------+----------+-----+------+---+-----+----------+\n|  20221218143859751|20221218143859751...|                 2|                      |e7ad30a4-27ce-4de...|     2|  Sandra Smith|        IT|   IL|101594| 38| 7895|1035622594|\n|  20221218143859751|20221218143859751...|                 0|                      |e7ad30a4-27ce-4de...|     0|Cassidy Hansen|        IT|   FL|106824| 51|25134| 631540494|\n|  20221218143859751|20221218143859751...|                 1|                      |e7ad30a4-27ce-4de...|     1|   Sherri Long|     Sales|   FL|119716| 54|50529| 465602763|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------+----------+-----+------+---+-----+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 10 Append into HUDI",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "impleDataUpd = [\n    (3, \"This is APPEND\", \"Sales\", \"RJ\", 81000, 30, 23000, 827307999),\n    (4, \"This is APPEND\", \"Engineering\", \"RJ\", 79000, 53, 15000, 1627694678),\n]\n\ncolumns = [\"emp_id\", \"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\", \"ts\"]\nspark_df = spark.createDataFrame(data=impleDataUpd, schema=columns)\n\nWriteDF = (\n    glueContext.write_dynamic_frame.from_options(\n        frame=DynamicFrame.fromDF(spark_df, glueContext,\"glue_df\"),\n        connection_type=\"marketplace.spark\",\n        connection_options=connection_options,\n        transformation_ctx=\"glue_df\",\n    )\n)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 91,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "ReadGlueDF = (\n    glueContext.create_dynamic_frame.from_options(\n        connection_type=\"marketplace.spark\",\n        connection_options=connection_options,\n        transformation_ctx=\"ReadGlueDF\",\n    )\n)\nglue_to_spark_df = ReadGlueDF.toDF()\nglue_to_spark_df.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 92,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------+-----------+-----+------+---+-----+----------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|emp_id| employee_name| department|state|salary|age|bonus|        ts|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------+-----------+-----+------+---+-----+----------+\n|  20221218143945423|20221218143945423...|                 4|                      |e7ad30a4-27ce-4de...|     4|This is APPEND|Engineering|   RJ| 79000| 53|15000|1627694678|\n|  20221218143859751|20221218143859751...|                 2|                      |e7ad30a4-27ce-4de...|     2|  Sandra Smith|         IT|   IL|101594| 38| 7895|1035622594|\n|  20221218143945423|20221218143945423...|                 3|                      |e7ad30a4-27ce-4de...|     3|This is APPEND|      Sales|   RJ| 81000| 30|23000| 827307999|\n|  20221218143859751|20221218143859751...|                 0|                      |e7ad30a4-27ce-4de...|     0|Cassidy Hansen|         IT|   FL|106824| 51|25134| 631540494|\n|  20221218143859751|20221218143859751...|                 1|                      |e7ad30a4-27ce-4de...|     1|   Sherri Long|      Sales|   FL|119716| 54|50529| 465602763|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------+-----------+-----+------+---+-----+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 11: Update in HUDI table",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "impleDataUpd = [\n    (3, \"this is update on data lake\", \"Sales\", \"RJ\", 81000, 30, 23000, 827307999),\n]\ncolumns = [\"emp_id\", \"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\", \"ts\"]\nspark_df = spark.createDataFrame(data=impleDataUpd, schema=columns)\n\nWriteDF = (\n    glueContext.write_dynamic_frame.from_options(\n        frame=DynamicFrame.fromDF(spark_df, glueContext,\"glue_df\"),\n        connection_type=\"marketplace.spark\",\n        connection_options=connection_options,\n        transformation_ctx=\"glue_df\",\n    )\n)\n\nReadGlueDF = (\n    glueContext.create_dynamic_frame.from_options(\n        connection_type=\"marketplace.spark\",\n        connection_options=connection_options,\n        transformation_ctx=\"ReadGlueDF\",\n    )\n)\nglue_to_spark_df = ReadGlueDF.toDF()\nglue_to_spark_df.show()\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 93,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------------+-----------+-----+------+---+-----+----------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|emp_id|       employee_name| department|state|salary|age|bonus|        ts|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------------+-----------+-----+------+---+-----+----------+\n|  20221218143945423|20221218143945423...|                 4|                      |e7ad30a4-27ce-4de...|     4|      This is APPEND|Engineering|   RJ| 79000| 53|15000|1627694678|\n|  20221218143859751|20221218143859751...|                 2|                      |e7ad30a4-27ce-4de...|     2|        Sandra Smith|         IT|   IL|101594| 38| 7895|1035622594|\n|  20221218144030080|20221218144030080...|                 3|                      |e7ad30a4-27ce-4de...|     3|this is update on...|      Sales|   RJ| 81000| 30|23000| 827307999|\n|  20221218143859751|20221218143859751...|                 0|                      |e7ad30a4-27ce-4de...|     0|      Cassidy Hansen|         IT|   FL|106824| 51|25134| 631540494|\n|  20221218143859751|20221218143859751...|                 1|                      |e7ad30a4-27ce-4de...|     1|         Sherri Long|      Sales|   FL|119716| 54|50529| 465602763|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------------+-----------+-----+------+---+-----+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 12: Query with Spark SQL",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"select * from  hudidb.hudi_table where emp_id = 3 \").show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 94,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------------+----------+-----+------+---+-----+---------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|emp_id|       employee_name|department|state|salary|age|bonus|       ts|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------------+----------+-----+------+---+-----+---------+\n|  20221218144030080|20221218144030080...|                 3|                      |e7ad30a4-27ce-4de...|     3|this is update on...|     Sales|   RJ| 81000| 30|23000|827307999|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------------+----------+-----+------+---+-----+---------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 13: Hard Delete",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "spark_df = spark.sql(\"SELECT * FROM hudidb.hudi_table where emp_id='4' \")\n\nprint(\"********************\")\nprint(spark_df.show())\nprint(\"********************\")\n\ndb_name = \"hudidb\"\ntable_name=\"hudi_table\"\n\nrecordkey = 'emp_id'\nprecombine = 'ts'\n\npath = \"s3://soumil-dms-learn/hudi/hudi_table/\"\nmethod = 'delete'\ntable_type = \"COPY_ON_WRITE\"\n\nconnection_options={\n    \"path\": path,\n    \"connectionName\": \"hudi-connection\",\n\n    \"hoodie.datasource.write.storage.type\": table_type,\n    'className': 'org.apache.hudi',\n    'hoodie.table.name': table_name,\n    'hoodie.datasource.write.recordkey.field': recordkey,\n    'hoodie.datasource.write.table.name': table_name,\n    'hoodie.datasource.write.operation': method,\n    'hoodie.datasource.write.precombine.field': precombine,\n\n\n    'hoodie.datasource.hive_sync.enable': 'true',\n    \"hoodie.datasource.hive_sync.mode\":\"hms\",\n    'hoodie.datasource.hive_sync.sync_as_datasource': 'false',\n    'hoodie.datasource.hive_sync.database': db_name,\n    'hoodie.datasource.hive_sync.table': table_name,\n    'hoodie.datasource.hive_sync.use_jdbc': 'false',\n    'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n    'hoodie.datasource.write.hive_style_partitioning': 'true',\n}\n\n\n\nWriteDF = (\n    glueContext.write_dynamic_frame.from_options(\n        frame=DynamicFrame.fromDF(spark_df, glueContext,\"glue_df\"),\n        connection_type=\"marketplace.spark\",\n        connection_options=connection_options,\n        transformation_ctx=\"glue_df\",\n    )\n)\n\nReadGlueDF = (\n    glueContext.create_dynamic_frame.from_options(\n        connection_type=\"marketplace.spark\",\n        connection_options=connection_options,\n        transformation_ctx=\"ReadGlueDF\",\n    )\n)\nglue_to_spark_df = ReadGlueDF.toDF()\nglue_to_spark_df.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 95,
			"outputs": [
				{
					"name": "stdout",
					"text": "********************\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------+-----------+-----+------+---+-----+----------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|emp_id| employee_name| department|state|salary|age|bonus|        ts|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------+-----------+-----+------+---+-----+----------+\n|  20221218143945423|20221218143945423...|                 4|                      |e7ad30a4-27ce-4de...|     4|This is APPEND|Engineering|   RJ| 79000| 53|15000|1627694678|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------+-----------+-----+------+---+-----+----------+\n\nNone\n********************\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------------+----------+-----+------+---+-----+----------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|emp_id|       employee_name|department|state|salary|age|bonus|        ts|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------------+----------+-----+------+---+-----+----------+\n|  20221218143859751|20221218143859751...|                 2|                      |e7ad30a4-27ce-4de...|     2|        Sandra Smith|        IT|   IL|101594| 38| 7895|1035622594|\n|  20221218144030080|20221218144030080...|                 3|                      |e7ad30a4-27ce-4de...|     3|this is update on...|     Sales|   RJ| 81000| 30|23000| 827307999|\n|  20221218143859751|20221218143859751...|                 0|                      |e7ad30a4-27ce-4de...|     0|      Cassidy Hansen|        IT|   FL|106824| 51|25134| 631540494|\n|  20221218143859751|20221218143859751...|                 1|                      |e7ad30a4-27ce-4de...|     1|         Sherri Long|     Sales|   FL|119716| 54|50529| 465602763|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------------+----------+-----+------+---+-----+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 14: Clustering and compaction",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": " ![image](https://user-images.githubusercontent.com/39345855/208302373-6624c2fc-b2ed-4fee-bb60-984d7cb0ec20.png)\n",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "db_name = \"hudidb\"\ntable_name=\"hudi_table\"\nrecordkey = 'emp_id'\nprecombine = 'ts'\npath = \"s3://soumil-dms-learn/hudi/hudi_table/\"\nmethod = 'upsert'\ntable_type = \"COPY_ON_WRITE\"\n\nconnection_options={\n    \"path\": path,\n    \"connectionName\": \"hudi-connection\",\n\n    \"hoodie.datasource.write.storage.type\": table_type,\n    'className': 'org.apache.hudi',\n    'hoodie.table.name': table_name,\n    'hoodie.datasource.write.recordkey.field': recordkey,\n    'hoodie.datasource.write.table.name': table_name,\n    'hoodie.datasource.write.operation': method,\n    'hoodie.datasource.write.precombine.field': precombine,\n\n\n    'hoodie.datasource.hive_sync.enable': 'true',\n    \"hoodie.datasource.hive_sync.mode\":\"hms\",\n    'hoodie.datasource.hive_sync.sync_as_datasource': 'false',\n    'hoodie.datasource.hive_sync.database': db_name,\n    'hoodie.datasource.hive_sync.table': table_name,\n    'hoodie.datasource.hive_sync.use_jdbc': 'false',\n    'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n    'hoodie.datasource.write.hive_style_partitioning': 'true',\n    \n    \"hoodie.clustering.plan.strategy.sort.columns\":\"state\",\n    \"hoodie.clustering.plan.strategy.max.bytes.per.group\" : '107374182400',\n    'hoodie.clustering.plan.strategy.max.num.groups' : '1',\n    'hoodie.cleaner.policy' : 'KEEP_LATEST_FILE_VERSIONS',\n    \n}\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 97,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "cluster_df = spark.sql(\"SELECT * FROM hudidb.hudi_table \")\n\nWriteDF = (\n    glueContext.write_dynamic_frame.from_options(\n        frame=DynamicFrame.fromDF(cluster_df, glueContext,\"glue_df\"),\n        connection_type=\"marketplace.spark\",\n        connection_options=connection_options,\n        transformation_ctx=\"glue_df\",\n    )\n)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 98,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": " ![image](https://user-images.githubusercontent.com/39345855/208302689-cd37ea24-6b65-411d-bc7f-146a3f979c7a.png)\n",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "# Step 15: Time Travel Query",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df = spark.read. \\\n  format(\"hudi\"). \\\n  option(\"as.of.instant\", \"2022-18-12\"). \\\n  load(path)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 99,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 100,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------------+----------+-----+------+---+-----+----------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|emp_id|       employee_name|department|state|salary|age|bonus|        ts|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------------+----------+-----+------+---+-----+----------+\n|  20221218144316383|20221218144316383...|                 1|                      |e7ad30a4-27ce-4de...|     1|         Sherri Long|     Sales|   FL|119716| 54|50529| 465602763|\n|  20221218144316383|20221218144316383...|                 0|                      |e7ad30a4-27ce-4de...|     0|      Cassidy Hansen|        IT|   FL|106824| 51|25134| 631540494|\n|  20221218144316383|20221218144316383...|                 2|                      |e7ad30a4-27ce-4de...|     2|        Sandra Smith|        IT|   IL|101594| 38| 7895|1035622594|\n|  20221218144316383|20221218144316383...|                 3|                      |e7ad30a4-27ce-4de...|     3|this is update on...|     Sales|   RJ| 81000| 30|23000| 827307999|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------------+----------+-----+------+---+-----+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Incremental query",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "spark. \\\n      read. \\\n      format(\"hudi\"). \\\n      load(path). \\\n      createOrReplaceTempView(\"hudi_snapshot\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 101,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "commits = list(map(lambda row: row[0], spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from  hudi_snapshot order by commitTime\").limit(50).collect()))\nbeginTime = commits[len(commits) - 2] # commit time we are interested in",
			"metadata": {
				"trusted": true
			},
			"execution_count": 102,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "beginTime",
			"metadata": {
				"trusted": true
			},
			"execution_count": 103,
			"outputs": [
				{
					"name": "stdout",
					"text": "'20221218144316383'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "commits",
			"metadata": {
				"trusted": true
			},
			"execution_count": 104,
			"outputs": [
				{
					"name": "stdout",
					"text": "['20221218144316383']\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "incremental_read_options = {\n  'hoodie.datasource.query.type': 'incremental',\n  'hoodie.datasource.read.begin.instanttime': beginTime,\n}",
			"metadata": {
				"trusted": true
			},
			"execution_count": 105,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "IncrementalDF = spark.read.format(\"hudi\"). \\\n  options(**incremental_read_options). \\\n  load(path)\n\nIncrementalDF.createOrReplaceTempView(\"hudi_incremental\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 106,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"select * from  hudi_incremental\").show()\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 107,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+--------------------+------------------+----------------------+-----------------+------+-------------+----------+-----+------+---+-----+---+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name|emp_id|employee_name|department|state|salary|age|bonus| ts|\n+-------------------+--------------------+------------------+----------------------+-----------------+------+-------------+----------+-----+------+---+-----+---+\n+-------------------+--------------------+------------------+----------------------+-----------------+------+-------------+----------+-----+------+---+-----+---+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Appending data for incremental data processing ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "impleDataUpd = [\n    (6, \"This is APPEND\", \"Sales\", \"RJ\", 81000, 30, 23000, 827307999),\n    (7, \"This is APPEND\", \"Engineering\", \"RJ\", 79000, 53, 15000, 1627694678),\n]\n\ncolumns = [\"emp_id\", \"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\", \"ts\"]\nspark_df = spark.createDataFrame(data=impleDataUpd, schema=columns)\n\nWriteDF = (\n    glueContext.write_dynamic_frame.from_options(\n        frame=DynamicFrame.fromDF(spark_df, glueContext,\"glue_df\"),\n        connection_type=\"marketplace.spark\",\n        connection_options=connection_options,\n        transformation_ctx=\"glue_df\",\n    )\n)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 108,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "incremental_read_options = {\n  'hoodie.datasource.query.type': 'incremental',\n  'hoodie.datasource.read.begin.instanttime': beginTime,\n}",
			"metadata": {
				"trusted": true
			},
			"execution_count": 109,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "IncrementalDF = spark.read.format(\"hudi\"). \\\n  options(**incremental_read_options). \\\n  load(path)\n\nIncrementalDF.createOrReplaceTempView(\"hudi_incremental\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 110,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"select * from  hudi_incremental\").show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 111,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------+-----------+-----+------+---+-----+----------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|emp_id| employee_name| department|state|salary|age|bonus|        ts|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------+-----------+-----+------+---+-----+----------+\n|  20221218144431412|20221218144431412...|                 6|                      |e7ad30a4-27ce-4de...|     6|This is APPEND|      Sales|   RJ| 81000| 30|23000| 827307999|\n|  20221218144431412|20221218144431412...|                 7|                      |e7ad30a4-27ce-4de...|     7|This is APPEND|Engineering|   RJ| 79000| 53|15000|1627694678|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------+-----------+-----+------+---+-----+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}